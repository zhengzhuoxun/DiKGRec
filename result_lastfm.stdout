2025-02-12 22:13:32.180485: Start
Begin to load knowledge graph triples ...
In KG: 18 relations, 106389 entities, 929134 triples
2025-02-12 22:13:43.881210: Load Data
2025-02-12 22:13:43.881229: Namespace(batch=400, cold_start_num=0, d_emb_size=10, data='lastfm', diff_type=0, dims='[1000]', entity=106389, epoch=1000, gpu='0', head=2, head_diff_ratio=0, item=48123, kg_loss_ratio=0.8, kg_norm=2, latdim=64, layer=4, load_model=None, lr=5e-05, lr2=0.0005, noise_max=0.005, noise_min=0.0005, noise_ratio=0.2, noise_scale=0.0005, norm=True, relation=18, res_lambda=0.5, sampling_steps=0, save_path='tem', seed=421, steps=5, topk=20, tstBat=512, tstEpoch=1, updateW=1.0, user=23566)
USER 23566 ITEM 48123
NUM OF INTERACTIONS 1289003
Number of all parameters: 96305269
2025-02-12 22:13:44.772957: Model Prepared
2025-02-12 22:13:44.772971: Model Initialized
2025-02-12 22:13:50.261227: Training Step 0/59: batchLoss = 3.8101, diffLoss = 18.7130, kgLoss = 0.0844
2025-02-12 22:13:51.173320: Training Step 1/59: batchLoss = 3.9833, diffLoss = 19.5408, kgLoss = 0.0940
2025-02-12 22:13:52.092907: Training Step 2/59: batchLoss = 4.1283, diffLoss = 20.2522, kgLoss = 0.0974
2025-02-12 22:13:53.000320: Training Step 3/59: batchLoss = 3.6258, diffLoss = 17.7926, kgLoss = 0.0840
2025-02-12 22:13:53.912989: Training Step 4/59: batchLoss = 3.8890, diffLoss = 19.0553, kgLoss = 0.0975
2025-02-12 22:13:54.820906: Training Step 5/59: batchLoss = 3.8915, diffLoss = 19.1002, kgLoss = 0.0893
2025-02-12 22:13:55.731772: Training Step 6/59: batchLoss = 3.9472, diffLoss = 19.3574, kgLoss = 0.0946
2025-02-12 22:13:56.649028: Training Step 7/59: batchLoss = 4.2657, diffLoss = 20.9395, kgLoss = 0.0972
2025-02-12 22:13:57.561898: Training Step 8/59: batchLoss = 3.6906, diffLoss = 18.1232, kgLoss = 0.0825
2025-02-12 22:13:58.487163: Training Step 9/59: batchLoss = 3.4570, diffLoss = 16.9694, kgLoss = 0.0789
2025-02-12 22:13:59.400550: Training Step 10/59: batchLoss = 3.8651, diffLoss = 18.9768, kgLoss = 0.0872
2025-02-12 22:14:00.318195: Training Step 11/59: batchLoss = 3.8160, diffLoss = 18.7209, kgLoss = 0.0898
2025-02-12 22:14:01.238242: Training Step 12/59: batchLoss = 4.1734, diffLoss = 20.4868, kgLoss = 0.0950
2025-02-12 22:14:02.143839: Training Step 13/59: batchLoss = 4.8620, diffLoss = 23.9011, kgLoss = 0.1023
2025-02-12 22:14:03.059364: Training Step 14/59: batchLoss = 3.5382, diffLoss = 17.3419, kgLoss = 0.0873
2025-02-12 22:14:03.974424: Training Step 15/59: batchLoss = 4.0175, diffLoss = 19.7090, kgLoss = 0.0946
2025-02-12 22:14:04.891718: Training Step 16/59: batchLoss = 3.9045, diffLoss = 19.1430, kgLoss = 0.0949
2025-02-12 22:14:05.814194: Training Step 17/59: batchLoss = 3.9152, diffLoss = 19.2062, kgLoss = 0.0924
2025-02-12 22:14:06.734804: Training Step 18/59: batchLoss = 4.3746, diffLoss = 21.4640, kgLoss = 0.1022
2025-02-12 22:14:07.648334: Training Step 19/59: batchLoss = 4.2626, diffLoss = 20.9449, kgLoss = 0.0920
2025-02-12 22:14:08.562310: Training Step 20/59: batchLoss = 4.0266, diffLoss = 19.7471, kgLoss = 0.0965
2025-02-12 22:14:09.481269: Training Step 21/59: batchLoss = 3.6041, diffLoss = 17.6987, kgLoss = 0.0804
2025-02-12 22:14:10.404956: Training Step 22/59: batchLoss = 3.9738, diffLoss = 19.4916, kgLoss = 0.0943
2025-02-12 22:14:11.332676: Training Step 23/59: batchLoss = 4.5656, diffLoss = 22.4071, kgLoss = 0.1052
2025-02-12 22:14:12.245165: Training Step 24/59: batchLoss = 3.8651, diffLoss = 18.9573, kgLoss = 0.0921
2025-02-12 22:14:13.167015: Training Step 25/59: batchLoss = 5.1615, diffLoss = 25.3292, kgLoss = 0.1195
2025-02-12 22:14:14.084400: Training Step 26/59: batchLoss = 4.3036, diffLoss = 21.1233, kgLoss = 0.0987
2025-02-12 22:14:14.999425: Training Step 27/59: batchLoss = 3.9525, diffLoss = 19.4083, kgLoss = 0.0885
2025-02-12 22:14:15.916147: Training Step 28/59: batchLoss = 3.5839, diffLoss = 17.5758, kgLoss = 0.0859
2025-02-12 22:14:16.829452: Training Step 29/59: batchLoss = 4.3212, diffLoss = 21.1998, kgLoss = 0.1016
2025-02-12 22:14:17.754872: Training Step 30/59: batchLoss = 3.6605, diffLoss = 17.9474, kgLoss = 0.0888
2025-02-12 22:14:18.670834: Training Step 31/59: batchLoss = 4.2905, diffLoss = 21.0717, kgLoss = 0.0952
2025-02-12 22:14:19.588020: Training Step 32/59: batchLoss = 3.9834, diffLoss = 19.5573, kgLoss = 0.0899
2025-02-12 22:14:20.502984: Training Step 33/59: batchLoss = 4.5551, diffLoss = 22.3552, kgLoss = 0.1051
2025-02-12 22:14:21.429299: Training Step 34/59: batchLoss = 3.6373, diffLoss = 17.8110, kgLoss = 0.0938
2025-02-12 22:14:22.345195: Training Step 35/59: batchLoss = 4.1833, diffLoss = 20.5286, kgLoss = 0.0970
2025-02-12 22:14:23.251975: Training Step 36/59: batchLoss = 4.3499, diffLoss = 21.3726, kgLoss = 0.0942
2025-02-12 22:14:24.185097: Training Step 37/59: batchLoss = 3.8322, diffLoss = 18.7559, kgLoss = 0.1013
2025-02-12 22:14:25.107203: Training Step 38/59: batchLoss = 3.7435, diffLoss = 18.3478, kgLoss = 0.0924
2025-02-12 22:14:26.032802: Training Step 39/59: batchLoss = 3.8840, diffLoss = 19.0554, kgLoss = 0.0911
2025-02-12 22:14:26.951123: Training Step 40/59: batchLoss = 4.0828, diffLoss = 20.0483, kgLoss = 0.0914
2025-02-12 22:14:27.875277: Training Step 41/59: batchLoss = 3.7576, diffLoss = 18.4066, kgLoss = 0.0954
2025-02-12 22:14:28.803006: Training Step 42/59: batchLoss = 4.4863, diffLoss = 22.0269, kgLoss = 0.1011
2025-02-12 22:14:29.723582: Training Step 43/59: batchLoss = 4.2252, diffLoss = 20.7196, kgLoss = 0.1017
2025-02-12 22:14:30.646650: Training Step 44/59: batchLoss = 3.6076, diffLoss = 17.6980, kgLoss = 0.0850
2025-02-12 22:14:31.573272: Training Step 45/59: batchLoss = 4.1513, diffLoss = 20.3715, kgLoss = 0.0963
2025-02-12 22:14:32.506465: Training Step 46/59: batchLoss = 3.9302, diffLoss = 19.2872, kgLoss = 0.0910
2025-02-12 22:14:33.416209: Training Step 47/59: batchLoss = 3.7529, diffLoss = 18.3746, kgLoss = 0.0975
2025-02-12 22:14:34.324750: Training Step 48/59: batchLoss = 4.5763, diffLoss = 22.4832, kgLoss = 0.0996
2025-02-12 22:14:35.243577: Training Step 49/59: batchLoss = 3.3784, diffLoss = 16.5315, kgLoss = 0.0901
2025-02-12 22:14:36.165674: Training Step 50/59: batchLoss = 4.6069, diffLoss = 22.6240, kgLoss = 0.1027
2025-02-12 22:14:37.090512: Training Step 51/59: batchLoss = 3.7316, diffLoss = 18.2732, kgLoss = 0.0961
2025-02-12 22:14:38.007075: Training Step 52/59: batchLoss = 3.4149, diffLoss = 16.7514, kgLoss = 0.0808
2025-02-12 22:14:38.928662: Training Step 53/59: batchLoss = 3.7075, diffLoss = 18.2034, kgLoss = 0.0835
2025-02-12 22:14:39.845680: Training Step 54/59: batchLoss = 4.3331, diffLoss = 21.2770, kgLoss = 0.0971
2025-02-12 22:14:40.762721: Training Step 55/59: batchLoss = 3.4132, diffLoss = 16.7356, kgLoss = 0.0826
2025-02-12 22:14:41.672479: Training Step 56/59: batchLoss = 3.4223, diffLoss = 16.7834, kgLoss = 0.0820
2025-02-12 22:14:42.512054: Training Step 57/59: batchLoss = 4.2279, diffLoss = 20.7604, kgLoss = 0.0948
2025-02-12 22:14:43.357731: Training Step 58/59: batchLoss = 4.4708, diffLoss = 21.9412, kgLoss = 0.1032
2025-02-12 22:14:43.439356: 
2025-02-12 22:14:43.439669: Epoch 0/1000, Train: epLoss = 0.5904, epDfLoss = 2.8969, epKgLoss = 0.0138  
2025-02-12 22:14:44.989578: Steps 0/47: batch_recall = 21.68, batch_ndcg = 23.65 
2025-02-12 22:14:46.296356: Steps 1/47: batch_recall = 28.02, batch_ndcg = 25.79 
2025-02-12 22:14:47.562839: Steps 2/47: batch_recall = 26.38, batch_ndcg = 26.33 
2025-02-12 22:14:48.839335: Steps 3/47: batch_recall = 30.84, batch_ndcg = 27.51 
2025-02-12 22:14:50.039265: Steps 4/47: batch_recall = 26.87, batch_ndcg = 26.01 
2025-02-12 22:14:51.257222: Steps 5/47: batch_recall = 25.74, batch_ndcg = 25.02 
2025-02-12 22:14:52.457046: Steps 6/47: batch_recall = 28.11, batch_ndcg = 26.15 
2025-02-12 22:14:53.631814: Steps 7/47: batch_recall = 29.55, batch_ndcg = 26.63 
2025-02-12 22:14:54.802770: Steps 8/47: batch_recall = 33.39, batch_ndcg = 28.47 
2025-02-12 22:14:55.941632: Steps 9/47: batch_recall = 30.29, batch_ndcg = 29.13 
2025-02-12 22:14:57.105408: Steps 10/47: batch_recall = 30.30, batch_ndcg = 28.04 
2025-02-12 22:14:58.244011: Steps 11/47: batch_recall = 35.10, batch_ndcg = 28.87 
2025-02-12 22:14:59.388082: Steps 12/47: batch_recall = 36.17, batch_ndcg = 30.51 
2025-02-12 22:15:00.516863: Steps 13/47: batch_recall = 32.98, batch_ndcg = 29.63 
2025-02-12 22:15:01.608719: Steps 14/47: batch_recall = 28.85, batch_ndcg = 23.64 
2025-02-12 22:15:02.688452: Steps 15/47: batch_recall = 42.43, batch_ndcg = 33.16 
2025-02-12 22:15:03.769194: Steps 16/47: batch_recall = 33.36, batch_ndcg = 28.69 
2025-02-12 22:15:04.814277: Steps 17/47: batch_recall = 38.60, batch_ndcg = 32.10 
2025-02-12 22:15:05.889443: Steps 18/47: batch_recall = 34.56, batch_ndcg = 27.28 
2025-02-12 22:15:06.957983: Steps 19/47: batch_recall = 43.81, batch_ndcg = 34.34 
2025-02-12 22:15:08.002807: Steps 20/47: batch_recall = 49.02, batch_ndcg = 38.83 
2025-02-12 22:15:09.045028: Steps 21/47: batch_recall = 45.13, batch_ndcg = 32.97 
2025-02-12 22:15:10.095261: Steps 22/47: batch_recall = 37.82, batch_ndcg = 30.73 
2025-02-12 22:15:11.150206: Steps 23/47: batch_recall = 44.62, batch_ndcg = 34.96 
2025-02-12 22:15:12.211813: Steps 24/47: batch_recall = 46.75, batch_ndcg = 37.93 
2025-02-12 22:15:13.255922: Steps 25/47: batch_recall = 51.74, batch_ndcg = 39.06 
2025-02-12 22:15:14.269105: Steps 26/47: batch_recall = 47.92, batch_ndcg = 37.17 
2025-02-12 22:15:15.294964: Steps 27/47: batch_recall = 40.96, batch_ndcg = 30.07 
2025-02-12 22:15:16.309573: Steps 28/47: batch_recall = 46.94, batch_ndcg = 35.23 
2025-02-12 22:15:17.315393: Steps 29/47: batch_recall = 47.94, batch_ndcg = 34.76 
2025-02-12 22:15:18.316566: Steps 30/47: batch_recall = 55.62, batch_ndcg = 41.91 
2025-02-12 22:15:19.339435: Steps 31/47: batch_recall = 45.30, batch_ndcg = 35.27 
2025-02-12 22:15:20.333993: Steps 32/47: batch_recall = 51.55, batch_ndcg = 40.86 
2025-02-12 22:15:21.335367: Steps 33/47: batch_recall = 53.09, batch_ndcg = 39.38 
2025-02-12 22:15:22.335494: Steps 34/47: batch_recall = 42.29, batch_ndcg = 31.76 
2025-02-12 22:15:23.311070: Steps 35/47: batch_recall = 55.13, batch_ndcg = 39.47 
2025-02-12 22:15:24.273711: Steps 36/47: batch_recall = 58.42, batch_ndcg = 45.10 
2025-02-12 22:15:25.233257: Steps 37/47: batch_recall = 60.45, batch_ndcg = 45.99 
2025-02-12 22:15:26.196380: Steps 38/47: batch_recall = 63.90, batch_ndcg = 45.48 
2025-02-12 22:15:27.147550: Steps 39/47: batch_recall = 59.82, batch_ndcg = 41.64 
2025-02-12 22:15:28.093640: Steps 40/47: batch_recall = 56.73, batch_ndcg = 42.91 
2025-02-12 22:15:29.040443: Steps 41/47: batch_recall = 65.86, batch_ndcg = 50.52 
2025-02-12 22:15:29.990699: Steps 42/47: batch_recall = 66.18, batch_ndcg = 48.53 
2025-02-12 22:15:30.948366: Steps 43/47: batch_recall = 67.36, batch_ndcg = 46.93 
2025-02-12 22:15:31.904078: Steps 44/47: batch_recall = 68.20, batch_ndcg = 46.65 
2025-02-12 22:15:32.841447: Steps 45/47: batch_recall = 75.55, batch_ndcg = 54.82 
2025-02-12 22:15:32.945469: Steps 46/47: batch_recall = 2.25, batch_ndcg = 1.60 
2025-02-12 22:15:32.945553: Epoch 0/1000, Test: Recall = 0.0867, NDCG = 0.0684  

2025-02-12 22:15:34.144773: Training Step 0/59: batchLoss = 3.5403, diffLoss = 17.3599, kgLoss = 0.0854
2025-02-12 22:15:35.067488: Training Step 1/59: batchLoss = 4.6246, diffLoss = 22.6964, kgLoss = 0.1067
2025-02-12 22:15:35.994184: Training Step 2/59: batchLoss = 4.0094, diffLoss = 19.6442, kgLoss = 0.1008
2025-02-12 22:15:36.918651: Training Step 3/59: batchLoss = 3.9779, diffLoss = 19.5223, kgLoss = 0.0918
2025-02-12 22:15:37.836504: Training Step 4/59: batchLoss = 3.8046, diffLoss = 18.6431, kgLoss = 0.0950
2025-02-12 22:15:38.760386: Training Step 5/59: batchLoss = 3.5903, diffLoss = 17.5930, kgLoss = 0.0897
2025-02-12 22:15:39.676298: Training Step 6/59: batchLoss = 4.0080, diffLoss = 19.6452, kgLoss = 0.0987
2025-02-12 22:15:40.590074: Training Step 7/59: batchLoss = 3.5816, diffLoss = 17.5439, kgLoss = 0.0910
2025-02-12 22:15:41.510622: Training Step 8/59: batchLoss = 3.7328, diffLoss = 18.3019, kgLoss = 0.0905
2025-02-12 22:15:42.430254: Training Step 9/59: batchLoss = 4.2897, diffLoss = 21.0355, kgLoss = 0.1032
2025-02-12 22:15:43.341505: Training Step 10/59: batchLoss = 3.9540, diffLoss = 19.3833, kgLoss = 0.0967
2025-02-12 22:15:44.257027: Training Step 11/59: batchLoss = 3.7566, diffLoss = 18.4214, kgLoss = 0.0904
2025-02-12 22:15:45.176894: Training Step 12/59: batchLoss = 3.4410, diffLoss = 16.8363, kgLoss = 0.0921
2025-02-12 22:15:46.090045: Training Step 13/59: batchLoss = 2.8635, diffLoss = 14.0300, kgLoss = 0.0719
2025-02-12 22:15:47.005025: Training Step 14/59: batchLoss = 3.4445, diffLoss = 16.8708, kgLoss = 0.0879
2025-02-12 22:15:47.918193: Training Step 15/59: batchLoss = 3.5170, diffLoss = 17.2451, kgLoss = 0.0849
2025-02-12 22:15:48.844567: Training Step 16/59: batchLoss = 3.4030, diffLoss = 16.6821, kgLoss = 0.0832
2025-02-12 22:15:49.768481: Training Step 17/59: batchLoss = 3.6849, diffLoss = 18.0507, kgLoss = 0.0935
2025-02-12 22:15:50.690680: Training Step 18/59: batchLoss = 3.7506, diffLoss = 18.3894, kgLoss = 0.0909
2025-02-12 22:15:51.620824: Training Step 19/59: batchLoss = 3.2647, diffLoss = 15.9976, kgLoss = 0.0815
2025-02-12 22:15:52.547906: Training Step 20/59: batchLoss = 3.3827, diffLoss = 16.5960, kgLoss = 0.0794
2025-02-12 22:15:53.474233: Training Step 21/59: batchLoss = 4.4118, diffLoss = 21.6566, kgLoss = 0.1006
2025-02-12 22:15:54.398890: Training Step 22/59: batchLoss = 3.8267, diffLoss = 18.7424, kgLoss = 0.0978
2025-02-12 22:15:55.336553: Training Step 23/59: batchLoss = 3.6914, diffLoss = 18.0924, kgLoss = 0.0911
2025-02-12 22:15:56.277027: Training Step 24/59: batchLoss = 3.7067, diffLoss = 18.1633, kgLoss = 0.0926
2025-02-12 22:15:57.198040: Training Step 25/59: batchLoss = 3.5920, diffLoss = 17.6240, kgLoss = 0.0840
2025-02-12 22:15:58.121602: Training Step 26/59: batchLoss = 3.4871, diffLoss = 17.0594, kgLoss = 0.0940
2025-02-12 22:15:59.048557: Training Step 27/59: batchLoss = 3.7037, diffLoss = 18.1260, kgLoss = 0.0981
2025-02-12 22:15:59.975876: Training Step 28/59: batchLoss = 3.4854, diffLoss = 17.0792, kgLoss = 0.0869
2025-02-12 22:16:00.896010: Training Step 29/59: batchLoss = 3.9379, diffLoss = 19.2674, kgLoss = 0.1055
2025-02-12 22:16:01.816289: Training Step 30/59: batchLoss = 3.9303, diffLoss = 19.2647, kgLoss = 0.0967
2025-02-12 22:16:02.741747: Training Step 31/59: batchLoss = 3.7228, diffLoss = 18.2437, kgLoss = 0.0925
2025-02-12 22:16:03.657427: Training Step 32/59: batchLoss = 3.7714, diffLoss = 18.4625, kgLoss = 0.0986
2025-02-12 22:16:04.575700: Training Step 33/59: batchLoss = 3.3503, diffLoss = 16.3970, kgLoss = 0.0887
2025-02-12 22:16:05.491081: Training Step 34/59: batchLoss = 3.6070, diffLoss = 17.6706, kgLoss = 0.0912
2025-02-12 22:16:06.407121: Training Step 35/59: batchLoss = 3.7800, diffLoss = 18.4957, kgLoss = 0.1010
2025-02-12 22:16:07.323814: Training Step 36/59: batchLoss = 3.7307, diffLoss = 18.2832, kgLoss = 0.0926
2025-02-12 22:16:08.243375: Training Step 37/59: batchLoss = 3.3903, diffLoss = 16.6038, kgLoss = 0.0869
2025-02-12 22:16:09.163372: Training Step 38/59: batchLoss = 4.2448, diffLoss = 20.8255, kgLoss = 0.0997
2025-02-12 22:16:10.086816: Training Step 39/59: batchLoss = 4.1255, diffLoss = 20.2360, kgLoss = 0.0978
2025-02-12 22:16:11.012881: Training Step 40/59: batchLoss = 3.7246, diffLoss = 18.2363, kgLoss = 0.0967
2025-02-12 22:16:11.934878: Training Step 41/59: batchLoss = 4.3244, diffLoss = 21.2022, kgLoss = 0.1049
2025-02-12 22:16:12.866905: Training Step 42/59: batchLoss = 3.4450, diffLoss = 16.8611, kgLoss = 0.0910
2025-02-12 22:16:13.796120: Training Step 43/59: batchLoss = 3.4262, diffLoss = 16.7964, kgLoss = 0.0837
2025-02-12 22:16:14.721663: Training Step 44/59: batchLoss = 3.3362, diffLoss = 16.3385, kgLoss = 0.0856
2025-02-12 22:16:15.647963: Training Step 45/59: batchLoss = 3.8195, diffLoss = 18.7039, kgLoss = 0.0984
2025-02-12 22:16:16.577240: Training Step 46/59: batchLoss = 3.5020, diffLoss = 17.1639, kgLoss = 0.0866
2025-02-12 22:16:17.505846: Training Step 47/59: batchLoss = 3.5772, diffLoss = 17.5147, kgLoss = 0.0928
2025-02-12 22:16:18.436235: Training Step 48/59: batchLoss = 4.3651, diffLoss = 21.3902, kgLoss = 0.1088
2025-02-12 22:16:19.363376: Training Step 49/59: batchLoss = 3.7261, diffLoss = 18.2700, kgLoss = 0.0902
2025-02-12 22:16:20.279812: Training Step 50/59: batchLoss = 3.9011, diffLoss = 19.1041, kgLoss = 0.1003
2025-02-12 22:16:21.190858: Training Step 51/59: batchLoss = 3.8517, diffLoss = 18.8551, kgLoss = 0.1009
2025-02-12 22:16:22.107172: Training Step 52/59: batchLoss = 3.5527, diffLoss = 17.3546, kgLoss = 0.1022
2025-02-12 22:16:23.023091: Training Step 53/59: batchLoss = 3.3934, diffLoss = 16.6266, kgLoss = 0.0851
2025-02-12 22:16:23.938393: Training Step 54/59: batchLoss = 3.7389, diffLoss = 18.3203, kgLoss = 0.0935
2025-02-12 22:16:24.855737: Training Step 55/59: batchLoss = 3.6149, diffLoss = 17.7186, kgLoss = 0.0889
2025-02-12 22:16:25.773782: Training Step 56/59: batchLoss = 3.2844, diffLoss = 16.0994, kgLoss = 0.0807
2025-02-12 22:16:26.611292: Training Step 57/59: batchLoss = 3.2715, diffLoss = 16.0075, kgLoss = 0.0875
2025-02-12 22:16:27.467095: Training Step 58/59: batchLoss = 3.6714, diffLoss = 17.9873, kgLoss = 0.0924
2025-02-12 22:16:27.567033: 
2025-02-12 22:16:27.567395: Epoch 1/1000, Train: epLoss = 0.5466, epDfLoss = 2.6783, epKgLoss = 0.0137  
2025-02-12 22:16:29.042566: Steps 0/47: batch_recall = 22.40, batch_ndcg = 24.06 
2025-02-12 22:16:30.340433: Steps 1/47: batch_recall = 29.82, batch_ndcg = 27.65 
2025-02-12 22:16:31.604692: Steps 2/47: batch_recall = 27.43, batch_ndcg = 27.51 
2025-02-12 22:16:32.864816: Steps 3/47: batch_recall = 32.43, batch_ndcg = 28.41 
2025-02-12 22:16:34.058737: Steps 4/47: batch_recall = 29.17, batch_ndcg = 27.99 
2025-02-12 22:16:35.269753: Steps 5/47: batch_recall = 24.41, batch_ndcg = 23.99 
2025-02-12 22:16:36.465094: Steps 6/47: batch_recall = 28.40, batch_ndcg = 26.79 
2025-02-12 22:16:37.639823: Steps 7/47: batch_recall = 30.54, batch_ndcg = 28.29 
2025-02-12 22:16:38.810770: Steps 8/47: batch_recall = 34.86, batch_ndcg = 31.34 
2025-02-12 22:16:39.948005: Steps 9/47: batch_recall = 30.67, batch_ndcg = 29.78 
2025-02-12 22:16:41.105875: Steps 10/47: batch_recall = 32.54, batch_ndcg = 29.51 
2025-02-12 22:16:42.237492: Steps 11/47: batch_recall = 36.12, batch_ndcg = 29.80 
2025-02-12 22:16:43.383254: Steps 12/47: batch_recall = 37.65, batch_ndcg = 30.44 
2025-02-12 22:16:44.507647: Steps 13/47: batch_recall = 37.46, batch_ndcg = 32.14 
2025-02-12 22:16:45.596649: Steps 14/47: batch_recall = 31.57, batch_ndcg = 26.90 
2025-02-12 22:16:46.682853: Steps 15/47: batch_recall = 44.62, batch_ndcg = 34.83 
2025-02-12 22:16:47.751212: Steps 16/47: batch_recall = 37.03, batch_ndcg = 31.87 
2025-02-12 22:16:48.786756: Steps 17/47: batch_recall = 39.42, batch_ndcg = 32.44 
2025-02-12 22:16:49.852689: Steps 18/47: batch_recall = 35.44, batch_ndcg = 28.64 
2025-02-12 22:16:50.910929: Steps 19/47: batch_recall = 46.71, batch_ndcg = 36.96 
2025-02-12 22:16:51.954307: Steps 20/47: batch_recall = 50.04, batch_ndcg = 39.33 
2025-02-12 22:16:52.997629: Steps 21/47: batch_recall = 45.20, batch_ndcg = 34.24 
2025-02-12 22:16:54.045381: Steps 22/47: batch_recall = 38.99, batch_ndcg = 29.67 
2025-02-12 22:16:55.090950: Steps 23/47: batch_recall = 48.26, batch_ndcg = 36.39 
2025-02-12 22:16:56.153034: Steps 24/47: batch_recall = 47.10, batch_ndcg = 38.33 
2025-02-12 22:16:57.194378: Steps 25/47: batch_recall = 49.32, batch_ndcg = 39.15 
2025-02-12 22:16:58.206538: Steps 26/47: batch_recall = 51.55, batch_ndcg = 38.59 
2025-02-12 22:16:59.238357: Steps 27/47: batch_recall = 41.52, batch_ndcg = 31.88 
2025-02-12 22:17:00.258794: Steps 28/47: batch_recall = 44.32, batch_ndcg = 33.49 
2025-02-12 22:17:01.272404: Steps 29/47: batch_recall = 48.15, batch_ndcg = 35.45 
2025-02-12 22:17:02.273244: Steps 30/47: batch_recall = 58.68, batch_ndcg = 41.11 
2025-02-12 22:17:03.291390: Steps 31/47: batch_recall = 46.13, batch_ndcg = 35.77 
2025-02-12 22:17:04.291731: Steps 32/47: batch_recall = 56.15, batch_ndcg = 43.56 
2025-02-12 22:17:05.295310: Steps 33/47: batch_recall = 54.52, batch_ndcg = 40.57 
2025-02-12 22:17:06.290676: Steps 34/47: batch_recall = 46.14, batch_ndcg = 35.90 
2025-02-12 22:17:07.268029: Steps 35/47: batch_recall = 56.94, batch_ndcg = 38.89 
2025-02-12 22:17:08.244627: Steps 36/47: batch_recall = 61.01, batch_ndcg = 47.11 
2025-02-12 22:17:09.210185: Steps 37/47: batch_recall = 63.53, batch_ndcg = 47.05 
2025-02-12 22:17:10.181684: Steps 38/47: batch_recall = 66.84, batch_ndcg = 49.22 
2025-02-12 22:17:11.153772: Steps 39/47: batch_recall = 64.72, batch_ndcg = 45.90 
2025-02-12 22:17:12.119400: Steps 40/47: batch_recall = 55.38, batch_ndcg = 42.31 
2025-02-12 22:17:13.080154: Steps 41/47: batch_recall = 70.26, batch_ndcg = 49.85 
2025-02-12 22:17:14.037066: Steps 42/47: batch_recall = 60.80, batch_ndcg = 44.64 
2025-02-12 22:17:14.996941: Steps 43/47: batch_recall = 67.88, batch_ndcg = 47.56 
2025-02-12 22:17:15.972003: Steps 44/47: batch_recall = 65.71, batch_ndcg = 45.65 
2025-02-12 22:17:16.912451: Steps 45/47: batch_recall = 76.63, batch_ndcg = 54.26 
2025-02-12 22:17:17.015172: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 22:17:17.015487: Epoch 1/1000, Test: Recall = 0.0894, NDCG = 0.0703  

2025-02-12 22:17:18.203220: Training Step 0/59: batchLoss = 3.2109, diffLoss = 15.7267, kgLoss = 0.0819
2025-02-12 22:17:19.131296: Training Step 1/59: batchLoss = 3.4046, diffLoss = 16.6624, kgLoss = 0.0901
2025-02-12 22:17:20.055276: Training Step 2/59: batchLoss = 3.6395, diffLoss = 17.8265, kgLoss = 0.0928
2025-02-12 22:17:20.980249: Training Step 3/59: batchLoss = 3.9058, diffLoss = 19.1248, kgLoss = 0.1010
2025-02-12 22:17:21.904804: Training Step 4/59: batchLoss = 3.6349, diffLoss = 17.8045, kgLoss = 0.0925
2025-02-12 22:17:22.827475: Training Step 5/59: batchLoss = 4.3002, diffLoss = 21.0722, kgLoss = 0.1072
2025-02-12 22:17:23.759917: Training Step 6/59: batchLoss = 3.3710, diffLoss = 16.5026, kgLoss = 0.0881
2025-02-12 22:17:24.687103: Training Step 7/59: batchLoss = 3.1190, diffLoss = 15.2643, kgLoss = 0.0827
2025-02-12 22:17:25.612788: Training Step 8/59: batchLoss = 3.2162, diffLoss = 15.7405, kgLoss = 0.0851
2025-02-12 22:17:26.531473: Training Step 9/59: batchLoss = 3.0115, diffLoss = 14.7118, kgLoss = 0.0864
2025-02-12 22:17:27.449483: Training Step 10/59: batchLoss = 3.2027, diffLoss = 15.6890, kgLoss = 0.0811
2025-02-12 22:17:28.374722: Training Step 11/59: batchLoss = 3.4641, diffLoss = 16.9346, kgLoss = 0.0964
2025-02-12 22:17:29.300067: Training Step 12/59: batchLoss = 3.1958, diffLoss = 15.6337, kgLoss = 0.0863
2025-02-12 22:17:30.223437: Training Step 13/59: batchLoss = 3.4925, diffLoss = 17.0906, kgLoss = 0.0929
2025-02-12 22:17:31.136797: Training Step 14/59: batchLoss = 3.3130, diffLoss = 16.2135, kgLoss = 0.0879
2025-02-12 22:17:32.043685: Training Step 15/59: batchLoss = 3.3090, diffLoss = 16.1884, kgLoss = 0.0892
2025-02-12 22:17:32.961491: Training Step 16/59: batchLoss = 4.1134, diffLoss = 20.1527, kgLoss = 0.1036
2025-02-12 22:17:33.879779: Training Step 17/59: batchLoss = 3.1239, diffLoss = 15.2763, kgLoss = 0.0858
2025-02-12 22:17:34.810983: Training Step 18/59: batchLoss = 3.6084, diffLoss = 17.6735, kgLoss = 0.0921
2025-02-12 22:17:35.740354: Training Step 19/59: batchLoss = 3.7681, diffLoss = 18.4433, kgLoss = 0.0993
2025-02-12 22:17:36.658929: Training Step 20/59: batchLoss = 3.0514, diffLoss = 14.9349, kgLoss = 0.0805
2025-02-12 22:17:37.582171: Training Step 21/59: batchLoss = 3.4009, diffLoss = 16.6270, kgLoss = 0.0944
2025-02-12 22:17:38.509452: Training Step 22/59: batchLoss = 3.0788, diffLoss = 15.0731, kgLoss = 0.0802
2025-02-12 22:17:39.435288: Training Step 23/59: batchLoss = 3.2414, diffLoss = 15.8716, kgLoss = 0.0839
2025-02-12 22:17:40.357955: Training Step 24/59: batchLoss = 3.2967, diffLoss = 16.1475, kgLoss = 0.0840
2025-02-12 22:17:41.293263: Training Step 25/59: batchLoss = 3.7354, diffLoss = 18.2816, kgLoss = 0.0989
2025-02-12 22:17:42.228511: Training Step 26/59: batchLoss = 3.2725, diffLoss = 15.9902, kgLoss = 0.0930
2025-02-12 22:17:43.152924: Training Step 27/59: batchLoss = 3.4163, diffLoss = 16.7076, kgLoss = 0.0935
2025-02-12 22:17:44.069674: Training Step 28/59: batchLoss = 3.6079, diffLoss = 17.6578, kgLoss = 0.0954
2025-02-12 22:17:44.991164: Training Step 29/59: batchLoss = 3.4073, diffLoss = 16.6821, kgLoss = 0.0886
2025-02-12 22:17:45.911157: Training Step 30/59: batchLoss = 3.3842, diffLoss = 16.5597, kgLoss = 0.0903
2025-02-12 22:17:46.835281: Training Step 31/59: batchLoss = 4.1192, diffLoss = 20.1695, kgLoss = 0.1066
2025-02-12 22:17:47.745094: Training Step 32/59: batchLoss = 3.5492, diffLoss = 17.3883, kgLoss = 0.0894
2025-02-12 22:17:48.655575: Training Step 33/59: batchLoss = 3.7906, diffLoss = 18.5623, kgLoss = 0.0977
2025-02-12 22:17:49.571548: Training Step 34/59: batchLoss = 4.0990, diffLoss = 20.0605, kgLoss = 0.1086
2025-02-12 22:17:50.482582: Training Step 35/59: batchLoss = 3.9351, diffLoss = 19.2804, kgLoss = 0.0988
2025-02-12 22:17:51.397216: Training Step 36/59: batchLoss = 3.2551, diffLoss = 15.9424, kgLoss = 0.0833
2025-02-12 22:17:52.307598: Training Step 37/59: batchLoss = 3.1980, diffLoss = 15.6568, kgLoss = 0.0833
2025-02-12 22:17:53.224441: Training Step 38/59: batchLoss = 3.7289, diffLoss = 18.2651, kgLoss = 0.0949
2025-02-12 22:17:54.145283: Training Step 39/59: batchLoss = 3.6509, diffLoss = 17.8440, kgLoss = 0.1027
2025-02-12 22:17:55.061009: Training Step 40/59: batchLoss = 3.4155, diffLoss = 16.7188, kgLoss = 0.0897
2025-02-12 22:17:55.987932: Training Step 41/59: batchLoss = 3.2638, diffLoss = 15.9776, kgLoss = 0.0854
2025-02-12 22:17:56.922185: Training Step 42/59: batchLoss = 3.9003, diffLoss = 19.0928, kgLoss = 0.1022
2025-02-12 22:17:57.849140: Training Step 43/59: batchLoss = 3.0269, diffLoss = 14.8135, kgLoss = 0.0803
2025-02-12 22:17:58.774711: Training Step 44/59: batchLoss = 3.2008, diffLoss = 15.6490, kgLoss = 0.0888
2025-02-12 22:17:59.699386: Training Step 45/59: batchLoss = 3.7063, diffLoss = 18.1269, kgLoss = 0.1011
2025-02-12 22:18:00.627324: Training Step 46/59: batchLoss = 3.3882, diffLoss = 16.6080, kgLoss = 0.0833
2025-02-12 22:18:01.552927: Training Step 47/59: batchLoss = 3.6351, diffLoss = 17.7978, kgLoss = 0.0944
2025-02-12 22:18:02.472436: Training Step 48/59: batchLoss = 3.8843, diffLoss = 19.0230, kgLoss = 0.0997
2025-02-12 22:18:03.390399: Training Step 49/59: batchLoss = 3.4838, diffLoss = 17.0418, kgLoss = 0.0943
2025-02-12 22:18:04.300045: Training Step 50/59: batchLoss = 3.9489, diffLoss = 19.3313, kgLoss = 0.1033
2025-02-12 22:18:05.212851: Training Step 51/59: batchLoss = 3.4949, diffLoss = 17.1128, kgLoss = 0.0905
2025-02-12 22:18:06.131838: Training Step 52/59: batchLoss = 3.8159, diffLoss = 18.6594, kgLoss = 0.1050
2025-02-12 22:18:07.047379: Training Step 53/59: batchLoss = 3.2484, diffLoss = 15.8849, kgLoss = 0.0892
2025-02-12 22:18:07.964781: Training Step 54/59: batchLoss = 3.9174, diffLoss = 19.1874, kgLoss = 0.0999
2025-02-12 22:18:08.881321: Training Step 55/59: batchLoss = 3.3364, diffLoss = 16.3435, kgLoss = 0.0847
2025-02-12 22:18:09.782768: Training Step 56/59: batchLoss = 3.8129, diffLoss = 18.6830, kgLoss = 0.0954
2025-02-12 22:18:10.618022: Training Step 57/59: batchLoss = 3.2792, diffLoss = 16.0559, kgLoss = 0.0851
2025-02-12 22:18:11.459685: Training Step 58/59: batchLoss = 3.4509, diffLoss = 16.8794, kgLoss = 0.0937
2025-02-12 22:18:11.552987: 
2025-02-12 22:18:11.553330: Epoch 2/1000, Train: epLoss = 0.5171, epDfLoss = 2.5311, epKgLoss = 0.0136  
2025-02-12 22:18:13.025012: Steps 0/47: batch_recall = 23.47, batch_ndcg = 25.63 
2025-02-12 22:18:14.317034: Steps 1/47: batch_recall = 30.75, batch_ndcg = 28.78 
2025-02-12 22:18:15.566686: Steps 2/47: batch_recall = 28.57, batch_ndcg = 29.07 
2025-02-12 22:18:16.826001: Steps 3/47: batch_recall = 34.03, batch_ndcg = 30.24 
2025-02-12 22:18:18.020572: Steps 4/47: batch_recall = 30.89, batch_ndcg = 30.19 
2025-02-12 22:18:19.225166: Steps 5/47: batch_recall = 26.17, batch_ndcg = 25.90 
2025-02-12 22:18:20.422766: Steps 6/47: batch_recall = 30.72, batch_ndcg = 29.55 
2025-02-12 22:18:21.590475: Steps 7/47: batch_recall = 32.68, batch_ndcg = 30.25 
2025-02-12 22:18:22.764600: Steps 8/47: batch_recall = 36.59, batch_ndcg = 33.25 
2025-02-12 22:18:23.898551: Steps 9/47: batch_recall = 32.61, batch_ndcg = 31.12 
2025-02-12 22:18:25.059278: Steps 10/47: batch_recall = 36.41, batch_ndcg = 32.07 
2025-02-12 22:18:26.193347: Steps 11/47: batch_recall = 37.21, batch_ndcg = 31.83 
2025-02-12 22:18:27.333332: Steps 12/47: batch_recall = 37.47, batch_ndcg = 31.91 
2025-02-12 22:18:28.461750: Steps 13/47: batch_recall = 36.27, batch_ndcg = 31.74 
2025-02-12 22:18:29.552748: Steps 14/47: batch_recall = 33.88, batch_ndcg = 28.74 
2025-02-12 22:18:30.638160: Steps 15/47: batch_recall = 45.90, batch_ndcg = 35.02 
2025-02-12 22:18:31.712160: Steps 16/47: batch_recall = 39.82, batch_ndcg = 33.40 
2025-02-12 22:18:32.750056: Steps 17/47: batch_recall = 41.22, batch_ndcg = 33.70 
2025-02-12 22:18:33.813116: Steps 18/47: batch_recall = 38.01, batch_ndcg = 30.84 
2025-02-12 22:18:34.881607: Steps 19/47: batch_recall = 51.03, batch_ndcg = 40.18 
2025-02-12 22:18:35.934406: Steps 20/47: batch_recall = 51.39, batch_ndcg = 41.21 
2025-02-12 22:18:36.995982: Steps 21/47: batch_recall = 50.37, batch_ndcg = 38.13 
2025-02-12 22:18:38.056339: Steps 22/47: batch_recall = 44.15, batch_ndcg = 34.87 
2025-02-12 22:18:39.109057: Steps 23/47: batch_recall = 48.41, batch_ndcg = 36.32 
2025-02-12 22:18:40.191859: Steps 24/47: batch_recall = 50.13, batch_ndcg = 39.14 
2025-02-12 22:18:41.227943: Steps 25/47: batch_recall = 51.13, batch_ndcg = 41.32 
2025-02-12 22:18:42.240311: Steps 26/47: batch_recall = 50.95, batch_ndcg = 39.05 
2025-02-12 22:18:43.275607: Steps 27/47: batch_recall = 43.22, batch_ndcg = 32.88 
2025-02-12 22:18:44.300855: Steps 28/47: batch_recall = 50.55, batch_ndcg = 37.80 
2025-02-12 22:18:45.311271: Steps 29/47: batch_recall = 52.39, batch_ndcg = 38.69 
2025-02-12 22:18:46.317211: Steps 30/47: batch_recall = 59.43, batch_ndcg = 44.06 
2025-02-12 22:18:47.334737: Steps 31/47: batch_recall = 50.41, batch_ndcg = 37.32 
2025-02-12 22:18:48.339559: Steps 32/47: batch_recall = 57.40, batch_ndcg = 44.59 
2025-02-12 22:18:49.350452: Steps 33/47: batch_recall = 60.09, batch_ndcg = 43.78 
2025-02-12 22:18:50.357798: Steps 34/47: batch_recall = 48.67, batch_ndcg = 37.53 
2025-02-12 22:18:51.332739: Steps 35/47: batch_recall = 61.10, batch_ndcg = 43.18 
2025-02-12 22:18:52.322623: Steps 36/47: batch_recall = 62.63, batch_ndcg = 48.09 
2025-02-12 22:18:53.293224: Steps 37/47: batch_recall = 63.54, batch_ndcg = 48.87 
2025-02-12 22:18:54.257094: Steps 38/47: batch_recall = 68.60, batch_ndcg = 51.49 
2025-02-12 22:18:55.220994: Steps 39/47: batch_recall = 69.00, batch_ndcg = 47.22 
2025-02-12 22:18:56.188926: Steps 40/47: batch_recall = 57.46, batch_ndcg = 45.52 
2025-02-12 22:18:57.156890: Steps 41/47: batch_recall = 73.75, batch_ndcg = 53.86 
2025-02-12 22:18:58.118942: Steps 42/47: batch_recall = 68.17, batch_ndcg = 48.23 
2025-02-12 22:18:59.085058: Steps 43/47: batch_recall = 72.53, batch_ndcg = 50.36 
2025-02-12 22:19:00.051822: Steps 44/47: batch_recall = 71.69, batch_ndcg = 49.48 
2025-02-12 22:19:00.988219: Steps 45/47: batch_recall = 80.20, batch_ndcg = 57.93 
2025-02-12 22:19:01.093371: Steps 46/47: batch_recall = 1.67, batch_ndcg = 1.40 
2025-02-12 22:19:01.093506: Epoch 2/1000, Test: Recall = 0.0943, NDCG = 0.0745  

2025-02-12 22:19:02.297157: Training Step 0/59: batchLoss = 3.3984, diffLoss = 16.6114, kgLoss = 0.0951
2025-02-12 22:19:03.228075: Training Step 1/59: batchLoss = 3.1763, diffLoss = 15.5249, kgLoss = 0.0892
2025-02-12 22:19:04.153296: Training Step 2/59: batchLoss = 3.1928, diffLoss = 15.5931, kgLoss = 0.0927
2025-02-12 22:19:05.083450: Training Step 3/59: batchLoss = 3.2070, diffLoss = 15.6837, kgLoss = 0.0879
2025-02-12 22:19:06.007312: Training Step 4/59: batchLoss = 3.3351, diffLoss = 16.2992, kgLoss = 0.0941
2025-02-12 22:19:06.938529: Training Step 5/59: batchLoss = 3.5397, diffLoss = 17.3110, kgLoss = 0.0969
2025-02-12 22:19:07.852798: Training Step 6/59: batchLoss = 3.3492, diffLoss = 16.3697, kgLoss = 0.0941
2025-02-12 22:19:08.774046: Training Step 7/59: batchLoss = 3.0466, diffLoss = 14.8865, kgLoss = 0.0867
2025-02-12 22:19:09.702482: Training Step 8/59: batchLoss = 3.3765, diffLoss = 16.5437, kgLoss = 0.0847
2025-02-12 22:19:10.621533: Training Step 9/59: batchLoss = 3.3256, diffLoss = 16.2680, kgLoss = 0.0900
2025-02-12 22:19:11.545392: Training Step 10/59: batchLoss = 2.9882, diffLoss = 14.5740, kgLoss = 0.0917
2025-02-12 22:19:12.470432: Training Step 11/59: batchLoss = 3.4179, diffLoss = 16.6579, kgLoss = 0.1079
2025-02-12 22:19:13.396137: Training Step 12/59: batchLoss = 3.3251, diffLoss = 16.2464, kgLoss = 0.0948
2025-02-12 22:19:14.316807: Training Step 13/59: batchLoss = 3.5048, diffLoss = 17.1521, kgLoss = 0.0930
2025-02-12 22:19:15.239556: Training Step 14/59: batchLoss = 3.3179, diffLoss = 16.2401, kgLoss = 0.0874
2025-02-12 22:19:16.161904: Training Step 15/59: batchLoss = 3.6565, diffLoss = 17.8707, kgLoss = 0.1030
2025-02-12 22:19:17.091090: Training Step 16/59: batchLoss = 3.4614, diffLoss = 16.9262, kgLoss = 0.0952
2025-02-12 22:19:18.025769: Training Step 17/59: batchLoss = 3.2152, diffLoss = 15.7154, kgLoss = 0.0901
2025-02-12 22:19:18.945528: Training Step 18/59: batchLoss = 2.9292, diffLoss = 14.3224, kgLoss = 0.0809
2025-02-12 22:19:19.864495: Training Step 19/59: batchLoss = 3.4257, diffLoss = 16.7573, kgLoss = 0.0928
2025-02-12 22:19:20.783703: Training Step 20/59: batchLoss = 2.8761, diffLoss = 14.0629, kgLoss = 0.0794
2025-02-12 22:19:21.709474: Training Step 21/59: batchLoss = 3.4412, diffLoss = 16.8215, kgLoss = 0.0962
2025-02-12 22:19:22.636150: Training Step 22/59: batchLoss = 3.1912, diffLoss = 15.5672, kgLoss = 0.0971
2025-02-12 22:19:23.561958: Training Step 23/59: batchLoss = 3.3155, diffLoss = 16.2071, kgLoss = 0.0926
2025-02-12 22:19:24.490207: Training Step 24/59: batchLoss = 3.2898, diffLoss = 16.0764, kgLoss = 0.0932
2025-02-12 22:19:25.412982: Training Step 25/59: batchLoss = 3.1385, diffLoss = 15.3482, kgLoss = 0.0861
2025-02-12 22:19:26.344370: Training Step 26/59: batchLoss = 3.1006, diffLoss = 15.1554, kgLoss = 0.0869
2025-02-12 22:19:27.268240: Training Step 27/59: batchLoss = 3.3975, diffLoss = 16.6142, kgLoss = 0.0933
2025-02-12 22:19:28.197344: Training Step 28/59: batchLoss = 3.1066, diffLoss = 15.2055, kgLoss = 0.0819
2025-02-12 22:19:29.119239: Training Step 29/59: batchLoss = 3.8092, diffLoss = 18.6249, kgLoss = 0.1053
2025-02-12 22:19:30.046381: Training Step 30/59: batchLoss = 3.8390, diffLoss = 18.8002, kgLoss = 0.0987
2025-02-12 22:19:30.967235: Training Step 31/59: batchLoss = 3.1669, diffLoss = 15.4700, kgLoss = 0.0911
2025-02-12 22:19:31.893293: Training Step 32/59: batchLoss = 3.6419, diffLoss = 17.7855, kgLoss = 0.1059
2025-02-12 22:19:32.816761: Training Step 33/59: batchLoss = 3.5439, diffLoss = 17.3519, kgLoss = 0.0919
2025-02-12 22:19:33.736416: Training Step 34/59: batchLoss = 3.4788, diffLoss = 17.0225, kgLoss = 0.0928
2025-02-12 22:19:34.654120: Training Step 35/59: batchLoss = 3.5411, diffLoss = 17.3208, kgLoss = 0.0962
2025-02-12 22:19:35.569262: Training Step 36/59: batchLoss = 2.9042, diffLoss = 14.2021, kgLoss = 0.0797
2025-02-12 22:19:36.490211: Training Step 37/59: batchLoss = 3.7515, diffLoss = 18.3491, kgLoss = 0.1020
2025-02-12 22:19:37.413138: Training Step 38/59: batchLoss = 3.5230, diffLoss = 17.2461, kgLoss = 0.0923
2025-02-12 22:19:38.327785: Training Step 39/59: batchLoss = 3.5783, diffLoss = 17.5325, kgLoss = 0.0897
2025-02-12 22:19:39.253342: Training Step 40/59: batchLoss = 3.1148, diffLoss = 15.2267, kgLoss = 0.0869
2025-02-12 22:19:40.181941: Training Step 41/59: batchLoss = 3.1535, diffLoss = 15.4346, kgLoss = 0.0833
2025-02-12 22:19:41.112132: Training Step 42/59: batchLoss = 3.5338, diffLoss = 17.3078, kgLoss = 0.0902
2025-02-12 22:19:42.038432: Training Step 43/59: batchLoss = 3.4466, diffLoss = 16.8893, kgLoss = 0.0859
2025-02-12 22:19:42.964249: Training Step 44/59: batchLoss = 2.9182, diffLoss = 14.2525, kgLoss = 0.0846
2025-02-12 22:19:43.889759: Training Step 45/59: batchLoss = 3.2090, diffLoss = 15.6831, kgLoss = 0.0904
2025-02-12 22:19:44.817798: Training Step 46/59: batchLoss = 3.4642, diffLoss = 16.9611, kgLoss = 0.0899
2025-02-12 22:19:45.758267: Training Step 47/59: batchLoss = 3.2493, diffLoss = 15.8903, kgLoss = 0.0890
2025-02-12 22:19:46.682314: Training Step 48/59: batchLoss = 3.1457, diffLoss = 15.3847, kgLoss = 0.0859
2025-02-12 22:19:47.612978: Training Step 49/59: batchLoss = 3.5156, diffLoss = 17.1751, kgLoss = 0.1007
2025-02-12 22:19:48.532556: Training Step 50/59: batchLoss = 3.8135, diffLoss = 18.6610, kgLoss = 0.1016
2025-02-12 22:19:49.466130: Training Step 51/59: batchLoss = 3.1361, diffLoss = 15.3321, kgLoss = 0.0871
2025-02-12 22:19:50.386883: Training Step 52/59: batchLoss = 3.2551, diffLoss = 15.9148, kgLoss = 0.0902
2025-02-12 22:19:51.313681: Training Step 53/59: batchLoss = 3.1594, diffLoss = 15.4683, kgLoss = 0.0822
2025-02-12 22:19:52.234626: Training Step 54/59: batchLoss = 3.1984, diffLoss = 15.6224, kgLoss = 0.0924
2025-02-12 22:19:53.158796: Training Step 55/59: batchLoss = 4.0272, diffLoss = 19.7101, kgLoss = 0.1065
2025-02-12 22:19:54.081414: Training Step 56/59: batchLoss = 2.8107, diffLoss = 13.7340, kgLoss = 0.0798
2025-02-12 22:19:54.918200: Training Step 57/59: batchLoss = 3.2521, diffLoss = 15.8697, kgLoss = 0.0976
2025-02-12 22:19:55.771368: Training Step 58/59: batchLoss = 2.8783, diffLoss = 14.0657, kgLoss = 0.0814
2025-02-12 22:19:55.864823: 
2025-02-12 22:19:55.865350: Epoch 3/1000, Train: epLoss = 0.4903, epDfLoss = 2.3973, epKgLoss = 0.0135  
2025-02-12 22:19:57.330345: Steps 0/47: batch_recall = 26.30, batch_ndcg = 27.85 
2025-02-12 22:19:58.622765: Steps 1/47: batch_recall = 29.84, batch_ndcg = 27.71 
2025-02-12 22:19:59.891464: Steps 2/47: batch_recall = 30.53, batch_ndcg = 31.63 
2025-02-12 22:20:01.150593: Steps 3/47: batch_recall = 35.17, batch_ndcg = 31.56 
2025-02-12 22:20:02.349913: Steps 4/47: batch_recall = 31.20, batch_ndcg = 30.87 
2025-02-12 22:20:03.558412: Steps 5/47: batch_recall = 26.81, batch_ndcg = 26.06 
2025-02-12 22:20:04.761112: Steps 6/47: batch_recall = 32.41, batch_ndcg = 31.03 
2025-02-12 22:20:05.937075: Steps 7/47: batch_recall = 34.19, batch_ndcg = 31.10 
2025-02-12 22:20:07.121334: Steps 8/47: batch_recall = 36.68, batch_ndcg = 33.46 
2025-02-12 22:20:08.256153: Steps 9/47: batch_recall = 35.32, batch_ndcg = 33.16 
2025-02-12 22:20:09.413566: Steps 10/47: batch_recall = 37.15, batch_ndcg = 33.78 
2025-02-12 22:20:10.549485: Steps 11/47: batch_recall = 39.88, batch_ndcg = 33.67 
2025-02-12 22:20:11.690582: Steps 12/47: batch_recall = 39.61, batch_ndcg = 33.51 
2025-02-12 22:20:12.811091: Steps 13/47: batch_recall = 36.37, batch_ndcg = 32.06 
2025-02-12 22:20:13.883681: Steps 14/47: batch_recall = 34.29, batch_ndcg = 29.15 
2025-02-12 22:20:14.964441: Steps 15/47: batch_recall = 43.47, batch_ndcg = 35.00 
2025-02-12 22:20:16.030173: Steps 16/47: batch_recall = 42.86, batch_ndcg = 35.05 
2025-02-12 22:20:17.069369: Steps 17/47: batch_recall = 45.90, batch_ndcg = 36.99 
2025-02-12 22:20:18.139095: Steps 18/47: batch_recall = 41.40, batch_ndcg = 33.21 
2025-02-12 22:20:19.195038: Steps 19/47: batch_recall = 53.71, batch_ndcg = 42.04 
2025-02-12 22:20:20.231103: Steps 20/47: batch_recall = 53.22, batch_ndcg = 42.23 
2025-02-12 22:20:21.268852: Steps 21/47: batch_recall = 54.33, batch_ndcg = 41.55 
2025-02-12 22:20:22.317415: Steps 22/47: batch_recall = 47.84, batch_ndcg = 37.94 
2025-02-12 22:20:23.372805: Steps 23/47: batch_recall = 50.01, batch_ndcg = 37.55 
2025-02-12 22:20:24.433171: Steps 24/47: batch_recall = 52.31, batch_ndcg = 40.38 
2025-02-12 22:20:25.479882: Steps 25/47: batch_recall = 53.48, batch_ndcg = 42.95 
2025-02-12 22:20:26.497562: Steps 26/47: batch_recall = 52.70, batch_ndcg = 40.10 
2025-02-12 22:20:27.536000: Steps 27/47: batch_recall = 46.72, batch_ndcg = 36.08 
2025-02-12 22:20:28.559195: Steps 28/47: batch_recall = 51.80, batch_ndcg = 38.93 
2025-02-12 22:20:29.578491: Steps 29/47: batch_recall = 56.01, batch_ndcg = 41.43 
2025-02-12 22:20:30.579164: Steps 30/47: batch_recall = 62.29, batch_ndcg = 46.97 
2025-02-12 22:20:31.593118: Steps 31/47: batch_recall = 52.38, batch_ndcg = 39.10 
2025-02-12 22:20:32.603270: Steps 32/47: batch_recall = 59.15, batch_ndcg = 48.17 
2025-02-12 22:20:33.609007: Steps 33/47: batch_recall = 64.85, batch_ndcg = 46.52 
2025-02-12 22:20:34.615095: Steps 34/47: batch_recall = 49.18, batch_ndcg = 37.19 
2025-02-12 22:20:35.592670: Steps 35/47: batch_recall = 62.32, batch_ndcg = 45.99 
2025-02-12 22:20:36.569979: Steps 36/47: batch_recall = 64.89, batch_ndcg = 49.52 
2025-02-12 22:20:37.538542: Steps 37/47: batch_recall = 66.30, batch_ndcg = 52.61 
2025-02-12 22:20:38.515290: Steps 38/47: batch_recall = 70.36, batch_ndcg = 53.51 
2025-02-12 22:20:39.468602: Steps 39/47: batch_recall = 69.12, batch_ndcg = 48.50 
2025-02-12 22:20:40.423692: Steps 40/47: batch_recall = 59.02, batch_ndcg = 46.65 
2025-02-12 22:20:41.374572: Steps 41/47: batch_recall = 74.78, batch_ndcg = 55.26 
2025-02-12 22:20:42.334741: Steps 42/47: batch_recall = 73.65, batch_ndcg = 50.42 
2025-02-12 22:20:43.297610: Steps 43/47: batch_recall = 73.41, batch_ndcg = 52.25 
2025-02-12 22:20:44.271446: Steps 44/47: batch_recall = 73.10, batch_ndcg = 53.17 
2025-02-12 22:20:45.206486: Steps 45/47: batch_recall = 86.52, batch_ndcg = 61.50 
2025-02-12 22:20:45.309364: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.42 
2025-02-12 22:20:45.309507: Epoch 3/1000, Test: Recall = 0.0982, NDCG = 0.0779  

2025-02-12 22:20:46.495264: Training Step 0/59: batchLoss = 2.8046, diffLoss = 13.6782, kgLoss = 0.0862
2025-02-12 22:20:47.421368: Training Step 1/59: batchLoss = 3.0279, diffLoss = 14.7757, kgLoss = 0.0909
2025-02-12 22:20:48.353689: Training Step 2/59: batchLoss = 2.8974, diffLoss = 14.1488, kgLoss = 0.0845
2025-02-12 22:20:49.280701: Training Step 3/59: batchLoss = 3.4095, diffLoss = 16.6585, kgLoss = 0.0972
2025-02-12 22:20:50.208263: Training Step 4/59: batchLoss = 3.4385, diffLoss = 16.8261, kgLoss = 0.0915
2025-02-12 22:20:51.124341: Training Step 5/59: batchLoss = 3.8021, diffLoss = 18.5793, kgLoss = 0.1078
2025-02-12 22:20:52.044530: Training Step 6/59: batchLoss = 3.6729, diffLoss = 17.9629, kgLoss = 0.1004
2025-02-12 22:20:52.961504: Training Step 7/59: batchLoss = 3.2514, diffLoss = 15.8769, kgLoss = 0.0950
2025-02-12 22:20:53.874529: Training Step 8/59: batchLoss = 2.4750, diffLoss = 12.0875, kgLoss = 0.0719
2025-02-12 22:20:54.799103: Training Step 9/59: batchLoss = 2.9266, diffLoss = 14.3096, kgLoss = 0.0808
2025-02-12 22:20:55.715426: Training Step 10/59: batchLoss = 2.8894, diffLoss = 14.0971, kgLoss = 0.0875
2025-02-12 22:20:56.634357: Training Step 11/59: batchLoss = 2.9627, diffLoss = 14.4810, kgLoss = 0.0831
2025-02-12 22:20:57.558127: Training Step 12/59: batchLoss = 2.9047, diffLoss = 14.1744, kgLoss = 0.0873
2025-02-12 22:20:58.481578: Training Step 13/59: batchLoss = 3.6363, diffLoss = 17.8040, kgLoss = 0.0943
2025-02-12 22:20:59.402060: Training Step 14/59: batchLoss = 3.3556, diffLoss = 16.3998, kgLoss = 0.0946
2025-02-12 22:21:00.321272: Training Step 15/59: batchLoss = 3.2184, diffLoss = 15.7130, kgLoss = 0.0947
2025-02-12 22:21:01.241977: Training Step 16/59: batchLoss = 3.3629, diffLoss = 16.4382, kgLoss = 0.0941
2025-02-12 22:21:02.163328: Training Step 17/59: batchLoss = 3.1595, diffLoss = 15.4335, kgLoss = 0.0910
2025-02-12 22:21:03.094156: Training Step 18/59: batchLoss = 3.3070, diffLoss = 16.1448, kgLoss = 0.0975
2025-02-12 22:21:04.020155: Training Step 19/59: batchLoss = 3.7155, diffLoss = 18.1608, kgLoss = 0.1042
2025-02-12 22:21:04.950855: Training Step 20/59: batchLoss = 3.5128, diffLoss = 17.1735, kgLoss = 0.0976
2025-02-12 22:21:05.878892: Training Step 21/59: batchLoss = 2.8123, diffLoss = 13.7266, kgLoss = 0.0838
2025-02-12 22:21:06.811242: Training Step 22/59: batchLoss = 3.0830, diffLoss = 15.0486, kgLoss = 0.0916
2025-02-12 22:21:07.751178: Training Step 23/59: batchLoss = 2.9837, diffLoss = 14.5989, kgLoss = 0.0799
2025-02-12 22:21:08.687322: Training Step 24/59: batchLoss = 3.1223, diffLoss = 15.2429, kgLoss = 0.0922
2025-02-12 22:21:09.623044: Training Step 25/59: batchLoss = 3.4854, diffLoss = 17.0398, kgLoss = 0.0968
2025-02-12 22:21:10.555960: Training Step 26/59: batchLoss = 3.0129, diffLoss = 14.7276, kgLoss = 0.0842
2025-02-12 22:21:11.484603: Training Step 27/59: batchLoss = 3.2969, diffLoss = 16.1299, kgLoss = 0.0886
2025-02-12 22:21:12.411788: Training Step 28/59: batchLoss = 3.0044, diffLoss = 14.6892, kgLoss = 0.0831
2025-02-12 22:21:13.339959: Training Step 29/59: batchLoss = 3.1169, diffLoss = 15.2485, kgLoss = 0.0841
2025-02-12 22:21:14.267441: Training Step 30/59: batchLoss = 3.1386, diffLoss = 15.3294, kgLoss = 0.0909
2025-02-12 22:21:15.185963: Training Step 31/59: batchLoss = 3.6442, diffLoss = 17.8271, kgLoss = 0.0984
2025-02-12 22:21:16.107207: Training Step 32/59: batchLoss = 3.2686, diffLoss = 15.9676, kgLoss = 0.0939
2025-02-12 22:21:17.013616: Training Step 33/59: batchLoss = 3.2811, diffLoss = 16.0177, kgLoss = 0.0969
2025-02-12 22:21:17.929277: Training Step 34/59: batchLoss = 3.4068, diffLoss = 16.6691, kgLoss = 0.0913
2025-02-12 22:21:18.847350: Training Step 35/59: batchLoss = 3.3651, diffLoss = 16.4586, kgLoss = 0.0918
2025-02-12 22:21:19.762617: Training Step 36/59: batchLoss = 2.7929, diffLoss = 13.6525, kgLoss = 0.0780
2025-02-12 22:21:20.687669: Training Step 37/59: batchLoss = 2.9697, diffLoss = 14.5010, kgLoss = 0.0869
2025-02-12 22:21:21.605304: Training Step 38/59: batchLoss = 3.1701, diffLoss = 15.4730, kgLoss = 0.0944
2025-02-12 22:21:22.530130: Training Step 39/59: batchLoss = 3.0889, diffLoss = 15.1052, kgLoss = 0.0848
2025-02-12 22:21:23.455613: Training Step 40/59: batchLoss = 3.7883, diffLoss = 18.5248, kgLoss = 0.1042
2025-02-12 22:21:24.378778: Training Step 41/59: batchLoss = 3.4146, diffLoss = 16.6852, kgLoss = 0.0969
2025-02-12 22:21:25.296340: Training Step 42/59: batchLoss = 3.1627, diffLoss = 15.4442, kgLoss = 0.0923
2025-02-12 22:21:26.216362: Training Step 43/59: batchLoss = 3.3409, diffLoss = 16.3295, kgLoss = 0.0938
2025-02-12 22:21:27.140272: Training Step 44/59: batchLoss = 3.2869, diffLoss = 16.0677, kgLoss = 0.0917
2025-02-12 22:21:28.064218: Training Step 45/59: batchLoss = 3.1824, diffLoss = 15.5410, kgLoss = 0.0928
2025-02-12 22:21:28.992427: Training Step 46/59: batchLoss = 3.0619, diffLoss = 14.9911, kgLoss = 0.0796
2025-02-12 22:21:29.923753: Training Step 47/59: batchLoss = 3.4669, diffLoss = 16.9514, kgLoss = 0.0957
2025-02-12 22:21:30.852250: Training Step 48/59: batchLoss = 2.8651, diffLoss = 13.9695, kgLoss = 0.0890
2025-02-12 22:21:31.781234: Training Step 49/59: batchLoss = 2.7028, diffLoss = 13.1673, kgLoss = 0.0867
2025-02-12 22:21:32.706967: Training Step 50/59: batchLoss = 3.2676, diffLoss = 15.9744, kgLoss = 0.0909
2025-02-12 22:21:33.629398: Training Step 51/59: batchLoss = 3.3139, diffLoss = 16.2146, kgLoss = 0.0887
2025-02-12 22:21:34.546969: Training Step 52/59: batchLoss = 3.1382, diffLoss = 15.3182, kgLoss = 0.0932
2025-02-12 22:21:35.471950: Training Step 53/59: batchLoss = 3.2159, diffLoss = 15.7198, kgLoss = 0.0900
2025-02-12 22:21:36.393365: Training Step 54/59: batchLoss = 3.0458, diffLoss = 14.8829, kgLoss = 0.0865
2025-02-12 22:21:37.319232: Training Step 55/59: batchLoss = 3.1872, diffLoss = 15.5703, kgLoss = 0.0914
2025-02-12 22:21:38.237597: Training Step 56/59: batchLoss = 3.0837, diffLoss = 15.0654, kgLoss = 0.0882
2025-02-12 22:21:39.076894: Training Step 57/59: batchLoss = 3.6062, diffLoss = 17.5695, kgLoss = 0.1154
2025-02-12 22:21:39.925538: Training Step 58/59: batchLoss = 3.2428, diffLoss = 15.8327, kgLoss = 0.0953
2025-02-12 22:21:40.016250: 
2025-02-12 22:21:40.016579: Epoch 4/1000, Train: epLoss = 0.4729, epDfLoss = 2.3105, epKgLoss = 0.0135  
2025-02-12 22:21:41.473874: Steps 0/47: batch_recall = 25.74, batch_ndcg = 29.08 
2025-02-12 22:21:42.757455: Steps 1/47: batch_recall = 29.73, batch_ndcg = 28.50 
2025-02-12 22:21:44.006550: Steps 2/47: batch_recall = 32.54, batch_ndcg = 32.83 
2025-02-12 22:21:45.271503: Steps 3/47: batch_recall = 36.72, batch_ndcg = 33.13 
2025-02-12 22:21:46.465823: Steps 4/47: batch_recall = 32.61, batch_ndcg = 32.42 
2025-02-12 22:21:47.680652: Steps 5/47: batch_recall = 27.40, batch_ndcg = 26.80 
2025-02-12 22:21:48.875192: Steps 6/47: batch_recall = 33.28, batch_ndcg = 31.35 
2025-02-12 22:21:50.043587: Steps 7/47: batch_recall = 35.38, batch_ndcg = 32.15 
2025-02-12 22:21:51.201183: Steps 8/47: batch_recall = 37.73, batch_ndcg = 34.60 
2025-02-12 22:21:52.332593: Steps 9/47: batch_recall = 35.72, batch_ndcg = 33.73 
2025-02-12 22:21:53.495743: Steps 10/47: batch_recall = 38.59, batch_ndcg = 34.02 
2025-02-12 22:21:54.638615: Steps 11/47: batch_recall = 40.65, batch_ndcg = 35.06 
2025-02-12 22:21:55.783046: Steps 12/47: batch_recall = 39.91, batch_ndcg = 35.72 
2025-02-12 22:21:56.921962: Steps 13/47: batch_recall = 39.72, batch_ndcg = 33.78 
2025-02-12 22:21:58.004720: Steps 14/47: batch_recall = 36.16, batch_ndcg = 30.32 
2025-02-12 22:21:59.089756: Steps 15/47: batch_recall = 46.21, batch_ndcg = 36.71 
2025-02-12 22:22:00.164265: Steps 16/47: batch_recall = 43.05, batch_ndcg = 36.17 
2025-02-12 22:22:01.208037: Steps 17/47: batch_recall = 47.07, batch_ndcg = 37.27 
2025-02-12 22:22:02.268867: Steps 18/47: batch_recall = 42.64, batch_ndcg = 35.02 
2025-02-12 22:22:03.329342: Steps 19/47: batch_recall = 55.64, batch_ndcg = 43.41 
2025-02-12 22:22:04.365629: Steps 20/47: batch_recall = 54.21, batch_ndcg = 43.89 
2025-02-12 22:22:05.414672: Steps 21/47: batch_recall = 57.36, batch_ndcg = 43.59 
2025-02-12 22:22:06.447485: Steps 22/47: batch_recall = 45.96, batch_ndcg = 37.29 
2025-02-12 22:22:07.473571: Steps 23/47: batch_recall = 50.02, batch_ndcg = 36.68 
2025-02-12 22:22:08.516657: Steps 24/47: batch_recall = 53.43, batch_ndcg = 42.89 
2025-02-12 22:22:09.540794: Steps 25/47: batch_recall = 56.73, batch_ndcg = 45.71 
2025-02-12 22:22:10.539185: Steps 26/47: batch_recall = 53.05, batch_ndcg = 41.24 
2025-02-12 22:22:11.552848: Steps 27/47: batch_recall = 46.50, batch_ndcg = 36.45 
2025-02-12 22:22:12.555214: Steps 28/47: batch_recall = 54.51, batch_ndcg = 41.48 
2025-02-12 22:22:13.562861: Steps 29/47: batch_recall = 57.34, batch_ndcg = 42.28 
2025-02-12 22:22:14.567687: Steps 30/47: batch_recall = 64.37, batch_ndcg = 50.38 
2025-02-12 22:22:15.594481: Steps 31/47: batch_recall = 53.52, batch_ndcg = 40.46 
2025-02-12 22:22:16.593520: Steps 32/47: batch_recall = 63.58, batch_ndcg = 51.17 
2025-02-12 22:22:17.603043: Steps 33/47: batch_recall = 65.05, batch_ndcg = 46.94 
2025-02-12 22:22:18.605501: Steps 34/47: batch_recall = 51.07, batch_ndcg = 38.68 
2025-02-12 22:22:19.579538: Steps 35/47: batch_recall = 66.55, batch_ndcg = 48.71 
2025-02-12 22:22:20.553313: Steps 36/47: batch_recall = 67.75, batch_ndcg = 51.10 
2025-02-12 22:22:21.515624: Steps 37/47: batch_recall = 67.73, batch_ndcg = 53.91 
2025-02-12 22:22:22.471665: Steps 38/47: batch_recall = 74.36, batch_ndcg = 56.82 
2025-02-12 22:22:23.430834: Steps 39/47: batch_recall = 72.74, batch_ndcg = 50.80 
2025-02-12 22:22:24.387589: Steps 40/47: batch_recall = 62.16, batch_ndcg = 49.48 
2025-02-12 22:22:25.346240: Steps 41/47: batch_recall = 77.87, batch_ndcg = 57.37 
2025-02-12 22:22:26.295964: Steps 42/47: batch_recall = 71.94, batch_ndcg = 50.11 
2025-02-12 22:22:27.251441: Steps 43/47: batch_recall = 77.72, batch_ndcg = 54.60 
2025-02-12 22:22:28.228796: Steps 44/47: batch_recall = 74.33, batch_ndcg = 53.42 
2025-02-12 22:22:29.168777: Steps 45/47: batch_recall = 85.17, batch_ndcg = 62.76 
2025-02-12 22:22:29.273927: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.49 
2025-02-12 22:22:29.274056: Epoch 4/1000, Test: Recall = 0.1011, NDCG = 0.0807  

2025-02-12 22:22:30.469258: Training Step 0/59: batchLoss = 2.8106, diffLoss = 13.6926, kgLoss = 0.0901
2025-02-12 22:22:31.390505: Training Step 1/59: batchLoss = 3.0073, diffLoss = 14.6571, kgLoss = 0.0949
2025-02-12 22:22:32.316048: Training Step 2/59: batchLoss = 2.9397, diffLoss = 14.3616, kgLoss = 0.0842
2025-02-12 22:22:33.231859: Training Step 3/59: batchLoss = 2.7591, diffLoss = 13.4562, kgLoss = 0.0848
2025-02-12 22:22:34.156909: Training Step 4/59: batchLoss = 3.3230, diffLoss = 16.2250, kgLoss = 0.0976
2025-02-12 22:22:35.078177: Training Step 5/59: batchLoss = 3.0999, diffLoss = 15.1417, kgLoss = 0.0894
2025-02-12 22:22:35.994093: Training Step 6/59: batchLoss = 3.2160, diffLoss = 15.7027, kgLoss = 0.0943
2025-02-12 22:22:36.906306: Training Step 7/59: batchLoss = 2.7021, diffLoss = 13.1928, kgLoss = 0.0795
2025-02-12 22:22:37.822388: Training Step 8/59: batchLoss = 3.1726, diffLoss = 15.4914, kgLoss = 0.0929
2025-02-12 22:22:38.739709: Training Step 9/59: batchLoss = 3.4695, diffLoss = 16.9252, kgLoss = 0.1056
2025-02-12 22:22:39.661226: Training Step 10/59: batchLoss = 2.9604, diffLoss = 14.4680, kgLoss = 0.0835
2025-02-12 22:22:40.583610: Training Step 11/59: batchLoss = 2.7066, diffLoss = 13.2190, kgLoss = 0.0786
2025-02-12 22:22:41.499771: Training Step 12/59: batchLoss = 2.8786, diffLoss = 14.0562, kgLoss = 0.0843
2025-02-12 22:22:42.425500: Training Step 13/59: batchLoss = 3.4313, diffLoss = 16.7279, kgLoss = 0.1071
2025-02-12 22:22:43.339911: Training Step 14/59: batchLoss = 2.8544, diffLoss = 13.9469, kgLoss = 0.0813
2025-02-12 22:22:44.257314: Training Step 15/59: batchLoss = 2.8440, diffLoss = 13.8857, kgLoss = 0.0836
2025-02-12 22:22:45.169170: Training Step 16/59: batchLoss = 3.5453, diffLoss = 17.3387, kgLoss = 0.0970
2025-02-12 22:22:46.088689: Training Step 17/59: batchLoss = 3.0249, diffLoss = 14.7838, kgLoss = 0.0852
2025-02-12 22:22:47.015968: Training Step 18/59: batchLoss = 3.1295, diffLoss = 15.2700, kgLoss = 0.0944
2025-02-12 22:22:47.945058: Training Step 19/59: batchLoss = 2.8547, diffLoss = 13.9600, kgLoss = 0.0783
2025-02-12 22:22:48.871171: Training Step 20/59: batchLoss = 3.2545, diffLoss = 15.8802, kgLoss = 0.0980
2025-02-12 22:22:49.794441: Training Step 21/59: batchLoss = 3.1581, diffLoss = 15.4223, kgLoss = 0.0920
2025-02-12 22:22:50.723455: Training Step 22/59: batchLoss = 2.9228, diffLoss = 14.2576, kgLoss = 0.0891
2025-02-12 22:22:51.659490: Training Step 23/59: batchLoss = 3.1444, diffLoss = 15.3730, kgLoss = 0.0872
2025-02-12 22:22:52.590625: Training Step 24/59: batchLoss = 3.1824, diffLoss = 15.5353, kgLoss = 0.0942
2025-02-12 22:22:53.522208: Training Step 25/59: batchLoss = 3.3357, diffLoss = 16.2819, kgLoss = 0.0991
2025-02-12 22:22:54.451505: Training Step 26/59: batchLoss = 3.2296, diffLoss = 15.7729, kgLoss = 0.0938
2025-02-12 22:22:55.389018: Training Step 27/59: batchLoss = 2.9679, diffLoss = 14.4852, kgLoss = 0.0885
2025-02-12 22:22:56.317963: Training Step 28/59: batchLoss = 3.0321, diffLoss = 14.8106, kgLoss = 0.0874
2025-02-12 22:22:57.250226: Training Step 29/59: batchLoss = 3.1056, diffLoss = 15.1665, kgLoss = 0.0904
2025-02-12 22:22:58.173014: Training Step 30/59: batchLoss = 3.8119, diffLoss = 18.6429, kgLoss = 0.1042
2025-02-12 22:22:59.098652: Training Step 31/59: batchLoss = 2.9573, diffLoss = 14.4502, kgLoss = 0.0841
2025-02-12 22:23:00.018326: Training Step 32/59: batchLoss = 3.1299, diffLoss = 15.2825, kgLoss = 0.0918
2025-02-12 22:23:00.936893: Training Step 33/59: batchLoss = 3.1463, diffLoss = 15.3952, kgLoss = 0.0840
2025-02-12 22:23:01.865590: Training Step 34/59: batchLoss = 3.0429, diffLoss = 14.8870, kgLoss = 0.0819
2025-02-12 22:23:02.784240: Training Step 35/59: batchLoss = 3.4707, diffLoss = 16.9565, kgLoss = 0.0993
2025-02-12 22:23:03.704793: Training Step 36/59: batchLoss = 2.8463, diffLoss = 13.9123, kgLoss = 0.0798
2025-02-12 22:23:04.632469: Training Step 37/59: batchLoss = 3.6158, diffLoss = 17.6930, kgLoss = 0.0965
2025-02-12 22:23:05.559051: Training Step 38/59: batchLoss = 3.5860, diffLoss = 17.5234, kgLoss = 0.1016
2025-02-12 22:23:06.489505: Training Step 39/59: batchLoss = 3.3644, diffLoss = 16.4586, kgLoss = 0.0908
2025-02-12 22:23:07.410745: Training Step 40/59: batchLoss = 3.5418, diffLoss = 17.2796, kgLoss = 0.1074
2025-02-12 22:23:08.330170: Training Step 41/59: batchLoss = 3.7627, diffLoss = 18.3828, kgLoss = 0.1077
2025-02-12 22:23:09.253733: Training Step 42/59: batchLoss = 2.6571, diffLoss = 12.9804, kgLoss = 0.0763
2025-02-12 22:23:10.188934: Training Step 43/59: batchLoss = 3.2056, diffLoss = 15.6593, kgLoss = 0.0922
2025-02-12 22:23:11.113806: Training Step 44/59: batchLoss = 3.5063, diffLoss = 17.1501, kgLoss = 0.0953
2025-02-12 22:23:12.034471: Training Step 45/59: batchLoss = 2.9592, diffLoss = 14.4521, kgLoss = 0.0859
2025-02-12 22:23:12.965764: Training Step 46/59: batchLoss = 3.2085, diffLoss = 15.6764, kgLoss = 0.0916
2025-02-12 22:23:13.893125: Training Step 47/59: batchLoss = 3.0489, diffLoss = 14.9262, kgLoss = 0.0795
2025-02-12 22:23:14.824380: Training Step 48/59: batchLoss = 3.5819, diffLoss = 17.4923, kgLoss = 0.1044
2025-02-12 22:23:15.751061: Training Step 49/59: batchLoss = 2.9377, diffLoss = 14.3572, kgLoss = 0.0829
2025-02-12 22:23:16.681994: Training Step 50/59: batchLoss = 3.0781, diffLoss = 15.0274, kgLoss = 0.0907
2025-02-12 22:23:17.609520: Training Step 51/59: batchLoss = 3.1577, diffLoss = 15.4383, kgLoss = 0.0876
2025-02-12 22:23:18.532411: Training Step 52/59: batchLoss = 3.2700, diffLoss = 15.9896, kgLoss = 0.0901
2025-02-12 22:23:19.462743: Training Step 53/59: batchLoss = 3.3758, diffLoss = 16.4951, kgLoss = 0.0960
2025-02-12 22:23:20.383710: Training Step 54/59: batchLoss = 3.1669, diffLoss = 15.4764, kgLoss = 0.0896
2025-02-12 22:23:21.307306: Training Step 55/59: batchLoss = 3.1102, diffLoss = 15.1888, kgLoss = 0.0905
2025-02-12 22:23:22.224810: Training Step 56/59: batchLoss = 3.0937, diffLoss = 15.1183, kgLoss = 0.0875
2025-02-12 22:23:23.074145: Training Step 57/59: batchLoss = 3.2554, diffLoss = 15.9009, kgLoss = 0.0940
2025-02-12 22:23:23.919035: Training Step 58/59: batchLoss = 3.2335, diffLoss = 15.7858, kgLoss = 0.0954
2025-02-12 22:23:24.013719: 
2025-02-12 22:23:24.014486: Epoch 5/1000, Train: epLoss = 0.4655, epDfLoss = 2.2737, epKgLoss = 0.0134  
2025-02-12 22:23:25.483866: Steps 0/47: batch_recall = 27.87, batch_ndcg = 31.03 
2025-02-12 22:23:26.774818: Steps 1/47: batch_recall = 30.66, batch_ndcg = 29.51 
2025-02-12 22:23:28.024786: Steps 2/47: batch_recall = 33.82, batch_ndcg = 34.00 
2025-02-12 22:23:29.295187: Steps 3/47: batch_recall = 39.19, batch_ndcg = 35.04 
2025-02-12 22:23:30.491380: Steps 4/47: batch_recall = 32.91, batch_ndcg = 32.56 
2025-02-12 22:23:31.716671: Steps 5/47: batch_recall = 26.89, batch_ndcg = 26.73 
2025-02-12 22:23:32.913347: Steps 6/47: batch_recall = 33.27, batch_ndcg = 31.51 
2025-02-12 22:23:34.073294: Steps 7/47: batch_recall = 37.13, batch_ndcg = 33.68 
2025-02-12 22:23:35.250691: Steps 8/47: batch_recall = 37.45, batch_ndcg = 34.65 
2025-02-12 22:23:36.389882: Steps 9/47: batch_recall = 36.18, batch_ndcg = 34.67 
2025-02-12 22:23:37.545873: Steps 10/47: batch_recall = 38.85, batch_ndcg = 34.43 
2025-02-12 22:23:38.677191: Steps 11/47: batch_recall = 44.11, batch_ndcg = 36.85 
2025-02-12 22:23:39.810229: Steps 12/47: batch_recall = 41.00, batch_ndcg = 36.55 
2025-02-12 22:23:40.943416: Steps 13/47: batch_recall = 39.58, batch_ndcg = 33.99 
2025-02-12 22:23:42.035271: Steps 14/47: batch_recall = 35.84, batch_ndcg = 31.11 
2025-02-12 22:23:43.119707: Steps 15/47: batch_recall = 44.65, batch_ndcg = 35.75 
2025-02-12 22:23:44.193652: Steps 16/47: batch_recall = 43.14, batch_ndcg = 36.67 
2025-02-12 22:23:45.232491: Steps 17/47: batch_recall = 46.81, batch_ndcg = 38.04 
2025-02-12 22:23:46.294398: Steps 18/47: batch_recall = 42.11, batch_ndcg = 35.62 
2025-02-12 22:23:47.359303: Steps 19/47: batch_recall = 55.70, batch_ndcg = 45.34 
2025-02-12 22:23:48.405683: Steps 20/47: batch_recall = 52.48, batch_ndcg = 43.38 
2025-02-12 22:23:49.461519: Steps 21/47: batch_recall = 56.29, batch_ndcg = 41.88 
2025-02-12 22:23:50.508926: Steps 22/47: batch_recall = 48.71, batch_ndcg = 39.94 
2025-02-12 22:23:51.557741: Steps 23/47: batch_recall = 50.72, batch_ndcg = 38.45 
2025-02-12 22:23:52.617390: Steps 24/47: batch_recall = 55.41, batch_ndcg = 44.07 
2025-02-12 22:23:53.658161: Steps 25/47: batch_recall = 57.04, batch_ndcg = 46.46 
2025-02-12 22:23:54.669008: Steps 26/47: batch_recall = 54.05, batch_ndcg = 42.29 
2025-02-12 22:23:55.706978: Steps 27/47: batch_recall = 49.12, batch_ndcg = 37.54 
2025-02-12 22:23:56.723995: Steps 28/47: batch_recall = 54.00, batch_ndcg = 41.31 
2025-02-12 22:23:57.743161: Steps 29/47: batch_recall = 58.28, batch_ndcg = 43.87 
2025-02-12 22:23:58.747971: Steps 30/47: batch_recall = 65.25, batch_ndcg = 51.63 
2025-02-12 22:23:59.785308: Steps 31/47: batch_recall = 54.36, batch_ndcg = 41.41 
2025-02-12 22:24:00.795637: Steps 32/47: batch_recall = 62.31, batch_ndcg = 51.00 
2025-02-12 22:24:01.800092: Steps 33/47: batch_recall = 66.51, batch_ndcg = 48.70 
2025-02-12 22:24:02.806619: Steps 34/47: batch_recall = 53.56, batch_ndcg = 40.55 
2025-02-12 22:24:03.780553: Steps 35/47: batch_recall = 67.86, batch_ndcg = 50.03 
2025-02-12 22:24:04.767280: Steps 36/47: batch_recall = 70.62, batch_ndcg = 51.79 
2025-02-12 22:24:05.745643: Steps 37/47: batch_recall = 67.67, batch_ndcg = 54.11 
2025-02-12 22:24:06.718165: Steps 38/47: batch_recall = 73.78, batch_ndcg = 55.74 
2025-02-12 22:24:07.680486: Steps 39/47: batch_recall = 73.50, batch_ndcg = 52.82 
2025-02-12 22:24:08.639122: Steps 40/47: batch_recall = 61.55, batch_ndcg = 49.71 
2025-02-12 22:24:09.604238: Steps 41/47: batch_recall = 77.13, batch_ndcg = 58.43 
2025-02-12 22:24:10.560562: Steps 42/47: batch_recall = 75.58, batch_ndcg = 53.40 
2025-02-12 22:24:11.522979: Steps 43/47: batch_recall = 76.16, batch_ndcg = 53.58 
2025-02-12 22:24:12.488991: Steps 44/47: batch_recall = 80.85, batch_ndcg = 56.82 
2025-02-12 22:24:13.430549: Steps 45/47: batch_recall = 87.59, batch_ndcg = 63.81 
2025-02-12 22:24:13.537771: Steps 46/47: batch_recall = 1.42, batch_ndcg = 1.29 
2025-02-12 22:24:13.537873: Epoch 5/1000, Test: Recall = 0.1026, NDCG = 0.0824  

2025-02-12 22:24:14.733987: Training Step 0/59: batchLoss = 3.2022, diffLoss = 15.6331, kgLoss = 0.0945
2025-02-12 22:24:15.665003: Training Step 1/59: batchLoss = 2.8614, diffLoss = 13.9686, kgLoss = 0.0847
2025-02-12 22:24:16.589443: Training Step 2/59: batchLoss = 2.6914, diffLoss = 13.1172, kgLoss = 0.0849
2025-02-12 22:24:17.515251: Training Step 3/59: batchLoss = 2.8956, diffLoss = 14.1241, kgLoss = 0.0885
2025-02-12 22:24:18.437951: Training Step 4/59: batchLoss = 3.2029, diffLoss = 15.6111, kgLoss = 0.1008
2025-02-12 22:24:19.365338: Training Step 5/59: batchLoss = 3.0645, diffLoss = 14.9569, kgLoss = 0.0913
2025-02-12 22:24:20.297000: Training Step 6/59: batchLoss = 2.6827, diffLoss = 13.1109, kgLoss = 0.0757
2025-02-12 22:24:21.224682: Training Step 7/59: batchLoss = 2.9142, diffLoss = 14.2106, kgLoss = 0.0901
2025-02-12 22:24:22.145493: Training Step 8/59: batchLoss = 2.5659, diffLoss = 12.5217, kgLoss = 0.0770
2025-02-12 22:24:23.066121: Training Step 9/59: batchLoss = 2.8688, diffLoss = 13.9601, kgLoss = 0.0960
2025-02-12 22:24:23.984434: Training Step 10/59: batchLoss = 2.9362, diffLoss = 14.3315, kgLoss = 0.0874
2025-02-12 22:24:24.902093: Training Step 11/59: batchLoss = 3.0129, diffLoss = 14.7055, kgLoss = 0.0897
2025-02-12 22:24:25.820259: Training Step 12/59: batchLoss = 2.8727, diffLoss = 14.0432, kgLoss = 0.0801
2025-02-12 22:24:26.745055: Training Step 13/59: batchLoss = 3.4149, diffLoss = 16.6831, kgLoss = 0.0979
2025-02-12 22:24:27.660898: Training Step 14/59: batchLoss = 2.7503, diffLoss = 13.4182, kgLoss = 0.0834
2025-02-12 22:24:28.582214: Training Step 15/59: batchLoss = 3.2210, diffLoss = 15.6852, kgLoss = 0.1050
2025-02-12 22:24:29.492856: Training Step 16/59: batchLoss = 2.9607, diffLoss = 14.4531, kgLoss = 0.0876
2025-02-12 22:24:30.411420: Training Step 17/59: batchLoss = 2.8646, diffLoss = 13.9843, kgLoss = 0.0847
2025-02-12 22:24:31.331650: Training Step 18/59: batchLoss = 3.4351, diffLoss = 16.7655, kgLoss = 0.1025
2025-02-12 22:24:32.256194: Training Step 19/59: batchLoss = 3.0957, diffLoss = 15.1236, kgLoss = 0.0887
2025-02-12 22:24:33.183991: Training Step 20/59: batchLoss = 3.2922, diffLoss = 16.0994, kgLoss = 0.0903
2025-02-12 22:24:34.105820: Training Step 21/59: batchLoss = 2.7291, diffLoss = 13.3275, kgLoss = 0.0794
2025-02-12 22:24:35.026379: Training Step 22/59: batchLoss = 3.1667, diffLoss = 15.4441, kgLoss = 0.0973
2025-02-12 22:24:35.957924: Training Step 23/59: batchLoss = 3.4378, diffLoss = 16.8152, kgLoss = 0.0934
2025-02-12 22:24:36.894261: Training Step 24/59: batchLoss = 3.2372, diffLoss = 15.8294, kgLoss = 0.0892
2025-02-12 22:24:37.821203: Training Step 25/59: batchLoss = 3.1026, diffLoss = 15.1492, kgLoss = 0.0909
2025-02-12 22:24:38.751612: Training Step 26/59: batchLoss = 3.0095, diffLoss = 14.7207, kgLoss = 0.0818
2025-02-12 22:24:39.675649: Training Step 27/59: batchLoss = 3.4584, diffLoss = 16.9038, kgLoss = 0.0971
2025-02-12 22:24:40.603044: Training Step 28/59: batchLoss = 3.0753, diffLoss = 15.0032, kgLoss = 0.0934
2025-02-12 22:24:41.530534: Training Step 29/59: batchLoss = 3.1876, diffLoss = 15.5749, kgLoss = 0.0908
2025-02-12 22:24:42.452858: Training Step 30/59: batchLoss = 3.1091, diffLoss = 15.1706, kgLoss = 0.0937
2025-02-12 22:24:43.378244: Training Step 31/59: batchLoss = 3.5530, diffLoss = 17.3734, kgLoss = 0.0978
2025-02-12 22:24:44.300785: Training Step 32/59: batchLoss = 3.3029, diffLoss = 16.1280, kgLoss = 0.0967
2025-02-12 22:24:45.221817: Training Step 33/59: batchLoss = 2.8360, diffLoss = 13.8680, kgLoss = 0.0779
2025-02-12 22:24:46.142095: Training Step 34/59: batchLoss = 3.1894, diffLoss = 15.5840, kgLoss = 0.0907
2025-02-12 22:24:47.064733: Training Step 35/59: batchLoss = 3.2560, diffLoss = 15.8833, kgLoss = 0.0992
2025-02-12 22:24:47.992859: Training Step 36/59: batchLoss = 3.0358, diffLoss = 14.8202, kgLoss = 0.0896
2025-02-12 22:24:48.922801: Training Step 37/59: batchLoss = 3.9183, diffLoss = 19.1574, kgLoss = 0.1085
2025-02-12 22:24:49.845850: Training Step 38/59: batchLoss = 2.6471, diffLoss = 12.9266, kgLoss = 0.0772
2025-02-12 22:24:50.770173: Training Step 39/59: batchLoss = 3.0858, diffLoss = 15.0710, kgLoss = 0.0894
2025-02-12 22:24:51.695357: Training Step 40/59: batchLoss = 3.6154, diffLoss = 17.6640, kgLoss = 0.1032
2025-02-12 22:24:52.627148: Training Step 41/59: batchLoss = 2.7715, diffLoss = 13.5433, kgLoss = 0.0785
2025-02-12 22:24:53.556695: Training Step 42/59: batchLoss = 3.2141, diffLoss = 15.6877, kgLoss = 0.0957
2025-02-12 22:24:54.482063: Training Step 43/59: batchLoss = 3.1382, diffLoss = 15.3211, kgLoss = 0.0924
2025-02-12 22:24:55.418240: Training Step 44/59: batchLoss = 3.0412, diffLoss = 14.8540, kgLoss = 0.0880
2025-02-12 22:24:56.346063: Training Step 45/59: batchLoss = 2.9518, diffLoss = 14.4143, kgLoss = 0.0862
2025-02-12 22:24:57.287070: Training Step 46/59: batchLoss = 3.2167, diffLoss = 15.6813, kgLoss = 0.1006
2025-02-12 22:24:58.219528: Training Step 47/59: batchLoss = 3.0155, diffLoss = 14.7212, kgLoss = 0.0890
2025-02-12 22:24:59.151511: Training Step 48/59: batchLoss = 3.0782, diffLoss = 15.0324, kgLoss = 0.0896
2025-02-12 22:25:00.079540: Training Step 49/59: batchLoss = 3.2458, diffLoss = 15.8318, kgLoss = 0.0994
2025-02-12 22:25:01.001237: Training Step 50/59: batchLoss = 3.4660, diffLoss = 16.9134, kgLoss = 0.1041
2025-02-12 22:25:01.927616: Training Step 51/59: batchLoss = 3.1188, diffLoss = 15.2576, kgLoss = 0.0841
2025-02-12 22:25:02.848495: Training Step 52/59: batchLoss = 3.3841, diffLoss = 16.5470, kgLoss = 0.0934
2025-02-12 22:25:03.768139: Training Step 53/59: batchLoss = 3.0868, diffLoss = 15.0872, kgLoss = 0.0867
2025-02-12 22:25:04.683279: Training Step 54/59: batchLoss = 3.2318, diffLoss = 15.7904, kgLoss = 0.0922
2025-02-12 22:25:05.619658: Training Step 55/59: batchLoss = 3.1884, diffLoss = 15.5920, kgLoss = 0.0875
2025-02-12 22:25:06.540908: Training Step 56/59: batchLoss = 3.2293, diffLoss = 15.7744, kgLoss = 0.0931
2025-02-12 22:25:07.377905: Training Step 57/59: batchLoss = 3.1351, diffLoss = 15.3241, kgLoss = 0.0878
2025-02-12 22:25:08.218381: Training Step 58/59: batchLoss = 3.1785, diffLoss = 15.5637, kgLoss = 0.0822
2025-02-12 22:25:08.311540: 
2025-02-12 22:25:08.311871: Epoch 6/1000, Train: epLoss = 0.4586, epDfLoss = 2.2396, epKgLoss = 0.0134  
2025-02-12 22:25:09.769622: Steps 0/47: batch_recall = 27.78, batch_ndcg = 31.18 
2025-02-12 22:25:11.051354: Steps 1/47: batch_recall = 31.97, batch_ndcg = 30.82 
2025-02-12 22:25:12.293639: Steps 2/47: batch_recall = 35.05, batch_ndcg = 35.14 
2025-02-12 22:25:13.549378: Steps 3/47: batch_recall = 38.58, batch_ndcg = 34.40 
2025-02-12 22:25:14.752646: Steps 4/47: batch_recall = 32.99, batch_ndcg = 32.45 
2025-02-12 22:25:15.973329: Steps 5/47: batch_recall = 27.94, batch_ndcg = 27.29 
2025-02-12 22:25:17.167396: Steps 6/47: batch_recall = 34.59, batch_ndcg = 32.71 
2025-02-12 22:25:18.339119: Steps 7/47: batch_recall = 36.49, batch_ndcg = 33.87 
2025-02-12 22:25:19.517032: Steps 8/47: batch_recall = 39.47, batch_ndcg = 35.76 
2025-02-12 22:25:20.658682: Steps 9/47: batch_recall = 36.57, batch_ndcg = 35.44 
2025-02-12 22:25:21.818581: Steps 10/47: batch_recall = 39.16, batch_ndcg = 34.90 
2025-02-12 22:25:22.958662: Steps 11/47: batch_recall = 43.35, batch_ndcg = 36.96 
2025-02-12 22:25:24.090828: Steps 12/47: batch_recall = 41.12, batch_ndcg = 36.62 
2025-02-12 22:25:25.220801: Steps 13/47: batch_recall = 39.43, batch_ndcg = 34.28 
2025-02-12 22:25:26.312788: Steps 14/47: batch_recall = 35.91, batch_ndcg = 31.17 
2025-02-12 22:25:27.395759: Steps 15/47: batch_recall = 46.92, batch_ndcg = 37.59 
2025-02-12 22:25:28.468926: Steps 16/47: batch_recall = 41.37, batch_ndcg = 35.77 
2025-02-12 22:25:29.522774: Steps 17/47: batch_recall = 46.24, batch_ndcg = 38.19 
2025-02-12 22:25:30.585054: Steps 18/47: batch_recall = 42.25, batch_ndcg = 35.77 
2025-02-12 22:25:31.638823: Steps 19/47: batch_recall = 56.56, batch_ndcg = 44.93 
2025-02-12 22:25:32.670783: Steps 20/47: batch_recall = 54.89, batch_ndcg = 45.73 
2025-02-12 22:25:33.716119: Steps 21/47: batch_recall = 59.93, batch_ndcg = 44.67 
2025-02-12 22:25:34.770987: Steps 22/47: batch_recall = 48.27, batch_ndcg = 39.55 
2025-02-12 22:25:35.824109: Steps 23/47: batch_recall = 51.69, batch_ndcg = 39.02 
2025-02-12 22:25:36.887000: Steps 24/47: batch_recall = 55.78, batch_ndcg = 45.29 
2025-02-12 22:25:37.936035: Steps 25/47: batch_recall = 59.64, batch_ndcg = 47.23 
2025-02-12 22:25:38.961616: Steps 26/47: batch_recall = 51.75, batch_ndcg = 41.20 
2025-02-12 22:25:39.996926: Steps 27/47: batch_recall = 51.23, batch_ndcg = 39.28 
2025-02-12 22:25:41.013178: Steps 28/47: batch_recall = 55.73, batch_ndcg = 42.06 
2025-02-12 22:25:42.027732: Steps 29/47: batch_recall = 59.26, batch_ndcg = 43.50 
2025-02-12 22:25:43.036821: Steps 30/47: batch_recall = 65.08, batch_ndcg = 51.76 
2025-02-12 22:25:44.070178: Steps 31/47: batch_recall = 53.13, batch_ndcg = 41.49 
2025-02-12 22:25:45.079164: Steps 32/47: batch_recall = 64.05, batch_ndcg = 52.30 
2025-02-12 22:25:46.091104: Steps 33/47: batch_recall = 68.06, batch_ndcg = 50.73 
2025-02-12 22:25:47.100577: Steps 34/47: batch_recall = 53.98, batch_ndcg = 40.62 
2025-02-12 22:25:48.080295: Steps 35/47: batch_recall = 68.38, batch_ndcg = 51.35 
2025-02-12 22:25:49.066535: Steps 36/47: batch_recall = 69.80, batch_ndcg = 52.85 
2025-02-12 22:25:50.037799: Steps 37/47: batch_recall = 69.72, batch_ndcg = 55.27 
2025-02-12 22:25:51.011251: Steps 38/47: batch_recall = 76.03, batch_ndcg = 56.89 
2025-02-12 22:25:51.975331: Steps 39/47: batch_recall = 74.68, batch_ndcg = 54.79 
2025-02-12 22:25:52.935604: Steps 40/47: batch_recall = 63.56, batch_ndcg = 51.18 
2025-02-12 22:25:53.882646: Steps 41/47: batch_recall = 77.81, batch_ndcg = 58.15 
2025-02-12 22:25:54.827708: Steps 42/47: batch_recall = 75.22, batch_ndcg = 54.39 
2025-02-12 22:25:55.796297: Steps 43/47: batch_recall = 77.44, batch_ndcg = 55.49 
2025-02-12 22:25:56.772658: Steps 44/47: batch_recall = 80.21, batch_ndcg = 56.64 
2025-02-12 22:25:57.711536: Steps 45/47: batch_recall = 90.11, batch_ndcg = 66.25 
2025-02-12 22:25:57.817929: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.58 
2025-02-12 22:25:57.818068: Epoch 6/1000, Test: Recall = 0.1040, NDCG = 0.0838  

2025-02-12 22:25:59.015560: Training Step 0/59: batchLoss = 3.0177, diffLoss = 14.7336, kgLoss = 0.0887
2025-02-12 22:25:59.950580: Training Step 1/59: batchLoss = 2.6523, diffLoss = 12.9288, kgLoss = 0.0832
2025-02-12 22:26:00.879375: Training Step 2/59: batchLoss = 2.7999, diffLoss = 13.6552, kgLoss = 0.0861
2025-02-12 22:26:01.809683: Training Step 3/59: batchLoss = 2.8586, diffLoss = 13.9620, kgLoss = 0.0828
2025-02-12 22:26:02.732866: Training Step 4/59: batchLoss = 3.4086, diffLoss = 16.6373, kgLoss = 0.1014
2025-02-12 22:26:03.656784: Training Step 5/59: batchLoss = 2.7371, diffLoss = 13.3474, kgLoss = 0.0846
2025-02-12 22:26:04.581652: Training Step 6/59: batchLoss = 3.0603, diffLoss = 14.9297, kgLoss = 0.0929
2025-02-12 22:26:05.501025: Training Step 7/59: batchLoss = 3.1263, diffLoss = 15.2737, kgLoss = 0.0895
2025-02-12 22:26:06.411188: Training Step 8/59: batchLoss = 3.1221, diffLoss = 15.1988, kgLoss = 0.1029
2025-02-12 22:26:07.326893: Training Step 9/59: batchLoss = 3.2142, diffLoss = 15.6839, kgLoss = 0.0968
2025-02-12 22:26:08.240145: Training Step 10/59: batchLoss = 2.8807, diffLoss = 14.0945, kgLoss = 0.0772
2025-02-12 22:26:09.155656: Training Step 11/59: batchLoss = 3.5479, diffLoss = 17.2996, kgLoss = 0.1100
2025-02-12 22:26:10.064950: Training Step 12/59: batchLoss = 2.8685, diffLoss = 13.9225, kgLoss = 0.1050
2025-02-12 22:26:10.976682: Training Step 13/59: batchLoss = 3.2649, diffLoss = 15.9478, kgLoss = 0.0942
2025-02-12 22:26:11.895782: Training Step 14/59: batchLoss = 3.4287, diffLoss = 16.7189, kgLoss = 0.1062
2025-02-12 22:26:12.812062: Training Step 15/59: batchLoss = 2.4517, diffLoss = 11.9577, kgLoss = 0.0753
2025-02-12 22:26:13.729635: Training Step 16/59: batchLoss = 2.6877, diffLoss = 13.1246, kgLoss = 0.0785
2025-02-12 22:26:14.652290: Training Step 17/59: batchLoss = 3.1641, diffLoss = 15.4591, kgLoss = 0.0904
2025-02-12 22:26:15.579925: Training Step 18/59: batchLoss = 3.0048, diffLoss = 14.6592, kgLoss = 0.0912
2025-02-12 22:26:16.500433: Training Step 19/59: batchLoss = 2.5154, diffLoss = 12.2420, kgLoss = 0.0838
2025-02-12 22:26:17.431651: Training Step 20/59: batchLoss = 3.0652, diffLoss = 14.9929, kgLoss = 0.0832
2025-02-12 22:26:18.355248: Training Step 21/59: batchLoss = 2.7992, diffLoss = 13.6598, kgLoss = 0.0841
2025-02-12 22:26:19.283486: Training Step 22/59: batchLoss = 3.2663, diffLoss = 15.9540, kgLoss = 0.0943
2025-02-12 22:26:20.210309: Training Step 23/59: batchLoss = 3.2814, diffLoss = 16.0348, kgLoss = 0.0931
2025-02-12 22:26:21.142346: Training Step 24/59: batchLoss = 2.9703, diffLoss = 14.5124, kgLoss = 0.0847
2025-02-12 22:26:22.074875: Training Step 25/59: batchLoss = 2.9717, diffLoss = 14.5020, kgLoss = 0.0892
2025-02-12 22:26:23.002636: Training Step 26/59: batchLoss = 3.0359, diffLoss = 14.8312, kgLoss = 0.0871
2025-02-12 22:26:23.933015: Training Step 27/59: batchLoss = 3.0880, diffLoss = 15.0791, kgLoss = 0.0902
2025-02-12 22:26:24.874922: Training Step 28/59: batchLoss = 2.6217, diffLoss = 12.7812, kgLoss = 0.0818
2025-02-12 22:26:25.803570: Training Step 29/59: batchLoss = 3.2111, diffLoss = 15.6968, kgLoss = 0.0897
2025-02-12 22:26:26.725663: Training Step 30/59: batchLoss = 3.2643, diffLoss = 15.9783, kgLoss = 0.0858
2025-02-12 22:26:27.640015: Training Step 31/59: batchLoss = 3.0864, diffLoss = 15.0583, kgLoss = 0.0934
2025-02-12 22:26:28.570624: Training Step 32/59: batchLoss = 2.8528, diffLoss = 13.9405, kgLoss = 0.0808
2025-02-12 22:26:29.485306: Training Step 33/59: batchLoss = 3.0548, diffLoss = 14.9237, kgLoss = 0.0876
2025-02-12 22:26:30.400837: Training Step 34/59: batchLoss = 3.0063, diffLoss = 14.6899, kgLoss = 0.0854
2025-02-12 22:26:31.326634: Training Step 35/59: batchLoss = 3.1806, diffLoss = 15.4964, kgLoss = 0.1016
2025-02-12 22:26:32.250488: Training Step 36/59: batchLoss = 2.8719, diffLoss = 14.0232, kgLoss = 0.0840
2025-02-12 22:26:33.166408: Training Step 37/59: batchLoss = 3.1222, diffLoss = 15.2482, kgLoss = 0.0907
2025-02-12 22:26:34.089145: Training Step 38/59: batchLoss = 2.9373, diffLoss = 14.3472, kgLoss = 0.0848
2025-02-12 22:26:35.009857: Training Step 39/59: batchLoss = 3.0455, diffLoss = 14.8658, kgLoss = 0.0904
2025-02-12 22:26:35.957495: Training Step 40/59: batchLoss = 2.9621, diffLoss = 14.4777, kgLoss = 0.0832
2025-02-12 22:26:36.883985: Training Step 41/59: batchLoss = 3.1023, diffLoss = 15.1302, kgLoss = 0.0953
2025-02-12 22:26:37.815395: Training Step 42/59: batchLoss = 3.2274, diffLoss = 15.7696, kgLoss = 0.0919
2025-02-12 22:26:38.745626: Training Step 43/59: batchLoss = 3.2122, diffLoss = 15.6877, kgLoss = 0.0933
2025-02-12 22:26:39.673816: Training Step 44/59: batchLoss = 2.8504, diffLoss = 13.9187, kgLoss = 0.0834
2025-02-12 22:26:40.609055: Training Step 45/59: batchLoss = 2.6974, diffLoss = 13.1662, kgLoss = 0.0802
2025-02-12 22:26:41.539304: Training Step 46/59: batchLoss = 2.9725, diffLoss = 14.5032, kgLoss = 0.0898
2025-02-12 22:26:42.465450: Training Step 47/59: batchLoss = 3.5025, diffLoss = 17.1341, kgLoss = 0.0945
2025-02-12 22:26:43.394125: Training Step 48/59: batchLoss = 3.0867, diffLoss = 15.0613, kgLoss = 0.0931
2025-02-12 22:26:44.325591: Training Step 49/59: batchLoss = 3.1380, diffLoss = 15.3389, kgLoss = 0.0877
2025-02-12 22:26:45.258582: Training Step 50/59: batchLoss = 3.4662, diffLoss = 16.9606, kgLoss = 0.0926
2025-02-12 22:26:46.182963: Training Step 51/59: batchLoss = 3.0433, diffLoss = 14.8565, kgLoss = 0.0900
2025-02-12 22:26:47.108782: Training Step 52/59: batchLoss = 3.1827, diffLoss = 15.5373, kgLoss = 0.0940
2025-02-12 22:26:48.028723: Training Step 53/59: batchLoss = 3.6096, diffLoss = 17.6405, kgLoss = 0.1018
2025-02-12 22:26:48.957442: Training Step 54/59: batchLoss = 2.8368, diffLoss = 13.8434, kgLoss = 0.0851
2025-02-12 22:26:49.884739: Training Step 55/59: batchLoss = 3.4209, diffLoss = 16.6983, kgLoss = 0.1015
2025-02-12 22:26:50.809498: Training Step 56/59: batchLoss = 3.5844, diffLoss = 17.5312, kgLoss = 0.0977
2025-02-12 22:26:51.647715: Training Step 57/59: batchLoss = 3.2012, diffLoss = 15.6087, kgLoss = 0.0994
2025-02-12 22:26:52.496233: Training Step 58/59: batchLoss = 3.6089, diffLoss = 17.6536, kgLoss = 0.0977
2025-02-12 22:26:52.587484: 
2025-02-12 22:26:52.588017: Epoch 7/1000, Train: epLoss = 0.4531, epDfLoss = 2.2123, epKgLoss = 0.0133  
2025-02-12 22:26:54.051458: Steps 0/47: batch_recall = 27.82, batch_ndcg = 31.41 
2025-02-12 22:26:55.335972: Steps 1/47: batch_recall = 31.10, batch_ndcg = 30.18 
2025-02-12 22:26:56.578951: Steps 2/47: batch_recall = 34.78, batch_ndcg = 34.94 
2025-02-12 22:26:57.830572: Steps 3/47: batch_recall = 38.02, batch_ndcg = 34.71 
2025-02-12 22:26:59.031338: Steps 4/47: batch_recall = 33.89, batch_ndcg = 33.49 
2025-02-12 22:27:00.247915: Steps 5/47: batch_recall = 28.86, batch_ndcg = 28.26 
2025-02-12 22:27:01.449176: Steps 6/47: batch_recall = 33.84, batch_ndcg = 32.32 
2025-02-12 22:27:02.627802: Steps 7/47: batch_recall = 36.35, batch_ndcg = 34.10 
2025-02-12 22:27:03.801101: Steps 8/47: batch_recall = 41.52, batch_ndcg = 37.56 
2025-02-12 22:27:04.947695: Steps 9/47: batch_recall = 37.69, batch_ndcg = 36.00 
2025-02-12 22:27:06.105058: Steps 10/47: batch_recall = 39.08, batch_ndcg = 34.52 
2025-02-12 22:27:07.235439: Steps 11/47: batch_recall = 45.75, batch_ndcg = 38.01 
2025-02-12 22:27:08.377125: Steps 12/47: batch_recall = 43.71, batch_ndcg = 38.04 
2025-02-12 22:27:09.501808: Steps 13/47: batch_recall = 39.74, batch_ndcg = 34.66 
2025-02-12 22:27:10.600980: Steps 14/47: batch_recall = 36.37, batch_ndcg = 31.48 
2025-02-12 22:27:11.692037: Steps 15/47: batch_recall = 48.84, batch_ndcg = 38.69 
2025-02-12 22:27:12.774275: Steps 16/47: batch_recall = 44.76, batch_ndcg = 37.98 
2025-02-12 22:27:13.822466: Steps 17/47: batch_recall = 47.41, batch_ndcg = 38.19 
2025-02-12 22:27:14.899132: Steps 18/47: batch_recall = 42.50, batch_ndcg = 35.49 
2025-02-12 22:27:15.968561: Steps 19/47: batch_recall = 57.61, batch_ndcg = 46.45 
2025-02-12 22:27:17.008627: Steps 20/47: batch_recall = 56.29, batch_ndcg = 46.88 
2025-02-12 22:27:18.063270: Steps 21/47: batch_recall = 59.23, batch_ndcg = 44.86 
2025-02-12 22:27:19.119683: Steps 22/47: batch_recall = 46.89, batch_ndcg = 39.46 
2025-02-12 22:27:20.169323: Steps 23/47: batch_recall = 51.63, batch_ndcg = 39.92 
2025-02-12 22:27:21.226965: Steps 24/47: batch_recall = 56.96, batch_ndcg = 46.05 
2025-02-12 22:27:22.270008: Steps 25/47: batch_recall = 60.16, batch_ndcg = 47.81 
2025-02-12 22:27:23.288301: Steps 26/47: batch_recall = 51.54, batch_ndcg = 42.94 
2025-02-12 22:27:24.326451: Steps 27/47: batch_recall = 51.23, batch_ndcg = 39.01 
2025-02-12 22:27:25.359355: Steps 28/47: batch_recall = 55.47, batch_ndcg = 42.32 
2025-02-12 22:27:26.381997: Steps 29/47: batch_recall = 60.81, batch_ndcg = 44.99 
2025-02-12 22:27:27.397102: Steps 30/47: batch_recall = 65.73, batch_ndcg = 51.97 
2025-02-12 22:27:28.423829: Steps 31/47: batch_recall = 55.46, batch_ndcg = 42.09 
2025-02-12 22:27:29.441517: Steps 32/47: batch_recall = 63.33, batch_ndcg = 52.51 
2025-02-12 22:27:30.450462: Steps 33/47: batch_recall = 67.41, batch_ndcg = 51.54 
2025-02-12 22:27:31.455271: Steps 34/47: batch_recall = 54.64, batch_ndcg = 41.02 
2025-02-12 22:27:32.436933: Steps 35/47: batch_recall = 70.23, batch_ndcg = 52.65 
2025-02-12 22:27:33.420797: Steps 36/47: batch_recall = 71.24, batch_ndcg = 53.64 
2025-02-12 22:27:34.395828: Steps 37/47: batch_recall = 68.17, batch_ndcg = 55.00 
2025-02-12 22:27:35.365950: Steps 38/47: batch_recall = 75.59, batch_ndcg = 57.74 
2025-02-12 22:27:36.330699: Steps 39/47: batch_recall = 75.08, batch_ndcg = 53.84 
2025-02-12 22:27:37.289465: Steps 40/47: batch_recall = 63.11, batch_ndcg = 51.49 
2025-02-12 22:27:38.253135: Steps 41/47: batch_recall = 77.21, batch_ndcg = 57.88 
2025-02-12 22:27:39.216391: Steps 42/47: batch_recall = 76.91, batch_ndcg = 55.40 
2025-02-12 22:27:40.169456: Steps 43/47: batch_recall = 76.03, batch_ndcg = 55.88 
2025-02-12 22:27:41.125416: Steps 44/47: batch_recall = 80.91, batch_ndcg = 57.76 
2025-02-12 22:27:42.050726: Steps 45/47: batch_recall = 90.22, batch_ndcg = 67.61 
2025-02-12 22:27:42.152726: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.46 
2025-02-12 22:27:42.152821: Epoch 7/1000, Test: Recall = 0.1049, NDCG = 0.0849  

2025-02-12 22:27:43.315194: Training Step 0/59: batchLoss = 2.9552, diffLoss = 14.4260, kgLoss = 0.0874
2025-02-12 22:27:44.230859: Training Step 1/59: batchLoss = 3.2129, diffLoss = 15.6563, kgLoss = 0.1020
2025-02-12 22:27:45.142834: Training Step 2/59: batchLoss = 2.7129, diffLoss = 13.2429, kgLoss = 0.0803
2025-02-12 22:27:46.054185: Training Step 3/59: batchLoss = 3.3332, diffLoss = 16.2765, kgLoss = 0.0974
2025-02-12 22:27:46.972708: Training Step 4/59: batchLoss = 3.3558, diffLoss = 16.3848, kgLoss = 0.0986
2025-02-12 22:27:47.883316: Training Step 5/59: batchLoss = 3.0556, diffLoss = 14.9093, kgLoss = 0.0921
2025-02-12 22:27:48.797776: Training Step 6/59: batchLoss = 3.1465, diffLoss = 15.3662, kgLoss = 0.0916
2025-02-12 22:27:49.720792: Training Step 7/59: batchLoss = 2.9180, diffLoss = 14.2366, kgLoss = 0.0884
2025-02-12 22:27:50.639082: Training Step 8/59: batchLoss = 2.9155, diffLoss = 14.2341, kgLoss = 0.0858
2025-02-12 22:27:51.557924: Training Step 9/59: batchLoss = 2.8118, diffLoss = 13.6998, kgLoss = 0.0898
2025-02-12 22:27:52.480277: Training Step 10/59: batchLoss = 3.1726, diffLoss = 15.4868, kgLoss = 0.0941
2025-02-12 22:27:53.402192: Training Step 11/59: batchLoss = 3.2802, diffLoss = 15.9587, kgLoss = 0.1106
2025-02-12 22:27:54.328598: Training Step 12/59: batchLoss = 2.5995, diffLoss = 12.6700, kgLoss = 0.0819
2025-02-12 22:27:55.251212: Training Step 13/59: batchLoss = 2.9703, diffLoss = 14.4964, kgLoss = 0.0887
2025-02-12 22:27:56.176570: Training Step 14/59: batchLoss = 2.6957, diffLoss = 13.1605, kgLoss = 0.0795
2025-02-12 22:27:57.096477: Training Step 15/59: batchLoss = 2.9688, diffLoss = 14.4609, kgLoss = 0.0958
2025-02-12 22:27:58.018688: Training Step 16/59: batchLoss = 3.2385, diffLoss = 15.8039, kgLoss = 0.0972
2025-02-12 22:27:58.946912: Training Step 17/59: batchLoss = 2.8363, diffLoss = 13.8703, kgLoss = 0.0777
2025-02-12 22:27:59.876210: Training Step 18/59: batchLoss = 2.7329, diffLoss = 13.3426, kgLoss = 0.0804
2025-02-12 22:28:00.808446: Training Step 19/59: batchLoss = 3.2179, diffLoss = 15.7173, kgLoss = 0.0930
2025-02-12 22:28:01.727910: Training Step 20/59: batchLoss = 2.7078, diffLoss = 13.2138, kgLoss = 0.0813
2025-02-12 22:28:02.643121: Training Step 21/59: batchLoss = 3.1699, diffLoss = 15.4731, kgLoss = 0.0941
2025-02-12 22:28:03.558194: Training Step 22/59: batchLoss = 3.1730, diffLoss = 15.4835, kgLoss = 0.0954
2025-02-12 22:28:04.472055: Training Step 23/59: batchLoss = 2.9301, diffLoss = 14.3098, kgLoss = 0.0852
2025-02-12 22:28:05.395309: Training Step 24/59: batchLoss = 3.2340, diffLoss = 15.8048, kgLoss = 0.0912
2025-02-12 22:28:06.302480: Training Step 25/59: batchLoss = 3.0337, diffLoss = 14.8161, kgLoss = 0.0881
2025-02-12 22:28:07.222151: Training Step 26/59: batchLoss = 3.0936, diffLoss = 15.1055, kgLoss = 0.0906
2025-02-12 22:28:08.138116: Training Step 27/59: batchLoss = 3.0001, diffLoss = 14.6493, kgLoss = 0.0879
2025-02-12 22:28:09.055651: Training Step 28/59: batchLoss = 3.0230, diffLoss = 14.7688, kgLoss = 0.0865
2025-02-12 22:28:09.974685: Training Step 29/59: batchLoss = 2.8365, diffLoss = 13.8710, kgLoss = 0.0779
2025-02-12 22:28:10.883577: Training Step 30/59: batchLoss = 3.3114, diffLoss = 16.1804, kgLoss = 0.0942
2025-02-12 22:28:11.803851: Training Step 31/59: batchLoss = 3.0139, diffLoss = 14.7265, kgLoss = 0.0857
2025-02-12 22:28:12.722243: Training Step 32/59: batchLoss = 3.4766, diffLoss = 16.9615, kgLoss = 0.1053
2025-02-12 22:28:13.639765: Training Step 33/59: batchLoss = 2.6623, diffLoss = 12.9702, kgLoss = 0.0853
2025-02-12 22:28:14.565898: Training Step 34/59: batchLoss = 3.1079, diffLoss = 15.1896, kgLoss = 0.0874
2025-02-12 22:28:15.495863: Training Step 35/59: batchLoss = 3.2377, diffLoss = 15.8093, kgLoss = 0.0948
2025-02-12 22:28:16.418320: Training Step 36/59: batchLoss = 3.2463, diffLoss = 15.8834, kgLoss = 0.0870
2025-02-12 22:28:17.341102: Training Step 37/59: batchLoss = 2.7043, diffLoss = 13.2035, kgLoss = 0.0795
2025-02-12 22:28:18.269463: Training Step 38/59: batchLoss = 3.0183, diffLoss = 14.7386, kgLoss = 0.0882
2025-02-12 22:28:19.188318: Training Step 39/59: batchLoss = 3.5165, diffLoss = 17.1667, kgLoss = 0.1039
2025-02-12 22:28:20.113573: Training Step 40/59: batchLoss = 3.0991, diffLoss = 15.1304, kgLoss = 0.0913
2025-02-12 22:28:21.035818: Training Step 41/59: batchLoss = 3.2432, diffLoss = 15.8650, kgLoss = 0.0877
2025-02-12 22:28:21.964987: Training Step 42/59: batchLoss = 2.8060, diffLoss = 13.6697, kgLoss = 0.0900
2025-02-12 22:28:22.888190: Training Step 43/59: batchLoss = 3.3488, diffLoss = 16.3702, kgLoss = 0.0935
2025-02-12 22:28:23.803442: Training Step 44/59: batchLoss = 2.9304, diffLoss = 14.3136, kgLoss = 0.0846
2025-02-12 22:28:24.718848: Training Step 45/59: batchLoss = 3.2491, diffLoss = 15.8748, kgLoss = 0.0926
2025-02-12 22:28:25.628725: Training Step 46/59: batchLoss = 2.8467, diffLoss = 13.9057, kgLoss = 0.0819
2025-02-12 22:28:26.549000: Training Step 47/59: batchLoss = 3.0053, diffLoss = 14.6731, kgLoss = 0.0883
2025-02-12 22:28:27.462827: Training Step 48/59: batchLoss = 3.6562, diffLoss = 17.8744, kgLoss = 0.1017
2025-02-12 22:28:28.385686: Training Step 49/59: batchLoss = 2.9798, diffLoss = 14.5466, kgLoss = 0.0882
2025-02-12 22:28:29.305513: Training Step 50/59: batchLoss = 3.4280, diffLoss = 16.7692, kgLoss = 0.0927
2025-02-12 22:28:30.228875: Training Step 51/59: batchLoss = 3.1265, diffLoss = 15.2738, kgLoss = 0.0897
2025-02-12 22:28:31.147282: Training Step 52/59: batchLoss = 3.1183, diffLoss = 15.2423, kgLoss = 0.0873
2025-02-12 22:28:32.068544: Training Step 53/59: batchLoss = 3.3885, diffLoss = 16.5743, kgLoss = 0.0920
2025-02-12 22:28:32.988525: Training Step 54/59: batchLoss = 3.0537, diffLoss = 14.9023, kgLoss = 0.0916
2025-02-12 22:28:33.907150: Training Step 55/59: batchLoss = 2.9369, diffLoss = 14.3632, kgLoss = 0.0803
2025-02-12 22:28:34.843018: Training Step 56/59: batchLoss = 3.2748, diffLoss = 16.0144, kgLoss = 0.0899
2025-02-12 22:28:35.685923: Training Step 57/59: batchLoss = 2.9719, diffLoss = 14.5028, kgLoss = 0.0892
2025-02-12 22:28:36.525870: Training Step 58/59: batchLoss = 3.6281, diffLoss = 17.7156, kgLoss = 0.1063
2025-02-12 22:28:36.622883: 
2025-02-12 22:28:36.623432: Epoch 8/1000, Train: epLoss = 0.4543, epDfLoss = 2.2183, epKgLoss = 0.0133  
2025-02-12 22:28:38.095230: Steps 0/47: batch_recall = 27.73, batch_ndcg = 32.47 
2025-02-12 22:28:39.391326: Steps 1/47: batch_recall = 30.57, batch_ndcg = 29.76 
2025-02-12 22:28:40.629991: Steps 2/47: batch_recall = 34.78, batch_ndcg = 34.74 
2025-02-12 22:28:41.875301: Steps 3/47: batch_recall = 37.39, batch_ndcg = 34.34 
2025-02-12 22:28:43.049493: Steps 4/47: batch_recall = 33.30, batch_ndcg = 34.30 
2025-02-12 22:28:44.245903: Steps 5/47: batch_recall = 28.16, batch_ndcg = 27.72 
2025-02-12 22:28:45.425304: Steps 6/47: batch_recall = 34.05, batch_ndcg = 32.96 
2025-02-12 22:28:46.570583: Steps 7/47: batch_recall = 36.58, batch_ndcg = 33.65 
2025-02-12 22:28:47.731661: Steps 8/47: batch_recall = 42.11, batch_ndcg = 38.24 
2025-02-12 22:28:48.859388: Steps 9/47: batch_recall = 36.93, batch_ndcg = 35.81 
2025-02-12 22:28:50.020651: Steps 10/47: batch_recall = 38.65, batch_ndcg = 34.84 
2025-02-12 22:28:51.158590: Steps 11/47: batch_recall = 44.06, batch_ndcg = 37.31 
2025-02-12 22:28:52.306000: Steps 12/47: batch_recall = 42.91, batch_ndcg = 37.58 
2025-02-12 22:28:53.433391: Steps 13/47: batch_recall = 40.66, batch_ndcg = 34.74 
2025-02-12 22:28:54.534427: Steps 14/47: batch_recall = 36.44, batch_ndcg = 32.04 
2025-02-12 22:28:55.625213: Steps 15/47: batch_recall = 50.55, batch_ndcg = 39.81 
2025-02-12 22:28:56.708407: Steps 16/47: batch_recall = 45.07, batch_ndcg = 37.51 
2025-02-12 22:28:57.762900: Steps 17/47: batch_recall = 46.71, batch_ndcg = 38.00 
2025-02-12 22:28:58.830127: Steps 18/47: batch_recall = 44.15, batch_ndcg = 37.82 
2025-02-12 22:28:59.891742: Steps 19/47: batch_recall = 57.49, batch_ndcg = 46.28 
2025-02-12 22:29:00.943462: Steps 20/47: batch_recall = 56.14, batch_ndcg = 47.37 
2025-02-12 22:29:01.994167: Steps 21/47: batch_recall = 61.35, batch_ndcg = 45.62 
2025-02-12 22:29:03.044027: Steps 22/47: batch_recall = 48.75, batch_ndcg = 41.15 
2025-02-12 22:29:04.095797: Steps 23/47: batch_recall = 53.07, batch_ndcg = 40.85 
2025-02-12 22:29:05.159361: Steps 24/47: batch_recall = 56.51, batch_ndcg = 45.77 
2025-02-12 22:29:06.200887: Steps 25/47: batch_recall = 60.23, batch_ndcg = 48.03 
2025-02-12 22:29:07.205280: Steps 26/47: batch_recall = 52.89, batch_ndcg = 43.17 
2025-02-12 22:29:08.224409: Steps 27/47: batch_recall = 50.21, batch_ndcg = 39.00 
2025-02-12 22:29:09.234045: Steps 28/47: batch_recall = 56.67, batch_ndcg = 43.05 
2025-02-12 22:29:10.227458: Steps 29/47: batch_recall = 59.42, batch_ndcg = 44.54 
2025-02-12 22:29:11.225191: Steps 30/47: batch_recall = 65.96, batch_ndcg = 52.36 
2025-02-12 22:29:12.228856: Steps 31/47: batch_recall = 54.62, batch_ndcg = 42.13 
2025-02-12 22:29:13.213536: Steps 32/47: batch_recall = 62.13, batch_ndcg = 52.55 
2025-02-12 22:29:14.206348: Steps 33/47: batch_recall = 67.38, batch_ndcg = 52.33 
2025-02-12 22:29:15.199092: Steps 34/47: batch_recall = 54.11, batch_ndcg = 41.29 
2025-02-12 22:29:16.166038: Steps 35/47: batch_recall = 68.73, batch_ndcg = 51.75 
2025-02-12 22:29:17.140510: Steps 36/47: batch_recall = 70.16, batch_ndcg = 53.14 
2025-02-12 22:29:18.105507: Steps 37/47: batch_recall = 68.00, batch_ndcg = 55.43 
2025-02-12 22:29:19.074028: Steps 38/47: batch_recall = 76.99, batch_ndcg = 57.74 
2025-02-12 22:29:20.035938: Steps 39/47: batch_recall = 74.89, batch_ndcg = 54.09 
2025-02-12 22:29:20.999612: Steps 40/47: batch_recall = 61.61, batch_ndcg = 51.44 
2025-02-12 22:29:21.970789: Steps 41/47: batch_recall = 77.97, batch_ndcg = 58.97 
2025-02-12 22:29:22.928397: Steps 42/47: batch_recall = 76.50, batch_ndcg = 54.74 
2025-02-12 22:29:23.886535: Steps 43/47: batch_recall = 77.83, batch_ndcg = 56.21 
2025-02-12 22:29:24.856924: Steps 44/47: batch_recall = 79.40, batch_ndcg = 57.09 
2025-02-12 22:29:25.797497: Steps 45/47: batch_recall = 89.70, batch_ndcg = 66.97 
2025-02-12 22:29:25.903899: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.69 
2025-02-12 22:29:25.904039: Epoch 8/1000, Test: Recall = 0.1049, NDCG = 0.0852  

2025-02-12 22:29:27.093740: Training Step 0/59: batchLoss = 3.0135, diffLoss = 14.7132, kgLoss = 0.0885
2025-02-12 22:29:28.015865: Training Step 1/59: batchLoss = 2.7064, diffLoss = 13.2022, kgLoss = 0.0825
2025-02-12 22:29:28.933104: Training Step 2/59: batchLoss = 3.0976, diffLoss = 15.1070, kgLoss = 0.0952
2025-02-12 22:29:29.852030: Training Step 3/59: batchLoss = 2.8553, diffLoss = 13.9180, kgLoss = 0.0896
2025-02-12 22:29:30.774661: Training Step 4/59: batchLoss = 3.3159, diffLoss = 16.1541, kgLoss = 0.1063
2025-02-12 22:29:31.691928: Training Step 5/59: batchLoss = 2.9434, diffLoss = 14.3523, kgLoss = 0.0912
2025-02-12 22:29:32.610781: Training Step 6/59: batchLoss = 3.4536, diffLoss = 16.8548, kgLoss = 0.1033
2025-02-12 22:29:33.528768: Training Step 7/59: batchLoss = 2.7046, diffLoss = 13.2003, kgLoss = 0.0807
2025-02-12 22:29:34.452242: Training Step 8/59: batchLoss = 2.9719, diffLoss = 14.5157, kgLoss = 0.0859
2025-02-12 22:29:35.378576: Training Step 9/59: batchLoss = 2.5379, diffLoss = 12.3745, kgLoss = 0.0787
2025-02-12 22:29:36.307177: Training Step 10/59: batchLoss = 2.4588, diffLoss = 11.9768, kgLoss = 0.0793
2025-02-12 22:29:37.233855: Training Step 11/59: batchLoss = 3.0593, diffLoss = 14.9299, kgLoss = 0.0916
2025-02-12 22:29:38.159017: Training Step 12/59: batchLoss = 3.3615, diffLoss = 16.3998, kgLoss = 0.1019
2025-02-12 22:29:39.083699: Training Step 13/59: batchLoss = 3.1185, diffLoss = 15.2129, kgLoss = 0.0949
2025-02-12 22:29:40.015164: Training Step 14/59: batchLoss = 2.8033, diffLoss = 13.6783, kgLoss = 0.0845
2025-02-12 22:29:40.945840: Training Step 15/59: batchLoss = 3.0985, diffLoss = 15.1169, kgLoss = 0.0939
2025-02-12 22:29:41.875231: Training Step 16/59: batchLoss = 3.0631, diffLoss = 14.9090, kgLoss = 0.1016
2025-02-12 22:29:42.803913: Training Step 17/59: batchLoss = 3.1291, diffLoss = 15.2578, kgLoss = 0.0970
2025-02-12 22:29:43.737143: Training Step 18/59: batchLoss = 3.0948, diffLoss = 15.1181, kgLoss = 0.0890
2025-02-12 22:29:44.673627: Training Step 19/59: batchLoss = 3.0347, diffLoss = 14.8055, kgLoss = 0.0920
2025-02-12 22:29:45.608324: Training Step 20/59: batchLoss = 3.0843, diffLoss = 15.0466, kgLoss = 0.0937
2025-02-12 22:29:46.530922: Training Step 21/59: batchLoss = 2.8876, diffLoss = 14.1081, kgLoss = 0.0825
2025-02-12 22:29:47.459878: Training Step 22/59: batchLoss = 3.0274, diffLoss = 14.7675, kgLoss = 0.0924
2025-02-12 22:29:48.379823: Training Step 23/59: batchLoss = 3.0563, diffLoss = 14.9468, kgLoss = 0.0836
2025-02-12 22:29:49.295581: Training Step 24/59: batchLoss = 3.2561, diffLoss = 15.8992, kgLoss = 0.0954
2025-02-12 22:29:50.209321: Training Step 25/59: batchLoss = 3.3562, diffLoss = 16.3712, kgLoss = 0.1025
2025-02-12 22:29:51.133633: Training Step 26/59: batchLoss = 3.1394, diffLoss = 15.3257, kgLoss = 0.0928
2025-02-12 22:29:52.051321: Training Step 27/59: batchLoss = 3.3387, diffLoss = 16.3001, kgLoss = 0.0983
2025-02-12 22:29:52.984164: Training Step 28/59: batchLoss = 2.8799, diffLoss = 14.0625, kgLoss = 0.0842
2025-02-12 22:29:53.897676: Training Step 29/59: batchLoss = 3.4959, diffLoss = 17.0801, kgLoss = 0.0999
2025-02-12 22:29:54.819764: Training Step 30/59: batchLoss = 3.1809, diffLoss = 15.5074, kgLoss = 0.0992
2025-02-12 22:29:55.741804: Training Step 31/59: batchLoss = 3.0176, diffLoss = 14.7241, kgLoss = 0.0910
2025-02-12 22:29:56.660716: Training Step 32/59: batchLoss = 3.0539, diffLoss = 14.9056, kgLoss = 0.0909
2025-02-12 22:29:57.588181: Training Step 33/59: batchLoss = 2.9352, diffLoss = 14.3144, kgLoss = 0.0904
2025-02-12 22:29:58.528592: Training Step 34/59: batchLoss = 3.4941, diffLoss = 17.0667, kgLoss = 0.1009
2025-02-12 22:29:59.463213: Training Step 35/59: batchLoss = 2.7842, diffLoss = 13.6004, kgLoss = 0.0802
2025-02-12 22:30:00.394514: Training Step 36/59: batchLoss = 3.0148, diffLoss = 14.7261, kgLoss = 0.0870
2025-02-12 22:30:01.320458: Training Step 37/59: batchLoss = 3.0384, diffLoss = 14.8349, kgLoss = 0.0893
2025-02-12 22:30:02.254032: Training Step 38/59: batchLoss = 2.9015, diffLoss = 14.1712, kgLoss = 0.0841
2025-02-12 22:30:03.189104: Training Step 39/59: batchLoss = 3.0952, diffLoss = 15.1315, kgLoss = 0.0862
2025-02-12 22:30:04.129481: Training Step 40/59: batchLoss = 2.9269, diffLoss = 14.3042, kgLoss = 0.0826
2025-02-12 22:30:05.060228: Training Step 41/59: batchLoss = 2.7274, diffLoss = 13.3349, kgLoss = 0.0755
2025-02-12 22:30:05.984896: Training Step 42/59: batchLoss = 3.1318, diffLoss = 15.2874, kgLoss = 0.0928
2025-02-12 22:30:06.913223: Training Step 43/59: batchLoss = 2.9977, diffLoss = 14.6421, kgLoss = 0.0866
2025-02-12 22:30:07.853880: Training Step 44/59: batchLoss = 3.1900, diffLoss = 15.5804, kgLoss = 0.0923
2025-02-12 22:30:08.776095: Training Step 45/59: batchLoss = 2.9711, diffLoss = 14.4894, kgLoss = 0.0915
2025-02-12 22:30:09.698608: Training Step 46/59: batchLoss = 3.1216, diffLoss = 15.2476, kgLoss = 0.0901
2025-02-12 22:30:10.619579: Training Step 47/59: batchLoss = 3.2274, diffLoss = 15.7463, kgLoss = 0.0976
2025-02-12 22:30:11.541043: Training Step 48/59: batchLoss = 2.9193, diffLoss = 14.2729, kgLoss = 0.0809
2025-02-12 22:30:12.466341: Training Step 49/59: batchLoss = 3.1322, diffLoss = 15.3228, kgLoss = 0.0845
2025-02-12 22:30:13.386746: Training Step 50/59: batchLoss = 3.1534, diffLoss = 15.3826, kgLoss = 0.0961
2025-02-12 22:30:14.306969: Training Step 51/59: batchLoss = 3.0502, diffLoss = 14.8771, kgLoss = 0.0935
2025-02-12 22:30:15.225282: Training Step 52/59: batchLoss = 2.6744, diffLoss = 13.0663, kgLoss = 0.0765
2025-02-12 22:30:16.143513: Training Step 53/59: batchLoss = 3.4048, diffLoss = 16.6344, kgLoss = 0.0974
2025-02-12 22:30:17.061407: Training Step 54/59: batchLoss = 2.9479, diffLoss = 14.4112, kgLoss = 0.0821
2025-02-12 22:30:17.979971: Training Step 55/59: batchLoss = 3.1214, diffLoss = 15.2666, kgLoss = 0.0851
2025-02-12 22:30:18.896054: Training Step 56/59: batchLoss = 3.0508, diffLoss = 14.9155, kgLoss = 0.0846
2025-02-12 22:30:19.739382: Training Step 57/59: batchLoss = 2.8588, diffLoss = 13.9715, kgLoss = 0.0806
2025-02-12 22:30:20.585146: Training Step 58/59: batchLoss = 2.9811, diffLoss = 14.5756, kgLoss = 0.0824
2025-02-12 22:30:20.679556: 
2025-02-12 22:30:20.679911: Epoch 9/1000, Train: epLoss = 0.4486, epDfLoss = 2.1900, epKgLoss = 0.0133  
2025-02-12 22:30:22.163766: Steps 0/47: batch_recall = 28.03, batch_ndcg = 32.28 
2025-02-12 22:30:23.465401: Steps 1/47: batch_recall = 30.37, batch_ndcg = 29.63 
2025-02-12 22:30:24.721635: Steps 2/47: batch_recall = 33.81, batch_ndcg = 35.40 
2025-02-12 22:30:25.981131: Steps 3/47: batch_recall = 38.63, batch_ndcg = 35.58 
2025-02-12 22:30:27.180035: Steps 4/47: batch_recall = 33.12, batch_ndcg = 34.76 
2025-02-12 22:30:28.402013: Steps 5/47: batch_recall = 27.72, batch_ndcg = 27.20 
2025-02-12 22:30:29.599684: Steps 6/47: batch_recall = 34.49, batch_ndcg = 32.79 
2025-02-12 22:30:30.766421: Steps 7/47: batch_recall = 36.86, batch_ndcg = 33.69 
2025-02-12 22:30:31.936271: Steps 8/47: batch_recall = 41.00, batch_ndcg = 37.83 
2025-02-12 22:30:33.059911: Steps 9/47: batch_recall = 38.08, batch_ndcg = 35.95 
2025-02-12 22:30:34.209136: Steps 10/47: batch_recall = 39.82, batch_ndcg = 35.20 
2025-02-12 22:30:35.334555: Steps 11/47: batch_recall = 45.07, batch_ndcg = 38.44 
2025-02-12 22:30:36.476635: Steps 12/47: batch_recall = 42.97, batch_ndcg = 37.89 
2025-02-12 22:30:37.602979: Steps 13/47: batch_recall = 40.21, batch_ndcg = 34.65 
2025-02-12 22:30:38.684161: Steps 14/47: batch_recall = 37.38, batch_ndcg = 32.47 
2025-02-12 22:30:39.761425: Steps 15/47: batch_recall = 50.33, batch_ndcg = 39.99 
2025-02-12 22:30:40.835309: Steps 16/47: batch_recall = 41.87, batch_ndcg = 35.86 
2025-02-12 22:30:41.887897: Steps 17/47: batch_recall = 48.72, batch_ndcg = 39.00 
2025-02-12 22:30:42.968777: Steps 18/47: batch_recall = 43.14, batch_ndcg = 37.18 
2025-02-12 22:30:44.038694: Steps 19/47: batch_recall = 57.53, batch_ndcg = 46.84 
2025-02-12 22:30:45.093812: Steps 20/47: batch_recall = 58.41, batch_ndcg = 48.03 
2025-02-12 22:30:46.139029: Steps 21/47: batch_recall = 59.08, batch_ndcg = 45.44 
2025-02-12 22:30:47.196288: Steps 22/47: batch_recall = 50.24, batch_ndcg = 41.44 
2025-02-12 22:30:48.253303: Steps 23/47: batch_recall = 53.85, batch_ndcg = 40.59 
2025-02-12 22:30:49.322342: Steps 24/47: batch_recall = 57.86, batch_ndcg = 47.01 
2025-02-12 22:30:50.370920: Steps 25/47: batch_recall = 61.65, batch_ndcg = 48.39 
2025-02-12 22:30:51.382966: Steps 26/47: batch_recall = 53.52, batch_ndcg = 44.17 
2025-02-12 22:30:52.419242: Steps 27/47: batch_recall = 52.12, batch_ndcg = 39.96 
2025-02-12 22:30:53.444040: Steps 28/47: batch_recall = 53.98, batch_ndcg = 42.13 
2025-02-12 22:30:54.458050: Steps 29/47: batch_recall = 60.18, batch_ndcg = 45.47 
2025-02-12 22:30:55.455417: Steps 30/47: batch_recall = 67.54, batch_ndcg = 54.24 
2025-02-12 22:30:56.479452: Steps 31/47: batch_recall = 53.95, batch_ndcg = 41.44 
2025-02-12 22:30:57.482837: Steps 32/47: batch_recall = 63.60, batch_ndcg = 52.33 
2025-02-12 22:30:58.488844: Steps 33/47: batch_recall = 66.97, batch_ndcg = 51.77 
2025-02-12 22:30:59.499143: Steps 34/47: batch_recall = 53.43, batch_ndcg = 41.86 
2025-02-12 22:31:00.481633: Steps 35/47: batch_recall = 69.58, batch_ndcg = 52.89 
2025-02-12 22:31:01.466611: Steps 36/47: batch_recall = 72.83, batch_ndcg = 55.25 
2025-02-12 22:31:02.453672: Steps 37/47: batch_recall = 69.67, batch_ndcg = 55.11 
2025-02-12 22:31:03.431433: Steps 38/47: batch_recall = 77.48, batch_ndcg = 57.56 
2025-02-12 22:31:04.406580: Steps 39/47: batch_recall = 77.90, batch_ndcg = 56.43 
2025-02-12 22:31:05.367539: Steps 40/47: batch_recall = 62.49, batch_ndcg = 51.74 
2025-02-12 22:31:06.344774: Steps 41/47: batch_recall = 77.12, batch_ndcg = 60.39 
2025-02-12 22:31:07.307355: Steps 42/47: batch_recall = 77.77, batch_ndcg = 55.77 
2025-02-12 22:31:08.279857: Steps 43/47: batch_recall = 78.71, batch_ndcg = 56.79 
2025-02-12 22:31:09.242687: Steps 44/47: batch_recall = 80.85, batch_ndcg = 58.59 
2025-02-12 22:31:10.192113: Steps 45/47: batch_recall = 90.18, batch_ndcg = 68.47 
2025-02-12 22:31:10.299331: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.57 
2025-02-12 22:31:10.299429: Epoch 9/1000, Test: Recall = 0.1058, NDCG = 0.0860  

2025-02-12 22:31:11.481998: Training Step 0/59: batchLoss = 2.9831, diffLoss = 14.5162, kgLoss = 0.0998
2025-02-12 22:31:12.398831: Training Step 1/59: batchLoss = 2.9395, diffLoss = 14.3366, kgLoss = 0.0903
2025-02-12 22:31:13.309949: Training Step 2/59: batchLoss = 3.2194, diffLoss = 15.7202, kgLoss = 0.0942
2025-02-12 22:31:14.224031: Training Step 3/59: batchLoss = 2.6758, diffLoss = 13.0342, kgLoss = 0.0862
2025-02-12 22:31:15.138918: Training Step 4/59: batchLoss = 2.7810, diffLoss = 13.5589, kgLoss = 0.0865
2025-02-12 22:31:16.053761: Training Step 5/59: batchLoss = 2.9350, diffLoss = 14.3160, kgLoss = 0.0898
2025-02-12 22:31:16.964159: Training Step 6/59: batchLoss = 2.8303, diffLoss = 13.8185, kgLoss = 0.0833
2025-02-12 22:31:17.879288: Training Step 7/59: batchLoss = 2.5648, diffLoss = 12.5114, kgLoss = 0.0782
2025-02-12 22:31:18.789061: Training Step 8/59: batchLoss = 2.7399, diffLoss = 13.3870, kgLoss = 0.0781
2025-02-12 22:31:19.698749: Training Step 9/59: batchLoss = 3.0588, diffLoss = 14.9177, kgLoss = 0.0941
2025-02-12 22:31:20.609177: Training Step 10/59: batchLoss = 2.9531, diffLoss = 14.4269, kgLoss = 0.0847
2025-02-12 22:31:21.527337: Training Step 11/59: batchLoss = 2.8212, diffLoss = 13.7779, kgLoss = 0.0821
2025-02-12 22:31:22.448556: Training Step 12/59: batchLoss = 2.7450, diffLoss = 13.3719, kgLoss = 0.0883
2025-02-12 22:31:23.382885: Training Step 13/59: batchLoss = 3.2412, diffLoss = 15.7837, kgLoss = 0.1056
2025-02-12 22:31:24.310802: Training Step 14/59: batchLoss = 2.5227, diffLoss = 12.2939, kgLoss = 0.0800
2025-02-12 22:31:25.230644: Training Step 15/59: batchLoss = 2.8163, diffLoss = 13.7218, kgLoss = 0.0900
2025-02-12 22:31:26.146893: Training Step 16/59: batchLoss = 2.8478, diffLoss = 13.8796, kgLoss = 0.0898
2025-02-12 22:31:27.060290: Training Step 17/59: batchLoss = 3.1031, diffLoss = 15.1472, kgLoss = 0.0920
2025-02-12 22:31:27.982995: Training Step 18/59: batchLoss = 2.9755, diffLoss = 14.5385, kgLoss = 0.0847
2025-02-12 22:31:28.905350: Training Step 19/59: batchLoss = 2.4511, diffLoss = 11.9643, kgLoss = 0.0728
2025-02-12 22:31:29.834043: Training Step 20/59: batchLoss = 2.8785, diffLoss = 14.0495, kgLoss = 0.0857
2025-02-12 22:31:30.759952: Training Step 21/59: batchLoss = 2.8108, diffLoss = 13.7209, kgLoss = 0.0833
2025-02-12 22:31:31.685249: Training Step 22/59: batchLoss = 3.1179, diffLoss = 15.2191, kgLoss = 0.0925
2025-02-12 22:31:32.615615: Training Step 23/59: batchLoss = 3.0047, diffLoss = 14.6669, kgLoss = 0.0892
2025-02-12 22:31:33.537959: Training Step 24/59: batchLoss = 2.7105, diffLoss = 13.2161, kgLoss = 0.0842
2025-02-12 22:31:34.464330: Training Step 25/59: batchLoss = 2.9226, diffLoss = 14.2644, kgLoss = 0.0871
2025-02-12 22:31:35.382784: Training Step 26/59: batchLoss = 2.5945, diffLoss = 12.6501, kgLoss = 0.0806
2025-02-12 22:31:36.296229: Training Step 27/59: batchLoss = 2.6390, diffLoss = 12.8993, kgLoss = 0.0740
2025-02-12 22:31:37.214115: Training Step 28/59: batchLoss = 2.9461, diffLoss = 14.3821, kgLoss = 0.0871
2025-02-12 22:31:38.127060: Training Step 29/59: batchLoss = 3.1464, diffLoss = 15.3631, kgLoss = 0.0922
2025-02-12 22:31:39.040402: Training Step 30/59: batchLoss = 3.3443, diffLoss = 16.3312, kgLoss = 0.0976
2025-02-12 22:31:39.960573: Training Step 31/59: batchLoss = 2.9913, diffLoss = 14.5910, kgLoss = 0.0914
2025-02-12 22:31:40.882355: Training Step 32/59: batchLoss = 3.3567, diffLoss = 16.3996, kgLoss = 0.0960
2025-02-12 22:31:41.810196: Training Step 33/59: batchLoss = 2.8946, diffLoss = 14.1144, kgLoss = 0.0896
2025-02-12 22:31:42.735807: Training Step 34/59: batchLoss = 3.0263, diffLoss = 14.7644, kgLoss = 0.0918
2025-02-12 22:31:43.664341: Training Step 35/59: batchLoss = 2.9115, diffLoss = 14.2174, kgLoss = 0.0850
2025-02-12 22:31:44.594670: Training Step 36/59: batchLoss = 3.2214, diffLoss = 15.7096, kgLoss = 0.0994
2025-02-12 22:31:45.527700: Training Step 37/59: batchLoss = 3.1263, diffLoss = 15.2587, kgLoss = 0.0932
2025-02-12 22:31:46.454103: Training Step 38/59: batchLoss = 3.4065, diffLoss = 16.6319, kgLoss = 0.1002
2025-02-12 22:31:47.387890: Training Step 39/59: batchLoss = 3.5506, diffLoss = 17.3616, kgLoss = 0.0979
2025-02-12 22:31:48.319149: Training Step 40/59: batchLoss = 2.9595, diffLoss = 14.4612, kgLoss = 0.0840
2025-02-12 22:31:49.246319: Training Step 41/59: batchLoss = 3.4214, diffLoss = 16.6798, kgLoss = 0.1068
2025-02-12 22:31:50.170507: Training Step 42/59: batchLoss = 3.2352, diffLoss = 15.8017, kgLoss = 0.0936
2025-02-12 22:31:51.105240: Training Step 43/59: batchLoss = 3.4511, diffLoss = 16.8637, kgLoss = 0.0980
2025-02-12 22:31:52.030447: Training Step 44/59: batchLoss = 2.9952, diffLoss = 14.6380, kgLoss = 0.0844
2025-02-12 22:31:52.948322: Training Step 45/59: batchLoss = 3.2822, diffLoss = 16.0565, kgLoss = 0.0886
2025-02-12 22:31:53.870915: Training Step 46/59: batchLoss = 3.2639, diffLoss = 15.9454, kgLoss = 0.0935
2025-02-12 22:31:54.788092: Training Step 47/59: batchLoss = 3.5874, diffLoss = 17.5147, kgLoss = 0.1056
2025-02-12 22:31:55.704842: Training Step 48/59: batchLoss = 3.1331, diffLoss = 15.3002, kgLoss = 0.0913
2025-02-12 22:31:56.623767: Training Step 49/59: batchLoss = 2.8621, diffLoss = 13.9682, kgLoss = 0.0855
2025-02-12 22:31:57.544813: Training Step 50/59: batchLoss = 3.2419, diffLoss = 15.8424, kgLoss = 0.0918
2025-02-12 22:31:58.468165: Training Step 51/59: batchLoss = 3.6995, diffLoss = 18.0953, kgLoss = 0.1006
2025-02-12 22:31:59.388994: Training Step 52/59: batchLoss = 2.9926, diffLoss = 14.6234, kgLoss = 0.0849
2025-02-12 22:32:00.310947: Training Step 53/59: batchLoss = 2.7266, diffLoss = 13.2956, kgLoss = 0.0844
2025-02-12 22:32:01.227452: Training Step 54/59: batchLoss = 2.7501, diffLoss = 13.4405, kgLoss = 0.0775
2025-02-12 22:32:02.150502: Training Step 55/59: batchLoss = 3.3179, diffLoss = 16.1851, kgLoss = 0.1011
2025-02-12 22:32:03.076878: Training Step 56/59: batchLoss = 3.0942, diffLoss = 15.1132, kgLoss = 0.0894
2025-02-12 22:32:03.918184: Training Step 57/59: batchLoss = 3.3250, diffLoss = 16.2583, kgLoss = 0.0917
2025-02-12 22:32:04.762766: Training Step 58/59: batchLoss = 3.2624, diffLoss = 15.9435, kgLoss = 0.0921
2025-02-12 22:32:04.862690: 
2025-02-12 22:32:04.863046: Epoch 10/1000, Train: epLoss = 0.4460, epDfLoss = 2.1772, epKgLoss = 0.0132  
2025-02-12 22:32:06.355323: Steps 0/47: batch_recall = 28.65, batch_ndcg = 32.86 
2025-02-12 22:32:07.657598: Steps 1/47: batch_recall = 30.63, batch_ndcg = 30.39 
2025-02-12 22:32:08.905852: Steps 2/47: batch_recall = 33.50, batch_ndcg = 35.07 
2025-02-12 22:32:10.167033: Steps 3/47: batch_recall = 38.80, batch_ndcg = 36.06 
2025-02-12 22:32:11.363432: Steps 4/47: batch_recall = 33.05, batch_ndcg = 35.06 
2025-02-12 22:32:12.579641: Steps 5/47: batch_recall = 26.65, batch_ndcg = 27.67 
2025-02-12 22:32:13.772703: Steps 6/47: batch_recall = 33.87, batch_ndcg = 32.61 
2025-02-12 22:32:14.943649: Steps 7/47: batch_recall = 37.41, batch_ndcg = 34.45 
2025-02-12 22:32:16.117662: Steps 8/47: batch_recall = 40.37, batch_ndcg = 38.31 
2025-02-12 22:32:17.250131: Steps 9/47: batch_recall = 38.41, batch_ndcg = 37.26 
2025-02-12 22:32:18.404568: Steps 10/47: batch_recall = 39.72, batch_ndcg = 35.37 
2025-02-12 22:32:19.530020: Steps 11/47: batch_recall = 44.54, batch_ndcg = 37.40 
2025-02-12 22:32:20.658582: Steps 12/47: batch_recall = 42.34, batch_ndcg = 38.20 
2025-02-12 22:32:21.771809: Steps 13/47: batch_recall = 41.61, batch_ndcg = 35.91 
2025-02-12 22:32:22.849113: Steps 14/47: batch_recall = 36.73, batch_ndcg = 32.53 
2025-02-12 22:32:23.938382: Steps 15/47: batch_recall = 49.30, batch_ndcg = 40.48 
2025-02-12 22:32:25.013377: Steps 16/47: batch_recall = 43.74, batch_ndcg = 37.94 
2025-02-12 22:32:26.071699: Steps 17/47: batch_recall = 49.21, batch_ndcg = 39.78 
2025-02-12 22:32:27.157205: Steps 18/47: batch_recall = 44.33, batch_ndcg = 38.93 
2025-02-12 22:32:28.234348: Steps 19/47: batch_recall = 56.57, batch_ndcg = 46.39 
2025-02-12 22:32:29.290999: Steps 20/47: batch_recall = 57.92, batch_ndcg = 48.45 
2025-02-12 22:32:30.344291: Steps 21/47: batch_recall = 59.22, batch_ndcg = 45.74 
2025-02-12 22:32:31.402346: Steps 22/47: batch_recall = 49.53, batch_ndcg = 42.52 
2025-02-12 22:32:32.450894: Steps 23/47: batch_recall = 54.14, batch_ndcg = 42.25 
2025-02-12 22:32:33.517455: Steps 24/47: batch_recall = 56.74, batch_ndcg = 45.90 
2025-02-12 22:32:34.561440: Steps 25/47: batch_recall = 61.66, batch_ndcg = 49.08 
2025-02-12 22:32:35.574577: Steps 26/47: batch_recall = 54.28, batch_ndcg = 44.75 
2025-02-12 22:32:36.599321: Steps 27/47: batch_recall = 51.89, batch_ndcg = 41.07 
2025-02-12 22:32:37.605275: Steps 28/47: batch_recall = 56.32, batch_ndcg = 43.73 
2025-02-12 22:32:38.609053: Steps 29/47: batch_recall = 61.21, batch_ndcg = 46.40 
2025-02-12 22:32:39.609282: Steps 30/47: batch_recall = 69.45, batch_ndcg = 54.78 
2025-02-12 22:32:40.635391: Steps 31/47: batch_recall = 55.02, batch_ndcg = 43.28 
2025-02-12 22:32:41.639046: Steps 32/47: batch_recall = 62.06, batch_ndcg = 52.66 
2025-02-12 22:32:42.649047: Steps 33/47: batch_recall = 68.31, batch_ndcg = 52.51 
2025-02-12 22:32:43.655334: Steps 34/47: batch_recall = 57.25, batch_ndcg = 44.01 
2025-02-12 22:32:44.630281: Steps 35/47: batch_recall = 69.33, batch_ndcg = 52.27 
2025-02-12 22:32:45.620620: Steps 36/47: batch_recall = 71.89, batch_ndcg = 54.56 
2025-02-12 22:32:46.602298: Steps 37/47: batch_recall = 69.98, batch_ndcg = 56.90 
2025-02-12 22:32:47.580641: Steps 38/47: batch_recall = 77.29, batch_ndcg = 59.23 
2025-02-12 22:32:48.555657: Steps 39/47: batch_recall = 75.72, batch_ndcg = 55.23 
2025-02-12 22:32:49.516445: Steps 40/47: batch_recall = 65.00, batch_ndcg = 52.47 
2025-02-12 22:32:50.475046: Steps 41/47: batch_recall = 76.35, batch_ndcg = 58.46 
2025-02-12 22:32:51.442683: Steps 42/47: batch_recall = 78.76, batch_ndcg = 57.17 
2025-02-12 22:32:52.403335: Steps 43/47: batch_recall = 77.64, batch_ndcg = 56.60 
2025-02-12 22:32:53.373832: Steps 44/47: batch_recall = 79.13, batch_ndcg = 57.79 
2025-02-12 22:32:54.328645: Steps 45/47: batch_recall = 90.71, batch_ndcg = 67.38 
2025-02-12 22:32:54.432542: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.46 
2025-02-12 22:32:54.432675: Epoch 10/1000, Test: Recall = 0.1060, NDCG = 0.0870  

2025-02-12 22:32:55.616651: Training Step 0/59: batchLoss = 2.7152, diffLoss = 13.2681, kgLoss = 0.0770
2025-02-12 22:32:56.535090: Training Step 1/59: batchLoss = 2.9231, diffLoss = 14.2527, kgLoss = 0.0907
2025-02-12 22:32:57.459046: Training Step 2/59: batchLoss = 3.1222, diffLoss = 15.2270, kgLoss = 0.0960
2025-02-12 22:32:58.379767: Training Step 3/59: batchLoss = 3.2739, diffLoss = 15.9139, kgLoss = 0.1139
2025-02-12 22:32:59.305037: Training Step 4/59: batchLoss = 2.9945, diffLoss = 14.5852, kgLoss = 0.0968
2025-02-12 22:33:00.225275: Training Step 5/59: batchLoss = 2.7009, diffLoss = 13.1935, kgLoss = 0.0777
2025-02-12 22:33:01.153164: Training Step 6/59: batchLoss = 2.7781, diffLoss = 13.5290, kgLoss = 0.0904
2025-02-12 22:33:02.078209: Training Step 7/59: batchLoss = 2.9806, diffLoss = 14.5500, kgLoss = 0.0883
2025-02-12 22:33:03.008830: Training Step 8/59: batchLoss = 2.9300, diffLoss = 14.2736, kgLoss = 0.0940
2025-02-12 22:33:03.930072: Training Step 9/59: batchLoss = 2.8977, diffLoss = 14.1301, kgLoss = 0.0895
2025-02-12 22:33:04.853445: Training Step 10/59: batchLoss = 3.3745, diffLoss = 16.4586, kgLoss = 0.1034
2025-02-12 22:33:05.781575: Training Step 11/59: batchLoss = 3.0147, diffLoss = 14.7124, kgLoss = 0.0903
2025-02-12 22:33:06.699543: Training Step 12/59: batchLoss = 2.6861, diffLoss = 13.1293, kgLoss = 0.0753
2025-02-12 22:33:07.627832: Training Step 13/59: batchLoss = 3.2943, diffLoss = 16.0733, kgLoss = 0.0996
2025-02-12 22:33:08.551016: Training Step 14/59: batchLoss = 2.9750, diffLoss = 14.5184, kgLoss = 0.0891
2025-02-12 22:33:09.469337: Training Step 15/59: batchLoss = 3.0243, diffLoss = 14.7326, kgLoss = 0.0972
2025-02-12 22:33:10.396465: Training Step 16/59: batchLoss = 2.8958, diffLoss = 14.1673, kgLoss = 0.0779
2025-02-12 22:33:11.319967: Training Step 17/59: batchLoss = 2.9092, diffLoss = 14.2249, kgLoss = 0.0803
2025-02-12 22:33:12.242872: Training Step 18/59: batchLoss = 3.3807, diffLoss = 16.5108, kgLoss = 0.0981
2025-02-12 22:33:13.166516: Training Step 19/59: batchLoss = 3.1496, diffLoss = 15.3966, kgLoss = 0.0878
2025-02-12 22:33:14.090981: Training Step 20/59: batchLoss = 3.1079, diffLoss = 15.1604, kgLoss = 0.0948
2025-02-12 22:33:15.019960: Training Step 21/59: batchLoss = 2.8798, diffLoss = 14.0593, kgLoss = 0.0850
2025-02-12 22:33:15.948442: Training Step 22/59: batchLoss = 2.7746, diffLoss = 13.5555, kgLoss = 0.0794
2025-02-12 22:33:16.870478: Training Step 23/59: batchLoss = 2.8083, diffLoss = 13.7144, kgLoss = 0.0817
2025-02-12 22:33:17.794343: Training Step 24/59: batchLoss = 2.8649, diffLoss = 13.9686, kgLoss = 0.0890
2025-02-12 22:33:18.716617: Training Step 25/59: batchLoss = 2.7828, diffLoss = 13.5976, kgLoss = 0.0791
2025-02-12 22:33:19.639703: Training Step 26/59: batchLoss = 3.3185, diffLoss = 16.2140, kgLoss = 0.0946
2025-02-12 22:33:20.566598: Training Step 27/59: batchLoss = 3.2405, diffLoss = 15.7829, kgLoss = 0.1049
2025-02-12 22:33:21.489480: Training Step 28/59: batchLoss = 3.1146, diffLoss = 15.2251, kgLoss = 0.0869
2025-02-12 22:33:22.408786: Training Step 29/59: batchLoss = 2.7275, diffLoss = 13.3045, kgLoss = 0.0833
2025-02-12 22:33:23.325015: Training Step 30/59: batchLoss = 3.1142, diffLoss = 15.1908, kgLoss = 0.0950
2025-02-12 22:33:24.242242: Training Step 31/59: batchLoss = 2.9825, diffLoss = 14.5712, kgLoss = 0.0853
2025-02-12 22:33:25.162190: Training Step 32/59: batchLoss = 3.0109, diffLoss = 14.7087, kgLoss = 0.0864
2025-02-12 22:33:26.081614: Training Step 33/59: batchLoss = 3.5472, diffLoss = 17.3279, kgLoss = 0.1020
2025-02-12 22:33:27.021747: Training Step 34/59: batchLoss = 3.2760, diffLoss = 16.0117, kgLoss = 0.0921
2025-02-12 22:33:27.944385: Training Step 35/59: batchLoss = 2.7787, diffLoss = 13.5750, kgLoss = 0.0796
2025-02-12 22:33:28.867679: Training Step 36/59: batchLoss = 3.2869, diffLoss = 16.0608, kgLoss = 0.0934
2025-02-12 22:33:29.799925: Training Step 37/59: batchLoss = 3.1441, diffLoss = 15.3681, kgLoss = 0.0881
2025-02-12 22:33:30.724874: Training Step 38/59: batchLoss = 3.3382, diffLoss = 16.3419, kgLoss = 0.0873
2025-02-12 22:33:31.652625: Training Step 39/59: batchLoss = 3.0130, diffLoss = 14.7192, kgLoss = 0.0864
2025-02-12 22:33:32.582877: Training Step 40/59: batchLoss = 2.9966, diffLoss = 14.6295, kgLoss = 0.0884
2025-02-12 22:33:33.515667: Training Step 41/59: batchLoss = 3.2288, diffLoss = 15.7635, kgLoss = 0.0951
2025-02-12 22:33:34.440978: Training Step 42/59: batchLoss = 3.0103, diffLoss = 14.6776, kgLoss = 0.0935
2025-02-12 22:33:35.381312: Training Step 43/59: batchLoss = 2.9101, diffLoss = 14.2102, kgLoss = 0.0851
2025-02-12 22:33:36.310570: Training Step 44/59: batchLoss = 3.1179, diffLoss = 15.2158, kgLoss = 0.0934
2025-02-12 22:33:37.238693: Training Step 45/59: batchLoss = 3.1503, diffLoss = 15.3820, kgLoss = 0.0924
2025-02-12 22:33:38.160403: Training Step 46/59: batchLoss = 3.2269, diffLoss = 15.7796, kgLoss = 0.0887
2025-02-12 22:33:39.085580: Training Step 47/59: batchLoss = 3.1377, diffLoss = 15.3322, kgLoss = 0.0891
2025-02-12 22:33:40.009449: Training Step 48/59: batchLoss = 3.2374, diffLoss = 15.8276, kgLoss = 0.0898
2025-02-12 22:33:40.934128: Training Step 49/59: batchLoss = 3.1277, diffLoss = 15.2685, kgLoss = 0.0925
2025-02-12 22:33:41.861630: Training Step 50/59: batchLoss = 3.1708, diffLoss = 15.4827, kgLoss = 0.0928
2025-02-12 22:33:42.785136: Training Step 51/59: batchLoss = 2.9673, diffLoss = 14.5110, kgLoss = 0.0814
2025-02-12 22:33:43.704529: Training Step 52/59: batchLoss = 2.9319, diffLoss = 14.3318, kgLoss = 0.0819
2025-02-12 22:33:44.624530: Training Step 53/59: batchLoss = 3.1604, diffLoss = 15.4480, kgLoss = 0.0885
2025-02-12 22:33:45.544963: Training Step 54/59: batchLoss = 3.3497, diffLoss = 16.3682, kgLoss = 0.0951
2025-02-12 22:33:46.469378: Training Step 55/59: batchLoss = 2.8570, diffLoss = 13.9672, kgLoss = 0.0795
2025-02-12 22:33:47.381939: Training Step 56/59: batchLoss = 3.1013, diffLoss = 15.1540, kgLoss = 0.0881
2025-02-12 22:33:48.222376: Training Step 57/59: batchLoss = 2.7823, diffLoss = 13.5777, kgLoss = 0.0834
2025-02-12 22:33:49.074188: Training Step 58/59: batchLoss = 2.9515, diffLoss = 14.3910, kgLoss = 0.0916
2025-02-12 22:33:49.175277: 
2025-02-12 22:33:49.175614: Epoch 11/1000, Train: epLoss = 0.4489, epDfLoss = 2.1916, epKgLoss = 0.0132  
2025-02-12 22:33:50.646949: Steps 0/47: batch_recall = 28.97, batch_ndcg = 33.91 
2025-02-12 22:33:51.950870: Steps 1/47: batch_recall = 31.38, batch_ndcg = 31.33 
2025-02-12 22:33:53.210235: Steps 2/47: batch_recall = 34.20, batch_ndcg = 36.33 
2025-02-12 22:33:54.477258: Steps 3/47: batch_recall = 38.73, batch_ndcg = 36.41 
2025-02-12 22:33:55.682353: Steps 4/47: batch_recall = 33.91, batch_ndcg = 34.83 
2025-02-12 22:33:56.886732: Steps 5/47: batch_recall = 26.82, batch_ndcg = 28.40 
2025-02-12 22:33:58.083842: Steps 6/47: batch_recall = 34.33, batch_ndcg = 32.96 
2025-02-12 22:33:59.259141: Steps 7/47: batch_recall = 36.61, batch_ndcg = 34.18 
2025-02-12 22:34:00.433489: Steps 8/47: batch_recall = 42.26, batch_ndcg = 39.12 
2025-02-12 22:34:01.569620: Steps 9/47: batch_recall = 39.20, batch_ndcg = 38.19 
2025-02-12 22:34:02.733671: Steps 10/47: batch_recall = 40.62, batch_ndcg = 36.24 
2025-02-12 22:34:03.877925: Steps 11/47: batch_recall = 46.32, batch_ndcg = 38.98 
2025-02-12 22:34:05.007318: Steps 12/47: batch_recall = 45.25, batch_ndcg = 40.26 
2025-02-12 22:34:06.128194: Steps 13/47: batch_recall = 41.96, batch_ndcg = 36.13 
2025-02-12 22:34:07.209393: Steps 14/47: batch_recall = 34.64, batch_ndcg = 31.33 
2025-02-12 22:34:08.287954: Steps 15/47: batch_recall = 49.88, batch_ndcg = 40.50 
2025-02-12 22:34:09.361520: Steps 16/47: batch_recall = 44.56, batch_ndcg = 37.75 
2025-02-12 22:34:10.406375: Steps 17/47: batch_recall = 49.73, batch_ndcg = 40.20 
2025-02-12 22:34:11.487459: Steps 18/47: batch_recall = 44.44, batch_ndcg = 38.73 
2025-02-12 22:34:12.555266: Steps 19/47: batch_recall = 57.21, batch_ndcg = 46.88 
2025-02-12 22:34:13.599362: Steps 20/47: batch_recall = 59.49, batch_ndcg = 49.57 
2025-02-12 22:34:14.652741: Steps 21/47: batch_recall = 59.90, batch_ndcg = 46.39 
2025-02-12 22:34:15.707287: Steps 22/47: batch_recall = 49.54, batch_ndcg = 42.54 
2025-02-12 22:34:16.763816: Steps 23/47: batch_recall = 53.72, batch_ndcg = 41.66 
2025-02-12 22:34:17.826310: Steps 24/47: batch_recall = 57.65, batch_ndcg = 46.58 
2025-02-12 22:34:18.878405: Steps 25/47: batch_recall = 62.07, batch_ndcg = 49.04 
2025-02-12 22:34:19.898352: Steps 26/47: batch_recall = 54.17, batch_ndcg = 44.53 
2025-02-12 22:34:20.926706: Steps 27/47: batch_recall = 51.57, batch_ndcg = 41.40 
2025-02-12 22:34:21.943834: Steps 28/47: batch_recall = 57.74, batch_ndcg = 44.55 
2025-02-12 22:34:22.959492: Steps 29/47: batch_recall = 61.38, batch_ndcg = 46.73 
2025-02-12 22:34:23.963163: Steps 30/47: batch_recall = 69.68, batch_ndcg = 55.55 
2025-02-12 22:34:24.990790: Steps 31/47: batch_recall = 54.44, batch_ndcg = 42.20 
2025-02-12 22:34:25.999630: Steps 32/47: batch_recall = 63.17, batch_ndcg = 54.35 
2025-02-12 22:34:27.023249: Steps 33/47: batch_recall = 69.40, batch_ndcg = 54.24 
2025-02-12 22:34:28.025834: Steps 34/47: batch_recall = 55.38, batch_ndcg = 43.34 
2025-02-12 22:34:29.005874: Steps 35/47: batch_recall = 70.00, batch_ndcg = 52.07 
2025-02-12 22:34:29.987151: Steps 36/47: batch_recall = 71.95, batch_ndcg = 54.91 
2025-02-12 22:34:30.955407: Steps 37/47: batch_recall = 72.96, batch_ndcg = 57.84 
2025-02-12 22:34:31.926329: Steps 38/47: batch_recall = 77.39, batch_ndcg = 60.02 
2025-02-12 22:34:32.899112: Steps 39/47: batch_recall = 76.50, batch_ndcg = 55.12 
2025-02-12 22:34:33.868356: Steps 40/47: batch_recall = 65.39, batch_ndcg = 53.76 
2025-02-12 22:34:34.830510: Steps 41/47: batch_recall = 77.46, batch_ndcg = 58.75 
2025-02-12 22:34:35.788045: Steps 42/47: batch_recall = 78.28, batch_ndcg = 57.03 
2025-02-12 22:34:36.750524: Steps 43/47: batch_recall = 80.49, batch_ndcg = 57.33 
2025-02-12 22:34:37.710913: Steps 44/47: batch_recall = 81.47, batch_ndcg = 59.07 
2025-02-12 22:34:38.649666: Steps 45/47: batch_recall = 87.64, batch_ndcg = 65.79 
2025-02-12 22:34:38.755109: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.73 
2025-02-12 22:34:38.755231: Epoch 11/1000, Test: Recall = 0.1070, NDCG = 0.0878  

2025-02-12 22:34:39.948548: Training Step 0/59: batchLoss = 2.9095, diffLoss = 14.1965, kgLoss = 0.0877
2025-02-12 22:34:40.867867: Training Step 1/59: batchLoss = 2.6333, diffLoss = 12.8363, kgLoss = 0.0825
2025-02-12 22:34:41.790538: Training Step 2/59: batchLoss = 3.0420, diffLoss = 14.8178, kgLoss = 0.0981
2025-02-12 22:34:42.716600: Training Step 3/59: batchLoss = 2.7529, diffLoss = 13.4324, kgLoss = 0.0830
2025-02-12 22:34:43.642255: Training Step 4/59: batchLoss = 2.9918, diffLoss = 14.6095, kgLoss = 0.0874
2025-02-12 22:34:44.567700: Training Step 5/59: batchLoss = 2.8946, diffLoss = 14.0991, kgLoss = 0.0935
2025-02-12 22:34:45.494682: Training Step 6/59: batchLoss = 3.0411, diffLoss = 14.8441, kgLoss = 0.0904
2025-02-12 22:34:46.422315: Training Step 7/59: batchLoss = 2.7576, diffLoss = 13.4633, kgLoss = 0.0812
2025-02-12 22:34:47.340630: Training Step 8/59: batchLoss = 2.9567, diffLoss = 14.4229, kgLoss = 0.0901
2025-02-12 22:34:48.264236: Training Step 9/59: batchLoss = 2.6787, diffLoss = 13.0568, kgLoss = 0.0841
2025-02-12 22:34:49.186329: Training Step 10/59: batchLoss = 3.3538, diffLoss = 16.3736, kgLoss = 0.0988
2025-02-12 22:34:50.109113: Training Step 11/59: batchLoss = 3.0101, diffLoss = 14.7075, kgLoss = 0.0858
2025-02-12 22:34:51.037857: Training Step 12/59: batchLoss = 2.8137, diffLoss = 13.7288, kgLoss = 0.0850
2025-02-12 22:34:51.969028: Training Step 13/59: batchLoss = 3.3318, diffLoss = 16.2390, kgLoss = 0.1049
2025-02-12 22:34:52.904478: Training Step 14/59: batchLoss = 3.1592, diffLoss = 15.3982, kgLoss = 0.0995
2025-02-12 22:34:53.827459: Training Step 15/59: batchLoss = 2.6972, diffLoss = 13.1363, kgLoss = 0.0874
2025-02-12 22:34:54.751404: Training Step 16/59: batchLoss = 3.2842, diffLoss = 16.0510, kgLoss = 0.0925
2025-02-12 22:34:55.677103: Training Step 17/59: batchLoss = 2.9073, diffLoss = 14.1802, kgLoss = 0.0890
2025-02-12 22:34:56.598226: Training Step 18/59: batchLoss = 2.8944, diffLoss = 14.1346, kgLoss = 0.0844
2025-02-12 22:34:57.529275: Training Step 19/59: batchLoss = 2.7178, diffLoss = 13.2651, kgLoss = 0.0810
2025-02-12 22:34:58.454742: Training Step 20/59: batchLoss = 3.2058, diffLoss = 15.6306, kgLoss = 0.0996
2025-02-12 22:34:59.384709: Training Step 21/59: batchLoss = 3.2319, diffLoss = 15.7521, kgLoss = 0.1019
2025-02-12 22:35:00.318405: Training Step 22/59: batchLoss = 2.9890, diffLoss = 14.6148, kgLoss = 0.0826
2025-02-12 22:35:01.238075: Training Step 23/59: batchLoss = 2.8964, diffLoss = 14.1357, kgLoss = 0.0865
2025-02-12 22:35:02.160889: Training Step 24/59: batchLoss = 3.1092, diffLoss = 15.1844, kgLoss = 0.0904
2025-02-12 22:35:03.088481: Training Step 25/59: batchLoss = 2.7271, diffLoss = 13.3095, kgLoss = 0.0815
2025-02-12 22:35:04.011149: Training Step 26/59: batchLoss = 2.7948, diffLoss = 13.6400, kgLoss = 0.0836
2025-02-12 22:35:04.930995: Training Step 27/59: batchLoss = 2.9855, diffLoss = 14.5736, kgLoss = 0.0885
2025-02-12 22:35:05.857360: Training Step 28/59: batchLoss = 3.0319, diffLoss = 14.7958, kgLoss = 0.0909
2025-02-12 22:35:06.775446: Training Step 29/59: batchLoss = 2.9323, diffLoss = 14.3161, kgLoss = 0.0863
2025-02-12 22:35:07.693243: Training Step 30/59: batchLoss = 3.0071, diffLoss = 14.6760, kgLoss = 0.0899
2025-02-12 22:35:08.611849: Training Step 31/59: batchLoss = 3.1588, diffLoss = 15.4418, kgLoss = 0.0880
2025-02-12 22:35:09.519378: Training Step 32/59: batchLoss = 2.9768, diffLoss = 14.5343, kgLoss = 0.0874
2025-02-12 22:35:10.436392: Training Step 33/59: batchLoss = 3.1040, diffLoss = 15.1497, kgLoss = 0.0925
2025-02-12 22:35:11.353126: Training Step 34/59: batchLoss = 3.1377, diffLoss = 15.2934, kgLoss = 0.0988
2025-02-12 22:35:12.279602: Training Step 35/59: batchLoss = 2.5714, diffLoss = 12.5330, kgLoss = 0.0809
2025-02-12 22:35:13.186496: Training Step 36/59: batchLoss = 3.0739, diffLoss = 15.0171, kgLoss = 0.0880
2025-02-12 22:35:14.099180: Training Step 37/59: batchLoss = 3.4159, diffLoss = 16.6915, kgLoss = 0.0970
2025-02-12 22:35:15.011253: Training Step 38/59: batchLoss = 3.1091, diffLoss = 15.1665, kgLoss = 0.0948
2025-02-12 22:35:15.929475: Training Step 39/59: batchLoss = 3.0308, diffLoss = 14.7903, kgLoss = 0.0909
2025-02-12 22:35:16.845531: Training Step 40/59: batchLoss = 2.8378, diffLoss = 13.8751, kgLoss = 0.0785
2025-02-12 22:35:17.763025: Training Step 41/59: batchLoss = 3.1027, diffLoss = 15.1522, kgLoss = 0.0903
2025-02-12 22:35:18.677707: Training Step 42/59: batchLoss = 3.2317, diffLoss = 15.7981, kgLoss = 0.0901
2025-02-12 22:35:19.600783: Training Step 43/59: batchLoss = 2.7764, diffLoss = 13.5602, kgLoss = 0.0805
2025-02-12 22:35:20.520569: Training Step 44/59: batchLoss = 3.1921, diffLoss = 15.6253, kgLoss = 0.0838
2025-02-12 22:35:21.449866: Training Step 45/59: batchLoss = 3.2888, diffLoss = 16.0791, kgLoss = 0.0912
2025-02-12 22:35:22.364576: Training Step 46/59: batchLoss = 2.7385, diffLoss = 13.3780, kgLoss = 0.0787
2025-02-12 22:35:23.286453: Training Step 47/59: batchLoss = 3.8354, diffLoss = 18.7236, kgLoss = 0.1134
2025-02-12 22:35:24.206669: Training Step 48/59: batchLoss = 3.0093, diffLoss = 14.6741, kgLoss = 0.0930
2025-02-12 22:35:25.125070: Training Step 49/59: batchLoss = 3.0050, diffLoss = 14.6482, kgLoss = 0.0941
2025-02-12 22:35:26.045339: Training Step 50/59: batchLoss = 3.6507, diffLoss = 17.8231, kgLoss = 0.1076
2025-02-12 22:35:26.967191: Training Step 51/59: batchLoss = 3.2202, diffLoss = 15.7382, kgLoss = 0.0908
2025-02-12 22:35:27.884653: Training Step 52/59: batchLoss = 3.0652, diffLoss = 14.9789, kgLoss = 0.0868
2025-02-12 22:35:28.801603: Training Step 53/59: batchLoss = 2.8720, diffLoss = 14.0403, kgLoss = 0.0799
2025-02-12 22:35:29.713927: Training Step 54/59: batchLoss = 2.8014, diffLoss = 13.6779, kgLoss = 0.0822
2025-02-12 22:35:30.641883: Training Step 55/59: batchLoss = 2.8770, diffLoss = 14.0518, kgLoss = 0.0833
2025-02-12 22:35:31.560550: Training Step 56/59: batchLoss = 2.8841, diffLoss = 14.0943, kgLoss = 0.0815
2025-02-12 22:35:32.403366: Training Step 57/59: batchLoss = 2.9673, diffLoss = 14.4975, kgLoss = 0.0847
2025-02-12 22:35:33.253784: Training Step 58/59: batchLoss = 3.0260, diffLoss = 14.7834, kgLoss = 0.0866
2025-02-12 22:35:33.353513: 
2025-02-12 22:35:33.353841: Epoch 12/1000, Train: epLoss = 0.4442, epDfLoss = 2.1686, epKgLoss = 0.0132  
2025-02-12 22:35:34.832512: Steps 0/47: batch_recall = 28.79, batch_ndcg = 33.39 
2025-02-12 22:35:36.136652: Steps 1/47: batch_recall = 31.09, batch_ndcg = 30.78 
2025-02-12 22:35:37.399066: Steps 2/47: batch_recall = 33.58, batch_ndcg = 35.31 
2025-02-12 22:35:38.655759: Steps 3/47: batch_recall = 39.22, batch_ndcg = 36.69 
2025-02-12 22:35:39.853762: Steps 4/47: batch_recall = 33.96, batch_ndcg = 35.49 
2025-02-12 22:35:41.067327: Steps 5/47: batch_recall = 28.93, batch_ndcg = 29.45 
2025-02-12 22:35:42.271029: Steps 6/47: batch_recall = 35.55, batch_ndcg = 34.32 
2025-02-12 22:35:43.441804: Steps 7/47: batch_recall = 37.78, batch_ndcg = 35.17 
2025-02-12 22:35:44.618236: Steps 8/47: batch_recall = 41.86, batch_ndcg = 40.19 
2025-02-12 22:35:45.756762: Steps 9/47: batch_recall = 39.20, batch_ndcg = 37.72 
2025-02-12 22:35:46.908435: Steps 10/47: batch_recall = 39.88, batch_ndcg = 36.10 
2025-02-12 22:35:48.031273: Steps 11/47: batch_recall = 46.80, batch_ndcg = 39.00 
2025-02-12 22:35:49.175296: Steps 12/47: batch_recall = 45.53, batch_ndcg = 39.82 
2025-02-12 22:35:50.295483: Steps 13/47: batch_recall = 43.07, batch_ndcg = 36.80 
2025-02-12 22:35:51.372249: Steps 14/47: batch_recall = 36.60, batch_ndcg = 32.34 
2025-02-12 22:35:52.462507: Steps 15/47: batch_recall = 51.51, batch_ndcg = 41.14 
2025-02-12 22:35:53.537116: Steps 16/47: batch_recall = 44.42, batch_ndcg = 38.31 
2025-02-12 22:35:54.594713: Steps 17/47: batch_recall = 51.01, batch_ndcg = 41.43 
2025-02-12 22:35:55.686195: Steps 18/47: batch_recall = 45.37, batch_ndcg = 38.80 
2025-02-12 22:35:56.760919: Steps 19/47: batch_recall = 57.04, batch_ndcg = 47.30 
2025-02-12 22:35:57.809946: Steps 20/47: batch_recall = 58.92, batch_ndcg = 49.44 
2025-02-12 22:35:58.856228: Steps 21/47: batch_recall = 59.44, batch_ndcg = 46.42 
2025-02-12 22:35:59.921579: Steps 22/47: batch_recall = 51.09, batch_ndcg = 44.17 
2025-02-12 22:36:00.972286: Steps 23/47: batch_recall = 56.14, batch_ndcg = 43.55 
2025-02-12 22:36:02.038973: Steps 24/47: batch_recall = 58.28, batch_ndcg = 48.64 
2025-02-12 22:36:03.078807: Steps 25/47: batch_recall = 61.06, batch_ndcg = 49.01 
2025-02-12 22:36:04.087243: Steps 26/47: batch_recall = 54.81, batch_ndcg = 45.70 
2025-02-12 22:36:05.114887: Steps 27/47: batch_recall = 52.36, batch_ndcg = 42.35 
2025-02-12 22:36:06.120787: Steps 28/47: batch_recall = 56.88, batch_ndcg = 44.73 
2025-02-12 22:36:07.120657: Steps 29/47: batch_recall = 61.13, batch_ndcg = 47.48 
2025-02-12 22:36:08.116877: Steps 30/47: batch_recall = 68.80, batch_ndcg = 55.10 
2025-02-12 22:36:09.134682: Steps 31/47: batch_recall = 54.96, batch_ndcg = 42.99 
2025-02-12 22:36:10.129531: Steps 32/47: batch_recall = 63.66, batch_ndcg = 54.60 
2025-02-12 22:36:11.129164: Steps 33/47: batch_recall = 69.05, batch_ndcg = 53.58 
2025-02-12 22:36:12.124207: Steps 34/47: batch_recall = 56.80, batch_ndcg = 44.08 
2025-02-12 22:36:13.098438: Steps 35/47: batch_recall = 69.94, batch_ndcg = 53.51 
2025-02-12 22:36:14.075217: Steps 36/47: batch_recall = 72.58, batch_ndcg = 55.30 
2025-02-12 22:36:15.040882: Steps 37/47: batch_recall = 70.94, batch_ndcg = 57.18 
2025-02-12 22:36:16.026622: Steps 38/47: batch_recall = 78.93, batch_ndcg = 60.00 
2025-02-12 22:36:17.001350: Steps 39/47: batch_recall = 78.82, batch_ndcg = 56.91 
2025-02-12 22:36:17.973534: Steps 40/47: batch_recall = 64.33, batch_ndcg = 53.23 
2025-02-12 22:36:18.936567: Steps 41/47: batch_recall = 77.38, batch_ndcg = 59.47 
2025-02-12 22:36:19.896536: Steps 42/47: batch_recall = 77.32, batch_ndcg = 57.77 
2025-02-12 22:36:20.869911: Steps 43/47: batch_recall = 80.80, batch_ndcg = 58.90 
2025-02-12 22:36:21.839220: Steps 44/47: batch_recall = 83.40, batch_ndcg = 61.09 
2025-02-12 22:36:22.781584: Steps 45/47: batch_recall = 88.38, batch_ndcg = 66.95 
2025-02-12 22:36:22.889030: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.64 
2025-02-12 22:36:22.889171: Epoch 12/1000, Test: Recall = 0.1077, NDCG = 0.0888  

2025-02-12 22:36:24.093420: Training Step 0/59: batchLoss = 2.6428, diffLoss = 12.8896, kgLoss = 0.0810
2025-02-12 22:36:25.018765: Training Step 1/59: batchLoss = 2.7152, diffLoss = 13.2550, kgLoss = 0.0803
2025-02-12 22:36:25.935814: Training Step 2/59: batchLoss = 2.8717, diffLoss = 13.9842, kgLoss = 0.0935
2025-02-12 22:36:26.852692: Training Step 3/59: batchLoss = 2.8926, diffLoss = 14.0999, kgLoss = 0.0908
2025-02-12 22:36:27.770347: Training Step 4/59: batchLoss = 2.7744, diffLoss = 13.5297, kgLoss = 0.0856
2025-02-12 22:36:28.687941: Training Step 5/59: batchLoss = 2.9329, diffLoss = 14.3289, kgLoss = 0.0839
2025-02-12 22:36:29.601370: Training Step 6/59: batchLoss = 2.9487, diffLoss = 14.3802, kgLoss = 0.0908
2025-02-12 22:36:30.518731: Training Step 7/59: batchLoss = 2.7267, diffLoss = 13.2883, kgLoss = 0.0863
2025-02-12 22:36:31.435759: Training Step 8/59: batchLoss = 3.0080, diffLoss = 14.6623, kgLoss = 0.0944
2025-02-12 22:36:32.355853: Training Step 9/59: batchLoss = 3.0057, diffLoss = 14.6628, kgLoss = 0.0915
2025-02-12 22:36:33.277645: Training Step 10/59: batchLoss = 2.7774, diffLoss = 13.5383, kgLoss = 0.0872
2025-02-12 22:36:34.201243: Training Step 11/59: batchLoss = 2.8027, diffLoss = 13.6824, kgLoss = 0.0827
2025-02-12 22:36:35.126999: Training Step 12/59: batchLoss = 3.5085, diffLoss = 17.0875, kgLoss = 0.1138
2025-02-12 22:36:36.059418: Training Step 13/59: batchLoss = 2.7537, diffLoss = 13.4375, kgLoss = 0.0828
2025-02-12 22:36:36.990170: Training Step 14/59: batchLoss = 3.0592, diffLoss = 14.9152, kgLoss = 0.0952
2025-02-12 22:36:37.924312: Training Step 15/59: batchLoss = 2.9675, diffLoss = 14.4861, kgLoss = 0.0878
2025-02-12 22:36:38.846643: Training Step 16/59: batchLoss = 2.8059, diffLoss = 13.6753, kgLoss = 0.0886
2025-02-12 22:36:39.773851: Training Step 17/59: batchLoss = 2.8428, diffLoss = 13.8912, kgLoss = 0.0807
2025-02-12 22:36:40.702528: Training Step 18/59: batchLoss = 2.9419, diffLoss = 14.3723, kgLoss = 0.0843
2025-02-12 22:36:41.639515: Training Step 19/59: batchLoss = 3.1348, diffLoss = 15.2988, kgLoss = 0.0938
2025-02-12 22:36:42.565227: Training Step 20/59: batchLoss = 3.0411, diffLoss = 14.8481, kgLoss = 0.0894
2025-02-12 22:36:43.493808: Training Step 21/59: batchLoss = 2.8608, diffLoss = 13.9654, kgLoss = 0.0847
2025-02-12 22:36:44.426449: Training Step 22/59: batchLoss = 2.9625, diffLoss = 14.4265, kgLoss = 0.0965
2025-02-12 22:36:45.358625: Training Step 23/59: batchLoss = 2.9775, diffLoss = 14.5159, kgLoss = 0.0930
2025-02-12 22:36:46.286741: Training Step 24/59: batchLoss = 3.0812, diffLoss = 15.0702, kgLoss = 0.0840
2025-02-12 22:36:47.214071: Training Step 25/59: batchLoss = 2.8135, diffLoss = 13.7384, kgLoss = 0.0822
2025-02-12 22:36:48.124936: Training Step 26/59: batchLoss = 2.6681, diffLoss = 13.0088, kgLoss = 0.0829
2025-02-12 22:36:49.037448: Training Step 27/59: batchLoss = 2.7036, diffLoss = 13.2013, kgLoss = 0.0792
2025-02-12 22:36:49.956863: Training Step 28/59: batchLoss = 3.2839, diffLoss = 16.0143, kgLoss = 0.1013
2025-02-12 22:36:50.868075: Training Step 29/59: batchLoss = 2.8485, diffLoss = 13.9115, kgLoss = 0.0827
2025-02-12 22:36:51.789692: Training Step 30/59: batchLoss = 2.9539, diffLoss = 14.4118, kgLoss = 0.0894
2025-02-12 22:36:52.708202: Training Step 31/59: batchLoss = 3.0274, diffLoss = 14.7888, kgLoss = 0.0871
2025-02-12 22:36:53.623777: Training Step 32/59: batchLoss = 3.2895, diffLoss = 16.0619, kgLoss = 0.0964
2025-02-12 22:36:54.537819: Training Step 33/59: batchLoss = 3.4811, diffLoss = 16.9967, kgLoss = 0.1023
2025-02-12 22:36:55.460367: Training Step 34/59: batchLoss = 3.0069, diffLoss = 14.6861, kgLoss = 0.0871
2025-02-12 22:36:56.386920: Training Step 35/59: batchLoss = 3.1083, diffLoss = 15.1914, kgLoss = 0.0875
2025-02-12 22:36:57.310755: Training Step 36/59: batchLoss = 2.9307, diffLoss = 14.2891, kgLoss = 0.0911
2025-02-12 22:36:58.238064: Training Step 37/59: batchLoss = 3.0753, diffLoss = 15.0102, kgLoss = 0.0916
2025-02-12 22:36:59.192593: Training Step 38/59: batchLoss = 3.2468, diffLoss = 15.8541, kgLoss = 0.0950
2025-02-12 22:37:00.111828: Training Step 39/59: batchLoss = 2.6602, diffLoss = 12.9730, kgLoss = 0.0820
2025-02-12 22:37:01.039973: Training Step 40/59: batchLoss = 2.8023, diffLoss = 13.6756, kgLoss = 0.0839
2025-02-12 22:37:01.966911: Training Step 41/59: batchLoss = 2.7802, diffLoss = 13.5582, kgLoss = 0.0858
2025-02-12 22:37:02.891489: Training Step 42/59: batchLoss = 2.8718, diffLoss = 14.0278, kgLoss = 0.0828
2025-02-12 22:37:03.823394: Training Step 43/59: batchLoss = 3.1379, diffLoss = 15.3220, kgLoss = 0.0918
2025-02-12 22:37:04.749156: Training Step 44/59: batchLoss = 2.9621, diffLoss = 14.4159, kgLoss = 0.0987
2025-02-12 22:37:05.672377: Training Step 45/59: batchLoss = 3.1186, diffLoss = 15.2401, kgLoss = 0.0882
2025-02-12 22:37:06.589544: Training Step 46/59: batchLoss = 3.1166, diffLoss = 15.2372, kgLoss = 0.0864
2025-02-12 22:37:07.518431: Training Step 47/59: batchLoss = 3.3764, diffLoss = 16.5141, kgLoss = 0.0920
2025-02-12 22:37:08.435876: Training Step 48/59: batchLoss = 2.9105, diffLoss = 14.2160, kgLoss = 0.0842
2025-02-12 22:37:09.368253: Training Step 49/59: batchLoss = 3.0515, diffLoss = 14.8888, kgLoss = 0.0922
2025-02-12 22:37:10.293596: Training Step 50/59: batchLoss = 2.9662, diffLoss = 14.4949, kgLoss = 0.0840
2025-02-12 22:37:11.225191: Training Step 51/59: batchLoss = 3.1227, diffLoss = 15.2562, kgLoss = 0.0893
2025-02-12 22:37:12.163984: Training Step 52/59: batchLoss = 3.0293, diffLoss = 14.7772, kgLoss = 0.0924
2025-02-12 22:37:13.097512: Training Step 53/59: batchLoss = 3.0534, diffLoss = 14.9102, kgLoss = 0.0891
2025-02-12 22:37:14.029823: Training Step 54/59: batchLoss = 3.3149, diffLoss = 16.2014, kgLoss = 0.0932
2025-02-12 22:37:14.960915: Training Step 55/59: batchLoss = 3.2287, diffLoss = 15.7682, kgLoss = 0.0939
2025-02-12 22:37:15.895521: Training Step 56/59: batchLoss = 3.3968, diffLoss = 16.6266, kgLoss = 0.0893
2025-02-12 22:37:16.744470: Training Step 57/59: batchLoss = 3.0521, diffLoss = 14.8922, kgLoss = 0.0921
2025-02-12 22:37:17.602970: Training Step 58/59: batchLoss = 2.6573, diffLoss = 12.9707, kgLoss = 0.0789
2025-02-12 22:37:17.706322: 
2025-02-12 22:37:17.706875: Epoch 13/1000, Train: epLoss = 0.4402, epDfLoss = 2.1486, epKgLoss = 0.0131  
2025-02-12 22:37:19.196614: Steps 0/47: batch_recall = 29.18, batch_ndcg = 34.13 
2025-02-12 22:37:20.497926: Steps 1/47: batch_recall = 31.31, batch_ndcg = 30.80 
2025-02-12 22:37:21.749931: Steps 2/47: batch_recall = 34.33, batch_ndcg = 36.02 
2025-02-12 22:37:23.005923: Steps 3/47: batch_recall = 39.47, batch_ndcg = 36.95 
2025-02-12 22:37:24.198017: Steps 4/47: batch_recall = 34.05, batch_ndcg = 35.97 
2025-02-12 22:37:25.425290: Steps 5/47: batch_recall = 28.96, batch_ndcg = 29.65 
2025-02-12 22:37:26.622675: Steps 6/47: batch_recall = 34.36, batch_ndcg = 33.31 
2025-02-12 22:37:27.791930: Steps 7/47: batch_recall = 39.05, batch_ndcg = 35.69 
2025-02-12 22:37:28.965679: Steps 8/47: batch_recall = 40.86, batch_ndcg = 39.69 
2025-02-12 22:37:30.121606: Steps 9/47: batch_recall = 40.35, batch_ndcg = 38.56 
2025-02-12 22:37:31.305963: Steps 10/47: batch_recall = 40.53, batch_ndcg = 36.24 
2025-02-12 22:37:32.452411: Steps 11/47: batch_recall = 47.18, batch_ndcg = 40.54 
2025-02-12 22:37:33.616182: Steps 12/47: batch_recall = 44.15, batch_ndcg = 39.40 
2025-02-12 22:37:34.767201: Steps 13/47: batch_recall = 42.87, batch_ndcg = 36.80 
2025-02-12 22:37:35.877758: Steps 14/47: batch_recall = 36.81, batch_ndcg = 33.27 
2025-02-12 22:37:36.988107: Steps 15/47: batch_recall = 51.42, batch_ndcg = 42.11 
2025-02-12 22:37:38.086510: Steps 16/47: batch_recall = 44.58, batch_ndcg = 38.36 
2025-02-12 22:37:39.135814: Steps 17/47: batch_recall = 50.24, batch_ndcg = 40.67 
2025-02-12 22:37:40.216053: Steps 18/47: batch_recall = 45.94, batch_ndcg = 40.02 
2025-02-12 22:37:41.283181: Steps 19/47: batch_recall = 56.40, batch_ndcg = 46.72 
2025-02-12 22:37:42.324335: Steps 20/47: batch_recall = 58.07, batch_ndcg = 49.30 
2025-02-12 22:37:43.374093: Steps 21/47: batch_recall = 60.29, batch_ndcg = 47.56 
2025-02-12 22:37:44.421235: Steps 22/47: batch_recall = 49.54, batch_ndcg = 42.83 
2025-02-12 22:37:45.468270: Steps 23/47: batch_recall = 54.33, batch_ndcg = 42.17 
2025-02-12 22:37:46.537240: Steps 24/47: batch_recall = 57.06, batch_ndcg = 47.43 
2025-02-12 22:37:47.574993: Steps 25/47: batch_recall = 59.74, batch_ndcg = 48.90 
2025-02-12 22:37:48.594467: Steps 26/47: batch_recall = 52.69, batch_ndcg = 44.32 
2025-02-12 22:37:49.637966: Steps 27/47: batch_recall = 52.48, batch_ndcg = 42.30 
2025-02-12 22:37:50.673783: Steps 28/47: batch_recall = 58.30, batch_ndcg = 46.51 
2025-02-12 22:37:51.702549: Steps 29/47: batch_recall = 60.94, batch_ndcg = 47.71 
2025-02-12 22:37:52.735436: Steps 30/47: batch_recall = 68.97, batch_ndcg = 55.36 
2025-02-12 22:37:53.783948: Steps 31/47: batch_recall = 53.92, batch_ndcg = 42.55 
2025-02-12 22:37:54.807436: Steps 32/47: batch_recall = 63.68, batch_ndcg = 54.23 
2025-02-12 22:37:55.834854: Steps 33/47: batch_recall = 71.74, batch_ndcg = 56.35 
2025-02-12 22:37:56.871452: Steps 34/47: batch_recall = 60.11, batch_ndcg = 45.52 
2025-02-12 22:37:57.855731: Steps 35/47: batch_recall = 70.22, batch_ndcg = 54.17 
2025-02-12 22:37:58.837576: Steps 36/47: batch_recall = 72.67, batch_ndcg = 56.46 
2025-02-12 22:37:59.805134: Steps 37/47: batch_recall = 72.71, batch_ndcg = 57.66 
2025-02-12 22:38:00.776998: Steps 38/47: batch_recall = 79.87, batch_ndcg = 61.91 
2025-02-12 22:38:01.737622: Steps 39/47: batch_recall = 76.59, batch_ndcg = 57.29 
2025-02-12 22:38:02.705038: Steps 40/47: batch_recall = 63.44, batch_ndcg = 52.44 
2025-02-12 22:38:03.661156: Steps 41/47: batch_recall = 77.83, batch_ndcg = 59.75 
2025-02-12 22:38:04.619207: Steps 42/47: batch_recall = 78.88, batch_ndcg = 59.36 
2025-02-12 22:38:05.579419: Steps 43/47: batch_recall = 81.74, batch_ndcg = 59.31 
2025-02-12 22:38:06.532774: Steps 44/47: batch_recall = 83.08, batch_ndcg = 59.72 
2025-02-12 22:38:07.471659: Steps 45/47: batch_recall = 87.92, batch_ndcg = 66.77 
2025-02-12 22:38:07.574794: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.47 
2025-02-12 22:38:07.574918: Epoch 13/1000, Test: Recall = 0.1078, NDCG = 0.0893  

2025-02-12 22:38:08.770302: Training Step 0/59: batchLoss = 3.1103, diffLoss = 15.1591, kgLoss = 0.0981
2025-02-12 22:38:09.702966: Training Step 1/59: batchLoss = 2.8181, diffLoss = 13.7373, kgLoss = 0.0883
2025-02-12 22:38:10.632175: Training Step 2/59: batchLoss = 2.6334, diffLoss = 12.8494, kgLoss = 0.0794
2025-02-12 22:38:11.567165: Training Step 3/59: batchLoss = 2.6983, diffLoss = 13.1555, kgLoss = 0.0840
2025-02-12 22:38:12.497767: Training Step 4/59: batchLoss = 2.8177, diffLoss = 13.7349, kgLoss = 0.0884
2025-02-12 22:38:13.421022: Training Step 5/59: batchLoss = 3.0004, diffLoss = 14.6037, kgLoss = 0.0996
2025-02-12 22:38:14.354379: Training Step 6/59: batchLoss = 3.0697, diffLoss = 14.9684, kgLoss = 0.0951
2025-02-12 22:38:15.286700: Training Step 7/59: batchLoss = 2.7064, diffLoss = 13.2063, kgLoss = 0.0815
2025-02-12 22:38:16.218814: Training Step 8/59: batchLoss = 2.7993, diffLoss = 13.6732, kgLoss = 0.0808
2025-02-12 22:38:17.143881: Training Step 9/59: batchLoss = 2.7679, diffLoss = 13.4914, kgLoss = 0.0870
2025-02-12 22:38:18.066219: Training Step 10/59: batchLoss = 3.0726, diffLoss = 14.9816, kgLoss = 0.0954
2025-02-12 22:38:18.988572: Training Step 11/59: batchLoss = 3.2025, diffLoss = 15.6314, kgLoss = 0.0953
2025-02-12 22:38:19.910518: Training Step 12/59: batchLoss = 3.1093, diffLoss = 15.1711, kgLoss = 0.0939
2025-02-12 22:38:20.824908: Training Step 13/59: batchLoss = 3.0705, diffLoss = 14.9461, kgLoss = 0.1016
2025-02-12 22:38:21.741294: Training Step 14/59: batchLoss = 3.1578, diffLoss = 15.3886, kgLoss = 0.1001
2025-02-12 22:38:22.653760: Training Step 15/59: batchLoss = 2.8425, diffLoss = 13.8801, kgLoss = 0.0830
2025-02-12 22:38:23.560779: Training Step 16/59: batchLoss = 2.6789, diffLoss = 13.0729, kgLoss = 0.0804
2025-02-12 22:38:24.485406: Training Step 17/59: batchLoss = 3.1570, diffLoss = 15.4029, kgLoss = 0.0955
2025-02-12 22:38:25.400470: Training Step 18/59: batchLoss = 3.1496, diffLoss = 15.3708, kgLoss = 0.0943
2025-02-12 22:38:26.325304: Training Step 19/59: batchLoss = 2.9417, diffLoss = 14.3595, kgLoss = 0.0872
2025-02-12 22:38:27.258095: Training Step 20/59: batchLoss = 2.8307, diffLoss = 13.8124, kgLoss = 0.0853
2025-02-12 22:38:28.186230: Training Step 21/59: batchLoss = 2.4834, diffLoss = 12.0993, kgLoss = 0.0794
2025-02-12 22:38:29.117810: Training Step 22/59: batchLoss = 2.9914, diffLoss = 14.6261, kgLoss = 0.0828
2025-02-12 22:38:30.049707: Training Step 23/59: batchLoss = 3.2064, diffLoss = 15.6536, kgLoss = 0.0946
2025-02-12 22:38:30.989138: Training Step 24/59: batchLoss = 3.2122, diffLoss = 15.6868, kgLoss = 0.0936
2025-02-12 22:38:31.928397: Training Step 25/59: batchLoss = 2.7409, diffLoss = 13.4035, kgLoss = 0.0753
2025-02-12 22:38:32.855890: Training Step 26/59: batchLoss = 3.2059, diffLoss = 15.6367, kgLoss = 0.0982
2025-02-12 22:38:33.788845: Training Step 27/59: batchLoss = 3.0243, diffLoss = 14.7746, kgLoss = 0.0868
2025-02-12 22:38:34.719258: Training Step 28/59: batchLoss = 2.8324, diffLoss = 13.8239, kgLoss = 0.0846
2025-02-12 22:38:35.654726: Training Step 29/59: batchLoss = 3.0033, diffLoss = 14.6348, kgLoss = 0.0954
2025-02-12 22:38:36.590226: Training Step 30/59: batchLoss = 3.2733, diffLoss = 15.9612, kgLoss = 0.1013
2025-02-12 22:38:37.515015: Training Step 31/59: batchLoss = 2.9624, diffLoss = 14.4541, kgLoss = 0.0895
2025-02-12 22:38:38.437669: Training Step 32/59: batchLoss = 2.6961, diffLoss = 13.1664, kgLoss = 0.0786
2025-02-12 22:38:39.356928: Training Step 33/59: batchLoss = 3.0124, diffLoss = 14.7240, kgLoss = 0.0845
2025-02-12 22:38:40.285601: Training Step 34/59: batchLoss = 2.9320, diffLoss = 14.3268, kgLoss = 0.0832
2025-02-12 22:38:41.199867: Training Step 35/59: batchLoss = 2.7364, diffLoss = 13.3751, kgLoss = 0.0767
2025-02-12 22:38:42.116574: Training Step 36/59: batchLoss = 3.0317, diffLoss = 14.7983, kgLoss = 0.0901
2025-02-12 22:38:43.033898: Training Step 37/59: batchLoss = 2.9577, diffLoss = 14.4372, kgLoss = 0.0879
2025-02-12 22:38:43.955991: Training Step 38/59: batchLoss = 3.0396, diffLoss = 14.8456, kgLoss = 0.0881
2025-02-12 22:38:44.867583: Training Step 39/59: batchLoss = 3.0932, diffLoss = 15.0725, kgLoss = 0.0984
2025-02-12 22:38:45.787834: Training Step 40/59: batchLoss = 2.8640, diffLoss = 13.9869, kgLoss = 0.0833
2025-02-12 22:38:46.715675: Training Step 41/59: batchLoss = 2.8100, diffLoss = 13.7273, kgLoss = 0.0806
2025-02-12 22:38:47.648776: Training Step 42/59: batchLoss = 3.1108, diffLoss = 15.1958, kgLoss = 0.0896
2025-02-12 22:38:48.584323: Training Step 43/59: batchLoss = 2.9660, diffLoss = 14.4801, kgLoss = 0.0874
2025-02-12 22:38:49.517998: Training Step 44/59: batchLoss = 3.2739, diffLoss = 15.9975, kgLoss = 0.0930
2025-02-12 22:38:50.449726: Training Step 45/59: batchLoss = 3.2773, diffLoss = 15.9939, kgLoss = 0.0981
2025-02-12 22:38:51.381195: Training Step 46/59: batchLoss = 2.8683, diffLoss = 13.9788, kgLoss = 0.0907
2025-02-12 22:38:52.327392: Training Step 47/59: batchLoss = 3.0894, diffLoss = 15.1031, kgLoss = 0.0860
2025-02-12 22:38:53.261688: Training Step 48/59: batchLoss = 3.1074, diffLoss = 15.1911, kgLoss = 0.0864
2025-02-12 22:38:54.195381: Training Step 49/59: batchLoss = 2.6763, diffLoss = 13.0569, kgLoss = 0.0811
2025-02-12 22:38:55.126487: Training Step 50/59: batchLoss = 2.9581, diffLoss = 14.4378, kgLoss = 0.0882
2025-02-12 22:38:56.044076: Training Step 51/59: batchLoss = 3.0761, diffLoss = 15.0240, kgLoss = 0.0891
2025-02-12 22:38:56.975197: Training Step 52/59: batchLoss = 2.9719, diffLoss = 14.5422, kgLoss = 0.0793
2025-02-12 22:38:57.893655: Training Step 53/59: batchLoss = 3.3421, diffLoss = 16.3472, kgLoss = 0.0908
2025-02-12 22:38:58.809904: Training Step 54/59: batchLoss = 3.1198, diffLoss = 15.2454, kgLoss = 0.0884
2025-02-12 22:38:59.730660: Training Step 55/59: batchLoss = 3.3218, diffLoss = 16.2493, kgLoss = 0.0899
2025-02-12 22:39:00.645720: Training Step 56/59: batchLoss = 2.8440, diffLoss = 13.8701, kgLoss = 0.0875
2025-02-12 22:39:01.490048: Training Step 57/59: batchLoss = 2.9522, diffLoss = 14.4033, kgLoss = 0.0894
2025-02-12 22:39:02.333060: Training Step 58/59: batchLoss = 2.8529, diffLoss = 13.8949, kgLoss = 0.0924
2025-02-12 22:39:02.430221: 
2025-02-12 22:39:02.430545: Epoch 14/1000, Train: epLoss = 0.4389, epDfLoss = 2.1421, epKgLoss = 0.0131  
2025-02-12 22:39:03.893789: Steps 0/47: batch_recall = 30.08, batch_ndcg = 34.72 
2025-02-12 22:39:05.180876: Steps 1/47: batch_recall = 32.42, batch_ndcg = 32.43 
2025-02-12 22:39:06.439024: Steps 2/47: batch_recall = 34.62, batch_ndcg = 36.61 
2025-02-12 22:39:07.701443: Steps 3/47: batch_recall = 39.42, batch_ndcg = 37.20 
2025-02-12 22:39:08.908624: Steps 4/47: batch_recall = 34.07, batch_ndcg = 36.44 
2025-02-12 22:39:10.127643: Steps 5/47: batch_recall = 28.15, batch_ndcg = 29.08 
2025-02-12 22:39:11.339961: Steps 6/47: batch_recall = 34.75, batch_ndcg = 34.17 
2025-02-12 22:39:12.515917: Steps 7/47: batch_recall = 38.18, batch_ndcg = 35.74 
2025-02-12 22:39:13.696221: Steps 8/47: batch_recall = 41.10, batch_ndcg = 39.85 
2025-02-12 22:39:14.845895: Steps 9/47: batch_recall = 40.54, batch_ndcg = 38.86 
2025-02-12 22:39:16.013500: Steps 10/47: batch_recall = 40.04, batch_ndcg = 36.33 
2025-02-12 22:39:17.151197: Steps 11/47: batch_recall = 47.04, batch_ndcg = 40.01 
2025-02-12 22:39:18.288745: Steps 12/47: batch_recall = 45.36, batch_ndcg = 40.26 
2025-02-12 22:39:19.416603: Steps 13/47: batch_recall = 42.66, batch_ndcg = 37.28 
2025-02-12 22:39:20.500714: Steps 14/47: batch_recall = 36.62, batch_ndcg = 33.78 
2025-02-12 22:39:21.580434: Steps 15/47: batch_recall = 50.21, batch_ndcg = 42.01 
2025-02-12 22:39:22.643414: Steps 16/47: batch_recall = 45.41, batch_ndcg = 39.50 
2025-02-12 22:39:23.691909: Steps 17/47: batch_recall = 50.59, batch_ndcg = 41.09 
2025-02-12 22:39:24.774458: Steps 18/47: batch_recall = 46.30, batch_ndcg = 40.27 
2025-02-12 22:39:25.852501: Steps 19/47: batch_recall = 56.61, batch_ndcg = 47.56 
2025-02-12 22:39:26.915764: Steps 20/47: batch_recall = 58.02, batch_ndcg = 49.32 
2025-02-12 22:39:27.977150: Steps 21/47: batch_recall = 61.75, batch_ndcg = 48.60 
2025-02-12 22:39:29.051119: Steps 22/47: batch_recall = 50.17, batch_ndcg = 43.86 
2025-02-12 22:39:30.111831: Steps 23/47: batch_recall = 53.24, batch_ndcg = 41.78 
2025-02-12 22:39:31.190219: Steps 24/47: batch_recall = 59.94, batch_ndcg = 49.31 
2025-02-12 22:39:32.240863: Steps 25/47: batch_recall = 60.35, batch_ndcg = 49.17 
2025-02-12 22:39:33.264932: Steps 26/47: batch_recall = 52.22, batch_ndcg = 44.82 
2025-02-12 22:39:34.296859: Steps 27/47: batch_recall = 53.73, batch_ndcg = 43.36 
2025-02-12 22:39:35.321283: Steps 28/47: batch_recall = 61.17, batch_ndcg = 48.07 
2025-02-12 22:39:36.343633: Steps 29/47: batch_recall = 61.15, batch_ndcg = 47.55 
2025-02-12 22:39:37.345754: Steps 30/47: batch_recall = 69.53, batch_ndcg = 55.37 
2025-02-12 22:39:38.357992: Steps 31/47: batch_recall = 55.41, batch_ndcg = 43.53 
2025-02-12 22:39:39.362399: Steps 32/47: batch_recall = 62.68, batch_ndcg = 54.47 
2025-02-12 22:39:40.366436: Steps 33/47: batch_recall = 73.40, batch_ndcg = 57.73 
2025-02-12 22:39:41.358039: Steps 34/47: batch_recall = 59.60, batch_ndcg = 45.54 
2025-02-12 22:39:42.342798: Steps 35/47: batch_recall = 70.73, batch_ndcg = 54.92 
2025-02-12 22:39:43.328766: Steps 36/47: batch_recall = 72.88, batch_ndcg = 57.84 
2025-02-12 22:39:44.311707: Steps 37/47: batch_recall = 74.20, batch_ndcg = 59.01 
2025-02-12 22:39:45.296595: Steps 38/47: batch_recall = 81.62, batch_ndcg = 63.32 
2025-02-12 22:39:46.276711: Steps 39/47: batch_recall = 77.98, batch_ndcg = 57.13 
2025-02-12 22:39:47.261469: Steps 40/47: batch_recall = 65.34, batch_ndcg = 54.25 
2025-02-12 22:39:48.234570: Steps 41/47: batch_recall = 79.80, batch_ndcg = 61.38 
2025-02-12 22:39:49.197724: Steps 42/47: batch_recall = 79.49, batch_ndcg = 59.05 
2025-02-12 22:39:50.173895: Steps 43/47: batch_recall = 82.13, batch_ndcg = 58.72 
2025-02-12 22:39:51.151871: Steps 44/47: batch_recall = 83.86, batch_ndcg = 61.21 
2025-02-12 22:39:52.095746: Steps 45/47: batch_recall = 88.18, batch_ndcg = 67.10 
2025-02-12 22:39:52.201616: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.49 
2025-02-12 22:39:52.201754: Epoch 14/1000, Test: Recall = 0.1088, NDCG = 0.0904  

2025-02-12 22:39:53.396232: Training Step 0/59: batchLoss = 3.0461, diffLoss = 14.8106, kgLoss = 0.1050
2025-02-12 22:39:54.319957: Training Step 1/59: batchLoss = 2.8128, diffLoss = 13.6917, kgLoss = 0.0930
2025-02-12 22:39:55.246998: Training Step 2/59: batchLoss = 2.5434, diffLoss = 12.4041, kgLoss = 0.0782
2025-02-12 22:39:56.173836: Training Step 3/59: batchLoss = 2.7913, diffLoss = 13.6241, kgLoss = 0.0831
2025-02-12 22:39:57.096195: Training Step 4/59: batchLoss = 2.9189, diffLoss = 14.2579, kgLoss = 0.0841
2025-02-12 22:39:58.025648: Training Step 5/59: batchLoss = 2.6732, diffLoss = 13.0375, kgLoss = 0.0821
2025-02-12 22:39:58.963144: Training Step 6/59: batchLoss = 2.7255, diffLoss = 13.3092, kgLoss = 0.0796
2025-02-12 22:39:59.889675: Training Step 7/59: batchLoss = 2.8010, diffLoss = 13.6671, kgLoss = 0.0844
2025-02-12 22:40:00.813249: Training Step 8/59: batchLoss = 3.0431, diffLoss = 14.8760, kgLoss = 0.0849
2025-02-12 22:40:01.739656: Training Step 9/59: batchLoss = 2.9349, diffLoss = 14.3221, kgLoss = 0.0881
2025-02-12 22:40:02.674840: Training Step 10/59: batchLoss = 2.9238, diffLoss = 14.2759, kgLoss = 0.0858
2025-02-12 22:40:03.608604: Training Step 11/59: batchLoss = 3.1132, diffLoss = 15.1662, kgLoss = 0.1000
2025-02-12 22:40:04.545781: Training Step 12/59: batchLoss = 2.8599, diffLoss = 13.9740, kgLoss = 0.0814
2025-02-12 22:40:05.489078: Training Step 13/59: batchLoss = 2.7148, diffLoss = 13.2566, kgLoss = 0.0794
2025-02-12 22:40:06.431822: Training Step 14/59: batchLoss = 2.8994, diffLoss = 14.1131, kgLoss = 0.0960
2025-02-12 22:40:07.368845: Training Step 15/59: batchLoss = 2.7384, diffLoss = 13.3897, kgLoss = 0.0756
2025-02-12 22:40:08.305168: Training Step 16/59: batchLoss = 2.9099, diffLoss = 14.1939, kgLoss = 0.0889
2025-02-12 22:40:09.229592: Training Step 17/59: batchLoss = 3.1293, diffLoss = 15.2653, kgLoss = 0.0953
2025-02-12 22:40:10.160775: Training Step 18/59: batchLoss = 2.8701, diffLoss = 14.0217, kgLoss = 0.0822
2025-02-12 22:40:11.083202: Training Step 19/59: batchLoss = 3.2532, diffLoss = 15.8655, kgLoss = 0.1001
2025-02-12 22:40:12.001868: Training Step 20/59: batchLoss = 3.0348, diffLoss = 14.7809, kgLoss = 0.0983
2025-02-12 22:40:12.923450: Training Step 21/59: batchLoss = 3.0727, diffLoss = 14.9935, kgLoss = 0.0925
2025-02-12 22:40:13.849996: Training Step 22/59: batchLoss = 2.8388, diffLoss = 13.8555, kgLoss = 0.0846
2025-02-12 22:40:14.785584: Training Step 23/59: batchLoss = 2.6568, diffLoss = 12.9531, kgLoss = 0.0828
2025-02-12 22:40:15.720393: Training Step 24/59: batchLoss = 3.1750, diffLoss = 15.5003, kgLoss = 0.0937
2025-02-12 22:40:16.642135: Training Step 25/59: batchLoss = 2.9617, diffLoss = 14.4410, kgLoss = 0.0919
2025-02-12 22:40:17.574036: Training Step 26/59: batchLoss = 3.2508, diffLoss = 15.8794, kgLoss = 0.0937
2025-02-12 22:40:18.496617: Training Step 27/59: batchLoss = 3.4513, diffLoss = 16.8505, kgLoss = 0.1015
2025-02-12 22:40:19.420910: Training Step 28/59: batchLoss = 2.9405, diffLoss = 14.3578, kgLoss = 0.0862
2025-02-12 22:40:20.342101: Training Step 29/59: batchLoss = 2.8686, diffLoss = 14.0146, kgLoss = 0.0821
2025-02-12 22:40:21.269862: Training Step 30/59: batchLoss = 2.8526, diffLoss = 13.9313, kgLoss = 0.0829
2025-02-12 22:40:22.220253: Training Step 31/59: batchLoss = 3.0228, diffLoss = 14.7597, kgLoss = 0.0886
2025-02-12 22:40:23.143608: Training Step 32/59: batchLoss = 3.3563, diffLoss = 16.3814, kgLoss = 0.1001
2025-02-12 22:40:24.067746: Training Step 33/59: batchLoss = 2.9542, diffLoss = 14.4353, kgLoss = 0.0839
2025-02-12 22:40:24.993341: Training Step 34/59: batchLoss = 2.8530, diffLoss = 13.9400, kgLoss = 0.0812
2025-02-12 22:40:25.924707: Training Step 35/59: batchLoss = 3.1893, diffLoss = 15.5628, kgLoss = 0.0959
2025-02-12 22:40:26.854734: Training Step 36/59: batchLoss = 3.1817, diffLoss = 15.5154, kgLoss = 0.0983
2025-02-12 22:40:27.791254: Training Step 37/59: batchLoss = 3.0389, diffLoss = 14.8285, kgLoss = 0.0915
2025-02-12 22:40:28.732767: Training Step 38/59: batchLoss = 2.8995, diffLoss = 14.1825, kgLoss = 0.0787
2025-02-12 22:40:29.672647: Training Step 39/59: batchLoss = 2.8964, diffLoss = 14.1281, kgLoss = 0.0885
2025-02-12 22:40:30.602298: Training Step 40/59: batchLoss = 3.2486, diffLoss = 15.8826, kgLoss = 0.0901
2025-02-12 22:40:31.532039: Training Step 41/59: batchLoss = 2.9686, diffLoss = 14.4839, kgLoss = 0.0897
2025-02-12 22:40:32.455708: Training Step 42/59: batchLoss = 2.9414, diffLoss = 14.3729, kgLoss = 0.0835
2025-02-12 22:40:33.382846: Training Step 43/59: batchLoss = 2.6023, diffLoss = 12.7000, kgLoss = 0.0778
2025-02-12 22:40:34.297148: Training Step 44/59: batchLoss = 3.0246, diffLoss = 14.7952, kgLoss = 0.0820
2025-02-12 22:40:35.216996: Training Step 45/59: batchLoss = 3.6267, diffLoss = 17.7097, kgLoss = 0.1060
2025-02-12 22:40:36.132965: Training Step 46/59: batchLoss = 2.7017, diffLoss = 13.1658, kgLoss = 0.0857
2025-02-12 22:40:37.059846: Training Step 47/59: batchLoss = 2.9957, diffLoss = 14.6388, kgLoss = 0.0849
2025-02-12 22:40:37.995714: Training Step 48/59: batchLoss = 3.0192, diffLoss = 14.7620, kgLoss = 0.0835
2025-02-12 22:40:38.917316: Training Step 49/59: batchLoss = 3.4185, diffLoss = 16.6890, kgLoss = 0.1009
2025-02-12 22:40:39.840841: Training Step 50/59: batchLoss = 3.2202, diffLoss = 15.7281, kgLoss = 0.0932
2025-02-12 22:40:40.775168: Training Step 51/59: batchLoss = 3.2289, diffLoss = 15.7506, kgLoss = 0.0984
2025-02-12 22:40:41.716923: Training Step 52/59: batchLoss = 2.9471, diffLoss = 14.4210, kgLoss = 0.0786
2025-02-12 22:40:42.648276: Training Step 53/59: batchLoss = 2.7841, diffLoss = 13.6134, kgLoss = 0.0768
2025-02-12 22:40:43.590320: Training Step 54/59: batchLoss = 3.1093, diffLoss = 15.1759, kgLoss = 0.0927
2025-02-12 22:40:44.525816: Training Step 55/59: batchLoss = 2.8415, diffLoss = 13.8927, kgLoss = 0.0787
2025-02-12 22:40:45.458373: Training Step 56/59: batchLoss = 3.5020, diffLoss = 17.1114, kgLoss = 0.0997
2025-02-12 22:40:46.309046: Training Step 57/59: batchLoss = 3.2398, diffLoss = 15.8561, kgLoss = 0.0858
2025-02-12 22:40:47.163099: Training Step 58/59: batchLoss = 3.2238, diffLoss = 15.7745, kgLoss = 0.0861
2025-02-12 22:40:47.257795: 
2025-02-12 22:40:47.258534: Epoch 15/1000, Train: epLoss = 0.4421, epDfLoss = 2.1583, epKgLoss = 0.0130  
2025-02-12 22:40:48.753669: Steps 0/47: batch_recall = 30.78, batch_ndcg = 36.46 
2025-02-12 22:40:50.060700: Steps 1/47: batch_recall = 31.58, batch_ndcg = 32.18 
2025-02-12 22:40:51.315243: Steps 2/47: batch_recall = 34.40, batch_ndcg = 37.37 
2025-02-12 22:40:52.569605: Steps 3/47: batch_recall = 39.07, batch_ndcg = 36.88 
2025-02-12 22:40:53.761330: Steps 4/47: batch_recall = 34.42, batch_ndcg = 36.62 
2025-02-12 22:40:54.962921: Steps 5/47: batch_recall = 28.90, batch_ndcg = 29.98 
2025-02-12 22:40:56.162169: Steps 6/47: batch_recall = 35.76, batch_ndcg = 35.28 
2025-02-12 22:40:57.334289: Steps 7/47: batch_recall = 38.38, batch_ndcg = 35.66 
2025-02-12 22:40:58.503266: Steps 8/47: batch_recall = 41.40, batch_ndcg = 39.95 
2025-02-12 22:40:59.653711: Steps 9/47: batch_recall = 40.81, batch_ndcg = 39.59 
2025-02-12 22:41:00.834843: Steps 10/47: batch_recall = 39.86, batch_ndcg = 37.26 
2025-02-12 22:41:01.978528: Steps 11/47: batch_recall = 47.90, batch_ndcg = 40.84 
2025-02-12 22:41:03.129855: Steps 12/47: batch_recall = 44.57, batch_ndcg = 40.51 
2025-02-12 22:41:04.276090: Steps 13/47: batch_recall = 43.70, batch_ndcg = 38.46 
2025-02-12 22:41:05.386376: Steps 14/47: batch_recall = 38.69, batch_ndcg = 34.61 
2025-02-12 22:41:06.488479: Steps 15/47: batch_recall = 50.71, batch_ndcg = 42.88 
2025-02-12 22:41:07.582307: Steps 16/47: batch_recall = 44.71, batch_ndcg = 38.54 
2025-02-12 22:41:08.652469: Steps 17/47: batch_recall = 48.60, batch_ndcg = 40.13 
2025-02-12 22:41:09.727196: Steps 18/47: batch_recall = 45.44, batch_ndcg = 40.21 
2025-02-12 22:41:10.795871: Steps 19/47: batch_recall = 58.37, batch_ndcg = 47.90 
2025-02-12 22:41:11.851160: Steps 20/47: batch_recall = 58.26, batch_ndcg = 50.48 
2025-02-12 22:41:12.905555: Steps 21/47: batch_recall = 63.17, batch_ndcg = 50.53 
2025-02-12 22:41:13.950940: Steps 22/47: batch_recall = 50.05, batch_ndcg = 43.28 
2025-02-12 22:41:15.002059: Steps 23/47: batch_recall = 56.18, batch_ndcg = 44.17 
2025-02-12 22:41:16.066771: Steps 24/47: batch_recall = 58.17, batch_ndcg = 48.63 
2025-02-12 22:41:17.107721: Steps 25/47: batch_recall = 61.24, batch_ndcg = 49.34 
2025-02-12 22:41:18.126610: Steps 26/47: batch_recall = 53.00, batch_ndcg = 45.00 
2025-02-12 22:41:19.175315: Steps 27/47: batch_recall = 54.88, batch_ndcg = 44.68 
2025-02-12 22:41:20.203725: Steps 28/47: batch_recall = 59.52, batch_ndcg = 47.84 
2025-02-12 22:41:21.221536: Steps 29/47: batch_recall = 61.36, batch_ndcg = 48.08 
2025-02-12 22:41:22.244790: Steps 30/47: batch_recall = 69.05, batch_ndcg = 56.48 
2025-02-12 22:41:23.288004: Steps 31/47: batch_recall = 55.69, batch_ndcg = 44.19 
2025-02-12 22:41:24.317622: Steps 32/47: batch_recall = 64.20, batch_ndcg = 54.96 
2025-02-12 22:41:25.345256: Steps 33/47: batch_recall = 71.48, batch_ndcg = 56.57 
2025-02-12 22:41:26.369570: Steps 34/47: batch_recall = 58.42, batch_ndcg = 45.04 
2025-02-12 22:41:27.366040: Steps 35/47: batch_recall = 71.71, batch_ndcg = 55.62 
2025-02-12 22:41:28.355656: Steps 36/47: batch_recall = 74.10, batch_ndcg = 57.63 
2025-02-12 22:41:29.339485: Steps 37/47: batch_recall = 75.78, batch_ndcg = 59.82 
2025-02-12 22:41:30.308804: Steps 38/47: batch_recall = 80.37, batch_ndcg = 63.07 
2025-02-12 22:41:31.279463: Steps 39/47: batch_recall = 77.55, batch_ndcg = 57.30 
2025-02-12 22:41:32.239704: Steps 40/47: batch_recall = 66.53, batch_ndcg = 55.09 
2025-02-12 22:41:33.196011: Steps 41/47: batch_recall = 80.83, batch_ndcg = 62.27 
2025-02-12 22:41:34.161291: Steps 42/47: batch_recall = 79.90, batch_ndcg = 59.95 
2025-02-12 22:41:35.109285: Steps 43/47: batch_recall = 84.04, batch_ndcg = 60.92 
2025-02-12 22:41:36.068043: Steps 44/47: batch_recall = 85.26, batch_ndcg = 62.69 
2025-02-12 22:41:36.992978: Steps 45/47: batch_recall = 88.24, batch_ndcg = 67.10 
2025-02-12 22:41:37.099735: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.66 
2025-02-12 22:41:37.099873: Epoch 15/1000, Test: Recall = 0.1094, NDCG = 0.0914  

2025-02-12 22:41:38.287144: Training Step 0/59: batchLoss = 2.8413, diffLoss = 13.8718, kgLoss = 0.0837
2025-02-12 22:41:39.210256: Training Step 1/59: batchLoss = 2.8184, diffLoss = 13.7442, kgLoss = 0.0870
2025-02-12 22:41:40.137385: Training Step 2/59: batchLoss = 2.9302, diffLoss = 14.2881, kgLoss = 0.0907
2025-02-12 22:41:41.074531: Training Step 3/59: batchLoss = 3.1463, diffLoss = 15.3413, kgLoss = 0.0976
2025-02-12 22:41:41.994954: Training Step 4/59: batchLoss = 2.9207, diffLoss = 14.2594, kgLoss = 0.0861
2025-02-12 22:41:42.923952: Training Step 5/59: batchLoss = 2.8792, diffLoss = 14.0224, kgLoss = 0.0934
2025-02-12 22:41:43.857585: Training Step 6/59: batchLoss = 3.0681, diffLoss = 14.9854, kgLoss = 0.0888
2025-02-12 22:41:44.784515: Training Step 7/59: batchLoss = 3.0109, diffLoss = 14.6676, kgLoss = 0.0967
2025-02-12 22:41:45.708826: Training Step 8/59: batchLoss = 2.4691, diffLoss = 12.0368, kgLoss = 0.0772
2025-02-12 22:41:46.627374: Training Step 9/59: batchLoss = 2.9072, diffLoss = 14.1662, kgLoss = 0.0925
2025-02-12 22:41:47.546701: Training Step 10/59: batchLoss = 3.2592, diffLoss = 15.9030, kgLoss = 0.0983
2025-02-12 22:41:48.462787: Training Step 11/59: batchLoss = 3.0194, diffLoss = 14.7175, kgLoss = 0.0948
2025-02-12 22:41:49.374040: Training Step 12/59: batchLoss = 2.6937, diffLoss = 13.1418, kgLoss = 0.0817
2025-02-12 22:41:50.292888: Training Step 13/59: batchLoss = 2.8149, diffLoss = 13.7475, kgLoss = 0.0818
2025-02-12 22:41:51.209706: Training Step 14/59: batchLoss = 2.6843, diffLoss = 13.0922, kgLoss = 0.0823
2025-02-12 22:41:52.129226: Training Step 15/59: batchLoss = 2.7079, diffLoss = 13.2037, kgLoss = 0.0840
2025-02-12 22:41:53.040690: Training Step 16/59: batchLoss = 3.0657, diffLoss = 14.9758, kgLoss = 0.0881
2025-02-12 22:41:53.960080: Training Step 17/59: batchLoss = 2.6444, diffLoss = 12.8886, kgLoss = 0.0833
2025-02-12 22:41:54.877529: Training Step 18/59: batchLoss = 2.9511, diffLoss = 14.4024, kgLoss = 0.0882
2025-02-12 22:41:55.798484: Training Step 19/59: batchLoss = 2.8980, diffLoss = 14.1484, kgLoss = 0.0854
2025-02-12 22:41:56.721671: Training Step 20/59: batchLoss = 3.2652, diffLoss = 15.9544, kgLoss = 0.0929
2025-02-12 22:41:57.644476: Training Step 21/59: batchLoss = 2.9496, diffLoss = 14.4065, kgLoss = 0.0854
2025-02-12 22:41:58.577370: Training Step 22/59: batchLoss = 3.1044, diffLoss = 15.1169, kgLoss = 0.1012
2025-02-12 22:41:59.519375: Training Step 23/59: batchLoss = 2.9790, diffLoss = 14.5517, kgLoss = 0.0858
2025-02-12 22:42:00.456462: Training Step 24/59: batchLoss = 2.9724, diffLoss = 14.5030, kgLoss = 0.0897
2025-02-12 22:42:01.389842: Training Step 25/59: batchLoss = 2.9820, diffLoss = 14.5486, kgLoss = 0.0903
2025-02-12 22:42:02.325741: Training Step 26/59: batchLoss = 3.0515, diffLoss = 14.8919, kgLoss = 0.0914
2025-02-12 22:42:03.261066: Training Step 27/59: batchLoss = 3.1850, diffLoss = 15.5608, kgLoss = 0.0910
2025-02-12 22:42:04.200139: Training Step 28/59: batchLoss = 3.4847, diffLoss = 17.0094, kgLoss = 0.1035
2025-02-12 22:42:05.131905: Training Step 29/59: batchLoss = 2.8831, diffLoss = 14.0673, kgLoss = 0.0870
2025-02-12 22:42:06.063610: Training Step 30/59: batchLoss = 3.3562, diffLoss = 16.4001, kgLoss = 0.0952
2025-02-12 22:42:06.996596: Training Step 31/59: batchLoss = 2.7124, diffLoss = 13.2426, kgLoss = 0.0798
2025-02-12 22:42:07.921023: Training Step 32/59: batchLoss = 2.7211, diffLoss = 13.2813, kgLoss = 0.0810
2025-02-12 22:42:08.850997: Training Step 33/59: batchLoss = 3.0443, diffLoss = 14.8723, kgLoss = 0.0873
2025-02-12 22:42:09.780755: Training Step 34/59: batchLoss = 3.3484, diffLoss = 16.3684, kgLoss = 0.0934
2025-02-12 22:42:10.712353: Training Step 35/59: batchLoss = 2.7269, diffLoss = 13.2959, kgLoss = 0.0847
2025-02-12 22:42:11.635452: Training Step 36/59: batchLoss = 2.8071, diffLoss = 13.6861, kgLoss = 0.0873
2025-02-12 22:42:12.555795: Training Step 37/59: batchLoss = 2.8875, diffLoss = 14.1160, kgLoss = 0.0804
2025-02-12 22:42:13.477060: Training Step 38/59: batchLoss = 3.0588, diffLoss = 14.9279, kgLoss = 0.0916
2025-02-12 22:42:14.399256: Training Step 39/59: batchLoss = 3.1810, diffLoss = 15.5413, kgLoss = 0.0909
2025-02-12 22:42:15.339261: Training Step 40/59: batchLoss = 2.7456, diffLoss = 13.3967, kgLoss = 0.0828
2025-02-12 22:42:16.262708: Training Step 41/59: batchLoss = 3.0594, diffLoss = 14.9366, kgLoss = 0.0901
2025-02-12 22:42:17.189132: Training Step 42/59: batchLoss = 3.0669, diffLoss = 14.9753, kgLoss = 0.0898
2025-02-12 22:42:18.127903: Training Step 43/59: batchLoss = 2.9125, diffLoss = 14.2405, kgLoss = 0.0805
2025-02-12 22:42:19.059221: Training Step 44/59: batchLoss = 2.8386, diffLoss = 13.8559, kgLoss = 0.0843
2025-02-12 22:42:19.987130: Training Step 45/59: batchLoss = 2.8301, diffLoss = 13.8042, kgLoss = 0.0866
2025-02-12 22:42:20.935745: Training Step 46/59: batchLoss = 2.8113, diffLoss = 13.7426, kgLoss = 0.0784
2025-02-12 22:42:21.865281: Training Step 47/59: batchLoss = 3.1191, diffLoss = 15.2437, kgLoss = 0.0879
2025-02-12 22:42:22.797033: Training Step 48/59: batchLoss = 3.2243, diffLoss = 15.7496, kgLoss = 0.0930
2025-02-12 22:42:23.736993: Training Step 49/59: batchLoss = 3.0951, diffLoss = 15.1255, kgLoss = 0.0875
2025-02-12 22:42:24.660072: Training Step 50/59: batchLoss = 3.0447, diffLoss = 14.8777, kgLoss = 0.0865
2025-02-12 22:42:25.591618: Training Step 51/59: batchLoss = 3.0026, diffLoss = 14.6736, kgLoss = 0.0848
2025-02-12 22:42:26.523970: Training Step 52/59: batchLoss = 3.0870, diffLoss = 15.0876, kgLoss = 0.0868
2025-02-12 22:42:27.447316: Training Step 53/59: batchLoss = 2.9890, diffLoss = 14.5665, kgLoss = 0.0946
2025-02-12 22:42:28.376328: Training Step 54/59: batchLoss = 3.0189, diffLoss = 14.7348, kgLoss = 0.0899
2025-02-12 22:42:29.298590: Training Step 55/59: batchLoss = 2.8645, diffLoss = 13.9802, kgLoss = 0.0855
2025-02-12 22:42:30.218028: Training Step 56/59: batchLoss = 3.0162, diffLoss = 14.7498, kgLoss = 0.0828
2025-02-12 22:42:31.062022: Training Step 57/59: batchLoss = 3.1957, diffLoss = 15.6190, kgLoss = 0.0899
2025-02-12 22:42:31.909438: Training Step 58/59: batchLoss = 2.9842, diffLoss = 14.5665, kgLoss = 0.0887
2025-02-12 22:42:32.004614: 
2025-02-12 22:42:32.004945: Epoch 16/1000, Train: epLoss = 0.4383, epDfLoss = 2.1397, epKgLoss = 0.0130  
2025-02-12 22:42:33.484599: Steps 0/47: batch_recall = 30.37, batch_ndcg = 36.08 
2025-02-12 22:42:34.779768: Steps 1/47: batch_recall = 32.31, batch_ndcg = 32.56 
2025-02-12 22:42:36.026454: Steps 2/47: batch_recall = 35.30, batch_ndcg = 37.99 
2025-02-12 22:42:37.283027: Steps 3/47: batch_recall = 39.07, batch_ndcg = 37.45 
2025-02-12 22:42:38.494857: Steps 4/47: batch_recall = 34.85, batch_ndcg = 37.45 
2025-02-12 22:42:39.713550: Steps 5/47: batch_recall = 29.79, batch_ndcg = 31.03 
2025-02-12 22:42:40.920941: Steps 6/47: batch_recall = 35.73, batch_ndcg = 35.22 
2025-02-12 22:42:42.104588: Steps 7/47: batch_recall = 37.21, batch_ndcg = 35.06 
2025-02-12 22:42:43.298215: Steps 8/47: batch_recall = 43.58, batch_ndcg = 41.29 
2025-02-12 22:42:44.450779: Steps 9/47: batch_recall = 40.56, batch_ndcg = 39.37 
2025-02-12 22:42:45.625649: Steps 10/47: batch_recall = 40.89, batch_ndcg = 38.05 
2025-02-12 22:42:46.768944: Steps 11/47: batch_recall = 47.78, batch_ndcg = 41.31 
2025-02-12 22:42:47.912844: Steps 12/47: batch_recall = 45.67, batch_ndcg = 41.35 
2025-02-12 22:42:49.045163: Steps 13/47: batch_recall = 44.70, batch_ndcg = 39.90 
2025-02-12 22:42:50.132433: Steps 14/47: batch_recall = 37.30, batch_ndcg = 34.27 
2025-02-12 22:42:51.214028: Steps 15/47: batch_recall = 52.10, batch_ndcg = 44.10 
2025-02-12 22:42:52.297488: Steps 16/47: batch_recall = 44.85, batch_ndcg = 38.68 
2025-02-12 22:42:53.346698: Steps 17/47: batch_recall = 50.56, batch_ndcg = 42.10 
2025-02-12 22:42:54.411883: Steps 18/47: batch_recall = 47.01, batch_ndcg = 41.25 
2025-02-12 22:42:55.486382: Steps 19/47: batch_recall = 57.62, batch_ndcg = 48.45 
2025-02-12 22:42:56.540440: Steps 20/47: batch_recall = 58.49, batch_ndcg = 50.96 
2025-02-12 22:42:57.589511: Steps 21/47: batch_recall = 61.22, batch_ndcg = 48.86 
2025-02-12 22:42:58.650889: Steps 22/47: batch_recall = 51.14, batch_ndcg = 44.71 
2025-02-12 22:42:59.707721: Steps 23/47: batch_recall = 53.79, batch_ndcg = 43.64 
2025-02-12 22:43:00.769574: Steps 24/47: batch_recall = 57.96, batch_ndcg = 49.50 
2025-02-12 22:43:01.808042: Steps 25/47: batch_recall = 60.96, batch_ndcg = 49.85 
2025-02-12 22:43:02.833039: Steps 26/47: batch_recall = 52.58, batch_ndcg = 45.37 
2025-02-12 22:43:03.878433: Steps 27/47: batch_recall = 56.43, batch_ndcg = 45.62 
2025-02-12 22:43:04.912252: Steps 28/47: batch_recall = 59.51, batch_ndcg = 47.92 
2025-02-12 22:43:05.925830: Steps 29/47: batch_recall = 63.24, batch_ndcg = 49.53 
2025-02-12 22:43:06.929364: Steps 30/47: batch_recall = 70.29, batch_ndcg = 56.81 
2025-02-12 22:43:07.965365: Steps 31/47: batch_recall = 56.12, batch_ndcg = 44.80 
2025-02-12 22:43:08.967860: Steps 32/47: batch_recall = 63.26, batch_ndcg = 55.09 
2025-02-12 22:43:09.983525: Steps 33/47: batch_recall = 73.41, batch_ndcg = 58.17 
2025-02-12 22:43:10.990302: Steps 34/47: batch_recall = 61.54, batch_ndcg = 47.34 
2025-02-12 22:43:11.970151: Steps 35/47: batch_recall = 70.81, batch_ndcg = 55.77 
2025-02-12 22:43:12.952539: Steps 36/47: batch_recall = 74.89, batch_ndcg = 58.81 
2025-02-12 22:43:13.922921: Steps 37/47: batch_recall = 77.22, batch_ndcg = 60.63 
2025-02-12 22:43:14.899069: Steps 38/47: batch_recall = 80.70, batch_ndcg = 63.07 
2025-02-12 22:43:15.887201: Steps 39/47: batch_recall = 80.91, batch_ndcg = 58.11 
2025-02-12 22:43:16.859945: Steps 40/47: batch_recall = 64.73, batch_ndcg = 55.07 
2025-02-12 22:43:17.833530: Steps 41/47: batch_recall = 81.85, batch_ndcg = 63.47 
2025-02-12 22:43:18.804428: Steps 42/47: batch_recall = 79.87, batch_ndcg = 59.46 
2025-02-12 22:43:19.779058: Steps 43/47: batch_recall = 84.07, batch_ndcg = 61.64 
2025-02-12 22:43:20.758223: Steps 44/47: batch_recall = 86.18, batch_ndcg = 63.90 
2025-02-12 22:43:21.696447: Steps 45/47: batch_recall = 90.64, batch_ndcg = 68.45 
2025-02-12 22:43:21.802406: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.64 
2025-02-12 22:43:21.802506: Epoch 16/1000, Test: Recall = 0.1104, NDCG = 0.0926  

2025-02-12 22:43:22.998414: Training Step 0/59: batchLoss = 3.1010, diffLoss = 15.1273, kgLoss = 0.0944
2025-02-12 22:43:23.927318: Training Step 1/59: batchLoss = 2.8829, diffLoss = 14.0463, kgLoss = 0.0921
2025-02-12 22:43:24.854428: Training Step 2/59: batchLoss = 3.1176, diffLoss = 15.2101, kgLoss = 0.0945
2025-02-12 22:43:25.778579: Training Step 3/59: batchLoss = 3.1678, diffLoss = 15.4285, kgLoss = 0.1026
2025-02-12 22:43:26.708818: Training Step 4/59: batchLoss = 3.0875, diffLoss = 15.0395, kgLoss = 0.0996
2025-02-12 22:43:27.629826: Training Step 5/59: batchLoss = 2.6858, diffLoss = 13.1244, kgLoss = 0.0761
2025-02-12 22:43:28.550549: Training Step 6/59: batchLoss = 2.7170, diffLoss = 13.2420, kgLoss = 0.0857
2025-02-12 22:43:29.473974: Training Step 7/59: batchLoss = 3.0594, diffLoss = 14.9225, kgLoss = 0.0936
2025-02-12 22:43:30.400933: Training Step 8/59: batchLoss = 3.0593, diffLoss = 14.9155, kgLoss = 0.0953
2025-02-12 22:43:31.318962: Training Step 9/59: batchLoss = 2.8223, diffLoss = 13.7601, kgLoss = 0.0878
2025-02-12 22:43:32.236386: Training Step 10/59: batchLoss = 2.7778, diffLoss = 13.5574, kgLoss = 0.0829
2025-02-12 22:43:33.166895: Training Step 11/59: batchLoss = 3.3290, diffLoss = 16.2326, kgLoss = 0.1031
2025-02-12 22:43:34.088238: Training Step 12/59: batchLoss = 2.7041, diffLoss = 13.1994, kgLoss = 0.0802
2025-02-12 22:43:35.015915: Training Step 13/59: batchLoss = 3.2588, diffLoss = 15.9165, kgLoss = 0.0944
2025-02-12 22:43:35.948570: Training Step 14/59: batchLoss = 2.8264, diffLoss = 13.7917, kgLoss = 0.0850
2025-02-12 22:43:36.887776: Training Step 15/59: batchLoss = 2.7645, diffLoss = 13.4817, kgLoss = 0.0852
2025-02-12 22:43:37.827529: Training Step 16/59: batchLoss = 3.1283, diffLoss = 15.2996, kgLoss = 0.0854
2025-02-12 22:43:38.765656: Training Step 17/59: batchLoss = 3.0583, diffLoss = 14.9157, kgLoss = 0.0939
2025-02-12 22:43:39.703409: Training Step 18/59: batchLoss = 2.9840, diffLoss = 14.5824, kgLoss = 0.0844
2025-02-12 22:43:40.641418: Training Step 19/59: batchLoss = 3.2443, diffLoss = 15.8018, kgLoss = 0.1049
2025-02-12 22:43:41.575790: Training Step 20/59: batchLoss = 2.8406, diffLoss = 13.8643, kgLoss = 0.0846
2025-02-12 22:43:42.513745: Training Step 21/59: batchLoss = 3.1223, diffLoss = 15.2520, kgLoss = 0.0899
2025-02-12 22:43:43.445379: Training Step 22/59: batchLoss = 3.0342, diffLoss = 14.7949, kgLoss = 0.0940
2025-02-12 22:43:44.368663: Training Step 23/59: batchLoss = 2.9599, diffLoss = 14.4484, kgLoss = 0.0878
2025-02-12 22:43:45.286876: Training Step 24/59: batchLoss = 2.8569, diffLoss = 13.9376, kgLoss = 0.0867
2025-02-12 22:43:46.201446: Training Step 25/59: batchLoss = 2.8072, diffLoss = 13.7100, kgLoss = 0.0815
2025-02-12 22:43:47.114212: Training Step 26/59: batchLoss = 2.7085, diffLoss = 13.2214, kgLoss = 0.0802
2025-02-12 22:43:48.037186: Training Step 27/59: batchLoss = 3.1662, diffLoss = 15.4665, kgLoss = 0.0911
2025-02-12 22:43:48.952479: Training Step 28/59: batchLoss = 3.1143, diffLoss = 15.2116, kgLoss = 0.0900
2025-02-12 22:43:49.867215: Training Step 29/59: batchLoss = 2.6768, diffLoss = 13.0597, kgLoss = 0.0811
2025-02-12 22:43:50.784893: Training Step 30/59: batchLoss = 3.0797, diffLoss = 15.0324, kgLoss = 0.0916
2025-02-12 22:43:51.692419: Training Step 31/59: batchLoss = 2.6241, diffLoss = 12.8018, kgLoss = 0.0797
2025-02-12 22:43:52.613892: Training Step 32/59: batchLoss = 3.0275, diffLoss = 14.8038, kgLoss = 0.0834
2025-02-12 22:43:53.537310: Training Step 33/59: batchLoss = 2.8486, diffLoss = 13.8865, kgLoss = 0.0891
2025-02-12 22:43:54.470397: Training Step 34/59: batchLoss = 2.9083, diffLoss = 14.2225, kgLoss = 0.0798
2025-02-12 22:43:55.407942: Training Step 35/59: batchLoss = 3.0065, diffLoss = 14.6868, kgLoss = 0.0864
2025-02-12 22:43:56.338945: Training Step 36/59: batchLoss = 3.0265, diffLoss = 14.8002, kgLoss = 0.0831
2025-02-12 22:43:57.270227: Training Step 37/59: batchLoss = 2.9989, diffLoss = 14.6326, kgLoss = 0.0904
2025-02-12 22:43:58.198015: Training Step 38/59: batchLoss = 3.3655, diffLoss = 16.4538, kgLoss = 0.0934
2025-02-12 22:43:59.125931: Training Step 39/59: batchLoss = 2.9135, diffLoss = 14.2457, kgLoss = 0.0804
2025-02-12 22:44:00.058898: Training Step 40/59: batchLoss = 2.9066, diffLoss = 14.2213, kgLoss = 0.0779
2025-02-12 22:44:00.994260: Training Step 41/59: batchLoss = 2.9157, diffLoss = 14.2097, kgLoss = 0.0923
2025-02-12 22:44:01.930077: Training Step 42/59: batchLoss = 3.2269, diffLoss = 15.7559, kgLoss = 0.0946
2025-02-12 22:44:02.857090: Training Step 43/59: batchLoss = 2.9960, diffLoss = 14.6328, kgLoss = 0.0868
2025-02-12 22:44:03.789627: Training Step 44/59: batchLoss = 3.4093, diffLoss = 16.6576, kgLoss = 0.0973
2025-02-12 22:44:04.710685: Training Step 45/59: batchLoss = 2.9313, diffLoss = 14.3349, kgLoss = 0.0804
2025-02-12 22:44:05.642128: Training Step 46/59: batchLoss = 3.0944, diffLoss = 15.1176, kgLoss = 0.0886
2025-02-12 22:44:06.563005: Training Step 47/59: batchLoss = 2.9187, diffLoss = 14.2480, kgLoss = 0.0864
2025-02-12 22:44:07.491206: Training Step 48/59: batchLoss = 3.2676, diffLoss = 15.9741, kgLoss = 0.0910
2025-02-12 22:44:08.413965: Training Step 49/59: batchLoss = 3.0215, diffLoss = 14.7817, kgLoss = 0.0815
2025-02-12 22:44:09.334872: Training Step 50/59: batchLoss = 2.7723, diffLoss = 13.5455, kgLoss = 0.0790
2025-02-12 22:44:10.257591: Training Step 51/59: batchLoss = 2.6871, diffLoss = 13.1289, kgLoss = 0.0767
2025-02-12 22:44:11.176528: Training Step 52/59: batchLoss = 3.0835, diffLoss = 15.0370, kgLoss = 0.0951
2025-02-12 22:44:12.101595: Training Step 53/59: batchLoss = 2.7557, diffLoss = 13.4644, kgLoss = 0.0785
2025-02-12 22:44:13.031278: Training Step 54/59: batchLoss = 3.1193, diffLoss = 15.2345, kgLoss = 0.0906
2025-02-12 22:44:13.961027: Training Step 55/59: batchLoss = 3.0689, diffLoss = 14.9966, kgLoss = 0.0870
2025-02-12 22:44:14.907307: Training Step 56/59: batchLoss = 2.7457, diffLoss = 13.3969, kgLoss = 0.0829
2025-02-12 22:44:15.753609: Training Step 57/59: batchLoss = 2.9327, diffLoss = 14.3378, kgLoss = 0.0814
2025-02-12 22:44:16.605897: Training Step 58/59: batchLoss = 3.1997, diffLoss = 15.6324, kgLoss = 0.0916
2025-02-12 22:44:16.704355: 
2025-02-12 22:44:16.704710: Epoch 17/1000, Train: epLoss = 0.4398, epDfLoss = 2.1471, epKgLoss = 0.0130  
2025-02-12 22:44:18.180314: Steps 0/47: batch_recall = 30.54, batch_ndcg = 36.95 
2025-02-12 22:44:19.480990: Steps 1/47: batch_recall = 32.70, batch_ndcg = 33.38 
2025-02-12 22:44:20.749286: Steps 2/47: batch_recall = 35.46, batch_ndcg = 39.35 
2025-02-12 22:44:22.015596: Steps 3/47: batch_recall = 38.55, batch_ndcg = 37.39 
2025-02-12 22:44:23.208583: Steps 4/47: batch_recall = 35.21, batch_ndcg = 37.77 
2025-02-12 22:44:24.414611: Steps 5/47: batch_recall = 29.35, batch_ndcg = 31.52 
2025-02-12 22:44:25.605317: Steps 6/47: batch_recall = 35.53, batch_ndcg = 36.02 
2025-02-12 22:44:26.764849: Steps 7/47: batch_recall = 38.49, batch_ndcg = 36.09 
2025-02-12 22:44:27.935128: Steps 8/47: batch_recall = 44.52, batch_ndcg = 42.71 
2025-02-12 22:44:29.072065: Steps 9/47: batch_recall = 40.45, batch_ndcg = 39.74 
2025-02-12 22:44:30.231057: Steps 10/47: batch_recall = 40.85, batch_ndcg = 38.26 
2025-02-12 22:44:31.376525: Steps 11/47: batch_recall = 48.78, batch_ndcg = 42.40 
2025-02-12 22:44:32.519383: Steps 12/47: batch_recall = 46.23, batch_ndcg = 42.60 
2025-02-12 22:44:33.660288: Steps 13/47: batch_recall = 44.90, batch_ndcg = 39.64 
2025-02-12 22:44:34.772986: Steps 14/47: batch_recall = 37.61, batch_ndcg = 35.58 
2025-02-12 22:44:35.874995: Steps 15/47: batch_recall = 52.41, batch_ndcg = 44.78 
2025-02-12 22:44:36.962840: Steps 16/47: batch_recall = 45.38, batch_ndcg = 39.47 
2025-02-12 22:44:38.027724: Steps 17/47: batch_recall = 52.06, batch_ndcg = 42.99 
2025-02-12 22:44:39.115705: Steps 18/47: batch_recall = 47.90, batch_ndcg = 41.83 
2025-02-12 22:44:40.191105: Steps 19/47: batch_recall = 58.30, batch_ndcg = 49.19 
2025-02-12 22:44:41.250860: Steps 20/47: batch_recall = 59.19, batch_ndcg = 51.48 
2025-02-12 22:44:42.293411: Steps 21/47: batch_recall = 62.20, batch_ndcg = 49.21 
2025-02-12 22:44:43.331128: Steps 22/47: batch_recall = 50.43, batch_ndcg = 44.07 
2025-02-12 22:44:44.357536: Steps 23/47: batch_recall = 55.45, batch_ndcg = 45.85 
2025-02-12 22:44:45.403172: Steps 24/47: batch_recall = 58.59, batch_ndcg = 49.81 
2025-02-12 22:44:46.423161: Steps 25/47: batch_recall = 61.04, batch_ndcg = 50.16 
2025-02-12 22:44:47.432893: Steps 26/47: batch_recall = 52.45, batch_ndcg = 45.69 
2025-02-12 22:44:48.459877: Steps 27/47: batch_recall = 56.90, batch_ndcg = 45.61 
2025-02-12 22:44:49.475733: Steps 28/47: batch_recall = 59.88, batch_ndcg = 48.36 
2025-02-12 22:44:50.480109: Steps 29/47: batch_recall = 64.42, batch_ndcg = 49.92 
2025-02-12 22:44:51.486448: Steps 30/47: batch_recall = 71.33, batch_ndcg = 58.14 
2025-02-12 22:44:52.520214: Steps 31/47: batch_recall = 59.65, batch_ndcg = 46.66 
2025-02-12 22:44:53.538241: Steps 32/47: batch_recall = 62.77, batch_ndcg = 56.03 
2025-02-12 22:44:54.549528: Steps 33/47: batch_recall = 75.83, batch_ndcg = 58.80 
2025-02-12 22:44:55.562975: Steps 34/47: batch_recall = 59.27, batch_ndcg = 46.98 
2025-02-12 22:44:56.556709: Steps 35/47: batch_recall = 71.94, batch_ndcg = 57.22 
2025-02-12 22:44:57.553314: Steps 36/47: batch_recall = 74.78, batch_ndcg = 59.25 
2025-02-12 22:44:58.535388: Steps 37/47: batch_recall = 78.38, batch_ndcg = 61.24 
2025-02-12 22:44:59.517677: Steps 38/47: batch_recall = 82.73, batch_ndcg = 64.69 
2025-02-12 22:45:00.486853: Steps 39/47: batch_recall = 81.29, batch_ndcg = 58.87 
2025-02-12 22:45:01.458080: Steps 40/47: batch_recall = 66.68, batch_ndcg = 56.47 
2025-02-12 22:45:02.417510: Steps 41/47: batch_recall = 82.33, batch_ndcg = 64.30 
2025-02-12 22:45:03.376765: Steps 42/47: batch_recall = 81.63, batch_ndcg = 60.84 
2025-02-12 22:45:04.324833: Steps 43/47: batch_recall = 83.33, batch_ndcg = 61.30 
2025-02-12 22:45:05.284822: Steps 44/47: batch_recall = 85.18, batch_ndcg = 63.72 
2025-02-12 22:45:06.219911: Steps 45/47: batch_recall = 91.29, batch_ndcg = 68.61 
2025-02-12 22:45:06.324983: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.53 
2025-02-12 22:45:06.325127: Epoch 17/1000, Test: Recall = 0.1114, NDCG = 0.0939  

2025-02-12 22:45:07.515526: Training Step 0/59: batchLoss = 3.1069, diffLoss = 15.1346, kgLoss = 0.0999
2025-02-12 22:45:08.438854: Training Step 1/59: batchLoss = 2.9253, diffLoss = 14.2794, kgLoss = 0.0868
2025-02-12 22:45:09.358165: Training Step 2/59: batchLoss = 2.9050, diffLoss = 14.1655, kgLoss = 0.0899
2025-02-12 22:45:10.284289: Training Step 3/59: batchLoss = 2.8865, diffLoss = 14.0914, kgLoss = 0.0853
2025-02-12 22:45:11.219075: Training Step 4/59: batchLoss = 2.7892, diffLoss = 13.6063, kgLoss = 0.0849
2025-02-12 22:45:12.143383: Training Step 5/59: batchLoss = 2.9244, diffLoss = 14.2820, kgLoss = 0.0851
2025-02-12 22:45:13.074692: Training Step 6/59: batchLoss = 3.1467, diffLoss = 15.3472, kgLoss = 0.0966
2025-02-12 22:45:14.005127: Training Step 7/59: batchLoss = 2.8629, diffLoss = 13.9819, kgLoss = 0.0831
2025-02-12 22:45:14.935195: Training Step 8/59: batchLoss = 3.0326, diffLoss = 14.7658, kgLoss = 0.0993
2025-02-12 22:45:15.864933: Training Step 9/59: batchLoss = 3.0054, diffLoss = 14.6514, kgLoss = 0.0939
2025-02-12 22:45:16.796311: Training Step 10/59: batchLoss = 2.4684, diffLoss = 12.0529, kgLoss = 0.0723
2025-02-12 22:45:17.726849: Training Step 11/59: batchLoss = 2.8240, diffLoss = 13.7622, kgLoss = 0.0894
2025-02-12 22:45:18.658163: Training Step 12/59: batchLoss = 2.8765, diffLoss = 14.0336, kgLoss = 0.0872
2025-02-12 22:45:19.597933: Training Step 13/59: batchLoss = 2.8260, diffLoss = 13.7909, kgLoss = 0.0848
2025-02-12 22:45:20.524939: Training Step 14/59: batchLoss = 2.9862, diffLoss = 14.5524, kgLoss = 0.0947
2025-02-12 22:45:21.445963: Training Step 15/59: batchLoss = 2.8659, diffLoss = 13.9805, kgLoss = 0.0872
2025-02-12 22:45:22.371938: Training Step 16/59: batchLoss = 2.7561, diffLoss = 13.4524, kgLoss = 0.0820
2025-02-12 22:45:23.289145: Training Step 17/59: batchLoss = 2.8751, diffLoss = 14.0451, kgLoss = 0.0825
2025-02-12 22:45:24.208643: Training Step 18/59: batchLoss = 2.7520, diffLoss = 13.4335, kgLoss = 0.0816
2025-02-12 22:45:25.126104: Training Step 19/59: batchLoss = 3.0007, diffLoss = 14.6609, kgLoss = 0.0857
2025-02-12 22:45:26.049594: Training Step 20/59: batchLoss = 2.7624, diffLoss = 13.4879, kgLoss = 0.0810
2025-02-12 22:45:26.974638: Training Step 21/59: batchLoss = 2.9609, diffLoss = 14.4351, kgLoss = 0.0923
2025-02-12 22:45:27.901474: Training Step 22/59: batchLoss = 3.1401, diffLoss = 15.3201, kgLoss = 0.0951
2025-02-12 22:45:28.823984: Training Step 23/59: batchLoss = 3.4573, diffLoss = 16.8965, kgLoss = 0.0975
2025-02-12 22:45:29.757066: Training Step 24/59: batchLoss = 2.8135, diffLoss = 13.7278, kgLoss = 0.0849
2025-02-12 22:45:30.692904: Training Step 25/59: batchLoss = 3.4615, diffLoss = 16.9102, kgLoss = 0.0993
2025-02-12 22:45:31.627644: Training Step 26/59: batchLoss = 2.7260, diffLoss = 13.2929, kgLoss = 0.0842
2025-02-12 22:45:32.566977: Training Step 27/59: batchLoss = 2.9221, diffLoss = 14.2627, kgLoss = 0.0869
2025-02-12 22:45:33.501805: Training Step 28/59: batchLoss = 3.3464, diffLoss = 16.3158, kgLoss = 0.1041
2025-02-12 22:45:34.435695: Training Step 29/59: batchLoss = 3.2725, diffLoss = 15.9910, kgLoss = 0.0929
2025-02-12 22:45:35.368356: Training Step 30/59: batchLoss = 2.9077, diffLoss = 14.2007, kgLoss = 0.0845
2025-02-12 22:45:36.319917: Training Step 31/59: batchLoss = 2.9804, diffLoss = 14.5586, kgLoss = 0.0859
2025-02-12 22:45:37.254915: Training Step 32/59: batchLoss = 2.9630, diffLoss = 14.4909, kgLoss = 0.0810
2025-02-12 22:45:38.189426: Training Step 33/59: batchLoss = 2.7426, diffLoss = 13.4171, kgLoss = 0.0739
2025-02-12 22:45:39.114456: Training Step 34/59: batchLoss = 2.8384, diffLoss = 13.8751, kgLoss = 0.0793
2025-02-12 22:45:40.040171: Training Step 35/59: batchLoss = 3.1644, diffLoss = 15.4729, kgLoss = 0.0873
2025-02-12 22:45:40.967457: Training Step 36/59: batchLoss = 3.2987, diffLoss = 16.1187, kgLoss = 0.0937
2025-02-12 22:45:41.890699: Training Step 37/59: batchLoss = 3.1945, diffLoss = 15.5892, kgLoss = 0.0959
2025-02-12 22:45:42.814455: Training Step 38/59: batchLoss = 3.1989, diffLoss = 15.6099, kgLoss = 0.0961
2025-02-12 22:45:43.734863: Training Step 39/59: batchLoss = 3.1285, diffLoss = 15.2964, kgLoss = 0.0865
2025-02-12 22:45:44.666803: Training Step 40/59: batchLoss = 2.9391, diffLoss = 14.3733, kgLoss = 0.0806
2025-02-12 22:45:45.581716: Training Step 41/59: batchLoss = 3.1077, diffLoss = 15.1787, kgLoss = 0.0899
2025-02-12 22:45:46.493806: Training Step 42/59: batchLoss = 3.2003, diffLoss = 15.6238, kgLoss = 0.0944
2025-02-12 22:45:47.413071: Training Step 43/59: batchLoss = 2.8983, diffLoss = 14.1835, kgLoss = 0.0771
2025-02-12 22:45:48.345668: Training Step 44/59: batchLoss = 2.9178, diffLoss = 14.2588, kgLoss = 0.0826
2025-02-12 22:45:49.277233: Training Step 45/59: batchLoss = 2.7403, diffLoss = 13.3891, kgLoss = 0.0781
2025-02-12 22:45:50.211865: Training Step 46/59: batchLoss = 3.0589, diffLoss = 14.9346, kgLoss = 0.0900
2025-02-12 22:45:51.140023: Training Step 47/59: batchLoss = 3.0940, diffLoss = 15.1176, kgLoss = 0.0881
2025-02-12 22:45:52.074400: Training Step 48/59: batchLoss = 3.2545, diffLoss = 15.9034, kgLoss = 0.0922
2025-02-12 22:45:53.014889: Training Step 49/59: batchLoss = 3.1867, diffLoss = 15.5666, kgLoss = 0.0917
2025-02-12 22:45:53.953320: Training Step 50/59: batchLoss = 3.0623, diffLoss = 14.9633, kgLoss = 0.0870
2025-02-12 22:45:54.894225: Training Step 51/59: batchLoss = 2.5753, diffLoss = 12.5812, kgLoss = 0.0738
2025-02-12 22:45:55.831515: Training Step 52/59: batchLoss = 3.0340, diffLoss = 14.8163, kgLoss = 0.0884
2025-02-12 22:45:56.768607: Training Step 53/59: batchLoss = 3.0230, diffLoss = 14.7608, kgLoss = 0.0886
2025-02-12 22:45:57.700979: Training Step 54/59: batchLoss = 3.1623, diffLoss = 15.4320, kgLoss = 0.0949
2025-02-12 22:45:58.626524: Training Step 55/59: batchLoss = 2.9752, diffLoss = 14.5332, kgLoss = 0.0857
2025-02-12 22:45:59.543776: Training Step 56/59: batchLoss = 3.1150, diffLoss = 15.2223, kgLoss = 0.0882
2025-02-12 22:46:00.395819: Training Step 57/59: batchLoss = 2.5543, diffLoss = 12.4668, kgLoss = 0.0762
2025-02-12 22:46:01.244617: Training Step 58/59: batchLoss = 2.9070, diffLoss = 14.2024, kgLoss = 0.0831
2025-02-12 22:46:01.343616: 
2025-02-12 22:46:01.343972: Epoch 18/1000, Train: epLoss = 0.4398, epDfLoss = 2.1471, epKgLoss = 0.0129  
2025-02-12 22:46:02.829882: Steps 0/47: batch_recall = 32.13, batch_ndcg = 39.31 
2025-02-12 22:46:04.124408: Steps 1/47: batch_recall = 33.77, batch_ndcg = 34.71 
2025-02-12 22:46:05.368970: Steps 2/47: batch_recall = 36.32, batch_ndcg = 39.14 
2025-02-12 22:46:06.622609: Steps 3/47: batch_recall = 40.66, batch_ndcg = 39.45 
2025-02-12 22:46:07.818189: Steps 4/47: batch_recall = 35.63, batch_ndcg = 38.23 
2025-02-12 22:46:09.046000: Steps 5/47: batch_recall = 30.73, batch_ndcg = 32.99 
2025-02-12 22:46:10.262391: Steps 6/47: batch_recall = 36.08, batch_ndcg = 36.61 
2025-02-12 22:46:11.453568: Steps 7/47: batch_recall = 39.78, batch_ndcg = 37.47 
2025-02-12 22:46:12.642860: Steps 8/47: batch_recall = 45.00, batch_ndcg = 43.79 
2025-02-12 22:46:13.798174: Steps 9/47: batch_recall = 42.46, batch_ndcg = 41.16 
2025-02-12 22:46:14.966261: Steps 10/47: batch_recall = 41.34, batch_ndcg = 38.20 
2025-02-12 22:46:16.116322: Steps 11/47: batch_recall = 48.40, batch_ndcg = 43.06 
2025-02-12 22:46:17.267761: Steps 12/47: batch_recall = 47.71, batch_ndcg = 43.37 
2025-02-12 22:46:18.401814: Steps 13/47: batch_recall = 45.09, batch_ndcg = 40.59 
2025-02-12 22:46:19.491634: Steps 14/47: batch_recall = 37.27, batch_ndcg = 35.54 
2025-02-12 22:46:20.586831: Steps 15/47: batch_recall = 52.45, batch_ndcg = 45.50 
2025-02-12 22:46:21.666276: Steps 16/47: batch_recall = 45.99, batch_ndcg = 40.21 
2025-02-12 22:46:22.703299: Steps 17/47: batch_recall = 52.94, batch_ndcg = 43.77 
2025-02-12 22:46:23.771978: Steps 18/47: batch_recall = 48.52, batch_ndcg = 41.98 
2025-02-12 22:46:24.826030: Steps 19/47: batch_recall = 56.74, batch_ndcg = 49.06 
2025-02-12 22:46:25.867779: Steps 20/47: batch_recall = 59.54, batch_ndcg = 51.71 
2025-02-12 22:46:26.920540: Steps 21/47: batch_recall = 61.58, batch_ndcg = 49.94 
2025-02-12 22:46:27.986704: Steps 22/47: batch_recall = 52.66, batch_ndcg = 45.75 
2025-02-12 22:46:29.052461: Steps 23/47: batch_recall = 56.81, batch_ndcg = 47.73 
2025-02-12 22:46:30.126301: Steps 24/47: batch_recall = 59.57, batch_ndcg = 50.27 
2025-02-12 22:46:31.178323: Steps 25/47: batch_recall = 62.13, batch_ndcg = 50.58 
2025-02-12 22:46:32.201151: Steps 26/47: batch_recall = 54.18, batch_ndcg = 47.28 
2025-02-12 22:46:33.246049: Steps 27/47: batch_recall = 56.78, batch_ndcg = 45.81 
2025-02-12 22:46:34.284481: Steps 28/47: batch_recall = 63.72, batch_ndcg = 50.44 
2025-02-12 22:46:35.315278: Steps 29/47: batch_recall = 64.37, batch_ndcg = 50.62 
2025-02-12 22:46:36.322752: Steps 30/47: batch_recall = 70.40, batch_ndcg = 58.14 
2025-02-12 22:46:37.352673: Steps 31/47: batch_recall = 59.61, batch_ndcg = 46.78 
2025-02-12 22:46:38.373687: Steps 32/47: batch_recall = 64.65, batch_ndcg = 57.65 
2025-02-12 22:46:39.391876: Steps 33/47: batch_recall = 75.65, batch_ndcg = 59.05 
2025-02-12 22:46:40.398459: Steps 34/47: batch_recall = 60.66, batch_ndcg = 47.70 
2025-02-12 22:46:41.388729: Steps 35/47: batch_recall = 72.76, batch_ndcg = 57.92 
2025-02-12 22:46:42.373614: Steps 36/47: batch_recall = 74.37, batch_ndcg = 59.75 
2025-02-12 22:46:43.349782: Steps 37/47: batch_recall = 77.77, batch_ndcg = 62.15 
2025-02-12 22:46:44.338188: Steps 38/47: batch_recall = 84.77, batch_ndcg = 65.76 
2025-02-12 22:46:45.304914: Steps 39/47: batch_recall = 82.10, batch_ndcg = 60.46 
2025-02-12 22:46:46.277661: Steps 40/47: batch_recall = 69.40, batch_ndcg = 57.94 
2025-02-12 22:46:47.242575: Steps 41/47: batch_recall = 83.38, batch_ndcg = 64.89 
2025-02-12 22:46:48.207882: Steps 42/47: batch_recall = 78.59, batch_ndcg = 59.77 
2025-02-12 22:46:49.171289: Steps 43/47: batch_recall = 85.62, batch_ndcg = 62.51 
2025-02-12 22:46:50.134212: Steps 44/47: batch_recall = 84.95, batch_ndcg = 63.60 
2025-02-12 22:46:51.081919: Steps 45/47: batch_recall = 89.77, batch_ndcg = 68.38 
2025-02-12 22:46:51.184592: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.47 
2025-02-12 22:46:51.184723: Epoch 18/1000, Test: Recall = 0.1127, NDCG = 0.0954  

2025-02-12 22:46:52.363539: Training Step 0/59: batchLoss = 2.8040, diffLoss = 13.6628, kgLoss = 0.0893
2025-02-12 22:46:53.274624: Training Step 1/59: batchLoss = 2.7533, diffLoss = 13.4187, kgLoss = 0.0869
2025-02-12 22:46:54.192439: Training Step 2/59: batchLoss = 2.8325, diffLoss = 13.8158, kgLoss = 0.0867
2025-02-12 22:46:55.107045: Training Step 3/59: batchLoss = 2.9891, diffLoss = 14.5890, kgLoss = 0.0891
2025-02-12 22:46:56.030019: Training Step 4/59: batchLoss = 2.8332, diffLoss = 13.8449, kgLoss = 0.0803
2025-02-12 22:46:56.948599: Training Step 5/59: batchLoss = 2.8937, diffLoss = 14.1102, kgLoss = 0.0896
2025-02-12 22:46:57.860478: Training Step 6/59: batchLoss = 2.7282, diffLoss = 13.3244, kgLoss = 0.0792
2025-02-12 22:46:58.771138: Training Step 7/59: batchLoss = 3.1695, diffLoss = 15.4327, kgLoss = 0.1037
2025-02-12 22:46:59.683122: Training Step 8/59: batchLoss = 2.9558, diffLoss = 14.4342, kgLoss = 0.0861
2025-02-12 22:47:00.609185: Training Step 9/59: batchLoss = 2.9979, diffLoss = 14.6091, kgLoss = 0.0951
2025-02-12 22:47:01.537490: Training Step 10/59: batchLoss = 2.5812, diffLoss = 12.5936, kgLoss = 0.0781
2025-02-12 22:47:02.470587: Training Step 11/59: batchLoss = 2.8100, diffLoss = 13.7215, kgLoss = 0.0821
2025-02-12 22:47:03.393105: Training Step 12/59: batchLoss = 2.7961, diffLoss = 13.6486, kgLoss = 0.0830
2025-02-12 22:47:04.319163: Training Step 13/59: batchLoss = 2.9558, diffLoss = 14.4351, kgLoss = 0.0860
2025-02-12 22:47:05.235993: Training Step 14/59: batchLoss = 2.7052, diffLoss = 13.1810, kgLoss = 0.0863
2025-02-12 22:47:06.158448: Training Step 15/59: batchLoss = 3.1313, diffLoss = 15.2757, kgLoss = 0.0952
2025-02-12 22:47:07.081779: Training Step 16/59: batchLoss = 2.6790, diffLoss = 13.0823, kgLoss = 0.0782
2025-02-12 22:47:08.004421: Training Step 17/59: batchLoss = 2.9174, diffLoss = 14.1928, kgLoss = 0.0986
2025-02-12 22:47:08.929193: Training Step 18/59: batchLoss = 3.1165, diffLoss = 15.2151, kgLoss = 0.0918
2025-02-12 22:47:09.855671: Training Step 19/59: batchLoss = 3.2790, diffLoss = 16.0085, kgLoss = 0.0967
2025-02-12 22:47:10.787209: Training Step 20/59: batchLoss = 2.8362, diffLoss = 13.8418, kgLoss = 0.0848
2025-02-12 22:47:11.713685: Training Step 21/59: batchLoss = 2.6806, diffLoss = 13.0868, kgLoss = 0.0791
2025-02-12 22:47:12.647352: Training Step 22/59: batchLoss = 3.0568, diffLoss = 14.9190, kgLoss = 0.0913
2025-02-12 22:47:13.574674: Training Step 23/59: batchLoss = 2.8185, diffLoss = 13.7403, kgLoss = 0.0881
2025-02-12 22:47:14.493744: Training Step 24/59: batchLoss = 3.2043, diffLoss = 15.6564, kgLoss = 0.0912
2025-02-12 22:47:15.433415: Training Step 25/59: batchLoss = 2.9604, diffLoss = 14.4502, kgLoss = 0.0879
2025-02-12 22:47:16.352611: Training Step 26/59: batchLoss = 2.7688, diffLoss = 13.5362, kgLoss = 0.0770
2025-02-12 22:47:17.276945: Training Step 27/59: batchLoss = 2.7157, diffLoss = 13.2726, kgLoss = 0.0765
2025-02-12 22:47:18.197170: Training Step 28/59: batchLoss = 3.0277, diffLoss = 14.7940, kgLoss = 0.0862
2025-02-12 22:47:19.114756: Training Step 29/59: batchLoss = 3.0176, diffLoss = 14.7454, kgLoss = 0.0856
2025-02-12 22:47:20.039277: Training Step 30/59: batchLoss = 3.0787, diffLoss = 15.0454, kgLoss = 0.0870
2025-02-12 22:47:20.961472: Training Step 31/59: batchLoss = 2.9764, diffLoss = 14.5393, kgLoss = 0.0856
2025-02-12 22:47:21.891701: Training Step 32/59: batchLoss = 2.9540, diffLoss = 14.4358, kgLoss = 0.0836
2025-02-12 22:47:22.812214: Training Step 33/59: batchLoss = 3.2293, diffLoss = 15.7820, kgLoss = 0.0911
2025-02-12 22:47:23.740697: Training Step 34/59: batchLoss = 3.0063, diffLoss = 14.6900, kgLoss = 0.0854
2025-02-12 22:47:24.679469: Training Step 35/59: batchLoss = 3.0946, diffLoss = 15.0838, kgLoss = 0.0974
2025-02-12 22:47:25.613007: Training Step 36/59: batchLoss = 3.0459, diffLoss = 14.8869, kgLoss = 0.0856
2025-02-12 22:47:26.549812: Training Step 37/59: batchLoss = 2.9553, diffLoss = 14.4198, kgLoss = 0.0891
2025-02-12 22:47:27.484573: Training Step 38/59: batchLoss = 2.8411, diffLoss = 13.8814, kgLoss = 0.0810
2025-02-12 22:47:28.419448: Training Step 39/59: batchLoss = 3.1033, diffLoss = 15.1379, kgLoss = 0.0947
2025-02-12 22:47:29.353336: Training Step 40/59: batchLoss = 2.9008, diffLoss = 14.1670, kgLoss = 0.0842
2025-02-12 22:47:30.287683: Training Step 41/59: batchLoss = 2.9110, diffLoss = 14.1963, kgLoss = 0.0897
2025-02-12 22:47:31.214546: Training Step 42/59: batchLoss = 3.2245, diffLoss = 15.7443, kgLoss = 0.0945
2025-02-12 22:47:32.144930: Training Step 43/59: batchLoss = 3.1871, diffLoss = 15.5845, kgLoss = 0.0878
2025-02-12 22:47:33.077959: Training Step 44/59: batchLoss = 3.2986, diffLoss = 16.1101, kgLoss = 0.0957
2025-02-12 22:47:34.015685: Training Step 45/59: batchLoss = 2.8629, diffLoss = 13.9773, kgLoss = 0.0843
2025-02-12 22:47:34.930181: Training Step 46/59: batchLoss = 2.7750, diffLoss = 13.5673, kgLoss = 0.0769
2025-02-12 22:47:35.851282: Training Step 47/59: batchLoss = 2.9479, diffLoss = 14.4118, kgLoss = 0.0819
2025-02-12 22:47:36.774113: Training Step 48/59: batchLoss = 3.1181, diffLoss = 15.2570, kgLoss = 0.0834
2025-02-12 22:47:37.702374: Training Step 49/59: batchLoss = 3.1731, diffLoss = 15.4912, kgLoss = 0.0936
2025-02-12 22:47:38.627259: Training Step 50/59: batchLoss = 3.0131, diffLoss = 14.7182, kgLoss = 0.0868
2025-02-12 22:47:39.551110: Training Step 51/59: batchLoss = 3.1156, diffLoss = 15.1790, kgLoss = 0.0998
2025-02-12 22:47:40.472953: Training Step 52/59: batchLoss = 3.0379, diffLoss = 14.8023, kgLoss = 0.0968
2025-02-12 22:47:41.391595: Training Step 53/59: batchLoss = 3.1685, diffLoss = 15.4812, kgLoss = 0.0903
2025-02-12 22:47:42.313456: Training Step 54/59: batchLoss = 3.2065, diffLoss = 15.6704, kgLoss = 0.0906
2025-02-12 22:47:43.256612: Training Step 55/59: batchLoss = 3.0585, diffLoss = 14.9539, kgLoss = 0.0847
2025-02-12 22:47:44.184675: Training Step 56/59: batchLoss = 3.0570, diffLoss = 14.9537, kgLoss = 0.0829
2025-02-12 22:47:45.030588: Training Step 57/59: batchLoss = 2.8203, diffLoss = 13.7867, kgLoss = 0.0787
2025-02-12 22:47:45.897667: Training Step 58/59: batchLoss = 2.8183, diffLoss = 13.7884, kgLoss = 0.0758
2025-02-12 22:47:45.999711: 
2025-02-12 22:47:46.000438: Epoch 19/1000, Train: epLoss = 0.4375, epDfLoss = 2.1360, epKgLoss = 0.0129  
2025-02-12 22:47:47.483273: Steps 0/47: batch_recall = 31.85, batch_ndcg = 39.25 
2025-02-12 22:47:48.802216: Steps 1/47: batch_recall = 33.13, batch_ndcg = 34.84 
2025-02-12 22:47:50.067344: Steps 2/47: batch_recall = 36.80, batch_ndcg = 40.66 
2025-02-12 22:47:51.325913: Steps 3/47: batch_recall = 40.65, batch_ndcg = 39.85 
2025-02-12 22:47:52.527496: Steps 4/47: batch_recall = 35.26, batch_ndcg = 38.79 
2025-02-12 22:47:53.742375: Steps 5/47: batch_recall = 30.12, batch_ndcg = 32.54 
2025-02-12 22:47:54.928958: Steps 6/47: batch_recall = 37.85, batch_ndcg = 38.00 
2025-02-12 22:47:56.089075: Steps 7/47: batch_recall = 39.97, batch_ndcg = 38.03 
2025-02-12 22:47:57.260929: Steps 8/47: batch_recall = 44.22, batch_ndcg = 43.53 
2025-02-12 22:47:58.413043: Steps 9/47: batch_recall = 41.96, batch_ndcg = 41.72 
2025-02-12 22:47:59.565307: Steps 10/47: batch_recall = 41.67, batch_ndcg = 39.00 
2025-02-12 22:48:00.690876: Steps 11/47: batch_recall = 49.56, batch_ndcg = 43.83 
2025-02-12 22:48:01.833306: Steps 12/47: batch_recall = 48.10, batch_ndcg = 43.66 
2025-02-12 22:48:02.977696: Steps 13/47: batch_recall = 45.45, batch_ndcg = 40.62 
2025-02-12 22:48:04.072026: Steps 14/47: batch_recall = 38.43, batch_ndcg = 36.64 
2025-02-12 22:48:05.166858: Steps 15/47: batch_recall = 52.96, batch_ndcg = 46.34 
2025-02-12 22:48:06.253687: Steps 16/47: batch_recall = 44.28, batch_ndcg = 39.40 
2025-02-12 22:48:07.311031: Steps 17/47: batch_recall = 54.20, batch_ndcg = 45.55 
2025-02-12 22:48:08.395790: Steps 18/47: batch_recall = 47.59, batch_ndcg = 41.76 
2025-02-12 22:48:09.468700: Steps 19/47: batch_recall = 57.51, batch_ndcg = 50.15 
2025-02-12 22:48:10.518892: Steps 20/47: batch_recall = 60.46, batch_ndcg = 53.31 
2025-02-12 22:48:11.554767: Steps 21/47: batch_recall = 62.40, batch_ndcg = 50.10 
2025-02-12 22:48:12.599706: Steps 22/47: batch_recall = 51.52, batch_ndcg = 46.12 
2025-02-12 22:48:13.634572: Steps 23/47: batch_recall = 58.01, batch_ndcg = 48.08 
2025-02-12 22:48:14.699787: Steps 24/47: batch_recall = 61.25, batch_ndcg = 51.54 
2025-02-12 22:48:15.723920: Steps 25/47: batch_recall = 61.47, batch_ndcg = 50.86 
2025-02-12 22:48:16.728824: Steps 26/47: batch_recall = 54.27, batch_ndcg = 47.37 
2025-02-12 22:48:17.751931: Steps 27/47: batch_recall = 58.71, batch_ndcg = 46.91 
2025-02-12 22:48:18.753438: Steps 28/47: batch_recall = 64.60, batch_ndcg = 52.53 
2025-02-12 22:48:19.748079: Steps 29/47: batch_recall = 65.54, batch_ndcg = 51.76 
2025-02-12 22:48:20.749372: Steps 30/47: batch_recall = 72.05, batch_ndcg = 59.62 
2025-02-12 22:48:21.773187: Steps 31/47: batch_recall = 59.17, batch_ndcg = 46.48 
2025-02-12 22:48:22.788658: Steps 32/47: batch_recall = 66.35, batch_ndcg = 58.92 
2025-02-12 22:48:23.800370: Steps 33/47: batch_recall = 75.44, batch_ndcg = 59.74 
2025-02-12 22:48:24.813518: Steps 34/47: batch_recall = 59.20, batch_ndcg = 47.43 
2025-02-12 22:48:25.807883: Steps 35/47: batch_recall = 73.01, batch_ndcg = 58.39 
2025-02-12 22:48:26.803859: Steps 36/47: batch_recall = 73.76, batch_ndcg = 60.00 
2025-02-12 22:48:27.789572: Steps 37/47: batch_recall = 78.38, batch_ndcg = 63.67 
2025-02-12 22:48:28.777949: Steps 38/47: batch_recall = 85.10, batch_ndcg = 66.74 
2025-02-12 22:48:29.753170: Steps 39/47: batch_recall = 83.28, batch_ndcg = 61.87 
2025-02-12 22:48:30.720211: Steps 40/47: batch_recall = 69.97, batch_ndcg = 59.14 
2025-02-12 22:48:31.681555: Steps 41/47: batch_recall = 81.93, batch_ndcg = 64.93 
2025-02-12 22:48:32.634714: Steps 42/47: batch_recall = 80.81, batch_ndcg = 60.90 
2025-02-12 22:48:33.623438: Steps 43/47: batch_recall = 85.68, batch_ndcg = 63.13 
2025-02-12 22:48:34.590054: Steps 44/47: batch_recall = 85.40, batch_ndcg = 65.26 
2025-02-12 22:48:35.525298: Steps 45/47: batch_recall = 91.73, batch_ndcg = 70.20 
2025-02-12 22:48:35.630610: Steps 46/47: batch_recall = 2.25, batch_ndcg = 1.67 
2025-02-12 22:48:35.630735: Epoch 19/1000, Test: Recall = 0.1134, NDCG = 0.0968  

2025-02-12 22:48:36.832203: Training Step 0/59: batchLoss = 2.8875, diffLoss = 14.0803, kgLoss = 0.0893
2025-02-12 22:48:37.766218: Training Step 1/59: batchLoss = 2.7979, diffLoss = 13.6044, kgLoss = 0.0963
2025-02-12 22:48:38.690480: Training Step 2/59: batchLoss = 2.9482, diffLoss = 14.3714, kgLoss = 0.0924
2025-02-12 22:48:39.621695: Training Step 3/59: batchLoss = 2.9284, diffLoss = 14.2895, kgLoss = 0.0882
2025-02-12 22:48:40.563149: Training Step 4/59: batchLoss = 2.9589, diffLoss = 14.4536, kgLoss = 0.0853
2025-02-12 22:48:41.505182: Training Step 5/59: batchLoss = 2.8650, diffLoss = 13.9725, kgLoss = 0.0881
2025-02-12 22:48:42.439149: Training Step 6/59: batchLoss = 2.8029, diffLoss = 13.6777, kgLoss = 0.0841
2025-02-12 22:48:43.375964: Training Step 7/59: batchLoss = 2.9560, diffLoss = 14.3963, kgLoss = 0.0960
2025-02-12 22:48:44.310601: Training Step 8/59: batchLoss = 2.9480, diffLoss = 14.4050, kgLoss = 0.0837
2025-02-12 22:48:45.252241: Training Step 9/59: batchLoss = 2.9992, diffLoss = 14.6190, kgLoss = 0.0943
2025-02-12 22:48:46.189129: Training Step 10/59: batchLoss = 3.3917, diffLoss = 16.5496, kgLoss = 0.1022
2025-02-12 22:48:47.136137: Training Step 11/59: batchLoss = 2.8896, diffLoss = 14.0839, kgLoss = 0.0910
2025-02-12 22:48:48.070096: Training Step 12/59: batchLoss = 2.7012, diffLoss = 13.1812, kgLoss = 0.0812
2025-02-12 22:48:49.010538: Training Step 13/59: batchLoss = 2.9233, diffLoss = 14.2773, kgLoss = 0.0848
2025-02-12 22:48:49.940362: Training Step 14/59: batchLoss = 2.8884, diffLoss = 14.1048, kgLoss = 0.0843
2025-02-12 22:48:50.875082: Training Step 15/59: batchLoss = 2.7915, diffLoss = 13.6203, kgLoss = 0.0843
2025-02-12 22:48:51.800240: Training Step 16/59: batchLoss = 3.2592, diffLoss = 15.9080, kgLoss = 0.0970
2025-02-12 22:48:52.732254: Training Step 17/59: batchLoss = 3.1729, diffLoss = 15.4931, kgLoss = 0.0928
2025-02-12 22:48:53.663128: Training Step 18/59: batchLoss = 3.0152, diffLoss = 14.7233, kgLoss = 0.0881
2025-02-12 22:48:54.593870: Training Step 19/59: batchLoss = 2.7360, diffLoss = 13.3553, kgLoss = 0.0811
2025-02-12 22:48:55.517235: Training Step 20/59: batchLoss = 2.9248, diffLoss = 14.2567, kgLoss = 0.0918
2025-02-12 22:48:56.440775: Training Step 21/59: batchLoss = 2.8803, diffLoss = 14.0701, kgLoss = 0.0828
2025-02-12 22:48:57.387725: Training Step 22/59: batchLoss = 2.5626, diffLoss = 12.5124, kgLoss = 0.0752
2025-02-12 22:48:58.308796: Training Step 23/59: batchLoss = 2.9647, diffLoss = 14.4827, kgLoss = 0.0852
2025-02-12 22:48:59.242017: Training Step 24/59: batchLoss = 2.9667, diffLoss = 14.4856, kgLoss = 0.0870
2025-02-12 22:49:00.175225: Training Step 25/59: batchLoss = 2.8457, diffLoss = 13.8905, kgLoss = 0.0844
2025-02-12 22:49:01.117415: Training Step 26/59: batchLoss = 2.5709, diffLoss = 12.5511, kgLoss = 0.0759
2025-02-12 22:49:02.052487: Training Step 27/59: batchLoss = 3.0203, diffLoss = 14.7757, kgLoss = 0.0814
2025-02-12 22:49:02.993620: Training Step 28/59: batchLoss = 2.6642, diffLoss = 13.0150, kgLoss = 0.0765
2025-02-12 22:49:03.931413: Training Step 29/59: batchLoss = 3.1110, diffLoss = 15.1840, kgLoss = 0.0927
2025-02-12 22:49:04.883128: Training Step 30/59: batchLoss = 3.0220, diffLoss = 14.7575, kgLoss = 0.0881
2025-02-12 22:49:05.824469: Training Step 31/59: batchLoss = 2.9315, diffLoss = 14.3255, kgLoss = 0.0830
2025-02-12 22:49:06.757190: Training Step 32/59: batchLoss = 2.9441, diffLoss = 14.3811, kgLoss = 0.0849
2025-02-12 22:49:07.690783: Training Step 33/59: batchLoss = 2.9910, diffLoss = 14.5845, kgLoss = 0.0927
2025-02-12 22:49:08.625031: Training Step 34/59: batchLoss = 3.1916, diffLoss = 15.6109, kgLoss = 0.0867
2025-02-12 22:49:09.551722: Training Step 35/59: batchLoss = 3.1051, diffLoss = 15.1773, kgLoss = 0.0870
2025-02-12 22:49:10.477176: Training Step 36/59: batchLoss = 3.2729, diffLoss = 15.9983, kgLoss = 0.0916
2025-02-12 22:49:11.409590: Training Step 37/59: batchLoss = 2.7700, diffLoss = 13.5292, kgLoss = 0.0803
2025-02-12 22:49:12.328766: Training Step 38/59: batchLoss = 2.8594, diffLoss = 13.9647, kgLoss = 0.0831
2025-02-12 22:49:13.257487: Training Step 39/59: batchLoss = 2.8760, diffLoss = 14.0632, kgLoss = 0.0791
2025-02-12 22:49:14.184666: Training Step 40/59: batchLoss = 3.0401, diffLoss = 14.8578, kgLoss = 0.0857
2025-02-12 22:49:15.112176: Training Step 41/59: batchLoss = 3.2814, diffLoss = 16.0015, kgLoss = 0.1014
2025-02-12 22:49:16.034872: Training Step 42/59: batchLoss = 2.8721, diffLoss = 14.0274, kgLoss = 0.0833
2025-02-12 22:49:16.958441: Training Step 43/59: batchLoss = 3.0260, diffLoss = 14.7624, kgLoss = 0.0919
2025-02-12 22:49:17.884068: Training Step 44/59: batchLoss = 3.0599, diffLoss = 14.9531, kgLoss = 0.0867
2025-02-12 22:49:18.816149: Training Step 45/59: batchLoss = 3.0625, diffLoss = 14.9694, kgLoss = 0.0858
2025-02-12 22:49:19.753604: Training Step 46/59: batchLoss = 3.3462, diffLoss = 16.3621, kgLoss = 0.0922
2025-02-12 22:49:20.689848: Training Step 47/59: batchLoss = 2.9872, diffLoss = 14.5982, kgLoss = 0.0845
2025-02-12 22:49:21.620843: Training Step 48/59: batchLoss = 3.1771, diffLoss = 15.5302, kgLoss = 0.0888
2025-02-12 22:49:22.558265: Training Step 49/59: batchLoss = 3.1213, diffLoss = 15.2576, kgLoss = 0.0873
2025-02-12 22:49:23.492994: Training Step 50/59: batchLoss = 2.7644, diffLoss = 13.5045, kgLoss = 0.0793
2025-02-12 22:49:24.434091: Training Step 51/59: batchLoss = 2.9909, diffLoss = 14.6280, kgLoss = 0.0816
2025-02-12 22:49:25.378101: Training Step 52/59: batchLoss = 3.1693, diffLoss = 15.4895, kgLoss = 0.0893
2025-02-12 22:49:26.315918: Training Step 53/59: batchLoss = 3.0735, diffLoss = 15.0292, kgLoss = 0.0846
2025-02-12 22:49:27.255402: Training Step 54/59: batchLoss = 2.7419, diffLoss = 13.3985, kgLoss = 0.0778
2025-02-12 22:49:28.190450: Training Step 55/59: batchLoss = 3.1132, diffLoss = 15.2320, kgLoss = 0.0835
2025-02-12 22:49:29.121748: Training Step 56/59: batchLoss = 3.0815, diffLoss = 15.0721, kgLoss = 0.0838
2025-02-12 22:49:29.963089: Training Step 57/59: batchLoss = 3.6583, diffLoss = 17.8950, kgLoss = 0.0991
2025-02-12 22:49:30.809833: Training Step 58/59: batchLoss = 3.2751, diffLoss = 16.0073, kgLoss = 0.0920
2025-02-12 22:49:30.903907: 
2025-02-12 22:49:30.904250: Epoch 20/1000, Train: epLoss = 0.4404, epDfLoss = 2.1509, epKgLoss = 0.0128  
2025-02-12 22:49:32.379009: Steps 0/47: batch_recall = 32.95, batch_ndcg = 39.83 
2025-02-12 22:49:33.664967: Steps 1/47: batch_recall = 32.76, batch_ndcg = 35.15 
2025-02-12 22:49:34.908921: Steps 2/47: batch_recall = 36.57, batch_ndcg = 40.34 
2025-02-12 22:49:36.157302: Steps 3/47: batch_recall = 40.99, batch_ndcg = 40.16 
2025-02-12 22:49:37.361980: Steps 4/47: batch_recall = 37.30, batch_ndcg = 40.46 
2025-02-12 22:49:38.594129: Steps 5/47: batch_recall = 31.03, batch_ndcg = 33.95 
2025-02-12 22:49:39.803559: Steps 6/47: batch_recall = 36.34, batch_ndcg = 38.38 
2025-02-12 22:49:40.993183: Steps 7/47: batch_recall = 41.55, batch_ndcg = 39.20 
2025-02-12 22:49:42.175013: Steps 8/47: batch_recall = 45.17, batch_ndcg = 45.24 
2025-02-12 22:49:43.326208: Steps 9/47: batch_recall = 42.72, batch_ndcg = 42.04 
2025-02-12 22:49:44.498558: Steps 10/47: batch_recall = 42.51, batch_ndcg = 39.06 
2025-02-12 22:49:45.655726: Steps 11/47: batch_recall = 51.12, batch_ndcg = 45.87 
2025-02-12 22:49:46.808591: Steps 12/47: batch_recall = 47.54, batch_ndcg = 43.51 
2025-02-12 22:49:47.945520: Steps 13/47: batch_recall = 47.25, batch_ndcg = 42.64 
2025-02-12 22:49:49.042833: Steps 14/47: batch_recall = 40.08, batch_ndcg = 37.62 
2025-02-12 22:49:50.141546: Steps 15/47: batch_recall = 53.92, batch_ndcg = 47.39 
2025-02-12 22:49:51.222335: Steps 16/47: batch_recall = 46.88, batch_ndcg = 41.10 
2025-02-12 22:49:52.276737: Steps 17/47: batch_recall = 53.55, batch_ndcg = 45.92 
2025-02-12 22:49:53.351963: Steps 18/47: batch_recall = 48.98, batch_ndcg = 43.34 
2025-02-12 22:49:54.416858: Steps 19/47: batch_recall = 58.96, batch_ndcg = 51.65 
2025-02-12 22:49:55.469600: Steps 20/47: batch_recall = 61.22, batch_ndcg = 54.92 
2025-02-12 22:49:56.527640: Steps 21/47: batch_recall = 61.21, batch_ndcg = 50.42 
2025-02-12 22:49:57.591211: Steps 22/47: batch_recall = 52.08, batch_ndcg = 47.27 
2025-02-12 22:49:58.649983: Steps 23/47: batch_recall = 58.04, batch_ndcg = 48.72 
2025-02-12 22:49:59.723482: Steps 24/47: batch_recall = 63.08, batch_ndcg = 52.32 
2025-02-12 22:50:00.775270: Steps 25/47: batch_recall = 61.93, batch_ndcg = 51.99 
2025-02-12 22:50:01.801420: Steps 26/47: batch_recall = 54.78, batch_ndcg = 47.92 
2025-02-12 22:50:02.841862: Steps 27/47: batch_recall = 56.58, batch_ndcg = 47.19 
2025-02-12 22:50:03.878307: Steps 28/47: batch_recall = 67.53, batch_ndcg = 54.26 
2025-02-12 22:50:04.900916: Steps 29/47: batch_recall = 65.89, batch_ndcg = 52.20 
2025-02-12 22:50:05.913131: Steps 30/47: batch_recall = 71.06, batch_ndcg = 59.14 
2025-02-12 22:50:06.940490: Steps 31/47: batch_recall = 59.93, batch_ndcg = 47.80 
2025-02-12 22:50:07.947905: Steps 32/47: batch_recall = 67.21, batch_ndcg = 59.12 
2025-02-12 22:50:08.962102: Steps 33/47: batch_recall = 76.99, batch_ndcg = 62.24 
2025-02-12 22:50:09.971981: Steps 34/47: batch_recall = 61.07, batch_ndcg = 48.78 
2025-02-12 22:50:10.952547: Steps 35/47: batch_recall = 74.19, batch_ndcg = 59.69 
2025-02-12 22:50:11.946823: Steps 36/47: batch_recall = 76.55, batch_ndcg = 61.57 
2025-02-12 22:50:12.917558: Steps 37/47: batch_recall = 79.00, batch_ndcg = 64.28 
2025-02-12 22:50:13.891406: Steps 38/47: batch_recall = 87.78, batch_ndcg = 68.49 
2025-02-12 22:50:14.863285: Steps 39/47: batch_recall = 82.92, batch_ndcg = 62.74 
2025-02-12 22:50:15.830110: Steps 40/47: batch_recall = 69.21, batch_ndcg = 59.24 
2025-02-12 22:50:16.802434: Steps 41/47: batch_recall = 83.64, batch_ndcg = 66.36 
2025-02-12 22:50:17.776852: Steps 42/47: batch_recall = 79.94, batch_ndcg = 61.13 
2025-02-12 22:50:18.750067: Steps 43/47: batch_recall = 85.90, batch_ndcg = 64.48 
2025-02-12 22:50:19.732930: Steps 44/47: batch_recall = 86.65, batch_ndcg = 66.10 
2025-02-12 22:50:20.678284: Steps 45/47: batch_recall = 91.36, batch_ndcg = 70.92 
2025-02-12 22:50:20.782809: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.49 
2025-02-12 22:50:20.782945: Epoch 20/1000, Test: Recall = 0.1148, NDCG = 0.0986  

2025-02-12 22:50:21.991054: Training Step 0/59: batchLoss = 3.0643, diffLoss = 14.9462, kgLoss = 0.0938
2025-02-12 22:50:22.924758: Training Step 1/59: batchLoss = 2.5902, diffLoss = 12.6719, kgLoss = 0.0698
2025-02-12 22:50:23.865095: Training Step 2/59: batchLoss = 3.0554, diffLoss = 14.9080, kgLoss = 0.0923
2025-02-12 22:50:24.807743: Training Step 3/59: batchLoss = 2.5219, diffLoss = 12.3178, kgLoss = 0.0729
2025-02-12 22:50:25.734720: Training Step 4/59: batchLoss = 2.7458, diffLoss = 13.3852, kgLoss = 0.0859
2025-02-12 22:50:26.668756: Training Step 5/59: batchLoss = 2.6865, diffLoss = 13.1097, kgLoss = 0.0807
2025-02-12 22:50:27.587862: Training Step 6/59: batchLoss = 2.9307, diffLoss = 14.2836, kgLoss = 0.0925
2025-02-12 22:50:28.509142: Training Step 7/59: batchLoss = 2.6950, diffLoss = 13.1430, kgLoss = 0.0830
2025-02-12 22:50:29.436562: Training Step 8/59: batchLoss = 2.7387, diffLoss = 13.3744, kgLoss = 0.0798
2025-02-12 22:50:30.365001: Training Step 9/59: batchLoss = 2.7758, diffLoss = 13.5281, kgLoss = 0.0877
2025-02-12 22:50:31.280851: Training Step 10/59: batchLoss = 2.8273, diffLoss = 13.8188, kgLoss = 0.0794
2025-02-12 22:50:32.205809: Training Step 11/59: batchLoss = 2.7697, diffLoss = 13.5073, kgLoss = 0.0854
2025-02-12 22:50:33.137807: Training Step 12/59: batchLoss = 3.0053, diffLoss = 14.6665, kgLoss = 0.0900
2025-02-12 22:50:34.064272: Training Step 13/59: batchLoss = 3.0147, diffLoss = 14.7328, kgLoss = 0.0852
2025-02-12 22:50:34.996488: Training Step 14/59: batchLoss = 2.8030, diffLoss = 13.6858, kgLoss = 0.0823
2025-02-12 22:50:35.929791: Training Step 15/59: batchLoss = 3.2810, diffLoss = 16.0234, kgLoss = 0.0954
2025-02-12 22:50:36.864125: Training Step 16/59: batchLoss = 2.7176, diffLoss = 13.2666, kgLoss = 0.0804
2025-02-12 22:50:37.800242: Training Step 17/59: batchLoss = 3.4194, diffLoss = 16.7121, kgLoss = 0.0962
2025-02-12 22:50:38.735148: Training Step 18/59: batchLoss = 3.0779, diffLoss = 15.0177, kgLoss = 0.0930
2025-02-12 22:50:39.670717: Training Step 19/59: batchLoss = 2.6305, diffLoss = 12.8582, kgLoss = 0.0736
2025-02-12 22:50:40.593089: Training Step 20/59: batchLoss = 3.0695, diffLoss = 14.9880, kgLoss = 0.0898
2025-02-12 22:50:41.518630: Training Step 21/59: batchLoss = 2.9627, diffLoss = 14.4595, kgLoss = 0.0885
2025-02-12 22:50:42.449061: Training Step 22/59: batchLoss = 3.0146, diffLoss = 14.7170, kgLoss = 0.0890
2025-02-12 22:50:43.387928: Training Step 23/59: batchLoss = 2.8762, diffLoss = 14.0283, kgLoss = 0.0882
2025-02-12 22:50:44.325408: Training Step 24/59: batchLoss = 2.8737, diffLoss = 14.0356, kgLoss = 0.0832
2025-02-12 22:50:45.250103: Training Step 25/59: batchLoss = 2.9737, diffLoss = 14.5108, kgLoss = 0.0894
2025-02-12 22:50:46.174337: Training Step 26/59: batchLoss = 2.6151, diffLoss = 12.7804, kgLoss = 0.0737
2025-02-12 22:50:47.095794: Training Step 27/59: batchLoss = 2.9173, diffLoss = 14.2559, kgLoss = 0.0827
2025-02-12 22:50:48.011299: Training Step 28/59: batchLoss = 2.7902, diffLoss = 13.6293, kgLoss = 0.0804
2025-02-12 22:50:48.934927: Training Step 29/59: batchLoss = 3.0783, diffLoss = 15.0338, kgLoss = 0.0894
2025-02-12 22:50:49.854362: Training Step 30/59: batchLoss = 3.0854, diffLoss = 15.0701, kgLoss = 0.0892
2025-02-12 22:50:50.773795: Training Step 31/59: batchLoss = 3.0078, diffLoss = 14.6967, kgLoss = 0.0856
2025-02-12 22:50:51.696423: Training Step 32/59: batchLoss = 2.7986, diffLoss = 13.6416, kgLoss = 0.0878
2025-02-12 22:50:52.614926: Training Step 33/59: batchLoss = 3.2078, diffLoss = 15.6822, kgLoss = 0.0892
2025-02-12 22:50:53.554631: Training Step 34/59: batchLoss = 2.9242, diffLoss = 14.2938, kgLoss = 0.0818
2025-02-12 22:50:54.483489: Training Step 35/59: batchLoss = 2.9658, diffLoss = 14.4640, kgLoss = 0.0912
2025-02-12 22:50:55.426159: Training Step 36/59: batchLoss = 3.2268, diffLoss = 15.7781, kgLoss = 0.0889
2025-02-12 22:50:56.366723: Training Step 37/59: batchLoss = 3.3577, diffLoss = 16.3882, kgLoss = 0.1000
2025-02-12 22:50:57.307819: Training Step 38/59: batchLoss = 3.1647, diffLoss = 15.4660, kgLoss = 0.0894
2025-02-12 22:50:58.249599: Training Step 39/59: batchLoss = 3.2953, diffLoss = 16.0940, kgLoss = 0.0957
2025-02-12 22:50:59.193185: Training Step 40/59: batchLoss = 3.0123, diffLoss = 14.7165, kgLoss = 0.0862
2025-02-12 22:51:00.125537: Training Step 41/59: batchLoss = 2.9555, diffLoss = 14.4057, kgLoss = 0.0930
2025-02-12 22:51:01.060937: Training Step 42/59: batchLoss = 3.1895, diffLoss = 15.6092, kgLoss = 0.0846
2025-02-12 22:51:02.026174: Training Step 43/59: batchLoss = 3.0790, diffLoss = 15.0554, kgLoss = 0.0848
2025-02-12 22:51:02.959431: Training Step 44/59: batchLoss = 2.8130, diffLoss = 13.7325, kgLoss = 0.0832
2025-02-12 22:51:03.891554: Training Step 45/59: batchLoss = 2.8697, diffLoss = 14.0063, kgLoss = 0.0855
2025-02-12 22:51:04.821766: Training Step 46/59: batchLoss = 3.0790, diffLoss = 15.0450, kgLoss = 0.0875
2025-02-12 22:51:05.743289: Training Step 47/59: batchLoss = 3.0203, diffLoss = 14.7653, kgLoss = 0.0841
2025-02-12 22:51:06.675662: Training Step 48/59: batchLoss = 3.3065, diffLoss = 16.1629, kgLoss = 0.0924
2025-02-12 22:51:07.603229: Training Step 49/59: batchLoss = 2.7964, diffLoss = 13.6538, kgLoss = 0.0820
2025-02-12 22:51:08.534924: Training Step 50/59: batchLoss = 3.1147, diffLoss = 15.2066, kgLoss = 0.0917
2025-02-12 22:51:09.464957: Training Step 51/59: batchLoss = 2.8582, diffLoss = 13.9821, kgLoss = 0.0772
2025-02-12 22:51:10.398324: Training Step 52/59: batchLoss = 3.2701, diffLoss = 15.9687, kgLoss = 0.0954
2025-02-12 22:51:11.330124: Training Step 53/59: batchLoss = 3.2320, diffLoss = 15.7759, kgLoss = 0.0960
2025-02-12 22:51:12.254757: Training Step 54/59: batchLoss = 2.9432, diffLoss = 14.3529, kgLoss = 0.0908
2025-02-12 22:51:13.187929: Training Step 55/59: batchLoss = 3.3646, diffLoss = 16.4367, kgLoss = 0.0965
2025-02-12 22:51:14.121698: Training Step 56/59: batchLoss = 2.8825, diffLoss = 14.0636, kgLoss = 0.0873
2025-02-12 22:51:14.972820: Training Step 57/59: batchLoss = 3.1650, diffLoss = 15.4723, kgLoss = 0.0882
2025-02-12 22:51:15.825602: Training Step 58/59: batchLoss = 2.9509, diffLoss = 14.4485, kgLoss = 0.0766
2025-02-12 22:51:15.923804: 
2025-02-12 22:51:15.924174: Epoch 21/1000, Train: epLoss = 0.4386, epDfLoss = 2.1421, epKgLoss = 0.0128  
2025-02-12 22:51:17.432594: Steps 0/47: batch_recall = 34.27, batch_ndcg = 42.01 
2025-02-12 22:51:18.753970: Steps 1/47: batch_recall = 33.65, batch_ndcg = 36.21 
2025-02-12 22:51:20.036627: Steps 2/47: batch_recall = 37.59, batch_ndcg = 42.40 
2025-02-12 22:51:21.325035: Steps 3/47: batch_recall = 41.22, batch_ndcg = 41.48 
2025-02-12 22:51:22.556141: Steps 4/47: batch_recall = 36.65, batch_ndcg = 40.46 
2025-02-12 22:51:23.775772: Steps 5/47: batch_recall = 31.49, batch_ndcg = 34.77 
2025-02-12 22:51:24.984010: Steps 6/47: batch_recall = 37.37, batch_ndcg = 39.17 
2025-02-12 22:51:26.163891: Steps 7/47: batch_recall = 42.96, batch_ndcg = 41.14 
2025-02-12 22:51:27.347731: Steps 8/47: batch_recall = 45.52, batch_ndcg = 45.12 
2025-02-12 22:51:28.482754: Steps 9/47: batch_recall = 43.96, batch_ndcg = 43.12 
2025-02-12 22:51:29.652667: Steps 10/47: batch_recall = 42.72, batch_ndcg = 40.05 
2025-02-12 22:51:30.801569: Steps 11/47: batch_recall = 52.70, batch_ndcg = 47.01 
2025-02-12 22:51:31.956929: Steps 12/47: batch_recall = 47.64, batch_ndcg = 43.96 
2025-02-12 22:51:33.102573: Steps 13/47: batch_recall = 47.49, batch_ndcg = 43.50 
2025-02-12 22:51:34.218066: Steps 14/47: batch_recall = 38.75, batch_ndcg = 38.24 
2025-02-12 22:51:35.340642: Steps 15/47: batch_recall = 54.04, batch_ndcg = 47.82 
2025-02-12 22:51:36.452766: Steps 16/47: batch_recall = 46.93, batch_ndcg = 41.93 
2025-02-12 22:51:37.523530: Steps 17/47: batch_recall = 54.54, batch_ndcg = 46.45 
2025-02-12 22:51:38.620751: Steps 18/47: batch_recall = 47.62, batch_ndcg = 43.05 
2025-02-12 22:51:39.707140: Steps 19/47: batch_recall = 59.56, batch_ndcg = 52.19 
2025-02-12 22:51:40.766434: Steps 20/47: batch_recall = 62.55, batch_ndcg = 56.20 
2025-02-12 22:51:41.831263: Steps 21/47: batch_recall = 61.94, batch_ndcg = 50.62 
2025-02-12 22:51:42.896725: Steps 22/47: batch_recall = 50.77, batch_ndcg = 46.76 
2025-02-12 22:51:43.942815: Steps 23/47: batch_recall = 57.39, batch_ndcg = 48.33 
2025-02-12 22:51:45.015854: Steps 24/47: batch_recall = 63.17, batch_ndcg = 52.95 
2025-02-12 22:51:46.059422: Steps 25/47: batch_recall = 62.16, batch_ndcg = 52.49 
2025-02-12 22:51:47.074754: Steps 26/47: batch_recall = 56.77, batch_ndcg = 48.81 
2025-02-12 22:51:48.109851: Steps 27/47: batch_recall = 58.16, batch_ndcg = 47.03 
2025-02-12 22:51:49.128058: Steps 28/47: batch_recall = 66.91, batch_ndcg = 54.31 
2025-02-12 22:51:50.131394: Steps 29/47: batch_recall = 65.69, batch_ndcg = 51.80 
2025-02-12 22:51:51.147212: Steps 30/47: batch_recall = 72.78, batch_ndcg = 60.28 
2025-02-12 22:51:52.184585: Steps 31/47: batch_recall = 61.24, batch_ndcg = 48.96 
2025-02-12 22:51:53.209894: Steps 32/47: batch_recall = 67.49, batch_ndcg = 59.98 
2025-02-12 22:51:54.234930: Steps 33/47: batch_recall = 76.66, batch_ndcg = 62.21 
2025-02-12 22:51:55.259041: Steps 34/47: batch_recall = 64.54, batch_ndcg = 50.29 
2025-02-12 22:51:56.252184: Steps 35/47: batch_recall = 74.19, batch_ndcg = 59.40 
2025-02-12 22:51:57.245412: Steps 36/47: batch_recall = 75.34, batch_ndcg = 60.75 
2025-02-12 22:51:58.238465: Steps 37/47: batch_recall = 79.87, batch_ndcg = 65.71 
2025-02-12 22:51:59.225524: Steps 38/47: batch_recall = 87.84, batch_ndcg = 69.08 
2025-02-12 22:52:00.193888: Steps 39/47: batch_recall = 86.34, batch_ndcg = 63.91 
2025-02-12 22:52:01.155765: Steps 40/47: batch_recall = 70.20, batch_ndcg = 59.15 
2025-02-12 22:52:02.161875: Steps 41/47: batch_recall = 83.69, batch_ndcg = 66.17 
2025-02-12 22:52:03.111266: Steps 42/47: batch_recall = 81.61, batch_ndcg = 62.00 
2025-02-12 22:52:04.108327: Steps 43/47: batch_recall = 85.62, batch_ndcg = 65.28 
2025-02-12 22:52:05.075228: Steps 44/47: batch_recall = 85.81, batch_ndcg = 65.53 
2025-02-12 22:52:06.008879: Steps 45/47: batch_recall = 92.15, batch_ndcg = 71.30 
2025-02-12 22:52:06.109794: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.52 
2025-02-12 22:52:06.109897: Epoch 21/1000, Test: Recall = 0.1158, NDCG = 0.0998  

2025-02-12 22:52:07.303061: Training Step 0/59: batchLoss = 2.7724, diffLoss = 13.5312, kgLoss = 0.0827
2025-02-12 22:52:08.229099: Training Step 1/59: batchLoss = 2.7078, diffLoss = 13.2233, kgLoss = 0.0789
2025-02-12 22:52:09.150434: Training Step 2/59: batchLoss = 2.8064, diffLoss = 13.7227, kgLoss = 0.0773
2025-02-12 22:52:10.077579: Training Step 3/59: batchLoss = 2.7388, diffLoss = 13.3792, kgLoss = 0.0787
2025-02-12 22:52:11.006147: Training Step 4/59: batchLoss = 2.8798, diffLoss = 14.0624, kgLoss = 0.0841
2025-02-12 22:52:11.922175: Training Step 5/59: batchLoss = 2.7656, diffLoss = 13.4766, kgLoss = 0.0879
2025-02-12 22:52:12.842427: Training Step 6/59: batchLoss = 2.8431, diffLoss = 13.8924, kgLoss = 0.0808
2025-02-12 22:52:13.772822: Training Step 7/59: batchLoss = 2.9327, diffLoss = 14.2931, kgLoss = 0.0926
2025-02-12 22:52:14.695796: Training Step 8/59: batchLoss = 2.7308, diffLoss = 13.3057, kgLoss = 0.0871
2025-02-12 22:52:15.625267: Training Step 9/59: batchLoss = 3.1482, diffLoss = 15.3921, kgLoss = 0.0873
2025-02-12 22:52:16.555864: Training Step 10/59: batchLoss = 2.9512, diffLoss = 14.4372, kgLoss = 0.0797
2025-02-12 22:52:17.478113: Training Step 11/59: batchLoss = 2.7931, diffLoss = 13.6179, kgLoss = 0.0869
2025-02-12 22:52:18.409703: Training Step 12/59: batchLoss = 2.8153, diffLoss = 13.7226, kgLoss = 0.0885
2025-02-12 22:52:19.335793: Training Step 13/59: batchLoss = 3.0195, diffLoss = 14.7594, kgLoss = 0.0846
2025-02-12 22:52:20.262606: Training Step 14/59: batchLoss = 2.7913, diffLoss = 13.5958, kgLoss = 0.0902
2025-02-12 22:52:21.193428: Training Step 15/59: batchLoss = 2.7526, diffLoss = 13.4669, kgLoss = 0.0740
2025-02-12 22:52:22.117411: Training Step 16/59: batchLoss = 3.2416, diffLoss = 15.8034, kgLoss = 0.1012
2025-02-12 22:52:23.046346: Training Step 17/59: batchLoss = 3.1870, diffLoss = 15.5398, kgLoss = 0.0988
2025-02-12 22:52:23.979902: Training Step 18/59: batchLoss = 2.9653, diffLoss = 14.4541, kgLoss = 0.0931
2025-02-12 22:52:24.909100: Training Step 19/59: batchLoss = 2.8471, diffLoss = 13.9125, kgLoss = 0.0808
2025-02-12 22:52:25.832786: Training Step 20/59: batchLoss = 3.0962, diffLoss = 15.0957, kgLoss = 0.0964
2025-02-12 22:52:26.751243: Training Step 21/59: batchLoss = 2.6054, diffLoss = 12.7077, kgLoss = 0.0798
2025-02-12 22:52:27.679588: Training Step 22/59: batchLoss = 3.0207, diffLoss = 14.7538, kgLoss = 0.0874
2025-02-12 22:52:28.597635: Training Step 23/59: batchLoss = 2.8181, diffLoss = 13.7608, kgLoss = 0.0824
2025-02-12 22:52:29.520359: Training Step 24/59: batchLoss = 2.7460, diffLoss = 13.4072, kgLoss = 0.0807
2025-02-12 22:52:30.444404: Training Step 25/59: batchLoss = 2.9857, diffLoss = 14.5658, kgLoss = 0.0907
2025-02-12 22:52:31.365671: Training Step 26/59: batchLoss = 2.8483, diffLoss = 13.8861, kgLoss = 0.0888
2025-02-12 22:52:32.291147: Training Step 27/59: batchLoss = 3.1581, diffLoss = 15.4390, kgLoss = 0.0878
2025-02-12 22:52:33.222590: Training Step 28/59: batchLoss = 2.7690, diffLoss = 13.5172, kgLoss = 0.0820
2025-02-12 22:52:34.154077: Training Step 29/59: batchLoss = 3.0610, diffLoss = 14.9302, kgLoss = 0.0938
2025-02-12 22:52:35.076745: Training Step 30/59: batchLoss = 2.8814, diffLoss = 14.0478, kgLoss = 0.0899
2025-02-12 22:52:36.001377: Training Step 31/59: batchLoss = 2.8156, diffLoss = 13.7283, kgLoss = 0.0874
2025-02-12 22:52:36.922177: Training Step 32/59: batchLoss = 2.9553, diffLoss = 14.4137, kgLoss = 0.0907
2025-02-12 22:52:37.850067: Training Step 33/59: batchLoss = 2.6593, diffLoss = 12.9782, kgLoss = 0.0795
2025-02-12 22:52:38.772095: Training Step 34/59: batchLoss = 2.9527, diffLoss = 14.4098, kgLoss = 0.0884
2025-02-12 22:52:39.700874: Training Step 35/59: batchLoss = 3.0291, diffLoss = 14.7981, kgLoss = 0.0869
2025-02-12 22:52:40.622729: Training Step 36/59: batchLoss = 2.8907, diffLoss = 14.1243, kgLoss = 0.0823
2025-02-12 22:52:41.545304: Training Step 37/59: batchLoss = 3.1691, diffLoss = 15.4528, kgLoss = 0.0981
2025-02-12 22:52:42.470534: Training Step 38/59: batchLoss = 3.0243, diffLoss = 14.7512, kgLoss = 0.0926
2025-02-12 22:52:43.391159: Training Step 39/59: batchLoss = 2.7238, diffLoss = 13.3089, kgLoss = 0.0775
2025-02-12 22:52:44.318202: Training Step 40/59: batchLoss = 2.8084, diffLoss = 13.7236, kgLoss = 0.0795
2025-02-12 22:52:45.242576: Training Step 41/59: batchLoss = 3.0283, diffLoss = 14.8241, kgLoss = 0.0793
2025-02-12 22:52:46.179982: Training Step 42/59: batchLoss = 3.0778, diffLoss = 15.0218, kgLoss = 0.0918
2025-02-12 22:52:47.114488: Training Step 43/59: batchLoss = 3.1674, diffLoss = 15.4580, kgLoss = 0.0948
2025-02-12 22:52:48.047044: Training Step 44/59: batchLoss = 2.8618, diffLoss = 13.9422, kgLoss = 0.0917
2025-02-12 22:52:48.979595: Training Step 45/59: batchLoss = 2.8816, diffLoss = 14.0840, kgLoss = 0.0810
2025-02-12 22:52:49.913453: Training Step 46/59: batchLoss = 3.0355, diffLoss = 14.8243, kgLoss = 0.0883
2025-02-12 22:52:50.850638: Training Step 47/59: batchLoss = 2.8042, diffLoss = 13.7020, kgLoss = 0.0798
2025-02-12 22:52:51.790407: Training Step 48/59: batchLoss = 3.1465, diffLoss = 15.3864, kgLoss = 0.0865
2025-02-12 22:52:52.732359: Training Step 49/59: batchLoss = 3.6017, diffLoss = 17.6051, kgLoss = 0.1009
2025-02-12 22:52:53.670865: Training Step 50/59: batchLoss = 2.9313, diffLoss = 14.3317, kgLoss = 0.0812
2025-02-12 22:52:54.607637: Training Step 51/59: batchLoss = 2.9285, diffLoss = 14.2993, kgLoss = 0.0858
2025-02-12 22:52:55.539023: Training Step 52/59: batchLoss = 2.9462, diffLoss = 14.4089, kgLoss = 0.0805
2025-02-12 22:52:56.478760: Training Step 53/59: batchLoss = 2.8625, diffLoss = 13.9805, kgLoss = 0.0829
2025-02-12 22:52:57.403267: Training Step 54/59: batchLoss = 2.9888, diffLoss = 14.6047, kgLoss = 0.0849
2025-02-12 22:52:58.334322: Training Step 55/59: batchLoss = 3.0754, diffLoss = 15.0308, kgLoss = 0.0866
2025-02-12 22:52:59.255338: Training Step 56/59: batchLoss = 3.2128, diffLoss = 15.6782, kgLoss = 0.0964
2025-02-12 22:53:00.102295: Training Step 57/59: batchLoss = 3.0665, diffLoss = 14.9579, kgLoss = 0.0936
2025-02-12 22:53:00.950575: Training Step 58/59: batchLoss = 2.8571, diffLoss = 13.9714, kgLoss = 0.0786
2025-02-12 22:53:01.047067: 
2025-02-12 22:53:01.047397: Epoch 22/1000, Train: epLoss = 0.4330, epDfLoss = 2.1137, epKgLoss = 0.0128  
2025-02-12 22:53:02.510155: Steps 0/47: batch_recall = 34.81, batch_ndcg = 43.00 
2025-02-12 22:53:03.811822: Steps 1/47: batch_recall = 33.77, batch_ndcg = 36.88 
2025-02-12 22:53:05.066234: Steps 2/47: batch_recall = 37.51, batch_ndcg = 42.86 
2025-02-12 22:53:06.335336: Steps 3/47: batch_recall = 43.19, batch_ndcg = 43.34 
2025-02-12 22:53:07.541080: Steps 4/47: batch_recall = 37.63, batch_ndcg = 40.99 
2025-02-12 22:53:08.760251: Steps 5/47: batch_recall = 30.54, batch_ndcg = 34.74 
2025-02-12 22:53:09.965148: Steps 6/47: batch_recall = 38.02, batch_ndcg = 39.44 
2025-02-12 22:53:11.145660: Steps 7/47: batch_recall = 43.06, batch_ndcg = 41.48 
2025-02-12 22:53:12.331677: Steps 8/47: batch_recall = 46.55, batch_ndcg = 46.50 
2025-02-12 22:53:13.466937: Steps 9/47: batch_recall = 45.69, batch_ndcg = 43.92 
2025-02-12 22:53:14.649808: Steps 10/47: batch_recall = 42.52, batch_ndcg = 39.99 
2025-02-12 22:53:15.788701: Steps 11/47: batch_recall = 52.26, batch_ndcg = 46.87 
2025-02-12 22:53:16.932618: Steps 12/47: batch_recall = 48.71, batch_ndcg = 44.85 
2025-02-12 22:53:18.068416: Steps 13/47: batch_recall = 48.30, batch_ndcg = 43.75 
2025-02-12 22:53:19.168747: Steps 14/47: batch_recall = 39.25, batch_ndcg = 37.99 
2025-02-12 22:53:20.254938: Steps 15/47: batch_recall = 54.79, batch_ndcg = 48.56 
2025-02-12 22:53:21.324884: Steps 16/47: batch_recall = 46.90, batch_ndcg = 41.90 
2025-02-12 22:53:22.367753: Steps 17/47: batch_recall = 54.48, batch_ndcg = 46.25 
2025-02-12 22:53:23.462274: Steps 18/47: batch_recall = 48.80, batch_ndcg = 44.20 
2025-02-12 22:53:24.542726: Steps 19/47: batch_recall = 59.78, batch_ndcg = 53.56 
2025-02-12 22:53:25.609129: Steps 20/47: batch_recall = 64.17, batch_ndcg = 56.76 
2025-02-12 22:53:26.670306: Steps 21/47: batch_recall = 63.20, batch_ndcg = 52.08 
2025-02-12 22:53:27.738936: Steps 22/47: batch_recall = 52.92, batch_ndcg = 48.32 
2025-02-12 22:53:28.811044: Steps 23/47: batch_recall = 59.68, batch_ndcg = 51.63 
2025-02-12 22:53:29.886946: Steps 24/47: batch_recall = 64.00, batch_ndcg = 53.95 
2025-02-12 22:53:30.944712: Steps 25/47: batch_recall = 63.00, batch_ndcg = 53.05 
2025-02-12 22:53:31.970871: Steps 26/47: batch_recall = 58.88, batch_ndcg = 50.26 
2025-02-12 22:53:33.000642: Steps 27/47: batch_recall = 58.83, batch_ndcg = 47.82 
2025-02-12 22:53:34.018543: Steps 28/47: batch_recall = 66.27, batch_ndcg = 54.27 
2025-02-12 22:53:35.043812: Steps 29/47: batch_recall = 66.08, batch_ndcg = 52.70 
2025-02-12 22:53:36.057678: Steps 30/47: batch_recall = 72.48, batch_ndcg = 60.68 
2025-02-12 22:53:37.083862: Steps 31/47: batch_recall = 63.42, batch_ndcg = 50.70 
2025-02-12 22:53:38.092374: Steps 32/47: batch_recall = 69.11, batch_ndcg = 60.55 
2025-02-12 22:53:39.096568: Steps 33/47: batch_recall = 76.80, batch_ndcg = 63.12 
2025-02-12 22:53:40.100485: Steps 34/47: batch_recall = 63.23, batch_ndcg = 50.12 
2025-02-12 22:53:41.078257: Steps 35/47: batch_recall = 73.07, batch_ndcg = 59.72 
2025-02-12 22:53:42.068923: Steps 36/47: batch_recall = 77.09, batch_ndcg = 62.26 
2025-02-12 22:53:43.041514: Steps 37/47: batch_recall = 79.87, batch_ndcg = 66.42 
2025-02-12 22:53:44.028640: Steps 38/47: batch_recall = 87.33, batch_ndcg = 69.10 
2025-02-12 22:53:45.006287: Steps 39/47: batch_recall = 88.29, batch_ndcg = 65.58 
2025-02-12 22:53:45.984654: Steps 40/47: batch_recall = 69.63, batch_ndcg = 59.51 
2025-02-12 22:53:46.950962: Steps 41/47: batch_recall = 83.60, batch_ndcg = 66.87 
2025-02-12 22:53:47.921099: Steps 42/47: batch_recall = 80.41, batch_ndcg = 61.38 
2025-02-12 22:53:48.892954: Steps 43/47: batch_recall = 88.19, batch_ndcg = 67.28 
2025-02-12 22:53:49.873194: Steps 44/47: batch_recall = 88.47, batch_ndcg = 68.05 
2025-02-12 22:53:50.825352: Steps 45/47: batch_recall = 91.82, batch_ndcg = 72.09 
2025-02-12 22:53:50.932148: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 22:53:50.932290: Epoch 22/1000, Test: Recall = 0.1171, NDCG = 0.1013  

2025-02-12 22:53:52.128026: Training Step 0/59: batchLoss = 2.6448, diffLoss = 12.9058, kgLoss = 0.0796
2025-02-12 22:53:53.056113: Training Step 1/59: batchLoss = 2.8052, diffLoss = 13.6881, kgLoss = 0.0845
2025-02-12 22:53:53.986077: Training Step 2/59: batchLoss = 2.9901, diffLoss = 14.6091, kgLoss = 0.0853
2025-02-12 22:53:54.911039: Training Step 3/59: batchLoss = 2.8101, diffLoss = 13.6772, kgLoss = 0.0934
2025-02-12 22:53:55.852321: Training Step 4/59: batchLoss = 2.7913, diffLoss = 13.6056, kgLoss = 0.0878
2025-02-12 22:53:56.772416: Training Step 5/59: batchLoss = 2.9350, diffLoss = 14.3186, kgLoss = 0.0891
2025-02-12 22:53:57.695367: Training Step 6/59: batchLoss = 3.0617, diffLoss = 14.9102, kgLoss = 0.0996
2025-02-12 22:53:58.624987: Training Step 7/59: batchLoss = 2.9055, diffLoss = 14.1813, kgLoss = 0.0865
2025-02-12 22:53:59.549069: Training Step 8/59: batchLoss = 2.7338, diffLoss = 13.3435, kgLoss = 0.0814
2025-02-12 22:54:00.474172: Training Step 9/59: batchLoss = 3.0533, diffLoss = 14.9163, kgLoss = 0.0876
2025-02-12 22:54:01.415128: Training Step 10/59: batchLoss = 2.8857, diffLoss = 14.0752, kgLoss = 0.0883
2025-02-12 22:54:02.346820: Training Step 11/59: batchLoss = 2.7546, diffLoss = 13.4512, kgLoss = 0.0805
2025-02-12 22:54:03.287707: Training Step 12/59: batchLoss = 3.2318, diffLoss = 15.7896, kgLoss = 0.0924
2025-02-12 22:54:04.222958: Training Step 13/59: batchLoss = 2.7761, diffLoss = 13.5742, kgLoss = 0.0766
2025-02-12 22:54:05.154393: Training Step 14/59: batchLoss = 2.9020, diffLoss = 14.1313, kgLoss = 0.0947
2025-02-12 22:54:06.092032: Training Step 15/59: batchLoss = 3.1412, diffLoss = 15.3297, kgLoss = 0.0940
2025-02-12 22:54:07.026150: Training Step 16/59: batchLoss = 3.0537, diffLoss = 14.9101, kgLoss = 0.0897
2025-02-12 22:54:07.954578: Training Step 17/59: batchLoss = 3.1350, diffLoss = 15.2916, kgLoss = 0.0958
2025-02-12 22:54:08.898893: Training Step 18/59: batchLoss = 2.9472, diffLoss = 14.4200, kgLoss = 0.0791
2025-02-12 22:54:09.836293: Training Step 19/59: batchLoss = 2.7077, diffLoss = 13.2250, kgLoss = 0.0784
2025-02-12 22:54:10.765807: Training Step 20/59: batchLoss = 2.9493, diffLoss = 14.3982, kgLoss = 0.0870
2025-02-12 22:54:11.697786: Training Step 21/59: batchLoss = 3.1964, diffLoss = 15.6083, kgLoss = 0.0934
2025-02-12 22:54:12.627549: Training Step 22/59: batchLoss = 2.8419, diffLoss = 13.8652, kgLoss = 0.0860
2025-02-12 22:54:13.551869: Training Step 23/59: batchLoss = 3.2447, diffLoss = 15.8685, kgLoss = 0.0888
2025-02-12 22:54:14.481035: Training Step 24/59: batchLoss = 2.7979, diffLoss = 13.6567, kgLoss = 0.0832
2025-02-12 22:54:15.415592: Training Step 25/59: batchLoss = 2.7693, diffLoss = 13.5351, kgLoss = 0.0779
2025-02-12 22:54:16.342281: Training Step 26/59: batchLoss = 2.6190, diffLoss = 12.7665, kgLoss = 0.0821
2025-02-12 22:54:17.277980: Training Step 27/59: batchLoss = 3.1661, diffLoss = 15.4423, kgLoss = 0.0971
2025-02-12 22:54:18.196435: Training Step 28/59: batchLoss = 2.9452, diffLoss = 14.3526, kgLoss = 0.0934
2025-02-12 22:54:19.133489: Training Step 29/59: batchLoss = 2.6522, diffLoss = 12.9545, kgLoss = 0.0766
2025-02-12 22:54:20.072540: Training Step 30/59: batchLoss = 2.9327, diffLoss = 14.3372, kgLoss = 0.0816
2025-02-12 22:54:21.007976: Training Step 31/59: batchLoss = 2.9575, diffLoss = 14.4443, kgLoss = 0.0859
2025-02-12 22:54:21.959047: Training Step 32/59: batchLoss = 2.6822, diffLoss = 13.0932, kgLoss = 0.0794
2025-02-12 22:54:22.894101: Training Step 33/59: batchLoss = 2.9541, diffLoss = 14.4576, kgLoss = 0.0782
2025-02-12 22:54:23.827670: Training Step 34/59: batchLoss = 2.9262, diffLoss = 14.2976, kgLoss = 0.0834
2025-02-12 22:54:24.764986: Training Step 35/59: batchLoss = 2.7550, diffLoss = 13.4673, kgLoss = 0.0769
2025-02-12 22:54:25.703431: Training Step 36/59: batchLoss = 3.1825, diffLoss = 15.5343, kgLoss = 0.0946
2025-02-12 22:54:26.636730: Training Step 37/59: batchLoss = 2.8429, diffLoss = 13.8907, kgLoss = 0.0810
2025-02-12 22:54:27.578578: Training Step 38/59: batchLoss = 2.8965, diffLoss = 14.1487, kgLoss = 0.0835
2025-02-12 22:54:28.516344: Training Step 39/59: batchLoss = 3.4666, diffLoss = 16.9457, kgLoss = 0.0969
2025-02-12 22:54:29.453219: Training Step 40/59: batchLoss = 2.6960, diffLoss = 13.1711, kgLoss = 0.0772
2025-02-12 22:54:30.389247: Training Step 41/59: batchLoss = 3.1385, diffLoss = 15.3078, kgLoss = 0.0962
2025-02-12 22:54:31.317286: Training Step 42/59: batchLoss = 2.9130, diffLoss = 14.2546, kgLoss = 0.0776
2025-02-12 22:54:32.239353: Training Step 43/59: batchLoss = 3.1282, diffLoss = 15.2734, kgLoss = 0.0919
2025-02-12 22:54:33.180285: Training Step 44/59: batchLoss = 3.4968, diffLoss = 17.0944, kgLoss = 0.0974
2025-02-12 22:54:34.109807: Training Step 45/59: batchLoss = 3.1720, diffLoss = 15.4858, kgLoss = 0.0935
2025-02-12 22:54:35.038575: Training Step 46/59: batchLoss = 3.0396, diffLoss = 14.8409, kgLoss = 0.0892
2025-02-12 22:54:35.977628: Training Step 47/59: batchLoss = 2.8843, diffLoss = 14.0970, kgLoss = 0.0811
2025-02-12 22:54:36.905954: Training Step 48/59: batchLoss = 2.9533, diffLoss = 14.4403, kgLoss = 0.0816
2025-02-12 22:54:37.844044: Training Step 49/59: batchLoss = 2.9458, diffLoss = 14.3996, kgLoss = 0.0824
2025-02-12 22:54:38.784271: Training Step 50/59: batchLoss = 3.0268, diffLoss = 14.7877, kgLoss = 0.0866
2025-02-12 22:54:39.722796: Training Step 51/59: batchLoss = 2.8366, diffLoss = 13.8604, kgLoss = 0.0807
2025-02-12 22:54:40.663787: Training Step 52/59: batchLoss = 2.9669, diffLoss = 14.5101, kgLoss = 0.0811
2025-02-12 22:54:41.601041: Training Step 53/59: batchLoss = 3.3447, diffLoss = 16.3573, kgLoss = 0.0916
2025-02-12 22:54:42.542162: Training Step 54/59: batchLoss = 2.9642, diffLoss = 14.4859, kgLoss = 0.0838
2025-02-12 22:54:43.483788: Training Step 55/59: batchLoss = 3.0878, diffLoss = 15.0875, kgLoss = 0.0879
2025-02-12 22:54:44.420163: Training Step 56/59: batchLoss = 2.9781, diffLoss = 14.5672, kgLoss = 0.0808
2025-02-12 22:54:45.276725: Training Step 57/59: batchLoss = 2.9226, diffLoss = 14.2838, kgLoss = 0.0823
2025-02-12 22:54:46.132026: Training Step 58/59: batchLoss = 3.3058, diffLoss = 16.1262, kgLoss = 0.1007
2025-02-12 22:54:46.233992: 
2025-02-12 22:54:46.234557: Epoch 23/1000, Train: epLoss = 0.4371, epDfLoss = 2.1346, epKgLoss = 0.0127  
2025-02-12 22:54:47.729775: Steps 0/47: batch_recall = 34.56, batch_ndcg = 43.03 
2025-02-12 22:54:49.031895: Steps 1/47: batch_recall = 34.18, batch_ndcg = 37.54 
2025-02-12 22:54:50.289145: Steps 2/47: batch_recall = 37.38, batch_ndcg = 42.80 
2025-02-12 22:54:51.548117: Steps 3/47: batch_recall = 42.54, batch_ndcg = 42.89 
2025-02-12 22:54:52.737157: Steps 4/47: batch_recall = 37.61, batch_ndcg = 41.45 
2025-02-12 22:54:53.933430: Steps 5/47: batch_recall = 31.00, batch_ndcg = 35.70 
2025-02-12 22:54:55.129009: Steps 6/47: batch_recall = 36.95, batch_ndcg = 39.31 
2025-02-12 22:54:56.278002: Steps 7/47: batch_recall = 42.29, batch_ndcg = 42.15 
2025-02-12 22:54:57.446554: Steps 8/47: batch_recall = 45.97, batch_ndcg = 46.96 
2025-02-12 22:54:58.582460: Steps 9/47: batch_recall = 45.63, batch_ndcg = 45.18 
2025-02-12 22:54:59.741633: Steps 10/47: batch_recall = 43.03, batch_ndcg = 41.19 
2025-02-12 22:55:00.875501: Steps 11/47: batch_recall = 52.61, batch_ndcg = 47.80 
2025-02-12 22:55:02.011737: Steps 12/47: batch_recall = 47.97, batch_ndcg = 44.90 
2025-02-12 22:55:03.138638: Steps 13/47: batch_recall = 49.06, batch_ndcg = 44.43 
2025-02-12 22:55:04.232474: Steps 14/47: batch_recall = 38.89, batch_ndcg = 37.65 
2025-02-12 22:55:05.323242: Steps 15/47: batch_recall = 53.74, batch_ndcg = 48.35 
2025-02-12 22:55:06.418356: Steps 16/47: batch_recall = 48.56, batch_ndcg = 43.48 
2025-02-12 22:55:07.471228: Steps 17/47: batch_recall = 55.67, batch_ndcg = 47.49 
2025-02-12 22:55:08.542321: Steps 18/47: batch_recall = 48.91, batch_ndcg = 44.73 
2025-02-12 22:55:09.604680: Steps 19/47: batch_recall = 59.37, batch_ndcg = 52.99 
2025-02-12 22:55:10.649256: Steps 20/47: batch_recall = 63.50, batch_ndcg = 57.77 
2025-02-12 22:55:11.690800: Steps 21/47: batch_recall = 62.36, batch_ndcg = 52.12 
2025-02-12 22:55:12.739362: Steps 22/47: batch_recall = 54.57, batch_ndcg = 49.36 
2025-02-12 22:55:13.781424: Steps 23/47: batch_recall = 59.81, batch_ndcg = 51.39 
2025-02-12 22:55:14.838270: Steps 24/47: batch_recall = 65.19, batch_ndcg = 55.40 
2025-02-12 22:55:15.875647: Steps 25/47: batch_recall = 64.87, batch_ndcg = 54.37 
2025-02-12 22:55:16.896972: Steps 26/47: batch_recall = 58.13, batch_ndcg = 50.41 
2025-02-12 22:55:17.937410: Steps 27/47: batch_recall = 60.43, batch_ndcg = 49.06 
2025-02-12 22:55:18.963073: Steps 28/47: batch_recall = 68.05, batch_ndcg = 55.56 
2025-02-12 22:55:19.991198: Steps 29/47: batch_recall = 65.95, batch_ndcg = 53.56 
2025-02-12 22:55:21.014185: Steps 30/47: batch_recall = 72.34, batch_ndcg = 60.65 
2025-02-12 22:55:22.055387: Steps 31/47: batch_recall = 63.92, batch_ndcg = 51.74 
2025-02-12 22:55:23.070017: Steps 32/47: batch_recall = 67.83, batch_ndcg = 61.18 
2025-02-12 22:55:24.097923: Steps 33/47: batch_recall = 77.38, batch_ndcg = 63.47 
2025-02-12 22:55:25.117515: Steps 34/47: batch_recall = 63.94, batch_ndcg = 51.28 
2025-02-12 22:55:26.104106: Steps 35/47: batch_recall = 72.92, batch_ndcg = 60.80 
2025-02-12 22:55:27.097423: Steps 36/47: batch_recall = 78.19, batch_ndcg = 62.90 
2025-02-12 22:55:28.070407: Steps 37/47: batch_recall = 81.11, batch_ndcg = 68.89 
2025-02-12 22:55:29.053501: Steps 38/47: batch_recall = 87.89, batch_ndcg = 70.57 
2025-02-12 22:55:30.016258: Steps 39/47: batch_recall = 86.20, batch_ndcg = 65.32 
2025-02-12 22:55:30.963323: Steps 40/47: batch_recall = 69.34, batch_ndcg = 59.08 
2025-02-12 22:55:31.916274: Steps 41/47: batch_recall = 85.14, batch_ndcg = 67.74 
2025-02-12 22:55:32.863235: Steps 42/47: batch_recall = 81.23, batch_ndcg = 61.88 
2025-02-12 22:55:33.812645: Steps 43/47: batch_recall = 87.39, batch_ndcg = 67.03 
2025-02-12 22:55:34.770736: Steps 44/47: batch_recall = 87.24, batch_ndcg = 66.93 
2025-02-12 22:55:35.717457: Steps 45/47: batch_recall = 92.66, batch_ndcg = 73.11 
2025-02-12 22:55:35.822152: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.54 
2025-02-12 22:55:35.822257: Epoch 23/1000, Test: Recall = 0.1174, NDCG = 0.1024  

2025-02-12 22:55:37.020586: Training Step 0/59: batchLoss = 2.7876, diffLoss = 13.6239, kgLoss = 0.0785
2025-02-12 22:55:37.957914: Training Step 1/59: batchLoss = 2.7430, diffLoss = 13.3725, kgLoss = 0.0857
2025-02-12 22:55:38.886844: Training Step 2/59: batchLoss = 2.8923, diffLoss = 14.1152, kgLoss = 0.0865
2025-02-12 22:55:39.822728: Training Step 3/59: batchLoss = 3.0830, diffLoss = 15.0607, kgLoss = 0.0886
2025-02-12 22:55:40.758430: Training Step 4/59: batchLoss = 3.0354, diffLoss = 14.8358, kgLoss = 0.0854
2025-02-12 22:55:41.686240: Training Step 5/59: batchLoss = 2.6917, diffLoss = 13.1499, kgLoss = 0.0771
2025-02-12 22:55:42.624255: Training Step 6/59: batchLoss = 3.0894, diffLoss = 15.0774, kgLoss = 0.0925
2025-02-12 22:55:43.554255: Training Step 7/59: batchLoss = 2.7018, diffLoss = 13.1774, kgLoss = 0.0829
2025-02-12 22:55:44.480398: Training Step 8/59: batchLoss = 2.9407, diffLoss = 14.3680, kgLoss = 0.0839
2025-02-12 22:55:45.406441: Training Step 9/59: batchLoss = 2.9856, diffLoss = 14.5851, kgLoss = 0.0858
2025-02-12 22:55:46.324447: Training Step 10/59: batchLoss = 2.5877, diffLoss = 12.6332, kgLoss = 0.0763
2025-02-12 22:55:47.251736: Training Step 11/59: batchLoss = 2.7322, diffLoss = 13.3665, kgLoss = 0.0736
2025-02-12 22:55:48.181468: Training Step 12/59: batchLoss = 3.0002, diffLoss = 14.6290, kgLoss = 0.0931
2025-02-12 22:55:49.110528: Training Step 13/59: batchLoss = 3.1923, diffLoss = 15.5689, kgLoss = 0.0982
2025-02-12 22:55:50.040494: Training Step 14/59: batchLoss = 2.8671, diffLoss = 13.9885, kgLoss = 0.0868
2025-02-12 22:55:50.968351: Training Step 15/59: batchLoss = 3.0081, diffLoss = 14.7175, kgLoss = 0.0807
2025-02-12 22:55:51.903861: Training Step 16/59: batchLoss = 2.7058, diffLoss = 13.2082, kgLoss = 0.0802
2025-02-12 22:55:52.829962: Training Step 17/59: batchLoss = 2.8871, diffLoss = 14.0890, kgLoss = 0.0867
2025-02-12 22:55:53.763804: Training Step 18/59: batchLoss = 2.9448, diffLoss = 14.3404, kgLoss = 0.0959
2025-02-12 22:55:54.695259: Training Step 19/59: batchLoss = 3.1150, diffLoss = 15.2402, kgLoss = 0.0838
2025-02-12 22:55:55.628396: Training Step 20/59: batchLoss = 3.0038, diffLoss = 14.6732, kgLoss = 0.0865
2025-02-12 22:55:56.561358: Training Step 21/59: batchLoss = 2.5796, diffLoss = 12.5734, kgLoss = 0.0811
2025-02-12 22:55:57.499432: Training Step 22/59: batchLoss = 2.9354, diffLoss = 14.3355, kgLoss = 0.0854
2025-02-12 22:55:58.443311: Training Step 23/59: batchLoss = 3.0532, diffLoss = 14.8989, kgLoss = 0.0918
2025-02-12 22:55:59.383043: Training Step 24/59: batchLoss = 2.8335, diffLoss = 13.8427, kgLoss = 0.0812
2025-02-12 22:56:00.320910: Training Step 25/59: batchLoss = 2.9955, diffLoss = 14.6330, kgLoss = 0.0862
2025-02-12 22:56:01.262076: Training Step 26/59: batchLoss = 2.5606, diffLoss = 12.4938, kgLoss = 0.0773
2025-02-12 22:56:02.196172: Training Step 27/59: batchLoss = 3.0244, diffLoss = 14.7431, kgLoss = 0.0947
2025-02-12 22:56:03.129358: Training Step 28/59: batchLoss = 3.0373, diffLoss = 14.8520, kgLoss = 0.0837
2025-02-12 22:56:04.058607: Training Step 29/59: batchLoss = 2.8069, diffLoss = 13.7029, kgLoss = 0.0830
2025-02-12 22:56:04.980181: Training Step 30/59: batchLoss = 3.1601, diffLoss = 15.4373, kgLoss = 0.0908
2025-02-12 22:56:05.912958: Training Step 31/59: batchLoss = 2.9708, diffLoss = 14.5282, kgLoss = 0.0815
2025-02-12 22:56:06.837615: Training Step 32/59: batchLoss = 2.6418, diffLoss = 12.8554, kgLoss = 0.0884
2025-02-12 22:56:07.773090: Training Step 33/59: batchLoss = 3.1093, diffLoss = 15.1872, kgLoss = 0.0899
2025-02-12 22:56:08.693550: Training Step 34/59: batchLoss = 2.6392, diffLoss = 12.8785, kgLoss = 0.0793
2025-02-12 22:56:09.616999: Training Step 35/59: batchLoss = 3.1358, diffLoss = 15.3281, kgLoss = 0.0877
2025-02-12 22:56:10.529347: Training Step 36/59: batchLoss = 2.8596, diffLoss = 13.9743, kgLoss = 0.0809
2025-02-12 22:56:11.449460: Training Step 37/59: batchLoss = 3.0685, diffLoss = 14.9929, kgLoss = 0.0874
2025-02-12 22:56:12.366237: Training Step 38/59: batchLoss = 3.0943, diffLoss = 15.1254, kgLoss = 0.0866
2025-02-12 22:56:13.289847: Training Step 39/59: batchLoss = 2.9092, diffLoss = 14.2079, kgLoss = 0.0845
2025-02-12 22:56:14.222703: Training Step 40/59: batchLoss = 3.1069, diffLoss = 15.1558, kgLoss = 0.0946
2025-02-12 22:56:15.160254: Training Step 41/59: batchLoss = 2.9334, diffLoss = 14.3264, kgLoss = 0.0851
2025-02-12 22:56:16.097574: Training Step 42/59: batchLoss = 2.9207, diffLoss = 14.2514, kgLoss = 0.0880
2025-02-12 22:56:17.039963: Training Step 43/59: batchLoss = 2.9865, diffLoss = 14.6151, kgLoss = 0.0793
2025-02-12 22:56:17.978846: Training Step 44/59: batchLoss = 2.9967, diffLoss = 14.6630, kgLoss = 0.0802
2025-02-12 22:56:18.915783: Training Step 45/59: batchLoss = 3.5089, diffLoss = 17.1574, kgLoss = 0.0967
2025-02-12 22:56:19.852894: Training Step 46/59: batchLoss = 3.8418, diffLoss = 18.7939, kgLoss = 0.1038
2025-02-12 22:56:20.791682: Training Step 47/59: batchLoss = 3.4035, diffLoss = 16.6364, kgLoss = 0.0952
2025-02-12 22:56:21.733977: Training Step 48/59: batchLoss = 3.2811, diffLoss = 16.0369, kgLoss = 0.0922
2025-02-12 22:56:22.670277: Training Step 49/59: batchLoss = 3.2836, diffLoss = 16.0319, kgLoss = 0.0965
2025-02-12 22:56:23.598121: Training Step 50/59: batchLoss = 3.3027, diffLoss = 16.1288, kgLoss = 0.0961
2025-02-12 22:56:24.524875: Training Step 51/59: batchLoss = 2.7623, diffLoss = 13.5072, kgLoss = 0.0761
2025-02-12 22:56:25.451834: Training Step 52/59: batchLoss = 2.9852, diffLoss = 14.5801, kgLoss = 0.0865
2025-02-12 22:56:26.397531: Training Step 53/59: batchLoss = 2.9705, diffLoss = 14.5129, kgLoss = 0.0849
2025-02-12 22:56:27.336052: Training Step 54/59: batchLoss = 2.9534, diffLoss = 14.4275, kgLoss = 0.0848
2025-02-12 22:56:28.261214: Training Step 55/59: batchLoss = 2.8306, diffLoss = 13.8405, kgLoss = 0.0781
2025-02-12 22:56:29.182040: Training Step 56/59: batchLoss = 3.3724, diffLoss = 16.4784, kgLoss = 0.0960
2025-02-12 22:56:30.015445: Training Step 57/59: batchLoss = 2.7791, diffLoss = 13.5850, kgLoss = 0.0776
2025-02-12 22:56:30.864049: Training Step 58/59: batchLoss = 3.1239, diffLoss = 15.2801, kgLoss = 0.0849
2025-02-12 22:56:30.953048: 
2025-02-12 22:56:30.953389: Epoch 24/1000, Train: epLoss = 0.4394, epDfLoss = 2.1460, epKgLoss = 0.0127  
2025-02-12 22:56:32.417783: Steps 0/47: batch_recall = 35.69, batch_ndcg = 43.50 
2025-02-12 22:56:33.726828: Steps 1/47: batch_recall = 34.90, batch_ndcg = 38.62 
2025-02-12 22:56:34.977432: Steps 2/47: batch_recall = 37.64, batch_ndcg = 43.45 
2025-02-12 22:56:36.252164: Steps 3/47: batch_recall = 42.38, batch_ndcg = 42.61 
2025-02-12 22:56:37.454886: Steps 4/47: batch_recall = 37.33, batch_ndcg = 41.67 
2025-02-12 22:56:38.672453: Steps 5/47: batch_recall = 31.58, batch_ndcg = 36.31 
2025-02-12 22:56:39.883555: Steps 6/47: batch_recall = 37.29, batch_ndcg = 39.94 
2025-02-12 22:56:41.068070: Steps 7/47: batch_recall = 43.36, batch_ndcg = 42.24 
2025-02-12 22:56:42.248591: Steps 8/47: batch_recall = 46.04, batch_ndcg = 47.63 
2025-02-12 22:56:43.399399: Steps 9/47: batch_recall = 46.17, batch_ndcg = 45.17 
2025-02-12 22:56:44.560346: Steps 10/47: batch_recall = 42.79, batch_ndcg = 41.04 
2025-02-12 22:56:45.698281: Steps 11/47: batch_recall = 52.10, batch_ndcg = 48.22 
2025-02-12 22:56:46.836784: Steps 12/47: batch_recall = 47.84, batch_ndcg = 45.65 
2025-02-12 22:56:47.976182: Steps 13/47: batch_recall = 47.55, batch_ndcg = 44.09 
2025-02-12 22:56:49.073700: Steps 14/47: batch_recall = 39.15, batch_ndcg = 37.87 
2025-02-12 22:56:50.161948: Steps 15/47: batch_recall = 56.12, batch_ndcg = 50.54 
2025-02-12 22:56:51.247159: Steps 16/47: batch_recall = 47.76, batch_ndcg = 42.28 
2025-02-12 22:56:52.340172: Steps 17/47: batch_recall = 55.86, batch_ndcg = 47.51 
2025-02-12 22:56:53.423113: Steps 18/47: batch_recall = 48.75, batch_ndcg = 45.80 
2025-02-12 22:56:54.500901: Steps 19/47: batch_recall = 60.10, batch_ndcg = 53.49 
2025-02-12 22:56:55.552960: Steps 20/47: batch_recall = 64.82, batch_ndcg = 58.16 
2025-02-12 22:56:56.610381: Steps 21/47: batch_recall = 62.86, batch_ndcg = 52.11 
2025-02-12 22:56:57.676600: Steps 22/47: batch_recall = 54.02, batch_ndcg = 49.62 
2025-02-12 22:56:58.734744: Steps 23/47: batch_recall = 61.32, batch_ndcg = 51.56 
2025-02-12 22:56:59.806521: Steps 24/47: batch_recall = 63.33, batch_ndcg = 55.14 
2025-02-12 22:57:00.847740: Steps 25/47: batch_recall = 63.39, batch_ndcg = 53.82 
2025-02-12 22:57:01.871340: Steps 26/47: batch_recall = 57.60, batch_ndcg = 50.40 
2025-02-12 22:57:02.898364: Steps 27/47: batch_recall = 59.84, batch_ndcg = 48.91 
2025-02-12 22:57:03.915150: Steps 28/47: batch_recall = 68.08, batch_ndcg = 56.02 
2025-02-12 22:57:04.942284: Steps 29/47: batch_recall = 65.82, batch_ndcg = 54.73 
2025-02-12 22:57:05.950840: Steps 30/47: batch_recall = 72.78, batch_ndcg = 61.63 
2025-02-12 22:57:06.979017: Steps 31/47: batch_recall = 64.15, batch_ndcg = 51.92 
2025-02-12 22:57:07.991026: Steps 32/47: batch_recall = 68.44, batch_ndcg = 61.44 
2025-02-12 22:57:09.002972: Steps 33/47: batch_recall = 77.32, batch_ndcg = 63.91 
2025-02-12 22:57:10.015019: Steps 34/47: batch_recall = 64.71, batch_ndcg = 51.40 
2025-02-12 22:57:11.003822: Steps 35/47: batch_recall = 73.22, batch_ndcg = 60.35 
2025-02-12 22:57:12.003754: Steps 36/47: batch_recall = 77.54, batch_ndcg = 62.34 
2025-02-12 22:57:12.984597: Steps 37/47: batch_recall = 82.57, batch_ndcg = 69.66 
2025-02-12 22:57:13.966203: Steps 38/47: batch_recall = 87.55, batch_ndcg = 71.55 
2025-02-12 22:57:14.988313: Steps 39/47: batch_recall = 89.26, batch_ndcg = 66.47 
2025-02-12 22:57:15.960473: Steps 40/47: batch_recall = 69.07, batch_ndcg = 59.93 
2025-02-12 22:57:16.935343: Steps 41/47: batch_recall = 86.63, batch_ndcg = 69.17 
2025-02-12 22:57:17.892306: Steps 42/47: batch_recall = 83.82, batch_ndcg = 62.82 
2025-02-12 22:57:18.866663: Steps 43/47: batch_recall = 89.55, batch_ndcg = 68.45 
2025-02-12 22:57:19.845168: Steps 44/47: batch_recall = 86.25, batch_ndcg = 67.63 
2025-02-12 22:57:20.785714: Steps 45/47: batch_recall = 91.78, batch_ndcg = 72.54 
2025-02-12 22:57:20.891470: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.57 
2025-02-12 22:57:20.891606: Epoch 24/1000, Test: Recall = 0.1179, NDCG = 0.1032  

2025-02-12 22:57:22.078802: Training Step 0/59: batchLoss = 2.8101, diffLoss = 13.7215, kgLoss = 0.0822
2025-02-12 22:57:23.006731: Training Step 1/59: batchLoss = 3.0520, diffLoss = 14.8736, kgLoss = 0.0966
2025-02-12 22:57:23.929394: Training Step 2/59: batchLoss = 2.9374, diffLoss = 14.3426, kgLoss = 0.0860
2025-02-12 22:57:24.853825: Training Step 3/59: batchLoss = 2.6245, diffLoss = 12.8035, kgLoss = 0.0798
2025-02-12 22:57:25.781266: Training Step 4/59: batchLoss = 2.9015, diffLoss = 14.1350, kgLoss = 0.0931
2025-02-12 22:57:26.708568: Training Step 5/59: batchLoss = 3.1252, diffLoss = 15.2765, kgLoss = 0.0874
2025-02-12 22:57:27.637990: Training Step 6/59: batchLoss = 2.8768, diffLoss = 14.0313, kgLoss = 0.0881
2025-02-12 22:57:28.553325: Training Step 7/59: batchLoss = 2.9004, diffLoss = 14.1439, kgLoss = 0.0895
2025-02-12 22:57:29.478997: Training Step 8/59: batchLoss = 2.4732, diffLoss = 12.0778, kgLoss = 0.0720
2025-02-12 22:57:30.412749: Training Step 9/59: batchLoss = 2.7370, diffLoss = 13.3419, kgLoss = 0.0858
2025-02-12 22:57:31.349471: Training Step 10/59: batchLoss = 2.8853, diffLoss = 14.0802, kgLoss = 0.0866
2025-02-12 22:57:32.281462: Training Step 11/59: batchLoss = 2.7426, diffLoss = 13.4144, kgLoss = 0.0746
2025-02-12 22:57:33.211889: Training Step 12/59: batchLoss = 2.9586, diffLoss = 14.4573, kgLoss = 0.0839
2025-02-12 22:57:34.150612: Training Step 13/59: batchLoss = 2.9363, diffLoss = 14.3313, kgLoss = 0.0875
2025-02-12 22:57:35.085724: Training Step 14/59: batchLoss = 2.6501, diffLoss = 12.9255, kgLoss = 0.0813
2025-02-12 22:57:36.020838: Training Step 15/59: batchLoss = 2.6697, diffLoss = 13.0298, kgLoss = 0.0796
2025-02-12 22:57:36.970888: Training Step 16/59: batchLoss = 2.9257, diffLoss = 14.2992, kgLoss = 0.0824
2025-02-12 22:57:37.902047: Training Step 17/59: batchLoss = 2.6712, diffLoss = 13.0052, kgLoss = 0.0877
2025-02-12 22:57:38.835328: Training Step 18/59: batchLoss = 3.1280, diffLoss = 15.2822, kgLoss = 0.0895
2025-02-12 22:57:39.759254: Training Step 19/59: batchLoss = 3.0274, diffLoss = 14.7914, kgLoss = 0.0864
2025-02-12 22:57:40.689532: Training Step 20/59: batchLoss = 2.6717, diffLoss = 13.0384, kgLoss = 0.0800
2025-02-12 22:57:41.614166: Training Step 21/59: batchLoss = 3.1403, diffLoss = 15.3148, kgLoss = 0.0967
2025-02-12 22:57:42.537605: Training Step 22/59: batchLoss = 2.9257, diffLoss = 14.3008, kgLoss = 0.0819
2025-02-12 22:57:43.469840: Training Step 23/59: batchLoss = 2.8248, diffLoss = 13.7995, kgLoss = 0.0811
2025-02-12 22:57:44.399482: Training Step 24/59: batchLoss = 2.8640, diffLoss = 13.9730, kgLoss = 0.0868
2025-02-12 22:57:45.332670: Training Step 25/59: batchLoss = 2.8440, diffLoss = 13.8753, kgLoss = 0.0861
2025-02-12 22:57:46.251403: Training Step 26/59: batchLoss = 2.9206, diffLoss = 14.2531, kgLoss = 0.0874
2025-02-12 22:57:47.177623: Training Step 27/59: batchLoss = 2.8372, diffLoss = 13.8622, kgLoss = 0.0809
2025-02-12 22:57:48.107973: Training Step 28/59: batchLoss = 3.0560, diffLoss = 14.9351, kgLoss = 0.0862
2025-02-12 22:57:49.038526: Training Step 29/59: batchLoss = 2.7839, diffLoss = 13.5721, kgLoss = 0.0869
2025-02-12 22:57:49.974130: Training Step 30/59: batchLoss = 3.3248, diffLoss = 16.2133, kgLoss = 0.1026
2025-02-12 22:57:50.905022: Training Step 31/59: batchLoss = 3.0594, diffLoss = 14.9336, kgLoss = 0.0908
2025-02-12 22:57:51.841838: Training Step 32/59: batchLoss = 2.7291, diffLoss = 13.3271, kgLoss = 0.0796
2025-02-12 22:57:52.792489: Training Step 33/59: batchLoss = 2.8953, diffLoss = 14.1092, kgLoss = 0.0918
2025-02-12 22:57:53.736870: Training Step 34/59: batchLoss = 2.7726, diffLoss = 13.4940, kgLoss = 0.0922
2025-02-12 22:57:54.678256: Training Step 35/59: batchLoss = 2.9630, diffLoss = 14.4658, kgLoss = 0.0873
2025-02-12 22:57:55.616017: Training Step 36/59: batchLoss = 3.2727, diffLoss = 15.9919, kgLoss = 0.0929
2025-02-12 22:57:56.553280: Training Step 37/59: batchLoss = 2.9443, diffLoss = 14.3829, kgLoss = 0.0847
2025-02-12 22:57:57.492459: Training Step 38/59: batchLoss = 2.9524, diffLoss = 14.4330, kgLoss = 0.0822
2025-02-12 22:57:58.421752: Training Step 39/59: batchLoss = 3.0626, diffLoss = 14.9664, kgLoss = 0.0867
2025-02-12 22:57:59.352194: Training Step 40/59: batchLoss = 2.6892, diffLoss = 13.1507, kgLoss = 0.0738
2025-02-12 22:58:00.298759: Training Step 41/59: batchLoss = 2.9682, diffLoss = 14.5033, kgLoss = 0.0844
2025-02-12 22:58:01.237607: Training Step 42/59: batchLoss = 2.8402, diffLoss = 13.8726, kgLoss = 0.0822
2025-02-12 22:58:02.168169: Training Step 43/59: batchLoss = 3.0088, diffLoss = 14.7178, kgLoss = 0.0815
2025-02-12 22:58:03.084243: Training Step 44/59: batchLoss = 2.7340, diffLoss = 13.3495, kgLoss = 0.0801
2025-02-12 22:58:04.012925: Training Step 45/59: batchLoss = 3.2721, diffLoss = 15.9770, kgLoss = 0.0959
2025-02-12 22:58:04.931492: Training Step 46/59: batchLoss = 2.9895, diffLoss = 14.6172, kgLoss = 0.0826
2025-02-12 22:58:05.852473: Training Step 47/59: batchLoss = 3.2316, diffLoss = 15.7957, kgLoss = 0.0906
2025-02-12 22:58:06.776517: Training Step 48/59: batchLoss = 2.9686, diffLoss = 14.4830, kgLoss = 0.0899
2025-02-12 22:58:07.709876: Training Step 49/59: batchLoss = 3.1002, diffLoss = 15.1439, kgLoss = 0.0892
2025-02-12 22:58:08.645928: Training Step 50/59: batchLoss = 3.1233, diffLoss = 15.2547, kgLoss = 0.0905
2025-02-12 22:58:09.585637: Training Step 51/59: batchLoss = 2.8509, diffLoss = 13.9391, kgLoss = 0.0788
2025-02-12 22:58:10.522151: Training Step 52/59: batchLoss = 3.2864, diffLoss = 16.0796, kgLoss = 0.0881
2025-02-12 22:58:11.459524: Training Step 53/59: batchLoss = 3.5323, diffLoss = 17.2474, kgLoss = 0.1035
2025-02-12 22:58:12.396187: Training Step 54/59: batchLoss = 2.8964, diffLoss = 14.1561, kgLoss = 0.0815
2025-02-12 22:58:13.332125: Training Step 55/59: batchLoss = 3.0380, diffLoss = 14.8212, kgLoss = 0.0922
2025-02-12 22:58:14.269774: Training Step 56/59: batchLoss = 2.9318, diffLoss = 14.3375, kgLoss = 0.0803
2025-02-12 22:58:15.123067: Training Step 57/59: batchLoss = 2.6962, diffLoss = 13.1542, kgLoss = 0.0817
2025-02-12 22:58:15.977458: Training Step 58/59: batchLoss = 3.3801, diffLoss = 16.5439, kgLoss = 0.0892
2025-02-12 22:58:16.079622: 
2025-02-12 22:58:16.080368: Epoch 25/1000, Train: epLoss = 0.4335, epDfLoss = 2.1169, epKgLoss = 0.0127  
2025-02-12 22:58:17.567402: Steps 0/47: batch_recall = 35.91, batch_ndcg = 44.17 
2025-02-12 22:58:18.862664: Steps 1/47: batch_recall = 35.31, batch_ndcg = 39.20 
2025-02-12 22:58:20.117484: Steps 2/47: batch_recall = 38.67, batch_ndcg = 44.55 
2025-02-12 22:58:21.375406: Steps 3/47: batch_recall = 42.32, batch_ndcg = 43.71 
2025-02-12 22:58:22.562995: Steps 4/47: batch_recall = 38.47, batch_ndcg = 42.69 
2025-02-12 22:58:23.768829: Steps 5/47: batch_recall = 31.41, batch_ndcg = 36.39 
2025-02-12 22:58:24.953345: Steps 6/47: batch_recall = 36.83, batch_ndcg = 39.94 
2025-02-12 22:58:26.122922: Steps 7/47: batch_recall = 43.43, batch_ndcg = 42.85 
2025-02-12 22:58:27.307713: Steps 8/47: batch_recall = 46.76, batch_ndcg = 47.84 
2025-02-12 22:58:28.467558: Steps 9/47: batch_recall = 44.78, batch_ndcg = 44.55 
2025-02-12 22:58:29.643105: Steps 10/47: batch_recall = 42.60, batch_ndcg = 41.79 
2025-02-12 22:58:30.787642: Steps 11/47: batch_recall = 52.86, batch_ndcg = 49.38 
2025-02-12 22:58:31.938181: Steps 12/47: batch_recall = 48.24, batch_ndcg = 45.94 
2025-02-12 22:58:33.075679: Steps 13/47: batch_recall = 48.36, batch_ndcg = 44.71 
2025-02-12 22:58:34.184428: Steps 14/47: batch_recall = 39.31, batch_ndcg = 38.44 
2025-02-12 22:58:35.304114: Steps 15/47: batch_recall = 54.65, batch_ndcg = 49.82 
2025-02-12 22:58:36.395264: Steps 16/47: batch_recall = 48.32, batch_ndcg = 43.64 
2025-02-12 22:58:37.448307: Steps 17/47: batch_recall = 54.74, batch_ndcg = 47.40 
2025-02-12 22:58:38.519452: Steps 18/47: batch_recall = 49.77, batch_ndcg = 46.42 
2025-02-12 22:58:39.581666: Steps 19/47: batch_recall = 59.45, batch_ndcg = 53.39 
2025-02-12 22:58:40.628141: Steps 20/47: batch_recall = 65.87, batch_ndcg = 59.22 
2025-02-12 22:58:41.665052: Steps 21/47: batch_recall = 63.24, batch_ndcg = 53.09 
2025-02-12 22:58:42.711585: Steps 22/47: batch_recall = 54.73, batch_ndcg = 49.93 
2025-02-12 22:58:43.762771: Steps 23/47: batch_recall = 60.94, batch_ndcg = 51.65 
2025-02-12 22:58:44.820493: Steps 24/47: batch_recall = 63.03, batch_ndcg = 55.39 
2025-02-12 22:58:45.863229: Steps 25/47: batch_recall = 64.08, batch_ndcg = 55.04 
2025-02-12 22:58:46.884121: Steps 26/47: batch_recall = 58.61, batch_ndcg = 50.71 
2025-02-12 22:58:47.930105: Steps 27/47: batch_recall = 60.55, batch_ndcg = 49.99 
2025-02-12 22:58:48.960304: Steps 28/47: batch_recall = 68.99, batch_ndcg = 56.92 
2025-02-12 22:58:49.982817: Steps 29/47: batch_recall = 68.16, batch_ndcg = 55.62 
2025-02-12 22:58:51.008914: Steps 30/47: batch_recall = 73.07, batch_ndcg = 62.20 
2025-02-12 22:58:52.039851: Steps 31/47: batch_recall = 64.93, batch_ndcg = 53.29 
2025-02-12 22:58:53.065824: Steps 32/47: batch_recall = 67.97, batch_ndcg = 61.50 
2025-02-12 22:58:54.088506: Steps 33/47: batch_recall = 77.75, batch_ndcg = 64.13 
2025-02-12 22:58:55.102719: Steps 34/47: batch_recall = 66.81, batch_ndcg = 52.38 
2025-02-12 22:58:56.077494: Steps 35/47: batch_recall = 74.58, batch_ndcg = 61.91 
2025-02-12 22:58:57.066780: Steps 36/47: batch_recall = 78.84, batch_ndcg = 63.71 
2025-02-12 22:58:58.040197: Steps 37/47: batch_recall = 81.24, batch_ndcg = 69.23 
2025-02-12 22:58:59.010965: Steps 38/47: batch_recall = 89.69, batch_ndcg = 72.45 
2025-02-12 22:58:59.974819: Steps 39/47: batch_recall = 87.79, batch_ndcg = 67.04 
2025-02-12 22:59:00.937961: Steps 40/47: batch_recall = 70.22, batch_ndcg = 60.33 
2025-02-12 22:59:01.898997: Steps 41/47: batch_recall = 87.19, batch_ndcg = 69.64 
2025-02-12 22:59:02.857442: Steps 42/47: batch_recall = 82.30, batch_ndcg = 62.74 
2025-02-12 22:59:03.811813: Steps 43/47: batch_recall = 88.92, batch_ndcg = 68.24 
2025-02-12 22:59:04.779638: Steps 44/47: batch_recall = 88.07, batch_ndcg = 69.07 
2025-02-12 22:59:05.728653: Steps 45/47: batch_recall = 92.71, batch_ndcg = 73.18 
2025-02-12 22:59:05.833794: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.49 
2025-02-12 22:59:05.833892: Epoch 25/1000, Test: Recall = 0.1186, NDCG = 0.1043  

2025-02-12 22:59:07.037327: Training Step 0/59: batchLoss = 2.7759, diffLoss = 13.5455, kgLoss = 0.0835
2025-02-12 22:59:07.979915: Training Step 1/59: batchLoss = 2.9901, diffLoss = 14.6071, kgLoss = 0.0858
2025-02-12 22:59:08.912835: Training Step 2/59: batchLoss = 2.8482, diffLoss = 13.9031, kgLoss = 0.0844
2025-02-12 22:59:09.851538: Training Step 3/59: batchLoss = 3.0356, diffLoss = 14.8403, kgLoss = 0.0844
2025-02-12 22:59:10.790823: Training Step 4/59: batchLoss = 2.9671, diffLoss = 14.4938, kgLoss = 0.0854
2025-02-12 22:59:11.734541: Training Step 5/59: batchLoss = 3.3120, diffLoss = 16.0984, kgLoss = 0.1154
2025-02-12 22:59:12.669193: Training Step 6/59: batchLoss = 2.9565, diffLoss = 14.4025, kgLoss = 0.0950
2025-02-12 22:59:13.611362: Training Step 7/59: batchLoss = 3.0751, diffLoss = 15.0198, kgLoss = 0.0889
2025-02-12 22:59:14.547326: Training Step 8/59: batchLoss = 2.6686, diffLoss = 13.0140, kgLoss = 0.0823
2025-02-12 22:59:15.491479: Training Step 9/59: batchLoss = 2.7260, diffLoss = 13.2961, kgLoss = 0.0835
2025-02-12 22:59:16.411594: Training Step 10/59: batchLoss = 2.9138, diffLoss = 14.2270, kgLoss = 0.0856
2025-02-12 22:59:17.338536: Training Step 11/59: batchLoss = 2.9757, diffLoss = 14.5419, kgLoss = 0.0841
2025-02-12 22:59:18.264224: Training Step 12/59: batchLoss = 2.7965, diffLoss = 13.6502, kgLoss = 0.0830
2025-02-12 22:59:19.193216: Training Step 13/59: batchLoss = 2.4877, diffLoss = 12.1419, kgLoss = 0.0741
2025-02-12 22:59:20.117908: Training Step 14/59: batchLoss = 2.4996, diffLoss = 12.2049, kgLoss = 0.0733
2025-02-12 22:59:21.047989: Training Step 15/59: batchLoss = 2.9562, diffLoss = 14.4431, kgLoss = 0.0845
2025-02-12 22:59:21.978990: Training Step 16/59: batchLoss = 2.7222, diffLoss = 13.2886, kgLoss = 0.0806
2025-02-12 22:59:22.898057: Training Step 17/59: batchLoss = 2.9409, diffLoss = 14.3707, kgLoss = 0.0835
2025-02-12 22:59:23.833050: Training Step 18/59: batchLoss = 2.8281, diffLoss = 13.8091, kgLoss = 0.0829
2025-02-12 22:59:24.764031: Training Step 19/59: batchLoss = 2.8673, diffLoss = 13.9858, kgLoss = 0.0877
2025-02-12 22:59:25.697666: Training Step 20/59: batchLoss = 3.0538, diffLoss = 14.8690, kgLoss = 0.1000
2025-02-12 22:59:26.633785: Training Step 21/59: batchLoss = 2.8197, diffLoss = 13.7612, kgLoss = 0.0844
2025-02-12 22:59:27.562170: Training Step 22/59: batchLoss = 2.9867, diffLoss = 14.5973, kgLoss = 0.0841
2025-02-12 22:59:28.497783: Training Step 23/59: batchLoss = 2.9357, diffLoss = 14.3253, kgLoss = 0.0883
2025-02-12 22:59:29.433033: Training Step 24/59: batchLoss = 2.9003, diffLoss = 14.1795, kgLoss = 0.0805
2025-02-12 22:59:30.367929: Training Step 25/59: batchLoss = 2.8980, diffLoss = 14.1664, kgLoss = 0.0809
2025-02-12 22:59:31.297484: Training Step 26/59: batchLoss = 2.9217, diffLoss = 14.2297, kgLoss = 0.0947
2025-02-12 22:59:32.241938: Training Step 27/59: batchLoss = 2.8885, diffLoss = 14.1080, kgLoss = 0.0836
2025-02-12 22:59:33.185698: Training Step 28/59: batchLoss = 2.8417, diffLoss = 13.8843, kgLoss = 0.0811
2025-02-12 22:59:34.115689: Training Step 29/59: batchLoss = 2.6686, diffLoss = 13.0201, kgLoss = 0.0807
2025-02-12 22:59:35.044539: Training Step 30/59: batchLoss = 3.0160, diffLoss = 14.7514, kgLoss = 0.0822
2025-02-12 22:59:35.976534: Training Step 31/59: batchLoss = 3.0701, diffLoss = 14.9707, kgLoss = 0.0949
2025-02-12 22:59:36.906317: Training Step 32/59: batchLoss = 2.6013, diffLoss = 12.6978, kgLoss = 0.0772
2025-02-12 22:59:37.833796: Training Step 33/59: batchLoss = 3.1316, diffLoss = 15.2873, kgLoss = 0.0926
2025-02-12 22:59:38.772149: Training Step 34/59: batchLoss = 3.0260, diffLoss = 14.7805, kgLoss = 0.0873
2025-02-12 22:59:39.698604: Training Step 35/59: batchLoss = 3.0694, diffLoss = 15.0072, kgLoss = 0.0850
2025-02-12 22:59:40.625000: Training Step 36/59: batchLoss = 2.9802, diffLoss = 14.5640, kgLoss = 0.0843
2025-02-12 22:59:41.555638: Training Step 37/59: batchLoss = 2.9289, diffLoss = 14.3097, kgLoss = 0.0837
2025-02-12 22:59:42.480981: Training Step 38/59: batchLoss = 2.9547, diffLoss = 14.4296, kgLoss = 0.0860
2025-02-12 22:59:43.420929: Training Step 39/59: batchLoss = 2.9402, diffLoss = 14.3694, kgLoss = 0.0829
2025-02-12 22:59:44.366750: Training Step 40/59: batchLoss = 3.0115, diffLoss = 14.7128, kgLoss = 0.0861
2025-02-12 22:59:45.307835: Training Step 41/59: batchLoss = 3.1497, diffLoss = 15.3830, kgLoss = 0.0913
2025-02-12 22:59:46.250987: Training Step 42/59: batchLoss = 2.9282, diffLoss = 14.3213, kgLoss = 0.0799
2025-02-12 22:59:47.190888: Training Step 43/59: batchLoss = 3.4853, diffLoss = 17.0449, kgLoss = 0.0953
2025-02-12 22:59:48.126330: Training Step 44/59: batchLoss = 2.9291, diffLoss = 14.3174, kgLoss = 0.0820
2025-02-12 22:59:49.067316: Training Step 45/59: batchLoss = 3.0872, diffLoss = 15.0808, kgLoss = 0.0888
2025-02-12 22:59:50.008003: Training Step 46/59: batchLoss = 3.1538, diffLoss = 15.4089, kgLoss = 0.0901
2025-02-12 22:59:50.944948: Training Step 47/59: batchLoss = 3.0423, diffLoss = 14.8787, kgLoss = 0.0832
2025-02-12 22:59:51.881351: Training Step 48/59: batchLoss = 3.5372, diffLoss = 17.2985, kgLoss = 0.0968
2025-02-12 22:59:52.807017: Training Step 49/59: batchLoss = 3.4460, diffLoss = 16.8298, kgLoss = 0.1000
2025-02-12 22:59:53.734896: Training Step 50/59: batchLoss = 3.2024, diffLoss = 15.6672, kgLoss = 0.0862
2025-02-12 22:59:54.663082: Training Step 51/59: batchLoss = 3.1238, diffLoss = 15.2579, kgLoss = 0.0902
2025-02-12 22:59:55.585163: Training Step 52/59: batchLoss = 2.9160, diffLoss = 14.2424, kgLoss = 0.0844
2025-02-12 22:59:56.523673: Training Step 53/59: batchLoss = 3.0247, diffLoss = 14.7843, kgLoss = 0.0848
2025-02-12 22:59:57.445601: Training Step 54/59: batchLoss = 2.7958, diffLoss = 13.6630, kgLoss = 0.0790
2025-02-12 22:59:58.379667: Training Step 55/59: batchLoss = 2.9798, diffLoss = 14.5787, kgLoss = 0.0801
2025-02-12 22:59:59.302206: Training Step 56/59: batchLoss = 3.1460, diffLoss = 15.3813, kgLoss = 0.0872
2025-02-12 23:00:00.139226: Training Step 57/59: batchLoss = 2.8468, diffLoss = 13.9155, kgLoss = 0.0797
2025-02-12 23:00:00.981712: Training Step 58/59: batchLoss = 3.2987, diffLoss = 16.1301, kgLoss = 0.0908
2025-02-12 23:00:01.072916: 
2025-02-12 23:00:01.073431: Epoch 26/1000, Train: epLoss = 0.4377, epDfLoss = 2.1377, epKgLoss = 0.0127  
2025-02-12 23:00:02.553663: Steps 0/47: batch_recall = 35.47, batch_ndcg = 44.04 
2025-02-12 23:00:03.874347: Steps 1/47: batch_recall = 34.54, batch_ndcg = 39.37 
2025-02-12 23:00:05.141914: Steps 2/47: batch_recall = 39.04, batch_ndcg = 44.29 
2025-02-12 23:00:06.406972: Steps 3/47: batch_recall = 42.96, batch_ndcg = 43.78 
2025-02-12 23:00:07.613378: Steps 4/47: batch_recall = 37.80, batch_ndcg = 42.20 
2025-02-12 23:00:08.837932: Steps 5/47: batch_recall = 31.49, batch_ndcg = 36.95 
2025-02-12 23:00:10.041846: Steps 6/47: batch_recall = 37.81, batch_ndcg = 39.95 
2025-02-12 23:00:11.223614: Steps 7/47: batch_recall = 43.10, batch_ndcg = 43.19 
2025-02-12 23:00:12.394534: Steps 8/47: batch_recall = 46.79, batch_ndcg = 48.21 
2025-02-12 23:00:13.536562: Steps 9/47: batch_recall = 45.13, batch_ndcg = 44.58 
2025-02-12 23:00:14.691432: Steps 10/47: batch_recall = 42.04, batch_ndcg = 41.36 
2025-02-12 23:00:15.822073: Steps 11/47: batch_recall = 53.48, batch_ndcg = 49.39 
2025-02-12 23:00:16.960673: Steps 12/47: batch_recall = 49.24, batch_ndcg = 47.38 
2025-02-12 23:00:18.087077: Steps 13/47: batch_recall = 48.65, batch_ndcg = 45.08 
2025-02-12 23:00:19.188802: Steps 14/47: batch_recall = 39.50, batch_ndcg = 39.00 
2025-02-12 23:00:20.284493: Steps 15/47: batch_recall = 55.13, batch_ndcg = 50.54 
2025-02-12 23:00:21.375254: Steps 16/47: batch_recall = 48.80, batch_ndcg = 44.54 
2025-02-12 23:00:22.439844: Steps 17/47: batch_recall = 55.64, batch_ndcg = 48.18 
2025-02-12 23:00:23.530312: Steps 18/47: batch_recall = 50.36, batch_ndcg = 46.26 
2025-02-12 23:00:24.599079: Steps 19/47: batch_recall = 59.14, batch_ndcg = 53.77 
2025-02-12 23:00:25.656372: Steps 20/47: batch_recall = 65.67, batch_ndcg = 59.59 
2025-02-12 23:00:26.714398: Steps 21/47: batch_recall = 62.45, batch_ndcg = 52.65 
2025-02-12 23:00:27.778649: Steps 22/47: batch_recall = 54.61, batch_ndcg = 49.45 
2025-02-12 23:00:28.846928: Steps 23/47: batch_recall = 62.34, batch_ndcg = 52.78 
2025-02-12 23:00:29.921825: Steps 24/47: batch_recall = 64.96, batch_ndcg = 56.24 
2025-02-12 23:00:30.963700: Steps 25/47: batch_recall = 65.96, batch_ndcg = 56.22 
2025-02-12 23:00:31.975617: Steps 26/47: batch_recall = 58.58, batch_ndcg = 51.50 
2025-02-12 23:00:33.007662: Steps 27/47: batch_recall = 60.56, batch_ndcg = 50.00 
2025-02-12 23:00:34.013447: Steps 28/47: batch_recall = 69.89, batch_ndcg = 57.28 
2025-02-12 23:00:35.018441: Steps 29/47: batch_recall = 68.36, batch_ndcg = 55.69 
2025-02-12 23:00:36.019057: Steps 30/47: batch_recall = 72.24, batch_ndcg = 62.37 
2025-02-12 23:00:37.038516: Steps 31/47: batch_recall = 65.06, batch_ndcg = 53.59 
2025-02-12 23:00:38.039215: Steps 32/47: batch_recall = 69.32, batch_ndcg = 62.33 
2025-02-12 23:00:39.058018: Steps 33/47: batch_recall = 78.13, batch_ndcg = 64.56 
2025-02-12 23:00:40.083687: Steps 34/47: batch_recall = 65.66, batch_ndcg = 52.92 
2025-02-12 23:00:41.082476: Steps 35/47: batch_recall = 76.03, batch_ndcg = 63.37 
2025-02-12 23:00:42.078699: Steps 36/47: batch_recall = 78.54, batch_ndcg = 64.49 
2025-02-12 23:00:43.067722: Steps 37/47: batch_recall = 82.06, batch_ndcg = 70.36 
2025-02-12 23:00:44.050175: Steps 38/47: batch_recall = 87.89, batch_ndcg = 70.41 
2025-02-12 23:00:45.034537: Steps 39/47: batch_recall = 87.95, batch_ndcg = 66.67 
2025-02-12 23:00:46.011206: Steps 40/47: batch_recall = 70.86, batch_ndcg = 61.08 
2025-02-12 23:00:46.986526: Steps 41/47: batch_recall = 87.67, batch_ndcg = 69.32 
2025-02-12 23:00:47.957432: Steps 42/47: batch_recall = 82.11, batch_ndcg = 63.33 
2025-02-12 23:00:48.926153: Steps 43/47: batch_recall = 90.08, batch_ndcg = 69.69 
2025-02-12 23:00:49.893524: Steps 44/47: batch_recall = 87.03, batch_ndcg = 68.75 
2025-02-12 23:00:50.832832: Steps 45/47: batch_recall = 93.16, batch_ndcg = 73.02 
2025-02-12 23:00:50.937501: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.53 
2025-02-12 23:00:50.937630: Epoch 26/1000, Test: Recall = 0.1190, NDCG = 0.1049  

2025-02-12 23:00:52.131357: Training Step 0/59: batchLoss = 2.7454, diffLoss = 13.3792, kgLoss = 0.0870
2025-02-12 23:00:53.060767: Training Step 1/59: batchLoss = 2.9432, diffLoss = 14.3370, kgLoss = 0.0948
2025-02-12 23:00:53.991128: Training Step 2/59: batchLoss = 2.8907, diffLoss = 14.1164, kgLoss = 0.0843
2025-02-12 23:00:54.914562: Training Step 3/59: batchLoss = 2.9461, diffLoss = 14.3732, kgLoss = 0.0894
2025-02-12 23:00:55.837774: Training Step 4/59: batchLoss = 2.8520, diffLoss = 13.9297, kgLoss = 0.0826
2025-02-12 23:00:56.759341: Training Step 5/59: batchLoss = 2.6404, diffLoss = 12.9057, kgLoss = 0.0741
2025-02-12 23:00:57.687731: Training Step 6/59: batchLoss = 2.5988, diffLoss = 12.6735, kgLoss = 0.0802
2025-02-12 23:00:58.614265: Training Step 7/59: batchLoss = 2.9560, diffLoss = 14.4095, kgLoss = 0.0926
2025-02-12 23:00:59.553593: Training Step 8/59: batchLoss = 2.8641, diffLoss = 13.9986, kgLoss = 0.0804
2025-02-12 23:01:00.488249: Training Step 9/59: batchLoss = 3.0130, diffLoss = 14.7247, kgLoss = 0.0850
2025-02-12 23:01:01.428761: Training Step 10/59: batchLoss = 3.0424, diffLoss = 14.8418, kgLoss = 0.0925
2025-02-12 23:01:02.362294: Training Step 11/59: batchLoss = 2.7955, diffLoss = 13.6215, kgLoss = 0.0890
2025-02-12 23:01:03.292255: Training Step 12/59: batchLoss = 3.3334, diffLoss = 16.2585, kgLoss = 0.1021
2025-02-12 23:01:04.217076: Training Step 13/59: batchLoss = 2.9948, diffLoss = 14.6251, kgLoss = 0.0873
2025-02-12 23:01:05.153172: Training Step 14/59: batchLoss = 2.7505, diffLoss = 13.4169, kgLoss = 0.0839
2025-02-12 23:01:06.093491: Training Step 15/59: batchLoss = 2.7556, diffLoss = 13.4654, kgLoss = 0.0781
2025-02-12 23:01:07.023192: Training Step 16/59: batchLoss = 2.7453, diffLoss = 13.3936, kgLoss = 0.0833
2025-02-12 23:01:07.957725: Training Step 17/59: batchLoss = 2.6297, diffLoss = 12.8304, kgLoss = 0.0795
2025-02-12 23:01:08.880143: Training Step 18/59: batchLoss = 2.8368, diffLoss = 13.8511, kgLoss = 0.0833
2025-02-12 23:01:09.798825: Training Step 19/59: batchLoss = 3.0066, diffLoss = 14.6945, kgLoss = 0.0846
2025-02-12 23:01:10.724265: Training Step 20/59: batchLoss = 2.7256, diffLoss = 13.2947, kgLoss = 0.0833
2025-02-12 23:01:11.650370: Training Step 21/59: batchLoss = 2.8963, diffLoss = 14.1370, kgLoss = 0.0861
2025-02-12 23:01:12.582652: Training Step 22/59: batchLoss = 2.8156, diffLoss = 13.7706, kgLoss = 0.0769
2025-02-12 23:01:13.509483: Training Step 23/59: batchLoss = 3.1050, diffLoss = 15.1680, kgLoss = 0.0892
2025-02-12 23:01:14.441477: Training Step 24/59: batchLoss = 3.1925, diffLoss = 15.5890, kgLoss = 0.0934
2025-02-12 23:01:15.377199: Training Step 25/59: batchLoss = 3.0456, diffLoss = 14.8835, kgLoss = 0.0861
2025-02-12 23:01:16.307349: Training Step 26/59: batchLoss = 3.2482, diffLoss = 15.8649, kgLoss = 0.0940
2025-02-12 23:01:17.235617: Training Step 27/59: batchLoss = 3.2297, diffLoss = 15.7826, kgLoss = 0.0914
2025-02-12 23:01:18.174361: Training Step 28/59: batchLoss = 2.9147, diffLoss = 14.2208, kgLoss = 0.0881
2025-02-12 23:01:19.118222: Training Step 29/59: batchLoss = 3.0016, diffLoss = 14.6664, kgLoss = 0.0854
2025-02-12 23:01:20.064685: Training Step 30/59: batchLoss = 2.8248, diffLoss = 13.7771, kgLoss = 0.0867
2025-02-12 23:01:21.004878: Training Step 31/59: batchLoss = 3.1366, diffLoss = 15.3233, kgLoss = 0.0899
2025-02-12 23:01:21.942390: Training Step 32/59: batchLoss = 2.9842, diffLoss = 14.5764, kgLoss = 0.0861
2025-02-12 23:01:22.883676: Training Step 33/59: batchLoss = 2.8339, diffLoss = 13.8079, kgLoss = 0.0904
2025-02-12 23:01:23.803051: Training Step 34/59: batchLoss = 2.9864, diffLoss = 14.5864, kgLoss = 0.0864
2025-02-12 23:01:24.736893: Training Step 35/59: batchLoss = 2.8469, diffLoss = 13.9427, kgLoss = 0.0729
2025-02-12 23:01:25.666858: Training Step 36/59: batchLoss = 2.8417, diffLoss = 13.8611, kgLoss = 0.0869
2025-02-12 23:01:26.604123: Training Step 37/59: batchLoss = 2.8537, diffLoss = 13.9553, kgLoss = 0.0783
2025-02-12 23:01:27.519925: Training Step 38/59: batchLoss = 2.8721, diffLoss = 14.0282, kgLoss = 0.0830
2025-02-12 23:01:28.460213: Training Step 39/59: batchLoss = 3.0449, diffLoss = 14.8719, kgLoss = 0.0881
2025-02-12 23:01:29.383430: Training Step 40/59: batchLoss = 2.8888, diffLoss = 14.0763, kgLoss = 0.0919
2025-02-12 23:01:30.302739: Training Step 41/59: batchLoss = 3.0440, diffLoss = 14.8884, kgLoss = 0.0828
2025-02-12 23:01:31.225876: Training Step 42/59: batchLoss = 2.7916, diffLoss = 13.6338, kgLoss = 0.0810
2025-02-12 23:01:32.139153: Training Step 43/59: batchLoss = 2.9046, diffLoss = 14.1893, kgLoss = 0.0834
2025-02-12 23:01:33.049668: Training Step 44/59: batchLoss = 3.0474, diffLoss = 14.8795, kgLoss = 0.0894
2025-02-12 23:01:33.959215: Training Step 45/59: batchLoss = 3.1720, diffLoss = 15.4870, kgLoss = 0.0932
2025-02-12 23:01:34.872399: Training Step 46/59: batchLoss = 2.8863, diffLoss = 14.1198, kgLoss = 0.0779
2025-02-12 23:01:35.780882: Training Step 47/59: batchLoss = 3.2009, diffLoss = 15.6542, kgLoss = 0.0876
2025-02-12 23:01:36.696367: Training Step 48/59: batchLoss = 3.0757, diffLoss = 15.0113, kgLoss = 0.0918
2025-02-12 23:01:37.622584: Training Step 49/59: batchLoss = 2.8738, diffLoss = 14.0485, kgLoss = 0.0801
2025-02-12 23:01:38.554470: Training Step 50/59: batchLoss = 3.4749, diffLoss = 16.9902, kgLoss = 0.0961
2025-02-12 23:01:39.481685: Training Step 51/59: batchLoss = 3.3973, diffLoss = 16.6233, kgLoss = 0.0908
2025-02-12 23:01:40.408426: Training Step 52/59: batchLoss = 3.4188, diffLoss = 16.7157, kgLoss = 0.0946
2025-02-12 23:01:41.338551: Training Step 53/59: batchLoss = 2.9915, diffLoss = 14.6557, kgLoss = 0.0755
2025-02-12 23:01:42.265151: Training Step 54/59: batchLoss = 3.0199, diffLoss = 14.7713, kgLoss = 0.0820
2025-02-12 23:01:43.193069: Training Step 55/59: batchLoss = 2.7047, diffLoss = 13.2229, kgLoss = 0.0751
2025-02-12 23:01:44.124918: Training Step 56/59: batchLoss = 2.8476, diffLoss = 13.9189, kgLoss = 0.0798
2025-02-12 23:01:44.969064: Training Step 57/59: batchLoss = 3.3289, diffLoss = 16.2562, kgLoss = 0.0971
2025-02-12 23:01:45.818983: Training Step 58/59: batchLoss = 3.0972, diffLoss = 15.1311, kgLoss = 0.0888
2025-02-12 23:01:45.915982: 
2025-02-12 23:01:45.916506: Epoch 27/1000, Train: epLoss = 0.4373, epDfLoss = 2.1356, epKgLoss = 0.0127  
2025-02-12 23:01:47.384192: Steps 0/47: batch_recall = 36.29, batch_ndcg = 45.39 
2025-02-12 23:01:48.653102: Steps 1/47: batch_recall = 35.03, batch_ndcg = 39.27 
2025-02-12 23:01:49.890561: Steps 2/47: batch_recall = 39.29, batch_ndcg = 44.80 
2025-02-12 23:01:51.134393: Steps 3/47: batch_recall = 42.37, batch_ndcg = 43.58 
2025-02-12 23:01:52.323786: Steps 4/47: batch_recall = 37.92, batch_ndcg = 42.24 
2025-02-12 23:01:53.527064: Steps 5/47: batch_recall = 31.88, batch_ndcg = 37.47 
2025-02-12 23:01:54.714536: Steps 6/47: batch_recall = 37.27, batch_ndcg = 40.13 
2025-02-12 23:01:55.883438: Steps 7/47: batch_recall = 42.20, batch_ndcg = 42.28 
2025-02-12 23:01:57.063385: Steps 8/47: batch_recall = 48.53, batch_ndcg = 49.96 
2025-02-12 23:01:58.212217: Steps 9/47: batch_recall = 45.51, batch_ndcg = 44.85 
2025-02-12 23:01:59.383841: Steps 10/47: batch_recall = 43.54, batch_ndcg = 42.54 
2025-02-12 23:02:00.534338: Steps 11/47: batch_recall = 53.45, batch_ndcg = 50.09 
2025-02-12 23:02:01.691317: Steps 12/47: batch_recall = 49.33, batch_ndcg = 47.55 
2025-02-12 23:02:02.838605: Steps 13/47: batch_recall = 48.67, batch_ndcg = 45.02 
2025-02-12 23:02:03.941225: Steps 14/47: batch_recall = 40.82, batch_ndcg = 39.64 
2025-02-12 23:02:05.043212: Steps 15/47: batch_recall = 54.64, batch_ndcg = 50.67 
2025-02-12 23:02:06.116732: Steps 16/47: batch_recall = 48.27, batch_ndcg = 44.76 
2025-02-12 23:02:07.171000: Steps 17/47: batch_recall = 55.52, batch_ndcg = 48.33 
2025-02-12 23:02:08.252175: Steps 18/47: batch_recall = 50.21, batch_ndcg = 46.59 
2025-02-12 23:02:09.310439: Steps 19/47: batch_recall = 59.71, batch_ndcg = 53.66 
2025-02-12 23:02:10.344937: Steps 20/47: batch_recall = 66.91, batch_ndcg = 59.67 
2025-02-12 23:02:11.397459: Steps 21/47: batch_recall = 63.48, batch_ndcg = 52.97 
2025-02-12 23:02:12.430266: Steps 22/47: batch_recall = 54.52, batch_ndcg = 50.28 
2025-02-12 23:02:13.472598: Steps 23/47: batch_recall = 62.99, batch_ndcg = 53.31 
2025-02-12 23:02:14.526448: Steps 24/47: batch_recall = 63.19, batch_ndcg = 55.22 
2025-02-12 23:02:15.565358: Steps 25/47: batch_recall = 65.08, batch_ndcg = 55.84 
2025-02-12 23:02:16.585176: Steps 26/47: batch_recall = 59.09, batch_ndcg = 52.20 
2025-02-12 23:02:17.624896: Steps 27/47: batch_recall = 59.56, batch_ndcg = 50.82 
2025-02-12 23:02:18.651405: Steps 28/47: batch_recall = 68.89, batch_ndcg = 57.03 
2025-02-12 23:02:19.663889: Steps 29/47: batch_recall = 69.14, batch_ndcg = 56.21 
2025-02-12 23:02:20.676249: Steps 30/47: batch_recall = 73.10, batch_ndcg = 63.24 
2025-02-12 23:02:21.720473: Steps 31/47: batch_recall = 65.58, batch_ndcg = 53.59 
2025-02-12 23:02:22.727730: Steps 32/47: batch_recall = 69.55, batch_ndcg = 62.62 
2025-02-12 23:02:23.738996: Steps 33/47: batch_recall = 77.64, batch_ndcg = 65.21 
2025-02-12 23:02:24.751526: Steps 34/47: batch_recall = 66.75, batch_ndcg = 53.85 
2025-02-12 23:02:25.735497: Steps 35/47: batch_recall = 75.04, batch_ndcg = 61.97 
2025-02-12 23:02:26.709526: Steps 36/47: batch_recall = 78.40, batch_ndcg = 63.10 
2025-02-12 23:02:27.678583: Steps 37/47: batch_recall = 82.06, batch_ndcg = 71.01 
2025-02-12 23:02:28.636349: Steps 38/47: batch_recall = 88.21, batch_ndcg = 71.46 
2025-02-12 23:02:29.591324: Steps 39/47: batch_recall = 87.17, batch_ndcg = 66.64 
2025-02-12 23:02:30.567941: Steps 40/47: batch_recall = 73.34, batch_ndcg = 62.12 
2025-02-12 23:02:31.506634: Steps 41/47: batch_recall = 86.86, batch_ndcg = 69.46 
2025-02-12 23:02:32.465731: Steps 42/47: batch_recall = 82.66, batch_ndcg = 63.83 
2025-02-12 23:02:33.424325: Steps 43/47: batch_recall = 89.89, batch_ndcg = 69.33 
2025-02-12 23:02:34.386149: Steps 44/47: batch_recall = 87.63, batch_ndcg = 68.97 
2025-02-12 23:02:35.339004: Steps 45/47: batch_recall = 93.25, batch_ndcg = 73.40 
2025-02-12 23:02:35.446327: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.54 
2025-02-12 23:02:35.446420: Epoch 27/1000, Test: Recall = 0.1193, NDCG = 0.1054  

2025-02-12 23:02:36.657582: Training Step 0/59: batchLoss = 2.7711, diffLoss = 13.4722, kgLoss = 0.0959
2025-02-12 23:02:37.598478: Training Step 1/59: batchLoss = 2.8635, diffLoss = 13.9645, kgLoss = 0.0882
2025-02-12 23:02:38.522987: Training Step 2/59: batchLoss = 2.8071, diffLoss = 13.7197, kgLoss = 0.0789
2025-02-12 23:02:39.458467: Training Step 3/59: batchLoss = 2.8138, diffLoss = 13.7459, kgLoss = 0.0807
2025-02-12 23:02:40.388388: Training Step 4/59: batchLoss = 2.5646, diffLoss = 12.5324, kgLoss = 0.0726
2025-02-12 23:02:41.312353: Training Step 5/59: batchLoss = 2.6097, diffLoss = 12.7613, kgLoss = 0.0718
2025-02-12 23:02:42.241672: Training Step 6/59: batchLoss = 2.9146, diffLoss = 14.2251, kgLoss = 0.0869
2025-02-12 23:02:43.177852: Training Step 7/59: batchLoss = 2.8360, diffLoss = 13.8096, kgLoss = 0.0926
2025-02-12 23:02:44.099703: Training Step 8/59: batchLoss = 2.9281, diffLoss = 14.3112, kgLoss = 0.0823
2025-02-12 23:02:45.026283: Training Step 9/59: batchLoss = 2.9806, diffLoss = 14.5437, kgLoss = 0.0898
2025-02-12 23:02:45.946374: Training Step 10/59: batchLoss = 2.8325, diffLoss = 13.7943, kgLoss = 0.0920
2025-02-12 23:02:46.856556: Training Step 11/59: batchLoss = 2.6769, diffLoss = 13.0821, kgLoss = 0.0755
2025-02-12 23:02:47.773611: Training Step 12/59: batchLoss = 2.7286, diffLoss = 13.3346, kgLoss = 0.0771
2025-02-12 23:02:48.688885: Training Step 13/59: batchLoss = 2.9345, diffLoss = 14.3184, kgLoss = 0.0886
2025-02-12 23:02:49.611960: Training Step 14/59: batchLoss = 2.9920, diffLoss = 14.6051, kgLoss = 0.0887
2025-02-12 23:02:50.530552: Training Step 15/59: batchLoss = 2.8494, diffLoss = 13.9274, kgLoss = 0.0799
2025-02-12 23:02:51.459492: Training Step 16/59: batchLoss = 3.0284, diffLoss = 14.7848, kgLoss = 0.0892
2025-02-12 23:02:52.372740: Training Step 17/59: batchLoss = 2.9050, diffLoss = 14.1764, kgLoss = 0.0871
2025-02-12 23:02:53.301158: Training Step 18/59: batchLoss = 2.9689, diffLoss = 14.5019, kgLoss = 0.0857
2025-02-12 23:02:54.237313: Training Step 19/59: batchLoss = 2.4256, diffLoss = 11.8596, kgLoss = 0.0671
2025-02-12 23:02:55.166104: Training Step 20/59: batchLoss = 2.9368, diffLoss = 14.3480, kgLoss = 0.0840
2025-02-12 23:02:56.094145: Training Step 21/59: batchLoss = 3.2151, diffLoss = 15.7086, kgLoss = 0.0917
2025-02-12 23:02:57.021525: Training Step 22/59: batchLoss = 2.8891, diffLoss = 14.1199, kgLoss = 0.0814
2025-02-12 23:02:57.952220: Training Step 23/59: batchLoss = 2.8955, diffLoss = 14.1394, kgLoss = 0.0845
2025-02-12 23:02:58.872673: Training Step 24/59: batchLoss = 2.4599, diffLoss = 11.9975, kgLoss = 0.0755
2025-02-12 23:02:59.798095: Training Step 25/59: batchLoss = 3.0282, diffLoss = 14.7833, kgLoss = 0.0894
2025-02-12 23:03:00.725237: Training Step 26/59: batchLoss = 2.8764, diffLoss = 14.0355, kgLoss = 0.0866
2025-02-12 23:03:01.654826: Training Step 27/59: batchLoss = 3.1408, diffLoss = 15.3396, kgLoss = 0.0911
2025-02-12 23:03:02.576630: Training Step 28/59: batchLoss = 3.1272, diffLoss = 15.2494, kgLoss = 0.0966
2025-02-12 23:03:03.496124: Training Step 29/59: batchLoss = 2.8495, diffLoss = 13.9340, kgLoss = 0.0784
2025-02-12 23:03:04.423939: Training Step 30/59: batchLoss = 3.0823, diffLoss = 15.0704, kgLoss = 0.0853
2025-02-12 23:03:05.346891: Training Step 31/59: batchLoss = 2.9279, diffLoss = 14.3018, kgLoss = 0.0844
2025-02-12 23:03:06.277214: Training Step 32/59: batchLoss = 3.0326, diffLoss = 14.8084, kgLoss = 0.0886
2025-02-12 23:03:07.211889: Training Step 33/59: batchLoss = 2.9462, diffLoss = 14.4058, kgLoss = 0.0812
2025-02-12 23:03:08.158338: Training Step 34/59: batchLoss = 2.9177, diffLoss = 14.2625, kgLoss = 0.0815
2025-02-12 23:03:09.100205: Training Step 35/59: batchLoss = 3.2115, diffLoss = 15.6961, kgLoss = 0.0904
2025-02-12 23:03:10.036773: Training Step 36/59: batchLoss = 3.0762, diffLoss = 15.0322, kgLoss = 0.0872
2025-02-12 23:03:10.974596: Training Step 37/59: batchLoss = 3.0655, diffLoss = 14.9547, kgLoss = 0.0932
2025-02-12 23:03:11.912622: Training Step 38/59: batchLoss = 2.9386, diffLoss = 14.3343, kgLoss = 0.0896
2025-02-12 23:03:12.848237: Training Step 39/59: batchLoss = 3.0510, diffLoss = 14.8999, kgLoss = 0.0888
2025-02-12 23:03:13.791583: Training Step 40/59: batchLoss = 3.2899, diffLoss = 16.0658, kgLoss = 0.0959
2025-02-12 23:03:14.724118: Training Step 41/59: batchLoss = 2.8740, diffLoss = 14.0310, kgLoss = 0.0847
2025-02-12 23:03:15.665622: Training Step 42/59: batchLoss = 3.0995, diffLoss = 15.1417, kgLoss = 0.0889
2025-02-12 23:03:16.609692: Training Step 43/59: batchLoss = 2.7040, diffLoss = 13.1645, kgLoss = 0.0889
2025-02-12 23:03:17.542368: Training Step 44/59: batchLoss = 3.1161, diffLoss = 15.2289, kgLoss = 0.0879
2025-02-12 23:03:18.475914: Training Step 45/59: batchLoss = 2.9201, diffLoss = 14.2848, kgLoss = 0.0789
2025-02-12 23:03:19.404622: Training Step 46/59: batchLoss = 3.0217, diffLoss = 14.7779, kgLoss = 0.0827
2025-02-12 23:03:20.332091: Training Step 47/59: batchLoss = 3.4509, diffLoss = 16.8626, kgLoss = 0.0980
2025-02-12 23:03:21.259329: Training Step 48/59: batchLoss = 3.1578, diffLoss = 15.4031, kgLoss = 0.0965
2025-02-12 23:03:22.187438: Training Step 49/59: batchLoss = 3.4043, diffLoss = 16.6226, kgLoss = 0.0998
2025-02-12 23:03:23.117853: Training Step 50/59: batchLoss = 3.0128, diffLoss = 14.7205, kgLoss = 0.0859
2025-02-12 23:03:24.040117: Training Step 51/59: batchLoss = 2.8468, diffLoss = 13.9156, kgLoss = 0.0796
2025-02-12 23:03:24.964203: Training Step 52/59: batchLoss = 3.1300, diffLoss = 15.2897, kgLoss = 0.0900
2025-02-12 23:03:25.915619: Training Step 53/59: batchLoss = 3.2002, diffLoss = 15.6256, kgLoss = 0.0939
2025-02-12 23:03:26.847689: Training Step 54/59: batchLoss = 3.0440, diffLoss = 14.8849, kgLoss = 0.0838
2025-02-12 23:03:27.785898: Training Step 55/59: batchLoss = 2.8584, diffLoss = 13.9600, kgLoss = 0.0831
2025-02-12 23:03:28.720134: Training Step 56/59: batchLoss = 3.0339, diffLoss = 14.8293, kgLoss = 0.0850
2025-02-12 23:03:29.566652: Training Step 57/59: batchLoss = 2.8669, diffLoss = 14.0110, kgLoss = 0.0809
2025-02-12 23:03:30.425336: Training Step 58/59: batchLoss = 3.0823, diffLoss = 15.0587, kgLoss = 0.0882
2025-02-12 23:03:30.522180: 
2025-02-12 23:03:30.522538: Epoch 28/1000, Train: epLoss = 0.4355, epDfLoss = 2.1269, epKgLoss = 0.0127  
2025-02-12 23:03:31.999966: Steps 0/47: batch_recall = 35.93, batch_ndcg = 45.22 
2025-02-12 23:03:33.302201: Steps 1/47: batch_recall = 34.91, batch_ndcg = 39.82 
2025-02-12 23:03:34.562925: Steps 2/47: batch_recall = 40.25, batch_ndcg = 45.52 
2025-02-12 23:03:35.828537: Steps 3/47: batch_recall = 43.64, batch_ndcg = 44.80 
2025-02-12 23:03:37.027871: Steps 4/47: batch_recall = 37.29, batch_ndcg = 42.14 
2025-02-12 23:03:38.240224: Steps 5/47: batch_recall = 31.27, batch_ndcg = 37.82 
2025-02-12 23:03:39.438239: Steps 6/47: batch_recall = 37.87, batch_ndcg = 40.36 
2025-02-12 23:03:40.603532: Steps 7/47: batch_recall = 44.17, batch_ndcg = 43.38 
2025-02-12 23:03:41.779859: Steps 8/47: batch_recall = 47.31, batch_ndcg = 49.49 
2025-02-12 23:03:42.930738: Steps 9/47: batch_recall = 47.22, batch_ndcg = 46.51 
2025-02-12 23:03:44.104663: Steps 10/47: batch_recall = 42.70, batch_ndcg = 42.06 
2025-02-12 23:03:45.259936: Steps 11/47: batch_recall = 51.95, batch_ndcg = 49.42 
2025-02-12 23:03:46.422607: Steps 12/47: batch_recall = 49.15, batch_ndcg = 46.87 
2025-02-12 23:03:47.565306: Steps 13/47: batch_recall = 49.01, batch_ndcg = 44.89 
2025-02-12 23:03:48.668586: Steps 14/47: batch_recall = 39.14, batch_ndcg = 39.00 
2025-02-12 23:03:49.775734: Steps 15/47: batch_recall = 56.16, batch_ndcg = 51.55 
2025-02-12 23:03:50.861821: Steps 16/47: batch_recall = 48.84, batch_ndcg = 44.63 
2025-02-12 23:03:51.925379: Steps 17/47: batch_recall = 56.04, batch_ndcg = 48.23 
2025-02-12 23:03:53.010387: Steps 18/47: batch_recall = 51.10, batch_ndcg = 47.80 
2025-02-12 23:03:54.077573: Steps 19/47: batch_recall = 57.80, batch_ndcg = 53.75 
2025-02-12 23:03:55.114358: Steps 20/47: batch_recall = 64.97, batch_ndcg = 58.99 
2025-02-12 23:03:56.163687: Steps 21/47: batch_recall = 63.50, batch_ndcg = 53.74 
2025-02-12 23:03:57.216233: Steps 22/47: batch_recall = 54.60, batch_ndcg = 50.65 
2025-02-12 23:03:58.257967: Steps 23/47: batch_recall = 64.13, batch_ndcg = 54.54 
2025-02-12 23:03:59.312615: Steps 24/47: batch_recall = 63.85, batch_ndcg = 55.91 
2025-02-12 23:04:00.359647: Steps 25/47: batch_recall = 65.32, batch_ndcg = 56.64 
2025-02-12 23:04:01.383299: Steps 26/47: batch_recall = 59.48, batch_ndcg = 51.46 
2025-02-12 23:04:02.432284: Steps 27/47: batch_recall = 61.73, batch_ndcg = 50.64 
2025-02-12 23:04:03.462397: Steps 28/47: batch_recall = 68.62, batch_ndcg = 57.40 
2025-02-12 23:04:04.481193: Steps 29/47: batch_recall = 68.27, batch_ndcg = 55.86 
2025-02-12 23:04:05.502222: Steps 30/47: batch_recall = 72.07, batch_ndcg = 62.94 
2025-02-12 23:04:06.545674: Steps 31/47: batch_recall = 65.92, batch_ndcg = 53.56 
2025-02-12 23:04:07.574562: Steps 32/47: batch_recall = 69.98, batch_ndcg = 63.15 
2025-02-12 23:04:08.593068: Steps 33/47: batch_recall = 77.93, batch_ndcg = 66.10 
2025-02-12 23:04:09.615640: Steps 34/47: batch_recall = 67.24, batch_ndcg = 53.50 
2025-02-12 23:04:10.595266: Steps 35/47: batch_recall = 75.46, batch_ndcg = 62.56 
2025-02-12 23:04:11.584644: Steps 36/47: batch_recall = 79.22, batch_ndcg = 64.37 
2025-02-12 23:04:12.551438: Steps 37/47: batch_recall = 81.11, batch_ndcg = 70.38 
2025-02-12 23:04:13.529754: Steps 38/47: batch_recall = 89.19, batch_ndcg = 71.63 
2025-02-12 23:04:14.498193: Steps 39/47: batch_recall = 86.16, batch_ndcg = 66.73 
2025-02-12 23:04:15.464156: Steps 40/47: batch_recall = 71.09, batch_ndcg = 60.93 
2025-02-12 23:04:16.420329: Steps 41/47: batch_recall = 86.37, batch_ndcg = 69.47 
2025-02-12 23:04:17.380833: Steps 42/47: batch_recall = 83.05, batch_ndcg = 63.61 
2025-02-12 23:04:18.359462: Steps 43/47: batch_recall = 88.10, batch_ndcg = 68.46 
2025-02-12 23:04:19.342371: Steps 44/47: batch_recall = 87.19, batch_ndcg = 67.75 
2025-02-12 23:04:20.287466: Steps 45/47: batch_recall = 94.44, batch_ndcg = 73.89 
2025-02-12 23:04:20.396422: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.59 
2025-02-12 23:04:20.396547: Epoch 28/1000, Test: Recall = 0.1194, NDCG = 0.1056  

2025-02-12 23:04:21.614680: Training Step 0/59: batchLoss = 2.7164, diffLoss = 13.2205, kgLoss = 0.0904
2025-02-12 23:04:22.551432: Training Step 1/59: batchLoss = 2.5277, diffLoss = 12.3261, kgLoss = 0.0782
2025-02-12 23:04:23.478773: Training Step 2/59: batchLoss = 2.6006, diffLoss = 12.6665, kgLoss = 0.0841
2025-02-12 23:04:24.403735: Training Step 3/59: batchLoss = 2.7368, diffLoss = 13.3377, kgLoss = 0.0866
2025-02-12 23:04:25.330760: Training Step 4/59: batchLoss = 3.0695, diffLoss = 14.9865, kgLoss = 0.0902
2025-02-12 23:04:26.268299: Training Step 5/59: batchLoss = 2.7839, diffLoss = 13.5884, kgLoss = 0.0828
2025-02-12 23:04:27.193468: Training Step 6/59: batchLoss = 3.0645, diffLoss = 14.9644, kgLoss = 0.0895
2025-02-12 23:04:28.127078: Training Step 7/59: batchLoss = 2.8993, diffLoss = 14.1420, kgLoss = 0.0886
2025-02-12 23:04:29.063264: Training Step 8/59: batchLoss = 2.8369, diffLoss = 13.8241, kgLoss = 0.0901
2025-02-12 23:04:29.991614: Training Step 9/59: batchLoss = 2.8531, diffLoss = 13.9474, kgLoss = 0.0796
2025-02-12 23:04:30.928023: Training Step 10/59: batchLoss = 2.8160, diffLoss = 13.7377, kgLoss = 0.0855
2025-02-12 23:04:31.858660: Training Step 11/59: batchLoss = 2.9856, diffLoss = 14.5641, kgLoss = 0.0909
2025-02-12 23:04:32.792351: Training Step 12/59: batchLoss = 3.0241, diffLoss = 14.7758, kgLoss = 0.0861
2025-02-12 23:04:33.715572: Training Step 13/59: batchLoss = 3.2005, diffLoss = 15.6356, kgLoss = 0.0917
2025-02-12 23:04:34.652449: Training Step 14/59: batchLoss = 2.8783, diffLoss = 14.0662, kgLoss = 0.0813
2025-02-12 23:04:35.593492: Training Step 15/59: batchLoss = 3.0310, diffLoss = 14.7937, kgLoss = 0.0904
2025-02-12 23:04:36.529295: Training Step 16/59: batchLoss = 2.8042, diffLoss = 13.6867, kgLoss = 0.0836
2025-02-12 23:04:37.484856: Training Step 17/59: batchLoss = 2.8092, diffLoss = 13.6996, kgLoss = 0.0867
2025-02-12 23:04:38.419499: Training Step 18/59: batchLoss = 3.0573, diffLoss = 14.9218, kgLoss = 0.0911
2025-02-12 23:04:39.357499: Training Step 19/59: batchLoss = 2.9394, diffLoss = 14.3477, kgLoss = 0.0874
2025-02-12 23:04:40.295945: Training Step 20/59: batchLoss = 3.0109, diffLoss = 14.7260, kgLoss = 0.0822
2025-02-12 23:04:41.228801: Training Step 21/59: batchLoss = 2.9441, diffLoss = 14.3899, kgLoss = 0.0827
2025-02-12 23:04:42.170527: Training Step 22/59: batchLoss = 2.6440, diffLoss = 12.8987, kgLoss = 0.0804
2025-02-12 23:04:43.111435: Training Step 23/59: batchLoss = 3.1196, diffLoss = 15.2460, kgLoss = 0.0880
2025-02-12 23:04:44.045248: Training Step 24/59: batchLoss = 3.2345, diffLoss = 15.7957, kgLoss = 0.0942
2025-02-12 23:04:44.982055: Training Step 25/59: batchLoss = 3.2334, diffLoss = 15.7872, kgLoss = 0.0950
2025-02-12 23:04:45.915313: Training Step 26/59: batchLoss = 2.7854, diffLoss = 13.5885, kgLoss = 0.0846
2025-02-12 23:04:46.848754: Training Step 27/59: batchLoss = 2.9759, diffLoss = 14.5407, kgLoss = 0.0847
2025-02-12 23:04:47.771697: Training Step 28/59: batchLoss = 3.4452, diffLoss = 16.8562, kgLoss = 0.0924
2025-02-12 23:04:48.698586: Training Step 29/59: batchLoss = 2.6869, diffLoss = 13.0986, kgLoss = 0.0839
2025-02-12 23:04:49.628516: Training Step 30/59: batchLoss = 2.7231, diffLoss = 13.3009, kgLoss = 0.0786
2025-02-12 23:04:50.556242: Training Step 31/59: batchLoss = 3.0243, diffLoss = 14.7906, kgLoss = 0.0827
2025-02-12 23:04:51.484564: Training Step 32/59: batchLoss = 2.9022, diffLoss = 14.1748, kgLoss = 0.0840
2025-02-12 23:04:52.427475: Training Step 33/59: batchLoss = 3.0865, diffLoss = 15.0814, kgLoss = 0.0878
2025-02-12 23:04:53.379339: Training Step 34/59: batchLoss = 3.0505, diffLoss = 14.8823, kgLoss = 0.0926
2025-02-12 23:04:54.313158: Training Step 35/59: batchLoss = 3.0732, diffLoss = 15.0065, kgLoss = 0.0899
2025-02-12 23:04:55.246977: Training Step 36/59: batchLoss = 2.8149, diffLoss = 13.7495, kgLoss = 0.0813
2025-02-12 23:04:56.184110: Training Step 37/59: batchLoss = 2.8204, diffLoss = 13.7987, kgLoss = 0.0759
2025-02-12 23:04:57.124579: Training Step 38/59: batchLoss = 3.1612, diffLoss = 15.4693, kgLoss = 0.0841
2025-02-12 23:04:58.058739: Training Step 39/59: batchLoss = 3.0279, diffLoss = 14.7996, kgLoss = 0.0849
2025-02-12 23:04:58.997043: Training Step 40/59: batchLoss = 3.1388, diffLoss = 15.3207, kgLoss = 0.0933
2025-02-12 23:04:59.936273: Training Step 41/59: batchLoss = 2.9451, diffLoss = 14.3846, kgLoss = 0.0852
2025-02-12 23:05:00.872193: Training Step 42/59: batchLoss = 2.9673, diffLoss = 14.4960, kgLoss = 0.0851
2025-02-12 23:05:01.808919: Training Step 43/59: batchLoss = 2.7435, diffLoss = 13.3890, kgLoss = 0.0822
2025-02-12 23:05:02.740104: Training Step 44/59: batchLoss = 3.1414, diffLoss = 15.3650, kgLoss = 0.0855
2025-02-12 23:05:03.664737: Training Step 45/59: batchLoss = 3.4007, diffLoss = 16.6066, kgLoss = 0.0992
2025-02-12 23:05:04.588766: Training Step 46/59: batchLoss = 2.8407, diffLoss = 13.9016, kgLoss = 0.0754
2025-02-12 23:05:05.513709: Training Step 47/59: batchLoss = 3.0037, diffLoss = 14.6814, kgLoss = 0.0842
2025-02-12 23:05:06.448395: Training Step 48/59: batchLoss = 3.0920, diffLoss = 15.0818, kgLoss = 0.0946
2025-02-12 23:05:07.377471: Training Step 49/59: batchLoss = 2.8940, diffLoss = 14.1270, kgLoss = 0.0857
2025-02-12 23:05:08.306553: Training Step 50/59: batchLoss = 2.9382, diffLoss = 14.3432, kgLoss = 0.0870
2025-02-12 23:05:09.258539: Training Step 51/59: batchLoss = 2.8519, diffLoss = 13.9221, kgLoss = 0.0843
2025-02-12 23:05:10.189213: Training Step 52/59: batchLoss = 3.0259, diffLoss = 14.7520, kgLoss = 0.0944
2025-02-12 23:05:11.121316: Training Step 53/59: batchLoss = 2.8124, diffLoss = 13.7384, kgLoss = 0.0809
2025-02-12 23:05:12.048699: Training Step 54/59: batchLoss = 2.7953, diffLoss = 13.6567, kgLoss = 0.0800
2025-02-12 23:05:12.987118: Training Step 55/59: batchLoss = 3.0135, diffLoss = 14.7326, kgLoss = 0.0838
2025-02-12 23:05:13.919498: Training Step 56/59: batchLoss = 2.6550, diffLoss = 12.9488, kgLoss = 0.0815
2025-02-12 23:05:14.770286: Training Step 57/59: batchLoss = 2.9294, diffLoss = 14.3100, kgLoss = 0.0842
2025-02-12 23:05:15.628547: Training Step 58/59: batchLoss = 2.7914, diffLoss = 13.6359, kgLoss = 0.0803
2025-02-12 23:05:15.726486: 
2025-02-12 23:05:15.726909: Epoch 29/1000, Train: epLoss = 0.4334, epDfLoss = 2.1165, epKgLoss = 0.0127  
2025-02-12 23:05:17.207401: Steps 0/47: batch_recall = 35.45, batch_ndcg = 44.90 
2025-02-12 23:05:18.513357: Steps 1/47: batch_recall = 35.34, batch_ndcg = 40.48 
2025-02-12 23:05:19.770665: Steps 2/47: batch_recall = 38.78, batch_ndcg = 44.56 
2025-02-12 23:05:21.035062: Steps 3/47: batch_recall = 42.46, batch_ndcg = 44.20 
2025-02-12 23:05:22.233727: Steps 4/47: batch_recall = 37.33, batch_ndcg = 42.86 
2025-02-12 23:05:23.441138: Steps 5/47: batch_recall = 32.04, batch_ndcg = 37.99 
2025-02-12 23:05:24.636371: Steps 6/47: batch_recall = 37.87, batch_ndcg = 40.97 
2025-02-12 23:05:25.814443: Steps 7/47: batch_recall = 43.27, batch_ndcg = 43.78 
2025-02-12 23:05:26.998810: Steps 8/47: batch_recall = 47.77, batch_ndcg = 50.51 
2025-02-12 23:05:28.150016: Steps 9/47: batch_recall = 47.83, batch_ndcg = 46.05 
2025-02-12 23:05:29.329143: Steps 10/47: batch_recall = 42.46, batch_ndcg = 42.61 
2025-02-12 23:05:30.483159: Steps 11/47: batch_recall = 53.93, batch_ndcg = 50.26 
2025-02-12 23:05:31.636224: Steps 12/47: batch_recall = 49.88, batch_ndcg = 46.73 
2025-02-12 23:05:32.782914: Steps 13/47: batch_recall = 48.03, batch_ndcg = 44.57 
2025-02-12 23:05:33.891847: Steps 14/47: batch_recall = 41.03, batch_ndcg = 39.71 
2025-02-12 23:05:34.987999: Steps 15/47: batch_recall = 55.57, batch_ndcg = 51.23 
2025-02-12 23:05:36.071225: Steps 16/47: batch_recall = 50.65, batch_ndcg = 44.90 
2025-02-12 23:05:37.120615: Steps 17/47: batch_recall = 56.04, batch_ndcg = 48.19 
2025-02-12 23:05:38.202100: Steps 18/47: batch_recall = 51.63, batch_ndcg = 47.61 
2025-02-12 23:05:39.269401: Steps 19/47: batch_recall = 59.77, batch_ndcg = 54.89 
2025-02-12 23:05:40.320594: Steps 20/47: batch_recall = 66.53, batch_ndcg = 60.15 
2025-02-12 23:05:41.367238: Steps 21/47: batch_recall = 64.48, batch_ndcg = 53.79 
2025-02-12 23:05:42.422674: Steps 22/47: batch_recall = 54.66, batch_ndcg = 50.64 
2025-02-12 23:05:43.469618: Steps 23/47: batch_recall = 61.19, batch_ndcg = 52.75 
2025-02-12 23:05:44.551542: Steps 24/47: batch_recall = 64.54, batch_ndcg = 55.84 
2025-02-12 23:05:45.612504: Steps 25/47: batch_recall = 64.44, batch_ndcg = 55.94 
2025-02-12 23:05:46.645598: Steps 26/47: batch_recall = 58.99, batch_ndcg = 51.38 
2025-02-12 23:05:47.694172: Steps 27/47: batch_recall = 61.52, batch_ndcg = 51.24 
2025-02-12 23:05:48.729876: Steps 28/47: batch_recall = 69.92, batch_ndcg = 58.28 
2025-02-12 23:05:49.770623: Steps 29/47: batch_recall = 67.17, batch_ndcg = 55.20 
2025-02-12 23:05:50.792827: Steps 30/47: batch_recall = 73.26, batch_ndcg = 63.21 
2025-02-12 23:05:51.826746: Steps 31/47: batch_recall = 66.48, batch_ndcg = 53.66 
2025-02-12 23:05:52.838266: Steps 32/47: batch_recall = 69.51, batch_ndcg = 63.45 
2025-02-12 23:05:53.857356: Steps 33/47: batch_recall = 78.28, batch_ndcg = 66.50 
2025-02-12 23:05:54.870938: Steps 34/47: batch_recall = 66.99, batch_ndcg = 53.78 
2025-02-12 23:05:55.847420: Steps 35/47: batch_recall = 76.40, batch_ndcg = 62.87 
2025-02-12 23:05:56.837366: Steps 36/47: batch_recall = 78.44, batch_ndcg = 63.65 
2025-02-12 23:05:57.802737: Steps 37/47: batch_recall = 83.18, batch_ndcg = 71.17 
2025-02-12 23:05:58.771366: Steps 38/47: batch_recall = 89.38, batch_ndcg = 70.94 
2025-02-12 23:05:59.735395: Steps 39/47: batch_recall = 85.99, batch_ndcg = 66.46 
2025-02-12 23:06:00.707737: Steps 40/47: batch_recall = 72.58, batch_ndcg = 61.77 
2025-02-12 23:06:01.685185: Steps 41/47: batch_recall = 87.75, batch_ndcg = 70.23 
2025-02-12 23:06:02.661950: Steps 42/47: batch_recall = 82.41, batch_ndcg = 63.95 
2025-02-12 23:06:03.637979: Steps 43/47: batch_recall = 89.91, batch_ndcg = 70.10 
2025-02-12 23:06:04.617716: Steps 44/47: batch_recall = 84.27, batch_ndcg = 66.63 
2025-02-12 23:06:05.572638: Steps 45/47: batch_recall = 92.41, batch_ndcg = 73.47 
2025-02-12 23:06:05.678835: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-12 23:06:05.678967: Epoch 29/1000, Test: Recall = 0.1197, NDCG = 0.1059  

2025-02-12 23:06:06.895192: Training Step 0/59: batchLoss = 2.5999, diffLoss = 12.6810, kgLoss = 0.0796
2025-02-12 23:06:07.837065: Training Step 1/59: batchLoss = 2.8292, diffLoss = 13.7748, kgLoss = 0.0928
2025-02-12 23:06:08.780247: Training Step 2/59: batchLoss = 2.6057, diffLoss = 12.7134, kgLoss = 0.0788
2025-02-12 23:06:09.720380: Training Step 3/59: batchLoss = 2.6301, diffLoss = 12.8372, kgLoss = 0.0783
2025-02-12 23:06:10.658054: Training Step 4/59: batchLoss = 2.8682, diffLoss = 14.0058, kgLoss = 0.0838
2025-02-12 23:06:11.588552: Training Step 5/59: batchLoss = 2.8254, diffLoss = 13.7685, kgLoss = 0.0896
2025-02-12 23:06:12.518874: Training Step 6/59: batchLoss = 3.0039, diffLoss = 14.6281, kgLoss = 0.0978
2025-02-12 23:06:13.444539: Training Step 7/59: batchLoss = 3.1639, diffLoss = 15.4256, kgLoss = 0.0985
2025-02-12 23:06:14.370354: Training Step 8/59: batchLoss = 2.9476, diffLoss = 14.3586, kgLoss = 0.0949
2025-02-12 23:06:15.296279: Training Step 9/59: batchLoss = 2.7179, diffLoss = 13.2532, kgLoss = 0.0841
2025-02-12 23:06:16.224154: Training Step 10/59: batchLoss = 2.7022, diffLoss = 13.1719, kgLoss = 0.0848
2025-02-12 23:06:17.153321: Training Step 11/59: batchLoss = 2.7127, diffLoss = 13.2272, kgLoss = 0.0840
2025-02-12 23:06:18.082272: Training Step 12/59: batchLoss = 2.9772, diffLoss = 14.5162, kgLoss = 0.0925
2025-02-12 23:06:19.025642: Training Step 13/59: batchLoss = 2.8479, diffLoss = 13.8708, kgLoss = 0.0921
2025-02-12 23:06:19.963209: Training Step 14/59: batchLoss = 2.8723, diffLoss = 14.0222, kgLoss = 0.0848
2025-02-12 23:06:20.906277: Training Step 15/59: batchLoss = 2.9274, diffLoss = 14.2768, kgLoss = 0.0900
2025-02-12 23:06:21.841993: Training Step 16/59: batchLoss = 2.6525, diffLoss = 12.9466, kgLoss = 0.0790
2025-02-12 23:06:22.776861: Training Step 17/59: batchLoss = 2.7425, diffLoss = 13.4070, kgLoss = 0.0764
2025-02-12 23:06:23.710171: Training Step 18/59: batchLoss = 2.5622, diffLoss = 12.5076, kgLoss = 0.0758
2025-02-12 23:06:24.652631: Training Step 19/59: batchLoss = 2.8429, diffLoss = 13.9110, kgLoss = 0.0759
2025-02-12 23:06:25.597952: Training Step 20/59: batchLoss = 2.6126, diffLoss = 12.7608, kgLoss = 0.0756
2025-02-12 23:06:26.535399: Training Step 21/59: batchLoss = 2.9419, diffLoss = 14.3808, kgLoss = 0.0822
2025-02-12 23:06:27.479154: Training Step 22/59: batchLoss = 2.4265, diffLoss = 11.8514, kgLoss = 0.0703
2025-02-12 23:06:28.412229: Training Step 23/59: batchLoss = 2.6964, diffLoss = 13.1518, kgLoss = 0.0826
2025-02-12 23:06:29.349113: Training Step 24/59: batchLoss = 2.9297, diffLoss = 14.3163, kgLoss = 0.0831
2025-02-12 23:06:30.274876: Training Step 25/59: batchLoss = 2.7605, diffLoss = 13.4778, kgLoss = 0.0812
2025-02-12 23:06:31.208222: Training Step 26/59: batchLoss = 2.5885, diffLoss = 12.6317, kgLoss = 0.0778
2025-02-12 23:06:32.142525: Training Step 27/59: batchLoss = 2.9107, diffLoss = 14.2019, kgLoss = 0.0880
2025-02-12 23:06:33.069174: Training Step 28/59: batchLoss = 3.3313, diffLoss = 16.2884, kgLoss = 0.0921
2025-02-12 23:06:33.999895: Training Step 29/59: batchLoss = 2.9016, diffLoss = 14.1321, kgLoss = 0.0939
2025-02-12 23:06:34.927144: Training Step 30/59: batchLoss = 2.8014, diffLoss = 13.6746, kgLoss = 0.0831
2025-02-12 23:06:35.858883: Training Step 31/59: batchLoss = 3.0055, diffLoss = 14.6752, kgLoss = 0.0880
2025-02-12 23:06:36.794739: Training Step 32/59: batchLoss = 2.9653, diffLoss = 14.4888, kgLoss = 0.0844
2025-02-12 23:06:37.739412: Training Step 33/59: batchLoss = 2.5456, diffLoss = 12.4233, kgLoss = 0.0762
2025-02-12 23:06:38.679183: Training Step 34/59: batchLoss = 2.9681, diffLoss = 14.5219, kgLoss = 0.0796
2025-02-12 23:06:39.612576: Training Step 35/59: batchLoss = 3.3418, diffLoss = 16.3247, kgLoss = 0.0961
2025-02-12 23:06:40.551866: Training Step 36/59: batchLoss = 2.9951, diffLoss = 14.6015, kgLoss = 0.0934
2025-02-12 23:06:41.496443: Training Step 37/59: batchLoss = 3.1206, diffLoss = 15.2497, kgLoss = 0.0883
2025-02-12 23:06:42.431386: Training Step 38/59: batchLoss = 2.9674, diffLoss = 14.5096, kgLoss = 0.0818
2025-02-12 23:06:43.375068: Training Step 39/59: batchLoss = 3.2548, diffLoss = 15.9072, kgLoss = 0.0917
2025-02-12 23:06:44.313386: Training Step 40/59: batchLoss = 2.7751, diffLoss = 13.5648, kgLoss = 0.0777
2025-02-12 23:06:45.249383: Training Step 41/59: batchLoss = 3.2701, diffLoss = 16.0064, kgLoss = 0.0860
2025-02-12 23:06:46.185324: Training Step 42/59: batchLoss = 3.0342, diffLoss = 14.8313, kgLoss = 0.0849
2025-02-12 23:06:47.112865: Training Step 43/59: batchLoss = 2.8963, diffLoss = 14.1506, kgLoss = 0.0827
2025-02-12 23:06:48.036112: Training Step 44/59: batchLoss = 3.4148, diffLoss = 16.6672, kgLoss = 0.1017
2025-02-12 23:06:48.965392: Training Step 45/59: batchLoss = 2.8910, diffLoss = 14.1419, kgLoss = 0.0783
2025-02-12 23:06:49.891223: Training Step 46/59: batchLoss = 3.4103, diffLoss = 16.6621, kgLoss = 0.0973
2025-02-12 23:06:50.819651: Training Step 47/59: batchLoss = 3.0984, diffLoss = 15.1278, kgLoss = 0.0910
2025-02-12 23:06:51.749069: Training Step 48/59: batchLoss = 2.9400, diffLoss = 14.3360, kgLoss = 0.0911
2025-02-12 23:06:52.698672: Training Step 49/59: batchLoss = 2.9890, diffLoss = 14.6020, kgLoss = 0.0857
2025-02-12 23:06:53.631264: Training Step 50/59: batchLoss = 3.0636, diffLoss = 14.9854, kgLoss = 0.0832
2025-02-12 23:06:54.576937: Training Step 51/59: batchLoss = 3.2697, diffLoss = 15.9801, kgLoss = 0.0922
2025-02-12 23:06:55.519166: Training Step 52/59: batchLoss = 3.2897, diffLoss = 16.0803, kgLoss = 0.0920
2025-02-12 23:06:56.464520: Training Step 53/59: batchLoss = 3.0640, diffLoss = 14.9822, kgLoss = 0.0845
2025-02-12 23:06:57.410705: Training Step 54/59: batchLoss = 2.7212, diffLoss = 13.3062, kgLoss = 0.0749
2025-02-12 23:06:58.346117: Training Step 55/59: batchLoss = 2.8741, diffLoss = 14.0026, kgLoss = 0.0920
2025-02-12 23:06:59.282429: Training Step 56/59: batchLoss = 3.2157, diffLoss = 15.7008, kgLoss = 0.0944
2025-02-12 23:07:00.128005: Training Step 57/59: batchLoss = 3.1984, diffLoss = 15.6420, kgLoss = 0.0875
2025-02-12 23:07:00.985302: Training Step 58/59: batchLoss = 3.1124, diffLoss = 15.2027, kgLoss = 0.0898
2025-02-12 23:07:01.085706: 
2025-02-12 23:07:01.086058: Epoch 30/1000, Train: epLoss = 0.4316, epDfLoss = 2.1071, epKgLoss = 0.0127  
2025-02-12 23:07:02.571985: Steps 0/47: batch_recall = 36.07, batch_ndcg = 45.44 
2025-02-12 23:07:03.859640: Steps 1/47: batch_recall = 35.90, batch_ndcg = 41.06 
2025-02-12 23:07:05.101900: Steps 2/47: batch_recall = 39.23, batch_ndcg = 44.76 
2025-02-12 23:07:06.402173: Steps 3/47: batch_recall = 43.27, batch_ndcg = 44.79 
2025-02-12 23:07:07.600931: Steps 4/47: batch_recall = 38.37, batch_ndcg = 43.65 
2025-02-12 23:07:08.822028: Steps 5/47: batch_recall = 31.97, batch_ndcg = 37.86 
2025-02-12 23:07:10.037857: Steps 6/47: batch_recall = 38.47, batch_ndcg = 40.85 
2025-02-12 23:07:11.217252: Steps 7/47: batch_recall = 43.06, batch_ndcg = 43.31 
2025-02-12 23:07:12.409495: Steps 8/47: batch_recall = 47.97, batch_ndcg = 50.36 
2025-02-12 23:07:13.558207: Steps 9/47: batch_recall = 45.92, batch_ndcg = 45.97 
2025-02-12 23:07:14.733466: Steps 10/47: batch_recall = 42.89, batch_ndcg = 42.55 
2025-02-12 23:07:15.889184: Steps 11/47: batch_recall = 52.72, batch_ndcg = 49.81 
2025-02-12 23:07:17.040360: Steps 12/47: batch_recall = 50.21, batch_ndcg = 47.31 
2025-02-12 23:07:18.182425: Steps 13/47: batch_recall = 48.56, batch_ndcg = 44.97 
2025-02-12 23:07:19.280419: Steps 14/47: batch_recall = 39.61, batch_ndcg = 38.96 
2025-02-12 23:07:20.376776: Steps 15/47: batch_recall = 56.17, batch_ndcg = 51.85 
2025-02-12 23:07:21.462291: Steps 16/47: batch_recall = 49.65, batch_ndcg = 45.45 
2025-02-12 23:07:22.510413: Steps 17/47: batch_recall = 55.44, batch_ndcg = 47.85 
2025-02-12 23:07:23.584840: Steps 18/47: batch_recall = 52.11, batch_ndcg = 47.27 
2025-02-12 23:07:24.651399: Steps 19/47: batch_recall = 58.53, batch_ndcg = 54.20 
2025-02-12 23:07:25.697392: Steps 20/47: batch_recall = 65.39, batch_ndcg = 59.76 
2025-02-12 23:07:26.759949: Steps 21/47: batch_recall = 63.11, batch_ndcg = 53.56 
2025-02-12 23:07:27.831440: Steps 22/47: batch_recall = 55.00, batch_ndcg = 51.23 
2025-02-12 23:07:28.895929: Steps 23/47: batch_recall = 62.25, batch_ndcg = 53.50 
2025-02-12 23:07:29.970491: Steps 24/47: batch_recall = 64.26, batch_ndcg = 56.59 
2025-02-12 23:07:31.023651: Steps 25/47: batch_recall = 65.06, batch_ndcg = 56.58 
2025-02-12 23:07:32.052715: Steps 26/47: batch_recall = 59.61, batch_ndcg = 51.44 
2025-02-12 23:07:33.100214: Steps 27/47: batch_recall = 61.38, batch_ndcg = 51.50 
2025-02-12 23:07:34.128030: Steps 28/47: batch_recall = 69.94, batch_ndcg = 58.24 
2025-02-12 23:07:35.160527: Steps 29/47: batch_recall = 68.58, batch_ndcg = 56.23 
2025-02-12 23:07:36.171694: Steps 30/47: batch_recall = 72.50, batch_ndcg = 63.03 
2025-02-12 23:07:37.203835: Steps 31/47: batch_recall = 65.46, batch_ndcg = 53.29 
2025-02-12 23:07:38.201998: Steps 32/47: batch_recall = 69.08, batch_ndcg = 63.38 
2025-02-12 23:07:39.201714: Steps 33/47: batch_recall = 78.19, batch_ndcg = 66.11 
2025-02-12 23:07:40.206773: Steps 34/47: batch_recall = 66.33, batch_ndcg = 53.47 
2025-02-12 23:07:41.185170: Steps 35/47: batch_recall = 75.21, batch_ndcg = 62.40 
2025-02-12 23:07:42.163482: Steps 36/47: batch_recall = 79.09, batch_ndcg = 63.82 
2025-02-12 23:07:43.129985: Steps 37/47: batch_recall = 81.01, batch_ndcg = 70.43 
2025-02-12 23:07:44.098668: Steps 38/47: batch_recall = 88.49, batch_ndcg = 71.06 
2025-02-12 23:07:45.071958: Steps 39/47: batch_recall = 86.95, batch_ndcg = 66.85 
2025-02-12 23:07:46.040734: Steps 40/47: batch_recall = 71.46, batch_ndcg = 61.84 
2025-02-12 23:07:47.012425: Steps 41/47: batch_recall = 88.54, batch_ndcg = 71.29 
2025-02-12 23:07:47.975303: Steps 42/47: batch_recall = 82.41, batch_ndcg = 64.03 
2025-02-12 23:07:48.940846: Steps 43/47: batch_recall = 89.45, batch_ndcg = 69.73 
2025-02-12 23:07:49.917274: Steps 44/47: batch_recall = 87.18, batch_ndcg = 68.84 
2025-02-12 23:07:50.882581: Steps 45/47: batch_recall = 93.06, batch_ndcg = 74.05 
2025-02-12 23:07:50.987801: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 23:07:50.987933: Epoch 30/1000, Test: Recall = 0.1195, NDCG = 0.1062  

2025-02-12 23:07:52.200646: Training Step 0/59: batchLoss = 2.9298, diffLoss = 14.3063, kgLoss = 0.0857
2025-02-12 23:07:53.135439: Training Step 1/59: batchLoss = 2.8472, diffLoss = 13.8966, kgLoss = 0.0849
2025-02-12 23:07:54.066981: Training Step 2/59: batchLoss = 3.0130, diffLoss = 14.6961, kgLoss = 0.0922
2025-02-12 23:07:55.000309: Training Step 3/59: batchLoss = 3.0475, diffLoss = 14.8886, kgLoss = 0.0872
2025-02-12 23:07:55.924670: Training Step 4/59: batchLoss = 2.6645, diffLoss = 13.0186, kgLoss = 0.0760
2025-02-12 23:07:56.854890: Training Step 5/59: batchLoss = 2.6773, diffLoss = 13.0513, kgLoss = 0.0838
2025-02-12 23:07:57.788505: Training Step 6/59: batchLoss = 2.7831, diffLoss = 13.5707, kgLoss = 0.0861
2025-02-12 23:07:58.724180: Training Step 7/59: batchLoss = 2.8813, diffLoss = 14.0698, kgLoss = 0.0842
2025-02-12 23:07:59.657139: Training Step 8/59: batchLoss = 2.8181, diffLoss = 13.7495, kgLoss = 0.0852
2025-02-12 23:08:00.588392: Training Step 9/59: batchLoss = 2.8932, diffLoss = 14.1118, kgLoss = 0.0885
2025-02-12 23:08:01.528162: Training Step 10/59: batchLoss = 3.0272, diffLoss = 14.7939, kgLoss = 0.0856
2025-02-12 23:08:02.477689: Training Step 11/59: batchLoss = 2.9483, diffLoss = 14.4124, kgLoss = 0.0823
2025-02-12 23:08:03.412392: Training Step 12/59: batchLoss = 2.8811, diffLoss = 14.0362, kgLoss = 0.0924
2025-02-12 23:08:04.354011: Training Step 13/59: batchLoss = 2.6338, diffLoss = 12.8545, kgLoss = 0.0787
2025-02-12 23:08:05.295150: Training Step 14/59: batchLoss = 2.6559, diffLoss = 12.9686, kgLoss = 0.0778
2025-02-12 23:08:06.239283: Training Step 15/59: batchLoss = 3.0706, diffLoss = 14.9910, kgLoss = 0.0905
2025-02-12 23:08:07.176674: Training Step 16/59: batchLoss = 3.3918, diffLoss = 16.5574, kgLoss = 0.1004
2025-02-12 23:08:08.115958: Training Step 17/59: batchLoss = 2.8087, diffLoss = 13.6954, kgLoss = 0.0871
2025-02-12 23:08:09.060439: Training Step 18/59: batchLoss = 2.7807, diffLoss = 13.5761, kgLoss = 0.0818
2025-02-12 23:08:10.010062: Training Step 19/59: batchLoss = 2.9663, diffLoss = 14.4854, kgLoss = 0.0866
2025-02-12 23:08:10.939235: Training Step 20/59: batchLoss = 3.0612, diffLoss = 14.9444, kgLoss = 0.0904
2025-02-12 23:08:11.862446: Training Step 21/59: batchLoss = 2.9466, diffLoss = 14.3874, kgLoss = 0.0864
2025-02-12 23:08:12.802076: Training Step 22/59: batchLoss = 2.7642, diffLoss = 13.4962, kgLoss = 0.0812
2025-02-12 23:08:13.729143: Training Step 23/59: batchLoss = 2.9841, diffLoss = 14.5970, kgLoss = 0.0808
2025-02-12 23:08:14.663667: Training Step 24/59: batchLoss = 2.9742, diffLoss = 14.5198, kgLoss = 0.0878
2025-02-12 23:08:15.590275: Training Step 25/59: batchLoss = 3.4719, diffLoss = 16.9734, kgLoss = 0.0965
2025-02-12 23:08:16.520469: Training Step 26/59: batchLoss = 3.0362, diffLoss = 14.8225, kgLoss = 0.0896
2025-02-12 23:08:17.450648: Training Step 27/59: batchLoss = 3.0848, diffLoss = 15.0624, kgLoss = 0.0905
2025-02-12 23:08:18.380495: Training Step 28/59: batchLoss = 2.8691, diffLoss = 14.0082, kgLoss = 0.0843
2025-02-12 23:08:19.320485: Training Step 29/59: batchLoss = 2.6810, diffLoss = 13.0932, kgLoss = 0.0779
2025-02-12 23:08:20.255205: Training Step 30/59: batchLoss = 3.0252, diffLoss = 14.7941, kgLoss = 0.0830
2025-02-12 23:08:21.194068: Training Step 31/59: batchLoss = 3.0754, diffLoss = 15.0408, kgLoss = 0.0841
2025-02-12 23:08:22.128631: Training Step 32/59: batchLoss = 2.6467, diffLoss = 12.9387, kgLoss = 0.0737
2025-02-12 23:08:23.061514: Training Step 33/59: batchLoss = 2.9931, diffLoss = 14.6272, kgLoss = 0.0846
2025-02-12 23:08:24.000605: Training Step 34/59: batchLoss = 2.9742, diffLoss = 14.5377, kgLoss = 0.0833
2025-02-12 23:08:24.941680: Training Step 35/59: batchLoss = 3.1694, diffLoss = 15.4614, kgLoss = 0.0964
2025-02-12 23:08:25.874833: Training Step 36/59: batchLoss = 3.3650, diffLoss = 16.4377, kgLoss = 0.0968
2025-02-12 23:08:26.809543: Training Step 37/59: batchLoss = 3.0300, diffLoss = 14.7955, kgLoss = 0.0887
2025-02-12 23:08:27.743434: Training Step 38/59: batchLoss = 3.0001, diffLoss = 14.6535, kgLoss = 0.0867
2025-02-12 23:08:28.668278: Training Step 39/59: batchLoss = 2.7716, diffLoss = 13.5604, kgLoss = 0.0744
2025-02-12 23:08:29.596736: Training Step 40/59: batchLoss = 3.1836, diffLoss = 15.5528, kgLoss = 0.0914
2025-02-12 23:08:30.526117: Training Step 41/59: batchLoss = 2.9455, diffLoss = 14.4102, kgLoss = 0.0793
2025-02-12 23:08:31.455895: Training Step 42/59: batchLoss = 3.2875, diffLoss = 16.0601, kgLoss = 0.0943
2025-02-12 23:08:32.392274: Training Step 43/59: batchLoss = 2.8026, diffLoss = 13.6931, kgLoss = 0.0800
2025-02-12 23:08:33.321545: Training Step 44/59: batchLoss = 2.8732, diffLoss = 14.0494, kgLoss = 0.0792
2025-02-12 23:08:34.253807: Training Step 45/59: batchLoss = 2.9588, diffLoss = 14.4452, kgLoss = 0.0871
2025-02-12 23:08:35.179467: Training Step 46/59: batchLoss = 2.9678, diffLoss = 14.5119, kgLoss = 0.0818
2025-02-12 23:08:36.108460: Training Step 47/59: batchLoss = 2.9354, diffLoss = 14.3432, kgLoss = 0.0834
2025-02-12 23:08:37.041441: Training Step 48/59: batchLoss = 3.0322, diffLoss = 14.8138, kgLoss = 0.0868
2025-02-12 23:08:37.973650: Training Step 49/59: batchLoss = 3.0731, diffLoss = 15.0193, kgLoss = 0.0866
2025-02-12 23:08:38.909399: Training Step 50/59: batchLoss = 2.9898, diffLoss = 14.6246, kgLoss = 0.0811
2025-02-12 23:08:39.846671: Training Step 51/59: batchLoss = 3.1760, diffLoss = 15.5223, kgLoss = 0.0894
2025-02-12 23:08:40.786344: Training Step 52/59: batchLoss = 2.8526, diffLoss = 13.9301, kgLoss = 0.0832
2025-02-12 23:08:41.725125: Training Step 53/59: batchLoss = 3.3067, diffLoss = 16.1652, kgLoss = 0.0920
2025-02-12 23:08:42.664417: Training Step 54/59: batchLoss = 3.2000, diffLoss = 15.6487, kgLoss = 0.0878
2025-02-12 23:08:43.601661: Training Step 55/59: batchLoss = 3.2319, diffLoss = 15.7965, kgLoss = 0.0907
2025-02-12 23:08:44.527698: Training Step 56/59: batchLoss = 3.0590, diffLoss = 14.9439, kgLoss = 0.0878
2025-02-12 23:08:45.372669: Training Step 57/59: batchLoss = 2.8106, diffLoss = 13.7178, kgLoss = 0.0838
2025-02-12 23:08:46.224937: Training Step 58/59: batchLoss = 3.1842, diffLoss = 15.5630, kgLoss = 0.0895
2025-02-12 23:08:46.323431: 
2025-02-12 23:08:46.323791: Epoch 31/1000, Train: epLoss = 0.4388, epDfLoss = 2.1432, epKgLoss = 0.0127  
2025-02-12 23:08:47.798796: Steps 0/47: batch_recall = 35.77, batch_ndcg = 45.07 
2025-02-12 23:08:49.098954: Steps 1/47: batch_recall = 35.84, batch_ndcg = 40.53 
2025-02-12 23:08:50.352094: Steps 2/47: batch_recall = 40.07, batch_ndcg = 45.65 
2025-02-12 23:08:51.618206: Steps 3/47: batch_recall = 43.35, batch_ndcg = 45.45 
2025-02-12 23:08:52.816651: Steps 4/47: batch_recall = 38.40, batch_ndcg = 43.73 
2025-02-12 23:08:54.038443: Steps 5/47: batch_recall = 32.66, batch_ndcg = 38.48 
2025-02-12 23:08:55.249551: Steps 6/47: batch_recall = 37.74, batch_ndcg = 41.44 
2025-02-12 23:08:56.430585: Steps 7/47: batch_recall = 43.51, batch_ndcg = 44.29 
2025-02-12 23:08:57.618112: Steps 8/47: batch_recall = 46.69, batch_ndcg = 49.51 
2025-02-12 23:08:58.769001: Steps 9/47: batch_recall = 46.85, batch_ndcg = 46.18 
2025-02-12 23:08:59.944516: Steps 10/47: batch_recall = 42.95, batch_ndcg = 43.01 
2025-02-12 23:09:01.103050: Steps 11/47: batch_recall = 53.29, batch_ndcg = 50.35 
2025-02-12 23:09:02.260530: Steps 12/47: batch_recall = 49.28, batch_ndcg = 47.01 
2025-02-12 23:09:03.401211: Steps 13/47: batch_recall = 48.96, batch_ndcg = 45.28 
2025-02-12 23:09:04.489461: Steps 14/47: batch_recall = 39.66, batch_ndcg = 39.35 
2025-02-12 23:09:05.593708: Steps 15/47: batch_recall = 56.34, batch_ndcg = 52.29 
2025-02-12 23:09:06.675888: Steps 16/47: batch_recall = 50.65, batch_ndcg = 45.92 
2025-02-12 23:09:07.724741: Steps 17/47: batch_recall = 55.85, batch_ndcg = 48.10 
2025-02-12 23:09:08.811488: Steps 18/47: batch_recall = 51.33, batch_ndcg = 47.81 
2025-02-12 23:09:09.885926: Steps 19/47: batch_recall = 57.99, batch_ndcg = 53.60 
2025-02-12 23:09:10.946646: Steps 20/47: batch_recall = 68.34, batch_ndcg = 61.06 
2025-02-12 23:09:12.003103: Steps 21/47: batch_recall = 63.82, batch_ndcg = 53.86 
2025-02-12 23:09:13.070998: Steps 22/47: batch_recall = 54.15, batch_ndcg = 51.05 
2025-02-12 23:09:14.126968: Steps 23/47: batch_recall = 63.55, batch_ndcg = 54.05 
2025-02-12 23:09:15.193379: Steps 24/47: batch_recall = 65.76, batch_ndcg = 57.34 
2025-02-12 23:09:16.246078: Steps 25/47: batch_recall = 65.82, batch_ndcg = 56.89 
2025-02-12 23:09:17.275919: Steps 26/47: batch_recall = 61.00, batch_ndcg = 52.03 
2025-02-12 23:09:18.320665: Steps 27/47: batch_recall = 61.94, batch_ndcg = 51.50 
2025-02-12 23:09:19.352512: Steps 28/47: batch_recall = 69.17, batch_ndcg = 57.65 
2025-02-12 23:09:20.367177: Steps 29/47: batch_recall = 68.41, batch_ndcg = 56.32 
2025-02-12 23:09:21.371518: Steps 30/47: batch_recall = 72.51, batch_ndcg = 62.99 
2025-02-12 23:09:22.404278: Steps 31/47: batch_recall = 65.92, batch_ndcg = 54.16 
2025-02-12 23:09:23.417744: Steps 32/47: batch_recall = 68.90, batch_ndcg = 63.74 
2025-02-12 23:09:24.427369: Steps 33/47: batch_recall = 79.20, batch_ndcg = 66.92 
2025-02-12 23:09:25.445705: Steps 34/47: batch_recall = 68.41, batch_ndcg = 54.45 
2025-02-12 23:09:26.432228: Steps 35/47: batch_recall = 77.50, batch_ndcg = 63.15 
2025-02-12 23:09:27.417868: Steps 36/47: batch_recall = 78.39, batch_ndcg = 63.96 
2025-02-12 23:09:28.399496: Steps 37/47: batch_recall = 82.21, batch_ndcg = 70.74 
2025-02-12 23:09:29.383638: Steps 38/47: batch_recall = 88.60, batch_ndcg = 70.86 
2025-02-12 23:09:30.369479: Steps 39/47: batch_recall = 87.72, batch_ndcg = 67.22 
2025-02-12 23:09:31.344597: Steps 40/47: batch_recall = 72.32, batch_ndcg = 61.76 
2025-02-12 23:09:32.349457: Steps 41/47: batch_recall = 86.73, batch_ndcg = 70.01 
2025-02-12 23:09:33.318576: Steps 42/47: batch_recall = 82.64, batch_ndcg = 64.39 
2025-02-12 23:09:34.290521: Steps 43/47: batch_recall = 87.29, batch_ndcg = 68.59 
2025-02-12 23:09:35.266010: Steps 44/47: batch_recall = 86.77, batch_ndcg = 69.13 
2025-02-12 23:09:36.215413: Steps 45/47: batch_recall = 93.36, batch_ndcg = 73.91 
2025-02-12 23:09:36.320925: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.61 
2025-02-12 23:09:36.321061: Epoch 31/1000, Test: Recall = 0.1201, NDCG = 0.1066  

2025-02-12 23:09:37.527018: Training Step 0/59: batchLoss = 2.8529, diffLoss = 13.9137, kgLoss = 0.0877
2025-02-12 23:09:38.460815: Training Step 1/59: batchLoss = 2.8965, diffLoss = 14.1536, kgLoss = 0.0823
2025-02-12 23:09:39.394215: Training Step 2/59: batchLoss = 2.7702, diffLoss = 13.5005, kgLoss = 0.0877
2025-02-12 23:09:40.323314: Training Step 3/59: batchLoss = 2.8110, diffLoss = 13.7153, kgLoss = 0.0849
2025-02-12 23:09:41.250764: Training Step 4/59: batchLoss = 2.8288, diffLoss = 13.7898, kgLoss = 0.0886
2025-02-12 23:09:42.180826: Training Step 5/59: batchLoss = 2.7937, diffLoss = 13.6398, kgLoss = 0.0821
2025-02-12 23:09:43.106743: Training Step 6/59: batchLoss = 2.7828, diffLoss = 13.5821, kgLoss = 0.0830
2025-02-12 23:09:44.041492: Training Step 7/59: batchLoss = 2.7201, diffLoss = 13.2679, kgLoss = 0.0832
2025-02-12 23:09:44.987166: Training Step 8/59: batchLoss = 2.7683, diffLoss = 13.5104, kgLoss = 0.0828
2025-02-12 23:09:45.929572: Training Step 9/59: batchLoss = 2.6913, diffLoss = 13.1039, kgLoss = 0.0881
2025-02-12 23:09:46.879025: Training Step 10/59: batchLoss = 3.0107, diffLoss = 14.6664, kgLoss = 0.0967
2025-02-12 23:09:47.827423: Training Step 11/59: batchLoss = 3.1888, diffLoss = 15.5566, kgLoss = 0.0969
2025-02-12 23:09:48.769185: Training Step 12/59: batchLoss = 3.0996, diffLoss = 15.1239, kgLoss = 0.0936
2025-02-12 23:09:49.708130: Training Step 13/59: batchLoss = 3.0371, diffLoss = 14.8244, kgLoss = 0.0902
2025-02-12 23:09:50.649209: Training Step 14/59: batchLoss = 2.5654, diffLoss = 12.5092, kgLoss = 0.0794
2025-02-12 23:09:51.592976: Training Step 15/59: batchLoss = 2.9863, diffLoss = 14.5889, kgLoss = 0.0856
2025-02-12 23:09:52.536949: Training Step 16/59: batchLoss = 3.0343, diffLoss = 14.8030, kgLoss = 0.0921
2025-02-12 23:09:53.474921: Training Step 17/59: batchLoss = 3.0079, diffLoss = 14.6876, kgLoss = 0.0880
2025-02-12 23:09:54.409917: Training Step 18/59: batchLoss = 2.7231, diffLoss = 13.3083, kgLoss = 0.0768
2025-02-12 23:09:55.355212: Training Step 19/59: batchLoss = 3.0086, diffLoss = 14.6901, kgLoss = 0.0882
2025-02-12 23:09:56.281687: Training Step 20/59: batchLoss = 2.8447, diffLoss = 13.8944, kgLoss = 0.0823
2025-02-12 23:09:57.209316: Training Step 21/59: batchLoss = 2.9924, diffLoss = 14.5859, kgLoss = 0.0940
2025-02-12 23:09:58.138944: Training Step 22/59: batchLoss = 3.2478, diffLoss = 15.8646, kgLoss = 0.0936
2025-02-12 23:09:59.063588: Training Step 23/59: batchLoss = 2.9895, diffLoss = 14.6127, kgLoss = 0.0837
2025-02-12 23:09:59.987232: Training Step 24/59: batchLoss = 2.8711, diffLoss = 14.0347, kgLoss = 0.0803
2025-02-12 23:10:00.918958: Training Step 25/59: batchLoss = 3.0674, diffLoss = 14.9642, kgLoss = 0.0932
2025-02-12 23:10:01.856732: Training Step 26/59: batchLoss = 2.7811, diffLoss = 13.5899, kgLoss = 0.0789
2025-02-12 23:10:02.787361: Training Step 27/59: batchLoss = 2.9448, diffLoss = 14.3748, kgLoss = 0.0873
2025-02-12 23:10:03.719716: Training Step 28/59: batchLoss = 2.6270, diffLoss = 12.8282, kgLoss = 0.0767
2025-02-12 23:10:04.665919: Training Step 29/59: batchLoss = 3.0088, diffLoss = 14.7069, kgLoss = 0.0842
2025-02-12 23:10:05.607537: Training Step 30/59: batchLoss = 2.9874, diffLoss = 14.5901, kgLoss = 0.0867
2025-02-12 23:10:06.542851: Training Step 31/59: batchLoss = 2.7606, diffLoss = 13.5013, kgLoss = 0.0755
2025-02-12 23:10:07.490610: Training Step 32/59: batchLoss = 2.9006, diffLoss = 14.1375, kgLoss = 0.0913
2025-02-12 23:10:08.422253: Training Step 33/59: batchLoss = 2.8872, diffLoss = 14.1176, kgLoss = 0.0796
2025-02-12 23:10:09.354415: Training Step 34/59: batchLoss = 2.8499, diffLoss = 13.9182, kgLoss = 0.0828
2025-02-12 23:10:10.292347: Training Step 35/59: batchLoss = 3.1516, diffLoss = 15.3969, kgLoss = 0.0903
2025-02-12 23:10:11.233647: Training Step 36/59: batchLoss = 2.8852, diffLoss = 14.0874, kgLoss = 0.0846
2025-02-12 23:10:12.171916: Training Step 37/59: batchLoss = 3.0493, diffLoss = 14.9076, kgLoss = 0.0847
2025-02-12 23:10:13.112592: Training Step 38/59: batchLoss = 3.0136, diffLoss = 14.7351, kgLoss = 0.0832
2025-02-12 23:10:14.036469: Training Step 39/59: batchLoss = 2.9705, diffLoss = 14.5177, kgLoss = 0.0838
2025-02-12 23:10:14.967867: Training Step 40/59: batchLoss = 3.2416, diffLoss = 15.8275, kgLoss = 0.0951
2025-02-12 23:10:15.899022: Training Step 41/59: batchLoss = 2.7851, diffLoss = 13.6096, kgLoss = 0.0789
2025-02-12 23:10:16.831443: Training Step 42/59: batchLoss = 2.9893, diffLoss = 14.6286, kgLoss = 0.0795
2025-02-12 23:10:17.767644: Training Step 43/59: batchLoss = 3.0251, diffLoss = 14.7658, kgLoss = 0.0899
2025-02-12 23:10:18.701283: Training Step 44/59: batchLoss = 3.0318, diffLoss = 14.8085, kgLoss = 0.0876
2025-02-12 23:10:19.646601: Training Step 45/59: batchLoss = 2.8630, diffLoss = 13.9829, kgLoss = 0.0830
2025-02-12 23:10:20.586517: Training Step 46/59: batchLoss = 3.0503, diffLoss = 14.9069, kgLoss = 0.0861
2025-02-12 23:10:21.527967: Training Step 47/59: batchLoss = 3.3609, diffLoss = 16.4287, kgLoss = 0.0939
2025-02-12 23:10:22.467699: Training Step 48/59: batchLoss = 2.8070, diffLoss = 13.7098, kgLoss = 0.0813
2025-02-12 23:10:23.409121: Training Step 49/59: batchLoss = 2.9428, diffLoss = 14.3797, kgLoss = 0.0836
2025-02-12 23:10:24.347612: Training Step 50/59: batchLoss = 3.2618, diffLoss = 15.9475, kgLoss = 0.0904
2025-02-12 23:10:25.278285: Training Step 51/59: batchLoss = 3.1681, diffLoss = 15.4818, kgLoss = 0.0897
2025-02-12 23:10:26.218625: Training Step 52/59: batchLoss = 2.8394, diffLoss = 13.8733, kgLoss = 0.0809
2025-02-12 23:10:27.157711: Training Step 53/59: batchLoss = 3.0283, diffLoss = 14.7884, kgLoss = 0.0882
2025-02-12 23:10:28.102776: Training Step 54/59: batchLoss = 2.9482, diffLoss = 14.4221, kgLoss = 0.0797
2025-02-12 23:10:29.036003: Training Step 55/59: batchLoss = 3.1702, diffLoss = 15.5138, kgLoss = 0.0843
2025-02-12 23:10:29.967283: Training Step 56/59: batchLoss = 2.7700, diffLoss = 13.5022, kgLoss = 0.0870
2025-02-12 23:10:30.811574: Training Step 57/59: batchLoss = 3.0709, diffLoss = 15.0102, kgLoss = 0.0860
2025-02-12 23:10:31.663025: Training Step 58/59: batchLoss = 3.0387, diffLoss = 14.8523, kgLoss = 0.0853
2025-02-12 23:10:31.760542: 
2025-02-12 23:10:31.761070: Epoch 32/1000, Train: epLoss = 0.4345, epDfLoss = 2.1219, epKgLoss = 0.0127  
2025-02-12 23:10:33.230928: Steps 0/47: batch_recall = 36.14, batch_ndcg = 45.27 
2025-02-12 23:10:34.524466: Steps 1/47: batch_recall = 36.06, batch_ndcg = 41.30 
2025-02-12 23:10:35.807931: Steps 2/47: batch_recall = 39.95, batch_ndcg = 45.22 
2025-02-12 23:10:37.069832: Steps 3/47: batch_recall = 43.41, batch_ndcg = 45.41 
2025-02-12 23:10:38.265168: Steps 4/47: batch_recall = 39.26, batch_ndcg = 43.99 
2025-02-12 23:10:39.489581: Steps 5/47: batch_recall = 32.36, batch_ndcg = 38.20 
2025-02-12 23:10:40.696787: Steps 6/47: batch_recall = 38.65, batch_ndcg = 41.37 
2025-02-12 23:10:41.868014: Steps 7/47: batch_recall = 43.84, batch_ndcg = 44.51 
2025-02-12 23:10:43.046028: Steps 8/47: batch_recall = 46.97, batch_ndcg = 50.62 
2025-02-12 23:10:44.185648: Steps 9/47: batch_recall = 45.68, batch_ndcg = 45.69 
2025-02-12 23:10:45.349152: Steps 10/47: batch_recall = 43.68, batch_ndcg = 43.59 
2025-02-12 23:10:46.499179: Steps 11/47: batch_recall = 53.81, batch_ndcg = 50.82 
2025-02-12 23:10:47.643712: Steps 12/47: batch_recall = 47.85, batch_ndcg = 46.84 
2025-02-12 23:10:48.768465: Steps 13/47: batch_recall = 48.76, batch_ndcg = 45.33 
2025-02-12 23:10:49.864034: Steps 14/47: batch_recall = 40.33, batch_ndcg = 39.95 
2025-02-12 23:10:50.958949: Steps 15/47: batch_recall = 56.10, batch_ndcg = 52.60 
2025-02-12 23:10:52.035743: Steps 16/47: batch_recall = 49.50, batch_ndcg = 44.92 
2025-02-12 23:10:53.080945: Steps 17/47: batch_recall = 56.09, batch_ndcg = 49.03 
2025-02-12 23:10:54.165135: Steps 18/47: batch_recall = 52.15, batch_ndcg = 47.92 
2025-02-12 23:10:55.246716: Steps 19/47: batch_recall = 59.87, batch_ndcg = 54.89 
2025-02-12 23:10:56.314346: Steps 20/47: batch_recall = 67.07, batch_ndcg = 60.48 
2025-02-12 23:10:57.378032: Steps 21/47: batch_recall = 64.00, batch_ndcg = 53.90 
2025-02-12 23:10:58.436686: Steps 22/47: batch_recall = 55.70, batch_ndcg = 51.22 
2025-02-12 23:10:59.503013: Steps 23/47: batch_recall = 60.86, batch_ndcg = 53.16 
2025-02-12 23:11:00.572748: Steps 24/47: batch_recall = 65.50, batch_ndcg = 56.37 
2025-02-12 23:11:01.624629: Steps 25/47: batch_recall = 66.46, batch_ndcg = 57.60 
2025-02-12 23:11:02.653148: Steps 26/47: batch_recall = 60.03, batch_ndcg = 51.71 
2025-02-12 23:11:03.684990: Steps 27/47: batch_recall = 62.77, batch_ndcg = 52.23 
2025-02-12 23:11:04.702653: Steps 28/47: batch_recall = 69.09, batch_ndcg = 57.29 
2025-02-12 23:11:05.709667: Steps 29/47: batch_recall = 69.75, batch_ndcg = 56.90 
2025-02-12 23:11:06.724363: Steps 30/47: batch_recall = 72.39, batch_ndcg = 62.60 
2025-02-12 23:11:07.760739: Steps 31/47: batch_recall = 66.35, batch_ndcg = 54.56 
2025-02-12 23:11:08.762004: Steps 32/47: batch_recall = 70.08, batch_ndcg = 63.64 
2025-02-12 23:11:09.767545: Steps 33/47: batch_recall = 78.59, batch_ndcg = 66.42 
2025-02-12 23:11:10.777462: Steps 34/47: batch_recall = 67.89, batch_ndcg = 54.89 
2025-02-12 23:11:11.760870: Steps 35/47: batch_recall = 76.33, batch_ndcg = 63.62 
2025-02-12 23:11:12.752615: Steps 36/47: batch_recall = 77.49, batch_ndcg = 63.06 
2025-02-12 23:11:13.729848: Steps 37/47: batch_recall = 82.92, batch_ndcg = 71.29 
2025-02-12 23:11:14.705573: Steps 38/47: batch_recall = 88.94, batch_ndcg = 71.60 
2025-02-12 23:11:15.685021: Steps 39/47: batch_recall = 87.94, batch_ndcg = 66.84 
2025-02-12 23:11:16.662929: Steps 40/47: batch_recall = 71.09, batch_ndcg = 61.22 
2025-02-12 23:11:17.632323: Steps 41/47: batch_recall = 85.57, batch_ndcg = 70.32 
2025-02-12 23:11:18.602964: Steps 42/47: batch_recall = 83.55, batch_ndcg = 65.46 
2025-02-12 23:11:19.580293: Steps 43/47: batch_recall = 87.63, batch_ndcg = 69.97 
2025-02-12 23:11:20.561984: Steps 44/47: batch_recall = 86.75, batch_ndcg = 68.95 
2025-02-12 23:11:21.500581: Steps 45/47: batch_recall = 92.21, batch_ndcg = 72.98 
2025-02-12 23:11:21.606140: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.63 
2025-02-12 23:11:21.606277: Epoch 32/1000, Test: Recall = 0.1201, NDCG = 0.1068  

2025-02-12 23:11:22.807160: Training Step 0/59: batchLoss = 2.9912, diffLoss = 14.6078, kgLoss = 0.0870
2025-02-12 23:11:23.740441: Training Step 1/59: batchLoss = 2.8242, diffLoss = 13.7376, kgLoss = 0.0959
2025-02-12 23:11:24.673111: Training Step 2/59: batchLoss = 2.8177, diffLoss = 13.7709, kgLoss = 0.0794
2025-02-12 23:11:25.596639: Training Step 3/59: batchLoss = 2.8876, diffLoss = 14.0616, kgLoss = 0.0941
2025-02-12 23:11:26.527290: Training Step 4/59: batchLoss = 2.7503, diffLoss = 13.3832, kgLoss = 0.0921
2025-02-12 23:11:27.455980: Training Step 5/59: batchLoss = 2.8700, diffLoss = 14.0200, kgLoss = 0.0825
2025-02-12 23:11:28.386450: Training Step 6/59: batchLoss = 2.9583, diffLoss = 14.4410, kgLoss = 0.0877
2025-02-12 23:11:29.325343: Training Step 7/59: batchLoss = 3.0893, diffLoss = 15.0848, kgLoss = 0.0905
2025-02-12 23:11:30.264144: Training Step 8/59: batchLoss = 2.8546, diffLoss = 13.9224, kgLoss = 0.0876
2025-02-12 23:11:31.200466: Training Step 9/59: batchLoss = 3.1555, diffLoss = 15.4013, kgLoss = 0.0941
2025-02-12 23:11:32.134343: Training Step 10/59: batchLoss = 2.8599, diffLoss = 13.9803, kgLoss = 0.0797
2025-02-12 23:11:33.066077: Training Step 11/59: batchLoss = 2.7216, diffLoss = 13.2505, kgLoss = 0.0894
2025-02-12 23:11:34.000674: Training Step 12/59: batchLoss = 2.9565, diffLoss = 14.4285, kgLoss = 0.0885
2025-02-12 23:11:34.935125: Training Step 13/59: batchLoss = 2.9131, diffLoss = 14.2058, kgLoss = 0.0899
2025-02-12 23:11:35.873937: Training Step 14/59: batchLoss = 3.0238, diffLoss = 14.7814, kgLoss = 0.0844
2025-02-12 23:11:36.806661: Training Step 15/59: batchLoss = 2.9200, diffLoss = 14.2376, kgLoss = 0.0906
2025-02-12 23:11:37.753659: Training Step 16/59: batchLoss = 2.6448, diffLoss = 12.9231, kgLoss = 0.0752
2025-02-12 23:11:38.682380: Training Step 17/59: batchLoss = 3.0853, diffLoss = 15.0907, kgLoss = 0.0839
2025-02-12 23:11:39.617105: Training Step 18/59: batchLoss = 2.7229, diffLoss = 13.2868, kgLoss = 0.0819
2025-02-12 23:11:40.549033: Training Step 19/59: batchLoss = 2.6043, diffLoss = 12.7293, kgLoss = 0.0731
2025-02-12 23:11:41.482780: Training Step 20/59: batchLoss = 3.0501, diffLoss = 14.9246, kgLoss = 0.0815
2025-02-12 23:11:42.418651: Training Step 21/59: batchLoss = 3.0013, diffLoss = 14.6624, kgLoss = 0.0860
2025-02-12 23:11:43.348703: Training Step 22/59: batchLoss = 2.8823, diffLoss = 14.0512, kgLoss = 0.0901
2025-02-12 23:11:44.272376: Training Step 23/59: batchLoss = 2.4969, diffLoss = 12.1652, kgLoss = 0.0798
2025-02-12 23:11:45.198508: Training Step 24/59: batchLoss = 2.7593, diffLoss = 13.4924, kgLoss = 0.0760
2025-02-12 23:11:46.134118: Training Step 25/59: batchLoss = 3.3394, diffLoss = 16.3192, kgLoss = 0.0945
2025-02-12 23:11:47.071293: Training Step 26/59: batchLoss = 3.0611, diffLoss = 14.9202, kgLoss = 0.0963
2025-02-12 23:11:48.013695: Training Step 27/59: batchLoss = 2.6983, diffLoss = 13.1827, kgLoss = 0.0772
2025-02-12 23:11:48.952752: Training Step 28/59: batchLoss = 2.9485, diffLoss = 14.3806, kgLoss = 0.0905
2025-02-12 23:11:49.901539: Training Step 29/59: batchLoss = 2.6345, diffLoss = 12.8522, kgLoss = 0.0801
2025-02-12 23:11:50.840713: Training Step 30/59: batchLoss = 2.8874, diffLoss = 14.1008, kgLoss = 0.0841
2025-02-12 23:11:51.776928: Training Step 31/59: batchLoss = 3.3989, diffLoss = 16.5925, kgLoss = 0.1005
2025-02-12 23:11:52.711538: Training Step 32/59: batchLoss = 2.9282, diffLoss = 14.3121, kgLoss = 0.0822
2025-02-12 23:11:53.650593: Training Step 33/59: batchLoss = 3.0740, diffLoss = 15.0132, kgLoss = 0.0892
2025-02-12 23:11:54.578606: Training Step 34/59: batchLoss = 3.0072, diffLoss = 14.6833, kgLoss = 0.0882
2025-02-12 23:11:55.518056: Training Step 35/59: batchLoss = 2.7896, diffLoss = 13.6385, kgLoss = 0.0773
2025-02-12 23:11:56.452713: Training Step 36/59: batchLoss = 2.8734, diffLoss = 14.0417, kgLoss = 0.0814
2025-02-12 23:11:57.385572: Training Step 37/59: batchLoss = 2.8778, diffLoss = 14.0559, kgLoss = 0.0832
2025-02-12 23:11:58.325252: Training Step 38/59: batchLoss = 2.9216, diffLoss = 14.2726, kgLoss = 0.0838
2025-02-12 23:11:59.251172: Training Step 39/59: batchLoss = 3.1875, diffLoss = 15.5764, kgLoss = 0.0903
2025-02-12 23:12:00.182846: Training Step 40/59: batchLoss = 3.5602, diffLoss = 17.3835, kgLoss = 0.1044
2025-02-12 23:12:01.118029: Training Step 41/59: batchLoss = 2.9316, diffLoss = 14.3417, kgLoss = 0.0791
2025-02-12 23:12:02.062258: Training Step 42/59: batchLoss = 2.7131, diffLoss = 13.2535, kgLoss = 0.0780
2025-02-12 23:12:02.995889: Training Step 43/59: batchLoss = 3.2150, diffLoss = 15.7083, kgLoss = 0.0916
2025-02-12 23:12:03.932459: Training Step 44/59: batchLoss = 2.9691, diffLoss = 14.4989, kgLoss = 0.0866
2025-02-12 23:12:04.871214: Training Step 45/59: batchLoss = 2.8910, diffLoss = 14.1186, kgLoss = 0.0840
2025-02-12 23:12:05.817903: Training Step 46/59: batchLoss = 3.2246, diffLoss = 15.7540, kgLoss = 0.0923
2025-02-12 23:12:06.749787: Training Step 47/59: batchLoss = 2.8295, diffLoss = 13.8249, kgLoss = 0.0807
2025-02-12 23:12:07.692330: Training Step 48/59: batchLoss = 2.8237, diffLoss = 13.7610, kgLoss = 0.0894
2025-02-12 23:12:08.632566: Training Step 49/59: batchLoss = 3.1137, diffLoss = 15.2086, kgLoss = 0.0900
2025-02-12 23:12:09.582498: Training Step 50/59: batchLoss = 3.0820, diffLoss = 15.0614, kgLoss = 0.0871
2025-02-12 23:12:10.515360: Training Step 51/59: batchLoss = 2.8530, diffLoss = 13.9388, kgLoss = 0.0815
2025-02-12 23:12:11.453527: Training Step 52/59: batchLoss = 2.7637, diffLoss = 13.4994, kgLoss = 0.0798
2025-02-12 23:12:12.390356: Training Step 53/59: batchLoss = 2.9772, diffLoss = 14.5658, kgLoss = 0.0800
2025-02-12 23:12:13.311135: Training Step 54/59: batchLoss = 3.0703, diffLoss = 15.0164, kgLoss = 0.0838
2025-02-12 23:12:14.232118: Training Step 55/59: batchLoss = 3.0957, diffLoss = 15.1356, kgLoss = 0.0858
2025-02-12 23:12:15.149455: Training Step 56/59: batchLoss = 2.9741, diffLoss = 14.5453, kgLoss = 0.0812
2025-02-12 23:12:15.994984: Training Step 57/59: batchLoss = 2.6228, diffLoss = 12.8052, kgLoss = 0.0772
2025-02-12 23:12:16.843406: Training Step 58/59: batchLoss = 3.5152, diffLoss = 17.2022, kgLoss = 0.0935
2025-02-12 23:12:16.943744: 
2025-02-12 23:12:16.944534: Epoch 33/1000, Train: epLoss = 0.4342, epDfLoss = 2.1205, epKgLoss = 0.0127  
2025-02-12 23:12:18.431106: Steps 0/47: batch_recall = 35.94, batch_ndcg = 45.85 
2025-02-12 23:12:19.738930: Steps 1/47: batch_recall = 36.11, batch_ndcg = 40.95 
2025-02-12 23:12:21.011454: Steps 2/47: batch_recall = 39.99, batch_ndcg = 46.11 
2025-02-12 23:12:22.309791: Steps 3/47: batch_recall = 43.08, batch_ndcg = 45.48 
2025-02-12 23:12:23.522974: Steps 4/47: batch_recall = 38.66, batch_ndcg = 44.05 
2025-02-12 23:12:24.769969: Steps 5/47: batch_recall = 33.22, batch_ndcg = 38.83 
2025-02-12 23:12:25.996611: Steps 6/47: batch_recall = 38.69, batch_ndcg = 41.93 
2025-02-12 23:12:27.229905: Steps 7/47: batch_recall = 44.15, batch_ndcg = 44.89 
2025-02-12 23:12:28.420508: Steps 8/47: batch_recall = 46.72, batch_ndcg = 49.62 
2025-02-12 23:12:29.578994: Steps 9/47: batch_recall = 45.32, batch_ndcg = 45.39 
2025-02-12 23:12:30.766349: Steps 10/47: batch_recall = 42.57, batch_ndcg = 42.22 
2025-02-12 23:12:31.916335: Steps 11/47: batch_recall = 53.00, batch_ndcg = 50.63 
2025-02-12 23:12:33.070649: Steps 12/47: batch_recall = 48.49, batch_ndcg = 46.23 
2025-02-12 23:12:34.220351: Steps 13/47: batch_recall = 48.36, batch_ndcg = 45.20 
2025-02-12 23:12:35.324884: Steps 14/47: batch_recall = 40.90, batch_ndcg = 40.91 
2025-02-12 23:12:36.429791: Steps 15/47: batch_recall = 56.20, batch_ndcg = 52.88 
2025-02-12 23:12:37.521777: Steps 16/47: batch_recall = 50.14, batch_ndcg = 45.70 
2025-02-12 23:12:38.585781: Steps 17/47: batch_recall = 56.24, batch_ndcg = 48.89 
2025-02-12 23:12:39.686186: Steps 18/47: batch_recall = 50.80, batch_ndcg = 47.14 
2025-02-12 23:12:40.771604: Steps 19/47: batch_recall = 58.97, batch_ndcg = 54.42 
2025-02-12 23:12:41.840410: Steps 20/47: batch_recall = 64.83, batch_ndcg = 59.00 
2025-02-12 23:12:42.900153: Steps 21/47: batch_recall = 64.12, batch_ndcg = 54.80 
2025-02-12 23:12:43.966823: Steps 22/47: batch_recall = 55.44, batch_ndcg = 51.48 
2025-02-12 23:12:45.023453: Steps 23/47: batch_recall = 62.53, batch_ndcg = 53.90 
2025-02-12 23:12:46.104787: Steps 24/47: batch_recall = 66.57, batch_ndcg = 57.74 
2025-02-12 23:12:47.161907: Steps 25/47: batch_recall = 65.93, batch_ndcg = 56.63 
2025-02-12 23:12:48.185895: Steps 26/47: batch_recall = 61.47, batch_ndcg = 52.15 
2025-02-12 23:12:49.213985: Steps 27/47: batch_recall = 62.54, batch_ndcg = 52.24 
2025-02-12 23:12:50.233504: Steps 28/47: batch_recall = 69.33, batch_ndcg = 57.90 
2025-02-12 23:12:51.246777: Steps 29/47: batch_recall = 68.15, batch_ndcg = 56.09 
2025-02-12 23:12:52.253001: Steps 30/47: batch_recall = 72.51, batch_ndcg = 63.44 
2025-02-12 23:12:53.271344: Steps 31/47: batch_recall = 65.36, batch_ndcg = 54.19 
2025-02-12 23:12:54.288147: Steps 32/47: batch_recall = 71.07, batch_ndcg = 64.07 
2025-02-12 23:12:55.303569: Steps 33/47: batch_recall = 78.28, batch_ndcg = 66.92 
2025-02-12 23:12:56.324184: Steps 34/47: batch_recall = 66.31, batch_ndcg = 53.48 
2025-02-12 23:12:57.316748: Steps 35/47: batch_recall = 76.24, batch_ndcg = 63.69 
2025-02-12 23:12:58.314424: Steps 36/47: batch_recall = 77.98, batch_ndcg = 63.42 
2025-02-12 23:12:59.302390: Steps 37/47: batch_recall = 81.62, batch_ndcg = 70.56 
2025-02-12 23:13:00.296774: Steps 38/47: batch_recall = 88.67, batch_ndcg = 71.16 
2025-02-12 23:13:01.276769: Steps 39/47: batch_recall = 87.89, batch_ndcg = 67.57 
2025-02-12 23:13:02.257305: Steps 40/47: batch_recall = 71.40, batch_ndcg = 62.08 
2025-02-12 23:13:03.224241: Steps 41/47: batch_recall = 86.73, batch_ndcg = 70.89 
2025-02-12 23:13:04.190598: Steps 42/47: batch_recall = 82.73, batch_ndcg = 64.01 
2025-02-12 23:13:05.151359: Steps 43/47: batch_recall = 87.77, batch_ndcg = 69.94 
2025-02-12 23:13:06.113225: Steps 44/47: batch_recall = 87.53, batch_ndcg = 69.99 
2025-02-12 23:13:07.053684: Steps 45/47: batch_recall = 94.51, batch_ndcg = 74.08 
2025-02-12 23:13:07.159506: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.59 
2025-02-12 23:13:07.159644: Epoch 33/1000, Test: Recall = 0.1200, NDCG = 0.1069  

2025-02-12 23:13:08.357833: Training Step 0/59: batchLoss = 2.8806, diffLoss = 14.0705, kgLoss = 0.0831
2025-02-12 23:13:09.286472: Training Step 1/59: batchLoss = 2.9322, diffLoss = 14.3219, kgLoss = 0.0847
2025-02-12 23:13:10.212007: Training Step 2/59: batchLoss = 2.8846, diffLoss = 14.0649, kgLoss = 0.0895
2025-02-12 23:13:11.137275: Training Step 3/59: batchLoss = 3.1977, diffLoss = 15.6026, kgLoss = 0.0965
2025-02-12 23:13:12.076375: Training Step 4/59: batchLoss = 2.9232, diffLoss = 14.2313, kgLoss = 0.0962
2025-02-12 23:13:13.009761: Training Step 5/59: batchLoss = 2.7157, diffLoss = 13.2693, kgLoss = 0.0773
2025-02-12 23:13:13.949031: Training Step 6/59: batchLoss = 2.5880, diffLoss = 12.6113, kgLoss = 0.0822
2025-02-12 23:13:14.887046: Training Step 7/59: batchLoss = 3.1875, diffLoss = 15.5432, kgLoss = 0.0986
2025-02-12 23:13:15.820626: Training Step 8/59: batchLoss = 2.5016, diffLoss = 12.2135, kgLoss = 0.0736
2025-02-12 23:13:16.765358: Training Step 9/59: batchLoss = 2.7280, diffLoss = 13.3161, kgLoss = 0.0810
2025-02-12 23:13:17.702338: Training Step 10/59: batchLoss = 2.9552, diffLoss = 14.4326, kgLoss = 0.0859
2025-02-12 23:13:18.638047: Training Step 11/59: batchLoss = 2.7818, diffLoss = 13.5786, kgLoss = 0.0826
2025-02-12 23:13:19.576902: Training Step 12/59: batchLoss = 3.0243, diffLoss = 14.7574, kgLoss = 0.0910
2025-02-12 23:13:20.520298: Training Step 13/59: batchLoss = 2.7050, diffLoss = 13.1678, kgLoss = 0.0893
2025-02-12 23:13:21.450823: Training Step 14/59: batchLoss = 2.8221, diffLoss = 13.7492, kgLoss = 0.0904
2025-02-12 23:13:22.373547: Training Step 15/59: batchLoss = 2.9043, diffLoss = 14.1564, kgLoss = 0.0913
2025-02-12 23:13:23.297286: Training Step 16/59: batchLoss = 3.0512, diffLoss = 14.9037, kgLoss = 0.0881
2025-02-12 23:13:24.220775: Training Step 17/59: batchLoss = 2.9035, diffLoss = 14.1894, kgLoss = 0.0820
2025-02-12 23:13:25.143309: Training Step 18/59: batchLoss = 2.9470, diffLoss = 14.3524, kgLoss = 0.0956
2025-02-12 23:13:26.062444: Training Step 19/59: batchLoss = 2.9224, diffLoss = 14.2522, kgLoss = 0.0900
2025-02-12 23:13:26.993360: Training Step 20/59: batchLoss = 2.8276, diffLoss = 13.8158, kgLoss = 0.0806
2025-02-12 23:13:27.916705: Training Step 21/59: batchLoss = 3.0480, diffLoss = 14.8969, kgLoss = 0.0858
2025-02-12 23:13:28.841972: Training Step 22/59: batchLoss = 2.9678, diffLoss = 14.5025, kgLoss = 0.0842
2025-02-12 23:13:29.773138: Training Step 23/59: batchLoss = 3.3745, diffLoss = 16.4879, kgLoss = 0.0961
2025-02-12 23:13:30.717749: Training Step 24/59: batchLoss = 3.0804, diffLoss = 15.0644, kgLoss = 0.0844
2025-02-12 23:13:31.673316: Training Step 25/59: batchLoss = 3.1695, diffLoss = 15.4712, kgLoss = 0.0940
2025-02-12 23:13:32.610993: Training Step 26/59: batchLoss = 2.8666, diffLoss = 14.0253, kgLoss = 0.0770
2025-02-12 23:13:33.542290: Training Step 27/59: batchLoss = 3.0663, diffLoss = 14.9606, kgLoss = 0.0927
2025-02-12 23:13:34.487636: Training Step 28/59: batchLoss = 3.0550, diffLoss = 14.9373, kgLoss = 0.0845
2025-02-12 23:13:35.428024: Training Step 29/59: batchLoss = 2.6953, diffLoss = 13.1467, kgLoss = 0.0825
2025-02-12 23:13:36.367710: Training Step 30/59: batchLoss = 3.0056, diffLoss = 14.6853, kgLoss = 0.0856
2025-02-12 23:13:37.306856: Training Step 31/59: batchLoss = 3.0201, diffLoss = 14.7449, kgLoss = 0.0889
2025-02-12 23:13:38.246514: Training Step 32/59: batchLoss = 3.1013, diffLoss = 15.1719, kgLoss = 0.0837
2025-02-12 23:13:39.191422: Training Step 33/59: batchLoss = 2.9909, diffLoss = 14.6414, kgLoss = 0.0783
2025-02-12 23:13:40.117662: Training Step 34/59: batchLoss = 2.7404, diffLoss = 13.3780, kgLoss = 0.0810
2025-02-12 23:13:41.043962: Training Step 35/59: batchLoss = 2.8986, diffLoss = 14.1410, kgLoss = 0.0880
2025-02-12 23:13:41.964566: Training Step 36/59: batchLoss = 2.9776, diffLoss = 14.5670, kgLoss = 0.0802
2025-02-12 23:13:42.886190: Training Step 37/59: batchLoss = 2.8020, diffLoss = 13.6767, kgLoss = 0.0833
2025-02-12 23:13:43.811166: Training Step 38/59: batchLoss = 3.1610, diffLoss = 15.4674, kgLoss = 0.0844
2025-02-12 23:13:44.733337: Training Step 39/59: batchLoss = 3.2667, diffLoss = 15.9358, kgLoss = 0.0994
2025-02-12 23:13:45.666039: Training Step 40/59: batchLoss = 3.0735, diffLoss = 15.0177, kgLoss = 0.0874
2025-02-12 23:13:46.603826: Training Step 41/59: batchLoss = 2.9045, diffLoss = 14.1968, kgLoss = 0.0814
2025-02-12 23:13:47.535883: Training Step 42/59: batchLoss = 3.1480, diffLoss = 15.3864, kgLoss = 0.0884
2025-02-12 23:13:48.477678: Training Step 43/59: batchLoss = 3.2345, diffLoss = 15.8064, kgLoss = 0.0915
2025-02-12 23:13:49.417407: Training Step 44/59: batchLoss = 2.9660, diffLoss = 14.4845, kgLoss = 0.0864
2025-02-12 23:13:50.361916: Training Step 45/59: batchLoss = 2.6402, diffLoss = 12.9080, kgLoss = 0.0732
2025-02-12 23:13:51.296168: Training Step 46/59: batchLoss = 2.9305, diffLoss = 14.3240, kgLoss = 0.0821
2025-02-12 23:13:52.234822: Training Step 47/59: batchLoss = 2.8565, diffLoss = 13.9280, kgLoss = 0.0886
2025-02-12 23:13:53.174158: Training Step 48/59: batchLoss = 2.9903, diffLoss = 14.6250, kgLoss = 0.0817
2025-02-12 23:13:54.118555: Training Step 49/59: batchLoss = 3.0092, diffLoss = 14.7180, kgLoss = 0.0819
2025-02-12 23:13:55.048459: Training Step 50/59: batchLoss = 2.8426, diffLoss = 13.8977, kgLoss = 0.0788
2025-02-12 23:13:55.977468: Training Step 51/59: batchLoss = 3.2883, diffLoss = 16.0641, kgLoss = 0.0943
2025-02-12 23:13:56.897961: Training Step 52/59: batchLoss = 3.1364, diffLoss = 15.3375, kgLoss = 0.0861
2025-02-12 23:13:57.824830: Training Step 53/59: batchLoss = 2.9082, diffLoss = 14.2059, kgLoss = 0.0838
2025-02-12 23:13:58.743610: Training Step 54/59: batchLoss = 2.7541, diffLoss = 13.4679, kgLoss = 0.0756
2025-02-12 23:13:59.672869: Training Step 55/59: batchLoss = 3.5088, diffLoss = 17.1595, kgLoss = 0.0961
2025-02-12 23:14:00.588368: Training Step 56/59: batchLoss = 3.0219, diffLoss = 14.7854, kgLoss = 0.0810
2025-02-12 23:14:01.424022: Training Step 57/59: batchLoss = 2.7721, diffLoss = 13.5539, kgLoss = 0.0766
2025-02-12 23:14:02.270297: Training Step 58/59: batchLoss = 2.8443, diffLoss = 13.8819, kgLoss = 0.0849
2025-02-12 23:14:02.363196: 
2025-02-12 23:14:02.363522: Epoch 34/1000, Train: epLoss = 0.4363, epDfLoss = 2.1306, epKgLoss = 0.0127  
2025-02-12 23:14:03.831420: Steps 0/47: batch_recall = 35.75, batch_ndcg = 46.10 
2025-02-12 23:14:05.133124: Steps 1/47: batch_recall = 35.98, batch_ndcg = 41.11 
2025-02-12 23:14:06.377750: Steps 2/47: batch_recall = 39.67, batch_ndcg = 45.89 
2025-02-12 23:14:07.642116: Steps 3/47: batch_recall = 43.42, batch_ndcg = 45.32 
2025-02-12 23:14:08.841125: Steps 4/47: batch_recall = 38.75, batch_ndcg = 44.37 
2025-02-12 23:14:10.069990: Steps 5/47: batch_recall = 32.78, batch_ndcg = 38.88 
2025-02-12 23:14:11.280999: Steps 6/47: batch_recall = 38.95, batch_ndcg = 42.01 
2025-02-12 23:14:12.453108: Steps 7/47: batch_recall = 44.04, batch_ndcg = 44.64 
2025-02-12 23:14:13.631054: Steps 8/47: batch_recall = 47.47, batch_ndcg = 49.92 
2025-02-12 23:14:14.768241: Steps 9/47: batch_recall = 46.14, batch_ndcg = 45.52 
2025-02-12 23:14:15.934710: Steps 10/47: batch_recall = 43.41, batch_ndcg = 42.74 
2025-02-12 23:14:17.070856: Steps 11/47: batch_recall = 53.05, batch_ndcg = 50.59 
2025-02-12 23:14:18.247779: Steps 12/47: batch_recall = 48.70, batch_ndcg = 46.73 
2025-02-12 23:14:19.373023: Steps 13/47: batch_recall = 48.24, batch_ndcg = 44.74 
2025-02-12 23:14:20.468598: Steps 14/47: batch_recall = 40.94, batch_ndcg = 39.98 
2025-02-12 23:14:21.577252: Steps 15/47: batch_recall = 56.63, batch_ndcg = 52.87 
2025-02-12 23:14:22.661401: Steps 16/47: batch_recall = 49.69, batch_ndcg = 45.98 
2025-02-12 23:14:23.725731: Steps 17/47: batch_recall = 56.88, batch_ndcg = 48.92 
2025-02-12 23:14:24.812997: Steps 18/47: batch_recall = 51.61, batch_ndcg = 48.70 
2025-02-12 23:14:25.893657: Steps 19/47: batch_recall = 61.53, batch_ndcg = 55.58 
2025-02-12 23:14:26.959446: Steps 20/47: batch_recall = 67.56, batch_ndcg = 60.84 
2025-02-12 23:14:28.022294: Steps 21/47: batch_recall = 62.81, batch_ndcg = 54.00 
2025-02-12 23:14:29.089051: Steps 22/47: batch_recall = 54.19, batch_ndcg = 50.33 
2025-02-12 23:14:30.149030: Steps 23/47: batch_recall = 63.42, batch_ndcg = 54.75 
2025-02-12 23:14:31.214474: Steps 24/47: batch_recall = 65.37, batch_ndcg = 57.32 
2025-02-12 23:14:32.255278: Steps 25/47: batch_recall = 65.17, batch_ndcg = 56.44 
2025-02-12 23:14:33.271573: Steps 26/47: batch_recall = 59.81, batch_ndcg = 52.23 
2025-02-12 23:14:34.302066: Steps 27/47: batch_recall = 62.13, batch_ndcg = 52.35 
2025-02-12 23:14:35.328264: Steps 28/47: batch_recall = 70.55, batch_ndcg = 58.35 
2025-02-12 23:14:36.341592: Steps 29/47: batch_recall = 68.98, batch_ndcg = 57.00 
2025-02-12 23:14:37.355769: Steps 30/47: batch_recall = 73.01, batch_ndcg = 63.54 
2025-02-12 23:14:38.393177: Steps 31/47: batch_recall = 65.77, batch_ndcg = 53.95 
2025-02-12 23:14:39.409654: Steps 32/47: batch_recall = 70.06, batch_ndcg = 63.43 
2025-02-12 23:14:40.431395: Steps 33/47: batch_recall = 79.28, batch_ndcg = 67.28 
2025-02-12 23:14:41.453719: Steps 34/47: batch_recall = 66.80, batch_ndcg = 55.35 
2025-02-12 23:14:42.446252: Steps 35/47: batch_recall = 75.03, batch_ndcg = 63.57 
2025-02-12 23:14:43.449120: Steps 36/47: batch_recall = 76.71, batch_ndcg = 63.10 
2025-02-12 23:14:44.424624: Steps 37/47: batch_recall = 84.36, batch_ndcg = 72.10 
2025-02-12 23:14:45.405886: Steps 38/47: batch_recall = 91.12, batch_ndcg = 72.11 
2025-02-12 23:14:46.382518: Steps 39/47: batch_recall = 87.13, batch_ndcg = 67.68 
2025-02-12 23:14:47.347341: Steps 40/47: batch_recall = 70.64, batch_ndcg = 61.24 
2025-02-12 23:14:48.315593: Steps 41/47: batch_recall = 86.99, batch_ndcg = 70.59 
2025-02-12 23:14:49.275876: Steps 42/47: batch_recall = 80.62, batch_ndcg = 63.23 
2025-02-12 23:14:50.234922: Steps 43/47: batch_recall = 88.97, batch_ndcg = 70.13 
2025-02-12 23:14:51.196578: Steps 44/47: batch_recall = 87.68, batch_ndcg = 68.94 
2025-02-12 23:14:52.127433: Steps 45/47: batch_recall = 92.40, batch_ndcg = 73.91 
2025-02-12 23:14:52.231793: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.59 
2025-02-12 23:14:52.231925: Epoch 34/1000, Test: Recall = 0.1202, NDCG = 0.1072  

2025-02-12 23:14:53.418705: Training Step 0/59: batchLoss = 2.8487, diffLoss = 13.8876, kgLoss = 0.0890
2025-02-12 23:14:54.354276: Training Step 1/59: batchLoss = 2.8258, diffLoss = 13.7541, kgLoss = 0.0938
2025-02-12 23:14:55.295695: Training Step 2/59: batchLoss = 2.6387, diffLoss = 12.8901, kgLoss = 0.0759
2025-02-12 23:14:56.235955: Training Step 3/59: batchLoss = 2.6710, diffLoss = 13.0154, kgLoss = 0.0848
2025-02-12 23:14:57.174334: Training Step 4/59: batchLoss = 3.1392, diffLoss = 15.3195, kgLoss = 0.0942
2025-02-12 23:14:58.116566: Training Step 5/59: batchLoss = 2.7011, diffLoss = 13.1588, kgLoss = 0.0867
2025-02-12 23:14:59.053959: Training Step 6/59: batchLoss = 2.8171, diffLoss = 13.7601, kgLoss = 0.0814
2025-02-12 23:14:59.999535: Training Step 7/59: batchLoss = 2.9274, diffLoss = 14.2695, kgLoss = 0.0919
2025-02-12 23:15:00.944286: Training Step 8/59: batchLoss = 2.9000, diffLoss = 14.1333, kgLoss = 0.0917
2025-02-12 23:15:01.878292: Training Step 9/59: batchLoss = 2.9014, diffLoss = 14.1665, kgLoss = 0.0851
2025-02-12 23:15:02.822287: Training Step 10/59: batchLoss = 2.5798, diffLoss = 12.5750, kgLoss = 0.0810
2025-02-12 23:15:03.763481: Training Step 11/59: batchLoss = 2.9298, diffLoss = 14.3205, kgLoss = 0.0821
2025-02-12 23:15:04.694460: Training Step 12/59: batchLoss = 3.0249, diffLoss = 14.7535, kgLoss = 0.0927
2025-02-12 23:15:05.631882: Training Step 13/59: batchLoss = 2.8280, diffLoss = 13.8205, kgLoss = 0.0799
2025-02-12 23:15:06.558541: Training Step 14/59: batchLoss = 2.8394, diffLoss = 13.8787, kgLoss = 0.0796
2025-02-12 23:15:07.479402: Training Step 15/59: batchLoss = 2.9314, diffLoss = 14.3148, kgLoss = 0.0856
2025-02-12 23:15:08.405589: Training Step 16/59: batchLoss = 2.9196, diffLoss = 14.2098, kgLoss = 0.0971
2025-02-12 23:15:09.334486: Training Step 17/59: batchLoss = 2.7804, diffLoss = 13.5761, kgLoss = 0.0815
2025-02-12 23:15:10.260740: Training Step 18/59: batchLoss = 3.1070, diffLoss = 15.1541, kgLoss = 0.0952
2025-02-12 23:15:11.198751: Training Step 19/59: batchLoss = 2.7823, diffLoss = 13.6157, kgLoss = 0.0739
2025-02-12 23:15:12.139963: Training Step 20/59: batchLoss = 2.9555, diffLoss = 14.4443, kgLoss = 0.0833
2025-02-12 23:15:13.080253: Training Step 21/59: batchLoss = 2.9376, diffLoss = 14.3362, kgLoss = 0.0879
2025-02-12 23:15:14.015461: Training Step 22/59: batchLoss = 3.0578, diffLoss = 14.9484, kgLoss = 0.0852
2025-02-12 23:15:14.951691: Training Step 23/59: batchLoss = 2.6923, diffLoss = 13.1537, kgLoss = 0.0770
2025-02-12 23:15:15.886502: Training Step 24/59: batchLoss = 3.0878, diffLoss = 15.0765, kgLoss = 0.0906
2025-02-12 23:15:16.815945: Training Step 25/59: batchLoss = 2.7143, diffLoss = 13.2435, kgLoss = 0.0820
2025-02-12 23:15:17.757031: Training Step 26/59: batchLoss = 2.9898, diffLoss = 14.6101, kgLoss = 0.0848
2025-02-12 23:15:18.700995: Training Step 27/59: batchLoss = 3.1777, diffLoss = 15.5417, kgLoss = 0.0866
2025-02-12 23:15:19.636449: Training Step 28/59: batchLoss = 3.1826, diffLoss = 15.5075, kgLoss = 0.1014
2025-02-12 23:15:20.577848: Training Step 29/59: batchLoss = 2.9903, diffLoss = 14.5922, kgLoss = 0.0898
2025-02-12 23:15:21.512025: Training Step 30/59: batchLoss = 2.9123, diffLoss = 14.2129, kgLoss = 0.0872
2025-02-12 23:15:22.444553: Training Step 31/59: batchLoss = 3.0694, diffLoss = 14.9771, kgLoss = 0.0925
2025-02-12 23:15:23.366301: Training Step 32/59: batchLoss = 2.7797, diffLoss = 13.5543, kgLoss = 0.0861
2025-02-12 23:15:24.290468: Training Step 33/59: batchLoss = 2.9406, diffLoss = 14.3529, kgLoss = 0.0875
2025-02-12 23:15:25.217986: Training Step 34/59: batchLoss = 2.8136, diffLoss = 13.7467, kgLoss = 0.0804
2025-02-12 23:15:26.142096: Training Step 35/59: batchLoss = 3.0623, diffLoss = 14.9655, kgLoss = 0.0865
2025-02-12 23:15:27.090439: Training Step 36/59: batchLoss = 3.3891, diffLoss = 16.5433, kgLoss = 0.1006
2025-02-12 23:15:28.021233: Training Step 37/59: batchLoss = 2.8207, diffLoss = 13.7784, kgLoss = 0.0813
2025-02-12 23:15:28.951527: Training Step 38/59: batchLoss = 2.8470, diffLoss = 13.8892, kgLoss = 0.0865
2025-02-12 23:15:29.896003: Training Step 39/59: batchLoss = 3.0539, diffLoss = 14.9290, kgLoss = 0.0851
2025-02-12 23:15:30.838233: Training Step 40/59: batchLoss = 3.3979, diffLoss = 16.6294, kgLoss = 0.0900
2025-02-12 23:15:31.777189: Training Step 41/59: batchLoss = 2.9697, diffLoss = 14.5342, kgLoss = 0.0786
2025-02-12 23:15:32.717180: Training Step 42/59: batchLoss = 3.1011, diffLoss = 15.1742, kgLoss = 0.0828
2025-02-12 23:15:33.654862: Training Step 43/59: batchLoss = 2.8643, diffLoss = 13.9886, kgLoss = 0.0832
2025-02-12 23:15:34.593763: Training Step 44/59: batchLoss = 2.7091, diffLoss = 13.2285, kgLoss = 0.0792
2025-02-12 23:15:35.537973: Training Step 45/59: batchLoss = 3.2573, diffLoss = 15.8982, kgLoss = 0.0971
2025-02-12 23:15:36.477531: Training Step 46/59: batchLoss = 2.9443, diffLoss = 14.3919, kgLoss = 0.0824
2025-02-12 23:15:37.420061: Training Step 47/59: batchLoss = 3.0906, diffLoss = 15.0920, kgLoss = 0.0902
2025-02-12 23:15:38.352131: Training Step 48/59: batchLoss = 3.0326, diffLoss = 14.8106, kgLoss = 0.0881
2025-02-12 23:15:39.281479: Training Step 49/59: batchLoss = 3.1933, diffLoss = 15.6025, kgLoss = 0.0910
2025-02-12 23:15:40.215921: Training Step 50/59: batchLoss = 2.8202, diffLoss = 13.7877, kgLoss = 0.0783
2025-02-12 23:15:41.149175: Training Step 51/59: batchLoss = 2.8851, diffLoss = 14.1228, kgLoss = 0.0757
2025-02-12 23:15:42.081719: Training Step 52/59: batchLoss = 3.0112, diffLoss = 14.7104, kgLoss = 0.0864
2025-02-12 23:15:43.020350: Training Step 53/59: batchLoss = 3.1069, diffLoss = 15.1889, kgLoss = 0.0864
2025-02-12 23:15:43.949300: Training Step 54/59: batchLoss = 3.2426, diffLoss = 15.8566, kgLoss = 0.0891
2025-02-12 23:15:44.878370: Training Step 55/59: batchLoss = 3.1243, diffLoss = 15.2616, kgLoss = 0.0900
2025-02-12 23:15:45.807351: Training Step 56/59: batchLoss = 2.7072, diffLoss = 13.2431, kgLoss = 0.0732
2025-02-12 23:15:46.651512: Training Step 57/59: batchLoss = 3.0389, diffLoss = 14.8841, kgLoss = 0.0776
2025-02-12 23:15:47.509099: Training Step 58/59: batchLoss = 2.9009, diffLoss = 14.1894, kgLoss = 0.0788
2025-02-12 23:15:47.609152: 
2025-02-12 23:15:47.609724: Epoch 35/1000, Train: epLoss = 0.4349, epDfLoss = 2.1239, epKgLoss = 0.0127  
2025-02-12 23:15:49.094944: Steps 0/47: batch_recall = 36.92, batch_ndcg = 46.56 
2025-02-12 23:15:50.393177: Steps 1/47: batch_recall = 36.27, batch_ndcg = 41.15 
2025-02-12 23:15:51.659267: Steps 2/47: batch_recall = 39.84, batch_ndcg = 46.01 
2025-02-12 23:15:52.937032: Steps 3/47: batch_recall = 44.04, batch_ndcg = 45.54 
2025-02-12 23:15:54.155701: Steps 4/47: batch_recall = 38.97, batch_ndcg = 44.52 
2025-02-12 23:15:55.373128: Steps 5/47: batch_recall = 33.13, batch_ndcg = 38.72 
2025-02-12 23:15:56.574249: Steps 6/47: batch_recall = 39.05, batch_ndcg = 41.99 
2025-02-12 23:15:57.744460: Steps 7/47: batch_recall = 43.50, batch_ndcg = 44.91 
2025-02-12 23:15:58.916183: Steps 8/47: batch_recall = 47.85, batch_ndcg = 50.49 
2025-02-12 23:16:00.045500: Steps 9/47: batch_recall = 45.17, batch_ndcg = 45.13 
2025-02-12 23:16:01.214628: Steps 10/47: batch_recall = 43.66, batch_ndcg = 42.67 
2025-02-12 23:16:02.358905: Steps 11/47: batch_recall = 53.63, batch_ndcg = 50.67 
2025-02-12 23:16:03.513283: Steps 12/47: batch_recall = 49.13, batch_ndcg = 47.10 
2025-02-12 23:16:04.663538: Steps 13/47: batch_recall = 49.34, batch_ndcg = 45.47 
2025-02-12 23:16:05.773043: Steps 14/47: batch_recall = 39.78, batch_ndcg = 40.22 
2025-02-12 23:16:06.873667: Steps 15/47: batch_recall = 55.93, batch_ndcg = 52.42 
2025-02-12 23:16:07.966240: Steps 16/47: batch_recall = 49.65, batch_ndcg = 46.02 
2025-02-12 23:16:09.031630: Steps 17/47: batch_recall = 57.08, batch_ndcg = 49.13 
2025-02-12 23:16:10.118898: Steps 18/47: batch_recall = 50.99, batch_ndcg = 47.30 
2025-02-12 23:16:11.197283: Steps 19/47: batch_recall = 61.20, batch_ndcg = 55.55 
2025-02-12 23:16:12.250993: Steps 20/47: batch_recall = 66.62, batch_ndcg = 60.94 
2025-02-12 23:16:13.303309: Steps 21/47: batch_recall = 65.48, batch_ndcg = 54.86 
2025-02-12 23:16:14.356807: Steps 22/47: batch_recall = 55.66, batch_ndcg = 51.83 
2025-02-12 23:16:15.402394: Steps 23/47: batch_recall = 61.64, batch_ndcg = 53.23 
2025-02-12 23:16:16.473581: Steps 24/47: batch_recall = 65.26, batch_ndcg = 56.64 
2025-02-12 23:16:17.518804: Steps 25/47: batch_recall = 66.46, batch_ndcg = 57.09 
2025-02-12 23:16:18.533020: Steps 26/47: batch_recall = 58.56, batch_ndcg = 51.70 
2025-02-12 23:16:19.571195: Steps 27/47: batch_recall = 62.57, batch_ndcg = 52.62 
2025-02-12 23:16:20.597052: Steps 28/47: batch_recall = 71.91, batch_ndcg = 59.55 
2025-02-12 23:16:21.624360: Steps 29/47: batch_recall = 67.50, batch_ndcg = 55.34 
2025-02-12 23:16:22.641077: Steps 30/47: batch_recall = 72.45, batch_ndcg = 63.36 
2025-02-12 23:16:23.685494: Steps 31/47: batch_recall = 65.93, batch_ndcg = 54.74 
2025-02-12 23:16:24.710182: Steps 32/47: batch_recall = 70.34, batch_ndcg = 63.88 
2025-02-12 23:16:25.730780: Steps 33/47: batch_recall = 80.25, batch_ndcg = 67.90 
2025-02-12 23:16:26.750565: Steps 34/47: batch_recall = 67.87, batch_ndcg = 55.33 
2025-02-12 23:16:27.743321: Steps 35/47: batch_recall = 75.43, batch_ndcg = 64.20 
2025-02-12 23:16:28.733961: Steps 36/47: batch_recall = 77.32, batch_ndcg = 63.23 
2025-02-12 23:16:29.710340: Steps 37/47: batch_recall = 83.64, batch_ndcg = 71.52 
2025-02-12 23:16:30.680995: Steps 38/47: batch_recall = 88.85, batch_ndcg = 70.63 
2025-02-12 23:16:31.656410: Steps 39/47: batch_recall = 87.10, batch_ndcg = 67.73 
2025-02-12 23:16:32.621146: Steps 40/47: batch_recall = 72.66, batch_ndcg = 61.99 
2025-02-12 23:16:33.582354: Steps 41/47: batch_recall = 87.00, batch_ndcg = 70.43 
2025-02-12 23:16:34.547183: Steps 42/47: batch_recall = 79.48, batch_ndcg = 63.40 
2025-02-12 23:16:35.515227: Steps 43/47: batch_recall = 89.26, batch_ndcg = 70.54 
2025-02-12 23:16:36.484952: Steps 44/47: batch_recall = 87.89, batch_ndcg = 70.11 
2025-02-12 23:16:37.438145: Steps 45/47: batch_recall = 93.30, batch_ndcg = 74.05 
2025-02-12 23:16:37.544561: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.53 
2025-02-12 23:16:37.544688: Epoch 35/1000, Test: Recall = 0.1204, NDCG = 0.1074  

2025-02-12 23:16:38.755041: Training Step 0/59: batchLoss = 2.8265, diffLoss = 13.7965, kgLoss = 0.0840
2025-02-12 23:16:39.699234: Training Step 1/59: batchLoss = 2.7288, diffLoss = 13.3218, kgLoss = 0.0806
2025-02-12 23:16:40.642809: Training Step 2/59: batchLoss = 2.8379, diffLoss = 13.8591, kgLoss = 0.0826
2025-02-12 23:16:41.588358: Training Step 3/59: batchLoss = 2.8227, diffLoss = 13.7967, kgLoss = 0.0793
2025-02-12 23:16:42.533691: Training Step 4/59: batchLoss = 2.6432, diffLoss = 12.9150, kgLoss = 0.0752
2025-02-12 23:16:43.471956: Training Step 5/59: batchLoss = 2.8587, diffLoss = 13.9319, kgLoss = 0.0904
2025-02-12 23:16:44.415653: Training Step 6/59: batchLoss = 2.9363, diffLoss = 14.3221, kgLoss = 0.0899
2025-02-12 23:16:45.353891: Training Step 7/59: batchLoss = 2.7532, diffLoss = 13.4557, kgLoss = 0.0776
2025-02-12 23:16:46.296744: Training Step 8/59: batchLoss = 2.8900, diffLoss = 14.1092, kgLoss = 0.0852
2025-02-12 23:16:47.236261: Training Step 9/59: batchLoss = 2.8768, diffLoss = 14.0296, kgLoss = 0.0886
2025-02-12 23:16:48.175769: Training Step 10/59: batchLoss = 2.9640, diffLoss = 14.4596, kgLoss = 0.0901
2025-02-12 23:16:49.107524: Training Step 11/59: batchLoss = 2.9251, diffLoss = 14.2918, kgLoss = 0.0834
2025-02-12 23:16:50.037619: Training Step 12/59: batchLoss = 2.8180, diffLoss = 13.7739, kgLoss = 0.0790
2025-02-12 23:16:50.964981: Training Step 13/59: batchLoss = 3.0742, diffLoss = 15.0080, kgLoss = 0.0907
2025-02-12 23:16:51.902708: Training Step 14/59: batchLoss = 3.1346, diffLoss = 15.3036, kgLoss = 0.0924
2025-02-12 23:16:52.831889: Training Step 15/59: batchLoss = 2.7332, diffLoss = 13.3520, kgLoss = 0.0785
2025-02-12 23:16:53.763095: Training Step 16/59: batchLoss = 2.7299, diffLoss = 13.3242, kgLoss = 0.0813
2025-02-12 23:16:54.702259: Training Step 17/59: batchLoss = 3.1404, diffLoss = 15.3370, kgLoss = 0.0912
2025-02-12 23:16:55.643553: Training Step 18/59: batchLoss = 2.7830, diffLoss = 13.5838, kgLoss = 0.0828
2025-02-12 23:16:56.584218: Training Step 19/59: batchLoss = 2.7125, diffLoss = 13.2386, kgLoss = 0.0810
2025-02-12 23:16:57.521629: Training Step 20/59: batchLoss = 2.9955, diffLoss = 14.6348, kgLoss = 0.0857
2025-02-12 23:16:58.466807: Training Step 21/59: batchLoss = 2.9930, diffLoss = 14.6141, kgLoss = 0.0877
2025-02-12 23:16:59.412975: Training Step 22/59: batchLoss = 3.0095, diffLoss = 14.6984, kgLoss = 0.0872
2025-02-12 23:17:00.354832: Training Step 23/59: batchLoss = 3.3318, diffLoss = 16.2653, kgLoss = 0.0985
2025-02-12 23:17:01.294746: Training Step 24/59: batchLoss = 3.1558, diffLoss = 15.4133, kgLoss = 0.0914
2025-02-12 23:17:02.233728: Training Step 25/59: batchLoss = 2.7661, diffLoss = 13.4772, kgLoss = 0.0883
2025-02-12 23:17:03.178758: Training Step 26/59: batchLoss = 3.1092, diffLoss = 15.1561, kgLoss = 0.0974
2025-02-12 23:17:04.114360: Training Step 27/59: batchLoss = 3.2570, diffLoss = 15.8993, kgLoss = 0.0964
2025-02-12 23:17:05.046816: Training Step 28/59: batchLoss = 2.9618, diffLoss = 14.4904, kgLoss = 0.0796
2025-02-12 23:17:05.980759: Training Step 29/59: batchLoss = 2.8938, diffLoss = 14.1231, kgLoss = 0.0864
2025-02-12 23:17:06.903001: Training Step 30/59: batchLoss = 2.8012, diffLoss = 13.6938, kgLoss = 0.0780
2025-02-12 23:17:07.828130: Training Step 31/59: batchLoss = 3.0676, diffLoss = 14.9767, kgLoss = 0.0903
2025-02-12 23:17:08.756759: Training Step 32/59: batchLoss = 2.8840, diffLoss = 14.0938, kgLoss = 0.0815
2025-02-12 23:17:09.685333: Training Step 33/59: batchLoss = 3.1144, diffLoss = 15.2124, kgLoss = 0.0899
2025-02-12 23:17:10.621305: Training Step 34/59: batchLoss = 3.2351, diffLoss = 15.7996, kgLoss = 0.0939
2025-02-12 23:17:11.551448: Training Step 35/59: batchLoss = 2.8999, diffLoss = 14.1687, kgLoss = 0.0827
2025-02-12 23:17:12.494308: Training Step 36/59: batchLoss = 2.9630, diffLoss = 14.4767, kgLoss = 0.0846
2025-02-12 23:17:13.430834: Training Step 37/59: batchLoss = 2.9399, diffLoss = 14.3669, kgLoss = 0.0832
2025-02-12 23:17:14.365938: Training Step 38/59: batchLoss = 2.8750, diffLoss = 14.0436, kgLoss = 0.0829
2025-02-12 23:17:15.302730: Training Step 39/59: batchLoss = 3.2686, diffLoss = 15.9424, kgLoss = 0.1001
2025-02-12 23:17:16.243807: Training Step 40/59: batchLoss = 3.0412, diffLoss = 14.8636, kgLoss = 0.0856
2025-02-12 23:17:17.174996: Training Step 41/59: batchLoss = 2.7933, diffLoss = 13.6330, kgLoss = 0.0834
2025-02-12 23:17:18.114019: Training Step 42/59: batchLoss = 3.0987, diffLoss = 15.1489, kgLoss = 0.0861
2025-02-12 23:17:19.051429: Training Step 43/59: batchLoss = 2.9646, diffLoss = 14.4925, kgLoss = 0.0827
2025-02-12 23:17:19.991358: Training Step 44/59: batchLoss = 2.9999, diffLoss = 14.6467, kgLoss = 0.0882
2025-02-12 23:17:20.938010: Training Step 45/59: batchLoss = 3.1550, diffLoss = 15.4299, kgLoss = 0.0863
2025-02-12 23:17:21.878365: Training Step 46/59: batchLoss = 2.7021, diffLoss = 13.2089, kgLoss = 0.0754
2025-02-12 23:17:22.814113: Training Step 47/59: batchLoss = 3.0311, diffLoss = 14.8317, kgLoss = 0.0809
2025-02-12 23:17:23.738211: Training Step 48/59: batchLoss = 3.0094, diffLoss = 14.7102, kgLoss = 0.0841
2025-02-12 23:17:24.665952: Training Step 49/59: batchLoss = 3.1426, diffLoss = 15.3508, kgLoss = 0.0905
2025-02-12 23:17:25.603700: Training Step 50/59: batchLoss = 2.8807, diffLoss = 14.0774, kgLoss = 0.0815
2025-02-12 23:17:26.529683: Training Step 51/59: batchLoss = 3.0907, diffLoss = 15.1153, kgLoss = 0.0846
2025-02-12 23:17:27.466129: Training Step 52/59: batchLoss = 2.8208, diffLoss = 13.8103, kgLoss = 0.0734
2025-02-12 23:17:28.395828: Training Step 53/59: batchLoss = 2.9446, diffLoss = 14.3625, kgLoss = 0.0902
2025-02-12 23:17:29.333252: Training Step 54/59: batchLoss = 3.3154, diffLoss = 16.2189, kgLoss = 0.0895
2025-02-12 23:17:30.282672: Training Step 55/59: batchLoss = 2.8783, diffLoss = 14.0609, kgLoss = 0.0826
2025-02-12 23:17:31.215877: Training Step 56/59: batchLoss = 3.0219, diffLoss = 14.7579, kgLoss = 0.0880
2025-02-12 23:17:32.066739: Training Step 57/59: batchLoss = 2.8809, diffLoss = 14.0389, kgLoss = 0.0914
2025-02-12 23:17:32.920535: Training Step 58/59: batchLoss = 3.1034, diffLoss = 15.1685, kgLoss = 0.0871
2025-02-12 23:17:33.022900: 
2025-02-12 23:17:33.023445: Epoch 36/1000, Train: epLoss = 0.4365, epDfLoss = 2.1317, epKgLoss = 0.0127  
2025-02-12 23:17:34.498132: Steps 0/47: batch_recall = 36.43, batch_ndcg = 46.36 
2025-02-12 23:17:35.808964: Steps 1/47: batch_recall = 36.27, batch_ndcg = 41.44 
2025-02-12 23:17:37.058650: Steps 2/47: batch_recall = 40.08, batch_ndcg = 45.12 
2025-02-12 23:17:38.327985: Steps 3/47: batch_recall = 43.69, batch_ndcg = 45.13 
2025-02-12 23:17:39.523758: Steps 4/47: batch_recall = 38.81, batch_ndcg = 43.90 
2025-02-12 23:17:40.740968: Steps 5/47: batch_recall = 33.63, batch_ndcg = 38.94 
2025-02-12 23:17:41.935358: Steps 6/47: batch_recall = 39.21, batch_ndcg = 41.82 
2025-02-12 23:17:43.114389: Steps 7/47: batch_recall = 44.42, batch_ndcg = 45.17 
2025-02-12 23:17:44.288903: Steps 8/47: batch_recall = 48.15, batch_ndcg = 50.95 
2025-02-12 23:17:45.429219: Steps 9/47: batch_recall = 44.89, batch_ndcg = 45.20 
2025-02-12 23:17:46.611566: Steps 10/47: batch_recall = 43.59, batch_ndcg = 43.24 
2025-02-12 23:17:47.765698: Steps 11/47: batch_recall = 52.41, batch_ndcg = 49.58 
2025-02-12 23:17:48.920144: Steps 12/47: batch_recall = 49.42, batch_ndcg = 47.75 
2025-02-12 23:17:50.058340: Steps 13/47: batch_recall = 49.26, batch_ndcg = 46.06 
2025-02-12 23:17:51.159631: Steps 14/47: batch_recall = 40.34, batch_ndcg = 40.09 
2025-02-12 23:17:52.265208: Steps 15/47: batch_recall = 56.44, batch_ndcg = 52.78 
2025-02-12 23:17:53.359929: Steps 16/47: batch_recall = 50.35, batch_ndcg = 46.16 
2025-02-12 23:17:54.425708: Steps 17/47: batch_recall = 57.80, batch_ndcg = 49.43 
2025-02-12 23:17:55.512376: Steps 18/47: batch_recall = 51.09, batch_ndcg = 47.68 
2025-02-12 23:17:56.581258: Steps 19/47: batch_recall = 59.92, batch_ndcg = 55.43 
2025-02-12 23:17:57.627748: Steps 20/47: batch_recall = 67.10, batch_ndcg = 61.55 
2025-02-12 23:17:58.678063: Steps 21/47: batch_recall = 66.21, batch_ndcg = 55.00 
2025-02-12 23:17:59.727643: Steps 22/47: batch_recall = 53.76, batch_ndcg = 50.63 
2025-02-12 23:18:00.776559: Steps 23/47: batch_recall = 62.45, batch_ndcg = 53.69 
2025-02-12 23:18:01.840015: Steps 24/47: batch_recall = 64.52, batch_ndcg = 57.34 
2025-02-12 23:18:02.889466: Steps 25/47: batch_recall = 65.99, batch_ndcg = 57.91 
2025-02-12 23:18:03.917003: Steps 26/47: batch_recall = 59.30, batch_ndcg = 51.75 
2025-02-12 23:18:04.960581: Steps 27/47: batch_recall = 62.58, batch_ndcg = 52.26 
2025-02-12 23:18:05.995140: Steps 28/47: batch_recall = 69.99, batch_ndcg = 59.03 
2025-02-12 23:18:07.029060: Steps 29/47: batch_recall = 70.07, batch_ndcg = 56.83 
2025-02-12 23:18:08.049958: Steps 30/47: batch_recall = 72.89, batch_ndcg = 63.26 
2025-02-12 23:18:09.091733: Steps 31/47: batch_recall = 65.42, batch_ndcg = 54.16 
2025-02-12 23:18:10.117635: Steps 32/47: batch_recall = 70.62, batch_ndcg = 64.23 
2025-02-12 23:18:11.144181: Steps 33/47: batch_recall = 78.88, batch_ndcg = 66.89 
2025-02-12 23:18:12.169032: Steps 34/47: batch_recall = 67.38, batch_ndcg = 55.03 
2025-02-12 23:18:13.155299: Steps 35/47: batch_recall = 75.83, batch_ndcg = 63.26 
2025-02-12 23:18:14.145237: Steps 36/47: batch_recall = 77.13, batch_ndcg = 64.06 
2025-02-12 23:18:15.122693: Steps 37/47: batch_recall = 83.10, batch_ndcg = 71.64 
2025-02-12 23:18:16.089609: Steps 38/47: batch_recall = 89.66, batch_ndcg = 72.12 
2025-02-12 23:18:17.062445: Steps 39/47: batch_recall = 86.21, batch_ndcg = 67.68 
2025-02-12 23:18:18.025707: Steps 40/47: batch_recall = 71.28, batch_ndcg = 61.83 
2025-02-12 23:18:18.988930: Steps 41/47: batch_recall = 87.13, batch_ndcg = 71.23 
2025-02-12 23:18:19.946845: Steps 42/47: batch_recall = 82.52, batch_ndcg = 64.73 
2025-02-12 23:18:20.906276: Steps 43/47: batch_recall = 89.39, batch_ndcg = 70.04 
2025-02-12 23:18:21.877286: Steps 44/47: batch_recall = 87.12, batch_ndcg = 69.84 
2025-02-12 23:18:22.829415: Steps 45/47: batch_recall = 92.63, batch_ndcg = 73.88 
2025-02-12 23:18:22.937981: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-12 23:18:22.938121: Epoch 36/1000, Test: Recall = 0.1204, NDCG = 0.1075  

2025-02-12 23:18:24.148565: Training Step 0/59: batchLoss = 2.7811, diffLoss = 13.5548, kgLoss = 0.0877
2025-02-12 23:18:25.078844: Training Step 1/59: batchLoss = 2.8319, diffLoss = 13.8268, kgLoss = 0.0832
2025-02-12 23:18:26.016092: Training Step 2/59: batchLoss = 2.6128, diffLoss = 12.7489, kgLoss = 0.0788
2025-02-12 23:18:26.953099: Training Step 3/59: batchLoss = 2.7737, diffLoss = 13.5448, kgLoss = 0.0809
2025-02-12 23:18:27.892547: Training Step 4/59: batchLoss = 3.0494, diffLoss = 14.8683, kgLoss = 0.0946
2025-02-12 23:18:28.824892: Training Step 5/59: batchLoss = 2.7043, diffLoss = 13.1570, kgLoss = 0.0912
2025-02-12 23:18:29.760523: Training Step 6/59: batchLoss = 2.7003, diffLoss = 13.1695, kgLoss = 0.0830
2025-02-12 23:18:30.697198: Training Step 7/59: batchLoss = 2.5459, diffLoss = 12.4461, kgLoss = 0.0708
2025-02-12 23:18:31.623796: Training Step 8/59: batchLoss = 2.7763, diffLoss = 13.5459, kgLoss = 0.0838
2025-02-12 23:18:32.554692: Training Step 9/59: batchLoss = 2.7962, diffLoss = 13.6341, kgLoss = 0.0867
2025-02-12 23:18:33.479810: Training Step 10/59: batchLoss = 2.8866, diffLoss = 14.0988, kgLoss = 0.0835
2025-02-12 23:18:34.405827: Training Step 11/59: batchLoss = 2.6130, diffLoss = 12.7717, kgLoss = 0.0734
2025-02-12 23:18:35.333317: Training Step 12/59: batchLoss = 3.1570, diffLoss = 15.4181, kgLoss = 0.0917
2025-02-12 23:18:36.265026: Training Step 13/59: batchLoss = 2.9354, diffLoss = 14.3307, kgLoss = 0.0865
2025-02-12 23:18:37.189879: Training Step 14/59: batchLoss = 2.8142, diffLoss = 13.7173, kgLoss = 0.0885
2025-02-12 23:18:38.115540: Training Step 15/59: batchLoss = 2.5899, diffLoss = 12.6027, kgLoss = 0.0867
2025-02-12 23:18:39.052117: Training Step 16/59: batchLoss = 3.2386, diffLoss = 15.7992, kgLoss = 0.0984
2025-02-12 23:18:39.977749: Training Step 17/59: batchLoss = 2.9405, diffLoss = 14.3645, kgLoss = 0.0845
2025-02-12 23:18:40.912243: Training Step 18/59: batchLoss = 2.9183, diffLoss = 14.2591, kgLoss = 0.0831
2025-02-12 23:18:41.849664: Training Step 19/59: batchLoss = 2.7097, diffLoss = 13.2426, kgLoss = 0.0765
2025-02-12 23:18:42.779563: Training Step 20/59: batchLoss = 3.1393, diffLoss = 15.3016, kgLoss = 0.0987
2025-02-12 23:18:43.709596: Training Step 21/59: batchLoss = 2.5916, diffLoss = 12.6496, kgLoss = 0.0771
2025-02-12 23:18:44.648310: Training Step 22/59: batchLoss = 2.7000, diffLoss = 13.1684, kgLoss = 0.0829
2025-02-12 23:18:45.589678: Training Step 23/59: batchLoss = 2.6981, diffLoss = 13.1612, kgLoss = 0.0824
2025-02-12 23:18:46.523704: Training Step 24/59: batchLoss = 2.7628, diffLoss = 13.4822, kgLoss = 0.0830
2025-02-12 23:18:47.457042: Training Step 25/59: batchLoss = 3.3529, diffLoss = 16.3669, kgLoss = 0.0994
2025-02-12 23:18:48.385238: Training Step 26/59: batchLoss = 3.0026, diffLoss = 14.6815, kgLoss = 0.0829
2025-02-12 23:18:49.320402: Training Step 27/59: batchLoss = 3.0676, diffLoss = 15.0005, kgLoss = 0.0844
2025-02-12 23:18:50.246428: Training Step 28/59: batchLoss = 2.8201, diffLoss = 13.7859, kgLoss = 0.0786
2025-02-12 23:18:51.176223: Training Step 29/59: batchLoss = 3.0381, diffLoss = 14.8585, kgLoss = 0.0830
2025-02-12 23:18:52.104157: Training Step 30/59: batchLoss = 3.0813, diffLoss = 15.0224, kgLoss = 0.0960
2025-02-12 23:18:53.037151: Training Step 31/59: batchLoss = 3.0223, diffLoss = 14.7830, kgLoss = 0.0821
2025-02-12 23:18:53.966929: Training Step 32/59: batchLoss = 2.9843, diffLoss = 14.5926, kgLoss = 0.0822
2025-02-12 23:18:54.888078: Training Step 33/59: batchLoss = 3.1340, diffLoss = 15.3230, kgLoss = 0.0868
2025-02-12 23:18:55.826981: Training Step 34/59: batchLoss = 2.9162, diffLoss = 14.2534, kgLoss = 0.0819
2025-02-12 23:18:56.774995: Training Step 35/59: batchLoss = 3.1470, diffLoss = 15.3352, kgLoss = 0.1000
2025-02-12 23:18:57.708526: Training Step 36/59: batchLoss = 2.8592, diffLoss = 13.9614, kgLoss = 0.0836
2025-02-12 23:18:58.641834: Training Step 37/59: batchLoss = 3.2643, diffLoss = 15.9633, kgLoss = 0.0896
2025-02-12 23:18:59.581414: Training Step 38/59: batchLoss = 3.0083, diffLoss = 14.6800, kgLoss = 0.0904
2025-02-12 23:19:00.513323: Training Step 39/59: batchLoss = 3.0360, diffLoss = 14.8164, kgLoss = 0.0909
2025-02-12 23:19:01.449923: Training Step 40/59: batchLoss = 3.1360, diffLoss = 15.3379, kgLoss = 0.0855
2025-02-12 23:19:02.392905: Training Step 41/59: batchLoss = 3.3486, diffLoss = 16.3655, kgLoss = 0.0943
2025-02-12 23:19:03.327906: Training Step 42/59: batchLoss = 2.9032, diffLoss = 14.2161, kgLoss = 0.0750
2025-02-12 23:19:04.257276: Training Step 43/59: batchLoss = 3.0246, diffLoss = 14.8045, kgLoss = 0.0796
2025-02-12 23:19:05.199286: Training Step 44/59: batchLoss = 2.9762, diffLoss = 14.5531, kgLoss = 0.0820
2025-02-12 23:19:06.129029: Training Step 45/59: batchLoss = 2.9283, diffLoss = 14.3017, kgLoss = 0.0850
2025-02-12 23:19:07.060816: Training Step 46/59: batchLoss = 3.1409, diffLoss = 15.3756, kgLoss = 0.0822
2025-02-12 23:19:07.988481: Training Step 47/59: batchLoss = 3.4465, diffLoss = 16.8500, kgLoss = 0.0956
2025-02-12 23:19:08.915808: Training Step 48/59: batchLoss = 3.0221, diffLoss = 14.7669, kgLoss = 0.0860
2025-02-12 23:19:09.850167: Training Step 49/59: batchLoss = 3.4231, diffLoss = 16.7512, kgLoss = 0.0911
2025-02-12 23:19:10.774036: Training Step 50/59: batchLoss = 2.8110, diffLoss = 13.7437, kgLoss = 0.0778
2025-02-12 23:19:11.707217: Training Step 51/59: batchLoss = 3.2199, diffLoss = 15.7455, kgLoss = 0.0885
2025-02-12 23:19:12.637684: Training Step 52/59: batchLoss = 3.0353, diffLoss = 14.8529, kgLoss = 0.0808
2025-02-12 23:19:13.571697: Training Step 53/59: batchLoss = 3.3444, diffLoss = 16.3473, kgLoss = 0.0937
2025-02-12 23:19:14.506924: Training Step 54/59: batchLoss = 3.2823, diffLoss = 16.0286, kgLoss = 0.0957
2025-02-12 23:19:15.442769: Training Step 55/59: batchLoss = 2.7388, diffLoss = 13.3677, kgLoss = 0.0816
2025-02-12 23:19:16.372404: Training Step 56/59: batchLoss = 3.0948, diffLoss = 15.1260, kgLoss = 0.0870
2025-02-12 23:19:17.222556: Training Step 57/59: batchLoss = 3.2275, diffLoss = 15.7869, kgLoss = 0.0877
2025-02-12 23:19:18.081623: Training Step 58/59: batchLoss = 2.9563, diffLoss = 14.4541, kgLoss = 0.0819
2025-02-12 23:19:18.179634: 
2025-02-12 23:19:18.179991: Epoch 37/1000, Train: epLoss = 0.4377, epDfLoss = 2.1377, epKgLoss = 0.0127  
2025-02-12 23:19:19.652188: Steps 0/47: batch_recall = 36.20, batch_ndcg = 45.79 
2025-02-12 23:19:20.960057: Steps 1/47: batch_recall = 36.30, batch_ndcg = 42.05 
2025-02-12 23:19:22.221135: Steps 2/47: batch_recall = 40.66, batch_ndcg = 46.24 
2025-02-12 23:19:23.481980: Steps 3/47: batch_recall = 43.61, batch_ndcg = 44.81 
2025-02-12 23:19:24.664267: Steps 4/47: batch_recall = 38.79, batch_ndcg = 44.10 
2025-02-12 23:19:25.865727: Steps 5/47: batch_recall = 32.42, batch_ndcg = 38.64 
2025-02-12 23:19:27.055404: Steps 6/47: batch_recall = 39.28, batch_ndcg = 42.35 
2025-02-12 23:19:28.215747: Steps 7/47: batch_recall = 44.23, batch_ndcg = 44.76 
2025-02-12 23:19:29.384471: Steps 8/47: batch_recall = 48.40, batch_ndcg = 51.36 
2025-02-12 23:19:30.521336: Steps 9/47: batch_recall = 46.12, batch_ndcg = 45.80 
2025-02-12 23:19:31.689163: Steps 10/47: batch_recall = 43.79, batch_ndcg = 42.55 
2025-02-12 23:19:32.829261: Steps 11/47: batch_recall = 54.00, batch_ndcg = 50.92 
2025-02-12 23:19:33.975977: Steps 12/47: batch_recall = 49.09, batch_ndcg = 47.18 
2025-02-12 23:19:35.104676: Steps 13/47: batch_recall = 47.69, batch_ndcg = 44.77 
2025-02-12 23:19:36.216277: Steps 14/47: batch_recall = 40.92, batch_ndcg = 41.13 
2025-02-12 23:19:37.313105: Steps 15/47: batch_recall = 56.17, batch_ndcg = 52.89 
2025-02-12 23:19:38.408011: Steps 16/47: batch_recall = 51.01, batch_ndcg = 46.03 
2025-02-12 23:19:39.462504: Steps 17/47: batch_recall = 56.95, batch_ndcg = 48.99 
2025-02-12 23:19:40.545511: Steps 18/47: batch_recall = 51.50, batch_ndcg = 48.16 
2025-02-12 23:19:41.612920: Steps 19/47: batch_recall = 59.93, batch_ndcg = 55.42 
2025-02-12 23:19:42.660531: Steps 20/47: batch_recall = 68.09, batch_ndcg = 61.05 
2025-02-12 23:19:43.701996: Steps 21/47: batch_recall = 65.46, batch_ndcg = 55.23 
2025-02-12 23:19:44.757905: Steps 22/47: batch_recall = 54.83, batch_ndcg = 51.13 
2025-02-12 23:19:45.799286: Steps 23/47: batch_recall = 62.98, batch_ndcg = 53.73 
2025-02-12 23:19:46.861921: Steps 24/47: batch_recall = 63.62, batch_ndcg = 57.27 
2025-02-12 23:19:47.907824: Steps 25/47: batch_recall = 65.11, batch_ndcg = 56.97 
2025-02-12 23:19:48.931689: Steps 26/47: batch_recall = 58.82, batch_ndcg = 51.35 
2025-02-12 23:19:49.976931: Steps 27/47: batch_recall = 61.72, batch_ndcg = 52.56 
2025-02-12 23:19:51.005329: Steps 28/47: batch_recall = 69.15, batch_ndcg = 58.09 
2025-02-12 23:19:52.035942: Steps 29/47: batch_recall = 69.85, batch_ndcg = 57.39 
2025-02-12 23:19:53.067783: Steps 30/47: batch_recall = 72.60, batch_ndcg = 64.26 
2025-02-12 23:19:54.112908: Steps 31/47: batch_recall = 65.79, batch_ndcg = 54.68 
2025-02-12 23:19:55.134762: Steps 32/47: batch_recall = 71.30, batch_ndcg = 65.29 
2025-02-12 23:19:56.154924: Steps 33/47: batch_recall = 79.30, batch_ndcg = 68.06 
2025-02-12 23:19:57.175649: Steps 34/47: batch_recall = 67.64, batch_ndcg = 55.43 
2025-02-12 23:19:58.161713: Steps 35/47: batch_recall = 77.41, batch_ndcg = 63.26 
2025-02-12 23:19:59.151844: Steps 36/47: batch_recall = 78.24, batch_ndcg = 64.52 
2025-02-12 23:20:00.126555: Steps 37/47: batch_recall = 82.28, batch_ndcg = 70.89 
2025-02-12 23:20:01.107445: Steps 38/47: batch_recall = 89.05, batch_ndcg = 71.82 
2025-02-12 23:20:02.075763: Steps 39/47: batch_recall = 86.42, batch_ndcg = 67.21 
2025-02-12 23:20:03.040388: Steps 40/47: batch_recall = 72.04, batch_ndcg = 61.18 
2025-02-12 23:20:04.007458: Steps 41/47: batch_recall = 87.92, batch_ndcg = 71.58 
2025-02-12 23:20:04.972579: Steps 42/47: batch_recall = 81.77, batch_ndcg = 64.08 
2025-02-12 23:20:05.936770: Steps 43/47: batch_recall = 89.04, batch_ndcg = 70.86 
2025-02-12 23:20:06.901370: Steps 44/47: batch_recall = 86.92, batch_ndcg = 68.60 
2025-02-12 23:20:07.853045: Steps 45/47: batch_recall = 92.08, batch_ndcg = 72.55 
2025-02-12 23:20:07.961759: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.51 
2025-02-12 23:20:07.961874: Epoch 37/1000, Test: Recall = 0.1204, NDCG = 0.1075  

2025-02-12 23:20:09.171477: Training Step 0/59: batchLoss = 2.7232, diffLoss = 13.2442, kgLoss = 0.0929
2025-02-12 23:20:10.107145: Training Step 1/59: batchLoss = 2.9565, diffLoss = 14.4288, kgLoss = 0.0884
2025-02-12 23:20:11.036645: Training Step 2/59: batchLoss = 2.8905, diffLoss = 14.0919, kgLoss = 0.0901
2025-02-12 23:20:11.970419: Training Step 3/59: batchLoss = 2.7999, diffLoss = 13.6766, kgLoss = 0.0807
2025-02-12 23:20:12.905897: Training Step 4/59: batchLoss = 2.7554, diffLoss = 13.4344, kgLoss = 0.0856
2025-02-12 23:20:13.843924: Training Step 5/59: batchLoss = 2.7909, diffLoss = 13.6301, kgLoss = 0.0812
2025-02-12 23:20:14.787952: Training Step 6/59: batchLoss = 2.8051, diffLoss = 13.6731, kgLoss = 0.0881
2025-02-12 23:20:15.716841: Training Step 7/59: batchLoss = 2.7421, diffLoss = 13.3388, kgLoss = 0.0930
2025-02-12 23:20:16.642105: Training Step 8/59: batchLoss = 3.2158, diffLoss = 15.6880, kgLoss = 0.0978
2025-02-12 23:20:17.574512: Training Step 9/59: batchLoss = 2.8860, diffLoss = 14.0809, kgLoss = 0.0872
2025-02-12 23:20:18.499947: Training Step 10/59: batchLoss = 2.8192, diffLoss = 13.7449, kgLoss = 0.0878
2025-02-12 23:20:19.427780: Training Step 11/59: batchLoss = 2.7534, diffLoss = 13.4098, kgLoss = 0.0893
2025-02-12 23:20:20.359268: Training Step 12/59: batchLoss = 2.8805, diffLoss = 14.0724, kgLoss = 0.0825
2025-02-12 23:20:21.284184: Training Step 13/59: batchLoss = 2.6962, diffLoss = 13.1489, kgLoss = 0.0830
2025-02-12 23:20:22.217956: Training Step 14/59: batchLoss = 3.1220, diffLoss = 15.2438, kgLoss = 0.0916
2025-02-12 23:20:23.154969: Training Step 15/59: batchLoss = 2.7314, diffLoss = 13.3216, kgLoss = 0.0838
2025-02-12 23:20:24.088995: Training Step 16/59: batchLoss = 3.1385, diffLoss = 15.3175, kgLoss = 0.0937
2025-02-12 23:20:25.026050: Training Step 17/59: batchLoss = 2.8252, diffLoss = 13.7782, kgLoss = 0.0869
2025-02-12 23:20:25.959678: Training Step 18/59: batchLoss = 2.6790, diffLoss = 13.0680, kgLoss = 0.0817
2025-02-12 23:20:26.892707: Training Step 19/59: batchLoss = 3.0063, diffLoss = 14.6978, kgLoss = 0.0835
2025-02-12 23:20:27.820629: Training Step 20/59: batchLoss = 2.7283, diffLoss = 13.3074, kgLoss = 0.0836
2025-02-12 23:20:28.749312: Training Step 21/59: batchLoss = 2.7835, diffLoss = 13.5979, kgLoss = 0.0799
2025-02-12 23:20:29.683844: Training Step 22/59: batchLoss = 2.9067, diffLoss = 14.1799, kgLoss = 0.0884
2025-02-12 23:20:30.619734: Training Step 23/59: batchLoss = 2.9673, diffLoss = 14.4927, kgLoss = 0.0860
2025-02-12 23:20:31.552846: Training Step 24/59: batchLoss = 2.8799, diffLoss = 14.0779, kgLoss = 0.0804
2025-02-12 23:20:32.485396: Training Step 25/59: batchLoss = 3.0592, diffLoss = 14.9242, kgLoss = 0.0930
2025-02-12 23:20:33.414304: Training Step 26/59: batchLoss = 2.9879, diffLoss = 14.5823, kgLoss = 0.0893
2025-02-12 23:20:34.340507: Training Step 27/59: batchLoss = 2.9802, diffLoss = 14.5965, kgLoss = 0.0761
2025-02-12 23:20:35.269731: Training Step 28/59: batchLoss = 2.8217, diffLoss = 13.7692, kgLoss = 0.0848
2025-02-12 23:20:36.200126: Training Step 29/59: batchLoss = 2.8189, diffLoss = 13.7905, kgLoss = 0.0761
2025-02-12 23:20:37.124476: Training Step 30/59: batchLoss = 3.1610, diffLoss = 15.4220, kgLoss = 0.0958
2025-02-12 23:20:38.047959: Training Step 31/59: batchLoss = 2.9807, diffLoss = 14.5516, kgLoss = 0.0879
2025-02-12 23:20:38.971620: Training Step 32/59: batchLoss = 2.7774, diffLoss = 13.5870, kgLoss = 0.0750
2025-02-12 23:20:39.884687: Training Step 33/59: batchLoss = 2.9339, diffLoss = 14.3217, kgLoss = 0.0870
2025-02-12 23:20:40.813130: Training Step 34/59: batchLoss = 2.7045, diffLoss = 13.2149, kgLoss = 0.0769
2025-02-12 23:20:41.751249: Training Step 35/59: batchLoss = 2.9614, diffLoss = 14.4410, kgLoss = 0.0914
2025-02-12 23:20:42.687133: Training Step 36/59: batchLoss = 3.0549, diffLoss = 14.9372, kgLoss = 0.0843
2025-02-12 23:20:43.611692: Training Step 37/59: batchLoss = 3.0450, diffLoss = 14.8912, kgLoss = 0.0835
2025-02-12 23:20:44.535570: Training Step 38/59: batchLoss = 2.8529, diffLoss = 13.9251, kgLoss = 0.0848
2025-02-12 23:20:45.471952: Training Step 39/59: batchLoss = 2.9273, diffLoss = 14.2802, kgLoss = 0.0890
2025-02-12 23:20:46.402198: Training Step 40/59: batchLoss = 2.7640, diffLoss = 13.4989, kgLoss = 0.0803
2025-02-12 23:20:47.328394: Training Step 41/59: batchLoss = 3.3921, diffLoss = 16.5779, kgLoss = 0.0957
2025-02-12 23:20:48.257211: Training Step 42/59: batchLoss = 2.9891, diffLoss = 14.6031, kgLoss = 0.0856
2025-02-12 23:20:49.185325: Training Step 43/59: batchLoss = 3.3611, diffLoss = 16.4191, kgLoss = 0.0966
2025-02-12 23:20:50.104445: Training Step 44/59: batchLoss = 2.8713, diffLoss = 14.0395, kgLoss = 0.0793
2025-02-12 23:20:51.029926: Training Step 45/59: batchLoss = 2.8434, diffLoss = 13.9148, kgLoss = 0.0755
2025-02-12 23:20:51.957297: Training Step 46/59: batchLoss = 3.0305, diffLoss = 14.8308, kgLoss = 0.0805
2025-02-12 23:20:52.887723: Training Step 47/59: batchLoss = 3.0822, diffLoss = 15.0721, kgLoss = 0.0847
2025-02-12 23:20:53.811318: Training Step 48/59: batchLoss = 2.8285, diffLoss = 13.7977, kgLoss = 0.0862
2025-02-12 23:20:54.743540: Training Step 49/59: batchLoss = 2.8032, diffLoss = 13.7117, kgLoss = 0.0761
2025-02-12 23:20:55.663396: Training Step 50/59: batchLoss = 2.8683, diffLoss = 13.9971, kgLoss = 0.0861
2025-02-12 23:20:56.587795: Training Step 51/59: batchLoss = 3.3657, diffLoss = 16.4296, kgLoss = 0.0998
2025-02-12 23:20:57.513796: Training Step 52/59: batchLoss = 3.2654, diffLoss = 15.9401, kgLoss = 0.0967
2025-02-12 23:20:58.451973: Training Step 53/59: batchLoss = 2.8301, diffLoss = 13.8166, kgLoss = 0.0835
2025-02-12 23:20:59.400596: Training Step 54/59: batchLoss = 2.8902, diffLoss = 14.1557, kgLoss = 0.0738
2025-02-12 23:21:00.335662: Training Step 55/59: batchLoss = 3.1048, diffLoss = 15.1576, kgLoss = 0.0916
2025-02-12 23:21:01.268399: Training Step 56/59: batchLoss = 3.1408, diffLoss = 15.3478, kgLoss = 0.0890
2025-02-12 23:21:02.117487: Training Step 57/59: batchLoss = 2.6299, diffLoss = 12.8421, kgLoss = 0.0769
2025-02-12 23:21:02.971697: Training Step 58/59: batchLoss = 2.8412, diffLoss = 13.8863, kgLoss = 0.0800
2025-02-12 23:21:03.071756: 
2025-02-12 23:21:03.072122: Epoch 38/1000, Train: epLoss = 0.4311, epDfLoss = 2.1050, epKgLoss = 0.0126  
2025-02-12 23:21:04.551739: Steps 0/47: batch_recall = 36.45, batch_ndcg = 46.38 
2025-02-12 23:21:05.854496: Steps 1/47: batch_recall = 36.57, batch_ndcg = 41.42 
2025-02-12 23:21:07.124649: Steps 2/47: batch_recall = 40.00, batch_ndcg = 46.06 
2025-02-12 23:21:08.379004: Steps 3/47: batch_recall = 43.63, batch_ndcg = 44.78 
2025-02-12 23:21:09.571716: Steps 4/47: batch_recall = 39.15, batch_ndcg = 45.19 
2025-02-12 23:21:10.778573: Steps 5/47: batch_recall = 32.65, batch_ndcg = 39.11 
2025-02-12 23:21:11.979694: Steps 6/47: batch_recall = 40.01, batch_ndcg = 42.62 
2025-02-12 23:21:13.147984: Steps 7/47: batch_recall = 43.97, batch_ndcg = 44.56 
2025-02-12 23:21:14.326114: Steps 8/47: batch_recall = 47.82, batch_ndcg = 51.18 
2025-02-12 23:21:15.480943: Steps 9/47: batch_recall = 45.95, batch_ndcg = 45.93 
2025-02-12 23:21:16.647872: Steps 10/47: batch_recall = 44.39, batch_ndcg = 43.43 
2025-02-12 23:21:17.799648: Steps 11/47: batch_recall = 52.82, batch_ndcg = 50.87 
2025-02-12 23:21:18.960431: Steps 12/47: batch_recall = 48.46, batch_ndcg = 46.91 
2025-02-12 23:21:20.110204: Steps 13/47: batch_recall = 48.17, batch_ndcg = 44.98 
2025-02-12 23:21:21.207927: Steps 14/47: batch_recall = 40.22, batch_ndcg = 40.15 
2025-02-12 23:21:22.313222: Steps 15/47: batch_recall = 55.88, batch_ndcg = 52.17 
2025-02-12 23:21:23.398821: Steps 16/47: batch_recall = 51.39, batch_ndcg = 46.66 
2025-02-12 23:21:24.453211: Steps 17/47: batch_recall = 57.20, batch_ndcg = 48.94 
2025-02-12 23:21:25.531687: Steps 18/47: batch_recall = 52.02, batch_ndcg = 48.57 
2025-02-12 23:21:26.593911: Steps 19/47: batch_recall = 59.57, batch_ndcg = 54.30 
2025-02-12 23:21:27.634363: Steps 20/47: batch_recall = 67.21, batch_ndcg = 59.90 
2025-02-12 23:21:28.674403: Steps 21/47: batch_recall = 65.25, batch_ndcg = 55.84 
2025-02-12 23:21:29.727274: Steps 22/47: batch_recall = 56.09, batch_ndcg = 51.62 
2025-02-12 23:21:30.771635: Steps 23/47: batch_recall = 63.69, batch_ndcg = 54.08 
2025-02-12 23:21:31.840357: Steps 24/47: batch_recall = 64.70, batch_ndcg = 57.04 
2025-02-12 23:21:32.890626: Steps 25/47: batch_recall = 65.34, batch_ndcg = 56.62 
2025-02-12 23:21:33.916432: Steps 26/47: batch_recall = 57.22, batch_ndcg = 52.11 
2025-02-12 23:21:34.964045: Steps 27/47: batch_recall = 63.25, batch_ndcg = 52.63 
2025-02-12 23:21:35.994009: Steps 28/47: batch_recall = 70.24, batch_ndcg = 58.89 
2025-02-12 23:21:37.009319: Steps 29/47: batch_recall = 68.66, batch_ndcg = 56.58 
2025-02-12 23:21:38.027702: Steps 30/47: batch_recall = 72.60, batch_ndcg = 64.17 
2025-02-12 23:21:39.058616: Steps 31/47: batch_recall = 65.80, batch_ndcg = 54.61 
2025-02-12 23:21:40.067855: Steps 32/47: batch_recall = 71.13, batch_ndcg = 64.27 
2025-02-12 23:21:41.092076: Steps 33/47: batch_recall = 77.75, batch_ndcg = 66.68 
2025-02-12 23:21:42.097815: Steps 34/47: batch_recall = 67.79, batch_ndcg = 54.64 
2025-02-12 23:21:43.079280: Steps 35/47: batch_recall = 77.57, batch_ndcg = 63.84 
2025-02-12 23:21:44.069510: Steps 36/47: batch_recall = 77.14, batch_ndcg = 63.84 
2025-02-12 23:21:45.041054: Steps 37/47: batch_recall = 82.02, batch_ndcg = 71.38 
2025-02-12 23:21:46.010496: Steps 38/47: batch_recall = 89.12, batch_ndcg = 71.02 
2025-02-12 23:21:46.980610: Steps 39/47: batch_recall = 86.87, batch_ndcg = 67.57 
2025-02-12 23:21:47.946339: Steps 40/47: batch_recall = 71.41, batch_ndcg = 61.28 
2025-02-12 23:21:48.908657: Steps 41/47: batch_recall = 87.86, batch_ndcg = 71.27 
2025-02-12 23:21:49.871913: Steps 42/47: batch_recall = 80.22, batch_ndcg = 63.34 
2025-02-12 23:21:50.842292: Steps 43/47: batch_recall = 88.17, batch_ndcg = 70.33 
2025-02-12 23:21:51.809746: Steps 44/47: batch_recall = 86.43, batch_ndcg = 68.67 
2025-02-12 23:21:52.758908: Steps 45/47: batch_recall = 94.28, batch_ndcg = 74.34 
2025-02-12 23:21:52.866820: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.54 
2025-02-12 23:21:52.866962: Epoch 38/1000, Test: Recall = 0.1203, NDCG = 0.1075  

2025-02-12 23:21:54.076228: Training Step 0/59: batchLoss = 2.8541, diffLoss = 13.9276, kgLoss = 0.0858
2025-02-12 23:21:55.012503: Training Step 1/59: batchLoss = 2.6513, diffLoss = 12.9595, kgLoss = 0.0742
2025-02-12 23:21:55.949401: Training Step 2/59: batchLoss = 2.9062, diffLoss = 14.1792, kgLoss = 0.0879
2025-02-12 23:21:56.881720: Training Step 3/59: batchLoss = 3.2286, diffLoss = 15.7660, kgLoss = 0.0943
2025-02-12 23:21:57.819764: Training Step 4/59: batchLoss = 2.6735, diffLoss = 13.0339, kgLoss = 0.0834
2025-02-12 23:21:58.745676: Training Step 5/59: batchLoss = 2.9689, diffLoss = 14.4803, kgLoss = 0.0911
2025-02-12 23:21:59.668642: Training Step 6/59: batchLoss = 2.7275, diffLoss = 13.3159, kgLoss = 0.0804
2025-02-12 23:22:00.595916: Training Step 7/59: batchLoss = 2.6806, diffLoss = 13.0983, kgLoss = 0.0762
2025-02-12 23:22:01.519878: Training Step 8/59: batchLoss = 2.9825, diffLoss = 14.5355, kgLoss = 0.0943
2025-02-12 23:22:02.456581: Training Step 9/59: batchLoss = 2.7571, diffLoss = 13.4691, kgLoss = 0.0791
2025-02-12 23:22:03.374945: Training Step 10/59: batchLoss = 3.0253, diffLoss = 14.7890, kgLoss = 0.0844
2025-02-12 23:22:04.294259: Training Step 11/59: batchLoss = 2.8352, diffLoss = 13.8300, kgLoss = 0.0865
2025-02-12 23:22:05.222905: Training Step 12/59: batchLoss = 2.7583, diffLoss = 13.4665, kgLoss = 0.0813
2025-02-12 23:22:06.153116: Training Step 13/59: batchLoss = 2.7298, diffLoss = 13.3076, kgLoss = 0.0853
2025-02-12 23:22:07.094537: Training Step 14/59: batchLoss = 3.0475, diffLoss = 14.8385, kgLoss = 0.0997
2025-02-12 23:22:08.029718: Training Step 15/59: batchLoss = 2.7514, diffLoss = 13.4366, kgLoss = 0.0801
2025-02-12 23:22:08.961980: Training Step 16/59: batchLoss = 2.9177, diffLoss = 14.2409, kgLoss = 0.0869
2025-02-12 23:22:09.932798: Training Step 17/59: batchLoss = 2.9313, diffLoss = 14.3382, kgLoss = 0.0795
2025-02-12 23:22:10.864457: Training Step 18/59: batchLoss = 2.7674, diffLoss = 13.5158, kgLoss = 0.0803
2025-02-12 23:22:11.800047: Training Step 19/59: batchLoss = 2.7828, diffLoss = 13.5929, kgLoss = 0.0803
2025-02-12 23:22:12.737035: Training Step 20/59: batchLoss = 2.9741, diffLoss = 14.5165, kgLoss = 0.0886
2025-02-12 23:22:13.677863: Training Step 21/59: batchLoss = 2.9987, diffLoss = 14.5964, kgLoss = 0.0993
2025-02-12 23:22:14.620788: Training Step 22/59: batchLoss = 2.5779, diffLoss = 12.5880, kgLoss = 0.0754
2025-02-12 23:22:15.551130: Training Step 23/59: batchLoss = 3.1296, diffLoss = 15.2858, kgLoss = 0.0905
2025-02-12 23:22:16.483627: Training Step 24/59: batchLoss = 2.9199, diffLoss = 14.2355, kgLoss = 0.0910
2025-02-12 23:22:17.409187: Training Step 25/59: batchLoss = 2.7561, diffLoss = 13.4736, kgLoss = 0.0767
2025-02-12 23:22:18.323677: Training Step 26/59: batchLoss = 2.9557, diffLoss = 14.4260, kgLoss = 0.0881
2025-02-12 23:22:19.253522: Training Step 27/59: batchLoss = 2.7766, diffLoss = 13.5748, kgLoss = 0.0771
2025-02-12 23:22:20.175707: Training Step 28/59: batchLoss = 2.8333, diffLoss = 13.8171, kgLoss = 0.0874
2025-02-12 23:22:21.108167: Training Step 29/59: batchLoss = 2.9980, diffLoss = 14.6453, kgLoss = 0.0862
2025-02-12 23:22:22.031325: Training Step 30/59: batchLoss = 2.8468, diffLoss = 13.9016, kgLoss = 0.0832
2025-02-12 23:22:22.958644: Training Step 31/59: batchLoss = 3.1413, diffLoss = 15.3571, kgLoss = 0.0874
2025-02-12 23:22:23.888398: Training Step 32/59: batchLoss = 2.8425, diffLoss = 13.8493, kgLoss = 0.0909
2025-02-12 23:22:24.818778: Training Step 33/59: batchLoss = 2.8381, diffLoss = 13.8808, kgLoss = 0.0775
2025-02-12 23:22:25.752886: Training Step 34/59: batchLoss = 2.9855, diffLoss = 14.5936, kgLoss = 0.0835
2025-02-12 23:22:26.701565: Training Step 35/59: batchLoss = 2.8251, diffLoss = 13.7980, kgLoss = 0.0819
2025-02-12 23:22:27.636891: Training Step 36/59: batchLoss = 2.9368, diffLoss = 14.3692, kgLoss = 0.0786
2025-02-12 23:22:28.572562: Training Step 37/59: batchLoss = 3.0478, diffLoss = 14.8798, kgLoss = 0.0898
2025-02-12 23:22:29.504622: Training Step 38/59: batchLoss = 3.4114, diffLoss = 16.6630, kgLoss = 0.0985
2025-02-12 23:22:30.436040: Training Step 39/59: batchLoss = 3.2591, diffLoss = 15.9528, kgLoss = 0.0857
2025-02-12 23:22:31.372053: Training Step 40/59: batchLoss = 2.6323, diffLoss = 12.8580, kgLoss = 0.0759
2025-02-12 23:22:32.308408: Training Step 41/59: batchLoss = 2.9841, diffLoss = 14.5560, kgLoss = 0.0911
2025-02-12 23:22:33.236696: Training Step 42/59: batchLoss = 2.8568, diffLoss = 13.9580, kgLoss = 0.0815
2025-02-12 23:22:34.170518: Training Step 43/59: batchLoss = 3.1496, diffLoss = 15.4108, kgLoss = 0.0844
2025-02-12 23:22:35.092320: Training Step 44/59: batchLoss = 3.1383, diffLoss = 15.3512, kgLoss = 0.0851
2025-02-12 23:22:36.015056: Training Step 45/59: batchLoss = 3.0904, diffLoss = 15.0985, kgLoss = 0.0884
2025-02-12 23:22:36.941631: Training Step 46/59: batchLoss = 3.2446, diffLoss = 15.8598, kgLoss = 0.0907
2025-02-12 23:22:37.870495: Training Step 47/59: batchLoss = 2.8728, diffLoss = 14.0265, kgLoss = 0.0843
2025-02-12 23:22:38.800346: Training Step 48/59: batchLoss = 2.7127, diffLoss = 13.2310, kgLoss = 0.0831
2025-02-12 23:22:39.720804: Training Step 49/59: batchLoss = 2.6397, diffLoss = 12.8993, kgLoss = 0.0748
2025-02-12 23:22:40.644103: Training Step 50/59: batchLoss = 3.2261, diffLoss = 15.7489, kgLoss = 0.0955
2025-02-12 23:22:41.583636: Training Step 51/59: batchLoss = 3.1796, diffLoss = 15.5471, kgLoss = 0.0877
2025-02-12 23:22:42.521694: Training Step 52/59: batchLoss = 3.0370, diffLoss = 14.8289, kgLoss = 0.0891
2025-02-12 23:22:43.457757: Training Step 53/59: batchLoss = 3.1683, diffLoss = 15.4947, kgLoss = 0.0868
2025-02-12 23:22:44.399620: Training Step 54/59: batchLoss = 3.1551, diffLoss = 15.4155, kgLoss = 0.0900
2025-02-12 23:22:45.338256: Training Step 55/59: batchLoss = 3.0412, diffLoss = 14.8725, kgLoss = 0.0834
2025-02-12 23:22:46.271475: Training Step 56/59: batchLoss = 3.2990, diffLoss = 16.1065, kgLoss = 0.0972
2025-02-12 23:22:47.124235: Training Step 57/59: batchLoss = 3.1099, diffLoss = 15.1915, kgLoss = 0.0895
2025-02-12 23:22:47.979991: Training Step 58/59: batchLoss = 2.9726, diffLoss = 14.4908, kgLoss = 0.0931
2025-02-12 23:22:48.077616: 
2025-02-12 23:22:48.077941: Epoch 39/1000, Train: epLoss = 0.4339, epDfLoss = 2.1187, epKgLoss = 0.0126  
2025-02-12 23:22:49.568278: Steps 0/47: batch_recall = 36.99, batch_ndcg = 47.11 
2025-02-12 23:22:50.869659: Steps 1/47: batch_recall = 36.31, batch_ndcg = 41.30 
2025-02-12 23:22:52.124098: Steps 2/47: batch_recall = 41.10, batch_ndcg = 46.40 
2025-02-12 23:22:53.381986: Steps 3/47: batch_recall = 44.03, batch_ndcg = 45.00 
2025-02-12 23:22:54.574233: Steps 4/47: batch_recall = 39.00, batch_ndcg = 44.84 
2025-02-12 23:22:55.781775: Steps 5/47: batch_recall = 33.02, batch_ndcg = 39.43 
2025-02-12 23:22:56.976074: Steps 6/47: batch_recall = 39.67, batch_ndcg = 42.33 
2025-02-12 23:22:58.139472: Steps 7/47: batch_recall = 43.89, batch_ndcg = 44.84 
2025-02-12 23:22:59.319972: Steps 8/47: batch_recall = 47.80, batch_ndcg = 50.96 
2025-02-12 23:23:00.464203: Steps 9/47: batch_recall = 46.48, batch_ndcg = 46.30 
2025-02-12 23:23:01.639821: Steps 10/47: batch_recall = 44.51, batch_ndcg = 43.55 
2025-02-12 23:23:02.791155: Steps 11/47: batch_recall = 54.16, batch_ndcg = 51.60 
2025-02-12 23:23:03.939360: Steps 12/47: batch_recall = 48.65, batch_ndcg = 47.62 
2025-02-12 23:23:05.081144: Steps 13/47: batch_recall = 47.96, batch_ndcg = 45.09 
2025-02-12 23:23:06.173331: Steps 14/47: batch_recall = 41.31, batch_ndcg = 40.68 
2025-02-12 23:23:07.268342: Steps 15/47: batch_recall = 56.08, batch_ndcg = 53.70 
2025-02-12 23:23:08.343747: Steps 16/47: batch_recall = 51.31, batch_ndcg = 46.81 
2025-02-12 23:23:09.396318: Steps 17/47: batch_recall = 57.10, batch_ndcg = 48.90 
2025-02-12 23:23:10.469241: Steps 18/47: batch_recall = 51.84, batch_ndcg = 48.58 
2025-02-12 23:23:11.536516: Steps 19/47: batch_recall = 59.90, batch_ndcg = 54.85 
2025-02-12 23:23:12.578233: Steps 20/47: batch_recall = 67.97, batch_ndcg = 61.33 
2025-02-12 23:23:13.623409: Steps 21/47: batch_recall = 66.24, batch_ndcg = 56.38 
2025-02-12 23:23:14.664289: Steps 22/47: batch_recall = 55.32, batch_ndcg = 51.35 
2025-02-12 23:23:15.716432: Steps 23/47: batch_recall = 61.55, batch_ndcg = 52.94 
2025-02-12 23:23:16.780359: Steps 24/47: batch_recall = 66.18, batch_ndcg = 57.48 
2025-02-12 23:23:17.827665: Steps 25/47: batch_recall = 65.12, batch_ndcg = 56.75 
2025-02-12 23:23:18.857876: Steps 26/47: batch_recall = 60.15, batch_ndcg = 52.33 
2025-02-12 23:23:19.904472: Steps 27/47: batch_recall = 61.56, batch_ndcg = 51.89 
2025-02-12 23:23:20.931240: Steps 28/47: batch_recall = 69.27, batch_ndcg = 58.50 
2025-02-12 23:23:21.970417: Steps 29/47: batch_recall = 70.66, batch_ndcg = 57.83 
2025-02-12 23:23:22.998098: Steps 30/47: batch_recall = 73.23, batch_ndcg = 63.58 
2025-02-12 23:23:24.036478: Steps 31/47: batch_recall = 67.09, batch_ndcg = 55.34 
2025-02-12 23:23:25.058481: Steps 32/47: batch_recall = 71.17, batch_ndcg = 64.43 
2025-02-12 23:23:26.069284: Steps 33/47: batch_recall = 78.82, batch_ndcg = 66.87 
2025-02-12 23:23:27.074583: Steps 34/47: batch_recall = 69.20, batch_ndcg = 55.03 
2025-02-12 23:23:28.050287: Steps 35/47: batch_recall = 76.61, batch_ndcg = 63.88 
2025-02-12 23:23:29.029749: Steps 36/47: batch_recall = 76.55, batch_ndcg = 63.27 
2025-02-12 23:23:29.989347: Steps 37/47: batch_recall = 83.64, batch_ndcg = 71.45 
2025-02-12 23:23:30.957442: Steps 38/47: batch_recall = 91.58, batch_ndcg = 71.93 
2025-02-12 23:23:31.919178: Steps 39/47: batch_recall = 87.31, batch_ndcg = 68.20 
2025-02-12 23:23:32.884954: Steps 40/47: batch_recall = 69.93, batch_ndcg = 61.45 
2025-02-12 23:23:33.856851: Steps 41/47: batch_recall = 85.82, batch_ndcg = 70.39 
2025-02-12 23:23:34.836237: Steps 42/47: batch_recall = 82.12, batch_ndcg = 63.85 
2025-02-12 23:23:35.811293: Steps 43/47: batch_recall = 90.29, batch_ndcg = 71.51 
2025-02-12 23:23:36.777441: Steps 44/47: batch_recall = 87.76, batch_ndcg = 69.46 
2025-02-12 23:23:37.724877: Steps 45/47: batch_recall = 92.70, batch_ndcg = 73.34 
2025-02-12 23:23:37.832571: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.57 
2025-02-12 23:23:37.832705: Epoch 39/1000, Test: Recall = 0.1210, NDCG = 0.1079  

2025-02-12 23:23:39.031399: Training Step 0/59: batchLoss = 2.7044, diffLoss = 13.1690, kgLoss = 0.0882
2025-02-12 23:23:39.968945: Training Step 1/59: batchLoss = 2.6842, diffLoss = 13.0621, kgLoss = 0.0897
2025-02-12 23:23:40.900851: Training Step 2/59: batchLoss = 2.5418, diffLoss = 12.4080, kgLoss = 0.0752
2025-02-12 23:23:41.832532: Training Step 3/59: batchLoss = 2.8334, diffLoss = 13.7924, kgLoss = 0.0937
2025-02-12 23:23:42.763938: Training Step 4/59: batchLoss = 2.9474, diffLoss = 14.3619, kgLoss = 0.0937
2025-02-12 23:23:43.695245: Training Step 5/59: batchLoss = 2.7542, diffLoss = 13.4339, kgLoss = 0.0843
2025-02-12 23:23:44.622081: Training Step 6/59: batchLoss = 2.7560, diffLoss = 13.4825, kgLoss = 0.0744
2025-02-12 23:23:45.545370: Training Step 7/59: batchLoss = 3.0461, diffLoss = 14.8567, kgLoss = 0.0935
2025-02-12 23:23:46.467005: Training Step 8/59: batchLoss = 2.8054, diffLoss = 13.6871, kgLoss = 0.0850
2025-02-12 23:23:47.387592: Training Step 9/59: batchLoss = 2.4594, diffLoss = 12.0036, kgLoss = 0.0733
2025-02-12 23:23:48.312825: Training Step 10/59: batchLoss = 3.0715, diffLoss = 14.9806, kgLoss = 0.0943
2025-02-12 23:23:49.236453: Training Step 11/59: batchLoss = 2.7660, diffLoss = 13.5175, kgLoss = 0.0781
2025-02-12 23:23:50.165468: Training Step 12/59: batchLoss = 2.7225, diffLoss = 13.2930, kgLoss = 0.0798
2025-02-12 23:23:51.098223: Training Step 13/59: batchLoss = 3.1261, diffLoss = 15.2454, kgLoss = 0.0962
2025-02-12 23:23:52.034079: Training Step 14/59: batchLoss = 3.1173, diffLoss = 15.2100, kgLoss = 0.0941
2025-02-12 23:23:52.972381: Training Step 15/59: batchLoss = 2.7935, diffLoss = 13.6458, kgLoss = 0.0804
2025-02-12 23:23:53.904332: Training Step 16/59: batchLoss = 2.9640, diffLoss = 14.4596, kgLoss = 0.0901
2025-02-12 23:23:54.837306: Training Step 17/59: batchLoss = 2.6748, diffLoss = 13.0565, kgLoss = 0.0794
2025-02-12 23:23:55.769007: Training Step 18/59: batchLoss = 2.6800, diffLoss = 13.0769, kgLoss = 0.0808
2025-02-12 23:23:56.703379: Training Step 19/59: batchLoss = 2.9317, diffLoss = 14.3138, kgLoss = 0.0862
2025-02-12 23:23:57.633532: Training Step 20/59: batchLoss = 3.2282, diffLoss = 15.7651, kgLoss = 0.0940
2025-02-12 23:23:58.563125: Training Step 21/59: batchLoss = 2.6423, diffLoss = 12.8860, kgLoss = 0.0813
2025-02-12 23:23:59.493512: Training Step 22/59: batchLoss = 3.0498, diffLoss = 14.8977, kgLoss = 0.0879
2025-02-12 23:24:00.421577: Training Step 23/59: batchLoss = 3.0292, diffLoss = 14.8075, kgLoss = 0.0846
2025-02-12 23:24:01.349315: Training Step 24/59: batchLoss = 2.8143, diffLoss = 13.7560, kgLoss = 0.0789
2025-02-12 23:24:02.276811: Training Step 25/59: batchLoss = 3.0523, diffLoss = 14.9266, kgLoss = 0.0837
2025-02-12 23:24:03.200848: Training Step 26/59: batchLoss = 3.2661, diffLoss = 15.9071, kgLoss = 0.1058
2025-02-12 23:24:04.132629: Training Step 27/59: batchLoss = 3.0174, diffLoss = 14.7521, kgLoss = 0.0837
2025-02-12 23:24:05.056457: Training Step 28/59: batchLoss = 3.2010, diffLoss = 15.6338, kgLoss = 0.0927
2025-02-12 23:24:05.975130: Training Step 29/59: batchLoss = 2.7071, diffLoss = 13.2248, kgLoss = 0.0777
2025-02-12 23:24:06.903763: Training Step 30/59: batchLoss = 2.8597, diffLoss = 13.9733, kgLoss = 0.0813
2025-02-12 23:24:07.832935: Training Step 31/59: batchLoss = 2.5726, diffLoss = 12.5693, kgLoss = 0.0734
2025-02-12 23:24:08.767079: Training Step 32/59: batchLoss = 3.2537, diffLoss = 15.8953, kgLoss = 0.0933
2025-02-12 23:24:09.702180: Training Step 33/59: batchLoss = 3.0029, diffLoss = 14.6603, kgLoss = 0.0886
2025-02-12 23:24:10.630466: Training Step 34/59: batchLoss = 3.1191, diffLoss = 15.2304, kgLoss = 0.0913
2025-02-12 23:24:11.559951: Training Step 35/59: batchLoss = 2.7012, diffLoss = 13.2086, kgLoss = 0.0744
2025-02-12 23:24:12.491307: Training Step 36/59: batchLoss = 2.9186, diffLoss = 14.2557, kgLoss = 0.0844
2025-02-12 23:24:13.429634: Training Step 37/59: batchLoss = 2.5920, diffLoss = 12.6586, kgLoss = 0.0754
2025-02-12 23:24:14.362396: Training Step 38/59: batchLoss = 3.1364, diffLoss = 15.3383, kgLoss = 0.0859
2025-02-12 23:24:15.296178: Training Step 39/59: batchLoss = 2.7761, diffLoss = 13.5637, kgLoss = 0.0792
2025-02-12 23:24:16.234625: Training Step 40/59: batchLoss = 2.8459, diffLoss = 13.8663, kgLoss = 0.0908
2025-02-12 23:24:17.178017: Training Step 41/59: batchLoss = 2.7516, diffLoss = 13.4091, kgLoss = 0.0873
2025-02-12 23:24:18.113268: Training Step 42/59: batchLoss = 2.8942, diffLoss = 14.1633, kgLoss = 0.0769
2025-02-12 23:24:19.039920: Training Step 43/59: batchLoss = 2.8928, diffLoss = 14.1432, kgLoss = 0.0802
2025-02-12 23:24:19.965657: Training Step 44/59: batchLoss = 2.6305, diffLoss = 12.8532, kgLoss = 0.0749
2025-02-12 23:24:20.890128: Training Step 45/59: batchLoss = 3.1498, diffLoss = 15.3824, kgLoss = 0.0916
2025-02-12 23:24:21.818334: Training Step 46/59: batchLoss = 3.2424, diffLoss = 15.7939, kgLoss = 0.1045
2025-02-12 23:24:22.754504: Training Step 47/59: batchLoss = 3.1921, diffLoss = 15.6030, kgLoss = 0.0894
2025-02-12 23:24:23.683279: Training Step 48/59: batchLoss = 2.7022, diffLoss = 13.2064, kgLoss = 0.0761
2025-02-12 23:24:24.616819: Training Step 49/59: batchLoss = 3.2331, diffLoss = 15.8146, kgLoss = 0.0877
2025-02-12 23:24:25.562955: Training Step 50/59: batchLoss = 3.2051, diffLoss = 15.6620, kgLoss = 0.0909
2025-02-12 23:24:26.493171: Training Step 51/59: batchLoss = 2.8319, diffLoss = 13.7998, kgLoss = 0.0899
2025-02-12 23:24:27.424690: Training Step 52/59: batchLoss = 3.1282, diffLoss = 15.2809, kgLoss = 0.0900
2025-02-12 23:24:28.366793: Training Step 53/59: batchLoss = 2.9479, diffLoss = 14.4091, kgLoss = 0.0827
2025-02-12 23:24:29.301240: Training Step 54/59: batchLoss = 3.0883, diffLoss = 15.1148, kgLoss = 0.0817
2025-02-12 23:24:30.245703: Training Step 55/59: batchLoss = 2.9235, diffLoss = 14.2961, kgLoss = 0.0803
2025-02-12 23:24:31.173034: Training Step 56/59: batchLoss = 2.7288, diffLoss = 13.3186, kgLoss = 0.0814
2025-02-12 23:24:32.019312: Training Step 57/59: batchLoss = 3.2621, diffLoss = 15.9361, kgLoss = 0.0936
2025-02-12 23:24:32.873791: Training Step 58/59: batchLoss = 3.3549, diffLoss = 16.3670, kgLoss = 0.1019
2025-02-12 23:24:32.967804: 
2025-02-12 23:24:32.968342: Epoch 40/1000, Train: epLoss = 0.4308, epDfLoss = 2.1035, epKgLoss = 0.0126  
2025-02-12 23:24:34.465461: Steps 0/47: batch_recall = 37.28, batch_ndcg = 46.90 
2025-02-12 23:24:35.760283: Steps 1/47: batch_recall = 37.08, batch_ndcg = 42.90 
2025-02-12 23:24:37.015649: Steps 2/47: batch_recall = 41.10, batch_ndcg = 46.46 
2025-02-12 23:24:38.278269: Steps 3/47: batch_recall = 43.74, batch_ndcg = 45.22 
2025-02-12 23:24:39.471441: Steps 4/47: batch_recall = 38.51, batch_ndcg = 44.80 
2025-02-12 23:24:40.699241: Steps 5/47: batch_recall = 33.21, batch_ndcg = 39.02 
2025-02-12 23:24:41.894371: Steps 6/47: batch_recall = 39.25, batch_ndcg = 42.18 
2025-02-12 23:24:43.076804: Steps 7/47: batch_recall = 44.56, batch_ndcg = 45.36 
2025-02-12 23:24:44.264663: Steps 8/47: batch_recall = 48.67, batch_ndcg = 51.81 
2025-02-12 23:24:45.416485: Steps 9/47: batch_recall = 46.08, batch_ndcg = 46.59 
2025-02-12 23:24:46.601563: Steps 10/47: batch_recall = 43.76, batch_ndcg = 42.89 
2025-02-12 23:24:47.757382: Steps 11/47: batch_recall = 54.97, batch_ndcg = 52.11 
2025-02-12 23:24:48.914656: Steps 12/47: batch_recall = 49.57, batch_ndcg = 48.07 
2025-02-12 23:24:50.057188: Steps 13/47: batch_recall = 48.30, batch_ndcg = 44.71 
2025-02-12 23:24:51.163806: Steps 14/47: batch_recall = 40.61, batch_ndcg = 40.79 
2025-02-12 23:24:52.272937: Steps 15/47: batch_recall = 56.75, batch_ndcg = 53.39 
2025-02-12 23:24:53.354683: Steps 16/47: batch_recall = 50.72, batch_ndcg = 46.30 
2025-02-12 23:24:54.398268: Steps 17/47: batch_recall = 57.12, batch_ndcg = 48.42 
2025-02-12 23:24:55.480176: Steps 18/47: batch_recall = 53.13, batch_ndcg = 48.84 
2025-02-12 23:24:56.548858: Steps 19/47: batch_recall = 59.69, batch_ndcg = 54.80 
2025-02-12 23:24:57.594706: Steps 20/47: batch_recall = 67.37, batch_ndcg = 61.78 
2025-02-12 23:24:58.635786: Steps 21/47: batch_recall = 64.40, batch_ndcg = 54.86 
2025-02-12 23:24:59.679708: Steps 22/47: batch_recall = 53.73, batch_ndcg = 50.89 
2025-02-12 23:25:00.736111: Steps 23/47: batch_recall = 62.18, batch_ndcg = 53.73 
2025-02-12 23:25:01.816313: Steps 24/47: batch_recall = 66.44, batch_ndcg = 57.64 
2025-02-12 23:25:02.870312: Steps 25/47: batch_recall = 65.41, batch_ndcg = 56.87 
2025-02-12 23:25:03.898581: Steps 26/47: batch_recall = 59.88, batch_ndcg = 52.18 
2025-02-12 23:25:04.948636: Steps 27/47: batch_recall = 61.88, batch_ndcg = 52.41 
2025-02-12 23:25:05.980854: Steps 28/47: batch_recall = 69.20, batch_ndcg = 58.81 
2025-02-12 23:25:07.004687: Steps 29/47: batch_recall = 70.26, batch_ndcg = 57.79 
2025-02-12 23:25:08.019250: Steps 30/47: batch_recall = 73.55, batch_ndcg = 63.95 
2025-02-12 23:25:09.050875: Steps 31/47: batch_recall = 66.71, batch_ndcg = 55.02 
2025-02-12 23:25:10.059955: Steps 32/47: batch_recall = 71.23, batch_ndcg = 64.43 
2025-02-12 23:25:11.069893: Steps 33/47: batch_recall = 79.33, batch_ndcg = 67.53 
2025-02-12 23:25:12.082861: Steps 34/47: batch_recall = 69.74, batch_ndcg = 56.26 
2025-02-12 23:25:13.067383: Steps 35/47: batch_recall = 76.56, batch_ndcg = 63.33 
2025-02-12 23:25:14.050576: Steps 36/47: batch_recall = 77.69, batch_ndcg = 63.40 
2025-02-12 23:25:15.028692: Steps 37/47: batch_recall = 82.45, batch_ndcg = 70.81 
2025-02-12 23:25:15.998261: Steps 38/47: batch_recall = 90.72, batch_ndcg = 71.95 
2025-02-12 23:25:16.975249: Steps 39/47: batch_recall = 88.51, batch_ndcg = 68.45 
2025-02-12 23:25:17.951396: Steps 40/47: batch_recall = 70.95, batch_ndcg = 61.67 
2025-02-12 23:25:18.927735: Steps 41/47: batch_recall = 87.73, batch_ndcg = 71.49 
2025-02-12 23:25:19.895749: Steps 42/47: batch_recall = 82.60, batch_ndcg = 64.26 
2025-02-12 23:25:20.862787: Steps 43/47: batch_recall = 89.57, batch_ndcg = 71.24 
2025-02-12 23:25:21.830208: Steps 44/47: batch_recall = 86.80, batch_ndcg = 69.73 
2025-02-12 23:25:22.779551: Steps 45/47: batch_recall = 93.64, batch_ndcg = 73.84 
2025-02-12 23:25:22.884974: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.51 
2025-02-12 23:25:22.885115: Epoch 40/1000, Test: Recall = 0.1211, NDCG = 0.1081  

2025-02-12 23:25:24.087444: Training Step 0/59: batchLoss = 2.9113, diffLoss = 14.1821, kgLoss = 0.0937
2025-02-12 23:25:25.013035: Training Step 1/59: batchLoss = 2.8187, diffLoss = 13.7088, kgLoss = 0.0961
2025-02-12 23:25:25.952080: Training Step 2/59: batchLoss = 2.9545, diffLoss = 14.4089, kgLoss = 0.0909
2025-02-12 23:25:26.890687: Training Step 3/59: batchLoss = 2.7663, diffLoss = 13.4796, kgLoss = 0.0880
2025-02-12 23:25:27.828492: Training Step 4/59: batchLoss = 3.1789, diffLoss = 15.5345, kgLoss = 0.0900
2025-02-12 23:25:28.751495: Training Step 5/59: batchLoss = 2.6478, diffLoss = 12.9261, kgLoss = 0.0782
2025-02-12 23:25:29.680116: Training Step 6/59: batchLoss = 2.7909, diffLoss = 13.6154, kgLoss = 0.0848
2025-02-12 23:25:30.611770: Training Step 7/59: batchLoss = 2.8270, diffLoss = 13.8037, kgLoss = 0.0828
2025-02-12 23:25:31.529955: Training Step 8/59: batchLoss = 3.1916, diffLoss = 15.5708, kgLoss = 0.0968
2025-02-12 23:25:32.465884: Training Step 9/59: batchLoss = 2.9330, diffLoss = 14.2884, kgLoss = 0.0941
2025-02-12 23:25:33.382338: Training Step 10/59: batchLoss = 2.9582, diffLoss = 14.4603, kgLoss = 0.0827
2025-02-12 23:25:34.312456: Training Step 11/59: batchLoss = 2.9031, diffLoss = 14.1617, kgLoss = 0.0884
2025-02-12 23:25:35.246739: Training Step 12/59: batchLoss = 2.6146, diffLoss = 12.7760, kgLoss = 0.0742
2025-02-12 23:25:36.183247: Training Step 13/59: batchLoss = 2.6700, diffLoss = 12.9775, kgLoss = 0.0931
2025-02-12 23:25:37.121312: Training Step 14/59: batchLoss = 2.7833, diffLoss = 13.5942, kgLoss = 0.0806
2025-02-12 23:25:38.053026: Training Step 15/59: batchLoss = 2.5468, diffLoss = 12.4596, kgLoss = 0.0685
2025-02-12 23:25:38.992238: Training Step 16/59: batchLoss = 2.9858, diffLoss = 14.5831, kgLoss = 0.0865
2025-02-12 23:25:39.934357: Training Step 17/59: batchLoss = 2.6255, diffLoss = 12.8128, kgLoss = 0.0787
2025-02-12 23:25:40.864982: Training Step 18/59: batchLoss = 2.7640, diffLoss = 13.5252, kgLoss = 0.0737
2025-02-12 23:25:41.801495: Training Step 19/59: batchLoss = 2.8154, diffLoss = 13.7366, kgLoss = 0.0851
2025-02-12 23:25:42.736331: Training Step 20/59: batchLoss = 2.8253, diffLoss = 13.7833, kgLoss = 0.0858
2025-02-12 23:25:43.665272: Training Step 21/59: batchLoss = 2.7804, diffLoss = 13.5405, kgLoss = 0.0904
2025-02-12 23:25:44.596027: Training Step 22/59: batchLoss = 2.5869, diffLoss = 12.6348, kgLoss = 0.0749
2025-02-12 23:25:45.525506: Training Step 23/59: batchLoss = 3.0299, diffLoss = 14.7928, kgLoss = 0.0891
2025-02-12 23:25:46.455920: Training Step 24/59: batchLoss = 2.8040, diffLoss = 13.6943, kgLoss = 0.0814
2025-02-12 23:25:47.383956: Training Step 25/59: batchLoss = 2.8982, diffLoss = 14.1830, kgLoss = 0.0770
2025-02-12 23:25:48.316457: Training Step 26/59: batchLoss = 2.7410, diffLoss = 13.3671, kgLoss = 0.0844
2025-02-12 23:25:49.253232: Training Step 27/59: batchLoss = 2.9465, diffLoss = 14.3720, kgLoss = 0.0902
2025-02-12 23:25:50.180958: Training Step 28/59: batchLoss = 2.9482, diffLoss = 14.3782, kgLoss = 0.0908
2025-02-12 23:25:51.105773: Training Step 29/59: batchLoss = 3.1642, diffLoss = 15.4569, kgLoss = 0.0910
2025-02-12 23:25:52.049643: Training Step 30/59: batchLoss = 3.2878, diffLoss = 16.0671, kgLoss = 0.0929
2025-02-12 23:25:52.982667: Training Step 31/59: batchLoss = 2.9473, diffLoss = 14.3986, kgLoss = 0.0845
2025-02-12 23:25:53.929036: Training Step 32/59: batchLoss = 3.0433, diffLoss = 14.8773, kgLoss = 0.0847
2025-02-12 23:25:54.861698: Training Step 33/59: batchLoss = 2.7890, diffLoss = 13.6140, kgLoss = 0.0827
2025-02-12 23:25:55.806349: Training Step 34/59: batchLoss = 3.0028, diffLoss = 14.6791, kgLoss = 0.0837
2025-02-12 23:25:56.744159: Training Step 35/59: batchLoss = 3.2290, diffLoss = 15.7829, kgLoss = 0.0906
2025-02-12 23:25:57.685276: Training Step 36/59: batchLoss = 2.9246, diffLoss = 14.2975, kgLoss = 0.0814
2025-02-12 23:25:58.623407: Training Step 37/59: batchLoss = 3.0351, diffLoss = 14.8479, kgLoss = 0.0819
2025-02-12 23:25:59.564994: Training Step 38/59: batchLoss = 2.9893, diffLoss = 14.6074, kgLoss = 0.0847
2025-02-12 23:26:00.504730: Training Step 39/59: batchLoss = 3.0945, diffLoss = 15.1499, kgLoss = 0.0807
2025-02-12 23:26:01.437316: Training Step 40/59: batchLoss = 2.9803, diffLoss = 14.5374, kgLoss = 0.0911
2025-02-12 23:26:02.366768: Training Step 41/59: batchLoss = 2.9978, diffLoss = 14.6089, kgLoss = 0.0950
2025-02-12 23:26:03.300824: Training Step 42/59: batchLoss = 3.1689, diffLoss = 15.5007, kgLoss = 0.0859
2025-02-12 23:26:04.222761: Training Step 43/59: batchLoss = 3.1261, diffLoss = 15.2810, kgLoss = 0.0874
2025-02-12 23:26:05.159521: Training Step 44/59: batchLoss = 2.8991, diffLoss = 14.1742, kgLoss = 0.0804
2025-02-12 23:26:06.091966: Training Step 45/59: batchLoss = 2.9294, diffLoss = 14.3199, kgLoss = 0.0817
2025-02-12 23:26:07.017741: Training Step 46/59: batchLoss = 3.1144, diffLoss = 15.2238, kgLoss = 0.0870
2025-02-12 23:26:07.948556: Training Step 47/59: batchLoss = 3.0670, diffLoss = 15.0074, kgLoss = 0.0818
2025-02-12 23:26:08.880816: Training Step 48/59: batchLoss = 2.9538, diffLoss = 14.4391, kgLoss = 0.0825
2025-02-12 23:26:09.828110: Training Step 49/59: batchLoss = 2.9508, diffLoss = 14.4034, kgLoss = 0.0877
2025-02-12 23:26:10.760697: Training Step 50/59: batchLoss = 2.9393, diffLoss = 14.3917, kgLoss = 0.0762
2025-02-12 23:26:11.694118: Training Step 51/59: batchLoss = 2.6493, diffLoss = 12.9395, kgLoss = 0.0768
2025-02-12 23:26:12.632210: Training Step 52/59: batchLoss = 3.2110, diffLoss = 15.7006, kgLoss = 0.0886
2025-02-12 23:26:13.566619: Training Step 53/59: batchLoss = 3.3225, diffLoss = 16.2369, kgLoss = 0.0939
2025-02-12 23:26:14.501401: Training Step 54/59: batchLoss = 3.0129, diffLoss = 14.7031, kgLoss = 0.0904
2025-02-12 23:26:15.438554: Training Step 55/59: batchLoss = 3.1782, diffLoss = 15.5241, kgLoss = 0.0917
2025-02-12 23:26:16.377456: Training Step 56/59: batchLoss = 3.0049, diffLoss = 14.6797, kgLoss = 0.0862
2025-02-12 23:26:17.225124: Training Step 57/59: batchLoss = 3.0020, diffLoss = 14.6778, kgLoss = 0.0830
2025-02-12 23:26:18.078603: Training Step 58/59: batchLoss = 3.5535, diffLoss = 17.3634, kgLoss = 0.1010
2025-02-12 23:26:18.179081: 
2025-02-12 23:26:18.179422: Epoch 41/1000, Train: epLoss = 0.4345, epDfLoss = 2.1221, epKgLoss = 0.0126  
2025-02-12 23:26:19.647436: Steps 0/47: batch_recall = 37.20, batch_ndcg = 46.91 
2025-02-12 23:26:20.935135: Steps 1/47: batch_recall = 36.36, batch_ndcg = 41.92 
2025-02-12 23:26:22.187861: Steps 2/47: batch_recall = 41.73, batch_ndcg = 46.86 
2025-02-12 23:26:23.439772: Steps 3/47: batch_recall = 42.62, batch_ndcg = 44.71 
2025-02-12 23:26:24.631131: Steps 4/47: batch_recall = 38.84, batch_ndcg = 44.23 
2025-02-12 23:26:25.843101: Steps 5/47: batch_recall = 33.25, batch_ndcg = 39.77 
2025-02-12 23:26:27.038373: Steps 6/47: batch_recall = 39.68, batch_ndcg = 42.28 
2025-02-12 23:26:28.218095: Steps 7/47: batch_recall = 43.51, batch_ndcg = 44.45 
2025-02-12 23:26:29.408021: Steps 8/47: batch_recall = 48.64, batch_ndcg = 51.57 
2025-02-12 23:26:30.556427: Steps 9/47: batch_recall = 46.47, batch_ndcg = 46.13 
2025-02-12 23:26:31.734772: Steps 10/47: batch_recall = 44.22, batch_ndcg = 43.14 
2025-02-12 23:26:32.882277: Steps 11/47: batch_recall = 54.56, batch_ndcg = 51.90 
2025-02-12 23:26:34.038963: Steps 12/47: batch_recall = 49.11, batch_ndcg = 47.49 
2025-02-12 23:26:35.180231: Steps 13/47: batch_recall = 48.60, batch_ndcg = 45.10 
2025-02-12 23:26:36.270507: Steps 14/47: batch_recall = 40.35, batch_ndcg = 39.99 
2025-02-12 23:26:37.356427: Steps 15/47: batch_recall = 56.72, batch_ndcg = 53.19 
2025-02-12 23:26:38.445010: Steps 16/47: batch_recall = 50.75, batch_ndcg = 46.72 
2025-02-12 23:26:39.494331: Steps 17/47: batch_recall = 57.76, batch_ndcg = 49.43 
2025-02-12 23:26:40.578584: Steps 18/47: batch_recall = 52.79, batch_ndcg = 49.30 
2025-02-12 23:26:41.643925: Steps 19/47: batch_recall = 61.11, batch_ndcg = 55.01 
2025-02-12 23:26:42.682097: Steps 20/47: batch_recall = 68.02, batch_ndcg = 61.65 
2025-02-12 23:26:43.736259: Steps 21/47: batch_recall = 65.25, batch_ndcg = 55.33 
2025-02-12 23:26:44.793533: Steps 22/47: batch_recall = 53.50, batch_ndcg = 50.97 
2025-02-12 23:26:45.852554: Steps 23/47: batch_recall = 63.00, batch_ndcg = 54.42 
2025-02-12 23:26:46.920236: Steps 24/47: batch_recall = 62.24, batch_ndcg = 56.34 
2025-02-12 23:26:47.971665: Steps 25/47: batch_recall = 66.00, batch_ndcg = 57.11 
2025-02-12 23:26:48.987968: Steps 26/47: batch_recall = 61.31, batch_ndcg = 52.38 
2025-02-12 23:26:50.025579: Steps 27/47: batch_recall = 62.65, batch_ndcg = 52.84 
2025-02-12 23:26:51.054607: Steps 28/47: batch_recall = 69.44, batch_ndcg = 58.88 
2025-02-12 23:26:52.075718: Steps 29/47: batch_recall = 68.46, batch_ndcg = 56.65 
2025-02-12 23:26:53.091234: Steps 30/47: batch_recall = 73.19, batch_ndcg = 64.21 
2025-02-12 23:26:54.119213: Steps 31/47: batch_recall = 66.15, batch_ndcg = 55.36 
2025-02-12 23:26:55.137681: Steps 32/47: batch_recall = 71.92, batch_ndcg = 64.43 
2025-02-12 23:26:56.146377: Steps 33/47: batch_recall = 79.55, batch_ndcg = 67.17 
2025-02-12 23:26:57.155650: Steps 34/47: batch_recall = 69.15, batch_ndcg = 55.82 
2025-02-12 23:26:58.130307: Steps 35/47: batch_recall = 76.70, batch_ndcg = 64.12 
2025-02-12 23:26:59.108213: Steps 36/47: batch_recall = 78.75, batch_ndcg = 63.73 
2025-02-12 23:27:00.058163: Steps 37/47: batch_recall = 83.28, batch_ndcg = 71.86 
2025-02-12 23:27:01.022457: Steps 38/47: batch_recall = 90.51, batch_ndcg = 72.47 
2025-02-12 23:27:01.995420: Steps 39/47: batch_recall = 87.23, batch_ndcg = 68.08 
2025-02-12 23:27:02.958982: Steps 40/47: batch_recall = 70.65, batch_ndcg = 61.58 
2025-02-12 23:27:03.921212: Steps 41/47: batch_recall = 88.08, batch_ndcg = 71.15 
2025-02-12 23:27:04.884382: Steps 42/47: batch_recall = 80.82, batch_ndcg = 64.60 
2025-02-12 23:27:05.869685: Steps 43/47: batch_recall = 87.95, batch_ndcg = 70.64 
2025-02-12 23:27:06.832057: Steps 44/47: batch_recall = 88.79, batch_ndcg = 70.32 
2025-02-12 23:27:07.763403: Steps 45/47: batch_recall = 94.07, batch_ndcg = 73.73 
2025-02-12 23:27:07.867550: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.54 
2025-02-12 23:27:07.867878: Epoch 41/1000, Test: Recall = 0.1211, NDCG = 0.1081  

2025-02-12 23:27:09.055188: Training Step 0/59: batchLoss = 3.2767, diffLoss = 15.9728, kgLoss = 0.1027
2025-02-12 23:27:09.981916: Training Step 1/59: batchLoss = 2.6516, diffLoss = 12.9616, kgLoss = 0.0741
2025-02-12 23:27:10.910716: Training Step 2/59: batchLoss = 2.8518, diffLoss = 13.9303, kgLoss = 0.0822
2025-02-12 23:27:11.837525: Training Step 3/59: batchLoss = 2.7342, diffLoss = 13.3321, kgLoss = 0.0847
2025-02-12 23:27:12.760785: Training Step 4/59: batchLoss = 2.9364, diffLoss = 14.3346, kgLoss = 0.0869
2025-02-12 23:27:13.688637: Training Step 5/59: batchLoss = 3.0519, diffLoss = 14.8665, kgLoss = 0.0982
2025-02-12 23:27:14.607244: Training Step 6/59: batchLoss = 2.7618, diffLoss = 13.4650, kgLoss = 0.0860
2025-02-12 23:27:15.525874: Training Step 7/59: batchLoss = 2.8351, diffLoss = 13.8355, kgLoss = 0.0851
2025-02-12 23:27:16.457583: Training Step 8/59: batchLoss = 2.7559, diffLoss = 13.4498, kgLoss = 0.0825
2025-02-12 23:27:17.381570: Training Step 9/59: batchLoss = 2.6216, diffLoss = 12.7784, kgLoss = 0.0824
2025-02-12 23:27:18.308664: Training Step 10/59: batchLoss = 2.8130, diffLoss = 13.7351, kgLoss = 0.0825
2025-02-12 23:27:19.236618: Training Step 11/59: batchLoss = 2.9466, diffLoss = 14.3681, kgLoss = 0.0912
2025-02-12 23:27:20.168905: Training Step 12/59: batchLoss = 2.7101, diffLoss = 13.2267, kgLoss = 0.0809
2025-02-12 23:27:21.105967: Training Step 13/59: batchLoss = 2.6935, diffLoss = 13.1620, kgLoss = 0.0764
2025-02-12 23:27:22.045492: Training Step 14/59: batchLoss = 2.8244, diffLoss = 13.7840, kgLoss = 0.0845
2025-02-12 23:27:22.979335: Training Step 15/59: batchLoss = 3.1674, diffLoss = 15.4612, kgLoss = 0.0940
2025-02-12 23:27:23.912015: Training Step 16/59: batchLoss = 2.7192, diffLoss = 13.3031, kgLoss = 0.0733
2025-02-12 23:27:24.850683: Training Step 17/59: batchLoss = 2.6027, diffLoss = 12.6966, kgLoss = 0.0792
2025-02-12 23:27:25.789438: Training Step 18/59: batchLoss = 3.3124, diffLoss = 16.1729, kgLoss = 0.0973
2025-02-12 23:27:26.723757: Training Step 19/59: batchLoss = 2.9576, diffLoss = 14.4493, kgLoss = 0.0847
2025-02-12 23:27:27.660891: Training Step 20/59: batchLoss = 2.8098, diffLoss = 13.7332, kgLoss = 0.0790
2025-02-12 23:27:28.592741: Training Step 21/59: batchLoss = 2.7218, diffLoss = 13.2708, kgLoss = 0.0846
2025-02-12 23:27:29.525459: Training Step 22/59: batchLoss = 3.0463, diffLoss = 14.8390, kgLoss = 0.0981
2025-02-12 23:27:30.450042: Training Step 23/59: batchLoss = 2.9062, diffLoss = 14.1751, kgLoss = 0.0890
2025-02-12 23:27:31.374871: Training Step 24/59: batchLoss = 2.9483, diffLoss = 14.3982, kgLoss = 0.0858
2025-02-12 23:27:32.311395: Training Step 25/59: batchLoss = 2.8915, diffLoss = 14.0888, kgLoss = 0.0922
2025-02-12 23:27:33.234818: Training Step 26/59: batchLoss = 3.0621, diffLoss = 14.9549, kgLoss = 0.0890
2025-02-12 23:27:34.160035: Training Step 27/59: batchLoss = 2.6803, diffLoss = 13.0921, kgLoss = 0.0774
2025-02-12 23:27:35.093057: Training Step 28/59: batchLoss = 2.8353, diffLoss = 13.8461, kgLoss = 0.0826
2025-02-12 23:27:36.027151: Training Step 29/59: batchLoss = 2.9723, diffLoss = 14.5215, kgLoss = 0.0850
2025-02-12 23:27:36.956190: Training Step 30/59: batchLoss = 3.1750, diffLoss = 15.4969, kgLoss = 0.0945
2025-02-12 23:27:37.897271: Training Step 31/59: batchLoss = 3.3305, diffLoss = 16.2641, kgLoss = 0.0970
2025-02-12 23:27:38.827939: Training Step 32/59: batchLoss = 3.4592, diffLoss = 16.9226, kgLoss = 0.0934
2025-02-12 23:27:39.760699: Training Step 33/59: batchLoss = 2.7192, diffLoss = 13.3060, kgLoss = 0.0725
2025-02-12 23:27:40.709362: Training Step 34/59: batchLoss = 2.8195, diffLoss = 13.7752, kgLoss = 0.0805
2025-02-12 23:27:41.648083: Training Step 35/59: batchLoss = 3.0528, diffLoss = 14.9055, kgLoss = 0.0897
2025-02-12 23:27:42.580538: Training Step 36/59: batchLoss = 2.9627, diffLoss = 14.4771, kgLoss = 0.0841
2025-02-12 23:27:43.518713: Training Step 37/59: batchLoss = 3.1172, diffLoss = 15.2359, kgLoss = 0.0875
2025-02-12 23:27:44.456922: Training Step 38/59: batchLoss = 2.7967, diffLoss = 13.6601, kgLoss = 0.0809
2025-02-12 23:27:45.384678: Training Step 39/59: batchLoss = 2.9524, diffLoss = 14.4170, kgLoss = 0.0863
2025-02-12 23:27:46.314632: Training Step 40/59: batchLoss = 2.9956, diffLoss = 14.6421, kgLoss = 0.0839
2025-02-12 23:27:47.241113: Training Step 41/59: batchLoss = 2.8444, diffLoss = 13.8829, kgLoss = 0.0847
2025-02-12 23:27:48.179950: Training Step 42/59: batchLoss = 3.2403, diffLoss = 15.8580, kgLoss = 0.0859
2025-02-12 23:27:49.125205: Training Step 43/59: batchLoss = 2.9171, diffLoss = 14.2353, kgLoss = 0.0876
2025-02-12 23:27:50.049795: Training Step 44/59: batchLoss = 2.9996, diffLoss = 14.6575, kgLoss = 0.0852
2025-02-12 23:27:50.972579: Training Step 45/59: batchLoss = 2.6176, diffLoss = 12.7863, kgLoss = 0.0754
2025-02-12 23:27:51.898000: Training Step 46/59: batchLoss = 3.1076, diffLoss = 15.1747, kgLoss = 0.0908
2025-02-12 23:27:52.835530: Training Step 47/59: batchLoss = 3.2341, diffLoss = 15.7997, kgLoss = 0.0927
2025-02-12 23:27:53.772145: Training Step 48/59: batchLoss = 2.7431, diffLoss = 13.3910, kgLoss = 0.0811
2025-02-12 23:27:54.710396: Training Step 49/59: batchLoss = 3.3148, diffLoss = 16.2049, kgLoss = 0.0923
2025-02-12 23:27:55.649898: Training Step 50/59: batchLoss = 2.9262, diffLoss = 14.3132, kgLoss = 0.0794
2025-02-12 23:27:56.584275: Training Step 51/59: batchLoss = 2.7648, diffLoss = 13.5017, kgLoss = 0.0806
2025-02-12 23:27:57.527925: Training Step 52/59: batchLoss = 2.9897, diffLoss = 14.6132, kgLoss = 0.0838
2025-02-12 23:27:58.464779: Training Step 53/59: batchLoss = 2.9875, diffLoss = 14.5848, kgLoss = 0.0882
2025-02-12 23:27:59.405902: Training Step 54/59: batchLoss = 3.0389, diffLoss = 14.8708, kgLoss = 0.0810
2025-02-12 23:28:00.336381: Training Step 55/59: batchLoss = 3.0470, diffLoss = 14.8915, kgLoss = 0.0859
2025-02-12 23:28:01.268525: Training Step 56/59: batchLoss = 3.2448, diffLoss = 15.8605, kgLoss = 0.0909
2025-02-12 23:28:02.114727: Training Step 57/59: batchLoss = 2.9370, diffLoss = 14.3609, kgLoss = 0.0810
2025-02-12 23:28:02.964037: Training Step 58/59: batchLoss = 3.0812, diffLoss = 15.0567, kgLoss = 0.0873
2025-02-12 23:28:03.059633: 
2025-02-12 23:28:03.059961: Epoch 42/1000, Train: epLoss = 0.4338, epDfLoss = 2.1184, epKgLoss = 0.0126  
2025-02-12 23:28:04.520270: Steps 0/47: batch_recall = 37.45, batch_ndcg = 46.97 
2025-02-12 23:28:05.810942: Steps 1/47: batch_recall = 36.52, batch_ndcg = 42.12 
2025-02-12 23:28:07.055166: Steps 2/47: batch_recall = 41.28, batch_ndcg = 46.53 
2025-02-12 23:28:08.313492: Steps 3/47: batch_recall = 43.53, batch_ndcg = 45.00 
2025-02-12 23:28:09.511272: Steps 4/47: batch_recall = 39.11, batch_ndcg = 44.16 
2025-02-12 23:28:10.733762: Steps 5/47: batch_recall = 33.23, batch_ndcg = 39.35 
2025-02-12 23:28:11.949933: Steps 6/47: batch_recall = 38.88, batch_ndcg = 41.74 
2025-02-12 23:28:13.133636: Steps 7/47: batch_recall = 44.24, batch_ndcg = 44.89 
2025-02-12 23:28:14.313716: Steps 8/47: batch_recall = 48.37, batch_ndcg = 51.20 
2025-02-12 23:28:15.465205: Steps 9/47: batch_recall = 46.45, batch_ndcg = 46.55 
2025-02-12 23:28:16.648309: Steps 10/47: batch_recall = 43.75, batch_ndcg = 42.43 
2025-02-12 23:28:17.795832: Steps 11/47: batch_recall = 55.21, batch_ndcg = 52.14 
2025-02-12 23:28:18.954805: Steps 12/47: batch_recall = 49.06, batch_ndcg = 47.72 
2025-02-12 23:28:20.086304: Steps 13/47: batch_recall = 47.85, batch_ndcg = 45.50 
2025-02-12 23:28:21.172382: Steps 14/47: batch_recall = 41.53, batch_ndcg = 41.35 
2025-02-12 23:28:22.256494: Steps 15/47: batch_recall = 57.04, batch_ndcg = 53.36 
2025-02-12 23:28:23.340282: Steps 16/47: batch_recall = 49.54, batch_ndcg = 45.79 
2025-02-12 23:28:24.379778: Steps 17/47: batch_recall = 57.00, batch_ndcg = 49.47 
2025-02-12 23:28:25.457972: Steps 18/47: batch_recall = 51.99, batch_ndcg = 48.96 
2025-02-12 23:28:26.521736: Steps 19/47: batch_recall = 60.95, batch_ndcg = 55.45 
2025-02-12 23:28:27.564239: Steps 20/47: batch_recall = 68.98, batch_ndcg = 62.02 
2025-02-12 23:28:28.622433: Steps 21/47: batch_recall = 64.96, batch_ndcg = 55.66 
2025-02-12 23:28:29.683089: Steps 22/47: batch_recall = 54.69, batch_ndcg = 51.01 
2025-02-12 23:28:30.744336: Steps 23/47: batch_recall = 63.75, batch_ndcg = 54.69 
2025-02-12 23:28:31.821188: Steps 24/47: batch_recall = 64.71, batch_ndcg = 56.91 
2025-02-12 23:28:32.876015: Steps 25/47: batch_recall = 65.60, batch_ndcg = 56.61 
2025-02-12 23:28:33.898360: Steps 26/47: batch_recall = 60.73, batch_ndcg = 53.09 
2025-02-12 23:28:34.939565: Steps 27/47: batch_recall = 61.81, batch_ndcg = 52.12 
2025-02-12 23:28:35.972933: Steps 28/47: batch_recall = 69.75, batch_ndcg = 58.96 
2025-02-12 23:28:36.994530: Steps 29/47: batch_recall = 70.04, batch_ndcg = 58.42 
2025-02-12 23:28:38.008267: Steps 30/47: batch_recall = 73.29, batch_ndcg = 64.46 
2025-02-12 23:28:39.031663: Steps 31/47: batch_recall = 66.29, batch_ndcg = 54.80 
2025-02-12 23:28:40.048565: Steps 32/47: batch_recall = 71.78, batch_ndcg = 64.69 
2025-02-12 23:28:41.056011: Steps 33/47: batch_recall = 80.05, batch_ndcg = 67.03 
2025-02-12 23:28:42.065915: Steps 34/47: batch_recall = 66.56, batch_ndcg = 54.64 
2025-02-12 23:28:43.039559: Steps 35/47: batch_recall = 75.13, batch_ndcg = 63.87 
2025-02-12 23:28:44.027317: Steps 36/47: batch_recall = 76.61, batch_ndcg = 63.59 
2025-02-12 23:28:45.005148: Steps 37/47: batch_recall = 81.69, batch_ndcg = 72.01 
2025-02-12 23:28:45.987251: Steps 38/47: batch_recall = 91.83, batch_ndcg = 73.22 
2025-02-12 23:28:46.973316: Steps 39/47: batch_recall = 89.93, batch_ndcg = 69.68 
2025-02-12 23:28:47.947373: Steps 40/47: batch_recall = 72.65, batch_ndcg = 62.87 
2025-02-12 23:28:48.911338: Steps 41/47: batch_recall = 87.51, batch_ndcg = 71.73 
2025-02-12 23:28:49.884501: Steps 42/47: batch_recall = 81.81, batch_ndcg = 63.98 
2025-02-12 23:28:50.851938: Steps 43/47: batch_recall = 90.59, batch_ndcg = 71.77 
2025-02-12 23:28:51.825948: Steps 44/47: batch_recall = 87.43, batch_ndcg = 69.73 
2025-02-12 23:28:52.769641: Steps 45/47: batch_recall = 94.87, batch_ndcg = 73.98 
2025-02-12 23:28:52.876725: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.55 
2025-02-12 23:28:52.876859: Epoch 42/1000, Test: Recall = 0.1213, NDCG = 0.1084  

2025-02-12 23:28:54.092210: Training Step 0/59: batchLoss = 2.6078, diffLoss = 12.7164, kgLoss = 0.0807
2025-02-12 23:28:55.024415: Training Step 1/59: batchLoss = 2.9908, diffLoss = 14.5884, kgLoss = 0.0914
2025-02-12 23:28:55.953887: Training Step 2/59: batchLoss = 2.6002, diffLoss = 12.6908, kgLoss = 0.0775
2025-02-12 23:28:56.880496: Training Step 3/59: batchLoss = 2.8873, diffLoss = 14.0876, kgLoss = 0.0873
2025-02-12 23:28:57.805769: Training Step 4/59: batchLoss = 2.6693, diffLoss = 13.0215, kgLoss = 0.0812
2025-02-12 23:28:58.738477: Training Step 5/59: batchLoss = 2.9424, diffLoss = 14.3603, kgLoss = 0.0879
2025-02-12 23:28:59.659756: Training Step 6/59: batchLoss = 2.7850, diffLoss = 13.5845, kgLoss = 0.0852
2025-02-12 23:29:00.590916: Training Step 7/59: batchLoss = 2.9307, diffLoss = 14.2943, kgLoss = 0.0898
2025-02-12 23:29:01.516603: Training Step 8/59: batchLoss = 3.0254, diffLoss = 14.7955, kgLoss = 0.0828
2025-02-12 23:29:02.447723: Training Step 9/59: batchLoss = 2.6521, diffLoss = 12.9328, kgLoss = 0.0819
2025-02-12 23:29:03.375928: Training Step 10/59: batchLoss = 2.9650, diffLoss = 14.4416, kgLoss = 0.0959
2025-02-12 23:29:04.313027: Training Step 11/59: batchLoss = 3.0353, diffLoss = 14.7951, kgLoss = 0.0953
2025-02-12 23:29:05.249020: Training Step 12/59: batchLoss = 2.6718, diffLoss = 13.0413, kgLoss = 0.0795
2025-02-12 23:29:06.185700: Training Step 13/59: batchLoss = 2.9337, diffLoss = 14.3269, kgLoss = 0.0854
2025-02-12 23:29:07.123412: Training Step 14/59: batchLoss = 2.9969, diffLoss = 14.6262, kgLoss = 0.0895
2025-02-12 23:29:08.059127: Training Step 15/59: batchLoss = 2.8706, diffLoss = 14.0528, kgLoss = 0.0751
2025-02-12 23:29:08.991423: Training Step 16/59: batchLoss = 2.8979, diffLoss = 14.1179, kgLoss = 0.0929
2025-02-12 23:29:09.929061: Training Step 17/59: batchLoss = 3.0534, diffLoss = 14.8809, kgLoss = 0.0965
2025-02-12 23:29:10.866015: Training Step 18/59: batchLoss = 2.5195, diffLoss = 12.2981, kgLoss = 0.0749
2025-02-12 23:29:11.800114: Training Step 19/59: batchLoss = 2.9717, diffLoss = 14.5003, kgLoss = 0.0895
2025-02-12 23:29:12.734181: Training Step 20/59: batchLoss = 2.8207, diffLoss = 13.7708, kgLoss = 0.0831
2025-02-12 23:29:13.660111: Training Step 21/59: batchLoss = 2.9726, diffLoss = 14.5189, kgLoss = 0.0861
2025-02-12 23:29:14.587280: Training Step 22/59: batchLoss = 3.0346, diffLoss = 14.8245, kgLoss = 0.0872
2025-02-12 23:29:15.515776: Training Step 23/59: batchLoss = 3.0246, diffLoss = 14.7779, kgLoss = 0.0863
2025-02-12 23:29:16.441678: Training Step 24/59: batchLoss = 2.7615, diffLoss = 13.4784, kgLoss = 0.0823
2025-02-12 23:29:17.370221: Training Step 25/59: batchLoss = 2.8820, diffLoss = 14.0869, kgLoss = 0.0807
2025-02-12 23:29:18.293433: Training Step 26/59: batchLoss = 2.7882, diffLoss = 13.6222, kgLoss = 0.0798
2025-02-12 23:29:19.226759: Training Step 27/59: batchLoss = 2.8116, diffLoss = 13.7498, kgLoss = 0.0771
2025-02-12 23:29:20.169051: Training Step 28/59: batchLoss = 2.6307, diffLoss = 12.8836, kgLoss = 0.0674
2025-02-12 23:29:21.105956: Training Step 29/59: batchLoss = 3.2331, diffLoss = 15.8101, kgLoss = 0.0888
2025-02-12 23:29:22.042159: Training Step 30/59: batchLoss = 2.8242, diffLoss = 13.7929, kgLoss = 0.0820
2025-02-12 23:29:22.989417: Training Step 31/59: batchLoss = 2.9821, diffLoss = 14.5727, kgLoss = 0.0845
2025-02-12 23:29:23.926567: Training Step 32/59: batchLoss = 2.9180, diffLoss = 14.2395, kgLoss = 0.0876
2025-02-12 23:29:24.863142: Training Step 33/59: batchLoss = 3.0329, diffLoss = 14.7930, kgLoss = 0.0929
2025-02-12 23:29:25.799200: Training Step 34/59: batchLoss = 2.8988, diffLoss = 14.1520, kgLoss = 0.0855
2025-02-12 23:29:26.738841: Training Step 35/59: batchLoss = 3.0574, diffLoss = 14.9361, kgLoss = 0.0877
2025-02-12 23:29:27.676342: Training Step 36/59: batchLoss = 2.8772, diffLoss = 14.0572, kgLoss = 0.0823
2025-02-12 23:29:28.608349: Training Step 37/59: batchLoss = 3.0601, diffLoss = 14.9507, kgLoss = 0.0874
2025-02-12 23:29:29.535819: Training Step 38/59: batchLoss = 2.8310, diffLoss = 13.8365, kgLoss = 0.0797
2025-02-12 23:29:30.461015: Training Step 39/59: batchLoss = 2.6796, diffLoss = 13.0828, kgLoss = 0.0789
2025-02-12 23:29:31.389875: Training Step 40/59: batchLoss = 2.7791, diffLoss = 13.5824, kgLoss = 0.0783
2025-02-12 23:29:32.319827: Training Step 41/59: batchLoss = 3.2438, diffLoss = 15.8651, kgLoss = 0.0885
2025-02-12 23:29:33.249354: Training Step 42/59: batchLoss = 3.1104, diffLoss = 15.2065, kgLoss = 0.0864
2025-02-12 23:29:34.179071: Training Step 43/59: batchLoss = 2.6679, diffLoss = 13.0410, kgLoss = 0.0747
2025-02-12 23:29:35.102739: Training Step 44/59: batchLoss = 3.3125, diffLoss = 16.1662, kgLoss = 0.0990
2025-02-12 23:29:36.032693: Training Step 45/59: batchLoss = 2.8218, diffLoss = 13.7641, kgLoss = 0.0862
2025-02-12 23:29:36.965567: Training Step 46/59: batchLoss = 3.6640, diffLoss = 17.9065, kgLoss = 0.1033
2025-02-12 23:29:37.901393: Training Step 47/59: batchLoss = 2.9803, diffLoss = 14.5663, kgLoss = 0.0838
2025-02-12 23:29:38.843027: Training Step 48/59: batchLoss = 3.0301, diffLoss = 14.8038, kgLoss = 0.0867
2025-02-12 23:29:39.774573: Training Step 49/59: batchLoss = 3.1425, diffLoss = 15.3121, kgLoss = 0.1002
2025-02-12 23:29:40.710503: Training Step 50/59: batchLoss = 3.5289, diffLoss = 17.2494, kgLoss = 0.0988
2025-02-12 23:29:41.650699: Training Step 51/59: batchLoss = 2.8219, diffLoss = 13.7811, kgLoss = 0.0821
2025-02-12 23:29:42.587445: Training Step 52/59: batchLoss = 2.9940, diffLoss = 14.6298, kgLoss = 0.0851
2025-02-12 23:29:43.529749: Training Step 53/59: batchLoss = 2.9288, diffLoss = 14.3136, kgLoss = 0.0826
2025-02-12 23:29:44.469319: Training Step 54/59: batchLoss = 3.0077, diffLoss = 14.6896, kgLoss = 0.0873
2025-02-12 23:29:45.407279: Training Step 55/59: batchLoss = 3.1197, diffLoss = 15.2573, kgLoss = 0.0852
2025-02-12 23:29:46.350613: Training Step 56/59: batchLoss = 3.0741, diffLoss = 15.0334, kgLoss = 0.0842
2025-02-12 23:29:47.196190: Training Step 57/59: batchLoss = 3.2119, diffLoss = 15.7010, kgLoss = 0.0897
2025-02-12 23:29:48.046005: Training Step 58/59: batchLoss = 3.0879, diffLoss = 15.1006, kgLoss = 0.0848
2025-02-12 23:29:48.146459: 
2025-02-12 23:29:48.146976: Epoch 43/1000, Train: epLoss = 0.4341, epDfLoss = 2.1201, epKgLoss = 0.0126  
2025-02-12 23:29:49.618660: Steps 0/47: batch_recall = 37.57, batch_ndcg = 47.64 
2025-02-12 23:29:50.910126: Steps 1/47: batch_recall = 36.67, batch_ndcg = 42.27 
2025-02-12 23:29:52.142818: Steps 2/47: batch_recall = 39.92, batch_ndcg = 46.48 
2025-02-12 23:29:53.398550: Steps 3/47: batch_recall = 43.95, batch_ndcg = 45.68 
2025-02-12 23:29:54.599296: Steps 4/47: batch_recall = 39.26, batch_ndcg = 44.61 
2025-02-12 23:29:55.828212: Steps 5/47: batch_recall = 34.20, batch_ndcg = 39.71 
2025-02-12 23:29:57.034256: Steps 6/47: batch_recall = 38.69, batch_ndcg = 42.30 
2025-02-12 23:29:58.223978: Steps 7/47: batch_recall = 43.48, batch_ndcg = 45.15 
2025-02-12 23:29:59.400355: Steps 8/47: batch_recall = 48.57, batch_ndcg = 52.48 
2025-02-12 23:30:00.545732: Steps 9/47: batch_recall = 46.40, batch_ndcg = 46.69 
2025-02-12 23:30:01.727747: Steps 10/47: batch_recall = 42.66, batch_ndcg = 42.79 
2025-02-12 23:30:02.873198: Steps 11/47: batch_recall = 54.49, batch_ndcg = 52.11 
2025-02-12 23:30:04.020311: Steps 12/47: batch_recall = 49.42, batch_ndcg = 48.40 
2025-02-12 23:30:05.155373: Steps 13/47: batch_recall = 48.58, batch_ndcg = 45.10 
2025-02-12 23:30:06.245207: Steps 14/47: batch_recall = 41.52, batch_ndcg = 41.24 
2025-02-12 23:30:07.343462: Steps 15/47: batch_recall = 57.03, batch_ndcg = 53.55 
2025-02-12 23:30:08.414883: Steps 16/47: batch_recall = 51.04, batch_ndcg = 46.42 
2025-02-12 23:30:09.456455: Steps 17/47: batch_recall = 56.55, batch_ndcg = 48.98 
2025-02-12 23:30:10.537241: Steps 18/47: batch_recall = 52.73, batch_ndcg = 48.86 
2025-02-12 23:30:11.615192: Steps 19/47: batch_recall = 59.33, batch_ndcg = 53.87 
2025-02-12 23:30:12.671427: Steps 20/47: batch_recall = 66.01, batch_ndcg = 61.72 
2025-02-12 23:30:13.729036: Steps 21/47: batch_recall = 64.40, batch_ndcg = 55.38 
2025-02-12 23:30:14.792718: Steps 22/47: batch_recall = 53.66, batch_ndcg = 50.25 
2025-02-12 23:30:15.849167: Steps 23/47: batch_recall = 60.54, batch_ndcg = 53.32 
2025-02-12 23:30:16.929465: Steps 24/47: batch_recall = 64.10, batch_ndcg = 56.35 
2025-02-12 23:30:17.976926: Steps 25/47: batch_recall = 64.68, batch_ndcg = 56.02 
2025-02-12 23:30:19.006013: Steps 26/47: batch_recall = 60.05, batch_ndcg = 53.09 
2025-02-12 23:30:20.047252: Steps 27/47: batch_recall = 63.06, batch_ndcg = 52.48 
2025-02-12 23:30:21.070671: Steps 28/47: batch_recall = 70.08, batch_ndcg = 59.01 
2025-02-12 23:30:22.083977: Steps 29/47: batch_recall = 69.81, batch_ndcg = 57.75 
2025-02-12 23:30:23.101153: Steps 30/47: batch_recall = 73.34, batch_ndcg = 63.25 
2025-02-12 23:30:24.128772: Steps 31/47: batch_recall = 67.21, batch_ndcg = 55.39 
2025-02-12 23:30:25.139037: Steps 32/47: batch_recall = 71.10, batch_ndcg = 64.35 
2025-02-12 23:30:26.148829: Steps 33/47: batch_recall = 79.24, batch_ndcg = 67.51 
2025-02-12 23:30:27.163033: Steps 34/47: batch_recall = 66.67, batch_ndcg = 55.50 
2025-02-12 23:30:28.150986: Steps 35/47: batch_recall = 76.21, batch_ndcg = 63.23 
2025-02-12 23:30:29.146601: Steps 36/47: batch_recall = 78.35, batch_ndcg = 63.81 
2025-02-12 23:30:30.130448: Steps 37/47: batch_recall = 83.03, batch_ndcg = 72.27 
2025-02-12 23:30:31.109744: Steps 38/47: batch_recall = 90.63, batch_ndcg = 72.50 
2025-02-12 23:30:32.088102: Steps 39/47: batch_recall = 88.29, batch_ndcg = 68.89 
2025-02-12 23:30:33.070000: Steps 40/47: batch_recall = 71.24, batch_ndcg = 61.89 
2025-02-12 23:30:34.038727: Steps 41/47: batch_recall = 86.59, batch_ndcg = 71.65 
2025-02-12 23:30:35.019841: Steps 42/47: batch_recall = 82.49, batch_ndcg = 64.26 
2025-02-12 23:30:35.994779: Steps 43/47: batch_recall = 88.09, batch_ndcg = 70.35 
2025-02-12 23:30:36.973895: Steps 44/47: batch_recall = 87.40, batch_ndcg = 69.86 
2025-02-12 23:30:37.926998: Steps 45/47: batch_recall = 95.08, batch_ndcg = 74.96 
2025-02-12 23:30:38.028930: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 23:30:38.029064: Epoch 43/1000, Test: Recall = 0.1207, NDCG = 0.1082  

2025-02-12 23:30:39.224243: Training Step 0/59: batchLoss = 2.8356, diffLoss = 13.8441, kgLoss = 0.0835
2025-02-12 23:30:40.152534: Training Step 1/59: batchLoss = 2.9139, diffLoss = 14.2261, kgLoss = 0.0858
2025-02-12 23:30:41.080446: Training Step 2/59: batchLoss = 2.8728, diffLoss = 14.0337, kgLoss = 0.0826
2025-02-12 23:30:42.007858: Training Step 3/59: batchLoss = 2.7776, diffLoss = 13.5443, kgLoss = 0.0860
2025-02-12 23:30:42.929252: Training Step 4/59: batchLoss = 2.6231, diffLoss = 12.7928, kgLoss = 0.0806
2025-02-12 23:30:43.854756: Training Step 5/59: batchLoss = 2.8475, diffLoss = 13.8915, kgLoss = 0.0865
2025-02-12 23:30:44.771799: Training Step 6/59: batchLoss = 2.8102, diffLoss = 13.7004, kgLoss = 0.0876
2025-02-12 23:30:45.703130: Training Step 7/59: batchLoss = 2.6366, diffLoss = 12.8956, kgLoss = 0.0719
2025-02-12 23:30:46.636741: Training Step 8/59: batchLoss = 2.5729, diffLoss = 12.5576, kgLoss = 0.0767
2025-02-12 23:30:47.573447: Training Step 9/59: batchLoss = 2.6027, diffLoss = 12.7200, kgLoss = 0.0734
2025-02-12 23:30:48.501719: Training Step 10/59: batchLoss = 3.1984, diffLoss = 15.6138, kgLoss = 0.0945
2025-02-12 23:30:49.429405: Training Step 11/59: batchLoss = 2.8840, diffLoss = 14.0816, kgLoss = 0.0846
2025-02-12 23:30:50.364690: Training Step 12/59: batchLoss = 2.9496, diffLoss = 14.3834, kgLoss = 0.0912
2025-02-12 23:30:51.302232: Training Step 13/59: batchLoss = 2.6885, diffLoss = 13.1224, kgLoss = 0.0800
2025-02-12 23:30:52.235001: Training Step 14/59: batchLoss = 3.0851, diffLoss = 15.0778, kgLoss = 0.0870
2025-02-12 23:30:53.169729: Training Step 15/59: batchLoss = 3.0874, diffLoss = 15.0714, kgLoss = 0.0914
2025-02-12 23:30:54.104305: Training Step 16/59: batchLoss = 2.6801, diffLoss = 13.0451, kgLoss = 0.0888
2025-02-12 23:30:55.039653: Training Step 17/59: batchLoss = 2.7923, diffLoss = 13.6285, kgLoss = 0.0832
2025-02-12 23:30:55.968202: Training Step 18/59: batchLoss = 2.6170, diffLoss = 12.7679, kgLoss = 0.0793
2025-02-12 23:30:56.894561: Training Step 19/59: batchLoss = 3.1078, diffLoss = 15.1662, kgLoss = 0.0933
2025-02-12 23:30:57.817985: Training Step 20/59: batchLoss = 3.0582, diffLoss = 14.9228, kgLoss = 0.0920
2025-02-12 23:30:58.743809: Training Step 21/59: batchLoss = 2.8764, diffLoss = 14.0526, kgLoss = 0.0824
2025-02-12 23:30:59.671516: Training Step 22/59: batchLoss = 2.6463, diffLoss = 12.9241, kgLoss = 0.0768
2025-02-12 23:31:00.600244: Training Step 23/59: batchLoss = 3.0322, diffLoss = 14.7805, kgLoss = 0.0951
2025-02-12 23:31:01.523787: Training Step 24/59: batchLoss = 2.9976, diffLoss = 14.6419, kgLoss = 0.0865
2025-02-12 23:31:02.444618: Training Step 25/59: batchLoss = 2.6777, diffLoss = 13.0652, kgLoss = 0.0809
2025-02-12 23:31:03.365653: Training Step 26/59: batchLoss = 2.8420, diffLoss = 13.8816, kgLoss = 0.0821
2025-02-12 23:31:04.299381: Training Step 27/59: batchLoss = 2.9929, diffLoss = 14.6095, kgLoss = 0.0888
2025-02-12 23:31:05.244352: Training Step 28/59: batchLoss = 2.8960, diffLoss = 14.1386, kgLoss = 0.0854
2025-02-12 23:31:06.188080: Training Step 29/59: batchLoss = 2.9399, diffLoss = 14.3659, kgLoss = 0.0834
2025-02-12 23:31:07.121864: Training Step 30/59: batchLoss = 3.3389, diffLoss = 16.2892, kgLoss = 0.1013
2025-02-12 23:31:08.066785: Training Step 31/59: batchLoss = 2.6027, diffLoss = 12.6936, kgLoss = 0.0799
2025-02-12 23:31:09.004290: Training Step 32/59: batchLoss = 2.6878, diffLoss = 13.1012, kgLoss = 0.0844
2025-02-12 23:31:09.951218: Training Step 33/59: batchLoss = 3.3574, diffLoss = 16.4088, kgLoss = 0.0946
2025-02-12 23:31:10.886469: Training Step 34/59: batchLoss = 2.7068, diffLoss = 13.2009, kgLoss = 0.0833
2025-02-12 23:31:11.823129: Training Step 35/59: batchLoss = 2.8207, diffLoss = 13.7736, kgLoss = 0.0825
2025-02-12 23:31:12.765882: Training Step 36/59: batchLoss = 2.7914, diffLoss = 13.6411, kgLoss = 0.0789
2025-02-12 23:31:13.699642: Training Step 37/59: batchLoss = 3.1085, diffLoss = 15.1876, kgLoss = 0.0887
2025-02-12 23:31:14.627036: Training Step 38/59: batchLoss = 3.3868, diffLoss = 16.5368, kgLoss = 0.0993
2025-02-12 23:31:15.550369: Training Step 39/59: batchLoss = 2.7524, diffLoss = 13.4652, kgLoss = 0.0742
2025-02-12 23:31:16.486901: Training Step 40/59: batchLoss = 3.0178, diffLoss = 14.7466, kgLoss = 0.0856
2025-02-12 23:31:17.410667: Training Step 41/59: batchLoss = 3.1456, diffLoss = 15.3631, kgLoss = 0.0912
2025-02-12 23:31:18.339878: Training Step 42/59: batchLoss = 2.8246, diffLoss = 13.8055, kgLoss = 0.0794
2025-02-12 23:31:19.262987: Training Step 43/59: batchLoss = 2.9950, diffLoss = 14.6386, kgLoss = 0.0841
2025-02-12 23:31:20.199156: Training Step 44/59: batchLoss = 2.6398, diffLoss = 12.8901, kgLoss = 0.0772
2025-02-12 23:31:21.134523: Training Step 45/59: batchLoss = 3.2410, diffLoss = 15.8236, kgLoss = 0.0954
2025-02-12 23:31:22.073895: Training Step 46/59: batchLoss = 3.0860, diffLoss = 15.0981, kgLoss = 0.0829
2025-02-12 23:31:23.008807: Training Step 47/59: batchLoss = 3.1011, diffLoss = 15.1514, kgLoss = 0.0886
2025-02-12 23:31:23.945390: Training Step 48/59: batchLoss = 3.0298, diffLoss = 14.7857, kgLoss = 0.0908
2025-02-12 23:31:24.877027: Training Step 49/59: batchLoss = 3.3756, diffLoss = 16.4840, kgLoss = 0.0985
2025-02-12 23:31:25.813669: Training Step 50/59: batchLoss = 2.9610, diffLoss = 14.4684, kgLoss = 0.0841
2025-02-12 23:31:26.745660: Training Step 51/59: batchLoss = 2.6828, diffLoss = 13.1034, kgLoss = 0.0777
2025-02-12 23:31:27.672766: Training Step 52/59: batchLoss = 3.0141, diffLoss = 14.7425, kgLoss = 0.0820
2025-02-12 23:31:28.604611: Training Step 53/59: batchLoss = 2.8132, diffLoss = 13.7639, kgLoss = 0.0755
2025-02-12 23:31:29.539609: Training Step 54/59: batchLoss = 3.0414, diffLoss = 14.8515, kgLoss = 0.0889
2025-02-12 23:31:30.468476: Training Step 55/59: batchLoss = 3.3031, diffLoss = 16.1478, kgLoss = 0.0919
2025-02-12 23:31:31.395906: Training Step 56/59: batchLoss = 3.1723, diffLoss = 15.4984, kgLoss = 0.0907
2025-02-12 23:31:32.240400: Training Step 57/59: batchLoss = 3.2738, diffLoss = 15.9739, kgLoss = 0.0987
2025-02-12 23:31:33.090986: Training Step 58/59: batchLoss = 3.0924, diffLoss = 15.1276, kgLoss = 0.0836
2025-02-12 23:31:33.190121: 
2025-02-12 23:31:33.190629: Epoch 44/1000, Train: epLoss = 0.4323, epDfLoss = 2.1108, epKgLoss = 0.0126  
2025-02-12 23:31:34.656126: Steps 0/47: batch_recall = 37.35, batch_ndcg = 47.84 
2025-02-12 23:31:35.946191: Steps 1/47: batch_recall = 35.96, batch_ndcg = 42.13 
2025-02-12 23:31:37.193031: Steps 2/47: batch_recall = 40.08, batch_ndcg = 45.99 
2025-02-12 23:31:38.465447: Steps 3/47: batch_recall = 44.07, batch_ndcg = 45.88 
2025-02-12 23:31:39.666918: Steps 4/47: batch_recall = 39.16, batch_ndcg = 45.24 
2025-02-12 23:31:40.879507: Steps 5/47: batch_recall = 33.57, batch_ndcg = 39.86 
2025-02-12 23:31:42.069021: Steps 6/47: batch_recall = 38.60, batch_ndcg = 41.73 
2025-02-12 23:31:43.255751: Steps 7/47: batch_recall = 44.24, batch_ndcg = 45.29 
2025-02-12 23:31:44.439241: Steps 8/47: batch_recall = 48.29, batch_ndcg = 51.34 
2025-02-12 23:31:45.591805: Steps 9/47: batch_recall = 47.29, batch_ndcg = 45.90 
2025-02-12 23:31:46.762124: Steps 10/47: batch_recall = 43.10, batch_ndcg = 43.21 
2025-02-12 23:31:47.903043: Steps 11/47: batch_recall = 53.55, batch_ndcg = 51.54 
2025-02-12 23:31:49.039175: Steps 12/47: batch_recall = 49.40, batch_ndcg = 47.76 
2025-02-12 23:31:50.166254: Steps 13/47: batch_recall = 48.72, batch_ndcg = 45.54 
2025-02-12 23:31:51.257311: Steps 14/47: batch_recall = 39.11, batch_ndcg = 40.44 
2025-02-12 23:31:52.341819: Steps 15/47: batch_recall = 56.16, batch_ndcg = 53.23 
2025-02-12 23:31:53.413146: Steps 16/47: batch_recall = 50.41, batch_ndcg = 46.44 
2025-02-12 23:31:54.459097: Steps 17/47: batch_recall = 56.72, batch_ndcg = 49.03 
2025-02-12 23:31:55.540120: Steps 18/47: batch_recall = 52.54, batch_ndcg = 48.61 
2025-02-12 23:31:56.609593: Steps 19/47: batch_recall = 58.35, batch_ndcg = 54.24 
2025-02-12 23:31:57.660464: Steps 20/47: batch_recall = 65.80, batch_ndcg = 60.70 
2025-02-12 23:31:58.705543: Steps 21/47: batch_recall = 65.74, batch_ndcg = 55.91 
2025-02-12 23:31:59.758516: Steps 22/47: batch_recall = 53.21, batch_ndcg = 50.72 
2025-02-12 23:32:00.808044: Steps 23/47: batch_recall = 62.22, batch_ndcg = 55.23 
2025-02-12 23:32:01.872859: Steps 24/47: batch_recall = 63.36, batch_ndcg = 56.58 
2025-02-12 23:32:02.918545: Steps 25/47: batch_recall = 66.13, batch_ndcg = 56.87 
2025-02-12 23:32:03.944986: Steps 26/47: batch_recall = 58.97, batch_ndcg = 52.46 
2025-02-12 23:32:04.985725: Steps 27/47: batch_recall = 62.27, batch_ndcg = 52.15 
2025-02-12 23:32:06.008316: Steps 28/47: batch_recall = 70.69, batch_ndcg = 60.01 
2025-02-12 23:32:07.029685: Steps 29/47: batch_recall = 68.77, batch_ndcg = 57.39 
2025-02-12 23:32:08.034956: Steps 30/47: batch_recall = 72.14, batch_ndcg = 63.14 
2025-02-12 23:32:09.060349: Steps 31/47: batch_recall = 65.70, batch_ndcg = 54.45 
2025-02-12 23:32:10.068791: Steps 32/47: batch_recall = 70.72, batch_ndcg = 64.53 
2025-02-12 23:32:11.084456: Steps 33/47: batch_recall = 80.04, batch_ndcg = 67.93 
2025-02-12 23:32:12.096253: Steps 34/47: batch_recall = 65.90, batch_ndcg = 55.58 
2025-02-12 23:32:13.086547: Steps 35/47: batch_recall = 77.23, batch_ndcg = 65.38 
2025-02-12 23:32:14.078175: Steps 36/47: batch_recall = 78.28, batch_ndcg = 64.45 
2025-02-12 23:32:15.067875: Steps 37/47: batch_recall = 82.96, batch_ndcg = 71.53 
2025-02-12 23:32:16.045291: Steps 38/47: batch_recall = 90.36, batch_ndcg = 71.43 
2025-02-12 23:32:17.022790: Steps 39/47: batch_recall = 89.00, batch_ndcg = 69.76 
2025-02-12 23:32:17.996350: Steps 40/47: batch_recall = 71.69, batch_ndcg = 62.49 
2025-02-12 23:32:18.967067: Steps 41/47: batch_recall = 84.95, batch_ndcg = 70.92 
2025-02-12 23:32:19.935041: Steps 42/47: batch_recall = 84.23, batch_ndcg = 64.96 
2025-02-12 23:32:20.909029: Steps 43/47: batch_recall = 90.68, batch_ndcg = 70.70 
2025-02-12 23:32:21.875754: Steps 44/47: batch_recall = 87.87, batch_ndcg = 69.91 
2025-02-12 23:32:22.816298: Steps 45/47: batch_recall = 92.41, batch_ndcg = 74.04 
2025-02-12 23:32:22.920435: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-12 23:32:22.920567: Epoch 44/1000, Test: Recall = 0.1205, NDCG = 0.1083  

2025-02-12 23:32:24.101015: Training Step 0/59: batchLoss = 2.7967, diffLoss = 13.6498, kgLoss = 0.0834
2025-02-12 23:32:25.023075: Training Step 1/59: batchLoss = 2.8035, diffLoss = 13.6744, kgLoss = 0.0858
2025-02-12 23:32:25.945975: Training Step 2/59: batchLoss = 2.7526, diffLoss = 13.4238, kgLoss = 0.0848
2025-02-12 23:32:26.871670: Training Step 3/59: batchLoss = 3.0778, diffLoss = 15.0224, kgLoss = 0.0917
2025-02-12 23:32:27.802366: Training Step 4/59: batchLoss = 2.5306, diffLoss = 12.3560, kgLoss = 0.0743
2025-02-12 23:32:28.725990: Training Step 5/59: batchLoss = 2.6722, diffLoss = 13.0341, kgLoss = 0.0817
2025-02-12 23:32:29.653840: Training Step 6/59: batchLoss = 2.6929, diffLoss = 13.1429, kgLoss = 0.0804
2025-02-12 23:32:30.591464: Training Step 7/59: batchLoss = 2.8666, diffLoss = 14.0037, kgLoss = 0.0823
2025-02-12 23:32:31.518368: Training Step 8/59: batchLoss = 2.9147, diffLoss = 14.2376, kgLoss = 0.0840
2025-02-12 23:32:32.456315: Training Step 9/59: batchLoss = 2.8796, diffLoss = 14.0770, kgLoss = 0.0803
2025-02-12 23:32:33.391632: Training Step 10/59: batchLoss = 2.6408, diffLoss = 12.8986, kgLoss = 0.0763
2025-02-12 23:32:34.322657: Training Step 11/59: batchLoss = 2.7828, diffLoss = 13.5714, kgLoss = 0.0856
2025-02-12 23:32:35.257884: Training Step 12/59: batchLoss = 3.1055, diffLoss = 15.1271, kgLoss = 0.1000
2025-02-12 23:32:36.191300: Training Step 13/59: batchLoss = 2.9629, diffLoss = 14.4311, kgLoss = 0.0958
2025-02-12 23:32:37.132370: Training Step 14/59: batchLoss = 2.7881, diffLoss = 13.5838, kgLoss = 0.0891
2025-02-12 23:32:38.064188: Training Step 15/59: batchLoss = 3.1620, diffLoss = 15.4391, kgLoss = 0.0927
2025-02-12 23:32:39.004360: Training Step 16/59: batchLoss = 2.6828, diffLoss = 13.1000, kgLoss = 0.0785
2025-02-12 23:32:39.928167: Training Step 17/59: batchLoss = 2.9119, diffLoss = 14.1858, kgLoss = 0.0934
2025-02-12 23:32:40.850838: Training Step 18/59: batchLoss = 3.0066, diffLoss = 14.6594, kgLoss = 0.0934
2025-02-12 23:32:41.773602: Training Step 19/59: batchLoss = 2.8675, diffLoss = 14.0012, kgLoss = 0.0841
2025-02-12 23:32:42.707247: Training Step 20/59: batchLoss = 2.8666, diffLoss = 13.9735, kgLoss = 0.0899
2025-02-12 23:32:43.633129: Training Step 21/59: batchLoss = 3.3068, diffLoss = 16.1423, kgLoss = 0.0979
2025-02-12 23:32:44.558855: Training Step 22/59: batchLoss = 2.8114, diffLoss = 13.7293, kgLoss = 0.0819
2025-02-12 23:32:45.491703: Training Step 23/59: batchLoss = 2.8539, diffLoss = 13.9246, kgLoss = 0.0862
2025-02-12 23:32:46.415302: Training Step 24/59: batchLoss = 2.6375, diffLoss = 12.8960, kgLoss = 0.0729
2025-02-12 23:32:47.348583: Training Step 25/59: batchLoss = 3.0733, diffLoss = 15.0013, kgLoss = 0.0913
2025-02-12 23:32:48.285967: Training Step 26/59: batchLoss = 2.8960, diffLoss = 14.1595, kgLoss = 0.0801
2025-02-12 23:32:49.215662: Training Step 27/59: batchLoss = 2.7675, diffLoss = 13.4998, kgLoss = 0.0845
2025-02-12 23:32:50.151567: Training Step 28/59: batchLoss = 2.6425, diffLoss = 12.8991, kgLoss = 0.0784
2025-02-12 23:32:51.084538: Training Step 29/59: batchLoss = 2.9225, diffLoss = 14.2847, kgLoss = 0.0820
2025-02-12 23:32:52.026787: Training Step 30/59: batchLoss = 3.5781, diffLoss = 17.4803, kgLoss = 0.1025
2025-02-12 23:32:52.971429: Training Step 31/59: batchLoss = 2.9019, diffLoss = 14.1372, kgLoss = 0.0930
2025-02-12 23:32:53.909195: Training Step 32/59: batchLoss = 3.2198, diffLoss = 15.7185, kgLoss = 0.0951
2025-02-12 23:32:54.845945: Training Step 33/59: batchLoss = 2.8125, diffLoss = 13.7540, kgLoss = 0.0771
2025-02-12 23:32:55.784112: Training Step 34/59: batchLoss = 2.7550, diffLoss = 13.4655, kgLoss = 0.0774
2025-02-12 23:32:56.720505: Training Step 35/59: batchLoss = 3.3570, diffLoss = 16.3780, kgLoss = 0.1018
2025-02-12 23:32:57.638601: Training Step 36/59: batchLoss = 2.8536, diffLoss = 13.9428, kgLoss = 0.0814
2025-02-12 23:32:58.562742: Training Step 37/59: batchLoss = 2.9507, diffLoss = 14.4102, kgLoss = 0.0858
2025-02-12 23:32:59.488513: Training Step 38/59: batchLoss = 2.9322, diffLoss = 14.3190, kgLoss = 0.0855
2025-02-12 23:33:00.416329: Training Step 39/59: batchLoss = 3.1488, diffLoss = 15.3631, kgLoss = 0.0953
2025-02-12 23:33:01.350372: Training Step 40/59: batchLoss = 3.1228, diffLoss = 15.2668, kgLoss = 0.0868
2025-02-12 23:33:02.288402: Training Step 41/59: batchLoss = 2.9723, diffLoss = 14.5129, kgLoss = 0.0871
2025-02-12 23:33:03.217172: Training Step 42/59: batchLoss = 3.3280, diffLoss = 16.2577, kgLoss = 0.0956
2025-02-12 23:33:04.143267: Training Step 43/59: batchLoss = 2.9216, diffLoss = 14.2627, kgLoss = 0.0863
2025-02-12 23:33:05.073372: Training Step 44/59: batchLoss = 3.2982, diffLoss = 16.1182, kgLoss = 0.0932
2025-02-12 23:33:06.008067: Training Step 45/59: batchLoss = 2.9418, diffLoss = 14.3806, kgLoss = 0.0821
2025-02-12 23:33:06.945522: Training Step 46/59: batchLoss = 3.1644, diffLoss = 15.4534, kgLoss = 0.0922
2025-02-12 23:33:07.879631: Training Step 47/59: batchLoss = 2.8199, diffLoss = 13.7844, kgLoss = 0.0787
2025-02-12 23:33:08.813354: Training Step 48/59: batchLoss = 2.7236, diffLoss = 13.2987, kgLoss = 0.0798
2025-02-12 23:33:09.746490: Training Step 49/59: batchLoss = 3.1308, diffLoss = 15.2879, kgLoss = 0.0915
2025-02-12 23:33:10.700954: Training Step 50/59: batchLoss = 2.7760, diffLoss = 13.5689, kgLoss = 0.0777
2025-02-12 23:33:11.635482: Training Step 51/59: batchLoss = 2.8274, diffLoss = 13.8102, kgLoss = 0.0817
2025-02-12 23:33:12.567440: Training Step 52/59: batchLoss = 3.2529, diffLoss = 15.9086, kgLoss = 0.0889
2025-02-12 23:33:13.506814: Training Step 53/59: batchLoss = 2.8084, diffLoss = 13.7373, kgLoss = 0.0761
2025-02-12 23:33:14.442399: Training Step 54/59: batchLoss = 2.9397, diffLoss = 14.3749, kgLoss = 0.0809
2025-02-12 23:33:15.372677: Training Step 55/59: batchLoss = 2.8390, diffLoss = 13.8906, kgLoss = 0.0761
2025-02-12 23:33:16.291281: Training Step 56/59: batchLoss = 2.8914, diffLoss = 14.1426, kgLoss = 0.0786
2025-02-12 23:33:17.135612: Training Step 57/59: batchLoss = 3.3240, diffLoss = 16.2653, kgLoss = 0.0887
2025-02-12 23:33:17.982476: Training Step 58/59: batchLoss = 2.6797, diffLoss = 13.1001, kgLoss = 0.0746
2025-02-12 23:33:18.074328: 
2025-02-12 23:33:18.074660: Epoch 45/1000, Train: epLoss = 0.4324, epDfLoss = 2.1113, epKgLoss = 0.0126  
2025-02-12 23:33:19.539458: Steps 0/47: batch_recall = 37.27, batch_ndcg = 47.19 
2025-02-12 23:33:20.832417: Steps 1/47: batch_recall = 35.95, batch_ndcg = 41.47 
2025-02-12 23:33:22.090997: Steps 2/47: batch_recall = 40.30, batch_ndcg = 46.24 
2025-02-12 23:33:23.362535: Steps 3/47: batch_recall = 42.54, batch_ndcg = 44.70 
2025-02-12 23:33:24.569226: Steps 4/47: batch_recall = 38.55, batch_ndcg = 44.13 
2025-02-12 23:33:25.801570: Steps 5/47: batch_recall = 33.08, batch_ndcg = 39.14 
2025-02-12 23:33:27.001713: Steps 6/47: batch_recall = 38.67, batch_ndcg = 41.62 
2025-02-12 23:33:28.179544: Steps 7/47: batch_recall = 42.76, batch_ndcg = 44.82 
2025-02-12 23:33:29.361954: Steps 8/47: batch_recall = 48.07, batch_ndcg = 51.79 
2025-02-12 23:33:30.501280: Steps 9/47: batch_recall = 46.96, batch_ndcg = 47.23 
2025-02-12 23:33:31.669978: Steps 10/47: batch_recall = 42.65, batch_ndcg = 42.52 
2025-02-12 23:33:32.799239: Steps 11/47: batch_recall = 54.69, batch_ndcg = 52.15 
2025-02-12 23:33:33.940574: Steps 12/47: batch_recall = 49.30, batch_ndcg = 47.45 
2025-02-12 23:33:35.056267: Steps 13/47: batch_recall = 47.92, batch_ndcg = 45.25 
2025-02-12 23:33:36.130845: Steps 14/47: batch_recall = 40.68, batch_ndcg = 41.25 
2025-02-12 23:33:37.207606: Steps 15/47: batch_recall = 57.21, batch_ndcg = 53.19 
2025-02-12 23:33:38.280950: Steps 16/47: batch_recall = 50.83, batch_ndcg = 47.08 
2025-02-12 23:33:39.331125: Steps 17/47: batch_recall = 57.33, batch_ndcg = 48.95 
2025-02-12 23:33:40.399719: Steps 18/47: batch_recall = 52.16, batch_ndcg = 48.68 
2025-02-12 23:33:41.467296: Steps 19/47: batch_recall = 59.23, batch_ndcg = 54.90 
2025-02-12 23:33:42.505119: Steps 20/47: batch_recall = 66.47, batch_ndcg = 60.22 
2025-02-12 23:33:43.547729: Steps 21/47: batch_recall = 65.55, batch_ndcg = 55.88 
2025-02-12 23:33:44.602158: Steps 22/47: batch_recall = 54.67, batch_ndcg = 51.35 
2025-02-12 23:33:45.661319: Steps 23/47: batch_recall = 62.26, batch_ndcg = 54.20 
2025-02-12 23:33:46.728735: Steps 24/47: batch_recall = 64.22, batch_ndcg = 56.57 
2025-02-12 23:33:47.774012: Steps 25/47: batch_recall = 67.17, batch_ndcg = 57.48 
2025-02-12 23:33:48.784807: Steps 26/47: batch_recall = 60.47, batch_ndcg = 52.61 
2025-02-12 23:33:49.815523: Steps 27/47: batch_recall = 61.65, batch_ndcg = 52.31 
2025-02-12 23:33:50.829951: Steps 28/47: batch_recall = 69.57, batch_ndcg = 59.19 
2025-02-12 23:33:51.837487: Steps 29/47: batch_recall = 70.52, batch_ndcg = 58.52 
2025-02-12 23:33:52.844515: Steps 30/47: batch_recall = 72.61, batch_ndcg = 63.33 
2025-02-12 23:33:53.875693: Steps 31/47: batch_recall = 66.54, batch_ndcg = 55.36 
2025-02-12 23:33:54.883512: Steps 32/47: batch_recall = 72.10, batch_ndcg = 65.24 
2025-02-12 23:33:55.898244: Steps 33/47: batch_recall = 79.40, batch_ndcg = 67.94 
2025-02-12 23:33:56.925433: Steps 34/47: batch_recall = 65.42, batch_ndcg = 55.29 
2025-02-12 23:33:57.911553: Steps 35/47: batch_recall = 76.38, batch_ndcg = 64.38 
2025-02-12 23:33:58.903478: Steps 36/47: batch_recall = 77.25, batch_ndcg = 64.39 
2025-02-12 23:33:59.883539: Steps 37/47: batch_recall = 82.27, batch_ndcg = 72.12 
2025-02-12 23:34:00.864726: Steps 38/47: batch_recall = 90.99, batch_ndcg = 71.29 
2025-02-12 23:34:01.842595: Steps 39/47: batch_recall = 88.38, batch_ndcg = 69.51 
2025-02-12 23:34:02.816518: Steps 40/47: batch_recall = 71.20, batch_ndcg = 62.73 
2025-02-12 23:34:03.783582: Steps 41/47: batch_recall = 86.64, batch_ndcg = 70.50 
2025-02-12 23:34:04.750544: Steps 42/47: batch_recall = 83.52, batch_ndcg = 65.09 
2025-02-12 23:34:05.712076: Steps 43/47: batch_recall = 89.32, batch_ndcg = 70.89 
2025-02-12 23:34:06.672352: Steps 44/47: batch_recall = 88.04, batch_ndcg = 69.86 
2025-02-12 23:34:07.612752: Steps 45/47: batch_recall = 94.44, batch_ndcg = 74.96 
2025-02-12 23:34:07.721230: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 23:34:07.721354: Epoch 45/1000, Test: Recall = 0.1207, NDCG = 0.1083  

2025-02-12 23:34:08.921607: Training Step 0/59: batchLoss = 3.0102, diffLoss = 14.6542, kgLoss = 0.0992
2025-02-12 23:34:09.851642: Training Step 1/59: batchLoss = 2.8588, diffLoss = 13.9576, kgLoss = 0.0841
2025-02-12 23:34:10.793983: Training Step 2/59: batchLoss = 2.6366, diffLoss = 12.8892, kgLoss = 0.0735
2025-02-12 23:34:11.720261: Training Step 3/59: batchLoss = 2.8299, diffLoss = 13.8122, kgLoss = 0.0843
2025-02-12 23:34:12.655385: Training Step 4/59: batchLoss = 2.8886, diffLoss = 14.0791, kgLoss = 0.0910
2025-02-12 23:34:13.596600: Training Step 5/59: batchLoss = 2.6316, diffLoss = 12.8274, kgLoss = 0.0826
2025-02-12 23:34:14.534362: Training Step 6/59: batchLoss = 2.9516, diffLoss = 14.4061, kgLoss = 0.0880
2025-02-12 23:34:15.475458: Training Step 7/59: batchLoss = 2.5480, diffLoss = 12.4292, kgLoss = 0.0778
2025-02-12 23:34:16.409393: Training Step 8/59: batchLoss = 2.7805, diffLoss = 13.5797, kgLoss = 0.0807
2025-02-12 23:34:17.345159: Training Step 9/59: batchLoss = 2.8734, diffLoss = 14.0180, kgLoss = 0.0873
2025-02-12 23:34:18.282228: Training Step 10/59: batchLoss = 2.8211, diffLoss = 13.7716, kgLoss = 0.0835
2025-02-12 23:34:19.224042: Training Step 11/59: batchLoss = 2.9443, diffLoss = 14.3763, kgLoss = 0.0862
2025-02-12 23:34:20.157471: Training Step 12/59: batchLoss = 2.9724, diffLoss = 14.5197, kgLoss = 0.0856
2025-02-12 23:34:21.097597: Training Step 13/59: batchLoss = 3.2140, diffLoss = 15.6968, kgLoss = 0.0933
2025-02-12 23:34:22.036668: Training Step 14/59: batchLoss = 2.7612, diffLoss = 13.4565, kgLoss = 0.0874
2025-02-12 23:34:22.967955: Training Step 15/59: batchLoss = 2.9945, diffLoss = 14.6102, kgLoss = 0.0906
2025-02-12 23:34:23.910789: Training Step 16/59: batchLoss = 2.8373, diffLoss = 13.8637, kgLoss = 0.0807
2025-02-12 23:34:24.841417: Training Step 17/59: batchLoss = 3.0591, diffLoss = 14.9495, kgLoss = 0.0865
2025-02-12 23:34:25.774624: Training Step 18/59: batchLoss = 2.7558, diffLoss = 13.4499, kgLoss = 0.0823
2025-02-12 23:34:26.704434: Training Step 19/59: batchLoss = 3.0554, diffLoss = 14.9034, kgLoss = 0.0935
2025-02-12 23:34:27.633010: Training Step 20/59: batchLoss = 2.8788, diffLoss = 14.0684, kgLoss = 0.0814
2025-02-12 23:34:28.562758: Training Step 21/59: batchLoss = 2.9025, diffLoss = 14.1714, kgLoss = 0.0853
2025-02-12 23:34:29.506749: Training Step 22/59: batchLoss = 2.7873, diffLoss = 13.6090, kgLoss = 0.0819
2025-02-12 23:34:30.438226: Training Step 23/59: batchLoss = 2.9909, diffLoss = 14.6059, kgLoss = 0.0871
2025-02-12 23:34:31.375204: Training Step 24/59: batchLoss = 3.1601, diffLoss = 15.4457, kgLoss = 0.0887
2025-02-12 23:34:32.320949: Training Step 25/59: batchLoss = 2.7702, diffLoss = 13.5253, kgLoss = 0.0814
2025-02-12 23:34:33.256293: Training Step 26/59: batchLoss = 3.0160, diffLoss = 14.6983, kgLoss = 0.0955
2025-02-12 23:34:34.189217: Training Step 27/59: batchLoss = 3.1860, diffLoss = 15.5621, kgLoss = 0.0919
2025-02-12 23:34:35.122457: Training Step 28/59: batchLoss = 3.2399, diffLoss = 15.8045, kgLoss = 0.0987
2025-02-12 23:34:36.064591: Training Step 29/59: batchLoss = 3.0113, diffLoss = 14.7028, kgLoss = 0.0884
2025-02-12 23:34:37.022821: Training Step 30/59: batchLoss = 3.3547, diffLoss = 16.3829, kgLoss = 0.0977
2025-02-12 23:34:37.963781: Training Step 31/59: batchLoss = 3.0468, diffLoss = 14.8855, kgLoss = 0.0871
2025-02-12 23:34:38.902246: Training Step 32/59: batchLoss = 3.2263, diffLoss = 15.7570, kgLoss = 0.0936
2025-02-12 23:34:39.834298: Training Step 33/59: batchLoss = 2.9144, diffLoss = 14.2706, kgLoss = 0.0753
2025-02-12 23:34:40.762039: Training Step 34/59: batchLoss = 2.9811, diffLoss = 14.5678, kgLoss = 0.0844
2025-02-12 23:34:41.695599: Training Step 35/59: batchLoss = 2.6483, diffLoss = 12.9424, kgLoss = 0.0747
2025-02-12 23:34:42.630066: Training Step 36/59: batchLoss = 2.9731, diffLoss = 14.5355, kgLoss = 0.0825
2025-02-12 23:34:43.556213: Training Step 37/59: batchLoss = 2.5499, diffLoss = 12.4269, kgLoss = 0.0806
2025-02-12 23:34:44.490105: Training Step 38/59: batchLoss = 2.9292, diffLoss = 14.3178, kgLoss = 0.0821
2025-02-12 23:34:45.418383: Training Step 39/59: batchLoss = 2.9329, diffLoss = 14.3303, kgLoss = 0.0835
2025-02-12 23:34:46.348480: Training Step 40/59: batchLoss = 2.7748, diffLoss = 13.5442, kgLoss = 0.0824
2025-02-12 23:34:47.272965: Training Step 41/59: batchLoss = 2.8868, diffLoss = 14.1013, kgLoss = 0.0832
2025-02-12 23:34:48.200070: Training Step 42/59: batchLoss = 2.8165, diffLoss = 13.7593, kgLoss = 0.0808
2025-02-12 23:34:49.137951: Training Step 43/59: batchLoss = 2.8864, diffLoss = 14.1032, kgLoss = 0.0822
2025-02-12 23:34:50.070779: Training Step 44/59: batchLoss = 3.1362, diffLoss = 15.3166, kgLoss = 0.0911
2025-02-12 23:34:51.005944: Training Step 45/59: batchLoss = 2.9033, diffLoss = 14.1692, kgLoss = 0.0869
2025-02-12 23:34:51.941661: Training Step 46/59: batchLoss = 2.9246, diffLoss = 14.3044, kgLoss = 0.0796
2025-02-12 23:34:52.884113: Training Step 47/59: batchLoss = 2.9210, diffLoss = 14.2690, kgLoss = 0.0839
2025-02-12 23:34:53.819832: Training Step 48/59: batchLoss = 3.0910, diffLoss = 15.1050, kgLoss = 0.0875
2025-02-12 23:34:54.759869: Training Step 49/59: batchLoss = 2.7096, diffLoss = 13.2389, kgLoss = 0.0772
2025-02-12 23:34:55.699735: Training Step 50/59: batchLoss = 3.0698, diffLoss = 15.0195, kgLoss = 0.0824
2025-02-12 23:34:56.643402: Training Step 51/59: batchLoss = 3.2288, diffLoss = 15.7761, kgLoss = 0.0920
2025-02-12 23:34:57.583129: Training Step 52/59: batchLoss = 3.1152, diffLoss = 15.2507, kgLoss = 0.0813
2025-02-12 23:34:58.511987: Training Step 53/59: batchLoss = 3.3846, diffLoss = 16.5433, kgLoss = 0.0949
2025-02-12 23:34:59.440104: Training Step 54/59: batchLoss = 3.4003, diffLoss = 16.6486, kgLoss = 0.0882
2025-02-12 23:35:00.366481: Training Step 55/59: batchLoss = 3.0964, diffLoss = 15.1258, kgLoss = 0.0890
2025-02-12 23:35:01.289502: Training Step 56/59: batchLoss = 2.7191, diffLoss = 13.2926, kgLoss = 0.0758
2025-02-12 23:35:02.132031: Training Step 57/59: batchLoss = 3.2436, diffLoss = 15.8623, kgLoss = 0.0889
2025-02-12 23:35:02.981156: Training Step 58/59: batchLoss = 3.1475, diffLoss = 15.3840, kgLoss = 0.0884
2025-02-12 23:35:03.083145: 
2025-02-12 23:35:03.083841: Epoch 46/1000, Train: epLoss = 0.4359, epDfLoss = 2.1292, epKgLoss = 0.0126  
2025-02-12 23:35:04.559233: Steps 0/47: batch_recall = 37.11, batch_ndcg = 47.31 
2025-02-12 23:35:05.852980: Steps 1/47: batch_recall = 36.49, batch_ndcg = 41.74 
2025-02-12 23:35:07.109489: Steps 2/47: batch_recall = 40.43, batch_ndcg = 46.44 
2025-02-12 23:35:08.379268: Steps 3/47: batch_recall = 42.26, batch_ndcg = 45.68 
2025-02-12 23:35:09.583564: Steps 4/47: batch_recall = 38.64, batch_ndcg = 44.71 
2025-02-12 23:35:10.811218: Steps 5/47: batch_recall = 32.85, batch_ndcg = 39.28 
2025-02-12 23:35:12.021846: Steps 6/47: batch_recall = 39.20, batch_ndcg = 42.64 
2025-02-12 23:35:13.196861: Steps 7/47: batch_recall = 43.77, batch_ndcg = 44.87 
2025-02-12 23:35:14.380178: Steps 8/47: batch_recall = 48.07, batch_ndcg = 52.51 
2025-02-12 23:35:15.518514: Steps 9/47: batch_recall = 47.29, batch_ndcg = 46.70 
2025-02-12 23:35:16.685709: Steps 10/47: batch_recall = 43.68, batch_ndcg = 43.02 
2025-02-12 23:35:17.820718: Steps 11/47: batch_recall = 54.18, batch_ndcg = 52.01 
2025-02-12 23:35:18.965407: Steps 12/47: batch_recall = 49.04, batch_ndcg = 47.61 
2025-02-12 23:35:20.090719: Steps 13/47: batch_recall = 48.26, batch_ndcg = 45.49 
2025-02-12 23:35:21.177267: Steps 14/47: batch_recall = 39.80, batch_ndcg = 40.06 
2025-02-12 23:35:22.269720: Steps 15/47: batch_recall = 57.98, batch_ndcg = 54.29 
2025-02-12 23:35:23.361221: Steps 16/47: batch_recall = 50.81, batch_ndcg = 47.42 
2025-02-12 23:35:24.426725: Steps 17/47: batch_recall = 57.68, batch_ndcg = 49.59 
2025-02-12 23:35:25.526162: Steps 18/47: batch_recall = 52.66, batch_ndcg = 49.57 
2025-02-12 23:35:26.597736: Steps 19/47: batch_recall = 59.69, batch_ndcg = 54.78 
2025-02-12 23:35:27.643975: Steps 20/47: batch_recall = 66.64, batch_ndcg = 61.06 
2025-02-12 23:35:28.694375: Steps 21/47: batch_recall = 66.46, batch_ndcg = 56.15 
2025-02-12 23:35:29.747308: Steps 22/47: batch_recall = 55.18, batch_ndcg = 51.32 
2025-02-12 23:35:30.802912: Steps 23/47: batch_recall = 63.90, batch_ndcg = 55.19 
2025-02-12 23:35:31.871544: Steps 24/47: batch_recall = 65.36, batch_ndcg = 57.33 
2025-02-12 23:35:32.907605: Steps 25/47: batch_recall = 66.45, batch_ndcg = 57.84 
2025-02-12 23:35:33.927940: Steps 26/47: batch_recall = 59.49, batch_ndcg = 53.17 
2025-02-12 23:35:34.948613: Steps 27/47: batch_recall = 62.57, batch_ndcg = 52.92 
2025-02-12 23:35:35.966004: Steps 28/47: batch_recall = 69.82, batch_ndcg = 59.25 
2025-02-12 23:35:36.980238: Steps 29/47: batch_recall = 70.42, batch_ndcg = 58.29 
2025-02-12 23:35:37.981710: Steps 30/47: batch_recall = 72.87, batch_ndcg = 63.47 
2025-02-12 23:35:39.009627: Steps 31/47: batch_recall = 65.22, batch_ndcg = 54.43 
2025-02-12 23:35:40.019476: Steps 32/47: batch_recall = 71.10, batch_ndcg = 64.60 
2025-02-12 23:35:41.035866: Steps 33/47: batch_recall = 79.14, batch_ndcg = 68.09 
2025-02-12 23:35:42.046946: Steps 34/47: batch_recall = 68.64, batch_ndcg = 55.80 
2025-02-12 23:35:43.033806: Steps 35/47: batch_recall = 75.91, batch_ndcg = 64.39 
2025-02-12 23:35:44.022697: Steps 36/47: batch_recall = 78.36, batch_ndcg = 64.36 
2025-02-12 23:35:45.006059: Steps 37/47: batch_recall = 83.99, batch_ndcg = 71.99 
2025-02-12 23:35:45.995235: Steps 38/47: batch_recall = 91.24, batch_ndcg = 71.25 
2025-02-12 23:35:46.975534: Steps 39/47: batch_recall = 87.54, batch_ndcg = 68.36 
2025-02-12 23:35:47.948766: Steps 40/47: batch_recall = 71.33, batch_ndcg = 61.85 
2025-02-12 23:35:48.909108: Steps 41/47: batch_recall = 86.75, batch_ndcg = 70.68 
2025-02-12 23:35:49.866586: Steps 42/47: batch_recall = 83.66, batch_ndcg = 64.71 
2025-02-12 23:35:50.823200: Steps 43/47: batch_recall = 89.80, batch_ndcg = 72.67 
2025-02-12 23:35:51.787126: Steps 44/47: batch_recall = 87.02, batch_ndcg = 69.19 
2025-02-12 23:35:52.726180: Steps 45/47: batch_recall = 94.60, batch_ndcg = 75.46 
2025-02-12 23:35:52.829175: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 23:35:52.829313: Epoch 46/1000, Test: Recall = 0.1212, NDCG = 0.1087  

2025-02-12 23:35:54.028896: Training Step 0/59: batchLoss = 2.7705, diffLoss = 13.5130, kgLoss = 0.0848
2025-02-12 23:35:54.954879: Training Step 1/59: batchLoss = 2.7756, diffLoss = 13.5317, kgLoss = 0.0866
2025-02-12 23:35:55.882089: Training Step 2/59: batchLoss = 2.7907, diffLoss = 13.6609, kgLoss = 0.0732
2025-02-12 23:35:56.820825: Training Step 3/59: batchLoss = 2.5691, diffLoss = 12.5391, kgLoss = 0.0766
2025-02-12 23:35:57.755422: Training Step 4/59: batchLoss = 2.7781, diffLoss = 13.5135, kgLoss = 0.0942
2025-02-12 23:35:58.689352: Training Step 5/59: batchLoss = 3.0086, diffLoss = 14.6751, kgLoss = 0.0920
2025-02-12 23:35:59.632070: Training Step 6/59: batchLoss = 2.9556, diffLoss = 14.3903, kgLoss = 0.0969
2025-02-12 23:36:00.568519: Training Step 7/59: batchLoss = 2.8653, diffLoss = 14.0012, kgLoss = 0.0813
2025-02-12 23:36:01.506727: Training Step 8/59: batchLoss = 2.6732, diffLoss = 12.9891, kgLoss = 0.0942
2025-02-12 23:36:02.447101: Training Step 9/59: batchLoss = 2.8974, diffLoss = 14.1692, kgLoss = 0.0794
2025-02-12 23:36:03.387143: Training Step 10/59: batchLoss = 2.9356, diffLoss = 14.3125, kgLoss = 0.0914
2025-02-12 23:36:04.321151: Training Step 11/59: batchLoss = 3.0084, diffLoss = 14.6642, kgLoss = 0.0945
2025-02-12 23:36:05.256632: Training Step 12/59: batchLoss = 2.9928, diffLoss = 14.5926, kgLoss = 0.0928
2025-02-12 23:36:06.184719: Training Step 13/59: batchLoss = 2.9809, diffLoss = 14.5395, kgLoss = 0.0912
2025-02-12 23:36:07.111557: Training Step 14/59: batchLoss = 2.9302, diffLoss = 14.3235, kgLoss = 0.0819
2025-02-12 23:36:08.033668: Training Step 15/59: batchLoss = 2.7440, diffLoss = 13.3764, kgLoss = 0.0859
2025-02-12 23:36:08.954246: Training Step 16/59: batchLoss = 2.8899, diffLoss = 14.1079, kgLoss = 0.0855
2025-02-12 23:36:09.884079: Training Step 17/59: batchLoss = 2.7791, diffLoss = 13.5648, kgLoss = 0.0827
2025-02-12 23:36:10.809031: Training Step 18/59: batchLoss = 3.0777, diffLoss = 15.0217, kgLoss = 0.0918
2025-02-12 23:36:11.729130: Training Step 19/59: batchLoss = 2.8268, diffLoss = 13.7880, kgLoss = 0.0865
2025-02-12 23:36:12.655038: Training Step 20/59: batchLoss = 2.9001, diffLoss = 14.1596, kgLoss = 0.0852
2025-02-12 23:36:13.584484: Training Step 21/59: batchLoss = 2.7427, diffLoss = 13.3940, kgLoss = 0.0799
2025-02-12 23:36:14.521880: Training Step 22/59: batchLoss = 2.9282, diffLoss = 14.3109, kgLoss = 0.0825
2025-02-12 23:36:15.464407: Training Step 23/59: batchLoss = 3.1407, diffLoss = 15.3509, kgLoss = 0.0882
2025-02-12 23:36:16.407762: Training Step 24/59: batchLoss = 3.0516, diffLoss = 14.9010, kgLoss = 0.0892
2025-02-12 23:36:17.345268: Training Step 25/59: batchLoss = 3.2089, diffLoss = 15.6559, kgLoss = 0.0972
2025-02-12 23:36:18.279367: Training Step 26/59: batchLoss = 2.7579, diffLoss = 13.4920, kgLoss = 0.0744
2025-02-12 23:36:19.217361: Training Step 27/59: batchLoss = 2.7900, diffLoss = 13.5825, kgLoss = 0.0919
2025-02-12 23:36:20.161017: Training Step 28/59: batchLoss = 2.6038, diffLoss = 12.7008, kgLoss = 0.0795
2025-02-12 23:36:21.102400: Training Step 29/59: batchLoss = 3.5554, diffLoss = 17.3819, kgLoss = 0.0987
2025-02-12 23:36:22.046934: Training Step 30/59: batchLoss = 3.4371, diffLoss = 16.7923, kgLoss = 0.0983
2025-02-12 23:36:22.978302: Training Step 31/59: batchLoss = 2.7391, diffLoss = 13.3770, kgLoss = 0.0796
2025-02-12 23:36:23.910805: Training Step 32/59: batchLoss = 2.8074, diffLoss = 13.7035, kgLoss = 0.0834
2025-02-12 23:36:24.840277: Training Step 33/59: batchLoss = 3.0367, diffLoss = 14.8168, kgLoss = 0.0917
2025-02-12 23:36:25.777314: Training Step 34/59: batchLoss = 2.8911, diffLoss = 14.1215, kgLoss = 0.0835
2025-02-12 23:36:26.706593: Training Step 35/59: batchLoss = 2.8451, diffLoss = 13.8934, kgLoss = 0.0830
2025-02-12 23:36:27.635284: Training Step 36/59: batchLoss = 3.0564, diffLoss = 14.9269, kgLoss = 0.0888
2025-02-12 23:36:28.575871: Training Step 37/59: batchLoss = 2.7657, diffLoss = 13.5282, kgLoss = 0.0751
2025-02-12 23:36:29.498947: Training Step 38/59: batchLoss = 3.1741, diffLoss = 15.4950, kgLoss = 0.0939
2025-02-12 23:36:30.426562: Training Step 39/59: batchLoss = 3.1556, diffLoss = 15.4409, kgLoss = 0.0843
2025-02-12 23:36:31.361320: Training Step 40/59: batchLoss = 3.1065, diffLoss = 15.1803, kgLoss = 0.0881
2025-02-12 23:36:32.307131: Training Step 41/59: batchLoss = 2.7848, diffLoss = 13.5944, kgLoss = 0.0824
2025-02-12 23:36:33.244722: Training Step 42/59: batchLoss = 3.2225, diffLoss = 15.7357, kgLoss = 0.0942
2025-02-12 23:36:34.188569: Training Step 43/59: batchLoss = 2.8609, diffLoss = 13.9835, kgLoss = 0.0803
2025-02-12 23:36:35.124112: Training Step 44/59: batchLoss = 2.9343, diffLoss = 14.3494, kgLoss = 0.0805
2025-02-12 23:36:36.063078: Training Step 45/59: batchLoss = 2.8354, diffLoss = 13.8444, kgLoss = 0.0831
2025-02-12 23:36:37.015696: Training Step 46/59: batchLoss = 2.9419, diffLoss = 14.3891, kgLoss = 0.0801
2025-02-12 23:36:37.962709: Training Step 47/59: batchLoss = 2.6909, diffLoss = 13.1546, kgLoss = 0.0750
2025-02-12 23:36:38.903053: Training Step 48/59: batchLoss = 3.0008, diffLoss = 14.6496, kgLoss = 0.0887
2025-02-12 23:36:39.837546: Training Step 49/59: batchLoss = 2.9360, diffLoss = 14.3578, kgLoss = 0.0806
2025-02-12 23:36:40.767556: Training Step 50/59: batchLoss = 3.2266, diffLoss = 15.7576, kgLoss = 0.0938
2025-02-12 23:36:41.694781: Training Step 51/59: batchLoss = 3.1702, diffLoss = 15.5118, kgLoss = 0.0848
2025-02-12 23:36:42.631534: Training Step 52/59: batchLoss = 2.6295, diffLoss = 12.8513, kgLoss = 0.0741
2025-02-12 23:36:43.561250: Training Step 53/59: batchLoss = 2.9555, diffLoss = 14.4473, kgLoss = 0.0826
2025-02-12 23:36:44.492014: Training Step 54/59: batchLoss = 3.0372, diffLoss = 14.8669, kgLoss = 0.0798
2025-02-12 23:36:45.423005: Training Step 55/59: batchLoss = 3.1193, diffLoss = 15.2400, kgLoss = 0.0891
2025-02-12 23:36:46.345026: Training Step 56/59: batchLoss = 2.4577, diffLoss = 12.0205, kgLoss = 0.0670
2025-02-12 23:36:47.188500: Training Step 57/59: batchLoss = 3.0886, diffLoss = 15.0724, kgLoss = 0.0927
2025-02-12 23:36:48.044234: Training Step 58/59: batchLoss = 3.1774, diffLoss = 15.5407, kgLoss = 0.0866
2025-02-12 23:36:48.145485: 
2025-02-12 23:36:48.145995: Epoch 47/1000, Train: epLoss = 0.4325, epDfLoss = 2.1121, epKgLoss = 0.0126  
2025-02-12 23:36:49.638111: Steps 0/47: batch_recall = 37.30, batch_ndcg = 47.48 
2025-02-12 23:36:50.950757: Steps 1/47: batch_recall = 36.81, batch_ndcg = 42.11 
2025-02-12 23:36:52.213889: Steps 2/47: batch_recall = 39.83, batch_ndcg = 46.07 
2025-02-12 23:36:53.488202: Steps 3/47: batch_recall = 43.35, batch_ndcg = 45.30 
2025-02-12 23:36:54.704098: Steps 4/47: batch_recall = 38.62, batch_ndcg = 44.63 
2025-02-12 23:36:55.916048: Steps 5/47: batch_recall = 32.56, batch_ndcg = 39.75 
2025-02-12 23:36:57.124564: Steps 6/47: batch_recall = 38.83, batch_ndcg = 41.92 
2025-02-12 23:36:58.295268: Steps 7/47: batch_recall = 43.93, batch_ndcg = 45.83 
2025-02-12 23:36:59.467565: Steps 8/47: batch_recall = 47.69, batch_ndcg = 51.72 
2025-02-12 23:37:00.603402: Steps 9/47: batch_recall = 45.76, batch_ndcg = 46.46 
2025-02-12 23:37:01.783561: Steps 10/47: batch_recall = 44.73, batch_ndcg = 44.21 
2025-02-12 23:37:02.920611: Steps 11/47: batch_recall = 55.09, batch_ndcg = 51.87 
2025-02-12 23:37:04.062055: Steps 12/47: batch_recall = 49.34, batch_ndcg = 48.60 
2025-02-12 23:37:05.207042: Steps 13/47: batch_recall = 48.66, batch_ndcg = 45.78 
2025-02-12 23:37:06.311037: Steps 14/47: batch_recall = 40.57, batch_ndcg = 40.60 
2025-02-12 23:37:07.414948: Steps 15/47: batch_recall = 56.98, batch_ndcg = 53.78 
2025-02-12 23:37:08.508989: Steps 16/47: batch_recall = 49.50, batch_ndcg = 46.33 
2025-02-12 23:37:09.573304: Steps 17/47: batch_recall = 57.93, batch_ndcg = 49.71 
2025-02-12 23:37:10.655761: Steps 18/47: batch_recall = 52.80, batch_ndcg = 49.44 
2025-02-12 23:37:11.729476: Steps 19/47: batch_recall = 60.28, batch_ndcg = 54.83 
2025-02-12 23:37:12.784121: Steps 20/47: batch_recall = 67.45, batch_ndcg = 62.04 
2025-02-12 23:37:13.913093: Steps 21/47: batch_recall = 65.60, batch_ndcg = 55.68 
2025-02-12 23:37:14.961617: Steps 22/47: batch_recall = 54.74, batch_ndcg = 51.12 
2025-02-12 23:37:16.014960: Steps 23/47: batch_recall = 64.30, batch_ndcg = 55.14 
2025-02-12 23:37:17.074922: Steps 24/47: batch_recall = 63.72, batch_ndcg = 57.10 
2025-02-12 23:37:18.109860: Steps 25/47: batch_recall = 67.36, batch_ndcg = 58.42 
2025-02-12 23:37:19.124463: Steps 26/47: batch_recall = 59.53, batch_ndcg = 52.52 
2025-02-12 23:37:20.155404: Steps 27/47: batch_recall = 61.79, batch_ndcg = 51.90 
2025-02-12 23:37:21.173008: Steps 28/47: batch_recall = 69.47, batch_ndcg = 58.94 
2025-02-12 23:37:22.191571: Steps 29/47: batch_recall = 69.21, batch_ndcg = 57.52 
2025-02-12 23:37:23.208561: Steps 30/47: batch_recall = 73.09, batch_ndcg = 63.80 
2025-02-12 23:37:24.242890: Steps 31/47: batch_recall = 66.76, batch_ndcg = 55.98 
2025-02-12 23:37:25.258839: Steps 32/47: batch_recall = 71.37, batch_ndcg = 65.03 
2025-02-12 23:37:26.271221: Steps 33/47: batch_recall = 80.08, batch_ndcg = 67.56 
2025-02-12 23:37:27.279797: Steps 34/47: batch_recall = 67.01, batch_ndcg = 55.87 
2025-02-12 23:37:28.279990: Steps 35/47: batch_recall = 77.18, batch_ndcg = 64.75 
2025-02-12 23:37:29.274041: Steps 36/47: batch_recall = 78.41, batch_ndcg = 64.11 
2025-02-12 23:37:30.252858: Steps 37/47: batch_recall = 83.56, batch_ndcg = 72.35 
2025-02-12 23:37:31.231460: Steps 38/47: batch_recall = 89.27, batch_ndcg = 71.12 
2025-02-12 23:37:32.198998: Steps 39/47: batch_recall = 86.69, batch_ndcg = 68.48 
2025-02-12 23:37:33.170288: Steps 40/47: batch_recall = 71.94, batch_ndcg = 62.23 
2025-02-12 23:37:34.135647: Steps 41/47: batch_recall = 85.85, batch_ndcg = 70.80 
2025-02-12 23:37:35.089123: Steps 42/47: batch_recall = 80.62, batch_ndcg = 63.43 
2025-02-12 23:37:36.050372: Steps 43/47: batch_recall = 89.96, batch_ndcg = 72.98 
2025-02-12 23:37:37.011048: Steps 44/47: batch_recall = 87.64, batch_ndcg = 70.00 
2025-02-12 23:37:37.947721: Steps 45/47: batch_recall = 95.09, batch_ndcg = 74.55 
2025-02-12 23:37:38.050007: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.61 
2025-02-12 23:37:38.050138: Epoch 47/1000, Test: Recall = 0.1209, NDCG = 0.1087  

2025-02-12 23:37:39.243306: Training Step 0/59: batchLoss = 3.0148, diffLoss = 14.6999, kgLoss = 0.0936
2025-02-12 23:37:40.185841: Training Step 1/59: batchLoss = 2.7886, diffLoss = 13.5958, kgLoss = 0.0868
2025-02-12 23:37:41.124453: Training Step 2/59: batchLoss = 2.8678, diffLoss = 13.9952, kgLoss = 0.0859
2025-02-12 23:37:42.067140: Training Step 3/59: batchLoss = 2.9526, diffLoss = 14.3976, kgLoss = 0.0913
2025-02-12 23:37:43.005571: Training Step 4/59: batchLoss = 3.0495, diffLoss = 14.8725, kgLoss = 0.0938
2025-02-12 23:37:43.937036: Training Step 5/59: batchLoss = 3.1490, diffLoss = 15.3688, kgLoss = 0.0940
2025-02-12 23:37:44.874415: Training Step 6/59: batchLoss = 3.0492, diffLoss = 14.8917, kgLoss = 0.0886
2025-02-12 23:37:45.813498: Training Step 7/59: batchLoss = 2.6020, diffLoss = 12.6968, kgLoss = 0.0783
2025-02-12 23:37:46.751793: Training Step 8/59: batchLoss = 2.6620, diffLoss = 12.9954, kgLoss = 0.0787
2025-02-12 23:37:47.692169: Training Step 9/59: batchLoss = 3.0932, diffLoss = 15.1076, kgLoss = 0.0896
2025-02-12 23:37:48.636359: Training Step 10/59: batchLoss = 3.2205, diffLoss = 15.7181, kgLoss = 0.0960
2025-02-12 23:37:49.563533: Training Step 11/59: batchLoss = 2.8723, diffLoss = 14.0266, kgLoss = 0.0838
2025-02-12 23:37:50.489821: Training Step 12/59: batchLoss = 3.1239, diffLoss = 15.2299, kgLoss = 0.0974
2025-02-12 23:37:51.415556: Training Step 13/59: batchLoss = 2.9416, diffLoss = 14.3798, kgLoss = 0.0821
2025-02-12 23:37:52.335551: Training Step 14/59: batchLoss = 2.9464, diffLoss = 14.3765, kgLoss = 0.0889
2025-02-12 23:37:53.259548: Training Step 15/59: batchLoss = 2.7213, diffLoss = 13.2799, kgLoss = 0.0816
2025-02-12 23:37:54.183586: Training Step 16/59: batchLoss = 2.8594, diffLoss = 13.9609, kgLoss = 0.0840
2025-02-12 23:37:55.108112: Training Step 17/59: batchLoss = 2.9153, diffLoss = 14.2339, kgLoss = 0.0857
2025-02-12 23:37:56.045790: Training Step 18/59: batchLoss = 2.6835, diffLoss = 13.0689, kgLoss = 0.0871
2025-02-12 23:37:56.979823: Training Step 19/59: batchLoss = 3.5736, diffLoss = 17.4341, kgLoss = 0.1085
2025-02-12 23:37:57.921714: Training Step 20/59: batchLoss = 2.8739, diffLoss = 14.0516, kgLoss = 0.0795
2025-02-12 23:37:58.865358: Training Step 21/59: batchLoss = 3.1795, diffLoss = 15.5145, kgLoss = 0.0958
2025-02-12 23:37:59.808860: Training Step 22/59: batchLoss = 3.2857, diffLoss = 16.0270, kgLoss = 0.1004
2025-02-12 23:38:00.752968: Training Step 23/59: batchLoss = 2.5883, diffLoss = 12.6503, kgLoss = 0.0728
2025-02-12 23:38:01.688947: Training Step 24/59: batchLoss = 3.2380, diffLoss = 15.8294, kgLoss = 0.0901
2025-02-12 23:38:02.627903: Training Step 25/59: batchLoss = 2.9304, diffLoss = 14.3089, kgLoss = 0.0858
2025-02-12 23:38:03.564241: Training Step 26/59: batchLoss = 2.8582, diffLoss = 13.9682, kgLoss = 0.0807
2025-02-12 23:38:04.502574: Training Step 27/59: batchLoss = 3.0779, diffLoss = 15.0297, kgLoss = 0.0899
2025-02-12 23:38:05.439132: Training Step 28/59: batchLoss = 2.8677, diffLoss = 13.9927, kgLoss = 0.0864
2025-02-12 23:38:06.375673: Training Step 29/59: batchLoss = 3.1574, diffLoss = 15.4183, kgLoss = 0.0922
2025-02-12 23:38:07.321327: Training Step 30/59: batchLoss = 2.8054, diffLoss = 13.7004, kgLoss = 0.0817
2025-02-12 23:38:08.249989: Training Step 31/59: batchLoss = 3.0687, diffLoss = 14.9866, kgLoss = 0.0892
2025-02-12 23:38:09.178471: Training Step 32/59: batchLoss = 2.6389, diffLoss = 12.8810, kgLoss = 0.0783
2025-02-12 23:38:10.110044: Training Step 33/59: batchLoss = 2.8431, diffLoss = 13.9023, kgLoss = 0.0783
2025-02-12 23:38:11.034848: Training Step 34/59: batchLoss = 2.7058, diffLoss = 13.2331, kgLoss = 0.0739
2025-02-12 23:38:11.962815: Training Step 35/59: batchLoss = 2.8393, diffLoss = 13.8545, kgLoss = 0.0855
2025-02-12 23:38:12.890898: Training Step 36/59: batchLoss = 2.7995, diffLoss = 13.6943, kgLoss = 0.0758
2025-02-12 23:38:13.830685: Training Step 37/59: batchLoss = 3.2199, diffLoss = 15.7536, kgLoss = 0.0865
2025-02-12 23:38:14.761521: Training Step 38/59: batchLoss = 2.8346, diffLoss = 13.8538, kgLoss = 0.0798
2025-02-12 23:38:15.699559: Training Step 39/59: batchLoss = 2.6005, diffLoss = 12.6853, kgLoss = 0.0793
2025-02-12 23:38:16.636884: Training Step 40/59: batchLoss = 2.8549, diffLoss = 13.9451, kgLoss = 0.0824
2025-02-12 23:38:17.570528: Training Step 41/59: batchLoss = 3.1193, diffLoss = 15.2346, kgLoss = 0.0905
2025-02-12 23:38:18.516698: Training Step 42/59: batchLoss = 2.9354, diffLoss = 14.3384, kgLoss = 0.0847
2025-02-12 23:38:19.454389: Training Step 43/59: batchLoss = 2.9921, diffLoss = 14.6372, kgLoss = 0.0808
2025-02-12 23:38:20.388171: Training Step 44/59: batchLoss = 3.2205, diffLoss = 15.7451, kgLoss = 0.0893
2025-02-12 23:38:21.333037: Training Step 45/59: batchLoss = 2.9090, diffLoss = 14.2297, kgLoss = 0.0788
2025-02-12 23:38:22.277786: Training Step 46/59: batchLoss = 3.0154, diffLoss = 14.7375, kgLoss = 0.0849
2025-02-12 23:38:23.212196: Training Step 47/59: batchLoss = 2.8371, diffLoss = 13.8620, kgLoss = 0.0808
2025-02-12 23:38:24.140453: Training Step 48/59: batchLoss = 2.9741, diffLoss = 14.5231, kgLoss = 0.0869
2025-02-12 23:38:25.058189: Training Step 49/59: batchLoss = 2.7755, diffLoss = 13.5539, kgLoss = 0.0809
2025-02-12 23:38:25.982344: Training Step 50/59: batchLoss = 3.0557, diffLoss = 14.9243, kgLoss = 0.0886
2025-02-12 23:38:26.908164: Training Step 51/59: batchLoss = 3.1754, diffLoss = 15.5327, kgLoss = 0.0861
2025-02-12 23:38:27.834244: Training Step 52/59: batchLoss = 2.8048, diffLoss = 13.6875, kgLoss = 0.0841
2025-02-12 23:38:28.763382: Training Step 53/59: batchLoss = 2.7276, diffLoss = 13.3061, kgLoss = 0.0830
2025-02-12 23:38:29.690773: Training Step 54/59: batchLoss = 2.6081, diffLoss = 12.7654, kgLoss = 0.0688
2025-02-12 23:38:30.621606: Training Step 55/59: batchLoss = 2.9515, diffLoss = 14.4449, kgLoss = 0.0781
2025-02-12 23:38:31.547611: Training Step 56/59: batchLoss = 3.2285, diffLoss = 15.7566, kgLoss = 0.0965
2025-02-12 23:38:32.394729: Training Step 57/59: batchLoss = 2.9794, diffLoss = 14.5533, kgLoss = 0.0859
2025-02-12 23:38:33.246105: Training Step 58/59: batchLoss = 3.0701, diffLoss = 15.0057, kgLoss = 0.0862
2025-02-12 23:38:33.348037: 
2025-02-12 23:38:33.348759: Epoch 48/1000, Train: epLoss = 0.4343, epDfLoss = 2.1211, epKgLoss = 0.0126  
2025-02-12 23:38:34.842540: Steps 0/47: batch_recall = 37.27, batch_ndcg = 47.79 
2025-02-12 23:38:36.139589: Steps 1/47: batch_recall = 36.21, batch_ndcg = 41.98 
2025-02-12 23:38:37.400394: Steps 2/47: batch_recall = 40.91, batch_ndcg = 46.16 
2025-02-12 23:38:38.658934: Steps 3/47: batch_recall = 43.48, batch_ndcg = 45.92 
2025-02-12 23:38:39.861530: Steps 4/47: batch_recall = 39.35, batch_ndcg = 45.51 
2025-02-12 23:38:41.071499: Steps 5/47: batch_recall = 32.30, batch_ndcg = 38.48 
2025-02-12 23:38:42.256078: Steps 6/47: batch_recall = 38.48, batch_ndcg = 41.98 
2025-02-12 23:38:43.411987: Steps 7/47: batch_recall = 43.65, batch_ndcg = 44.72 
2025-02-12 23:38:44.589165: Steps 8/47: batch_recall = 48.36, batch_ndcg = 51.89 
2025-02-12 23:38:45.721336: Steps 9/47: batch_recall = 46.33, batch_ndcg = 46.41 
2025-02-12 23:38:46.880828: Steps 10/47: batch_recall = 44.30, batch_ndcg = 43.92 
2025-02-12 23:38:48.018255: Steps 11/47: batch_recall = 54.52, batch_ndcg = 51.84 
2025-02-12 23:38:49.168975: Steps 12/47: batch_recall = 48.35, batch_ndcg = 48.28 
2025-02-12 23:38:50.305999: Steps 13/47: batch_recall = 49.08, batch_ndcg = 45.94 
2025-02-12 23:38:51.417994: Steps 14/47: batch_recall = 39.91, batch_ndcg = 40.24 
2025-02-12 23:38:52.521870: Steps 15/47: batch_recall = 56.52, batch_ndcg = 54.00 
2025-02-12 23:38:53.608104: Steps 16/47: batch_recall = 50.19, batch_ndcg = 45.76 
2025-02-12 23:38:54.661715: Steps 17/47: batch_recall = 58.56, batch_ndcg = 49.70 
2025-02-12 23:38:55.761734: Steps 18/47: batch_recall = 53.31, batch_ndcg = 49.78 
2025-02-12 23:38:56.841887: Steps 19/47: batch_recall = 60.06, batch_ndcg = 55.65 
2025-02-12 23:38:57.881608: Steps 20/47: batch_recall = 67.85, batch_ndcg = 63.00 
2025-02-12 23:38:58.927817: Steps 21/47: batch_recall = 66.48, batch_ndcg = 56.12 
2025-02-12 23:38:59.985527: Steps 22/47: batch_recall = 55.10, batch_ndcg = 51.23 
2025-02-12 23:39:01.026768: Steps 23/47: batch_recall = 63.95, batch_ndcg = 55.23 
2025-02-12 23:39:02.080476: Steps 24/47: batch_recall = 65.11, batch_ndcg = 57.84 
2025-02-12 23:39:03.115952: Steps 25/47: batch_recall = 66.69, batch_ndcg = 57.91 
2025-02-12 23:39:04.126003: Steps 26/47: batch_recall = 60.65, batch_ndcg = 52.64 
2025-02-12 23:39:05.171910: Steps 27/47: batch_recall = 60.24, batch_ndcg = 50.97 
2025-02-12 23:39:06.201400: Steps 28/47: batch_recall = 69.39, batch_ndcg = 58.66 
2025-02-12 23:39:07.224979: Steps 29/47: batch_recall = 69.28, batch_ndcg = 57.80 
2025-02-12 23:39:08.240976: Steps 30/47: batch_recall = 73.05, batch_ndcg = 64.33 
2025-02-12 23:39:09.287447: Steps 31/47: batch_recall = 65.38, batch_ndcg = 54.78 
2025-02-12 23:39:10.302602: Steps 32/47: batch_recall = 71.09, batch_ndcg = 64.95 
2025-02-12 23:39:11.319574: Steps 33/47: batch_recall = 78.78, batch_ndcg = 67.89 
2025-02-12 23:39:12.334587: Steps 34/47: batch_recall = 66.42, batch_ndcg = 54.87 
2025-02-12 23:39:13.323995: Steps 35/47: batch_recall = 76.78, batch_ndcg = 64.87 
2025-02-12 23:39:14.305441: Steps 36/47: batch_recall = 78.90, batch_ndcg = 64.47 
2025-02-12 23:39:15.281454: Steps 37/47: batch_recall = 83.78, batch_ndcg = 73.39 
2025-02-12 23:39:16.247711: Steps 38/47: batch_recall = 89.88, batch_ndcg = 71.67 
2025-02-12 23:39:17.223675: Steps 39/47: batch_recall = 87.64, batch_ndcg = 68.43 
2025-02-12 23:39:18.183800: Steps 40/47: batch_recall = 71.59, batch_ndcg = 62.27 
2025-02-12 23:39:19.138366: Steps 41/47: batch_recall = 87.88, batch_ndcg = 72.01 
2025-02-12 23:39:20.087442: Steps 42/47: batch_recall = 82.26, batch_ndcg = 63.87 
2025-02-12 23:39:21.046027: Steps 43/47: batch_recall = 90.06, batch_ndcg = 72.08 
2025-02-12 23:39:22.012702: Steps 44/47: batch_recall = 87.02, batch_ndcg = 69.35 
2025-02-12 23:39:22.959056: Steps 45/47: batch_recall = 95.75, batch_ndcg = 74.94 
2025-02-12 23:39:23.065268: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.62 
2025-02-12 23:39:23.065403: Epoch 48/1000, Test: Recall = 0.1211, NDCG = 0.1088  

2025-02-12 23:39:24.277960: Training Step 0/59: batchLoss = 2.6680, diffLoss = 13.0080, kgLoss = 0.0830
2025-02-12 23:39:25.218227: Training Step 1/59: batchLoss = 2.5277, diffLoss = 12.2856, kgLoss = 0.0883
2025-02-12 23:39:26.158313: Training Step 2/59: batchLoss = 2.9887, diffLoss = 14.5657, kgLoss = 0.0945
2025-02-12 23:39:27.097952: Training Step 3/59: batchLoss = 2.9659, diffLoss = 14.5030, kgLoss = 0.0816
2025-02-12 23:39:28.038890: Training Step 4/59: batchLoss = 3.0595, diffLoss = 14.9165, kgLoss = 0.0952
2025-02-12 23:39:28.975303: Training Step 5/59: batchLoss = 2.9594, diffLoss = 14.4651, kgLoss = 0.0830
2025-02-12 23:39:29.914530: Training Step 6/59: batchLoss = 3.0886, diffLoss = 15.0851, kgLoss = 0.0895
2025-02-12 23:39:30.850301: Training Step 7/59: batchLoss = 3.0009, diffLoss = 14.6492, kgLoss = 0.0888
2025-02-12 23:39:31.780965: Training Step 8/59: batchLoss = 2.8061, diffLoss = 13.6668, kgLoss = 0.0910
2025-02-12 23:39:32.714648: Training Step 9/59: batchLoss = 2.9629, diffLoss = 14.4500, kgLoss = 0.0911
2025-02-12 23:39:33.645328: Training Step 10/59: batchLoss = 2.7803, diffLoss = 13.5574, kgLoss = 0.0861
2025-02-12 23:39:34.571976: Training Step 11/59: batchLoss = 3.0558, diffLoss = 14.9085, kgLoss = 0.0926
2025-02-12 23:39:35.504179: Training Step 12/59: batchLoss = 2.8758, diffLoss = 14.0254, kgLoss = 0.0884
2025-02-12 23:39:36.438490: Training Step 13/59: batchLoss = 2.9556, diffLoss = 14.4127, kgLoss = 0.0913
2025-02-12 23:39:37.368466: Training Step 14/59: batchLoss = 2.9652, diffLoss = 14.4486, kgLoss = 0.0943
2025-02-12 23:39:38.300969: Training Step 15/59: batchLoss = 2.9215, diffLoss = 14.2300, kgLoss = 0.0944
2025-02-12 23:39:39.231233: Training Step 16/59: batchLoss = 2.8834, diffLoss = 14.0981, kgLoss = 0.0797
2025-02-12 23:39:40.163461: Training Step 17/59: batchLoss = 2.8600, diffLoss = 13.9884, kgLoss = 0.0779
2025-02-12 23:39:41.100421: Training Step 18/59: batchLoss = 2.8065, diffLoss = 13.7189, kgLoss = 0.0784
2025-02-12 23:39:42.037978: Training Step 19/59: batchLoss = 3.2439, diffLoss = 15.8639, kgLoss = 0.0889
2025-02-12 23:39:42.974496: Training Step 20/59: batchLoss = 3.6413, diffLoss = 17.7791, kgLoss = 0.1069
2025-02-12 23:39:43.918245: Training Step 21/59: batchLoss = 2.5876, diffLoss = 12.6501, kgLoss = 0.0720
2025-02-12 23:39:44.853602: Training Step 22/59: batchLoss = 2.5780, diffLoss = 12.5844, kgLoss = 0.0764
2025-02-12 23:39:45.794117: Training Step 23/59: batchLoss = 2.8229, diffLoss = 13.8164, kgLoss = 0.0746
2025-02-12 23:39:46.737104: Training Step 24/59: batchLoss = 2.8211, diffLoss = 13.7921, kgLoss = 0.0784
2025-02-12 23:39:47.669638: Training Step 25/59: batchLoss = 3.0872, diffLoss = 15.0947, kgLoss = 0.0854
2025-02-12 23:39:48.606089: Training Step 26/59: batchLoss = 2.7184, diffLoss = 13.2513, kgLoss = 0.0851
2025-02-12 23:39:49.541817: Training Step 27/59: batchLoss = 2.7116, diffLoss = 13.2556, kgLoss = 0.0755
2025-02-12 23:39:50.483794: Training Step 28/59: batchLoss = 2.9790, diffLoss = 14.5648, kgLoss = 0.0825
2025-02-12 23:39:51.406812: Training Step 29/59: batchLoss = 2.8674, diffLoss = 13.9893, kgLoss = 0.0869
2025-02-12 23:39:52.337074: Training Step 30/59: batchLoss = 2.8323, diffLoss = 13.8178, kgLoss = 0.0859
2025-02-12 23:39:53.265806: Training Step 31/59: batchLoss = 2.6884, diffLoss = 13.1185, kgLoss = 0.0809
2025-02-12 23:39:54.201358: Training Step 32/59: batchLoss = 3.0215, diffLoss = 14.7738, kgLoss = 0.0834
2025-02-12 23:39:55.133306: Training Step 33/59: batchLoss = 3.0460, diffLoss = 14.8749, kgLoss = 0.0888
2025-02-12 23:39:56.062772: Training Step 34/59: batchLoss = 2.8497, diffLoss = 13.9308, kgLoss = 0.0795
2025-02-12 23:39:56.995270: Training Step 35/59: batchLoss = 2.9650, diffLoss = 14.4576, kgLoss = 0.0919
2025-02-12 23:39:57.932104: Training Step 36/59: batchLoss = 2.9632, diffLoss = 14.4803, kgLoss = 0.0840
2025-02-12 23:39:58.864388: Training Step 37/59: batchLoss = 2.8185, diffLoss = 13.7918, kgLoss = 0.0752
2025-02-12 23:39:59.785547: Training Step 38/59: batchLoss = 2.9071, diffLoss = 14.1702, kgLoss = 0.0913
2025-02-12 23:40:00.714727: Training Step 39/59: batchLoss = 3.5259, diffLoss = 17.2263, kgLoss = 0.1008
2025-02-12 23:40:01.655266: Training Step 40/59: batchLoss = 2.9477, diffLoss = 14.3939, kgLoss = 0.0861
2025-02-12 23:40:02.594082: Training Step 41/59: batchLoss = 3.0525, diffLoss = 14.9209, kgLoss = 0.0854
2025-02-12 23:40:03.512948: Training Step 42/59: batchLoss = 3.2155, diffLoss = 15.7090, kgLoss = 0.0922
2025-02-12 23:40:04.448638: Training Step 43/59: batchLoss = 3.0205, diffLoss = 14.7551, kgLoss = 0.0869
2025-02-12 23:40:05.378958: Training Step 44/59: batchLoss = 2.9794, diffLoss = 14.5643, kgLoss = 0.0832
2025-02-12 23:40:06.297394: Training Step 45/59: batchLoss = 3.0690, diffLoss = 14.9871, kgLoss = 0.0894
2025-02-12 23:40:07.221758: Training Step 46/59: batchLoss = 2.9650, diffLoss = 14.4794, kgLoss = 0.0864
2025-02-12 23:40:08.142202: Training Step 47/59: batchLoss = 2.9857, diffLoss = 14.6052, kgLoss = 0.0808
2025-02-12 23:40:09.069395: Training Step 48/59: batchLoss = 3.2610, diffLoss = 15.9416, kgLoss = 0.0909
2025-02-12 23:40:09.989648: Training Step 49/59: batchLoss = 2.9163, diffLoss = 14.2654, kgLoss = 0.0790
2025-02-12 23:40:10.917583: Training Step 50/59: batchLoss = 3.0968, diffLoss = 15.1592, kgLoss = 0.0812
2025-02-12 23:40:11.845623: Training Step 51/59: batchLoss = 3.0799, diffLoss = 15.0618, kgLoss = 0.0844
2025-02-12 23:40:12.771970: Training Step 52/59: batchLoss = 3.0966, diffLoss = 15.1144, kgLoss = 0.0922
2025-02-12 23:40:13.697965: Training Step 53/59: batchLoss = 2.6201, diffLoss = 12.8046, kgLoss = 0.0739
2025-02-12 23:40:14.627641: Training Step 54/59: batchLoss = 2.8475, diffLoss = 13.9204, kgLoss = 0.0793
2025-02-12 23:40:15.561980: Training Step 55/59: batchLoss = 2.8857, diffLoss = 14.0763, kgLoss = 0.0880
2025-02-12 23:40:16.506458: Training Step 56/59: batchLoss = 3.0549, diffLoss = 14.9526, kgLoss = 0.0805
2025-02-12 23:40:17.354567: Training Step 57/59: batchLoss = 3.0853, diffLoss = 15.0804, kgLoss = 0.0865
2025-02-12 23:40:18.216511: Training Step 58/59: batchLoss = 3.0320, diffLoss = 14.8243, kgLoss = 0.0839
2025-02-12 23:40:18.314472: 
2025-02-12 23:40:18.314867: Epoch 49/1000, Train: epLoss = 0.4350, epDfLoss = 2.1247, epKgLoss = 0.0126  
2025-02-12 23:40:19.802290: Steps 0/47: batch_recall = 37.15, batch_ndcg = 47.44 
2025-02-12 23:40:21.117341: Steps 1/47: batch_recall = 36.02, batch_ndcg = 42.14 
2025-02-12 23:40:22.422348: Steps 2/47: batch_recall = 41.30, batch_ndcg = 46.48 
2025-02-12 23:40:23.696815: Steps 3/47: batch_recall = 43.42, batch_ndcg = 45.42 
2025-02-12 23:40:24.880760: Steps 4/47: batch_recall = 39.36, batch_ndcg = 45.82 
2025-02-12 23:40:26.079746: Steps 5/47: batch_recall = 33.10, batch_ndcg = 39.19 
2025-02-12 23:40:27.277386: Steps 6/47: batch_recall = 37.77, batch_ndcg = 41.22 
2025-02-12 23:40:28.450034: Steps 7/47: batch_recall = 43.64, batch_ndcg = 45.04 
2025-02-12 23:40:29.618235: Steps 8/47: batch_recall = 49.31, batch_ndcg = 52.30 
2025-02-12 23:40:30.749258: Steps 9/47: batch_recall = 46.42, batch_ndcg = 46.73 
2025-02-12 23:40:31.914506: Steps 10/47: batch_recall = 42.82, batch_ndcg = 43.69 
2025-02-12 23:40:33.059800: Steps 11/47: batch_recall = 53.30, batch_ndcg = 51.95 
2025-02-12 23:40:34.212814: Steps 12/47: batch_recall = 49.62, batch_ndcg = 48.59 
2025-02-12 23:40:35.393479: Steps 13/47: batch_recall = 48.22, batch_ndcg = 45.29 
2025-02-12 23:40:36.492896: Steps 14/47: batch_recall = 40.59, batch_ndcg = 41.06 
2025-02-12 23:40:37.592919: Steps 15/47: batch_recall = 58.71, batch_ndcg = 54.82 
2025-02-12 23:40:38.681939: Steps 16/47: batch_recall = 49.42, batch_ndcg = 45.37 
2025-02-12 23:40:39.750406: Steps 17/47: batch_recall = 57.49, batch_ndcg = 49.51 
2025-02-12 23:40:40.840013: Steps 18/47: batch_recall = 54.49, batch_ndcg = 50.03 
2025-02-12 23:40:41.904916: Steps 19/47: batch_recall = 60.27, batch_ndcg = 55.24 
2025-02-12 23:40:42.947457: Steps 20/47: batch_recall = 66.71, batch_ndcg = 61.64 
2025-02-12 23:40:43.995144: Steps 21/47: batch_recall = 64.71, batch_ndcg = 55.26 
2025-02-12 23:40:45.047317: Steps 22/47: batch_recall = 55.70, batch_ndcg = 51.59 
2025-02-12 23:40:46.083578: Steps 23/47: batch_recall = 63.12, batch_ndcg = 54.57 
2025-02-12 23:40:47.146640: Steps 24/47: batch_recall = 66.41, batch_ndcg = 58.20 
2025-02-12 23:40:48.193318: Steps 25/47: batch_recall = 66.12, batch_ndcg = 57.91 
2025-02-12 23:40:49.221303: Steps 26/47: batch_recall = 58.70, batch_ndcg = 52.71 
2025-02-12 23:40:50.267865: Steps 27/47: batch_recall = 61.22, batch_ndcg = 52.35 
2025-02-12 23:40:51.303474: Steps 28/47: batch_recall = 71.57, batch_ndcg = 59.99 
2025-02-12 23:40:52.330387: Steps 29/47: batch_recall = 69.87, batch_ndcg = 58.38 
2025-02-12 23:40:53.356148: Steps 30/47: batch_recall = 72.70, batch_ndcg = 63.72 
2025-02-12 23:40:54.400663: Steps 31/47: batch_recall = 66.13, batch_ndcg = 55.66 
2025-02-12 23:40:55.433509: Steps 32/47: batch_recall = 70.48, batch_ndcg = 64.38 
2025-02-12 23:40:56.459386: Steps 33/47: batch_recall = 79.78, batch_ndcg = 67.83 
2025-02-12 23:40:57.487308: Steps 34/47: batch_recall = 68.36, batch_ndcg = 56.39 
2025-02-12 23:40:58.482824: Steps 35/47: batch_recall = 77.41, batch_ndcg = 64.47 
2025-02-12 23:40:59.479779: Steps 36/47: batch_recall = 79.87, batch_ndcg = 65.21 
2025-02-12 23:41:00.449330: Steps 37/47: batch_recall = 81.68, batch_ndcg = 70.73 
2025-02-12 23:41:01.415976: Steps 38/47: batch_recall = 88.89, batch_ndcg = 71.49 
2025-02-12 23:41:02.382740: Steps 39/47: batch_recall = 89.33, batch_ndcg = 69.39 
2025-02-12 23:41:03.341929: Steps 40/47: batch_recall = 72.60, batch_ndcg = 62.31 
2025-02-12 23:41:04.294345: Steps 41/47: batch_recall = 86.26, batch_ndcg = 71.61 
2025-02-12 23:41:05.256966: Steps 42/47: batch_recall = 82.89, batch_ndcg = 64.76 
2025-02-12 23:41:06.217761: Steps 43/47: batch_recall = 89.77, batch_ndcg = 71.97 
2025-02-12 23:41:07.190695: Steps 44/47: batch_recall = 88.64, batch_ndcg = 70.50 
2025-02-12 23:41:08.127625: Steps 45/47: batch_recall = 93.25, batch_ndcg = 73.38 
2025-02-12 23:41:08.235135: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.64 
2025-02-12 23:41:08.235275: Epoch 49/1000, Test: Recall = 0.1212, NDCG = 0.1089  

2025-02-12 23:41:09.457006: Training Step 0/59: batchLoss = 2.7144, diffLoss = 13.2677, kgLoss = 0.0760
2025-02-12 23:41:10.388073: Training Step 1/59: batchLoss = 3.0076, diffLoss = 14.6920, kgLoss = 0.0865
2025-02-12 23:41:11.327380: Training Step 2/59: batchLoss = 2.7027, diffLoss = 13.1762, kgLoss = 0.0843
2025-02-12 23:41:12.261073: Training Step 3/59: batchLoss = 2.6005, diffLoss = 12.6574, kgLoss = 0.0862
2025-02-12 23:41:13.200254: Training Step 4/59: batchLoss = 2.8645, diffLoss = 13.9424, kgLoss = 0.0950
2025-02-12 23:41:14.139793: Training Step 5/59: batchLoss = 2.7774, diffLoss = 13.5626, kgLoss = 0.0811
2025-02-12 23:41:15.066165: Training Step 6/59: batchLoss = 2.7024, diffLoss = 13.1669, kgLoss = 0.0863
2025-02-12 23:41:15.997258: Training Step 7/59: batchLoss = 2.6306, diffLoss = 12.8422, kgLoss = 0.0777
2025-02-12 23:41:16.930549: Training Step 8/59: batchLoss = 2.6792, diffLoss = 13.0861, kgLoss = 0.0775
2025-02-12 23:41:17.854936: Training Step 9/59: batchLoss = 2.5987, diffLoss = 12.6969, kgLoss = 0.0741
2025-02-12 23:41:18.781637: Training Step 10/59: batchLoss = 3.1778, diffLoss = 15.5269, kgLoss = 0.0906
2025-02-12 23:41:19.707644: Training Step 11/59: batchLoss = 2.7999, diffLoss = 13.6344, kgLoss = 0.0913
2025-02-12 23:41:20.632537: Training Step 12/59: batchLoss = 2.8722, diffLoss = 13.9825, kgLoss = 0.0946
2025-02-12 23:41:21.557367: Training Step 13/59: batchLoss = 2.6142, diffLoss = 12.7653, kgLoss = 0.0764
2025-02-12 23:41:22.484198: Training Step 14/59: batchLoss = 2.9926, diffLoss = 14.6017, kgLoss = 0.0904
2025-02-12 23:41:23.417911: Training Step 15/59: batchLoss = 2.6519, diffLoss = 12.9566, kgLoss = 0.0757
2025-02-12 23:41:24.360972: Training Step 16/59: batchLoss = 3.0638, diffLoss = 14.9785, kgLoss = 0.0851
2025-02-12 23:41:25.303211: Training Step 17/59: batchLoss = 2.8401, diffLoss = 13.8877, kgLoss = 0.0782
2025-02-12 23:41:26.242811: Training Step 18/59: batchLoss = 2.9302, diffLoss = 14.3020, kgLoss = 0.0872
2025-02-12 23:41:27.179604: Training Step 19/59: batchLoss = 3.2318, diffLoss = 15.7690, kgLoss = 0.0974
2025-02-12 23:41:28.109324: Training Step 20/59: batchLoss = 3.1779, diffLoss = 15.5160, kgLoss = 0.0933
2025-02-12 23:41:29.047873: Training Step 21/59: batchLoss = 2.8130, diffLoss = 13.7481, kgLoss = 0.0792
2025-02-12 23:41:29.988527: Training Step 22/59: batchLoss = 3.1659, diffLoss = 15.4313, kgLoss = 0.0996
2025-02-12 23:41:30.928452: Training Step 23/59: batchLoss = 2.7870, diffLoss = 13.6159, kgLoss = 0.0798
2025-02-12 23:41:31.863070: Training Step 24/59: batchLoss = 3.1273, diffLoss = 15.2668, kgLoss = 0.0924
2025-02-12 23:41:32.799697: Training Step 25/59: batchLoss = 2.8073, diffLoss = 13.7092, kgLoss = 0.0818
2025-02-12 23:41:33.734800: Training Step 26/59: batchLoss = 2.8886, diffLoss = 14.1081, kgLoss = 0.0837
2025-02-12 23:41:34.668028: Training Step 27/59: batchLoss = 3.1033, diffLoss = 15.1642, kgLoss = 0.0880
2025-02-12 23:41:35.597392: Training Step 28/59: batchLoss = 2.8407, diffLoss = 13.8523, kgLoss = 0.0878
2025-02-12 23:41:36.526761: Training Step 29/59: batchLoss = 3.1949, diffLoss = 15.5984, kgLoss = 0.0940
2025-02-12 23:41:37.467076: Training Step 30/59: batchLoss = 2.9517, diffLoss = 14.4054, kgLoss = 0.0882
2025-02-12 23:41:38.395210: Training Step 31/59: batchLoss = 2.7943, diffLoss = 13.6408, kgLoss = 0.0826
2025-02-12 23:41:39.325704: Training Step 32/59: batchLoss = 3.0043, diffLoss = 14.6744, kgLoss = 0.0868
2025-02-12 23:41:40.259511: Training Step 33/59: batchLoss = 2.8431, diffLoss = 13.8946, kgLoss = 0.0802
2025-02-12 23:41:41.193112: Training Step 34/59: batchLoss = 3.1582, diffLoss = 15.4397, kgLoss = 0.0878
2025-02-12 23:41:42.131560: Training Step 35/59: batchLoss = 2.5802, diffLoss = 12.6078, kgLoss = 0.0733
2025-02-12 23:41:43.066216: Training Step 36/59: batchLoss = 2.7945, diffLoss = 13.6347, kgLoss = 0.0844
2025-02-12 23:41:44.007894: Training Step 37/59: batchLoss = 2.8049, diffLoss = 13.7080, kgLoss = 0.0791
2025-02-12 23:41:44.932692: Training Step 38/59: batchLoss = 3.1435, diffLoss = 15.3470, kgLoss = 0.0927
2025-02-12 23:41:45.873018: Training Step 39/59: batchLoss = 3.1729, diffLoss = 15.4993, kgLoss = 0.0913
2025-02-12 23:41:46.813160: Training Step 40/59: batchLoss = 2.8774, diffLoss = 14.0709, kgLoss = 0.0790
2025-02-12 23:41:47.754010: Training Step 41/59: batchLoss = 3.0848, diffLoss = 15.0640, kgLoss = 0.0900
2025-02-12 23:41:48.687925: Training Step 42/59: batchLoss = 2.9885, diffLoss = 14.5906, kgLoss = 0.0880
2025-02-12 23:41:49.632421: Training Step 43/59: batchLoss = 2.7014, diffLoss = 13.1825, kgLoss = 0.0811
2025-02-12 23:41:50.577050: Training Step 44/59: batchLoss = 2.7965, diffLoss = 13.6572, kgLoss = 0.0814
2025-02-12 23:41:51.506946: Training Step 45/59: batchLoss = 2.9080, diffLoss = 14.2283, kgLoss = 0.0779
2025-02-12 23:41:52.435947: Training Step 46/59: batchLoss = 3.0985, diffLoss = 15.1109, kgLoss = 0.0954
2025-02-12 23:41:53.364988: Training Step 47/59: batchLoss = 3.0108, diffLoss = 14.7183, kgLoss = 0.0839
2025-02-12 23:41:54.292005: Training Step 48/59: batchLoss = 2.9975, diffLoss = 14.6315, kgLoss = 0.0890
2025-02-12 23:41:55.215996: Training Step 49/59: batchLoss = 3.2219, diffLoss = 15.7258, kgLoss = 0.0959
2025-02-12 23:41:56.145152: Training Step 50/59: batchLoss = 2.9784, diffLoss = 14.5487, kgLoss = 0.0858
2025-02-12 23:41:57.072864: Training Step 51/59: batchLoss = 2.9188, diffLoss = 14.2781, kgLoss = 0.0790
2025-02-12 23:41:58.006130: Training Step 52/59: batchLoss = 3.1850, diffLoss = 15.5704, kgLoss = 0.0886
2025-02-12 23:41:58.955691: Training Step 53/59: batchLoss = 3.1630, diffLoss = 15.4601, kgLoss = 0.0887
2025-02-12 23:41:59.895930: Training Step 54/59: batchLoss = 3.1452, diffLoss = 15.3862, kgLoss = 0.0849
2025-02-12 23:42:00.833958: Training Step 55/59: batchLoss = 2.7608, diffLoss = 13.4848, kgLoss = 0.0797
2025-02-12 23:42:01.762825: Training Step 56/59: batchLoss = 3.3465, diffLoss = 16.3774, kgLoss = 0.0888
2025-02-12 23:42:02.612131: Training Step 57/59: batchLoss = 3.1876, diffLoss = 15.5859, kgLoss = 0.0880
2025-02-12 23:42:03.468961: Training Step 58/59: batchLoss = 2.7798, diffLoss = 13.5403, kgLoss = 0.0896
2025-02-12 23:42:03.566227: 
2025-02-12 23:42:03.566590: Epoch 50/1000, Train: epLoss = 0.4319, epDfLoss = 2.1089, epKgLoss = 0.0126  
2025-02-12 23:42:05.045406: Steps 0/47: batch_recall = 37.99, batch_ndcg = 47.83 
2025-02-12 23:42:06.347534: Steps 1/47: batch_recall = 36.69, batch_ndcg = 42.79 
2025-02-12 23:42:07.605492: Steps 2/47: batch_recall = 40.55, batch_ndcg = 46.32 
2025-02-12 23:42:08.870123: Steps 3/47: batch_recall = 42.86, batch_ndcg = 45.80 
2025-02-12 23:42:10.062102: Steps 4/47: batch_recall = 39.46, batch_ndcg = 46.08 
2025-02-12 23:42:11.278457: Steps 5/47: batch_recall = 32.20, batch_ndcg = 38.66 
2025-02-12 23:42:12.457958: Steps 6/47: batch_recall = 38.84, batch_ndcg = 41.82 
2025-02-12 23:42:13.626626: Steps 7/47: batch_recall = 43.16, batch_ndcg = 44.99 
2025-02-12 23:42:14.799723: Steps 8/47: batch_recall = 47.67, batch_ndcg = 51.47 
2025-02-12 23:42:15.939214: Steps 9/47: batch_recall = 46.82, batch_ndcg = 46.40 
2025-02-12 23:42:17.114779: Steps 10/47: batch_recall = 44.11, batch_ndcg = 43.66 
2025-02-12 23:42:18.258202: Steps 11/47: batch_recall = 54.09, batch_ndcg = 52.18 
2025-02-12 23:42:19.409699: Steps 12/47: batch_recall = 51.62, batch_ndcg = 49.73 
2025-02-12 23:42:20.553580: Steps 13/47: batch_recall = 48.32, batch_ndcg = 45.74 
2025-02-12 23:42:21.653853: Steps 14/47: batch_recall = 39.39, batch_ndcg = 40.30 
2025-02-12 23:42:22.759784: Steps 15/47: batch_recall = 56.55, batch_ndcg = 53.63 
2025-02-12 23:42:23.846747: Steps 16/47: batch_recall = 48.97, batch_ndcg = 45.40 
2025-02-12 23:42:24.893124: Steps 17/47: batch_recall = 57.92, batch_ndcg = 49.67 
2025-02-12 23:42:25.969638: Steps 18/47: batch_recall = 53.38, batch_ndcg = 49.42 
2025-02-12 23:42:27.033877: Steps 19/47: batch_recall = 59.02, batch_ndcg = 55.07 
2025-02-12 23:42:28.077804: Steps 20/47: batch_recall = 68.29, batch_ndcg = 61.97 
2025-02-12 23:42:29.120558: Steps 21/47: batch_recall = 66.08, batch_ndcg = 55.68 
2025-02-12 23:42:30.172950: Steps 22/47: batch_recall = 56.05, batch_ndcg = 50.82 
2025-02-12 23:42:31.213910: Steps 23/47: batch_recall = 63.32, batch_ndcg = 55.18 
2025-02-12 23:42:32.275485: Steps 24/47: batch_recall = 64.47, batch_ndcg = 57.99 
2025-02-12 23:42:33.323744: Steps 25/47: batch_recall = 64.07, batch_ndcg = 56.77 
2025-02-12 23:42:34.337367: Steps 26/47: batch_recall = 59.42, batch_ndcg = 53.40 
2025-02-12 23:42:35.389519: Steps 27/47: batch_recall = 62.00, batch_ndcg = 52.03 
2025-02-12 23:42:36.421260: Steps 28/47: batch_recall = 69.81, batch_ndcg = 59.16 
2025-02-12 23:42:37.443864: Steps 29/47: batch_recall = 68.13, batch_ndcg = 56.75 
2025-02-12 23:42:38.460161: Steps 30/47: batch_recall = 72.35, batch_ndcg = 63.23 
2025-02-12 23:42:39.505954: Steps 31/47: batch_recall = 67.04, batch_ndcg = 55.63 
2025-02-12 23:42:40.534649: Steps 32/47: batch_recall = 70.12, batch_ndcg = 64.65 
2025-02-12 23:42:41.558624: Steps 33/47: batch_recall = 78.85, batch_ndcg = 67.74 
2025-02-12 23:42:42.567632: Steps 34/47: batch_recall = 67.71, batch_ndcg = 55.93 
2025-02-12 23:42:43.545751: Steps 35/47: batch_recall = 76.78, batch_ndcg = 64.50 
2025-02-12 23:42:44.535617: Steps 36/47: batch_recall = 80.50, batch_ndcg = 64.67 
2025-02-12 23:42:45.500498: Steps 37/47: batch_recall = 82.80, batch_ndcg = 71.75 
2025-02-12 23:42:46.473916: Steps 38/47: batch_recall = 90.20, batch_ndcg = 72.78 
2025-02-12 23:42:47.440570: Steps 39/47: batch_recall = 88.82, batch_ndcg = 70.71 
2025-02-12 23:42:48.399709: Steps 40/47: batch_recall = 71.84, batch_ndcg = 62.59 
2025-02-12 23:42:49.368900: Steps 41/47: batch_recall = 85.99, batch_ndcg = 71.03 
2025-02-12 23:42:50.332276: Steps 42/47: batch_recall = 81.66, batch_ndcg = 64.48 
2025-02-12 23:42:51.295622: Steps 43/47: batch_recall = 89.89, batch_ndcg = 72.26 
2025-02-12 23:42:52.268447: Steps 44/47: batch_recall = 87.98, batch_ndcg = 69.50 
2025-02-12 23:42:53.212105: Steps 45/47: batch_recall = 93.18, batch_ndcg = 74.61 
2025-02-12 23:42:53.319102: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.55 
2025-02-12 23:42:53.319238: Epoch 50/1000, Test: Recall = 0.1209, NDCG = 0.1088  

2025-02-12 23:42:54.531527: Training Step 0/59: batchLoss = 2.7386, diffLoss = 13.3598, kgLoss = 0.0833
2025-02-12 23:42:55.466812: Training Step 1/59: batchLoss = 2.6877, diffLoss = 13.1137, kgLoss = 0.0812
2025-02-12 23:42:56.387502: Training Step 2/59: batchLoss = 2.8340, diffLoss = 13.8461, kgLoss = 0.0809
2025-02-12 23:42:57.323673: Training Step 3/59: batchLoss = 2.9278, diffLoss = 14.2862, kgLoss = 0.0883
2025-02-12 23:42:58.260914: Training Step 4/59: batchLoss = 2.7018, diffLoss = 13.2039, kgLoss = 0.0762
2025-02-12 23:42:59.190758: Training Step 5/59: batchLoss = 2.9742, diffLoss = 14.5027, kgLoss = 0.0921
2025-02-12 23:43:00.123602: Training Step 6/59: batchLoss = 2.8863, diffLoss = 14.0884, kgLoss = 0.0858
2025-02-12 23:43:01.056224: Training Step 7/59: batchLoss = 2.9296, diffLoss = 14.3055, kgLoss = 0.0856
2025-02-12 23:43:01.989373: Training Step 8/59: batchLoss = 2.8082, diffLoss = 13.7263, kgLoss = 0.0787
2025-02-12 23:43:02.926548: Training Step 9/59: batchLoss = 2.8933, diffLoss = 14.1368, kgLoss = 0.0824
2025-02-12 23:43:03.853555: Training Step 10/59: batchLoss = 2.8832, diffLoss = 14.0644, kgLoss = 0.0879
2025-02-12 23:43:04.788653: Training Step 11/59: batchLoss = 2.9857, diffLoss = 14.5761, kgLoss = 0.0882
2025-02-12 23:43:05.722767: Training Step 12/59: batchLoss = 2.7435, diffLoss = 13.3802, kgLoss = 0.0843
2025-02-12 23:43:06.659141: Training Step 13/59: batchLoss = 2.9990, diffLoss = 14.6354, kgLoss = 0.0898
2025-02-12 23:43:07.590268: Training Step 14/59: batchLoss = 3.3283, diffLoss = 16.2222, kgLoss = 0.1048
2025-02-12 23:43:08.525996: Training Step 15/59: batchLoss = 3.1806, diffLoss = 15.5104, kgLoss = 0.0982
2025-02-12 23:43:09.457415: Training Step 16/59: batchLoss = 2.7970, diffLoss = 13.6321, kgLoss = 0.0882
2025-02-12 23:43:10.399750: Training Step 17/59: batchLoss = 2.8068, diffLoss = 13.6913, kgLoss = 0.0857
2025-02-12 23:43:11.338541: Training Step 18/59: batchLoss = 2.8341, diffLoss = 13.8486, kgLoss = 0.0804
2025-02-12 23:43:12.279245: Training Step 19/59: batchLoss = 2.6853, diffLoss = 13.0944, kgLoss = 0.0830
2025-02-12 23:43:13.209271: Training Step 20/59: batchLoss = 3.0013, diffLoss = 14.6591, kgLoss = 0.0868
2025-02-12 23:43:14.142388: Training Step 21/59: batchLoss = 3.0828, diffLoss = 15.0347, kgLoss = 0.0948
2025-02-12 23:43:15.077481: Training Step 22/59: batchLoss = 2.7682, diffLoss = 13.5045, kgLoss = 0.0841
2025-02-12 23:43:16.012608: Training Step 23/59: batchLoss = 2.7887, diffLoss = 13.6037, kgLoss = 0.0850
2025-02-12 23:43:16.950137: Training Step 24/59: batchLoss = 2.9012, diffLoss = 14.1768, kgLoss = 0.0823
2025-02-12 23:43:17.891771: Training Step 25/59: batchLoss = 2.8411, diffLoss = 13.8562, kgLoss = 0.0873
2025-02-12 23:43:18.823946: Training Step 26/59: batchLoss = 2.9412, diffLoss = 14.3693, kgLoss = 0.0842
2025-02-12 23:43:19.749587: Training Step 27/59: batchLoss = 2.8980, diffLoss = 14.1496, kgLoss = 0.0851
2025-02-12 23:43:20.674587: Training Step 28/59: batchLoss = 2.9185, diffLoss = 14.2770, kgLoss = 0.0789
2025-02-12 23:43:21.606171: Training Step 29/59: batchLoss = 3.2449, diffLoss = 15.8058, kgLoss = 0.1047
2025-02-12 23:43:22.541778: Training Step 30/59: batchLoss = 3.0022, diffLoss = 14.6813, kgLoss = 0.0825
2025-02-12 23:43:23.471467: Training Step 31/59: batchLoss = 3.0198, diffLoss = 14.7944, kgLoss = 0.0762
2025-02-12 23:43:24.423011: Training Step 32/59: batchLoss = 3.0169, diffLoss = 14.7297, kgLoss = 0.0887
2025-02-12 23:43:25.361290: Training Step 33/59: batchLoss = 2.9611, diffLoss = 14.4761, kgLoss = 0.0824
2025-02-12 23:43:26.298915: Training Step 34/59: batchLoss = 3.2069, diffLoss = 15.6937, kgLoss = 0.0853
2025-02-12 23:43:27.225618: Training Step 35/59: batchLoss = 3.0760, diffLoss = 15.0273, kgLoss = 0.0882
2025-02-12 23:43:28.163130: Training Step 36/59: batchLoss = 2.9895, diffLoss = 14.6172, kgLoss = 0.0826
2025-02-12 23:43:29.098512: Training Step 37/59: batchLoss = 3.0829, diffLoss = 15.0765, kgLoss = 0.0846
2025-02-12 23:43:30.035991: Training Step 38/59: batchLoss = 2.8924, diffLoss = 14.1300, kgLoss = 0.0830
2025-02-12 23:43:30.972184: Training Step 39/59: batchLoss = 2.9691, diffLoss = 14.5075, kgLoss = 0.0845
2025-02-12 23:43:31.907185: Training Step 40/59: batchLoss = 2.6206, diffLoss = 12.7801, kgLoss = 0.0807
2025-02-12 23:43:32.841236: Training Step 41/59: batchLoss = 2.5658, diffLoss = 12.5276, kgLoss = 0.0754
2025-02-12 23:43:33.778325: Training Step 42/59: batchLoss = 2.8770, diffLoss = 14.0544, kgLoss = 0.0826
2025-02-12 23:43:34.710737: Training Step 43/59: batchLoss = 2.7189, diffLoss = 13.2885, kgLoss = 0.0765
2025-02-12 23:43:35.643955: Training Step 44/59: batchLoss = 3.3380, diffLoss = 16.3236, kgLoss = 0.0916
2025-02-12 23:43:36.572265: Training Step 45/59: batchLoss = 3.1623, diffLoss = 15.4574, kgLoss = 0.0885
2025-02-12 23:43:37.500083: Training Step 46/59: batchLoss = 3.2122, diffLoss = 15.6683, kgLoss = 0.0981
2025-02-12 23:43:38.435918: Training Step 47/59: batchLoss = 3.2824, diffLoss = 16.0309, kgLoss = 0.0953
2025-02-12 23:43:39.361443: Training Step 48/59: batchLoss = 2.6505, diffLoss = 12.9154, kgLoss = 0.0843
2025-02-12 23:43:40.287558: Training Step 49/59: batchLoss = 3.0474, diffLoss = 14.9061, kgLoss = 0.0827
2025-02-12 23:43:41.220223: Training Step 50/59: batchLoss = 3.3749, diffLoss = 16.5085, kgLoss = 0.0915
2025-02-12 23:43:42.146316: Training Step 51/59: batchLoss = 3.0457, diffLoss = 14.8854, kgLoss = 0.0858
2025-02-12 23:43:43.085883: Training Step 52/59: batchLoss = 2.8446, diffLoss = 13.9120, kgLoss = 0.0778
2025-02-12 23:43:44.020156: Training Step 53/59: batchLoss = 3.2157, diffLoss = 15.7285, kgLoss = 0.0875
2025-02-12 23:43:44.955960: Training Step 54/59: batchLoss = 2.9181, diffLoss = 14.2791, kgLoss = 0.0778
2025-02-12 23:43:45.889940: Training Step 55/59: batchLoss = 2.9535, diffLoss = 14.4105, kgLoss = 0.0893
2025-02-12 23:43:46.816686: Training Step 56/59: batchLoss = 3.0170, diffLoss = 14.7648, kgLoss = 0.0800
2025-02-12 23:43:47.666153: Training Step 57/59: batchLoss = 3.1914, diffLoss = 15.5911, kgLoss = 0.0915
2025-02-12 23:43:48.521714: Training Step 58/59: batchLoss = 2.9895, diffLoss = 14.6337, kgLoss = 0.0784
2025-02-12 23:43:48.623307: 
2025-02-12 23:43:48.623867: Epoch 51/1000, Train: epLoss = 0.4356, epDfLoss = 2.1277, epKgLoss = 0.0126  
2025-02-12 23:43:50.111295: Steps 0/47: batch_recall = 36.53, batch_ndcg = 47.74 
2025-02-12 23:43:51.406953: Steps 1/47: batch_recall = 37.43, batch_ndcg = 42.74 
2025-02-12 23:43:52.656798: Steps 2/47: batch_recall = 40.47, batch_ndcg = 46.65 
2025-02-12 23:43:53.917055: Steps 3/47: batch_recall = 43.25, batch_ndcg = 45.92 
2025-02-12 23:43:55.111734: Steps 4/47: batch_recall = 39.16, batch_ndcg = 45.52 
2025-02-12 23:43:56.315210: Steps 5/47: batch_recall = 32.44, batch_ndcg = 39.27 
2025-02-12 23:43:57.508520: Steps 6/47: batch_recall = 38.90, batch_ndcg = 41.80 
2025-02-12 23:43:58.687104: Steps 7/47: batch_recall = 44.66, batch_ndcg = 45.95 
2025-02-12 23:43:59.868400: Steps 8/47: batch_recall = 47.78, batch_ndcg = 51.71 
2025-02-12 23:44:01.008148: Steps 9/47: batch_recall = 46.09, batch_ndcg = 46.37 
2025-02-12 23:44:02.179557: Steps 10/47: batch_recall = 43.44, batch_ndcg = 43.20 
2025-02-12 23:44:03.329126: Steps 11/47: batch_recall = 53.84, batch_ndcg = 51.58 
2025-02-12 23:44:04.481351: Steps 12/47: batch_recall = 49.20, batch_ndcg = 48.59 
2025-02-12 23:44:05.618934: Steps 13/47: batch_recall = 47.64, batch_ndcg = 44.92 
2025-02-12 23:44:06.718471: Steps 14/47: batch_recall = 40.81, batch_ndcg = 41.11 
2025-02-12 23:44:07.813398: Steps 15/47: batch_recall = 57.86, batch_ndcg = 53.85 
2025-02-12 23:44:08.890027: Steps 16/47: batch_recall = 50.39, batch_ndcg = 45.95 
2025-02-12 23:44:09.935164: Steps 17/47: batch_recall = 57.45, batch_ndcg = 50.12 
2025-02-12 23:44:11.010427: Steps 18/47: batch_recall = 53.67, batch_ndcg = 49.57 
2025-02-12 23:44:12.075655: Steps 19/47: batch_recall = 60.53, batch_ndcg = 55.46 
2025-02-12 23:44:13.117738: Steps 20/47: batch_recall = 68.62, batch_ndcg = 61.34 
2025-02-12 23:44:14.156356: Steps 21/47: batch_recall = 65.69, batch_ndcg = 55.79 
2025-02-12 23:44:15.208282: Steps 22/47: batch_recall = 54.56, batch_ndcg = 50.66 
2025-02-12 23:44:16.263046: Steps 23/47: batch_recall = 64.37, batch_ndcg = 55.74 
2025-02-12 23:44:17.327283: Steps 24/47: batch_recall = 64.90, batch_ndcg = 57.39 
2025-02-12 23:44:18.377426: Steps 25/47: batch_recall = 64.17, batch_ndcg = 56.95 
2025-02-12 23:44:19.406261: Steps 26/47: batch_recall = 59.35, batch_ndcg = 52.68 
2025-02-12 23:44:20.449408: Steps 27/47: batch_recall = 62.68, batch_ndcg = 52.46 
2025-02-12 23:44:21.481697: Steps 28/47: batch_recall = 70.03, batch_ndcg = 59.16 
2025-02-12 23:44:22.513166: Steps 29/47: batch_recall = 68.91, batch_ndcg = 57.23 
2025-02-12 23:44:23.531126: Steps 30/47: batch_recall = 72.37, batch_ndcg = 63.60 
2025-02-12 23:44:24.567233: Steps 31/47: batch_recall = 67.47, batch_ndcg = 56.32 
2025-02-12 23:44:25.576764: Steps 32/47: batch_recall = 69.95, batch_ndcg = 64.41 
2025-02-12 23:44:26.583985: Steps 33/47: batch_recall = 80.63, batch_ndcg = 68.56 
2025-02-12 23:44:27.595017: Steps 34/47: batch_recall = 68.84, batch_ndcg = 57.24 
2025-02-12 23:44:28.578774: Steps 35/47: batch_recall = 78.11, batch_ndcg = 64.53 
2025-02-12 23:44:29.565043: Steps 36/47: batch_recall = 76.68, batch_ndcg = 63.61 
2025-02-12 23:44:30.539565: Steps 37/47: batch_recall = 81.67, batch_ndcg = 71.44 
2025-02-12 23:44:31.512627: Steps 38/47: batch_recall = 89.80, batch_ndcg = 72.32 
2025-02-12 23:44:32.484417: Steps 39/47: batch_recall = 88.77, batch_ndcg = 69.74 
2025-02-12 23:44:33.466158: Steps 40/47: batch_recall = 71.97, batch_ndcg = 63.23 
2025-02-12 23:44:34.439770: Steps 41/47: batch_recall = 87.05, batch_ndcg = 71.62 
2025-02-12 23:44:35.409639: Steps 42/47: batch_recall = 82.55, batch_ndcg = 64.92 
2025-02-12 23:44:36.375792: Steps 43/47: batch_recall = 90.41, batch_ndcg = 71.99 
2025-02-12 23:44:37.344289: Steps 44/47: batch_recall = 87.41, batch_ndcg = 69.88 
2025-02-12 23:44:38.293024: Steps 45/47: batch_recall = 92.73, batch_ndcg = 74.29 
2025-02-12 23:44:38.400258: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.48 
2025-02-12 23:44:38.400392: Epoch 51/1000, Test: Recall = 0.1211, NDCG = 0.1089  

2025-02-12 23:44:39.612979: Training Step 0/59: batchLoss = 2.9639, diffLoss = 14.4629, kgLoss = 0.0892
2025-02-12 23:44:40.549991: Training Step 1/59: batchLoss = 3.1218, diffLoss = 15.2299, kgLoss = 0.0948
2025-02-12 23:44:41.481169: Training Step 2/59: batchLoss = 2.9157, diffLoss = 14.2047, kgLoss = 0.0934
2025-02-12 23:44:42.417624: Training Step 3/59: batchLoss = 2.5526, diffLoss = 12.4449, kgLoss = 0.0795
2025-02-12 23:44:43.340885: Training Step 4/59: batchLoss = 2.7800, diffLoss = 13.5780, kgLoss = 0.0805
2025-02-12 23:44:44.264053: Training Step 5/59: batchLoss = 2.9789, diffLoss = 14.5449, kgLoss = 0.0874
2025-02-12 23:44:45.189072: Training Step 6/59: batchLoss = 2.9099, diffLoss = 14.1540, kgLoss = 0.0989
2025-02-12 23:44:46.119053: Training Step 7/59: batchLoss = 3.0143, diffLoss = 14.6934, kgLoss = 0.0945
2025-02-12 23:44:47.039920: Training Step 8/59: batchLoss = 2.8226, diffLoss = 13.7756, kgLoss = 0.0843
2025-02-12 23:44:47.973317: Training Step 9/59: batchLoss = 2.6662, diffLoss = 13.0002, kgLoss = 0.0828
2025-02-12 23:44:48.904975: Training Step 10/59: batchLoss = 2.6611, diffLoss = 13.0007, kgLoss = 0.0762
2025-02-12 23:44:49.837303: Training Step 11/59: batchLoss = 3.0067, diffLoss = 14.6617, kgLoss = 0.0930
2025-02-12 23:44:50.775253: Training Step 12/59: batchLoss = 2.8887, diffLoss = 14.1236, kgLoss = 0.0799
2025-02-12 23:44:51.707014: Training Step 13/59: batchLoss = 3.3179, diffLoss = 16.1930, kgLoss = 0.0991
2025-02-12 23:44:52.641074: Training Step 14/59: batchLoss = 2.9073, diffLoss = 14.1830, kgLoss = 0.0883
2025-02-12 23:44:53.569795: Training Step 15/59: batchLoss = 2.9531, diffLoss = 14.4473, kgLoss = 0.0795
2025-02-12 23:44:54.507389: Training Step 16/59: batchLoss = 2.8087, diffLoss = 13.7182, kgLoss = 0.0814
2025-02-12 23:44:55.440669: Training Step 17/59: batchLoss = 3.1839, diffLoss = 15.5398, kgLoss = 0.0949
2025-02-12 23:44:56.375192: Training Step 18/59: batchLoss = 2.9440, diffLoss = 14.3826, kgLoss = 0.0843
2025-02-12 23:44:57.317146: Training Step 19/59: batchLoss = 3.0110, diffLoss = 14.7033, kgLoss = 0.0879
2025-02-12 23:44:58.253857: Training Step 20/59: batchLoss = 3.2409, diffLoss = 15.8332, kgLoss = 0.0928
2025-02-12 23:44:59.192602: Training Step 21/59: batchLoss = 3.0037, diffLoss = 14.6605, kgLoss = 0.0895
2025-02-12 23:45:00.123040: Training Step 22/59: batchLoss = 2.7429, diffLoss = 13.4004, kgLoss = 0.0786
2025-02-12 23:45:01.051581: Training Step 23/59: batchLoss = 3.1042, diffLoss = 15.1472, kgLoss = 0.0934
2025-02-12 23:45:01.980752: Training Step 24/59: batchLoss = 2.9207, diffLoss = 14.2683, kgLoss = 0.0838
2025-02-12 23:45:02.902282: Training Step 25/59: batchLoss = 2.7392, diffLoss = 13.3837, kgLoss = 0.0781
2025-02-12 23:45:03.826332: Training Step 26/59: batchLoss = 2.5620, diffLoss = 12.4979, kgLoss = 0.0780
2025-02-12 23:45:04.753012: Training Step 27/59: batchLoss = 2.7444, diffLoss = 13.4151, kgLoss = 0.0767
2025-02-12 23:45:05.684259: Training Step 28/59: batchLoss = 2.8815, diffLoss = 14.1083, kgLoss = 0.0748
2025-02-12 23:45:06.608618: Training Step 29/59: batchLoss = 3.1254, diffLoss = 15.2753, kgLoss = 0.0880
2025-02-12 23:45:07.542658: Training Step 30/59: batchLoss = 3.0646, diffLoss = 14.9584, kgLoss = 0.0912
2025-02-12 23:45:08.478275: Training Step 31/59: batchLoss = 3.1581, diffLoss = 15.4446, kgLoss = 0.0864
2025-02-12 23:45:09.411869: Training Step 32/59: batchLoss = 3.1533, diffLoss = 15.4197, kgLoss = 0.0868
2025-02-12 23:45:10.358366: Training Step 33/59: batchLoss = 2.8669, diffLoss = 14.0066, kgLoss = 0.0819
2025-02-12 23:45:11.291958: Training Step 34/59: batchLoss = 2.8348, diffLoss = 13.8275, kgLoss = 0.0866
2025-02-12 23:45:12.222142: Training Step 35/59: batchLoss = 3.1269, diffLoss = 15.2841, kgLoss = 0.0876
2025-02-12 23:45:13.167609: Training Step 36/59: batchLoss = 3.1039, diffLoss = 15.1486, kgLoss = 0.0927
2025-02-12 23:45:14.108292: Training Step 37/59: batchLoss = 2.8868, diffLoss = 14.0678, kgLoss = 0.0915
2025-02-12 23:45:15.061940: Training Step 38/59: batchLoss = 2.8291, diffLoss = 13.8268, kgLoss = 0.0796
2025-02-12 23:45:16.001095: Training Step 39/59: batchLoss = 3.1650, diffLoss = 15.4472, kgLoss = 0.0945
2025-02-12 23:45:16.939755: Training Step 40/59: batchLoss = 2.9202, diffLoss = 14.2749, kgLoss = 0.0815
2025-02-12 23:45:17.869911: Training Step 41/59: batchLoss = 2.8504, diffLoss = 13.9274, kgLoss = 0.0811
2025-02-12 23:45:18.795993: Training Step 42/59: batchLoss = 2.9994, diffLoss = 14.6455, kgLoss = 0.0879
2025-02-12 23:45:19.726131: Training Step 43/59: batchLoss = 3.1436, diffLoss = 15.3488, kgLoss = 0.0923
2025-02-12 23:45:20.648732: Training Step 44/59: batchLoss = 2.7854, diffLoss = 13.6277, kgLoss = 0.0748
2025-02-12 23:45:21.571619: Training Step 45/59: batchLoss = 3.3042, diffLoss = 16.1462, kgLoss = 0.0937
2025-02-12 23:45:22.497778: Training Step 46/59: batchLoss = 2.7921, diffLoss = 13.6408, kgLoss = 0.0799
2025-02-12 23:45:23.417462: Training Step 47/59: batchLoss = 3.7797, diffLoss = 18.5099, kgLoss = 0.0971
2025-02-12 23:45:24.362869: Training Step 48/59: batchLoss = 2.9082, diffLoss = 14.2160, kgLoss = 0.0813
2025-02-12 23:45:25.305152: Training Step 49/59: batchLoss = 2.9204, diffLoss = 14.2884, kgLoss = 0.0784
2025-02-12 23:45:26.237425: Training Step 50/59: batchLoss = 2.9663, diffLoss = 14.4859, kgLoss = 0.0864
2025-02-12 23:45:27.172276: Training Step 51/59: batchLoss = 2.8810, diffLoss = 14.0871, kgLoss = 0.0794
2025-02-12 23:45:28.105148: Training Step 52/59: batchLoss = 2.8245, diffLoss = 13.7738, kgLoss = 0.0872
2025-02-12 23:45:29.038578: Training Step 53/59: batchLoss = 2.9213, diffLoss = 14.2818, kgLoss = 0.0812
2025-02-12 23:45:29.973606: Training Step 54/59: batchLoss = 2.7083, diffLoss = 13.2400, kgLoss = 0.0753
2025-02-12 23:45:30.915467: Training Step 55/59: batchLoss = 3.0454, diffLoss = 14.8949, kgLoss = 0.0831
2025-02-12 23:45:31.839263: Training Step 56/59: batchLoss = 2.8361, diffLoss = 13.8552, kgLoss = 0.0814
2025-02-12 23:45:32.691029: Training Step 57/59: batchLoss = 3.0062, diffLoss = 14.6877, kgLoss = 0.0858
2025-02-12 23:45:33.546810: Training Step 58/59: batchLoss = 2.9545, diffLoss = 14.4463, kgLoss = 0.0815
2025-02-12 23:45:33.641890: 
2025-02-12 23:45:33.642429: Epoch 52/1000, Train: epLoss = 0.4355, epDfLoss = 2.1271, epKgLoss = 0.0126  
2025-02-12 23:45:35.112099: Steps 0/47: batch_recall = 36.93, batch_ndcg = 47.36 
2025-02-12 23:45:36.411472: Steps 1/47: batch_recall = 36.81, batch_ndcg = 42.80 
2025-02-12 23:45:37.655792: Steps 2/47: batch_recall = 41.35, batch_ndcg = 47.09 
2025-02-12 23:45:38.913207: Steps 3/47: batch_recall = 42.01, batch_ndcg = 44.65 
2025-02-12 23:45:40.107962: Steps 4/47: batch_recall = 39.65, batch_ndcg = 45.68 
2025-02-12 23:45:41.311149: Steps 5/47: batch_recall = 32.48, batch_ndcg = 38.68 
2025-02-12 23:45:42.515638: Steps 6/47: batch_recall = 38.84, batch_ndcg = 42.21 
2025-02-12 23:45:43.707216: Steps 7/47: batch_recall = 43.87, batch_ndcg = 45.46 
2025-02-12 23:45:44.890325: Steps 8/47: batch_recall = 47.28, batch_ndcg = 52.21 
2025-02-12 23:45:46.023851: Steps 9/47: batch_recall = 47.24, batch_ndcg = 46.78 
2025-02-12 23:45:47.190002: Steps 10/47: batch_recall = 44.25, batch_ndcg = 44.44 
2025-02-12 23:45:48.345842: Steps 11/47: batch_recall = 54.71, batch_ndcg = 52.09 
2025-02-12 23:45:49.499841: Steps 12/47: batch_recall = 48.73, batch_ndcg = 48.61 
2025-02-12 23:45:50.637041: Steps 13/47: batch_recall = 48.74, batch_ndcg = 45.36 
2025-02-12 23:45:51.732954: Steps 14/47: batch_recall = 40.08, batch_ndcg = 40.61 
2025-02-12 23:45:52.821544: Steps 15/47: batch_recall = 57.44, batch_ndcg = 53.64 
2025-02-12 23:45:53.904495: Steps 16/47: batch_recall = 51.07, batch_ndcg = 47.43 
2025-02-12 23:45:54.957744: Steps 17/47: batch_recall = 57.93, batch_ndcg = 49.51 
2025-02-12 23:45:56.035250: Steps 18/47: batch_recall = 53.63, batch_ndcg = 49.71 
2025-02-12 23:45:57.107337: Steps 19/47: batch_recall = 61.46, batch_ndcg = 56.00 
2025-02-12 23:45:58.155558: Steps 20/47: batch_recall = 66.62, batch_ndcg = 61.38 
2025-02-12 23:45:59.210543: Steps 21/47: batch_recall = 65.43, batch_ndcg = 56.09 
2025-02-12 23:46:00.259394: Steps 22/47: batch_recall = 56.77, batch_ndcg = 51.40 
2025-02-12 23:46:01.312732: Steps 23/47: batch_recall = 63.17, batch_ndcg = 55.40 
2025-02-12 23:46:02.380408: Steps 24/47: batch_recall = 65.29, batch_ndcg = 57.76 
2025-02-12 23:46:03.434830: Steps 25/47: batch_recall = 65.44, batch_ndcg = 57.16 
2025-02-12 23:46:04.460915: Steps 26/47: batch_recall = 58.60, batch_ndcg = 52.71 
2025-02-12 23:46:05.504426: Steps 27/47: batch_recall = 62.54, batch_ndcg = 52.35 
2025-02-12 23:46:06.530967: Steps 28/47: batch_recall = 70.23, batch_ndcg = 59.16 
2025-02-12 23:46:07.558154: Steps 29/47: batch_recall = 69.10, batch_ndcg = 57.93 
2025-02-12 23:46:08.572001: Steps 30/47: batch_recall = 72.19, batch_ndcg = 63.33 
2025-02-12 23:46:09.601957: Steps 31/47: batch_recall = 67.21, batch_ndcg = 56.19 
2025-02-12 23:46:10.613699: Steps 32/47: batch_recall = 71.20, batch_ndcg = 64.65 
2025-02-12 23:46:11.620329: Steps 33/47: batch_recall = 79.72, batch_ndcg = 68.04 
2025-02-12 23:46:12.632114: Steps 34/47: batch_recall = 66.84, batch_ndcg = 55.93 
2025-02-12 23:46:13.613435: Steps 35/47: batch_recall = 78.03, batch_ndcg = 64.68 
2025-02-12 23:46:14.595296: Steps 36/47: batch_recall = 78.26, batch_ndcg = 64.72 
2025-02-12 23:46:15.576647: Steps 37/47: batch_recall = 82.11, batch_ndcg = 71.04 
2025-02-12 23:46:16.547269: Steps 38/47: batch_recall = 90.32, batch_ndcg = 72.53 
2025-02-12 23:46:17.524267: Steps 39/47: batch_recall = 87.09, batch_ndcg = 69.37 
2025-02-12 23:46:18.501003: Steps 40/47: batch_recall = 71.84, batch_ndcg = 62.24 
2025-02-12 23:46:19.471729: Steps 41/47: batch_recall = 86.54, batch_ndcg = 72.21 
2025-02-12 23:46:20.441755: Steps 42/47: batch_recall = 80.75, batch_ndcg = 64.09 
2025-02-12 23:46:21.426073: Steps 43/47: batch_recall = 89.23, batch_ndcg = 72.28 
2025-02-12 23:46:22.401780: Steps 44/47: batch_recall = 86.58, batch_ndcg = 68.93 
2025-02-12 23:46:23.352652: Steps 45/47: batch_recall = 93.84, batch_ndcg = 75.57 
2025-02-12 23:46:23.461502: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.52 
2025-02-12 23:46:23.461636: Epoch 52/1000, Test: Recall = 0.1210, NDCG = 0.1090  

2025-02-12 23:46:24.669560: Training Step 0/59: batchLoss = 2.6979, diffLoss = 13.1206, kgLoss = 0.0922
2025-02-12 23:46:25.603858: Training Step 1/59: batchLoss = 2.7644, diffLoss = 13.4713, kgLoss = 0.0877
2025-02-12 23:46:26.527978: Training Step 2/59: batchLoss = 2.9021, diffLoss = 14.1428, kgLoss = 0.0920
2025-02-12 23:46:27.453704: Training Step 3/59: batchLoss = 2.9367, diffLoss = 14.3116, kgLoss = 0.0930
2025-02-12 23:46:28.384130: Training Step 4/59: batchLoss = 2.9313, diffLoss = 14.3386, kgLoss = 0.0795
2025-02-12 23:46:29.309203: Training Step 5/59: batchLoss = 2.6816, diffLoss = 13.0822, kgLoss = 0.0814
2025-02-12 23:46:30.233951: Training Step 6/59: batchLoss = 2.8970, diffLoss = 14.1484, kgLoss = 0.0841
2025-02-12 23:46:31.162090: Training Step 7/59: batchLoss = 2.8886, diffLoss = 14.0897, kgLoss = 0.0883
2025-02-12 23:46:32.081304: Training Step 8/59: batchLoss = 3.0015, diffLoss = 14.6594, kgLoss = 0.0871
2025-02-12 23:46:33.007745: Training Step 9/59: batchLoss = 2.7303, diffLoss = 13.3133, kgLoss = 0.0846
2025-02-12 23:46:33.941249: Training Step 10/59: batchLoss = 2.7419, diffLoss = 13.4086, kgLoss = 0.0752
2025-02-12 23:46:34.878887: Training Step 11/59: batchLoss = 2.8785, diffLoss = 14.0608, kgLoss = 0.0830
2025-02-12 23:46:35.817303: Training Step 12/59: batchLoss = 2.5754, diffLoss = 12.5656, kgLoss = 0.0778
2025-02-12 23:46:36.748926: Training Step 13/59: batchLoss = 3.0500, diffLoss = 14.8972, kgLoss = 0.0882
2025-02-12 23:46:37.683146: Training Step 14/59: batchLoss = 3.1312, diffLoss = 15.2750, kgLoss = 0.0952
2025-02-12 23:46:38.612863: Training Step 15/59: batchLoss = 2.6970, diffLoss = 13.1673, kgLoss = 0.0794
2025-02-12 23:46:39.556686: Training Step 16/59: batchLoss = 2.9285, diffLoss = 14.3119, kgLoss = 0.0827
2025-02-12 23:46:40.477949: Training Step 17/59: batchLoss = 2.7767, diffLoss = 13.5638, kgLoss = 0.0799
2025-02-12 23:46:41.402975: Training Step 18/59: batchLoss = 2.8272, diffLoss = 13.8218, kgLoss = 0.0785
2025-02-12 23:46:42.327872: Training Step 19/59: batchLoss = 2.8714, diffLoss = 14.0482, kgLoss = 0.0772
2025-02-12 23:46:43.257026: Training Step 20/59: batchLoss = 3.0484, diffLoss = 14.8876, kgLoss = 0.0886
2025-02-12 23:46:44.175880: Training Step 21/59: batchLoss = 2.8505, diffLoss = 13.9352, kgLoss = 0.0793
2025-02-12 23:46:45.100785: Training Step 22/59: batchLoss = 2.8609, diffLoss = 13.9684, kgLoss = 0.0841
2025-02-12 23:46:46.014494: Training Step 23/59: batchLoss = 2.8783, diffLoss = 14.0574, kgLoss = 0.0835
2025-02-12 23:46:46.940501: Training Step 24/59: batchLoss = 3.0412, diffLoss = 14.8171, kgLoss = 0.0973
2025-02-12 23:46:47.860916: Training Step 25/59: batchLoss = 3.2350, diffLoss = 15.7995, kgLoss = 0.0939
2025-02-12 23:46:48.785475: Training Step 26/59: batchLoss = 3.1620, diffLoss = 15.4317, kgLoss = 0.0946
2025-02-12 23:46:49.712420: Training Step 27/59: batchLoss = 2.9377, diffLoss = 14.3378, kgLoss = 0.0877
2025-02-12 23:46:50.639674: Training Step 28/59: batchLoss = 3.0995, diffLoss = 15.1470, kgLoss = 0.0877
2025-02-12 23:46:51.570254: Training Step 29/59: batchLoss = 2.9190, diffLoss = 14.2573, kgLoss = 0.0844
2025-02-12 23:46:52.508000: Training Step 30/59: batchLoss = 2.8629, diffLoss = 13.9746, kgLoss = 0.0850
2025-02-12 23:46:53.446703: Training Step 31/59: batchLoss = 2.7951, diffLoss = 13.6532, kgLoss = 0.0806
2025-02-12 23:46:54.385167: Training Step 32/59: batchLoss = 3.2067, diffLoss = 15.6911, kgLoss = 0.0856
2025-02-12 23:46:55.319936: Training Step 33/59: batchLoss = 3.0958, diffLoss = 15.1425, kgLoss = 0.0842
2025-02-12 23:46:56.256829: Training Step 34/59: batchLoss = 3.2339, diffLoss = 15.7998, kgLoss = 0.0924
2025-02-12 23:46:57.195625: Training Step 35/59: batchLoss = 3.6407, diffLoss = 17.7893, kgLoss = 0.1036
2025-02-12 23:46:58.137623: Training Step 36/59: batchLoss = 2.8974, diffLoss = 14.1701, kgLoss = 0.0792
2025-02-12 23:46:59.071704: Training Step 37/59: batchLoss = 3.2389, diffLoss = 15.8144, kgLoss = 0.0950
2025-02-12 23:47:00.006706: Training Step 38/59: batchLoss = 2.7338, diffLoss = 13.3602, kgLoss = 0.0772
2025-02-12 23:47:00.933694: Training Step 39/59: batchLoss = 2.8211, diffLoss = 13.7760, kgLoss = 0.0824
2025-02-12 23:47:01.855618: Training Step 40/59: batchLoss = 3.0400, diffLoss = 14.8589, kgLoss = 0.0852
2025-02-12 23:47:02.783944: Training Step 41/59: batchLoss = 3.0159, diffLoss = 14.7584, kgLoss = 0.0803
2025-02-12 23:47:03.705222: Training Step 42/59: batchLoss = 2.5836, diffLoss = 12.6075, kgLoss = 0.0776
2025-02-12 23:47:04.632317: Training Step 43/59: batchLoss = 3.1459, diffLoss = 15.3521, kgLoss = 0.0944
2025-02-12 23:47:05.561894: Training Step 44/59: batchLoss = 2.7459, diffLoss = 13.4070, kgLoss = 0.0806
2025-02-12 23:47:06.485059: Training Step 45/59: batchLoss = 2.9489, diffLoss = 14.4283, kgLoss = 0.0791
2025-02-12 23:47:07.413284: Training Step 46/59: batchLoss = 2.9474, diffLoss = 14.4185, kgLoss = 0.0797
2025-02-12 23:47:08.349377: Training Step 47/59: batchLoss = 3.1780, diffLoss = 15.5586, kgLoss = 0.0829
2025-02-12 23:47:09.287955: Training Step 48/59: batchLoss = 3.2875, diffLoss = 16.0797, kgLoss = 0.0894
2025-02-12 23:47:10.218225: Training Step 49/59: batchLoss = 3.3171, diffLoss = 16.2241, kgLoss = 0.0903
2025-02-12 23:47:11.152521: Training Step 50/59: batchLoss = 3.1440, diffLoss = 15.3655, kgLoss = 0.0887
2025-02-12 23:47:12.084317: Training Step 51/59: batchLoss = 3.1720, diffLoss = 15.5008, kgLoss = 0.0898
2025-02-12 23:47:13.022403: Training Step 52/59: batchLoss = 2.7936, diffLoss = 13.6458, kgLoss = 0.0806
2025-02-12 23:47:13.950161: Training Step 53/59: batchLoss = 2.9188, diffLoss = 14.2434, kgLoss = 0.0876
2025-02-12 23:47:14.881466: Training Step 54/59: batchLoss = 2.9058, diffLoss = 14.1854, kgLoss = 0.0859
2025-02-12 23:47:15.811331: Training Step 55/59: batchLoss = 2.6896, diffLoss = 13.1541, kgLoss = 0.0735
2025-02-12 23:47:16.755904: Training Step 56/59: batchLoss = 3.2083, diffLoss = 15.6353, kgLoss = 0.1015
2025-02-12 23:47:17.598444: Training Step 57/59: batchLoss = 2.9461, diffLoss = 14.3843, kgLoss = 0.0866
2025-02-12 23:47:18.452220: Training Step 58/59: batchLoss = 2.9740, diffLoss = 14.5367, kgLoss = 0.0833
2025-02-12 23:47:18.549242: 
2025-02-12 23:47:18.549567: Epoch 53/1000, Train: epLoss = 0.4361, epDfLoss = 2.1299, epKgLoss = 0.0126  
2025-02-12 23:47:20.015452: Steps 0/47: batch_recall = 36.36, batch_ndcg = 47.40 
2025-02-12 23:47:21.303721: Steps 1/47: batch_recall = 36.60, batch_ndcg = 42.62 
2025-02-12 23:47:22.549199: Steps 2/47: batch_recall = 41.12, batch_ndcg = 47.19 
2025-02-12 23:47:23.800175: Steps 3/47: batch_recall = 42.09, batch_ndcg = 45.74 
2025-02-12 23:47:24.997597: Steps 4/47: batch_recall = 39.03, batch_ndcg = 45.38 
2025-02-12 23:47:26.212412: Steps 5/47: batch_recall = 32.42, batch_ndcg = 38.68 
2025-02-12 23:47:27.410373: Steps 6/47: batch_recall = 38.66, batch_ndcg = 42.35 
2025-02-12 23:47:28.580880: Steps 7/47: batch_recall = 43.95, batch_ndcg = 45.55 
2025-02-12 23:47:29.765385: Steps 8/47: batch_recall = 47.71, batch_ndcg = 51.83 
2025-02-12 23:47:30.904006: Steps 9/47: batch_recall = 46.78, batch_ndcg = 46.58 
2025-02-12 23:47:32.071378: Steps 10/47: batch_recall = 43.50, batch_ndcg = 43.42 
2025-02-12 23:47:33.222643: Steps 11/47: batch_recall = 54.48, batch_ndcg = 51.91 
2025-02-12 23:47:34.370942: Steps 12/47: batch_recall = 48.82, batch_ndcg = 48.53 
2025-02-12 23:47:35.496174: Steps 13/47: batch_recall = 47.90, batch_ndcg = 45.22 
2025-02-12 23:47:36.590822: Steps 14/47: batch_recall = 40.09, batch_ndcg = 40.55 
2025-02-12 23:47:37.692869: Steps 15/47: batch_recall = 56.78, batch_ndcg = 53.36 
2025-02-12 23:47:38.775177: Steps 16/47: batch_recall = 51.32, batch_ndcg = 47.55 
2025-02-12 23:47:39.822925: Steps 17/47: batch_recall = 58.28, batch_ndcg = 49.75 
2025-02-12 23:47:40.888321: Steps 18/47: batch_recall = 53.36, batch_ndcg = 49.44 
2025-02-12 23:47:41.951175: Steps 19/47: batch_recall = 60.00, batch_ndcg = 55.30 
2025-02-12 23:47:43.009243: Steps 20/47: batch_recall = 68.52, batch_ndcg = 62.31 
2025-02-12 23:47:44.065407: Steps 21/47: batch_recall = 65.09, batch_ndcg = 55.81 
2025-02-12 23:47:45.134855: Steps 22/47: batch_recall = 56.51, batch_ndcg = 51.80 
2025-02-12 23:47:46.200790: Steps 23/47: batch_recall = 63.90, batch_ndcg = 55.16 
2025-02-12 23:47:47.275336: Steps 24/47: batch_recall = 65.53, batch_ndcg = 57.95 
2025-02-12 23:47:48.323750: Steps 25/47: batch_recall = 65.93, batch_ndcg = 57.24 
2025-02-12 23:47:49.345797: Steps 26/47: batch_recall = 58.52, batch_ndcg = 52.54 
2025-02-12 23:47:50.391163: Steps 27/47: batch_recall = 61.18, batch_ndcg = 53.05 
2025-02-12 23:47:51.420953: Steps 28/47: batch_recall = 71.27, batch_ndcg = 59.54 
2025-02-12 23:47:52.429897: Steps 29/47: batch_recall = 69.15, batch_ndcg = 57.59 
2025-02-12 23:47:53.433055: Steps 30/47: batch_recall = 72.68, batch_ndcg = 64.33 
2025-02-12 23:47:54.460518: Steps 31/47: batch_recall = 67.24, batch_ndcg = 56.35 
2025-02-12 23:47:55.466659: Steps 32/47: batch_recall = 69.94, batch_ndcg = 63.51 
2025-02-12 23:47:56.469291: Steps 33/47: batch_recall = 78.77, batch_ndcg = 67.88 
2025-02-12 23:47:57.477032: Steps 34/47: batch_recall = 68.61, batch_ndcg = 56.43 
2025-02-12 23:47:58.451393: Steps 35/47: batch_recall = 77.36, batch_ndcg = 64.61 
2025-02-12 23:47:59.439912: Steps 36/47: batch_recall = 79.17, batch_ndcg = 63.78 
2025-02-12 23:48:00.426712: Steps 37/47: batch_recall = 83.72, batch_ndcg = 72.02 
2025-02-12 23:48:01.411516: Steps 38/47: batch_recall = 91.34, batch_ndcg = 72.83 
2025-02-12 23:48:02.387137: Steps 39/47: batch_recall = 88.16, batch_ndcg = 69.03 
2025-02-12 23:48:03.359859: Steps 40/47: batch_recall = 72.65, batch_ndcg = 62.39 
2025-02-12 23:48:04.321745: Steps 41/47: batch_recall = 86.14, batch_ndcg = 71.14 
2025-02-12 23:48:05.291781: Steps 42/47: batch_recall = 82.85, batch_ndcg = 65.01 
2025-02-12 23:48:06.258730: Steps 43/47: batch_recall = 88.13, batch_ndcg = 71.36 
2025-02-12 23:48:07.222044: Steps 44/47: batch_recall = 86.42, batch_ndcg = 69.10 
2025-02-12 23:48:08.162074: Steps 45/47: batch_recall = 95.98, batch_ndcg = 76.02 
2025-02-12 23:48:08.267968: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.47 
2025-02-12 23:48:08.268107: Epoch 53/1000, Test: Recall = 0.1212, NDCG = 0.1090  

2025-02-12 23:48:09.462381: Training Step 0/59: batchLoss = 3.1367, diffLoss = 15.2912, kgLoss = 0.0981
2025-02-12 23:48:10.385812: Training Step 1/59: batchLoss = 2.8022, diffLoss = 13.6647, kgLoss = 0.0865
2025-02-12 23:48:11.313033: Training Step 2/59: batchLoss = 2.6758, diffLoss = 13.0552, kgLoss = 0.0809
2025-02-12 23:48:12.236926: Training Step 3/59: batchLoss = 2.9072, diffLoss = 14.1977, kgLoss = 0.0846
2025-02-12 23:48:13.161058: Training Step 4/59: batchLoss = 3.0077, diffLoss = 14.6477, kgLoss = 0.0976
2025-02-12 23:48:14.088246: Training Step 5/59: batchLoss = 2.7347, diffLoss = 13.3725, kgLoss = 0.0753
2025-02-12 23:48:15.003705: Training Step 6/59: batchLoss = 2.6582, diffLoss = 12.9550, kgLoss = 0.0840
2025-02-12 23:48:15.938372: Training Step 7/59: batchLoss = 2.7340, diffLoss = 13.2877, kgLoss = 0.0956
2025-02-12 23:48:16.872024: Training Step 8/59: batchLoss = 2.7972, diffLoss = 13.6728, kgLoss = 0.0783
2025-02-12 23:48:17.800791: Training Step 9/59: batchLoss = 3.1077, diffLoss = 15.1674, kgLoss = 0.0928
2025-02-12 23:48:18.738986: Training Step 10/59: batchLoss = 2.9149, diffLoss = 14.2477, kgLoss = 0.0818
2025-02-12 23:48:19.675506: Training Step 11/59: batchLoss = 2.6087, diffLoss = 12.7314, kgLoss = 0.0780
2025-02-12 23:48:20.607962: Training Step 12/59: batchLoss = 2.8462, diffLoss = 13.8930, kgLoss = 0.0845
2025-02-12 23:48:21.547019: Training Step 13/59: batchLoss = 2.7196, diffLoss = 13.2971, kgLoss = 0.0753
2025-02-12 23:48:22.488306: Training Step 14/59: batchLoss = 2.8244, diffLoss = 13.8093, kgLoss = 0.0782
2025-02-12 23:48:23.421455: Training Step 15/59: batchLoss = 2.9129, diffLoss = 14.2165, kgLoss = 0.0870
2025-02-12 23:48:24.355076: Training Step 16/59: batchLoss = 2.9291, diffLoss = 14.2993, kgLoss = 0.0865
2025-02-12 23:48:25.289383: Training Step 17/59: batchLoss = 3.2082, diffLoss = 15.6601, kgLoss = 0.0952
2025-02-12 23:48:26.219820: Training Step 18/59: batchLoss = 2.8632, diffLoss = 13.9970, kgLoss = 0.0798
2025-02-12 23:48:27.145965: Training Step 19/59: batchLoss = 2.6753, diffLoss = 13.0513, kgLoss = 0.0813
2025-02-12 23:48:28.076885: Training Step 20/59: batchLoss = 2.9161, diffLoss = 14.2310, kgLoss = 0.0874
2025-02-12 23:48:29.002314: Training Step 21/59: batchLoss = 2.7578, diffLoss = 13.4862, kgLoss = 0.0758
2025-02-12 23:48:29.931507: Training Step 22/59: batchLoss = 2.9678, diffLoss = 14.4861, kgLoss = 0.0882
2025-02-12 23:48:30.857928: Training Step 23/59: batchLoss = 2.9588, diffLoss = 14.4388, kgLoss = 0.0888
2025-02-12 23:48:31.786863: Training Step 24/59: batchLoss = 2.7329, diffLoss = 13.3479, kgLoss = 0.0792
2025-02-12 23:48:32.715197: Training Step 25/59: batchLoss = 2.8577, diffLoss = 13.9306, kgLoss = 0.0895
2025-02-12 23:48:33.647847: Training Step 26/59: batchLoss = 2.9263, diffLoss = 14.3047, kgLoss = 0.0817
2025-02-12 23:48:34.578307: Training Step 27/59: batchLoss = 3.4302, diffLoss = 16.7624, kgLoss = 0.0972
2025-02-12 23:48:35.517364: Training Step 28/59: batchLoss = 2.8770, diffLoss = 14.0625, kgLoss = 0.0807
2025-02-12 23:48:36.450473: Training Step 29/59: batchLoss = 2.9799, diffLoss = 14.5412, kgLoss = 0.0895
2025-02-12 23:48:37.386554: Training Step 30/59: batchLoss = 2.8414, diffLoss = 13.8713, kgLoss = 0.0840
2025-02-12 23:48:38.326146: Training Step 31/59: batchLoss = 2.9686, diffLoss = 14.4984, kgLoss = 0.0862
2025-02-12 23:48:39.254213: Training Step 32/59: batchLoss = 2.9428, diffLoss = 14.3709, kgLoss = 0.0858
2025-02-12 23:48:40.202616: Training Step 33/59: batchLoss = 2.6999, diffLoss = 13.1776, kgLoss = 0.0805
2025-02-12 23:48:41.137988: Training Step 34/59: batchLoss = 3.3602, diffLoss = 16.3880, kgLoss = 0.1032
2025-02-12 23:48:42.068527: Training Step 35/59: batchLoss = 2.7882, diffLoss = 13.5959, kgLoss = 0.0863
2025-02-12 23:48:43.000425: Training Step 36/59: batchLoss = 3.0247, diffLoss = 14.7649, kgLoss = 0.0897
2025-02-12 23:48:43.934104: Training Step 37/59: batchLoss = 2.7240, diffLoss = 13.3043, kgLoss = 0.0789
2025-02-12 23:48:44.856886: Training Step 38/59: batchLoss = 3.0588, diffLoss = 14.9467, kgLoss = 0.0868
2025-02-12 23:48:45.780027: Training Step 39/59: batchLoss = 2.8415, diffLoss = 13.8979, kgLoss = 0.0774
2025-02-12 23:48:46.700836: Training Step 40/59: batchLoss = 2.9406, diffLoss = 14.3892, kgLoss = 0.0785
2025-02-12 23:48:47.642995: Training Step 41/59: batchLoss = 2.8381, diffLoss = 13.8749, kgLoss = 0.0789
2025-02-12 23:48:48.563483: Training Step 42/59: batchLoss = 3.1475, diffLoss = 15.3636, kgLoss = 0.0935
2025-02-12 23:48:49.495640: Training Step 43/59: batchLoss = 3.2740, diffLoss = 16.0058, kgLoss = 0.0910
2025-02-12 23:48:50.416067: Training Step 44/59: batchLoss = 2.9281, diffLoss = 14.3024, kgLoss = 0.0846
2025-02-12 23:48:51.337138: Training Step 45/59: batchLoss = 2.7360, diffLoss = 13.3656, kgLoss = 0.0787
2025-02-12 23:48:52.268232: Training Step 46/59: batchLoss = 2.9143, diffLoss = 14.2616, kgLoss = 0.0774
2025-02-12 23:48:53.200838: Training Step 47/59: batchLoss = 2.9793, diffLoss = 14.5658, kgLoss = 0.0827
2025-02-12 23:48:54.128892: Training Step 48/59: batchLoss = 3.1470, diffLoss = 15.3592, kgLoss = 0.0939
2025-02-12 23:48:55.062274: Training Step 49/59: batchLoss = 3.0429, diffLoss = 14.8645, kgLoss = 0.0875
2025-02-12 23:48:55.987285: Training Step 50/59: batchLoss = 3.0640, diffLoss = 14.9262, kgLoss = 0.0985
2025-02-12 23:48:56.926921: Training Step 51/59: batchLoss = 3.4781, diffLoss = 17.0047, kgLoss = 0.0964
2025-02-12 23:48:57.865067: Training Step 52/59: batchLoss = 2.8706, diffLoss = 14.0523, kgLoss = 0.0752
2025-02-12 23:48:58.797122: Training Step 53/59: batchLoss = 3.4597, diffLoss = 16.9253, kgLoss = 0.0933
2025-02-12 23:48:59.730278: Training Step 54/59: batchLoss = 3.1550, diffLoss = 15.4315, kgLoss = 0.0859
2025-02-12 23:49:00.655941: Training Step 55/59: batchLoss = 2.9834, diffLoss = 14.5553, kgLoss = 0.0904
2025-02-12 23:49:01.581475: Training Step 56/59: batchLoss = 3.0279, diffLoss = 14.7953, kgLoss = 0.0861
2025-02-12 23:49:02.424749: Training Step 57/59: batchLoss = 2.8830, diffLoss = 14.1186, kgLoss = 0.0741
2025-02-12 23:49:03.271718: Training Step 58/59: batchLoss = 3.0087, diffLoss = 14.6961, kgLoss = 0.0868
2025-02-12 23:49:03.370773: 
2025-02-12 23:49:03.371155: Epoch 54/1000, Train: epLoss = 0.4340, epDfLoss = 2.1195, epKgLoss = 0.0126  
2025-02-12 23:49:04.841822: Steps 0/47: batch_recall = 36.17, batch_ndcg = 46.43 
2025-02-12 23:49:06.130184: Steps 1/47: batch_recall = 37.32, batch_ndcg = 42.25 
2025-02-12 23:49:07.371653: Steps 2/47: batch_recall = 40.72, batch_ndcg = 46.81 
2025-02-12 23:49:08.625926: Steps 3/47: batch_recall = 43.79, batch_ndcg = 46.01 
2025-02-12 23:49:09.826620: Steps 4/47: batch_recall = 39.32, batch_ndcg = 45.54 
2025-02-12 23:49:11.039525: Steps 5/47: batch_recall = 33.39, batch_ndcg = 39.02 
2025-02-12 23:49:12.249441: Steps 6/47: batch_recall = 38.61, batch_ndcg = 41.83 
2025-02-12 23:49:13.421647: Steps 7/47: batch_recall = 44.52, batch_ndcg = 45.60 
2025-02-12 23:49:14.601079: Steps 8/47: batch_recall = 47.49, batch_ndcg = 52.29 
2025-02-12 23:49:15.739609: Steps 9/47: batch_recall = 46.25, batch_ndcg = 46.36 
2025-02-12 23:49:16.902656: Steps 10/47: batch_recall = 43.86, batch_ndcg = 42.92 
2025-02-12 23:49:18.038412: Steps 11/47: batch_recall = 55.74, batch_ndcg = 52.09 
2025-02-12 23:49:19.180090: Steps 12/47: batch_recall = 50.04, batch_ndcg = 49.08 
2025-02-12 23:49:20.305342: Steps 13/47: batch_recall = 48.18, batch_ndcg = 45.41 
2025-02-12 23:49:21.390456: Steps 14/47: batch_recall = 39.69, batch_ndcg = 39.92 
2025-02-12 23:49:22.480401: Steps 15/47: batch_recall = 56.01, batch_ndcg = 53.45 
2025-02-12 23:49:23.561209: Steps 16/47: batch_recall = 49.95, batch_ndcg = 46.84 
2025-02-12 23:49:24.606489: Steps 17/47: batch_recall = 57.85, batch_ndcg = 49.56 
2025-02-12 23:49:25.684333: Steps 18/47: batch_recall = 52.56, batch_ndcg = 49.49 
2025-02-12 23:49:26.758077: Steps 19/47: batch_recall = 58.47, batch_ndcg = 55.01 
2025-02-12 23:49:27.815410: Steps 20/47: batch_recall = 65.87, batch_ndcg = 60.70 
2025-02-12 23:49:28.873501: Steps 21/47: batch_recall = 66.96, batch_ndcg = 55.95 
2025-02-12 23:49:29.939063: Steps 22/47: batch_recall = 55.84, batch_ndcg = 51.87 
2025-02-12 23:49:30.988940: Steps 23/47: batch_recall = 64.57, batch_ndcg = 55.52 
2025-02-12 23:49:32.059974: Steps 24/47: batch_recall = 66.24, batch_ndcg = 58.60 
2025-02-12 23:49:33.105866: Steps 25/47: batch_recall = 67.12, batch_ndcg = 58.29 
2025-02-12 23:49:34.123210: Steps 26/47: batch_recall = 58.07, batch_ndcg = 52.34 
2025-02-12 23:49:35.158885: Steps 27/47: batch_recall = 61.42, batch_ndcg = 52.28 
2025-02-12 23:49:36.176663: Steps 28/47: batch_recall = 71.29, batch_ndcg = 59.39 
2025-02-12 23:49:37.190420: Steps 29/47: batch_recall = 69.31, batch_ndcg = 57.97 
2025-02-12 23:49:38.198278: Steps 30/47: batch_recall = 73.80, batch_ndcg = 64.20 
2025-02-12 23:49:39.216441: Steps 31/47: batch_recall = 66.67, batch_ndcg = 56.46 
2025-02-12 23:49:40.222316: Steps 32/47: batch_recall = 71.86, batch_ndcg = 65.95 
2025-02-12 23:49:41.225905: Steps 33/47: batch_recall = 79.33, batch_ndcg = 68.22 
2025-02-12 23:49:42.235865: Steps 34/47: batch_recall = 66.67, batch_ndcg = 56.53 
2025-02-12 23:49:43.225165: Steps 35/47: batch_recall = 78.54, batch_ndcg = 65.21 
2025-02-12 23:49:44.241828: Steps 36/47: batch_recall = 77.47, batch_ndcg = 63.76 
2025-02-12 23:49:45.236039: Steps 37/47: batch_recall = 83.80, batch_ndcg = 72.34 
2025-02-12 23:49:46.223470: Steps 38/47: batch_recall = 90.31, batch_ndcg = 73.27 
2025-02-12 23:49:47.206026: Steps 39/47: batch_recall = 88.48, batch_ndcg = 68.70 
2025-02-12 23:49:48.174646: Steps 40/47: batch_recall = 72.70, batch_ndcg = 63.53 
2025-02-12 23:49:49.140536: Steps 41/47: batch_recall = 87.40, batch_ndcg = 71.99 
2025-02-12 23:49:50.106724: Steps 42/47: batch_recall = 82.85, batch_ndcg = 64.38 
2025-02-12 23:49:51.074106: Steps 43/47: batch_recall = 89.80, batch_ndcg = 71.48 
2025-02-12 23:49:52.047105: Steps 44/47: batch_recall = 88.77, batch_ndcg = 69.34 
2025-02-12 23:49:52.988585: Steps 45/47: batch_recall = 93.90, batch_ndcg = 75.19 
2025-02-12 23:49:53.093746: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-12 23:49:53.093835: Epoch 54/1000, Test: Recall = 0.1214, NDCG = 0.1091  

2025-02-12 23:49:54.296744: Training Step 0/59: batchLoss = 2.6871, diffLoss = 13.0876, kgLoss = 0.0870
2025-02-12 23:49:55.222120: Training Step 1/59: batchLoss = 3.1172, diffLoss = 15.1802, kgLoss = 0.1014
2025-02-12 23:49:56.144476: Training Step 2/59: batchLoss = 2.7162, diffLoss = 13.2429, kgLoss = 0.0845
2025-02-12 23:49:57.067499: Training Step 3/59: batchLoss = 3.4375, diffLoss = 16.7539, kgLoss = 0.1084
2025-02-12 23:49:57.983362: Training Step 4/59: batchLoss = 3.0194, diffLoss = 14.7314, kgLoss = 0.0914
2025-02-12 23:49:58.905548: Training Step 5/59: batchLoss = 2.8245, diffLoss = 13.7414, kgLoss = 0.0953
2025-02-12 23:49:59.830433: Training Step 6/59: batchLoss = 3.0041, diffLoss = 14.6683, kgLoss = 0.0880
2025-02-12 23:50:00.755787: Training Step 7/59: batchLoss = 2.7286, diffLoss = 13.3358, kgLoss = 0.0768
2025-02-12 23:50:01.692880: Training Step 8/59: batchLoss = 2.8350, diffLoss = 13.8490, kgLoss = 0.0815
2025-02-12 23:50:02.624313: Training Step 9/59: batchLoss = 2.7587, diffLoss = 13.4468, kgLoss = 0.0867
2025-02-12 23:50:03.551021: Training Step 10/59: batchLoss = 2.8108, diffLoss = 13.7236, kgLoss = 0.0827
2025-02-12 23:50:04.476343: Training Step 11/59: batchLoss = 2.4800, diffLoss = 12.0964, kgLoss = 0.0759
2025-02-12 23:50:05.408222: Training Step 12/59: batchLoss = 2.6821, diffLoss = 13.0723, kgLoss = 0.0846
2025-02-12 23:50:06.344269: Training Step 13/59: batchLoss = 3.1941, diffLoss = 15.6127, kgLoss = 0.0894
2025-02-12 23:50:07.279778: Training Step 14/59: batchLoss = 2.9926, diffLoss = 14.5867, kgLoss = 0.0941
2025-02-12 23:50:08.215708: Training Step 15/59: batchLoss = 3.1565, diffLoss = 15.4267, kgLoss = 0.0889
2025-02-12 23:50:09.151344: Training Step 16/59: batchLoss = 2.8818, diffLoss = 14.0743, kgLoss = 0.0836
2025-02-12 23:50:10.080160: Training Step 17/59: batchLoss = 2.9668, diffLoss = 14.4876, kgLoss = 0.0866
2025-02-12 23:50:11.005816: Training Step 18/59: batchLoss = 3.1739, diffLoss = 15.4940, kgLoss = 0.0939
2025-02-12 23:50:11.934378: Training Step 19/59: batchLoss = 3.0273, diffLoss = 14.7813, kgLoss = 0.0888
2025-02-12 23:50:12.862368: Training Step 20/59: batchLoss = 2.6235, diffLoss = 12.8482, kgLoss = 0.0674
2025-02-12 23:50:13.787212: Training Step 21/59: batchLoss = 2.8677, diffLoss = 14.0014, kgLoss = 0.0842
2025-02-12 23:50:14.707417: Training Step 22/59: batchLoss = 2.6666, diffLoss = 13.0164, kgLoss = 0.0792
2025-02-12 23:50:15.635519: Training Step 23/59: batchLoss = 2.9764, diffLoss = 14.5101, kgLoss = 0.0930
2025-02-12 23:50:16.566342: Training Step 24/59: batchLoss = 3.1132, diffLoss = 15.2014, kgLoss = 0.0911
2025-02-12 23:50:17.493567: Training Step 25/59: batchLoss = 2.8853, diffLoss = 14.0987, kgLoss = 0.0819
2025-02-12 23:50:18.420458: Training Step 26/59: batchLoss = 2.8013, diffLoss = 13.6763, kgLoss = 0.0826
2025-02-12 23:50:19.352655: Training Step 27/59: batchLoss = 2.6564, diffLoss = 12.9561, kgLoss = 0.0815
2025-02-12 23:50:20.282742: Training Step 28/59: batchLoss = 2.8808, diffLoss = 14.0991, kgLoss = 0.0763
2025-02-12 23:50:21.214687: Training Step 29/59: batchLoss = 3.0803, diffLoss = 15.0287, kgLoss = 0.0932
2025-02-12 23:50:22.150972: Training Step 30/59: batchLoss = 3.0447, diffLoss = 14.8748, kgLoss = 0.0872
2025-02-12 23:50:23.083014: Training Step 31/59: batchLoss = 2.8654, diffLoss = 14.0143, kgLoss = 0.0781
2025-02-12 23:50:24.017034: Training Step 32/59: batchLoss = 3.0051, diffLoss = 14.6689, kgLoss = 0.0891
2025-02-12 23:50:24.950211: Training Step 33/59: batchLoss = 3.5126, diffLoss = 17.1417, kgLoss = 0.1053
2025-02-12 23:50:25.889813: Training Step 34/59: batchLoss = 3.1968, diffLoss = 15.6221, kgLoss = 0.0905
2025-02-12 23:50:26.826066: Training Step 35/59: batchLoss = 2.9310, diffLoss = 14.3116, kgLoss = 0.0859
2025-02-12 23:50:27.756797: Training Step 36/59: batchLoss = 2.7509, diffLoss = 13.4588, kgLoss = 0.0739
2025-02-12 23:50:28.688226: Training Step 37/59: batchLoss = 3.1047, diffLoss = 15.1641, kgLoss = 0.0898
2025-02-12 23:50:29.617510: Training Step 38/59: batchLoss = 2.9942, diffLoss = 14.6535, kgLoss = 0.0794
2025-02-12 23:50:30.536231: Training Step 39/59: batchLoss = 3.0441, diffLoss = 14.8643, kgLoss = 0.0890
2025-02-12 23:50:31.456474: Training Step 40/59: batchLoss = 2.9867, diffLoss = 14.6090, kgLoss = 0.0811
2025-02-12 23:50:32.377593: Training Step 41/59: batchLoss = 2.8931, diffLoss = 14.1262, kgLoss = 0.0848
2025-02-12 23:50:33.297911: Training Step 42/59: batchLoss = 3.1371, diffLoss = 15.3315, kgLoss = 0.0885
2025-02-12 23:50:34.224196: Training Step 43/59: batchLoss = 3.0309, diffLoss = 14.8356, kgLoss = 0.0797
2025-02-12 23:50:35.154407: Training Step 44/59: batchLoss = 3.2034, diffLoss = 15.6629, kgLoss = 0.0885
2025-02-12 23:50:36.084875: Training Step 45/59: batchLoss = 3.2086, diffLoss = 15.6831, kgLoss = 0.0900
2025-02-12 23:50:37.020612: Training Step 46/59: batchLoss = 2.7041, diffLoss = 13.2055, kgLoss = 0.0787
2025-02-12 23:50:37.952725: Training Step 47/59: batchLoss = 2.8626, diffLoss = 13.9782, kgLoss = 0.0837
2025-02-12 23:50:38.889062: Training Step 48/59: batchLoss = 3.0062, diffLoss = 14.7026, kgLoss = 0.0821
2025-02-12 23:50:39.824020: Training Step 49/59: batchLoss = 2.8730, diffLoss = 14.0517, kgLoss = 0.0783
2025-02-12 23:50:40.757683: Training Step 50/59: batchLoss = 2.7420, diffLoss = 13.4039, kgLoss = 0.0765
2025-02-12 23:50:41.687046: Training Step 51/59: batchLoss = 2.7718, diffLoss = 13.5513, kgLoss = 0.0770
2025-02-12 23:50:42.623609: Training Step 52/59: batchLoss = 2.7448, diffLoss = 13.3877, kgLoss = 0.0840
2025-02-12 23:50:43.554990: Training Step 53/59: batchLoss = 2.7982, diffLoss = 13.6685, kgLoss = 0.0806
2025-02-12 23:50:44.487090: Training Step 54/59: batchLoss = 3.2576, diffLoss = 15.9396, kgLoss = 0.0871
2025-02-12 23:50:45.440263: Training Step 55/59: batchLoss = 2.8715, diffLoss = 14.0431, kgLoss = 0.0786
2025-02-12 23:50:46.364885: Training Step 56/59: batchLoss = 2.6925, diffLoss = 13.1643, kgLoss = 0.0745
2025-02-12 23:50:47.209428: Training Step 57/59: batchLoss = 3.1107, diffLoss = 15.1845, kgLoss = 0.0922
2025-02-12 23:50:48.056970: Training Step 58/59: batchLoss = 3.3629, diffLoss = 16.4167, kgLoss = 0.0994
2025-02-12 23:50:48.154619: 
2025-02-12 23:50:48.154944: Epoch 55/1000, Train: epLoss = 0.4344, epDfLoss = 2.1214, epKgLoss = 0.0126  
2025-02-12 23:50:49.620539: Steps 0/47: batch_recall = 37.28, batch_ndcg = 47.63 
2025-02-12 23:50:50.915185: Steps 1/47: batch_recall = 36.03, batch_ndcg = 41.82 
2025-02-12 23:50:52.167168: Steps 2/47: batch_recall = 40.50, batch_ndcg = 46.00 
2025-02-12 23:50:53.430402: Steps 3/47: batch_recall = 44.30, batch_ndcg = 46.42 
2025-02-12 23:50:54.630219: Steps 4/47: batch_recall = 39.26, batch_ndcg = 45.19 
2025-02-12 23:50:55.857994: Steps 5/47: batch_recall = 34.50, batch_ndcg = 40.03 
2025-02-12 23:50:57.057558: Steps 6/47: batch_recall = 38.52, batch_ndcg = 41.84 
2025-02-12 23:50:58.233182: Steps 7/47: batch_recall = 43.91, batch_ndcg = 45.66 
2025-02-12 23:50:59.414639: Steps 8/47: batch_recall = 47.44, batch_ndcg = 51.73 
2025-02-12 23:51:00.568153: Steps 9/47: batch_recall = 46.97, batch_ndcg = 46.68 
2025-02-12 23:51:01.729201: Steps 10/47: batch_recall = 43.52, batch_ndcg = 43.64 
2025-02-12 23:51:02.870614: Steps 11/47: batch_recall = 55.73, batch_ndcg = 53.17 
2025-02-12 23:51:04.009737: Steps 12/47: batch_recall = 50.19, batch_ndcg = 49.08 
2025-02-12 23:51:05.134770: Steps 13/47: batch_recall = 48.64, batch_ndcg = 46.22 
2025-02-12 23:51:06.220658: Steps 14/47: batch_recall = 40.18, batch_ndcg = 40.33 
2025-02-12 23:51:07.308316: Steps 15/47: batch_recall = 57.48, batch_ndcg = 53.81 
2025-02-12 23:51:08.383097: Steps 16/47: batch_recall = 50.29, batch_ndcg = 47.80 
2025-02-12 23:51:09.437474: Steps 17/47: batch_recall = 58.09, batch_ndcg = 49.57 
2025-02-12 23:51:10.520918: Steps 18/47: batch_recall = 53.47, batch_ndcg = 49.45 
2025-02-12 23:51:11.587792: Steps 19/47: batch_recall = 59.86, batch_ndcg = 55.51 
2025-02-12 23:51:12.640603: Steps 20/47: batch_recall = 67.30, batch_ndcg = 62.15 
2025-02-12 23:51:13.689536: Steps 21/47: batch_recall = 65.49, batch_ndcg = 55.40 
2025-02-12 23:51:14.751483: Steps 22/47: batch_recall = 56.02, batch_ndcg = 51.90 
2025-02-12 23:51:15.799677: Steps 23/47: batch_recall = 65.29, batch_ndcg = 56.38 
2025-02-12 23:51:16.867825: Steps 24/47: batch_recall = 66.55, batch_ndcg = 58.88 
2025-02-12 23:51:17.924305: Steps 25/47: batch_recall = 65.70, batch_ndcg = 58.13 
2025-02-12 23:51:18.938103: Steps 26/47: batch_recall = 58.28, batch_ndcg = 52.70 
2025-02-12 23:51:19.967400: Steps 27/47: batch_recall = 61.11, batch_ndcg = 52.26 
2025-02-12 23:51:20.987700: Steps 28/47: batch_recall = 70.21, batch_ndcg = 59.23 
2025-02-12 23:51:21.995313: Steps 29/47: batch_recall = 67.64, batch_ndcg = 56.76 
2025-02-12 23:51:23.003311: Steps 30/47: batch_recall = 73.13, batch_ndcg = 63.78 
2025-02-12 23:51:24.030429: Steps 31/47: batch_recall = 67.95, batch_ndcg = 56.45 
2025-02-12 23:51:25.037009: Steps 32/47: batch_recall = 71.32, batch_ndcg = 64.91 
2025-02-12 23:51:26.043407: Steps 33/47: batch_recall = 79.35, batch_ndcg = 69.01 
2025-02-12 23:51:27.062649: Steps 34/47: batch_recall = 69.11, batch_ndcg = 56.61 
2025-02-12 23:51:28.052783: Steps 35/47: batch_recall = 78.15, batch_ndcg = 66.12 
2025-02-12 23:51:29.053028: Steps 36/47: batch_recall = 76.48, batch_ndcg = 62.57 
2025-02-12 23:51:30.029246: Steps 37/47: batch_recall = 84.67, batch_ndcg = 72.69 
2025-02-12 23:51:31.013855: Steps 38/47: batch_recall = 90.49, batch_ndcg = 72.63 
2025-02-12 23:51:31.987380: Steps 39/47: batch_recall = 87.84, batch_ndcg = 68.84 
2025-02-12 23:51:32.947872: Steps 40/47: batch_recall = 73.32, batch_ndcg = 63.19 
2025-02-12 23:51:33.916541: Steps 41/47: batch_recall = 87.05, batch_ndcg = 71.83 
2025-02-12 23:51:34.882202: Steps 42/47: batch_recall = 81.08, batch_ndcg = 64.35 
2025-02-12 23:51:35.846097: Steps 43/47: batch_recall = 87.67, batch_ndcg = 71.15 
2025-02-12 23:51:36.811223: Steps 44/47: batch_recall = 86.98, batch_ndcg = 69.30 
2025-02-12 23:51:37.746902: Steps 45/47: batch_recall = 95.23, batch_ndcg = 75.02 
2025-02-12 23:51:37.849545: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-12 23:51:37.849673: Epoch 55/1000, Test: Recall = 0.1214, NDCG = 0.1093  

2025-02-12 23:51:39.047435: Training Step 0/59: batchLoss = 2.8161, diffLoss = 13.7118, kgLoss = 0.0922
2025-02-12 23:51:39.971697: Training Step 1/59: batchLoss = 2.5995, diffLoss = 12.6904, kgLoss = 0.0768
2025-02-12 23:51:40.893122: Training Step 2/59: batchLoss = 2.7601, diffLoss = 13.4936, kgLoss = 0.0767
2025-02-12 23:51:41.819732: Training Step 3/59: batchLoss = 2.5987, diffLoss = 12.6705, kgLoss = 0.0807
2025-02-12 23:51:42.736879: Training Step 4/59: batchLoss = 2.6931, diffLoss = 13.1487, kgLoss = 0.0792
2025-02-12 23:51:43.661627: Training Step 5/59: batchLoss = 2.8619, diffLoss = 13.9680, kgLoss = 0.0854
2025-02-12 23:51:44.596008: Training Step 6/59: batchLoss = 2.9421, diffLoss = 14.3570, kgLoss = 0.0884
2025-02-12 23:51:45.532212: Training Step 7/59: batchLoss = 2.6383, diffLoss = 12.8748, kgLoss = 0.0792
2025-02-12 23:51:46.468425: Training Step 8/59: batchLoss = 2.8765, diffLoss = 14.0384, kgLoss = 0.0860
2025-02-12 23:51:47.404096: Training Step 9/59: batchLoss = 3.0187, diffLoss = 14.7365, kgLoss = 0.0893
2025-02-12 23:51:48.336794: Training Step 10/59: batchLoss = 2.8786, diffLoss = 14.0515, kgLoss = 0.0854
2025-02-12 23:51:49.276720: Training Step 11/59: batchLoss = 2.7426, diffLoss = 13.3940, kgLoss = 0.0797
2025-02-12 23:51:50.210875: Training Step 12/59: batchLoss = 2.9658, diffLoss = 14.4916, kgLoss = 0.0843
2025-02-12 23:51:51.148730: Training Step 13/59: batchLoss = 2.7907, diffLoss = 13.6189, kgLoss = 0.0836
2025-02-12 23:51:52.081424: Training Step 14/59: batchLoss = 2.7844, diffLoss = 13.5975, kgLoss = 0.0811
2025-02-12 23:51:53.021610: Training Step 15/59: batchLoss = 2.7664, diffLoss = 13.5122, kgLoss = 0.0800
2025-02-12 23:51:53.951785: Training Step 16/59: batchLoss = 2.6883, diffLoss = 13.1198, kgLoss = 0.0804
2025-02-12 23:51:54.882643: Training Step 17/59: batchLoss = 2.8148, diffLoss = 13.7265, kgLoss = 0.0868
2025-02-12 23:51:55.815491: Training Step 18/59: batchLoss = 3.2579, diffLoss = 15.9215, kgLoss = 0.0919
2025-02-12 23:51:56.741298: Training Step 19/59: batchLoss = 2.6913, diffLoss = 13.1292, kgLoss = 0.0818
2025-02-12 23:51:57.673017: Training Step 20/59: batchLoss = 2.9256, diffLoss = 14.2932, kgLoss = 0.0837
2025-02-12 23:51:58.605656: Training Step 21/59: batchLoss = 2.8831, diffLoss = 14.0908, kgLoss = 0.0812
2025-02-12 23:51:59.528590: Training Step 22/59: batchLoss = 2.8051, diffLoss = 13.7158, kgLoss = 0.0775
2025-02-12 23:52:00.460058: Training Step 23/59: batchLoss = 2.9979, diffLoss = 14.6291, kgLoss = 0.0901
2025-02-12 23:52:01.391888: Training Step 24/59: batchLoss = 2.8351, diffLoss = 13.8576, kgLoss = 0.0795
2025-02-12 23:52:02.322609: Training Step 25/59: batchLoss = 3.0905, diffLoss = 15.1059, kgLoss = 0.0866
2025-02-12 23:52:03.260026: Training Step 26/59: batchLoss = 2.9545, diffLoss = 14.4236, kgLoss = 0.0872
2025-02-12 23:52:04.186176: Training Step 27/59: batchLoss = 2.8091, diffLoss = 13.7141, kgLoss = 0.0829
2025-02-12 23:52:05.119700: Training Step 28/59: batchLoss = 3.2594, diffLoss = 15.8865, kgLoss = 0.1027
2025-02-12 23:52:06.067140: Training Step 29/59: batchLoss = 2.7832, diffLoss = 13.5849, kgLoss = 0.0828
2025-02-12 23:52:07.015829: Training Step 30/59: batchLoss = 2.8585, diffLoss = 13.9185, kgLoss = 0.0935
2025-02-12 23:52:07.946971: Training Step 31/59: batchLoss = 2.9963, diffLoss = 14.6412, kgLoss = 0.0850
2025-02-12 23:52:08.890149: Training Step 32/59: batchLoss = 2.7760, diffLoss = 13.5654, kgLoss = 0.0787
2025-02-12 23:52:09.819580: Training Step 33/59: batchLoss = 3.1543, diffLoss = 15.4260, kgLoss = 0.0863
2025-02-12 23:52:10.750063: Training Step 34/59: batchLoss = 2.8890, diffLoss = 14.1104, kgLoss = 0.0836
2025-02-12 23:52:11.680130: Training Step 35/59: batchLoss = 3.0083, diffLoss = 14.6643, kgLoss = 0.0942
2025-02-12 23:52:12.605225: Training Step 36/59: batchLoss = 3.2180, diffLoss = 15.7287, kgLoss = 0.0903
2025-02-12 23:52:13.539654: Training Step 37/59: batchLoss = 2.9911, diffLoss = 14.6220, kgLoss = 0.0833
2025-02-12 23:52:14.467841: Training Step 38/59: batchLoss = 3.2129, diffLoss = 15.7127, kgLoss = 0.0880
2025-02-12 23:52:15.395004: Training Step 39/59: batchLoss = 2.9714, diffLoss = 14.5379, kgLoss = 0.0797
2025-02-12 23:52:16.317738: Training Step 40/59: batchLoss = 2.8964, diffLoss = 14.1520, kgLoss = 0.0825
2025-02-12 23:52:17.238933: Training Step 41/59: batchLoss = 2.6502, diffLoss = 12.9437, kgLoss = 0.0768
2025-02-12 23:52:18.219933: Training Step 42/59: batchLoss = 3.2238, diffLoss = 15.7349, kgLoss = 0.0961
2025-02-12 23:52:19.156571: Training Step 43/59: batchLoss = 3.3409, diffLoss = 16.3048, kgLoss = 0.0999
2025-02-12 23:52:20.094780: Training Step 44/59: batchLoss = 2.7552, diffLoss = 13.4685, kgLoss = 0.0769
2025-02-12 23:52:21.036751: Training Step 45/59: batchLoss = 2.9997, diffLoss = 14.6473, kgLoss = 0.0879
2025-02-12 23:52:21.985791: Training Step 46/59: batchLoss = 3.0079, diffLoss = 14.7219, kgLoss = 0.0794
2025-02-12 23:52:22.922866: Training Step 47/59: batchLoss = 3.2061, diffLoss = 15.6461, kgLoss = 0.0961
2025-02-12 23:52:23.859430: Training Step 48/59: batchLoss = 2.7109, diffLoss = 13.2299, kgLoss = 0.0811
2025-02-12 23:52:24.797708: Training Step 49/59: batchLoss = 2.8630, diffLoss = 13.9913, kgLoss = 0.0809
2025-02-12 23:52:25.736861: Training Step 50/59: batchLoss = 3.0051, diffLoss = 14.6947, kgLoss = 0.0827
2025-02-12 23:52:26.675176: Training Step 51/59: batchLoss = 3.2199, diffLoss = 15.7352, kgLoss = 0.0911
2025-02-12 23:52:27.604323: Training Step 52/59: batchLoss = 3.2134, diffLoss = 15.6975, kgLoss = 0.0924
2025-02-12 23:52:28.534203: Training Step 53/59: batchLoss = 3.3444, diffLoss = 16.3436, kgLoss = 0.0945
2025-02-12 23:52:29.459420: Training Step 54/59: batchLoss = 2.5989, diffLoss = 12.6798, kgLoss = 0.0787
2025-02-12 23:52:30.387138: Training Step 55/59: batchLoss = 3.2951, diffLoss = 16.1116, kgLoss = 0.0909
2025-02-12 23:52:31.313599: Training Step 56/59: batchLoss = 3.3618, diffLoss = 16.4068, kgLoss = 0.1006
2025-02-12 23:52:32.152431: Training Step 57/59: batchLoss = 3.3992, diffLoss = 16.6139, kgLoss = 0.0955
2025-02-12 23:52:33.005496: Training Step 58/59: batchLoss = 2.8590, diffLoss = 13.9710, kgLoss = 0.0810
2025-02-12 23:52:33.105500: 
2025-02-12 23:52:33.106056: Epoch 56/1000, Train: epLoss = 0.4339, epDfLoss = 2.1189, epKgLoss = 0.0126  
2025-02-12 23:52:34.580330: Steps 0/47: batch_recall = 38.03, batch_ndcg = 48.48 
2025-02-12 23:52:35.895964: Steps 1/47: batch_recall = 37.34, batch_ndcg = 42.64 
2025-02-12 23:52:37.157208: Steps 2/47: batch_recall = 40.98, batch_ndcg = 46.24 
2025-02-12 23:52:38.419618: Steps 3/47: batch_recall = 43.60, batch_ndcg = 46.47 
2025-02-12 23:52:39.627048: Steps 4/47: batch_recall = 38.57, batch_ndcg = 45.34 
2025-02-12 23:52:40.851511: Steps 5/47: batch_recall = 33.95, batch_ndcg = 39.48 
2025-02-12 23:52:42.067164: Steps 6/47: batch_recall = 39.12, batch_ndcg = 41.76 
2025-02-12 23:52:43.254442: Steps 7/47: batch_recall = 43.56, batch_ndcg = 45.41 
2025-02-12 23:52:44.439970: Steps 8/47: batch_recall = 48.19, batch_ndcg = 52.90 
2025-02-12 23:52:45.579217: Steps 9/47: batch_recall = 45.44, batch_ndcg = 46.04 
2025-02-12 23:52:46.742616: Steps 10/47: batch_recall = 43.21, batch_ndcg = 42.92 
2025-02-12 23:52:47.881975: Steps 11/47: batch_recall = 55.85, batch_ndcg = 52.89 
2025-02-12 23:52:49.037901: Steps 12/47: batch_recall = 48.82, batch_ndcg = 47.94 
2025-02-12 23:52:50.204455: Steps 13/47: batch_recall = 48.25, batch_ndcg = 45.19 
2025-02-12 23:52:51.296520: Steps 14/47: batch_recall = 39.97, batch_ndcg = 40.96 
2025-02-12 23:52:52.440841: Steps 15/47: batch_recall = 56.50, batch_ndcg = 53.69 
2025-02-12 23:52:53.534193: Steps 16/47: batch_recall = 49.57, batch_ndcg = 46.40 
2025-02-12 23:52:54.606839: Steps 17/47: batch_recall = 57.30, batch_ndcg = 49.43 
2025-02-12 23:52:55.697232: Steps 18/47: batch_recall = 53.49, batch_ndcg = 48.93 
2025-02-12 23:52:56.776686: Steps 19/47: batch_recall = 59.46, batch_ndcg = 55.17 
2025-02-12 23:52:57.837823: Steps 20/47: batch_recall = 66.53, batch_ndcg = 61.52 
2025-02-12 23:52:58.899741: Steps 21/47: batch_recall = 63.91, batch_ndcg = 55.55 
2025-02-12 23:52:59.964689: Steps 22/47: batch_recall = 54.76, batch_ndcg = 51.24 
2025-02-12 23:53:01.025797: Steps 23/47: batch_recall = 62.78, batch_ndcg = 54.68 
2025-02-12 23:53:02.092711: Steps 24/47: batch_recall = 67.07, batch_ndcg = 58.89 
2025-02-12 23:53:03.126865: Steps 25/47: batch_recall = 66.31, batch_ndcg = 57.84 
2025-02-12 23:53:04.130839: Steps 26/47: batch_recall = 58.88, batch_ndcg = 52.71 
2025-02-12 23:53:05.148617: Steps 27/47: batch_recall = 61.63, batch_ndcg = 52.60 
2025-02-12 23:53:06.159406: Steps 28/47: batch_recall = 70.38, batch_ndcg = 59.16 
2025-02-12 23:53:07.165441: Steps 29/47: batch_recall = 69.35, batch_ndcg = 57.77 
2025-02-12 23:53:08.172434: Steps 30/47: batch_recall = 73.37, batch_ndcg = 64.19 
2025-02-12 23:53:09.194152: Steps 31/47: batch_recall = 67.24, batch_ndcg = 56.02 
2025-02-12 23:53:10.202490: Steps 32/47: batch_recall = 70.64, batch_ndcg = 64.18 
2025-02-12 23:53:11.216821: Steps 33/47: batch_recall = 79.26, batch_ndcg = 67.78 
2025-02-12 23:53:12.227420: Steps 34/47: batch_recall = 67.00, batch_ndcg = 56.32 
2025-02-12 23:53:13.219169: Steps 35/47: batch_recall = 77.83, batch_ndcg = 65.84 
2025-02-12 23:53:14.206546: Steps 36/47: batch_recall = 78.24, batch_ndcg = 63.97 
2025-02-12 23:53:15.178630: Steps 37/47: batch_recall = 84.29, batch_ndcg = 72.19 
2025-02-12 23:53:16.165238: Steps 38/47: batch_recall = 89.23, batch_ndcg = 71.93 
2025-02-12 23:53:17.196717: Steps 39/47: batch_recall = 88.05, batch_ndcg = 68.54 
2025-02-12 23:53:18.172331: Steps 40/47: batch_recall = 71.93, batch_ndcg = 61.76 
2025-02-12 23:53:19.134153: Steps 41/47: batch_recall = 86.05, batch_ndcg = 71.79 
2025-02-12 23:53:20.094260: Steps 42/47: batch_recall = 82.43, batch_ndcg = 64.64 
2025-02-12 23:53:21.055966: Steps 43/47: batch_recall = 87.85, batch_ndcg = 71.24 
2025-02-12 23:53:22.017216: Steps 44/47: batch_recall = 84.76, batch_ndcg = 68.26 
2025-02-12 23:53:22.954300: Steps 45/47: batch_recall = 94.72, batch_ndcg = 75.09 
2025-02-12 23:53:23.060346: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.64 
2025-02-12 23:53:23.060444: Epoch 56/1000, Test: Recall = 0.1208, NDCG = 0.1089  

2025-02-12 23:53:24.258638: Training Step 0/59: batchLoss = 2.7026, diffLoss = 13.1645, kgLoss = 0.0872
2025-02-12 23:53:25.187053: Training Step 1/59: batchLoss = 2.9156, diffLoss = 14.2356, kgLoss = 0.0856
2025-02-12 23:53:26.120818: Training Step 2/59: batchLoss = 2.9311, diffLoss = 14.2947, kgLoss = 0.0902
2025-02-12 23:53:27.057373: Training Step 3/59: batchLoss = 3.0301, diffLoss = 14.7626, kgLoss = 0.0969
2025-02-12 23:53:27.998226: Training Step 4/59: batchLoss = 2.5800, diffLoss = 12.5899, kgLoss = 0.0775
2025-02-12 23:53:28.928071: Training Step 5/59: batchLoss = 2.9676, diffLoss = 14.5067, kgLoss = 0.0828
2025-02-12 23:53:29.862555: Training Step 6/59: batchLoss = 2.7051, diffLoss = 13.1510, kgLoss = 0.0937
2025-02-12 23:53:30.791425: Training Step 7/59: batchLoss = 2.8854, diffLoss = 14.0997, kgLoss = 0.0818
2025-02-12 23:53:31.722019: Training Step 8/59: batchLoss = 2.6657, diffLoss = 12.9878, kgLoss = 0.0851
2025-02-12 23:53:32.662956: Training Step 9/59: batchLoss = 2.9230, diffLoss = 14.2938, kgLoss = 0.0803
2025-02-12 23:53:33.596074: Training Step 10/59: batchLoss = 2.7584, diffLoss = 13.4644, kgLoss = 0.0819
2025-02-12 23:53:34.531871: Training Step 11/59: batchLoss = 3.0250, diffLoss = 14.7912, kgLoss = 0.0834
2025-02-12 23:53:35.469727: Training Step 12/59: batchLoss = 2.9491, diffLoss = 14.4120, kgLoss = 0.0834
2025-02-12 23:53:36.403524: Training Step 13/59: batchLoss = 2.9121, diffLoss = 14.1994, kgLoss = 0.0903
2025-02-12 23:53:37.332876: Training Step 14/59: batchLoss = 2.9826, diffLoss = 14.5708, kgLoss = 0.0855
2025-02-12 23:53:38.262423: Training Step 15/59: batchLoss = 2.5124, diffLoss = 12.2427, kgLoss = 0.0798
2025-02-12 23:53:39.190504: Training Step 16/59: batchLoss = 2.9244, diffLoss = 14.3000, kgLoss = 0.0805
2025-02-12 23:53:40.118561: Training Step 17/59: batchLoss = 2.9357, diffLoss = 14.3272, kgLoss = 0.0879
2025-02-12 23:53:41.050393: Training Step 18/59: batchLoss = 2.9302, diffLoss = 14.3073, kgLoss = 0.0859
2025-02-12 23:53:41.980244: Training Step 19/59: batchLoss = 2.7401, diffLoss = 13.3840, kgLoss = 0.0791
2025-02-12 23:53:42.914151: Training Step 20/59: batchLoss = 3.1074, diffLoss = 15.1688, kgLoss = 0.0921
2025-02-12 23:53:43.849673: Training Step 21/59: batchLoss = 2.8365, diffLoss = 13.8416, kgLoss = 0.0852
2025-02-12 23:53:44.785813: Training Step 22/59: batchLoss = 2.7624, diffLoss = 13.4720, kgLoss = 0.0850
2025-02-12 23:53:45.724608: Training Step 23/59: batchLoss = 2.8825, diffLoss = 14.0894, kgLoss = 0.0808
2025-02-12 23:53:46.659953: Training Step 24/59: batchLoss = 2.6394, diffLoss = 12.9145, kgLoss = 0.0706
2025-02-12 23:53:47.601101: Training Step 25/59: batchLoss = 2.8300, diffLoss = 13.8114, kgLoss = 0.0846
2025-02-12 23:53:48.540697: Training Step 26/59: batchLoss = 2.9757, diffLoss = 14.4691, kgLoss = 0.1024
2025-02-12 23:53:49.473931: Training Step 27/59: batchLoss = 2.7020, diffLoss = 13.2156, kgLoss = 0.0736
2025-02-12 23:53:50.399895: Training Step 28/59: batchLoss = 2.9992, diffLoss = 14.6381, kgLoss = 0.0895
2025-02-12 23:53:51.341955: Training Step 29/59: batchLoss = 2.9179, diffLoss = 14.2540, kgLoss = 0.0838
2025-02-12 23:53:52.281762: Training Step 30/59: batchLoss = 3.2346, diffLoss = 15.7950, kgLoss = 0.0945
2025-02-12 23:53:53.210496: Training Step 31/59: batchLoss = 2.9059, diffLoss = 14.1464, kgLoss = 0.0957
2025-02-12 23:53:54.136200: Training Step 32/59: batchLoss = 2.9355, diffLoss = 14.3160, kgLoss = 0.0904
2025-02-12 23:53:55.065129: Training Step 33/59: batchLoss = 2.8286, diffLoss = 13.8327, kgLoss = 0.0776
2025-02-12 23:53:55.985903: Training Step 34/59: batchLoss = 3.2999, diffLoss = 16.1417, kgLoss = 0.0894
2025-02-12 23:53:56.912680: Training Step 35/59: batchLoss = 3.2006, diffLoss = 15.6065, kgLoss = 0.0991
2025-02-12 23:53:57.834070: Training Step 36/59: batchLoss = 3.1586, diffLoss = 15.4169, kgLoss = 0.0940
2025-02-12 23:53:58.761839: Training Step 37/59: batchLoss = 2.8225, diffLoss = 13.8141, kgLoss = 0.0745
2025-02-12 23:53:59.685518: Training Step 38/59: batchLoss = 3.0202, diffLoss = 14.7146, kgLoss = 0.0966
2025-02-12 23:54:00.623116: Training Step 39/59: batchLoss = 3.3763, diffLoss = 16.5033, kgLoss = 0.0945
2025-02-12 23:54:01.555183: Training Step 40/59: batchLoss = 2.9249, diffLoss = 14.3019, kgLoss = 0.0806
2025-02-12 23:54:02.489459: Training Step 41/59: batchLoss = 3.1412, diffLoss = 15.3444, kgLoss = 0.0905
2025-02-12 23:54:03.421131: Training Step 42/59: batchLoss = 2.8787, diffLoss = 14.0626, kgLoss = 0.0828
2025-02-12 23:54:04.349289: Training Step 43/59: batchLoss = 2.9696, diffLoss = 14.5227, kgLoss = 0.0814
2025-02-12 23:54:05.281395: Training Step 44/59: batchLoss = 2.9253, diffLoss = 14.2889, kgLoss = 0.0844
2025-02-12 23:54:06.222306: Training Step 45/59: batchLoss = 3.0356, diffLoss = 14.8396, kgLoss = 0.0846
2025-02-12 23:54:07.156121: Training Step 46/59: batchLoss = 3.0943, diffLoss = 15.1382, kgLoss = 0.0833
2025-02-12 23:54:08.095907: Training Step 47/59: batchLoss = 2.9136, diffLoss = 14.2683, kgLoss = 0.0749
2025-02-12 23:54:09.040709: Training Step 48/59: batchLoss = 2.7048, diffLoss = 13.1836, kgLoss = 0.0851
2025-02-12 23:54:09.974605: Training Step 49/59: batchLoss = 2.8488, diffLoss = 13.9102, kgLoss = 0.0835
2025-02-12 23:54:10.909360: Training Step 50/59: batchLoss = 3.2624, diffLoss = 15.9263, kgLoss = 0.0965
2025-02-12 23:54:11.837798: Training Step 51/59: batchLoss = 2.9824, diffLoss = 14.5517, kgLoss = 0.0901
2025-02-12 23:54:12.767507: Training Step 52/59: batchLoss = 2.8060, diffLoss = 13.7065, kgLoss = 0.0809
2025-02-12 23:54:13.703228: Training Step 53/59: batchLoss = 2.7749, diffLoss = 13.5568, kgLoss = 0.0794
2025-02-12 23:54:14.630734: Training Step 54/59: batchLoss = 3.0280, diffLoss = 14.7973, kgLoss = 0.0857
2025-02-12 23:54:15.564345: Training Step 55/59: batchLoss = 2.7791, diffLoss = 13.5820, kgLoss = 0.0784
2025-02-12 23:54:16.476804: Training Step 56/59: batchLoss = 2.9294, diffLoss = 14.3407, kgLoss = 0.0766
2025-02-12 23:54:17.315697: Training Step 57/59: batchLoss = 2.8894, diffLoss = 14.1070, kgLoss = 0.0850
2025-02-12 23:54:18.167720: Training Step 58/59: batchLoss = 3.2744, diffLoss = 15.9908, kgLoss = 0.0954
2025-02-12 23:54:18.262490: 
2025-02-12 23:54:18.262822: Epoch 57/1000, Train: epLoss = 0.4314, epDfLoss = 2.1067, epKgLoss = 0.0126  
2025-02-12 23:54:19.740476: Steps 0/47: batch_recall = 37.46, batch_ndcg = 47.50 
2025-02-12 23:54:21.044392: Steps 1/47: batch_recall = 37.43, batch_ndcg = 42.37 
2025-02-12 23:54:22.302659: Steps 2/47: batch_recall = 41.35, batch_ndcg = 46.09 
2025-02-12 23:54:23.565143: Steps 3/47: batch_recall = 43.71, batch_ndcg = 45.81 
2025-02-12 23:54:24.776997: Steps 4/47: batch_recall = 39.18, batch_ndcg = 45.11 
2025-02-12 23:54:26.009176: Steps 5/47: batch_recall = 32.76, batch_ndcg = 39.09 
2025-02-12 23:54:27.225493: Steps 6/47: batch_recall = 39.22, batch_ndcg = 41.83 
2025-02-12 23:54:28.395207: Steps 7/47: batch_recall = 44.20, batch_ndcg = 45.63 
2025-02-12 23:54:29.573467: Steps 8/47: batch_recall = 47.85, batch_ndcg = 52.20 
2025-02-12 23:54:30.714640: Steps 9/47: batch_recall = 47.30, batch_ndcg = 46.91 
2025-02-12 23:54:31.872102: Steps 10/47: batch_recall = 43.20, batch_ndcg = 42.89 
2025-02-12 23:54:33.008767: Steps 11/47: batch_recall = 55.33, batch_ndcg = 52.44 
2025-02-12 23:54:34.151545: Steps 12/47: batch_recall = 49.80, batch_ndcg = 49.48 
2025-02-12 23:54:35.288590: Steps 13/47: batch_recall = 48.89, batch_ndcg = 45.50 
2025-02-12 23:54:36.397918: Steps 14/47: batch_recall = 42.14, batch_ndcg = 41.40 
2025-02-12 23:54:37.497655: Steps 15/47: batch_recall = 56.62, batch_ndcg = 54.02 
2025-02-12 23:54:38.591325: Steps 16/47: batch_recall = 49.57, batch_ndcg = 47.29 
2025-02-12 23:54:39.657375: Steps 17/47: batch_recall = 57.85, batch_ndcg = 49.51 
2025-02-12 23:54:40.749262: Steps 18/47: batch_recall = 53.12, batch_ndcg = 49.40 
2025-02-12 23:54:41.829150: Steps 19/47: batch_recall = 60.16, batch_ndcg = 55.53 
2025-02-12 23:54:42.887607: Steps 20/47: batch_recall = 66.80, batch_ndcg = 61.29 
2025-02-12 23:54:43.942050: Steps 21/47: batch_recall = 65.36, batch_ndcg = 55.48 
2025-02-12 23:54:44.996782: Steps 22/47: batch_recall = 54.97, batch_ndcg = 51.65 
2025-02-12 23:54:46.046536: Steps 23/47: batch_recall = 63.67, batch_ndcg = 54.42 
2025-02-12 23:54:47.112996: Steps 24/47: batch_recall = 66.10, batch_ndcg = 58.18 
2025-02-12 23:54:48.154556: Steps 25/47: batch_recall = 66.26, batch_ndcg = 57.55 
2025-02-12 23:54:49.172271: Steps 26/47: batch_recall = 60.10, batch_ndcg = 54.39 
2025-02-12 23:54:50.205316: Steps 27/47: batch_recall = 61.94, batch_ndcg = 52.53 
2025-02-12 23:54:51.222496: Steps 28/47: batch_recall = 71.06, batch_ndcg = 60.16 
2025-02-12 23:54:52.240622: Steps 29/47: batch_recall = 69.90, batch_ndcg = 58.17 
2025-02-12 23:54:53.258899: Steps 30/47: batch_recall = 72.25, batch_ndcg = 64.00 
2025-02-12 23:54:54.292795: Steps 31/47: batch_recall = 67.20, batch_ndcg = 56.28 
2025-02-12 23:54:55.314694: Steps 32/47: batch_recall = 70.55, batch_ndcg = 64.83 
2025-02-12 23:54:56.338253: Steps 33/47: batch_recall = 80.31, batch_ndcg = 68.06 
2025-02-12 23:54:57.359429: Steps 34/47: batch_recall = 68.10, batch_ndcg = 56.89 
2025-02-12 23:54:58.354542: Steps 35/47: batch_recall = 77.18, batch_ndcg = 64.75 
2025-02-12 23:54:59.361994: Steps 36/47: batch_recall = 79.66, batch_ndcg = 64.95 
2025-02-12 23:55:00.339712: Steps 37/47: batch_recall = 84.81, batch_ndcg = 72.23 
2025-02-12 23:55:01.321641: Steps 38/47: batch_recall = 89.72, batch_ndcg = 72.55 
2025-02-12 23:55:02.296040: Steps 39/47: batch_recall = 88.44, batch_ndcg = 69.92 
2025-02-12 23:55:03.256348: Steps 40/47: batch_recall = 74.22, batch_ndcg = 63.64 
2025-02-12 23:55:04.221101: Steps 41/47: batch_recall = 87.16, batch_ndcg = 72.05 
2025-02-12 23:55:05.176539: Steps 42/47: batch_recall = 81.17, batch_ndcg = 63.33 
2025-02-12 23:55:06.133056: Steps 43/47: batch_recall = 89.11, batch_ndcg = 71.76 
2025-02-12 23:55:07.098304: Steps 44/47: batch_recall = 86.76, batch_ndcg = 68.98 
2025-02-12 23:55:08.035228: Steps 45/47: batch_recall = 93.00, batch_ndcg = 75.15 
2025-02-12 23:55:08.141326: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.60 
2025-02-12 23:55:08.141657: Epoch 57/1000, Test: Recall = 0.1216, NDCG = 0.1093  

2025-02-12 23:55:09.348905: Training Step 0/59: batchLoss = 2.9869, diffLoss = 14.5840, kgLoss = 0.0877
2025-02-12 23:55:10.282270: Training Step 1/59: batchLoss = 2.8185, diffLoss = 13.7117, kgLoss = 0.0952
2025-02-12 23:55:11.220648: Training Step 2/59: batchLoss = 3.1133, diffLoss = 15.1923, kgLoss = 0.0936
2025-02-12 23:55:12.161714: Training Step 3/59: batchLoss = 2.7931, diffLoss = 13.6346, kgLoss = 0.0828
2025-02-12 23:55:13.097179: Training Step 4/59: batchLoss = 2.8686, diffLoss = 14.0091, kgLoss = 0.0835
2025-02-12 23:55:14.037275: Training Step 5/59: batchLoss = 2.7621, diffLoss = 13.4634, kgLoss = 0.0868
2025-02-12 23:55:14.975712: Training Step 6/59: batchLoss = 2.7716, diffLoss = 13.5252, kgLoss = 0.0832
2025-02-12 23:55:15.913275: Training Step 7/59: batchLoss = 3.1878, diffLoss = 15.5783, kgLoss = 0.0902
2025-02-12 23:55:16.845098: Training Step 8/59: batchLoss = 2.7130, diffLoss = 13.2596, kgLoss = 0.0764
2025-02-12 23:55:17.783749: Training Step 9/59: batchLoss = 2.8962, diffLoss = 14.1269, kgLoss = 0.0885
2025-02-12 23:55:18.718267: Training Step 10/59: batchLoss = 2.7472, diffLoss = 13.4052, kgLoss = 0.0827
2025-02-12 23:55:19.649060: Training Step 11/59: batchLoss = 2.6622, diffLoss = 12.9915, kgLoss = 0.0799
2025-02-12 23:55:20.577382: Training Step 12/59: batchLoss = 2.9714, diffLoss = 14.5033, kgLoss = 0.0884
2025-02-12 23:55:21.502473: Training Step 13/59: batchLoss = 2.9839, diffLoss = 14.5766, kgLoss = 0.0858
2025-02-12 23:55:22.433499: Training Step 14/59: batchLoss = 2.8979, diffLoss = 14.1572, kgLoss = 0.0831
2025-02-12 23:55:23.357153: Training Step 15/59: batchLoss = 3.0939, diffLoss = 15.0952, kgLoss = 0.0936
2025-02-12 23:55:24.285718: Training Step 16/59: batchLoss = 2.8271, diffLoss = 13.7893, kgLoss = 0.0865
2025-02-12 23:55:25.210504: Training Step 17/59: batchLoss = 2.7020, diffLoss = 13.1883, kgLoss = 0.0804
2025-02-12 23:55:26.143105: Training Step 18/59: batchLoss = 2.5437, diffLoss = 12.4256, kgLoss = 0.0732
2025-02-12 23:55:27.078430: Training Step 19/59: batchLoss = 3.0181, diffLoss = 14.7341, kgLoss = 0.0891
2025-02-12 23:55:28.019531: Training Step 20/59: batchLoss = 2.5976, diffLoss = 12.6893, kgLoss = 0.0747
2025-02-12 23:55:28.950869: Training Step 21/59: batchLoss = 2.8750, diffLoss = 14.0171, kgLoss = 0.0894
2025-02-12 23:55:29.900096: Training Step 22/59: batchLoss = 2.8454, diffLoss = 13.9277, kgLoss = 0.0748
2025-02-12 23:55:30.830074: Training Step 23/59: batchLoss = 2.8578, diffLoss = 13.9793, kgLoss = 0.0775
2025-02-12 23:55:31.763799: Training Step 24/59: batchLoss = 2.9141, diffLoss = 14.2296, kgLoss = 0.0852
2025-02-12 23:55:32.694752: Training Step 25/59: batchLoss = 2.8507, diffLoss = 13.9376, kgLoss = 0.0790
2025-02-12 23:55:33.646556: Training Step 26/59: batchLoss = 2.8216, diffLoss = 13.7781, kgLoss = 0.0825
2025-02-12 23:55:34.583256: Training Step 27/59: batchLoss = 2.9283, diffLoss = 14.2938, kgLoss = 0.0869
2025-02-12 23:55:35.519527: Training Step 28/59: batchLoss = 2.7591, diffLoss = 13.4652, kgLoss = 0.0826
2025-02-12 23:55:36.447187: Training Step 29/59: batchLoss = 2.9724, diffLoss = 14.5103, kgLoss = 0.0879
2025-02-12 23:55:37.379031: Training Step 30/59: batchLoss = 3.0518, diffLoss = 14.9084, kgLoss = 0.0876
2025-02-12 23:55:38.307407: Training Step 31/59: batchLoss = 2.7608, diffLoss = 13.4681, kgLoss = 0.0839
2025-02-12 23:55:39.229736: Training Step 32/59: batchLoss = 2.8685, diffLoss = 13.9910, kgLoss = 0.0879
2025-02-12 23:55:40.164394: Training Step 33/59: batchLoss = 3.0151, diffLoss = 14.7176, kgLoss = 0.0895
2025-02-12 23:55:41.094305: Training Step 34/59: batchLoss = 3.1001, diffLoss = 15.1157, kgLoss = 0.0961
2025-02-12 23:55:42.035176: Training Step 35/59: batchLoss = 3.2019, diffLoss = 15.5998, kgLoss = 0.1024
2025-02-12 23:55:42.969776: Training Step 36/59: batchLoss = 3.1394, diffLoss = 15.3404, kgLoss = 0.0891
2025-02-12 23:55:43.904223: Training Step 37/59: batchLoss = 2.5419, diffLoss = 12.4042, kgLoss = 0.0763
2025-02-12 23:55:44.839669: Training Step 38/59: batchLoss = 2.9780, diffLoss = 14.5621, kgLoss = 0.0820
2025-02-12 23:55:45.773914: Training Step 39/59: batchLoss = 3.0515, diffLoss = 14.9043, kgLoss = 0.0883
2025-02-12 23:55:46.711749: Training Step 40/59: batchLoss = 2.9366, diffLoss = 14.3613, kgLoss = 0.0805
2025-02-12 23:55:47.651754: Training Step 41/59: batchLoss = 3.1380, diffLoss = 15.3527, kgLoss = 0.0843
2025-02-12 23:55:48.580777: Training Step 42/59: batchLoss = 3.2385, diffLoss = 15.8163, kgLoss = 0.0941
2025-02-12 23:55:49.511481: Training Step 43/59: batchLoss = 2.9685, diffLoss = 14.5192, kgLoss = 0.0808
2025-02-12 23:55:50.447041: Training Step 44/59: batchLoss = 3.2685, diffLoss = 15.9667, kgLoss = 0.0939
2025-02-12 23:55:51.383843: Training Step 45/59: batchLoss = 3.1092, diffLoss = 15.1875, kgLoss = 0.0896
2025-02-12 23:55:52.324770: Training Step 46/59: batchLoss = 2.9912, diffLoss = 14.5746, kgLoss = 0.0953
2025-02-12 23:55:53.260997: Training Step 47/59: batchLoss = 3.2151, diffLoss = 15.7233, kgLoss = 0.0881
2025-02-12 23:55:54.193808: Training Step 48/59: batchLoss = 2.9134, diffLoss = 14.2380, kgLoss = 0.0822
2025-02-12 23:55:55.121780: Training Step 49/59: batchLoss = 2.9889, diffLoss = 14.5917, kgLoss = 0.0882
2025-02-12 23:55:56.049250: Training Step 50/59: batchLoss = 2.7543, diffLoss = 13.4586, kgLoss = 0.0782
2025-02-12 23:55:56.978180: Training Step 51/59: batchLoss = 2.6067, diffLoss = 12.7454, kgLoss = 0.0721
2025-02-12 23:55:57.909769: Training Step 52/59: batchLoss = 2.9801, diffLoss = 14.5539, kgLoss = 0.0867
2025-02-12 23:55:58.841767: Training Step 53/59: batchLoss = 2.9696, diffLoss = 14.5119, kgLoss = 0.0841
2025-02-12 23:55:59.766243: Training Step 54/59: batchLoss = 2.9816, diffLoss = 14.5710, kgLoss = 0.0842
2025-02-12 23:56:00.698312: Training Step 55/59: batchLoss = 2.9942, diffLoss = 14.6139, kgLoss = 0.0893
2025-02-12 23:56:01.629704: Training Step 56/59: batchLoss = 3.4429, diffLoss = 16.8260, kgLoss = 0.0971
2025-02-12 23:56:02.480369: Training Step 57/59: batchLoss = 2.9922, diffLoss = 14.6286, kgLoss = 0.0831
2025-02-12 23:56:03.338686: Training Step 58/59: batchLoss = 3.0068, diffLoss = 14.6907, kgLoss = 0.0858
2025-02-12 23:56:03.435573: 
2025-02-12 23:56:03.435923: Epoch 58/1000, Train: epLoss = 0.4325, epDfLoss = 2.1118, epKgLoss = 0.0126  
2025-02-12 23:56:04.917724: Steps 0/47: batch_recall = 37.43, batch_ndcg = 46.53 
2025-02-12 23:56:06.217518: Steps 1/47: batch_recall = 37.67, batch_ndcg = 43.14 
2025-02-12 23:56:07.472554: Steps 2/47: batch_recall = 41.79, batch_ndcg = 46.55 
2025-02-12 23:56:08.755726: Steps 3/47: batch_recall = 43.75, batch_ndcg = 45.88 
2025-02-12 23:56:09.949253: Steps 4/47: batch_recall = 39.42, batch_ndcg = 45.98 
2025-02-12 23:56:11.166298: Steps 5/47: batch_recall = 33.00, batch_ndcg = 39.88 
2025-02-12 23:56:12.365172: Steps 6/47: batch_recall = 39.43, batch_ndcg = 42.14 
2025-02-12 23:56:13.535092: Steps 7/47: batch_recall = 42.92, batch_ndcg = 44.63 
2025-02-12 23:56:14.704553: Steps 8/47: batch_recall = 48.66, batch_ndcg = 52.25 
2025-02-12 23:56:15.844833: Steps 9/47: batch_recall = 46.99, batch_ndcg = 47.12 
2025-02-12 23:56:17.000071: Steps 10/47: batch_recall = 44.64, batch_ndcg = 44.07 
2025-02-12 23:56:18.132034: Steps 11/47: batch_recall = 54.88, batch_ndcg = 52.01 
2025-02-12 23:56:19.287849: Steps 12/47: batch_recall = 48.63, batch_ndcg = 48.09 
2025-02-12 23:56:20.420485: Steps 13/47: batch_recall = 48.86, batch_ndcg = 45.19 
2025-02-12 23:56:21.511482: Steps 14/47: batch_recall = 40.16, batch_ndcg = 40.67 
2025-02-12 23:56:22.606924: Steps 15/47: batch_recall = 56.00, batch_ndcg = 53.49 
2025-02-12 23:56:23.689428: Steps 16/47: batch_recall = 50.07, batch_ndcg = 46.03 
2025-02-12 23:56:24.735135: Steps 17/47: batch_recall = 58.18, batch_ndcg = 49.81 
2025-02-12 23:56:25.803554: Steps 18/47: batch_recall = 52.59, batch_ndcg = 48.80 
2025-02-12 23:56:26.869699: Steps 19/47: batch_recall = 60.17, batch_ndcg = 55.93 
2025-02-12 23:56:27.914908: Steps 20/47: batch_recall = 69.63, batch_ndcg = 62.90 
2025-02-12 23:56:28.955297: Steps 21/47: batch_recall = 64.21, batch_ndcg = 55.18 
2025-02-12 23:56:30.002529: Steps 22/47: batch_recall = 56.96, batch_ndcg = 51.94 
2025-02-12 23:56:31.049337: Steps 23/47: batch_recall = 62.72, batch_ndcg = 54.61 
2025-02-12 23:56:32.129041: Steps 24/47: batch_recall = 66.88, batch_ndcg = 58.93 
2025-02-12 23:56:33.176445: Steps 25/47: batch_recall = 66.29, batch_ndcg = 57.48 
2025-02-12 23:56:34.204727: Steps 26/47: batch_recall = 59.45, batch_ndcg = 53.02 
2025-02-12 23:56:35.250195: Steps 27/47: batch_recall = 62.05, batch_ndcg = 53.61 
2025-02-12 23:56:36.281229: Steps 28/47: batch_recall = 71.86, batch_ndcg = 60.97 
2025-02-12 23:56:37.358426: Steps 29/47: batch_recall = 69.82, batch_ndcg = 58.41 
2025-02-12 23:56:38.373953: Steps 30/47: batch_recall = 71.88, batch_ndcg = 63.82 
2025-02-12 23:56:39.405450: Steps 31/47: batch_recall = 67.43, batch_ndcg = 56.49 
2025-02-12 23:56:40.413515: Steps 32/47: batch_recall = 71.57, batch_ndcg = 65.74 
2025-02-12 23:56:41.415678: Steps 33/47: batch_recall = 79.83, batch_ndcg = 67.85 
2025-02-12 23:56:42.430189: Steps 34/47: batch_recall = 66.58, batch_ndcg = 56.06 
2025-02-12 23:56:43.422628: Steps 35/47: batch_recall = 75.54, batch_ndcg = 63.04 
2025-02-12 23:56:44.407637: Steps 36/47: batch_recall = 77.00, batch_ndcg = 63.95 
2025-02-12 23:56:45.383485: Steps 37/47: batch_recall = 83.95, batch_ndcg = 72.14 
2025-02-12 23:56:46.353369: Steps 38/47: batch_recall = 90.10, batch_ndcg = 72.80 
2025-02-12 23:56:47.323581: Steps 39/47: batch_recall = 88.25, batch_ndcg = 69.83 
2025-02-12 23:56:48.283174: Steps 40/47: batch_recall = 71.76, batch_ndcg = 62.52 
2025-02-12 23:56:49.241715: Steps 41/47: batch_recall = 85.53, batch_ndcg = 71.37 
2025-02-12 23:56:50.189690: Steps 42/47: batch_recall = 82.08, batch_ndcg = 64.46 
2025-02-12 23:56:51.155746: Steps 43/47: batch_recall = 90.11, batch_ndcg = 72.27 
2025-02-12 23:56:52.140703: Steps 44/47: batch_recall = 86.85, batch_ndcg = 69.66 
2025-02-12 23:56:53.078506: Steps 45/47: batch_recall = 93.65, batch_ndcg = 75.90 
2025-02-12 23:56:53.180765: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-12 23:56:53.180896: Epoch 58/1000, Test: Recall = 0.1213, NDCG = 0.1093  

2025-02-12 23:56:54.366098: Training Step 0/59: batchLoss = 2.5419, diffLoss = 12.3980, kgLoss = 0.0778
2025-02-12 23:56:55.307145: Training Step 1/59: batchLoss = 2.9741, diffLoss = 14.4977, kgLoss = 0.0931
2025-02-12 23:56:56.240378: Training Step 2/59: batchLoss = 2.7206, diffLoss = 13.2813, kgLoss = 0.0805
2025-02-12 23:56:57.177050: Training Step 3/59: batchLoss = 2.6732, diffLoss = 13.0461, kgLoss = 0.0799
2025-02-12 23:56:58.115986: Training Step 4/59: batchLoss = 2.6805, diffLoss = 13.0905, kgLoss = 0.0780
2025-02-12 23:56:59.041240: Training Step 5/59: batchLoss = 2.6870, diffLoss = 13.0934, kgLoss = 0.0854
2025-02-12 23:56:59.972853: Training Step 6/59: batchLoss = 2.6139, diffLoss = 12.7212, kgLoss = 0.0871
2025-02-12 23:57:00.895556: Training Step 7/59: batchLoss = 2.8453, diffLoss = 13.8493, kgLoss = 0.0943
2025-02-12 23:57:01.822657: Training Step 8/59: batchLoss = 2.6450, diffLoss = 12.9188, kgLoss = 0.0765
2025-02-12 23:57:02.754638: Training Step 9/59: batchLoss = 2.8095, diffLoss = 13.7236, kgLoss = 0.0809
2025-02-12 23:57:03.677963: Training Step 10/59: batchLoss = 2.5923, diffLoss = 12.6411, kgLoss = 0.0801
2025-02-12 23:57:04.608628: Training Step 11/59: batchLoss = 3.0564, diffLoss = 14.9054, kgLoss = 0.0941
2025-02-12 23:57:05.534443: Training Step 12/59: batchLoss = 2.8309, diffLoss = 13.8277, kgLoss = 0.0817
2025-02-12 23:57:06.457155: Training Step 13/59: batchLoss = 2.9034, diffLoss = 14.1781, kgLoss = 0.0847
2025-02-12 23:57:07.387758: Training Step 14/59: batchLoss = 3.0042, diffLoss = 14.6278, kgLoss = 0.0983
2025-02-12 23:57:08.315674: Training Step 15/59: batchLoss = 2.8253, diffLoss = 13.7635, kgLoss = 0.0908
2025-02-12 23:57:09.239998: Training Step 16/59: batchLoss = 2.6125, diffLoss = 12.7362, kgLoss = 0.0816
2025-02-12 23:57:10.174845: Training Step 17/59: batchLoss = 3.0997, diffLoss = 15.1208, kgLoss = 0.0944
2025-02-12 23:57:11.105061: Training Step 18/59: batchLoss = 2.7715, diffLoss = 13.5589, kgLoss = 0.0747
2025-02-12 23:57:12.037684: Training Step 19/59: batchLoss = 2.8963, diffLoss = 14.1127, kgLoss = 0.0921
2025-02-12 23:57:12.962969: Training Step 20/59: batchLoss = 2.8604, diffLoss = 13.9854, kgLoss = 0.0791
2025-02-12 23:57:13.903281: Training Step 21/59: batchLoss = 3.1381, diffLoss = 15.3495, kgLoss = 0.0852
2025-02-12 23:57:14.836006: Training Step 22/59: batchLoss = 2.9790, diffLoss = 14.5510, kgLoss = 0.0860
2025-02-12 23:57:15.772430: Training Step 23/59: batchLoss = 2.8390, diffLoss = 13.8784, kgLoss = 0.0791
2025-02-12 23:57:16.709648: Training Step 24/59: batchLoss = 2.9023, diffLoss = 14.1732, kgLoss = 0.0846
2025-02-12 23:57:17.637872: Training Step 25/59: batchLoss = 3.0111, diffLoss = 14.6952, kgLoss = 0.0900
2025-02-12 23:57:18.564892: Training Step 26/59: batchLoss = 2.8025, diffLoss = 13.6811, kgLoss = 0.0828
2025-02-12 23:57:19.492026: Training Step 27/59: batchLoss = 2.8328, diffLoss = 13.8469, kgLoss = 0.0793
2025-02-12 23:57:20.419747: Training Step 28/59: batchLoss = 3.2825, diffLoss = 15.9950, kgLoss = 0.1044
2025-02-12 23:57:21.347879: Training Step 29/59: batchLoss = 3.0144, diffLoss = 14.7159, kgLoss = 0.0891
2025-02-12 23:57:22.274581: Training Step 30/59: batchLoss = 2.7281, diffLoss = 13.3291, kgLoss = 0.0778
2025-02-12 23:57:23.189554: Training Step 31/59: batchLoss = 2.9142, diffLoss = 14.2352, kgLoss = 0.0840
2025-02-12 23:57:24.111735: Training Step 32/59: batchLoss = 2.9666, diffLoss = 14.4804, kgLoss = 0.0881
2025-02-12 23:57:25.033788: Training Step 33/59: batchLoss = 3.0118, diffLoss = 14.7086, kgLoss = 0.0876
2025-02-12 23:57:25.959562: Training Step 34/59: batchLoss = 2.9087, diffLoss = 14.2117, kgLoss = 0.0829
2025-02-12 23:57:26.887646: Training Step 35/59: batchLoss = 2.8185, diffLoss = 13.7513, kgLoss = 0.0853
2025-02-12 23:57:27.817713: Training Step 36/59: batchLoss = 3.0395, diffLoss = 14.8642, kgLoss = 0.0833
2025-02-12 23:57:28.750453: Training Step 37/59: batchLoss = 3.0081, diffLoss = 14.6884, kgLoss = 0.0881
2025-02-12 23:57:29.679506: Training Step 38/59: batchLoss = 3.0729, diffLoss = 14.9912, kgLoss = 0.0933
2025-02-12 23:57:30.611616: Training Step 39/59: batchLoss = 2.7927, diffLoss = 13.6431, kgLoss = 0.0801
2025-02-12 23:57:31.545541: Training Step 40/59: batchLoss = 2.8109, diffLoss = 13.7294, kgLoss = 0.0813
2025-02-12 23:57:32.487825: Training Step 41/59: batchLoss = 2.9892, diffLoss = 14.6074, kgLoss = 0.0846
2025-02-12 23:57:33.417648: Training Step 42/59: batchLoss = 3.0941, diffLoss = 15.1204, kgLoss = 0.0876
2025-02-12 23:57:34.353743: Training Step 43/59: batchLoss = 2.6317, diffLoss = 12.8482, kgLoss = 0.0775
2025-02-12 23:57:35.291058: Training Step 44/59: batchLoss = 2.8548, diffLoss = 13.9241, kgLoss = 0.0874
2025-02-12 23:57:36.222927: Training Step 45/59: batchLoss = 2.9344, diffLoss = 14.3313, kgLoss = 0.0852
2025-02-12 23:57:37.157483: Training Step 46/59: batchLoss = 3.2182, diffLoss = 15.7041, kgLoss = 0.0967
2025-02-12 23:57:38.088202: Training Step 47/59: batchLoss = 2.8123, diffLoss = 13.7504, kgLoss = 0.0778
2025-02-12 23:57:39.012800: Training Step 48/59: batchLoss = 3.0073, diffLoss = 14.7121, kgLoss = 0.0811
2025-02-12 23:57:39.941778: Training Step 49/59: batchLoss = 2.9241, diffLoss = 14.2798, kgLoss = 0.0852
2025-02-12 23:57:40.866311: Training Step 50/59: batchLoss = 2.9560, diffLoss = 14.4399, kgLoss = 0.0851
2025-02-12 23:57:41.809888: Training Step 51/59: batchLoss = 3.0906, diffLoss = 15.1235, kgLoss = 0.0824
2025-02-12 23:57:42.731817: Training Step 52/59: batchLoss = 3.0971, diffLoss = 15.1356, kgLoss = 0.0875
2025-02-12 23:57:43.662618: Training Step 53/59: batchLoss = 3.0474, diffLoss = 14.9030, kgLoss = 0.0835
2025-02-12 23:57:44.588741: Training Step 54/59: batchLoss = 2.9824, diffLoss = 14.5853, kgLoss = 0.0817
2025-02-12 23:57:45.518051: Training Step 55/59: batchLoss = 3.0469, diffLoss = 14.8866, kgLoss = 0.0870
2025-02-12 23:57:46.450523: Training Step 56/59: batchLoss = 3.1421, diffLoss = 15.3385, kgLoss = 0.0930
2025-02-12 23:57:47.302962: Training Step 57/59: batchLoss = 3.4217, diffLoss = 16.7182, kgLoss = 0.0976
2025-02-12 23:57:48.158011: Training Step 58/59: batchLoss = 3.2215, diffLoss = 15.7395, kgLoss = 0.0920
2025-02-12 23:57:48.259509: 
2025-02-12 23:57:48.260060: Epoch 59/1000, Train: epLoss = 0.4300, epDfLoss = 2.0994, epKgLoss = 0.0126  
2025-02-12 23:57:49.749950: Steps 0/47: batch_recall = 36.19, batch_ndcg = 47.57 
2025-02-12 23:57:51.052210: Steps 1/47: batch_recall = 37.09, batch_ndcg = 42.04 
2025-02-12 23:57:52.316792: Steps 2/47: batch_recall = 40.33, batch_ndcg = 47.11 
2025-02-12 23:57:53.588749: Steps 3/47: batch_recall = 43.83, batch_ndcg = 46.51 
2025-02-12 23:57:54.796228: Steps 4/47: batch_recall = 39.52, batch_ndcg = 45.35 
2025-02-12 23:57:56.012141: Steps 5/47: batch_recall = 34.47, batch_ndcg = 40.16 
2025-02-12 23:57:57.201940: Steps 6/47: batch_recall = 39.39, batch_ndcg = 41.84 
2025-02-12 23:57:58.366257: Steps 7/47: batch_recall = 44.57, batch_ndcg = 45.90 
2025-02-12 23:57:59.525713: Steps 8/47: batch_recall = 47.78, batch_ndcg = 52.88 
2025-02-12 23:58:00.664313: Steps 9/47: batch_recall = 46.45, batch_ndcg = 46.68 
2025-02-12 23:58:01.829208: Steps 10/47: batch_recall = 43.25, batch_ndcg = 43.09 
2025-02-12 23:58:02.961460: Steps 11/47: batch_recall = 54.65, batch_ndcg = 52.68 
2025-02-12 23:58:04.102161: Steps 12/47: batch_recall = 49.52, batch_ndcg = 48.84 
2025-02-12 23:58:05.241159: Steps 13/47: batch_recall = 48.43, batch_ndcg = 45.77 
2025-02-12 23:58:06.351958: Steps 14/47: batch_recall = 42.05, batch_ndcg = 40.99 
2025-02-12 23:58:07.452132: Steps 15/47: batch_recall = 56.74, batch_ndcg = 53.32 
2025-02-12 23:58:08.547227: Steps 16/47: batch_recall = 50.62, batch_ndcg = 46.43 
2025-02-12 23:58:09.598156: Steps 17/47: batch_recall = 57.82, batch_ndcg = 49.71 
2025-02-12 23:58:10.685376: Steps 18/47: batch_recall = 52.05, batch_ndcg = 48.84 
2025-02-12 23:58:11.755516: Steps 19/47: batch_recall = 61.96, batch_ndcg = 56.20 
2025-02-12 23:58:12.807880: Steps 20/47: batch_recall = 70.07, batch_ndcg = 63.23 
2025-02-12 23:58:13.872788: Steps 21/47: batch_recall = 64.76, batch_ndcg = 55.75 
2025-02-12 23:58:14.932385: Steps 22/47: batch_recall = 54.95, batch_ndcg = 51.31 
2025-02-12 23:58:15.988139: Steps 23/47: batch_recall = 64.37, batch_ndcg = 55.02 
2025-02-12 23:58:17.047359: Steps 24/47: batch_recall = 66.54, batch_ndcg = 59.07 
2025-02-12 23:58:18.089890: Steps 25/47: batch_recall = 66.96, batch_ndcg = 57.57 
2025-02-12 23:58:19.107825: Steps 26/47: batch_recall = 59.40, batch_ndcg = 53.15 
2025-02-12 23:58:20.132875: Steps 27/47: batch_recall = 62.04, batch_ndcg = 53.44 
2025-02-12 23:58:21.154961: Steps 28/47: batch_recall = 70.58, batch_ndcg = 59.91 
2025-02-12 23:58:22.171031: Steps 29/47: batch_recall = 69.28, batch_ndcg = 57.96 
2025-02-12 23:58:23.182671: Steps 30/47: batch_recall = 72.10, batch_ndcg = 63.82 
2025-02-12 23:58:24.205737: Steps 31/47: batch_recall = 67.52, batch_ndcg = 55.96 
2025-02-12 23:58:25.209471: Steps 32/47: batch_recall = 70.91, batch_ndcg = 64.76 
2025-02-12 23:58:26.229671: Steps 33/47: batch_recall = 79.44, batch_ndcg = 68.37 
2025-02-12 23:58:27.246791: Steps 34/47: batch_recall = 67.32, batch_ndcg = 55.85 
2025-02-12 23:58:28.251840: Steps 35/47: batch_recall = 77.10, batch_ndcg = 64.99 
2025-02-12 23:58:29.252418: Steps 36/47: batch_recall = 78.86, batch_ndcg = 64.65 
2025-02-12 23:58:30.221112: Steps 37/47: batch_recall = 83.13, batch_ndcg = 71.56 
2025-02-12 23:58:31.186751: Steps 38/47: batch_recall = 90.75, batch_ndcg = 73.31 
2025-02-12 23:58:32.152823: Steps 39/47: batch_recall = 88.24, batch_ndcg = 69.31 
2025-02-12 23:58:33.135591: Steps 40/47: batch_recall = 73.39, batch_ndcg = 63.18 
2025-02-12 23:58:34.096891: Steps 41/47: batch_recall = 88.00, batch_ndcg = 72.78 
2025-02-12 23:58:35.053228: Steps 42/47: batch_recall = 81.43, batch_ndcg = 64.08 
2025-02-12 23:58:36.007324: Steps 43/47: batch_recall = 89.49, batch_ndcg = 71.54 
2025-02-12 23:58:36.972010: Steps 44/47: batch_recall = 88.57, batch_ndcg = 69.90 
2025-02-12 23:58:37.913003: Steps 45/47: batch_recall = 91.75, batch_ndcg = 74.13 
2025-02-12 23:58:38.020167: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.57 
2025-02-12 23:58:38.020296: Epoch 59/1000, Test: Recall = 0.1216, NDCG = 0.1094  

2025-02-12 23:58:39.220481: Training Step 0/59: batchLoss = 2.9121, diffLoss = 14.2050, kgLoss = 0.0889
2025-02-12 23:58:40.148956: Training Step 1/59: batchLoss = 2.7811, diffLoss = 13.5879, kgLoss = 0.0794
2025-02-12 23:58:41.076560: Training Step 2/59: batchLoss = 2.7453, diffLoss = 13.3959, kgLoss = 0.0826
2025-02-12 23:58:42.001798: Training Step 3/59: batchLoss = 2.8419, diffLoss = 13.8864, kgLoss = 0.0808
2025-02-12 23:58:42.921893: Training Step 4/59: batchLoss = 3.1050, diffLoss = 15.1577, kgLoss = 0.0918
2025-02-12 23:58:43.856143: Training Step 5/59: batchLoss = 2.6065, diffLoss = 12.7063, kgLoss = 0.0815
2025-02-12 23:58:44.789289: Training Step 6/59: batchLoss = 2.8833, diffLoss = 14.0682, kgLoss = 0.0871
2025-02-12 23:58:45.728597: Training Step 7/59: batchLoss = 2.6480, diffLoss = 12.9156, kgLoss = 0.0811
2025-02-12 23:58:46.666354: Training Step 8/59: batchLoss = 2.8531, diffLoss = 13.9171, kgLoss = 0.0871
2025-02-12 23:58:47.599214: Training Step 9/59: batchLoss = 2.6969, diffLoss = 13.1518, kgLoss = 0.0831
2025-02-12 23:58:48.538193: Training Step 10/59: batchLoss = 2.8395, diffLoss = 13.8561, kgLoss = 0.0854
2025-02-12 23:58:49.480695: Training Step 11/59: batchLoss = 3.0147, diffLoss = 14.7341, kgLoss = 0.0848
2025-02-12 23:58:50.414407: Training Step 12/59: batchLoss = 2.6178, diffLoss = 12.7664, kgLoss = 0.0807
2025-02-12 23:58:51.345760: Training Step 13/59: batchLoss = 2.4882, diffLoss = 12.1457, kgLoss = 0.0738
2025-02-12 23:58:52.275309: Training Step 14/59: batchLoss = 3.2181, diffLoss = 15.7263, kgLoss = 0.0911
2025-02-12 23:58:53.207659: Training Step 15/59: batchLoss = 2.7635, diffLoss = 13.4882, kgLoss = 0.0823
2025-02-12 23:58:54.134324: Training Step 16/59: batchLoss = 2.7996, diffLoss = 13.6730, kgLoss = 0.0812
2025-02-12 23:58:55.064077: Training Step 17/59: batchLoss = 3.1230, diffLoss = 15.2613, kgLoss = 0.0884
2025-02-12 23:58:55.986443: Training Step 18/59: batchLoss = 2.7034, diffLoss = 13.1837, kgLoss = 0.0834
2025-02-12 23:58:56.902683: Training Step 19/59: batchLoss = 2.9660, diffLoss = 14.5180, kgLoss = 0.0780
2025-02-12 23:58:57.820167: Training Step 20/59: batchLoss = 2.6412, diffLoss = 12.9020, kgLoss = 0.0760
2025-02-12 23:58:58.740757: Training Step 21/59: batchLoss = 2.9973, diffLoss = 14.6055, kgLoss = 0.0952
2025-02-12 23:58:59.657635: Training Step 22/59: batchLoss = 3.1946, diffLoss = 15.5951, kgLoss = 0.0945
2025-02-12 23:59:00.596932: Training Step 23/59: batchLoss = 3.0901, diffLoss = 15.0915, kgLoss = 0.0898
2025-02-12 23:59:01.520353: Training Step 24/59: batchLoss = 2.9694, diffLoss = 14.4592, kgLoss = 0.0970
2025-02-12 23:59:02.452130: Training Step 25/59: batchLoss = 2.8701, diffLoss = 14.0310, kgLoss = 0.0799
2025-02-12 23:59:03.383329: Training Step 26/59: batchLoss = 2.5784, diffLoss = 12.5866, kgLoss = 0.0764
2025-02-12 23:59:04.315159: Training Step 27/59: batchLoss = 3.1576, diffLoss = 15.4283, kgLoss = 0.0899
2025-02-12 23:59:05.249712: Training Step 28/59: batchLoss = 3.2051, diffLoss = 15.6697, kgLoss = 0.0890
2025-02-12 23:59:06.186144: Training Step 29/59: batchLoss = 2.7493, diffLoss = 13.4291, kgLoss = 0.0794
2025-02-12 23:59:07.120932: Training Step 30/59: batchLoss = 3.3355, diffLoss = 16.3044, kgLoss = 0.0933
2025-02-12 23:59:08.056545: Training Step 31/59: batchLoss = 3.1708, diffLoss = 15.5070, kgLoss = 0.0867
2025-02-12 23:59:08.986489: Training Step 32/59: batchLoss = 3.1716, diffLoss = 15.4963, kgLoss = 0.0904
2025-02-12 23:59:09.914503: Training Step 33/59: batchLoss = 2.7186, diffLoss = 13.2594, kgLoss = 0.0834
2025-02-12 23:59:10.849114: Training Step 34/59: batchLoss = 3.0673, diffLoss = 14.9822, kgLoss = 0.0885
2025-02-12 23:59:11.783575: Training Step 35/59: batchLoss = 3.2293, diffLoss = 15.7825, kgLoss = 0.0910
2025-02-12 23:59:12.706856: Training Step 36/59: batchLoss = 2.9821, diffLoss = 14.5729, kgLoss = 0.0844
2025-02-12 23:59:13.638506: Training Step 37/59: batchLoss = 3.2382, diffLoss = 15.7937, kgLoss = 0.0993
2025-02-12 23:59:14.557721: Training Step 38/59: batchLoss = 2.9889, diffLoss = 14.5947, kgLoss = 0.0874
2025-02-12 23:59:15.481458: Training Step 39/59: batchLoss = 3.0168, diffLoss = 14.7602, kgLoss = 0.0810
2025-02-12 23:59:16.406451: Training Step 40/59: batchLoss = 3.1481, diffLoss = 15.3680, kgLoss = 0.0931
2025-02-12 23:59:17.332300: Training Step 41/59: batchLoss = 2.9646, diffLoss = 14.4520, kgLoss = 0.0928
2025-02-12 23:59:18.264532: Training Step 42/59: batchLoss = 2.8791, diffLoss = 14.0714, kgLoss = 0.0811
2025-02-12 23:59:19.194228: Training Step 43/59: batchLoss = 2.8161, diffLoss = 13.7517, kgLoss = 0.0822
2025-02-12 23:59:20.120162: Training Step 44/59: batchLoss = 2.9024, diffLoss = 14.1840, kgLoss = 0.0820
2025-02-12 23:59:21.046902: Training Step 45/59: batchLoss = 3.0532, diffLoss = 14.9123, kgLoss = 0.0885
2025-02-12 23:59:21.975201: Training Step 46/59: batchLoss = 3.1857, diffLoss = 15.5862, kgLoss = 0.0856
2025-02-12 23:59:22.911877: Training Step 47/59: batchLoss = 2.9998, diffLoss = 14.6642, kgLoss = 0.0837
2025-02-12 23:59:23.851764: Training Step 48/59: batchLoss = 3.0403, diffLoss = 14.8537, kgLoss = 0.0870
2025-02-12 23:59:24.780383: Training Step 49/59: batchLoss = 2.8777, diffLoss = 14.0483, kgLoss = 0.0850
2025-02-12 23:59:25.713877: Training Step 50/59: batchLoss = 3.1482, diffLoss = 15.3928, kgLoss = 0.0871
2025-02-12 23:59:26.639900: Training Step 51/59: batchLoss = 2.7665, diffLoss = 13.5061, kgLoss = 0.0816
2025-02-12 23:59:27.559695: Training Step 52/59: batchLoss = 3.1761, diffLoss = 15.5289, kgLoss = 0.0879
2025-02-12 23:59:28.484805: Training Step 53/59: batchLoss = 3.1790, diffLoss = 15.5087, kgLoss = 0.0966
2025-02-12 23:59:29.420619: Training Step 54/59: batchLoss = 2.9847, diffLoss = 14.5940, kgLoss = 0.0823
2025-02-12 23:59:30.353518: Training Step 55/59: batchLoss = 2.5356, diffLoss = 12.3879, kgLoss = 0.0725
2025-02-12 23:59:31.270977: Training Step 56/59: batchLoss = 2.9927, diffLoss = 14.6327, kgLoss = 0.0827
2025-02-12 23:59:32.108211: Training Step 57/59: batchLoss = 3.0550, diffLoss = 14.9254, kgLoss = 0.0874
2025-02-12 23:59:32.958562: Training Step 58/59: batchLoss = 2.9355, diffLoss = 14.3363, kgLoss = 0.0853
2025-02-12 23:59:33.051350: 
2025-02-12 23:59:33.051674: Epoch 60/1000, Train: epLoss = 0.4340, epDfLoss = 2.1198, epKgLoss = 0.0126  
2025-02-12 23:59:34.499966: Steps 0/47: batch_recall = 36.94, batch_ndcg = 48.05 
2025-02-12 23:59:35.791269: Steps 1/47: batch_recall = 37.88, batch_ndcg = 43.14 
2025-02-12 23:59:37.022567: Steps 2/47: batch_recall = 40.50, batch_ndcg = 46.82 
2025-02-12 23:59:38.266848: Steps 3/47: batch_recall = 44.36, batch_ndcg = 45.82 
2025-02-12 23:59:39.456402: Steps 4/47: batch_recall = 39.04, batch_ndcg = 45.29 
2025-02-12 23:59:40.669167: Steps 5/47: batch_recall = 33.32, batch_ndcg = 39.94 
2025-02-12 23:59:41.871067: Steps 6/47: batch_recall = 38.51, batch_ndcg = 41.87 
2025-02-12 23:59:43.048536: Steps 7/47: batch_recall = 45.02, batch_ndcg = 45.05 
2025-02-12 23:59:44.238671: Steps 8/47: batch_recall = 47.39, batch_ndcg = 52.28 
2025-02-12 23:59:45.387824: Steps 9/47: batch_recall = 47.42, batch_ndcg = 47.09 
2025-02-12 23:59:46.556286: Steps 10/47: batch_recall = 44.24, batch_ndcg = 43.72 
2025-02-12 23:59:47.699002: Steps 11/47: batch_recall = 55.66, batch_ndcg = 52.60 
2025-02-12 23:59:48.845736: Steps 12/47: batch_recall = 49.37, batch_ndcg = 48.28 
2025-02-12 23:59:49.986042: Steps 13/47: batch_recall = 48.68, batch_ndcg = 45.37 
2025-02-12 23:59:51.076955: Steps 14/47: batch_recall = 40.16, batch_ndcg = 40.72 
2025-02-12 23:59:52.182858: Steps 15/47: batch_recall = 57.01, batch_ndcg = 53.65 
2025-02-12 23:59:53.266683: Steps 16/47: batch_recall = 49.77, batch_ndcg = 46.35 
2025-02-12 23:59:54.312100: Steps 17/47: batch_recall = 57.90, batch_ndcg = 49.72 
2025-02-12 23:59:55.398098: Steps 18/47: batch_recall = 52.41, batch_ndcg = 49.61 
2025-02-12 23:59:56.455722: Steps 19/47: batch_recall = 60.27, batch_ndcg = 55.95 
2025-02-12 23:59:57.489022: Steps 20/47: batch_recall = 69.47, batch_ndcg = 62.63 
2025-02-12 23:59:58.541917: Steps 21/47: batch_recall = 65.80, batch_ndcg = 54.93 
2025-02-12 23:59:59.589633: Steps 22/47: batch_recall = 56.47, batch_ndcg = 51.64 
2025-02-13 00:00:00.644890: Steps 23/47: batch_recall = 63.40, batch_ndcg = 55.00 
2025-02-13 00:00:01.722598: Steps 24/47: batch_recall = 65.13, batch_ndcg = 57.90 
2025-02-13 00:00:02.776216: Steps 25/47: batch_recall = 65.21, batch_ndcg = 56.78 
2025-02-13 00:00:03.799818: Steps 26/47: batch_recall = 59.90, batch_ndcg = 53.12 
2025-02-13 00:00:04.842062: Steps 27/47: batch_recall = 61.44, batch_ndcg = 52.48 
2025-02-13 00:00:05.870104: Steps 28/47: batch_recall = 69.84, batch_ndcg = 59.58 
2025-02-13 00:00:06.885752: Steps 29/47: batch_recall = 68.95, batch_ndcg = 57.11 
2025-02-13 00:00:07.906739: Steps 30/47: batch_recall = 72.58, batch_ndcg = 64.10 
2025-02-13 00:00:08.947247: Steps 31/47: batch_recall = 66.91, batch_ndcg = 56.44 
2025-02-13 00:00:09.948217: Steps 32/47: batch_recall = 72.18, batch_ndcg = 65.55 
2025-02-13 00:00:10.963299: Steps 33/47: batch_recall = 81.61, batch_ndcg = 68.67 
2025-02-13 00:00:11.968994: Steps 34/47: batch_recall = 68.96, batch_ndcg = 56.24 
2025-02-13 00:00:12.943605: Steps 35/47: batch_recall = 76.13, batch_ndcg = 64.64 
2025-02-13 00:00:13.931997: Steps 36/47: batch_recall = 78.59, batch_ndcg = 64.64 
2025-02-13 00:00:14.899543: Steps 37/47: batch_recall = 84.27, batch_ndcg = 72.58 
2025-02-13 00:00:15.862795: Steps 38/47: batch_recall = 90.03, batch_ndcg = 72.42 
2025-02-13 00:00:16.827606: Steps 39/47: batch_recall = 87.30, batch_ndcg = 68.99 
2025-02-13 00:00:17.785550: Steps 40/47: batch_recall = 73.24, batch_ndcg = 62.56 
2025-02-13 00:00:18.748902: Steps 41/47: batch_recall = 87.05, batch_ndcg = 71.98 
2025-02-13 00:00:19.722687: Steps 42/47: batch_recall = 82.40, batch_ndcg = 64.60 
2025-02-13 00:00:20.691222: Steps 43/47: batch_recall = 90.43, batch_ndcg = 72.65 
2025-02-13 00:00:21.659337: Steps 44/47: batch_recall = 87.64, batch_ndcg = 69.37 
2025-02-13 00:00:22.603147: Steps 45/47: batch_recall = 92.61, batch_ndcg = 74.79 
2025-02-13 00:00:22.711103: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.65 
2025-02-13 00:00:22.711238: Epoch 60/1000, Test: Recall = 0.1216, NDCG = 0.1092  

2025-02-13 00:00:23.913680: Training Step 0/59: batchLoss = 2.8323, diffLoss = 13.8095, kgLoss = 0.0880
2025-02-13 00:00:24.851260: Training Step 1/59: batchLoss = 3.0665, diffLoss = 14.9879, kgLoss = 0.0861
2025-02-13 00:00:25.781950: Training Step 2/59: batchLoss = 2.7976, diffLoss = 13.6272, kgLoss = 0.0902
2025-02-13 00:00:26.710817: Training Step 3/59: batchLoss = 2.7777, diffLoss = 13.5258, kgLoss = 0.0907
2025-02-13 00:00:27.644192: Training Step 4/59: batchLoss = 2.8185, diffLoss = 13.7484, kgLoss = 0.0861
2025-02-13 00:00:28.576942: Training Step 5/59: batchLoss = 2.6559, diffLoss = 12.9682, kgLoss = 0.0778
2025-02-13 00:00:29.497026: Training Step 6/59: batchLoss = 2.8952, diffLoss = 14.1497, kgLoss = 0.0816
2025-02-13 00:00:30.420172: Training Step 7/59: batchLoss = 2.4746, diffLoss = 12.0730, kgLoss = 0.0750
2025-02-13 00:00:31.341202: Training Step 8/59: batchLoss = 2.8784, diffLoss = 14.0410, kgLoss = 0.0878
2025-02-13 00:00:32.263053: Training Step 9/59: batchLoss = 2.8942, diffLoss = 14.1484, kgLoss = 0.0807
2025-02-13 00:00:33.186337: Training Step 10/59: batchLoss = 2.9461, diffLoss = 14.3477, kgLoss = 0.0957
2025-02-13 00:00:34.107096: Training Step 11/59: batchLoss = 2.9354, diffLoss = 14.3383, kgLoss = 0.0847
2025-02-13 00:00:35.020938: Training Step 12/59: batchLoss = 2.7501, diffLoss = 13.4121, kgLoss = 0.0846
2025-02-13 00:00:35.941030: Training Step 13/59: batchLoss = 2.8082, diffLoss = 13.7136, kgLoss = 0.0819
2025-02-13 00:00:36.855719: Training Step 14/59: batchLoss = 2.8545, diffLoss = 13.9038, kgLoss = 0.0922
2025-02-13 00:00:37.778011: Training Step 15/59: batchLoss = 2.7848, diffLoss = 13.5770, kgLoss = 0.0868
2025-02-13 00:00:38.711378: Training Step 16/59: batchLoss = 2.7157, diffLoss = 13.2623, kgLoss = 0.0791
2025-02-13 00:00:39.649409: Training Step 17/59: batchLoss = 2.8223, diffLoss = 13.8086, kgLoss = 0.0757
2025-02-13 00:00:40.580322: Training Step 18/59: batchLoss = 3.1017, diffLoss = 15.1493, kgLoss = 0.0898
2025-02-13 00:00:41.520169: Training Step 19/59: batchLoss = 3.1230, diffLoss = 15.2531, kgLoss = 0.0904
2025-02-13 00:00:42.451995: Training Step 20/59: batchLoss = 2.9776, diffLoss = 14.5499, kgLoss = 0.0846
2025-02-13 00:00:43.391285: Training Step 21/59: batchLoss = 3.0154, diffLoss = 14.7238, kgLoss = 0.0883
2025-02-13 00:00:44.324906: Training Step 22/59: batchLoss = 2.9753, diffLoss = 14.5241, kgLoss = 0.0882
2025-02-13 00:00:45.261136: Training Step 23/59: batchLoss = 3.0086, diffLoss = 14.6588, kgLoss = 0.0960
2025-02-13 00:00:46.203491: Training Step 24/59: batchLoss = 2.8831, diffLoss = 14.0825, kgLoss = 0.0833
2025-02-13 00:00:47.138205: Training Step 25/59: batchLoss = 2.9719, diffLoss = 14.5108, kgLoss = 0.0872
2025-02-13 00:00:48.060417: Training Step 26/59: batchLoss = 2.6094, diffLoss = 12.7531, kgLoss = 0.0735
2025-02-13 00:00:48.993261: Training Step 27/59: batchLoss = 2.8453, diffLoss = 13.8845, kgLoss = 0.0855
2025-02-13 00:00:49.915119: Training Step 28/59: batchLoss = 3.0489, diffLoss = 14.8933, kgLoss = 0.0878
2025-02-13 00:00:50.842969: Training Step 29/59: batchLoss = 2.9273, diffLoss = 14.2997, kgLoss = 0.0842
2025-02-13 00:00:51.767091: Training Step 30/59: batchLoss = 2.9323, diffLoss = 14.3419, kgLoss = 0.0798
2025-02-13 00:00:52.696980: Training Step 31/59: batchLoss = 2.9856, diffLoss = 14.5948, kgLoss = 0.0833
2025-02-13 00:00:53.625562: Training Step 32/59: batchLoss = 3.1157, diffLoss = 15.2177, kgLoss = 0.0902
2025-02-13 00:00:54.557447: Training Step 33/59: batchLoss = 3.2509, diffLoss = 15.9010, kgLoss = 0.0883
2025-02-13 00:00:55.478142: Training Step 34/59: batchLoss = 3.0582, diffLoss = 14.9279, kgLoss = 0.0908
2025-02-13 00:00:56.395093: Training Step 35/59: batchLoss = 2.7446, diffLoss = 13.4036, kgLoss = 0.0798
2025-02-13 00:00:57.319233: Training Step 36/59: batchLoss = 2.8498, diffLoss = 13.9271, kgLoss = 0.0805
2025-02-13 00:00:58.250446: Training Step 37/59: batchLoss = 3.0279, diffLoss = 14.7770, kgLoss = 0.0906
2025-02-13 00:00:59.180173: Training Step 38/59: batchLoss = 2.9524, diffLoss = 14.4264, kgLoss = 0.0839
2025-02-13 00:01:00.123641: Training Step 39/59: batchLoss = 3.2234, diffLoss = 15.7593, kgLoss = 0.0894
2025-02-13 00:01:01.059316: Training Step 40/59: batchLoss = 3.1802, diffLoss = 15.5428, kgLoss = 0.0895
2025-02-13 00:01:01.995213: Training Step 41/59: batchLoss = 3.0993, diffLoss = 15.1486, kgLoss = 0.0870
2025-02-13 00:01:02.934491: Training Step 42/59: batchLoss = 2.9848, diffLoss = 14.5707, kgLoss = 0.0884
2025-02-13 00:01:03.872521: Training Step 43/59: batchLoss = 2.9489, diffLoss = 14.4005, kgLoss = 0.0860
2025-02-13 00:01:04.802239: Training Step 44/59: batchLoss = 3.1702, diffLoss = 15.5039, kgLoss = 0.0868
2025-02-13 00:01:05.742924: Training Step 45/59: batchLoss = 3.0594, diffLoss = 14.9655, kgLoss = 0.0828
2025-02-13 00:01:06.680914: Training Step 46/59: batchLoss = 2.8774, diffLoss = 14.0594, kgLoss = 0.0819
2025-02-13 00:01:07.622421: Training Step 47/59: batchLoss = 3.0411, diffLoss = 14.8768, kgLoss = 0.0822
2025-02-13 00:01:08.573124: Training Step 48/59: batchLoss = 2.9676, diffLoss = 14.4788, kgLoss = 0.0898
2025-02-13 00:01:09.503061: Training Step 49/59: batchLoss = 3.2908, diffLoss = 16.0650, kgLoss = 0.0972
2025-02-13 00:01:10.424640: Training Step 50/59: batchLoss = 2.8523, diffLoss = 13.9393, kgLoss = 0.0805
2025-02-13 00:01:11.352800: Training Step 51/59: batchLoss = 2.8644, diffLoss = 14.0134, kgLoss = 0.0772
2025-02-13 00:01:12.271655: Training Step 52/59: batchLoss = 3.0790, diffLoss = 15.0779, kgLoss = 0.0793
2025-02-13 00:01:13.203118: Training Step 53/59: batchLoss = 3.0132, diffLoss = 14.7245, kgLoss = 0.0853
2025-02-13 00:01:14.138190: Training Step 54/59: batchLoss = 2.8593, diffLoss = 13.9649, kgLoss = 0.0828
2025-02-13 00:01:15.063379: Training Step 55/59: batchLoss = 3.2497, diffLoss = 15.8678, kgLoss = 0.0952
2025-02-13 00:01:15.989646: Training Step 56/59: batchLoss = 3.0301, diffLoss = 14.8107, kgLoss = 0.0849
2025-02-13 00:01:16.840234: Training Step 57/59: batchLoss = 3.3886, diffLoss = 16.5710, kgLoss = 0.0929
2025-02-13 00:01:17.693528: Training Step 58/59: batchLoss = 2.9140, diffLoss = 14.2551, kgLoss = 0.0787
2025-02-13 00:01:17.794643: 
2025-02-13 00:01:17.794998: Epoch 61/1000, Train: epLoss = 0.4355, epDfLoss = 2.1271, epKgLoss = 0.0126  
2025-02-13 00:01:19.285406: Steps 0/47: batch_recall = 36.83, batch_ndcg = 47.25 
2025-02-13 00:01:20.590282: Steps 1/47: batch_recall = 37.00, batch_ndcg = 42.23 
2025-02-13 00:01:21.848371: Steps 2/47: batch_recall = 41.79, batch_ndcg = 47.12 
2025-02-13 00:01:23.113328: Steps 3/47: batch_recall = 43.86, batch_ndcg = 46.66 
2025-02-13 00:01:24.322714: Steps 4/47: batch_recall = 38.94, batch_ndcg = 45.09 
2025-02-13 00:01:25.551922: Steps 5/47: batch_recall = 33.14, batch_ndcg = 39.55 
2025-02-13 00:01:26.751482: Steps 6/47: batch_recall = 39.21, batch_ndcg = 41.99 
2025-02-13 00:01:27.914445: Steps 7/47: batch_recall = 43.73, batch_ndcg = 45.68 
2025-02-13 00:01:29.090807: Steps 8/47: batch_recall = 47.51, batch_ndcg = 52.31 
2025-02-13 00:01:30.224280: Steps 9/47: batch_recall = 47.06, batch_ndcg = 47.00 
2025-02-13 00:01:31.393930: Steps 10/47: batch_recall = 43.89, batch_ndcg = 43.42 
2025-02-13 00:01:32.530347: Steps 11/47: batch_recall = 54.87, batch_ndcg = 52.06 
2025-02-13 00:01:33.670847: Steps 12/47: batch_recall = 49.26, batch_ndcg = 49.09 
2025-02-13 00:01:34.803119: Steps 13/47: batch_recall = 48.64, batch_ndcg = 46.48 
2025-02-13 00:01:35.903531: Steps 14/47: batch_recall = 40.86, batch_ndcg = 40.88 
2025-02-13 00:01:37.003093: Steps 15/47: batch_recall = 56.67, batch_ndcg = 53.98 
2025-02-13 00:01:38.099224: Steps 16/47: batch_recall = 49.86, batch_ndcg = 46.68 
2025-02-13 00:01:39.164557: Steps 17/47: batch_recall = 58.76, batch_ndcg = 49.64 
2025-02-13 00:01:40.264338: Steps 18/47: batch_recall = 53.21, batch_ndcg = 50.03 
2025-02-13 00:01:41.347164: Steps 19/47: batch_recall = 61.92, batch_ndcg = 56.15 
2025-02-13 00:01:42.409245: Steps 20/47: batch_recall = 69.68, batch_ndcg = 62.80 
2025-02-13 00:01:43.470219: Steps 21/47: batch_recall = 65.12, batch_ndcg = 55.63 
2025-02-13 00:01:44.533564: Steps 22/47: batch_recall = 55.22, batch_ndcg = 51.33 
2025-02-13 00:01:45.584889: Steps 23/47: batch_recall = 64.33, batch_ndcg = 55.61 
2025-02-13 00:01:46.645649: Steps 24/47: batch_recall = 64.59, batch_ndcg = 57.58 
2025-02-13 00:01:47.683571: Steps 25/47: batch_recall = 65.76, batch_ndcg = 56.96 
2025-02-13 00:01:48.704959: Steps 26/47: batch_recall = 59.30, batch_ndcg = 53.56 
2025-02-13 00:01:49.737522: Steps 27/47: batch_recall = 62.46, batch_ndcg = 53.08 
2025-02-13 00:01:50.751150: Steps 28/47: batch_recall = 70.26, batch_ndcg = 60.06 
2025-02-13 00:01:51.765195: Steps 29/47: batch_recall = 70.56, batch_ndcg = 58.17 
2025-02-13 00:01:52.777063: Steps 30/47: batch_recall = 73.47, batch_ndcg = 64.71 
2025-02-13 00:01:53.815472: Steps 31/47: batch_recall = 65.66, batch_ndcg = 55.14 
2025-02-13 00:01:54.833688: Steps 32/47: batch_recall = 70.61, batch_ndcg = 65.14 
2025-02-13 00:01:55.850925: Steps 33/47: batch_recall = 80.16, batch_ndcg = 67.74 
2025-02-13 00:01:56.866856: Steps 34/47: batch_recall = 67.63, batch_ndcg = 55.86 
2025-02-13 00:01:57.858606: Steps 35/47: batch_recall = 78.48, batch_ndcg = 66.15 
2025-02-13 00:01:58.856879: Steps 36/47: batch_recall = 79.32, batch_ndcg = 64.92 
2025-02-13 00:01:59.848663: Steps 37/47: batch_recall = 85.04, batch_ndcg = 72.81 
2025-02-13 00:02:00.823741: Steps 38/47: batch_recall = 91.52, batch_ndcg = 73.48 
2025-02-13 00:02:01.797038: Steps 39/47: batch_recall = 88.59, batch_ndcg = 69.94 
2025-02-13 00:02:02.762523: Steps 40/47: batch_recall = 73.64, batch_ndcg = 63.32 
2025-02-13 00:02:03.716787: Steps 41/47: batch_recall = 88.13, batch_ndcg = 71.59 
2025-02-13 00:02:04.683390: Steps 42/47: batch_recall = 82.08, batch_ndcg = 64.56 
2025-02-13 00:02:05.642365: Steps 43/47: batch_recall = 89.50, batch_ndcg = 72.31 
2025-02-13 00:02:06.604345: Steps 44/47: batch_recall = 86.81, batch_ndcg = 68.91 
2025-02-13 00:02:07.542813: Steps 45/47: batch_recall = 91.75, batch_ndcg = 73.97 
2025-02-13 00:02:07.646928: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.67 
2025-02-13 00:02:07.647062: Epoch 61/1000, Test: Recall = 0.1217, NDCG = 0.1095  

2025-02-13 00:02:08.833866: Training Step 0/59: batchLoss = 2.7545, diffLoss = 13.4340, kgLoss = 0.0846
2025-02-13 00:02:09.755827: Training Step 1/59: batchLoss = 2.9956, diffLoss = 14.6289, kgLoss = 0.0873
2025-02-13 00:02:10.680494: Training Step 2/59: batchLoss = 2.9867, diffLoss = 14.5451, kgLoss = 0.0971
2025-02-13 00:02:11.610401: Training Step 3/59: batchLoss = 2.8359, diffLoss = 13.8160, kgLoss = 0.0909
2025-02-13 00:02:12.537216: Training Step 4/59: batchLoss = 2.6292, diffLoss = 12.8264, kgLoss = 0.0799
2025-02-13 00:02:13.469491: Training Step 5/59: batchLoss = 2.6208, diffLoss = 12.7800, kgLoss = 0.0810
2025-02-13 00:02:14.398160: Training Step 6/59: batchLoss = 2.8512, diffLoss = 13.9180, kgLoss = 0.0845
2025-02-13 00:02:15.329226: Training Step 7/59: batchLoss = 3.0485, diffLoss = 14.8853, kgLoss = 0.0893
2025-02-13 00:02:16.254446: Training Step 8/59: batchLoss = 2.6414, diffLoss = 12.8995, kgLoss = 0.0769
2025-02-13 00:02:17.187176: Training Step 9/59: batchLoss = 2.8591, diffLoss = 13.9531, kgLoss = 0.0856
2025-02-13 00:02:18.106721: Training Step 10/59: batchLoss = 2.7811, diffLoss = 13.5749, kgLoss = 0.0826
2025-02-13 00:02:19.044389: Training Step 11/59: batchLoss = 2.9324, diffLoss = 14.3260, kgLoss = 0.0840
2025-02-13 00:02:19.978899: Training Step 12/59: batchLoss = 2.9037, diffLoss = 14.1799, kgLoss = 0.0847
2025-02-13 00:02:20.913807: Training Step 13/59: batchLoss = 2.7663, diffLoss = 13.5235, kgLoss = 0.0770
2025-02-13 00:02:21.849802: Training Step 14/59: batchLoss = 2.9698, diffLoss = 14.4301, kgLoss = 0.1047
2025-02-13 00:02:22.794036: Training Step 15/59: batchLoss = 3.2301, diffLoss = 15.7359, kgLoss = 0.1036
2025-02-13 00:02:23.731637: Training Step 16/59: batchLoss = 3.0716, diffLoss = 14.9863, kgLoss = 0.0929
2025-02-13 00:02:24.669643: Training Step 17/59: batchLoss = 2.9572, diffLoss = 14.4330, kgLoss = 0.0882
2025-02-13 00:02:25.611490: Training Step 18/59: batchLoss = 3.0036, diffLoss = 14.6961, kgLoss = 0.0804
2025-02-13 00:02:26.555668: Training Step 19/59: batchLoss = 2.9589, diffLoss = 14.4638, kgLoss = 0.0827
2025-02-13 00:02:27.491093: Training Step 20/59: batchLoss = 2.5797, diffLoss = 12.5989, kgLoss = 0.0749
2025-02-13 00:02:28.430240: Training Step 21/59: batchLoss = 2.8565, diffLoss = 13.9572, kgLoss = 0.0813
2025-02-13 00:02:29.363359: Training Step 22/59: batchLoss = 3.0938, diffLoss = 15.1154, kgLoss = 0.0884
2025-02-13 00:02:30.293129: Training Step 23/59: batchLoss = 2.9317, diffLoss = 14.3224, kgLoss = 0.0840
2025-02-13 00:02:31.222188: Training Step 24/59: batchLoss = 3.1211, diffLoss = 15.2523, kgLoss = 0.0883
2025-02-13 00:02:32.147702: Training Step 25/59: batchLoss = 2.7666, diffLoss = 13.5074, kgLoss = 0.0814
2025-02-13 00:02:33.084036: Training Step 26/59: batchLoss = 2.9954, diffLoss = 14.6045, kgLoss = 0.0931
2025-02-13 00:02:34.011082: Training Step 27/59: batchLoss = 2.9780, diffLoss = 14.5349, kgLoss = 0.0888
2025-02-13 00:02:34.932830: Training Step 28/59: batchLoss = 3.0243, diffLoss = 14.7874, kgLoss = 0.0836
2025-02-13 00:02:35.860031: Training Step 29/59: batchLoss = 2.9965, diffLoss = 14.6413, kgLoss = 0.0854
2025-02-13 00:02:36.785104: Training Step 30/59: batchLoss = 2.6528, diffLoss = 12.9548, kgLoss = 0.0773
2025-02-13 00:02:37.717441: Training Step 31/59: batchLoss = 2.6564, diffLoss = 12.9785, kgLoss = 0.0759
2025-02-13 00:02:38.652131: Training Step 32/59: batchLoss = 2.8450, diffLoss = 13.9139, kgLoss = 0.0778
2025-02-13 00:02:39.594053: Training Step 33/59: batchLoss = 3.0343, diffLoss = 14.8038, kgLoss = 0.0919
2025-02-13 00:02:40.525641: Training Step 34/59: batchLoss = 3.1544, diffLoss = 15.4133, kgLoss = 0.0897
2025-02-13 00:02:41.467010: Training Step 35/59: batchLoss = 3.2727, diffLoss = 15.9735, kgLoss = 0.0975
2025-02-13 00:02:42.404532: Training Step 36/59: batchLoss = 3.0594, diffLoss = 14.9468, kgLoss = 0.0876
2025-02-13 00:02:43.340349: Training Step 37/59: batchLoss = 2.9489, diffLoss = 14.4012, kgLoss = 0.0859
2025-02-13 00:02:44.278904: Training Step 38/59: batchLoss = 2.6954, diffLoss = 13.1841, kgLoss = 0.0733
2025-02-13 00:02:45.213255: Training Step 39/59: batchLoss = 3.2400, diffLoss = 15.8238, kgLoss = 0.0941
2025-02-13 00:02:46.146289: Training Step 40/59: batchLoss = 2.7578, diffLoss = 13.4797, kgLoss = 0.0773
2025-02-13 00:02:47.082424: Training Step 41/59: batchLoss = 3.0098, diffLoss = 14.7184, kgLoss = 0.0827
2025-02-13 00:02:48.014104: Training Step 42/59: batchLoss = 2.9408, diffLoss = 14.3808, kgLoss = 0.0808
2025-02-13 00:02:48.950476: Training Step 43/59: batchLoss = 2.7894, diffLoss = 13.6063, kgLoss = 0.0851
2025-02-13 00:02:49.875935: Training Step 44/59: batchLoss = 2.7931, diffLoss = 13.6121, kgLoss = 0.0884
2025-02-13 00:02:50.806398: Training Step 45/59: batchLoss = 2.8405, diffLoss = 13.8981, kgLoss = 0.0761
2025-02-13 00:02:51.730191: Training Step 46/59: batchLoss = 3.0233, diffLoss = 14.7914, kgLoss = 0.0813
2025-02-13 00:02:52.658388: Training Step 47/59: batchLoss = 2.9807, diffLoss = 14.5628, kgLoss = 0.0852
2025-02-13 00:02:53.591262: Training Step 48/59: batchLoss = 3.0348, diffLoss = 14.8225, kgLoss = 0.0878
2025-02-13 00:02:54.520173: Training Step 49/59: batchLoss = 2.7618, diffLoss = 13.4875, kgLoss = 0.0804
2025-02-13 00:02:55.452049: Training Step 50/59: batchLoss = 2.8838, diffLoss = 14.0983, kgLoss = 0.0801
2025-02-13 00:02:56.392774: Training Step 51/59: batchLoss = 3.4267, diffLoss = 16.7022, kgLoss = 0.1078
2025-02-13 00:02:57.325237: Training Step 52/59: batchLoss = 3.0711, diffLoss = 14.9971, kgLoss = 0.0896
2025-02-13 00:02:58.251836: Training Step 53/59: batchLoss = 2.8937, diffLoss = 14.1283, kgLoss = 0.0851
2025-02-13 00:02:59.198276: Training Step 54/59: batchLoss = 2.8632, diffLoss = 14.0026, kgLoss = 0.0783
2025-02-13 00:03:00.129442: Training Step 55/59: batchLoss = 3.0330, diffLoss = 14.8496, kgLoss = 0.0788
2025-02-13 00:03:01.076500: Training Step 56/59: batchLoss = 2.9453, diffLoss = 14.3909, kgLoss = 0.0839
2025-02-13 00:03:01.926076: Training Step 57/59: batchLoss = 3.0020, diffLoss = 14.6656, kgLoss = 0.0861
2025-02-13 00:03:02.783629: Training Step 58/59: batchLoss = 3.0386, diffLoss = 14.7994, kgLoss = 0.0984
2025-02-13 00:03:02.881220: 
2025-02-13 00:03:02.881768: Epoch 62/1000, Train: epLoss = 0.4320, epDfLoss = 2.1094, epKgLoss = 0.0126  
2025-02-13 00:03:04.374303: Steps 0/47: batch_recall = 37.05, batch_ndcg = 47.59 
2025-02-13 00:03:05.686082: Steps 1/47: batch_recall = 36.91, batch_ndcg = 42.99 
2025-02-13 00:03:06.948777: Steps 2/47: batch_recall = 41.86, batch_ndcg = 47.34 
2025-02-13 00:03:08.217851: Steps 3/47: batch_recall = 44.13, batch_ndcg = 46.45 
2025-02-13 00:03:09.417877: Steps 4/47: batch_recall = 40.00, batch_ndcg = 45.73 
2025-02-13 00:03:10.625523: Steps 5/47: batch_recall = 33.71, batch_ndcg = 40.04 
2025-02-13 00:03:11.824720: Steps 6/47: batch_recall = 38.49, batch_ndcg = 41.77 
2025-02-13 00:03:12.993607: Steps 7/47: batch_recall = 44.03, batch_ndcg = 45.10 
2025-02-13 00:03:14.166301: Steps 8/47: batch_recall = 47.68, batch_ndcg = 52.77 
2025-02-13 00:03:15.298560: Steps 9/47: batch_recall = 45.92, batch_ndcg = 46.06 
2025-02-13 00:03:16.474011: Steps 10/47: batch_recall = 44.31, batch_ndcg = 44.52 
2025-02-13 00:03:17.634064: Steps 11/47: batch_recall = 53.49, batch_ndcg = 52.24 
2025-02-13 00:03:18.789910: Steps 12/47: batch_recall = 48.68, batch_ndcg = 47.76 
2025-02-13 00:03:19.940777: Steps 13/47: batch_recall = 48.57, batch_ndcg = 46.66 
2025-02-13 00:03:21.050472: Steps 14/47: batch_recall = 41.32, batch_ndcg = 41.49 
2025-02-13 00:03:22.169843: Steps 15/47: batch_recall = 56.74, batch_ndcg = 53.29 
2025-02-13 00:03:23.270634: Steps 16/47: batch_recall = 50.18, batch_ndcg = 46.68 
2025-02-13 00:03:24.346788: Steps 17/47: batch_recall = 58.59, batch_ndcg = 49.73 
2025-02-13 00:03:25.456934: Steps 18/47: batch_recall = 52.28, batch_ndcg = 49.47 
2025-02-13 00:03:26.528842: Steps 19/47: batch_recall = 61.49, batch_ndcg = 55.24 
2025-02-13 00:03:27.576058: Steps 20/47: batch_recall = 69.95, batch_ndcg = 62.48 
2025-02-13 00:03:28.623993: Steps 21/47: batch_recall = 64.91, batch_ndcg = 54.79 
2025-02-13 00:03:29.672961: Steps 22/47: batch_recall = 54.48, batch_ndcg = 50.59 
2025-02-13 00:03:30.726875: Steps 23/47: batch_recall = 63.23, batch_ndcg = 55.35 
2025-02-13 00:03:31.791858: Steps 24/47: batch_recall = 65.74, batch_ndcg = 58.23 
2025-02-13 00:03:32.832464: Steps 25/47: batch_recall = 65.55, batch_ndcg = 57.47 
2025-02-13 00:03:33.842652: Steps 26/47: batch_recall = 58.85, batch_ndcg = 53.24 
2025-02-13 00:03:34.881049: Steps 27/47: batch_recall = 61.20, batch_ndcg = 53.33 
2025-02-13 00:03:35.908812: Steps 28/47: batch_recall = 69.82, batch_ndcg = 59.66 
2025-02-13 00:03:36.932719: Steps 29/47: batch_recall = 71.40, batch_ndcg = 58.49 
2025-02-13 00:03:37.955096: Steps 30/47: batch_recall = 72.78, batch_ndcg = 64.21 
2025-02-13 00:03:39.006260: Steps 31/47: batch_recall = 66.11, batch_ndcg = 55.91 
2025-02-13 00:03:40.028372: Steps 32/47: batch_recall = 70.00, batch_ndcg = 64.46 
2025-02-13 00:03:41.051285: Steps 33/47: batch_recall = 79.22, batch_ndcg = 67.75 
2025-02-13 00:03:42.072081: Steps 34/47: batch_recall = 68.55, batch_ndcg = 56.53 
2025-02-13 00:03:43.073336: Steps 35/47: batch_recall = 77.67, batch_ndcg = 65.28 
2025-02-13 00:03:44.070924: Steps 36/47: batch_recall = 78.64, batch_ndcg = 64.48 
2025-02-13 00:03:45.045890: Steps 37/47: batch_recall = 84.30, batch_ndcg = 72.05 
2025-02-13 00:03:46.016016: Steps 38/47: batch_recall = 90.61, batch_ndcg = 73.27 
2025-02-13 00:03:46.980940: Steps 39/47: batch_recall = 88.17, batch_ndcg = 69.02 
2025-02-13 00:03:47.941960: Steps 40/47: batch_recall = 73.01, batch_ndcg = 62.76 
2025-02-13 00:03:48.895277: Steps 41/47: batch_recall = 87.73, batch_ndcg = 71.96 
2025-02-13 00:03:49.846091: Steps 42/47: batch_recall = 82.57, batch_ndcg = 64.76 
2025-02-13 00:03:50.815967: Steps 43/47: batch_recall = 92.17, batch_ndcg = 74.02 
2025-02-13 00:03:51.778612: Steps 44/47: batch_recall = 88.10, batch_ndcg = 70.61 
2025-02-13 00:03:52.721280: Steps 45/47: batch_recall = 94.05, batch_ndcg = 75.94 
2025-02-13 00:03:52.828308: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.62 
2025-02-13 00:03:52.828438: Epoch 62/1000, Test: Recall = 0.1216, NDCG = 0.1095  

2025-02-13 00:03:54.019507: Training Step 0/59: batchLoss = 2.7342, diffLoss = 13.3447, kgLoss = 0.0816
2025-02-13 00:03:54.943654: Training Step 1/59: batchLoss = 2.7796, diffLoss = 13.5713, kgLoss = 0.0816
2025-02-13 00:03:55.867889: Training Step 2/59: batchLoss = 2.8455, diffLoss = 13.8756, kgLoss = 0.0880
2025-02-13 00:03:56.809027: Training Step 3/59: batchLoss = 2.8776, diffLoss = 14.0722, kgLoss = 0.0790
2025-02-13 00:03:57.739388: Training Step 4/59: batchLoss = 2.7327, diffLoss = 13.3203, kgLoss = 0.0858
2025-02-13 00:03:58.664958: Training Step 5/59: batchLoss = 2.7112, diffLoss = 13.2080, kgLoss = 0.0870
2025-02-13 00:03:59.593297: Training Step 6/59: batchLoss = 2.6139, diffLoss = 12.7652, kgLoss = 0.0760
2025-02-13 00:04:00.522380: Training Step 7/59: batchLoss = 2.7563, diffLoss = 13.4548, kgLoss = 0.0816
2025-02-13 00:04:01.444922: Training Step 8/59: batchLoss = 2.7235, diffLoss = 13.2922, kgLoss = 0.0813
2025-02-13 00:04:02.370881: Training Step 9/59: batchLoss = 3.0356, diffLoss = 14.8259, kgLoss = 0.0880
2025-02-13 00:04:03.295285: Training Step 10/59: batchLoss = 2.6148, diffLoss = 12.7622, kgLoss = 0.0779
2025-02-13 00:04:04.220923: Training Step 11/59: batchLoss = 2.6257, diffLoss = 12.8329, kgLoss = 0.0739
2025-02-13 00:04:05.140238: Training Step 12/59: batchLoss = 2.8145, diffLoss = 13.7541, kgLoss = 0.0796
2025-02-13 00:04:06.061602: Training Step 13/59: batchLoss = 2.9735, diffLoss = 14.5257, kgLoss = 0.0854
2025-02-13 00:04:06.994283: Training Step 14/59: batchLoss = 3.1469, diffLoss = 15.3516, kgLoss = 0.0958
2025-02-13 00:04:07.917651: Training Step 15/59: batchLoss = 3.1627, diffLoss = 15.4353, kgLoss = 0.0946
2025-02-13 00:04:08.850997: Training Step 16/59: batchLoss = 2.7598, diffLoss = 13.4700, kgLoss = 0.0822
2025-02-13 00:04:09.780035: Training Step 17/59: batchLoss = 3.0633, diffLoss = 14.9369, kgLoss = 0.0949
2025-02-13 00:04:10.723125: Training Step 18/59: batchLoss = 2.8942, diffLoss = 14.1172, kgLoss = 0.0884
2025-02-13 00:04:11.650062: Training Step 19/59: batchLoss = 2.7933, diffLoss = 13.6381, kgLoss = 0.0821
2025-02-13 00:04:12.612135: Training Step 20/59: batchLoss = 3.0763, diffLoss = 15.0141, kgLoss = 0.0919
2025-02-13 00:04:13.542600: Training Step 21/59: batchLoss = 3.0417, diffLoss = 14.8457, kgLoss = 0.0907
2025-02-13 00:04:14.478256: Training Step 22/59: batchLoss = 2.8482, diffLoss = 13.9353, kgLoss = 0.0764
2025-02-13 00:04:15.411111: Training Step 23/59: batchLoss = 3.0127, diffLoss = 14.7171, kgLoss = 0.0866
2025-02-13 00:04:16.344749: Training Step 24/59: batchLoss = 3.0910, diffLoss = 15.1136, kgLoss = 0.0854
2025-02-13 00:04:17.278340: Training Step 25/59: batchLoss = 2.8205, diffLoss = 13.7825, kgLoss = 0.0800
2025-02-13 00:04:18.210722: Training Step 26/59: batchLoss = 2.9237, diffLoss = 14.2929, kgLoss = 0.0814
2025-02-13 00:04:19.133080: Training Step 27/59: batchLoss = 2.7768, diffLoss = 13.5484, kgLoss = 0.0839
2025-02-13 00:04:20.054429: Training Step 28/59: batchLoss = 2.8835, diffLoss = 14.0761, kgLoss = 0.0854
2025-02-13 00:04:20.971747: Training Step 29/59: batchLoss = 2.9656, diffLoss = 14.4485, kgLoss = 0.0949
2025-02-13 00:04:21.903593: Training Step 30/59: batchLoss = 3.2040, diffLoss = 15.6251, kgLoss = 0.0987
2025-02-13 00:04:22.829218: Training Step 31/59: batchLoss = 3.2430, diffLoss = 15.8260, kgLoss = 0.0973
2025-02-13 00:04:23.757038: Training Step 32/59: batchLoss = 2.6825, diffLoss = 13.0598, kgLoss = 0.0882
2025-02-13 00:04:24.686240: Training Step 33/59: batchLoss = 2.9302, diffLoss = 14.2935, kgLoss = 0.0894
2025-02-13 00:04:25.632850: Training Step 34/59: batchLoss = 2.8285, diffLoss = 13.8051, kgLoss = 0.0843
2025-02-13 00:04:26.562419: Training Step 35/59: batchLoss = 3.1480, diffLoss = 15.3867, kgLoss = 0.0884
2025-02-13 00:04:27.491233: Training Step 36/59: batchLoss = 2.6706, diffLoss = 13.0466, kgLoss = 0.0766
2025-02-13 00:04:28.421386: Training Step 37/59: batchLoss = 3.2635, diffLoss = 15.9728, kgLoss = 0.0862
2025-02-13 00:04:29.341973: Training Step 38/59: batchLoss = 2.9640, diffLoss = 14.4465, kgLoss = 0.0933
2025-02-13 00:04:30.268294: Training Step 39/59: batchLoss = 3.2733, diffLoss = 15.9917, kgLoss = 0.0937
2025-02-13 00:04:31.198980: Training Step 40/59: batchLoss = 2.7872, diffLoss = 13.6251, kgLoss = 0.0777
2025-02-13 00:04:32.135636: Training Step 41/59: batchLoss = 2.9700, diffLoss = 14.4955, kgLoss = 0.0886
2025-02-13 00:04:33.079697: Training Step 42/59: batchLoss = 2.9470, diffLoss = 14.4066, kgLoss = 0.0821
2025-02-13 00:04:34.017052: Training Step 43/59: batchLoss = 2.6600, diffLoss = 12.9909, kgLoss = 0.0773
2025-02-13 00:04:34.952720: Training Step 44/59: batchLoss = 3.0358, diffLoss = 14.8047, kgLoss = 0.0936
2025-02-13 00:04:35.890840: Training Step 45/59: batchLoss = 3.1975, diffLoss = 15.6399, kgLoss = 0.0869
2025-02-13 00:04:36.830752: Training Step 46/59: batchLoss = 3.0728, diffLoss = 14.9989, kgLoss = 0.0912
2025-02-13 00:04:37.770850: Training Step 47/59: batchLoss = 3.0290, diffLoss = 14.8093, kgLoss = 0.0839
2025-02-13 00:04:38.707066: Training Step 48/59: batchLoss = 3.1236, diffLoss = 15.2582, kgLoss = 0.0899
2025-02-13 00:04:39.641692: Training Step 49/59: batchLoss = 3.2699, diffLoss = 15.9911, kgLoss = 0.0896
2025-02-13 00:04:40.581477: Training Step 50/59: batchLoss = 2.6883, diffLoss = 13.1266, kgLoss = 0.0788
2025-02-13 00:04:41.517343: Training Step 51/59: batchLoss = 2.8698, diffLoss = 14.0238, kgLoss = 0.0813
2025-02-13 00:04:42.448725: Training Step 52/59: batchLoss = 2.9948, diffLoss = 14.6405, kgLoss = 0.0833
2025-02-13 00:04:43.380473: Training Step 53/59: batchLoss = 3.2091, diffLoss = 15.7003, kgLoss = 0.0863
2025-02-13 00:04:44.304656: Training Step 54/59: batchLoss = 2.5953, diffLoss = 12.6633, kgLoss = 0.0783
2025-02-13 00:04:45.224675: Training Step 55/59: batchLoss = 2.7698, diffLoss = 13.5478, kgLoss = 0.0753
2025-02-13 00:04:46.150670: Training Step 56/59: batchLoss = 3.2092, diffLoss = 15.6821, kgLoss = 0.0909
2025-02-13 00:04:46.996648: Training Step 57/59: batchLoss = 2.9848, diffLoss = 14.6018, kgLoss = 0.0806
2025-02-13 00:04:47.846887: Training Step 58/59: batchLoss = 3.2564, diffLoss = 15.8884, kgLoss = 0.0984
2025-02-13 00:04:47.943553: 
2025-02-13 00:04:47.943908: Epoch 63/1000, Train: epLoss = 0.4324, epDfLoss = 2.1117, epKgLoss = 0.0126  
2025-02-13 00:04:49.412201: Steps 0/47: batch_recall = 36.70, batch_ndcg = 48.18 
2025-02-13 00:04:50.708664: Steps 1/47: batch_recall = 37.76, batch_ndcg = 42.67 
2025-02-13 00:04:51.972482: Steps 2/47: batch_recall = 42.07, batch_ndcg = 47.44 
2025-02-13 00:04:53.240240: Steps 3/47: batch_recall = 44.27, batch_ndcg = 45.72 
2025-02-13 00:04:54.441014: Steps 4/47: batch_recall = 39.60, batch_ndcg = 45.51 
2025-02-13 00:04:55.655880: Steps 5/47: batch_recall = 33.27, batch_ndcg = 39.89 
2025-02-13 00:04:56.866829: Steps 6/47: batch_recall = 39.86, batch_ndcg = 42.26 
2025-02-13 00:04:58.074310: Steps 7/47: batch_recall = 45.09, batch_ndcg = 45.99 
2025-02-13 00:04:59.258875: Steps 8/47: batch_recall = 47.91, batch_ndcg = 53.34 
2025-02-13 00:05:00.403808: Steps 9/47: batch_recall = 46.22, batch_ndcg = 46.33 
2025-02-13 00:05:01.581707: Steps 10/47: batch_recall = 43.89, batch_ndcg = 44.04 
2025-02-13 00:05:02.727577: Steps 11/47: batch_recall = 54.30, batch_ndcg = 52.19 
2025-02-13 00:05:03.878092: Steps 12/47: batch_recall = 49.94, batch_ndcg = 48.49 
2025-02-13 00:05:05.008588: Steps 13/47: batch_recall = 48.32, batch_ndcg = 45.37 
2025-02-13 00:05:06.098750: Steps 14/47: batch_recall = 41.95, batch_ndcg = 41.56 
2025-02-13 00:05:07.192233: Steps 15/47: batch_recall = 57.19, batch_ndcg = 53.85 
2025-02-13 00:05:08.261805: Steps 16/47: batch_recall = 50.60, batch_ndcg = 46.70 
2025-02-13 00:05:09.313964: Steps 17/47: batch_recall = 58.60, batch_ndcg = 49.45 
2025-02-13 00:05:10.387389: Steps 18/47: batch_recall = 52.48, batch_ndcg = 48.86 
2025-02-13 00:05:11.452497: Steps 19/47: batch_recall = 61.18, batch_ndcg = 56.06 
2025-02-13 00:05:12.491774: Steps 20/47: batch_recall = 68.95, batch_ndcg = 62.57 
2025-02-13 00:05:13.548381: Steps 21/47: batch_recall = 64.59, batch_ndcg = 56.05 
2025-02-13 00:05:14.599639: Steps 22/47: batch_recall = 55.06, batch_ndcg = 51.16 
2025-02-13 00:05:15.660774: Steps 23/47: batch_recall = 63.32, batch_ndcg = 55.41 
2025-02-13 00:05:16.728725: Steps 24/47: batch_recall = 64.36, batch_ndcg = 57.60 
2025-02-13 00:05:17.777215: Steps 25/47: batch_recall = 66.54, batch_ndcg = 58.48 
2025-02-13 00:05:18.801026: Steps 26/47: batch_recall = 60.31, batch_ndcg = 53.29 
2025-02-13 00:05:19.845904: Steps 27/47: batch_recall = 62.44, batch_ndcg = 52.78 
2025-02-13 00:05:20.869854: Steps 28/47: batch_recall = 69.80, batch_ndcg = 59.16 
2025-02-13 00:05:21.893583: Steps 29/47: batch_recall = 69.49, batch_ndcg = 58.39 
2025-02-13 00:05:22.905320: Steps 30/47: batch_recall = 72.95, batch_ndcg = 64.00 
2025-02-13 00:05:23.931454: Steps 31/47: batch_recall = 67.21, batch_ndcg = 55.90 
2025-02-13 00:05:24.937860: Steps 32/47: batch_recall = 71.55, batch_ndcg = 65.21 
2025-02-13 00:05:25.936054: Steps 33/47: batch_recall = 80.52, batch_ndcg = 69.29 
2025-02-13 00:05:26.955327: Steps 34/47: batch_recall = 69.79, batch_ndcg = 57.00 
2025-02-13 00:05:27.933155: Steps 35/47: batch_recall = 78.61, batch_ndcg = 66.36 
2025-02-13 00:05:28.921679: Steps 36/47: batch_recall = 79.13, batch_ndcg = 64.89 
2025-02-13 00:05:29.893838: Steps 37/47: batch_recall = 84.91, batch_ndcg = 72.34 
2025-02-13 00:05:30.862179: Steps 38/47: batch_recall = 90.27, batch_ndcg = 72.12 
2025-02-13 00:05:31.841781: Steps 39/47: batch_recall = 85.93, batch_ndcg = 68.15 
2025-02-13 00:05:32.801128: Steps 40/47: batch_recall = 72.24, batch_ndcg = 62.66 
2025-02-13 00:05:33.751584: Steps 41/47: batch_recall = 87.94, batch_ndcg = 71.80 
2025-02-13 00:05:34.715036: Steps 42/47: batch_recall = 82.84, batch_ndcg = 64.99 
2025-02-13 00:05:35.684939: Steps 43/47: batch_recall = 89.30, batch_ndcg = 72.14 
2025-02-13 00:05:36.644026: Steps 44/47: batch_recall = 87.14, batch_ndcg = 70.10 
2025-02-13 00:05:37.585459: Steps 45/47: batch_recall = 94.75, batch_ndcg = 76.01 
2025-02-13 00:05:37.691266: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.59 
2025-02-13 00:05:37.691360: Epoch 63/1000, Test: Recall = 0.1219, NDCG = 0.1096  

2025-02-13 00:05:38.907329: Training Step 0/59: batchLoss = 2.9951, diffLoss = 14.5930, kgLoss = 0.0956
2025-02-13 00:05:39.846357: Training Step 1/59: batchLoss = 2.6033, diffLoss = 12.7348, kgLoss = 0.0704
2025-02-13 00:05:40.783904: Training Step 2/59: batchLoss = 2.8052, diffLoss = 13.6894, kgLoss = 0.0841
2025-02-13 00:05:41.716505: Training Step 3/59: batchLoss = 2.9743, diffLoss = 14.5229, kgLoss = 0.0872
2025-02-13 00:05:42.641828: Training Step 4/59: batchLoss = 2.8281, diffLoss = 13.7838, kgLoss = 0.0892
2025-02-13 00:05:43.564523: Training Step 5/59: batchLoss = 2.8041, diffLoss = 13.6933, kgLoss = 0.0818
2025-02-13 00:05:44.489525: Training Step 6/59: batchLoss = 3.1311, diffLoss = 15.2520, kgLoss = 0.1008
2025-02-13 00:05:45.409142: Training Step 7/59: batchLoss = 3.1561, diffLoss = 15.4115, kgLoss = 0.0922
2025-02-13 00:05:46.330029: Training Step 8/59: batchLoss = 2.7912, diffLoss = 13.6118, kgLoss = 0.0860
2025-02-13 00:05:47.252487: Training Step 9/59: batchLoss = 2.6893, diffLoss = 13.1319, kgLoss = 0.0786
2025-02-13 00:05:48.175643: Training Step 10/59: batchLoss = 2.8398, diffLoss = 13.8918, kgLoss = 0.0769
2025-02-13 00:05:49.104175: Training Step 11/59: batchLoss = 2.5358, diffLoss = 12.3810, kgLoss = 0.0745
2025-02-13 00:05:50.026949: Training Step 12/59: batchLoss = 3.0812, diffLoss = 15.0335, kgLoss = 0.0931
2025-02-13 00:05:50.953524: Training Step 13/59: batchLoss = 2.7440, diffLoss = 13.3946, kgLoss = 0.0814
2025-02-13 00:05:51.874593: Training Step 14/59: batchLoss = 3.1202, diffLoss = 15.2406, kgLoss = 0.0901
2025-02-13 00:05:52.801508: Training Step 15/59: batchLoss = 2.6652, diffLoss = 13.0031, kgLoss = 0.0807
2025-02-13 00:05:53.725303: Training Step 16/59: batchLoss = 2.8439, diffLoss = 13.8494, kgLoss = 0.0926
2025-02-13 00:05:54.649508: Training Step 17/59: batchLoss = 2.9325, diffLoss = 14.3059, kgLoss = 0.0891
2025-02-13 00:05:55.577099: Training Step 18/59: batchLoss = 3.1929, diffLoss = 15.5755, kgLoss = 0.0972
2025-02-13 00:05:56.507278: Training Step 19/59: batchLoss = 3.2270, diffLoss = 15.7603, kgLoss = 0.0937
2025-02-13 00:05:57.432014: Training Step 20/59: batchLoss = 2.6104, diffLoss = 12.7346, kgLoss = 0.0794
2025-02-13 00:05:58.363802: Training Step 21/59: batchLoss = 2.8496, diffLoss = 13.9425, kgLoss = 0.0764
2025-02-13 00:05:59.288496: Training Step 22/59: batchLoss = 2.8903, diffLoss = 14.1248, kgLoss = 0.0817
2025-02-13 00:06:00.209494: Training Step 23/59: batchLoss = 3.0677, diffLoss = 14.9766, kgLoss = 0.0904
2025-02-13 00:06:01.136129: Training Step 24/59: batchLoss = 2.8517, diffLoss = 13.9489, kgLoss = 0.0774
2025-02-13 00:06:02.055173: Training Step 25/59: batchLoss = 3.2093, diffLoss = 15.6745, kgLoss = 0.0930
2025-02-13 00:06:02.979597: Training Step 26/59: batchLoss = 3.0273, diffLoss = 14.7677, kgLoss = 0.0922
2025-02-13 00:06:03.912241: Training Step 27/59: batchLoss = 2.9529, diffLoss = 14.4219, kgLoss = 0.0857
2025-02-13 00:06:04.820981: Training Step 28/59: batchLoss = 3.1323, diffLoss = 15.2841, kgLoss = 0.0944
2025-02-13 00:06:05.742654: Training Step 29/59: batchLoss = 3.1781, diffLoss = 15.5232, kgLoss = 0.0918
2025-02-13 00:06:06.654834: Training Step 30/59: batchLoss = 2.9558, diffLoss = 14.4555, kgLoss = 0.0809
2025-02-13 00:06:07.569970: Training Step 31/59: batchLoss = 2.9223, diffLoss = 14.2861, kgLoss = 0.0814
2025-02-13 00:06:08.490019: Training Step 32/59: batchLoss = 2.7861, diffLoss = 13.6117, kgLoss = 0.0797
2025-02-13 00:06:09.411436: Training Step 33/59: batchLoss = 2.9703, diffLoss = 14.5036, kgLoss = 0.0870
2025-02-13 00:06:10.337796: Training Step 34/59: batchLoss = 2.9407, diffLoss = 14.3792, kgLoss = 0.0811
2025-02-13 00:06:11.249857: Training Step 35/59: batchLoss = 2.7399, diffLoss = 13.3781, kgLoss = 0.0803
2025-02-13 00:06:12.170647: Training Step 36/59: batchLoss = 3.2450, diffLoss = 15.8579, kgLoss = 0.0918
2025-02-13 00:06:13.083605: Training Step 37/59: batchLoss = 2.9841, diffLoss = 14.5815, kgLoss = 0.0847
2025-02-13 00:06:14.003790: Training Step 38/59: batchLoss = 2.9840, diffLoss = 14.5800, kgLoss = 0.0851
2025-02-13 00:06:14.920229: Training Step 39/59: batchLoss = 2.7015, diffLoss = 13.2035, kgLoss = 0.0760
2025-02-13 00:06:15.844205: Training Step 40/59: batchLoss = 2.9618, diffLoss = 14.4762, kgLoss = 0.0832
2025-02-13 00:06:16.780025: Training Step 41/59: batchLoss = 3.0999, diffLoss = 15.1488, kgLoss = 0.0876
2025-02-13 00:06:17.712570: Training Step 42/59: batchLoss = 2.8642, diffLoss = 13.9935, kgLoss = 0.0818
2025-02-13 00:06:18.650602: Training Step 43/59: batchLoss = 3.1639, diffLoss = 15.4790, kgLoss = 0.0852
2025-02-13 00:06:19.579804: Training Step 44/59: batchLoss = 3.0316, diffLoss = 14.8070, kgLoss = 0.0878
2025-02-13 00:06:20.516473: Training Step 45/59: batchLoss = 3.4314, diffLoss = 16.7603, kgLoss = 0.0992
2025-02-13 00:06:21.442801: Training Step 46/59: batchLoss = 3.0810, diffLoss = 15.0454, kgLoss = 0.0899
2025-02-13 00:06:22.381215: Training Step 47/59: batchLoss = 2.9143, diffLoss = 14.2643, kgLoss = 0.0768
2025-02-13 00:06:23.314842: Training Step 48/59: batchLoss = 3.0083, diffLoss = 14.6982, kgLoss = 0.0858
2025-02-13 00:06:24.245153: Training Step 49/59: batchLoss = 2.8857, diffLoss = 14.0998, kgLoss = 0.0821
2025-02-13 00:06:25.182623: Training Step 50/59: batchLoss = 2.9313, diffLoss = 14.3355, kgLoss = 0.0803
2025-02-13 00:06:26.113707: Training Step 51/59: batchLoss = 2.9762, diffLoss = 14.5303, kgLoss = 0.0877
2025-02-13 00:06:27.045363: Training Step 52/59: batchLoss = 2.9344, diffLoss = 14.3330, kgLoss = 0.0848
2025-02-13 00:06:27.974961: Training Step 53/59: batchLoss = 3.5061, diffLoss = 17.1492, kgLoss = 0.0953
2025-02-13 00:06:28.900056: Training Step 54/59: batchLoss = 2.9331, diffLoss = 14.3321, kgLoss = 0.0833
2025-02-13 00:06:29.840044: Training Step 55/59: batchLoss = 2.7775, diffLoss = 13.5664, kgLoss = 0.0803
2025-02-13 00:06:30.751822: Training Step 56/59: batchLoss = 2.8869, diffLoss = 14.0869, kgLoss = 0.0868
2025-02-13 00:06:31.599258: Training Step 57/59: batchLoss = 3.0292, diffLoss = 14.8072, kgLoss = 0.0846
2025-02-13 00:06:32.442794: Training Step 58/59: batchLoss = 2.9015, diffLoss = 14.1873, kgLoss = 0.0800
2025-02-13 00:06:32.536338: 
2025-02-13 00:06:32.536671: Epoch 64/1000, Train: epLoss = 0.4357, epDfLoss = 2.1280, epKgLoss = 0.0126  
2025-02-13 00:06:34.009106: Steps 0/47: batch_recall = 37.30, batch_ndcg = 48.12 
2025-02-13 00:06:35.297409: Steps 1/47: batch_recall = 36.67, batch_ndcg = 42.53 
2025-02-13 00:06:36.550546: Steps 2/47: batch_recall = 39.91, batch_ndcg = 46.18 
2025-02-13 00:06:37.814873: Steps 3/47: batch_recall = 43.61, batch_ndcg = 46.30 
2025-02-13 00:06:39.024707: Steps 4/47: batch_recall = 39.35, batch_ndcg = 45.22 
2025-02-13 00:06:40.259836: Steps 5/47: batch_recall = 32.90, batch_ndcg = 40.32 
2025-02-13 00:06:41.474122: Steps 6/47: batch_recall = 39.27, batch_ndcg = 42.46 
2025-02-13 00:06:42.659314: Steps 7/47: batch_recall = 43.16, batch_ndcg = 45.00 
2025-02-13 00:06:43.850539: Steps 8/47: batch_recall = 47.91, batch_ndcg = 52.39 
2025-02-13 00:06:44.992631: Steps 9/47: batch_recall = 46.83, batch_ndcg = 46.83 
2025-02-13 00:06:46.168976: Steps 10/47: batch_recall = 44.20, batch_ndcg = 43.43 
2025-02-13 00:06:47.299044: Steps 11/47: batch_recall = 54.20, batch_ndcg = 52.37 
2025-02-13 00:06:48.433854: Steps 12/47: batch_recall = 49.43, batch_ndcg = 48.45 
2025-02-13 00:06:49.557445: Steps 13/47: batch_recall = 48.66, batch_ndcg = 45.98 
2025-02-13 00:06:50.655649: Steps 14/47: batch_recall = 42.46, batch_ndcg = 41.84 
2025-02-13 00:06:51.742518: Steps 15/47: batch_recall = 57.64, batch_ndcg = 53.95 
2025-02-13 00:06:52.811627: Steps 16/47: batch_recall = 50.55, batch_ndcg = 46.39 
2025-02-13 00:06:53.865471: Steps 17/47: batch_recall = 58.11, batch_ndcg = 48.93 
2025-02-13 00:06:54.938357: Steps 18/47: batch_recall = 51.96, batch_ndcg = 48.52 
2025-02-13 00:06:56.004968: Steps 19/47: batch_recall = 60.81, batch_ndcg = 56.09 
2025-02-13 00:06:57.072991: Steps 20/47: batch_recall = 68.31, batch_ndcg = 62.87 
2025-02-13 00:06:58.119196: Steps 21/47: batch_recall = 64.18, batch_ndcg = 55.38 
2025-02-13 00:06:59.188105: Steps 22/47: batch_recall = 54.43, batch_ndcg = 50.74 
2025-02-13 00:07:00.236807: Steps 23/47: batch_recall = 64.73, batch_ndcg = 55.29 
2025-02-13 00:07:01.295223: Steps 24/47: batch_recall = 64.76, batch_ndcg = 57.95 
2025-02-13 00:07:02.347804: Steps 25/47: batch_recall = 66.82, batch_ndcg = 58.16 
2025-02-13 00:07:03.380589: Steps 26/47: batch_recall = 59.96, batch_ndcg = 53.02 
2025-02-13 00:07:04.416313: Steps 27/47: batch_recall = 62.13, batch_ndcg = 53.44 
2025-02-13 00:07:05.448871: Steps 28/47: batch_recall = 70.18, batch_ndcg = 59.27 
2025-02-13 00:07:06.478930: Steps 29/47: batch_recall = 69.32, batch_ndcg = 57.78 
2025-02-13 00:07:07.504466: Steps 30/47: batch_recall = 72.66, batch_ndcg = 64.50 
2025-02-13 00:07:08.531277: Steps 31/47: batch_recall = 66.24, batch_ndcg = 56.26 
2025-02-13 00:07:09.543517: Steps 32/47: batch_recall = 70.02, batch_ndcg = 64.52 
2025-02-13 00:07:10.560186: Steps 33/47: batch_recall = 80.15, batch_ndcg = 68.91 
2025-02-13 00:07:11.566630: Steps 34/47: batch_recall = 70.15, batch_ndcg = 56.88 
2025-02-13 00:07:12.548891: Steps 35/47: batch_recall = 77.09, batch_ndcg = 65.03 
2025-02-13 00:07:13.525147: Steps 36/47: batch_recall = 77.99, batch_ndcg = 64.56 
2025-02-13 00:07:14.484871: Steps 37/47: batch_recall = 84.99, batch_ndcg = 73.19 
2025-02-13 00:07:15.453161: Steps 38/47: batch_recall = 88.61, batch_ndcg = 71.57 
2025-02-13 00:07:16.420997: Steps 39/47: batch_recall = 88.82, batch_ndcg = 69.45 
2025-02-13 00:07:17.385485: Steps 40/47: batch_recall = 72.38, batch_ndcg = 62.57 
2025-02-13 00:07:18.348420: Steps 41/47: batch_recall = 87.42, batch_ndcg = 73.17 
2025-02-13 00:07:19.317858: Steps 42/47: batch_recall = 83.68, batch_ndcg = 65.18 
2025-02-13 00:07:20.290902: Steps 43/47: batch_recall = 89.05, batch_ndcg = 71.39 
2025-02-13 00:07:21.264868: Steps 44/47: batch_recall = 86.82, batch_ndcg = 69.88 
2025-02-13 00:07:22.272633: Steps 45/47: batch_recall = 91.39, batch_ndcg = 74.13 
2025-02-13 00:07:22.376152: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.62 
2025-02-13 00:07:22.376277: Epoch 64/1000, Test: Recall = 0.1213, NDCG = 0.1094  

2025-02-13 00:07:23.549975: Training Step 0/59: batchLoss = 3.0674, diffLoss = 14.9593, kgLoss = 0.0944
2025-02-13 00:07:24.481078: Training Step 1/59: batchLoss = 3.1571, diffLoss = 15.4114, kgLoss = 0.0936
2025-02-13 00:07:25.413104: Training Step 2/59: batchLoss = 2.8610, diffLoss = 13.9645, kgLoss = 0.0851
2025-02-13 00:07:26.344890: Training Step 3/59: batchLoss = 2.8651, diffLoss = 13.9912, kgLoss = 0.0836
2025-02-13 00:07:27.268270: Training Step 4/59: batchLoss = 2.8444, diffLoss = 13.8844, kgLoss = 0.0844
2025-02-13 00:07:28.195732: Training Step 5/59: batchLoss = 2.7024, diffLoss = 13.2221, kgLoss = 0.0725
2025-02-13 00:07:29.123401: Training Step 6/59: batchLoss = 2.6469, diffLoss = 12.9157, kgLoss = 0.0797
2025-02-13 00:07:30.049350: Training Step 7/59: batchLoss = 3.0698, diffLoss = 15.0077, kgLoss = 0.0854
2025-02-13 00:07:30.971602: Training Step 8/59: batchLoss = 2.7728, diffLoss = 13.5266, kgLoss = 0.0844
2025-02-13 00:07:31.894184: Training Step 9/59: batchLoss = 2.9776, diffLoss = 14.5204, kgLoss = 0.0919
2025-02-13 00:07:32.819877: Training Step 10/59: batchLoss = 2.8317, diffLoss = 13.8199, kgLoss = 0.0846
2025-02-13 00:07:33.739182: Training Step 11/59: batchLoss = 2.6560, diffLoss = 12.9431, kgLoss = 0.0842
2025-02-13 00:07:34.664130: Training Step 12/59: batchLoss = 2.9161, diffLoss = 14.2465, kgLoss = 0.0835
2025-02-13 00:07:35.582783: Training Step 13/59: batchLoss = 2.9755, diffLoss = 14.5074, kgLoss = 0.0925
2025-02-13 00:07:36.500296: Training Step 14/59: batchLoss = 2.6981, diffLoss = 13.1770, kgLoss = 0.0783
2025-02-13 00:07:37.424921: Training Step 15/59: batchLoss = 2.6992, diffLoss = 13.1784, kgLoss = 0.0794
2025-02-13 00:07:38.342970: Training Step 16/59: batchLoss = 2.6840, diffLoss = 13.1073, kgLoss = 0.0781
2025-02-13 00:07:39.268648: Training Step 17/59: batchLoss = 2.8373, diffLoss = 13.8796, kgLoss = 0.0767
2025-02-13 00:07:40.197549: Training Step 18/59: batchLoss = 2.8466, diffLoss = 13.8966, kgLoss = 0.0841
2025-02-13 00:07:41.127140: Training Step 19/59: batchLoss = 2.8898, diffLoss = 14.1096, kgLoss = 0.0848
2025-02-13 00:07:42.060319: Training Step 20/59: batchLoss = 2.9073, diffLoss = 14.1774, kgLoss = 0.0897
2025-02-13 00:07:42.989384: Training Step 21/59: batchLoss = 2.8959, diffLoss = 14.1606, kgLoss = 0.0798
2025-02-13 00:07:43.924250: Training Step 22/59: batchLoss = 2.9642, diffLoss = 14.4939, kgLoss = 0.0817
2025-02-13 00:07:44.854795: Training Step 23/59: batchLoss = 2.7475, diffLoss = 13.4313, kgLoss = 0.0766
2025-02-13 00:07:45.788511: Training Step 24/59: batchLoss = 2.8700, diffLoss = 14.0020, kgLoss = 0.0870
2025-02-13 00:07:46.719799: Training Step 25/59: batchLoss = 2.7684, diffLoss = 13.5031, kgLoss = 0.0847
2025-02-13 00:07:47.653195: Training Step 26/59: batchLoss = 2.9251, diffLoss = 14.2300, kgLoss = 0.0988
2025-02-13 00:07:48.587548: Training Step 27/59: batchLoss = 3.0787, diffLoss = 15.0137, kgLoss = 0.0950
2025-02-13 00:07:49.528304: Training Step 28/59: batchLoss = 2.8513, diffLoss = 13.9283, kgLoss = 0.0821
2025-02-13 00:07:50.459403: Training Step 29/59: batchLoss = 2.8410, diffLoss = 13.8695, kgLoss = 0.0839
2025-02-13 00:07:51.384187: Training Step 30/59: batchLoss = 2.9095, diffLoss = 14.2230, kgLoss = 0.0811
2025-02-13 00:07:52.313336: Training Step 31/59: batchLoss = 2.9610, diffLoss = 14.4699, kgLoss = 0.0838
2025-02-13 00:07:53.233829: Training Step 32/59: batchLoss = 2.8085, diffLoss = 13.7230, kgLoss = 0.0799
2025-02-13 00:07:54.157431: Training Step 33/59: batchLoss = 2.9917, diffLoss = 14.6089, kgLoss = 0.0874
2025-02-13 00:07:55.077352: Training Step 34/59: batchLoss = 2.8713, diffLoss = 14.0234, kgLoss = 0.0832
2025-02-13 00:07:55.993949: Training Step 35/59: batchLoss = 2.8612, diffLoss = 13.9734, kgLoss = 0.0831
2025-02-13 00:07:56.932183: Training Step 36/59: batchLoss = 2.7702, diffLoss = 13.5340, kgLoss = 0.0793
2025-02-13 00:07:57.851198: Training Step 37/59: batchLoss = 2.9361, diffLoss = 14.3405, kgLoss = 0.0850
2025-02-13 00:07:58.777796: Training Step 38/59: batchLoss = 3.0423, diffLoss = 14.8687, kgLoss = 0.0857
2025-02-13 00:07:59.705048: Training Step 39/59: batchLoss = 3.1630, diffLoss = 15.4429, kgLoss = 0.0930
2025-02-13 00:08:00.641142: Training Step 40/59: batchLoss = 3.2406, diffLoss = 15.8383, kgLoss = 0.0912
2025-02-13 00:08:01.574650: Training Step 41/59: batchLoss = 2.6933, diffLoss = 13.1498, kgLoss = 0.0792
2025-02-13 00:08:02.512280: Training Step 42/59: batchLoss = 3.1786, diffLoss = 15.5472, kgLoss = 0.0864
2025-02-13 00:08:03.455487: Training Step 43/59: batchLoss = 3.0051, diffLoss = 14.6138, kgLoss = 0.1029
2025-02-13 00:08:04.392324: Training Step 44/59: batchLoss = 2.9633, diffLoss = 14.4681, kgLoss = 0.0871
2025-02-13 00:08:05.333311: Training Step 45/59: batchLoss = 2.8540, diffLoss = 13.9505, kgLoss = 0.0799
2025-02-13 00:08:06.264421: Training Step 46/59: batchLoss = 3.0448, diffLoss = 14.8953, kgLoss = 0.0822
2025-02-13 00:08:07.197016: Training Step 47/59: batchLoss = 3.2642, diffLoss = 15.9514, kgLoss = 0.0924
2025-02-13 00:08:08.128027: Training Step 48/59: batchLoss = 2.8301, diffLoss = 13.7935, kgLoss = 0.0892
2025-02-13 00:08:09.057894: Training Step 49/59: batchLoss = 2.9664, diffLoss = 14.5163, kgLoss = 0.0789
2025-02-13 00:08:09.990559: Training Step 50/59: batchLoss = 3.1839, diffLoss = 15.5687, kgLoss = 0.0877
2025-02-13 00:08:10.913878: Training Step 51/59: batchLoss = 2.9044, diffLoss = 14.1827, kgLoss = 0.0848
2025-02-13 00:08:11.841831: Training Step 52/59: batchLoss = 2.9979, diffLoss = 14.6392, kgLoss = 0.0876
2025-02-13 00:08:12.759829: Training Step 53/59: batchLoss = 3.1708, diffLoss = 15.4748, kgLoss = 0.0949
2025-02-13 00:08:13.679775: Training Step 54/59: batchLoss = 2.9270, diffLoss = 14.3027, kgLoss = 0.0830
2025-02-13 00:08:14.607128: Training Step 55/59: batchLoss = 3.0359, diffLoss = 14.8170, kgLoss = 0.0906
2025-02-13 00:08:15.533500: Training Step 56/59: batchLoss = 3.3888, diffLoss = 16.5743, kgLoss = 0.0925
2025-02-13 00:08:16.380068: Training Step 57/59: batchLoss = 2.6749, diffLoss = 13.0376, kgLoss = 0.0842
2025-02-13 00:08:17.228485: Training Step 58/59: batchLoss = 2.9747, diffLoss = 14.5071, kgLoss = 0.0916
2025-02-13 00:08:17.323921: 
2025-02-13 00:08:17.324256: Epoch 65/1000, Train: epLoss = 0.4312, epDfLoss = 2.1053, epKgLoss = 0.0126  
2025-02-13 00:08:18.797619: Steps 0/47: batch_recall = 36.76, batch_ndcg = 47.11 
2025-02-13 00:08:20.082020: Steps 1/47: batch_recall = 37.27, batch_ndcg = 42.18 
2025-02-13 00:08:21.317561: Steps 2/47: batch_recall = 41.48, batch_ndcg = 46.32 
2025-02-13 00:08:22.571957: Steps 3/47: batch_recall = 44.07, batch_ndcg = 45.68 
2025-02-13 00:08:23.777493: Steps 4/47: batch_recall = 39.84, batch_ndcg = 45.40 
2025-02-13 00:08:24.995372: Steps 5/47: batch_recall = 33.45, batch_ndcg = 39.82 
2025-02-13 00:08:26.186852: Steps 6/47: batch_recall = 39.28, batch_ndcg = 42.14 
2025-02-13 00:08:27.359399: Steps 7/47: batch_recall = 43.12, batch_ndcg = 44.71 
2025-02-13 00:08:28.533311: Steps 8/47: batch_recall = 48.09, batch_ndcg = 53.23 
2025-02-13 00:08:29.671809: Steps 9/47: batch_recall = 46.75, batch_ndcg = 46.68 
2025-02-13 00:08:30.838727: Steps 10/47: batch_recall = 43.43, batch_ndcg = 43.01 
2025-02-13 00:08:31.969672: Steps 11/47: batch_recall = 53.24, batch_ndcg = 51.90 
2025-02-13 00:08:33.112997: Steps 12/47: batch_recall = 49.99, batch_ndcg = 48.24 
2025-02-13 00:08:34.240723: Steps 13/47: batch_recall = 49.34, batch_ndcg = 46.26 
2025-02-13 00:08:35.321777: Steps 14/47: batch_recall = 42.71, batch_ndcg = 41.91 
2025-02-13 00:08:36.403171: Steps 15/47: batch_recall = 56.59, batch_ndcg = 53.13 
2025-02-13 00:08:37.478974: Steps 16/47: batch_recall = 51.24, batch_ndcg = 47.02 
2025-02-13 00:08:38.524641: Steps 17/47: batch_recall = 58.62, batch_ndcg = 50.04 
2025-02-13 00:08:39.594508: Steps 18/47: batch_recall = 52.70, batch_ndcg = 48.44 
2025-02-13 00:08:40.652022: Steps 19/47: batch_recall = 60.87, batch_ndcg = 55.39 
2025-02-13 00:08:41.685266: Steps 20/47: batch_recall = 68.96, batch_ndcg = 62.66 
2025-02-13 00:08:42.722241: Steps 21/47: batch_recall = 65.34, batch_ndcg = 56.21 
2025-02-13 00:08:43.770849: Steps 22/47: batch_recall = 55.11, batch_ndcg = 51.23 
2025-02-13 00:08:44.827612: Steps 23/47: batch_recall = 62.86, batch_ndcg = 55.30 
2025-02-13 00:08:45.890388: Steps 24/47: batch_recall = 65.77, batch_ndcg = 58.18 
2025-02-13 00:08:46.931786: Steps 25/47: batch_recall = 65.80, batch_ndcg = 58.30 
2025-02-13 00:08:47.953917: Steps 26/47: batch_recall = 59.40, batch_ndcg = 53.50 
2025-02-13 00:08:48.994478: Steps 27/47: batch_recall = 63.28, batch_ndcg = 53.70 
2025-02-13 00:08:50.020971: Steps 28/47: batch_recall = 71.58, batch_ndcg = 60.47 
2025-02-13 00:08:51.033584: Steps 29/47: batch_recall = 71.34, batch_ndcg = 58.86 
2025-02-13 00:08:52.047159: Steps 30/47: batch_recall = 73.26, batch_ndcg = 63.76 
2025-02-13 00:08:53.079256: Steps 31/47: batch_recall = 66.69, batch_ndcg = 55.99 
2025-02-13 00:08:54.074871: Steps 32/47: batch_recall = 70.45, batch_ndcg = 64.42 
2025-02-13 00:08:55.090106: Steps 33/47: batch_recall = 80.34, batch_ndcg = 68.32 
2025-02-13 00:08:56.096561: Steps 34/47: batch_recall = 67.99, batch_ndcg = 55.96 
2025-02-13 00:08:57.074887: Steps 35/47: batch_recall = 76.95, batch_ndcg = 64.11 
2025-02-13 00:08:58.063500: Steps 36/47: batch_recall = 79.71, batch_ndcg = 64.66 
2025-02-13 00:08:59.034690: Steps 37/47: batch_recall = 84.28, batch_ndcg = 72.76 
2025-02-13 00:09:00.007279: Steps 38/47: batch_recall = 90.49, batch_ndcg = 72.65 
2025-02-13 00:09:00.964659: Steps 39/47: batch_recall = 87.94, batch_ndcg = 69.62 
2025-02-13 00:09:01.932330: Steps 40/47: batch_recall = 70.38, batch_ndcg = 61.72 
2025-02-13 00:09:02.890605: Steps 41/47: batch_recall = 86.03, batch_ndcg = 71.78 
2025-02-13 00:09:03.855321: Steps 42/47: batch_recall = 82.51, batch_ndcg = 64.31 
2025-02-13 00:09:04.831966: Steps 43/47: batch_recall = 91.75, batch_ndcg = 73.47 
2025-02-13 00:09:05.808333: Steps 44/47: batch_recall = 85.08, batch_ndcg = 68.90 
2025-02-13 00:09:06.749494: Steps 45/47: batch_recall = 94.17, batch_ndcg = 73.97 
2025-02-13 00:09:06.854205: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.52 
2025-02-13 00:09:06.854332: Epoch 65/1000, Test: Recall = 0.1217, NDCG = 0.1093  

2025-02-13 00:09:08.044291: Training Step 0/59: batchLoss = 2.7184, diffLoss = 13.2652, kgLoss = 0.0818
2025-02-13 00:09:08.967365: Training Step 1/59: batchLoss = 2.6368, diffLoss = 12.8375, kgLoss = 0.0867
2025-02-13 00:09:09.893322: Training Step 2/59: batchLoss = 2.6478, diffLoss = 12.9376, kgLoss = 0.0753
2025-02-13 00:09:10.825647: Training Step 3/59: batchLoss = 3.1089, diffLoss = 15.1827, kgLoss = 0.0904
2025-02-13 00:09:11.763201: Training Step 4/59: batchLoss = 2.8883, diffLoss = 14.0932, kgLoss = 0.0870
2025-02-13 00:09:12.691218: Training Step 5/59: batchLoss = 2.8287, diffLoss = 13.8280, kgLoss = 0.0789
2025-02-13 00:09:13.628448: Training Step 6/59: batchLoss = 2.8742, diffLoss = 14.0398, kgLoss = 0.0828
2025-02-13 00:09:14.551010: Training Step 7/59: batchLoss = 2.9514, diffLoss = 14.3793, kgLoss = 0.0944
2025-02-13 00:09:15.472765: Training Step 8/59: batchLoss = 2.6643, diffLoss = 13.0000, kgLoss = 0.0804
2025-02-13 00:09:16.399385: Training Step 9/59: batchLoss = 2.8937, diffLoss = 14.0837, kgLoss = 0.0962
2025-02-13 00:09:17.317614: Training Step 10/59: batchLoss = 3.1650, diffLoss = 15.4674, kgLoss = 0.0894
2025-02-13 00:09:18.242197: Training Step 11/59: batchLoss = 2.8494, diffLoss = 13.9232, kgLoss = 0.0809
2025-02-13 00:09:19.169739: Training Step 12/59: batchLoss = 2.8731, diffLoss = 14.0058, kgLoss = 0.0899
2025-02-13 00:09:20.090320: Training Step 13/59: batchLoss = 3.1122, diffLoss = 15.1925, kgLoss = 0.0921
2025-02-13 00:09:21.017174: Training Step 14/59: batchLoss = 2.8590, diffLoss = 13.9650, kgLoss = 0.0825
2025-02-13 00:09:21.943535: Training Step 15/59: batchLoss = 2.7174, diffLoss = 13.2785, kgLoss = 0.0771
2025-02-13 00:09:22.875006: Training Step 16/59: batchLoss = 2.7634, diffLoss = 13.4769, kgLoss = 0.0850
2025-02-13 00:09:23.798474: Training Step 17/59: batchLoss = 2.9146, diffLoss = 14.2253, kgLoss = 0.0870
2025-02-13 00:09:24.720176: Training Step 18/59: batchLoss = 2.8327, diffLoss = 13.8189, kgLoss = 0.0862
2025-02-13 00:09:25.654337: Training Step 19/59: batchLoss = 2.9203, diffLoss = 14.2660, kgLoss = 0.0839
2025-02-13 00:09:26.585517: Training Step 20/59: batchLoss = 3.0419, diffLoss = 14.8401, kgLoss = 0.0924
2025-02-13 00:09:27.521164: Training Step 21/59: batchLoss = 3.0126, diffLoss = 14.7167, kgLoss = 0.0866
2025-02-13 00:09:28.444943: Training Step 22/59: batchLoss = 2.8388, diffLoss = 13.8441, kgLoss = 0.0875
2025-02-13 00:09:29.374897: Training Step 23/59: batchLoss = 3.0917, diffLoss = 15.0903, kgLoss = 0.0920
2025-02-13 00:09:30.303226: Training Step 24/59: batchLoss = 2.7212, diffLoss = 13.2567, kgLoss = 0.0874
2025-02-13 00:09:31.240047: Training Step 25/59: batchLoss = 2.9090, diffLoss = 14.2094, kgLoss = 0.0839
2025-02-13 00:09:32.175188: Training Step 26/59: batchLoss = 2.8785, diffLoss = 14.0553, kgLoss = 0.0843
2025-02-13 00:09:33.105920: Training Step 27/59: batchLoss = 2.8850, diffLoss = 14.1058, kgLoss = 0.0797
2025-02-13 00:09:34.041683: Training Step 28/59: batchLoss = 3.1702, diffLoss = 15.4536, kgLoss = 0.0994
2025-02-13 00:09:34.964402: Training Step 29/59: batchLoss = 2.8944, diffLoss = 14.1344, kgLoss = 0.0844
2025-02-13 00:09:35.878660: Training Step 30/59: batchLoss = 2.6258, diffLoss = 12.8370, kgLoss = 0.0730
2025-02-13 00:09:36.805502: Training Step 31/59: batchLoss = 2.9176, diffLoss = 14.2691, kgLoss = 0.0797
2025-02-13 00:09:37.725195: Training Step 32/59: batchLoss = 2.9330, diffLoss = 14.3415, kgLoss = 0.0809
2025-02-13 00:09:38.649757: Training Step 33/59: batchLoss = 2.7717, diffLoss = 13.5598, kgLoss = 0.0747
2025-02-13 00:09:39.583238: Training Step 34/59: batchLoss = 2.6950, diffLoss = 13.1708, kgLoss = 0.0761
2025-02-13 00:09:40.503638: Training Step 35/59: batchLoss = 3.0216, diffLoss = 14.7679, kgLoss = 0.0851
2025-02-13 00:09:41.436317: Training Step 36/59: batchLoss = 2.7455, diffLoss = 13.3678, kgLoss = 0.0899
2025-02-13 00:09:42.358262: Training Step 37/59: batchLoss = 3.1666, diffLoss = 15.4712, kgLoss = 0.0904
2025-02-13 00:09:43.284111: Training Step 38/59: batchLoss = 2.8963, diffLoss = 14.1731, kgLoss = 0.0772
2025-02-13 00:09:44.202022: Training Step 39/59: batchLoss = 3.1865, diffLoss = 15.5176, kgLoss = 0.1037
2025-02-13 00:09:45.124959: Training Step 40/59: batchLoss = 3.2514, diffLoss = 15.8750, kgLoss = 0.0955
2025-02-13 00:09:46.050421: Training Step 41/59: batchLoss = 3.2679, diffLoss = 15.9610, kgLoss = 0.0946
2025-02-13 00:09:46.976950: Training Step 42/59: batchLoss = 2.9917, diffLoss = 14.6181, kgLoss = 0.0850
2025-02-13 00:09:47.914996: Training Step 43/59: batchLoss = 2.9077, diffLoss = 14.2142, kgLoss = 0.0811
2025-02-13 00:09:48.866575: Training Step 44/59: batchLoss = 2.8354, diffLoss = 13.8329, kgLoss = 0.0861
2025-02-13 00:09:49.801641: Training Step 45/59: batchLoss = 2.7750, diffLoss = 13.5527, kgLoss = 0.0805
2025-02-13 00:09:50.743580: Training Step 46/59: batchLoss = 3.1621, diffLoss = 15.4326, kgLoss = 0.0944
2025-02-13 00:09:51.680623: Training Step 47/59: batchLoss = 2.9839, diffLoss = 14.5984, kgLoss = 0.0803
2025-02-13 00:09:52.619523: Training Step 48/59: batchLoss = 2.8001, diffLoss = 13.6829, kgLoss = 0.0793
2025-02-13 00:09:53.552353: Training Step 49/59: batchLoss = 2.9437, diffLoss = 14.3834, kgLoss = 0.0838
2025-02-13 00:09:54.485863: Training Step 50/59: batchLoss = 3.3397, diffLoss = 16.3258, kgLoss = 0.0931
2025-02-13 00:09:55.407209: Training Step 51/59: batchLoss = 2.7845, diffLoss = 13.5983, kgLoss = 0.0811
2025-02-13 00:09:56.341705: Training Step 52/59: batchLoss = 3.3908, diffLoss = 16.5896, kgLoss = 0.0911
2025-02-13 00:09:57.264796: Training Step 53/59: batchLoss = 3.2077, diffLoss = 15.6702, kgLoss = 0.0920
2025-02-13 00:09:58.192843: Training Step 54/59: batchLoss = 2.7232, diffLoss = 13.2970, kgLoss = 0.0798
2025-02-13 00:09:59.108096: Training Step 55/59: batchLoss = 3.1432, diffLoss = 15.3570, kgLoss = 0.0898
2025-02-13 00:10:00.019588: Training Step 56/59: batchLoss = 3.0045, diffLoss = 14.6945, kgLoss = 0.0819
2025-02-13 00:10:00.866056: Training Step 57/59: batchLoss = 3.0687, diffLoss = 15.0193, kgLoss = 0.0811
2025-02-13 00:10:01.718402: Training Step 58/59: batchLoss = 2.8551, diffLoss = 13.9461, kgLoss = 0.0824
2025-02-13 00:10:01.810884: 
2025-02-13 00:10:01.811229: Epoch 66/1000, Train: epLoss = 0.4326, epDfLoss = 2.1127, epKgLoss = 0.0126  
2025-02-13 00:10:03.283171: Steps 0/47: batch_recall = 37.49, batch_ndcg = 47.55 
2025-02-13 00:10:04.574422: Steps 1/47: batch_recall = 36.42, batch_ndcg = 42.09 
2025-02-13 00:10:05.833518: Steps 2/47: batch_recall = 41.05, batch_ndcg = 46.61 
2025-02-13 00:10:07.098380: Steps 3/47: batch_recall = 43.73, batch_ndcg = 46.78 
2025-02-13 00:10:08.312598: Steps 4/47: batch_recall = 39.45, batch_ndcg = 45.75 
2025-02-13 00:10:09.545007: Steps 5/47: batch_recall = 33.40, batch_ndcg = 40.09 
2025-02-13 00:10:10.754253: Steps 6/47: batch_recall = 39.92, batch_ndcg = 42.69 
2025-02-13 00:10:11.946574: Steps 7/47: batch_recall = 44.11, batch_ndcg = 46.10 
2025-02-13 00:10:13.132152: Steps 8/47: batch_recall = 47.86, batch_ndcg = 51.95 
2025-02-13 00:10:14.279514: Steps 9/47: batch_recall = 47.86, batch_ndcg = 46.82 
2025-02-13 00:10:15.456911: Steps 10/47: batch_recall = 44.34, batch_ndcg = 44.14 
2025-02-13 00:10:16.599227: Steps 11/47: batch_recall = 53.80, batch_ndcg = 52.27 
2025-02-13 00:10:17.748720: Steps 12/47: batch_recall = 49.23, batch_ndcg = 48.44 
2025-02-13 00:10:18.885204: Steps 13/47: batch_recall = 49.48, batch_ndcg = 45.71 
2025-02-13 00:10:19.973747: Steps 14/47: batch_recall = 41.22, batch_ndcg = 41.41 
2025-02-13 00:10:21.057493: Steps 15/47: batch_recall = 56.62, batch_ndcg = 54.39 
2025-02-13 00:10:22.140041: Steps 16/47: batch_recall = 50.17, batch_ndcg = 47.39 
2025-02-13 00:10:23.192331: Steps 17/47: batch_recall = 58.17, batch_ndcg = 49.87 
2025-02-13 00:10:24.267698: Steps 18/47: batch_recall = 51.95, batch_ndcg = 48.85 
2025-02-13 00:10:25.329329: Steps 19/47: batch_recall = 61.39, batch_ndcg = 56.24 
2025-02-13 00:10:26.374360: Steps 20/47: batch_recall = 68.46, batch_ndcg = 61.89 
2025-02-13 00:10:27.417846: Steps 21/47: batch_recall = 63.84, batch_ndcg = 54.87 
2025-02-13 00:10:28.476369: Steps 22/47: batch_recall = 55.10, batch_ndcg = 51.57 
2025-02-13 00:10:29.524057: Steps 23/47: batch_recall = 63.82, batch_ndcg = 55.02 
2025-02-13 00:10:30.593857: Steps 24/47: batch_recall = 65.49, batch_ndcg = 57.70 
2025-02-13 00:10:31.646155: Steps 25/47: batch_recall = 66.45, batch_ndcg = 57.90 
2025-02-13 00:10:32.668200: Steps 26/47: batch_recall = 59.12, batch_ndcg = 52.92 
2025-02-13 00:10:33.703878: Steps 27/47: batch_recall = 62.79, batch_ndcg = 53.67 
2025-02-13 00:10:34.726761: Steps 28/47: batch_recall = 70.66, batch_ndcg = 60.00 
2025-02-13 00:10:35.750585: Steps 29/47: batch_recall = 70.83, batch_ndcg = 58.79 
2025-02-13 00:10:36.776378: Steps 30/47: batch_recall = 73.00, batch_ndcg = 63.83 
2025-02-13 00:10:37.806658: Steps 31/47: batch_recall = 66.46, batch_ndcg = 56.12 
2025-02-13 00:10:38.817877: Steps 32/47: batch_recall = 70.70, batch_ndcg = 64.63 
2025-02-13 00:10:39.831873: Steps 33/47: batch_recall = 80.19, batch_ndcg = 68.83 
2025-02-13 00:10:40.834531: Steps 34/47: batch_recall = 68.09, batch_ndcg = 56.68 
2025-02-13 00:10:41.812467: Steps 35/47: batch_recall = 76.86, batch_ndcg = 65.11 
2025-02-13 00:10:42.791417: Steps 36/47: batch_recall = 76.54, batch_ndcg = 63.23 
2025-02-13 00:10:43.755238: Steps 37/47: batch_recall = 82.77, batch_ndcg = 72.19 
2025-02-13 00:10:44.727135: Steps 38/47: batch_recall = 90.30, batch_ndcg = 73.02 
2025-02-13 00:10:45.686604: Steps 39/47: batch_recall = 88.52, batch_ndcg = 69.50 
2025-02-13 00:10:46.651338: Steps 40/47: batch_recall = 71.93, batch_ndcg = 62.79 
2025-02-13 00:10:47.612145: Steps 41/47: batch_recall = 89.22, batch_ndcg = 73.85 
2025-02-13 00:10:48.573554: Steps 42/47: batch_recall = 81.56, batch_ndcg = 64.72 
2025-02-13 00:10:49.541288: Steps 43/47: batch_recall = 90.54, batch_ndcg = 72.54 
2025-02-13 00:10:50.514548: Steps 44/47: batch_recall = 87.32, batch_ndcg = 69.27 
2025-02-13 00:10:51.461831: Steps 45/47: batch_recall = 94.99, batch_ndcg = 75.09 
2025-02-13 00:10:51.569880: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-13 00:10:51.570015: Epoch 66/1000, Test: Recall = 0.1216, NDCG = 0.1096  

2025-02-13 00:10:52.779769: Training Step 0/59: batchLoss = 2.8346, diffLoss = 13.8470, kgLoss = 0.0814
2025-02-13 00:10:53.726437: Training Step 1/59: batchLoss = 2.8081, diffLoss = 13.7238, kgLoss = 0.0792
2025-02-13 00:10:54.656499: Training Step 2/59: batchLoss = 2.8295, diffLoss = 13.8009, kgLoss = 0.0866
2025-02-13 00:10:55.591101: Training Step 3/59: batchLoss = 2.4749, diffLoss = 12.0801, kgLoss = 0.0736
2025-02-13 00:10:56.519171: Training Step 4/59: batchLoss = 2.7616, diffLoss = 13.4709, kgLoss = 0.0843
2025-02-13 00:10:57.444232: Training Step 5/59: batchLoss = 2.8988, diffLoss = 14.1398, kgLoss = 0.0885
2025-02-13 00:10:58.381799: Training Step 6/59: batchLoss = 2.8454, diffLoss = 13.8582, kgLoss = 0.0922
2025-02-13 00:10:59.311778: Training Step 7/59: batchLoss = 2.8486, diffLoss = 13.8817, kgLoss = 0.0903
2025-02-13 00:11:00.238664: Training Step 8/59: batchLoss = 2.6678, diffLoss = 13.0236, kgLoss = 0.0788
2025-02-13 00:11:01.166315: Training Step 9/59: batchLoss = 2.7067, diffLoss = 13.2140, kgLoss = 0.0799
2025-02-13 00:11:02.091064: Training Step 10/59: batchLoss = 2.9071, diffLoss = 14.1729, kgLoss = 0.0906
2025-02-13 00:11:03.016296: Training Step 11/59: batchLoss = 3.0439, diffLoss = 14.8186, kgLoss = 0.1002
2025-02-13 00:11:03.939843: Training Step 12/59: batchLoss = 2.6131, diffLoss = 12.7839, kgLoss = 0.0704
2025-02-13 00:11:04.866855: Training Step 13/59: batchLoss = 3.0709, diffLoss = 14.9715, kgLoss = 0.0957
2025-02-13 00:11:05.781573: Training Step 14/59: batchLoss = 3.1777, diffLoss = 15.5379, kgLoss = 0.0877
2025-02-13 00:11:06.703317: Training Step 15/59: batchLoss = 2.7747, diffLoss = 13.5570, kgLoss = 0.0791
2025-02-13 00:11:07.624876: Training Step 16/59: batchLoss = 2.9140, diffLoss = 14.2333, kgLoss = 0.0841
2025-02-13 00:11:08.547597: Training Step 17/59: batchLoss = 2.9122, diffLoss = 14.2293, kgLoss = 0.0829
2025-02-13 00:11:09.479363: Training Step 18/59: batchLoss = 2.7514, diffLoss = 13.4400, kgLoss = 0.0793
2025-02-13 00:11:10.417136: Training Step 19/59: batchLoss = 2.9982, diffLoss = 14.6355, kgLoss = 0.0889
2025-02-13 00:11:11.353642: Training Step 20/59: batchLoss = 2.8723, diffLoss = 14.0134, kgLoss = 0.0870
2025-02-13 00:11:12.295562: Training Step 21/59: batchLoss = 3.0979, diffLoss = 15.1608, kgLoss = 0.0822
2025-02-13 00:11:13.231067: Training Step 22/59: batchLoss = 2.9322, diffLoss = 14.3195, kgLoss = 0.0854
2025-02-13 00:11:14.176116: Training Step 23/59: batchLoss = 2.9631, diffLoss = 14.4568, kgLoss = 0.0897
2025-02-13 00:11:15.109307: Training Step 24/59: batchLoss = 3.1798, diffLoss = 15.5320, kgLoss = 0.0917
2025-02-13 00:11:16.037716: Training Step 25/59: batchLoss = 3.0638, diffLoss = 14.9855, kgLoss = 0.0833
2025-02-13 00:11:16.973054: Training Step 26/59: batchLoss = 2.9087, diffLoss = 14.1816, kgLoss = 0.0905
2025-02-13 00:11:17.904164: Training Step 27/59: batchLoss = 2.8739, diffLoss = 14.0556, kgLoss = 0.0784
2025-02-13 00:11:18.840881: Training Step 28/59: batchLoss = 2.8386, diffLoss = 13.8706, kgLoss = 0.0807
2025-02-13 00:11:19.763958: Training Step 29/59: batchLoss = 2.9723, diffLoss = 14.4984, kgLoss = 0.0907
2025-02-13 00:11:20.693810: Training Step 30/59: batchLoss = 3.1216, diffLoss = 15.2576, kgLoss = 0.0876
2025-02-13 00:11:21.620379: Training Step 31/59: batchLoss = 3.3665, diffLoss = 16.4571, kgLoss = 0.0938
2025-02-13 00:11:22.546904: Training Step 32/59: batchLoss = 3.0926, diffLoss = 15.1005, kgLoss = 0.0906
2025-02-13 00:11:23.469188: Training Step 33/59: batchLoss = 2.9649, diffLoss = 14.4563, kgLoss = 0.0921
2025-02-13 00:11:24.387996: Training Step 34/59: batchLoss = 3.0878, diffLoss = 15.0865, kgLoss = 0.0882
2025-02-13 00:11:25.324366: Training Step 35/59: batchLoss = 2.7526, diffLoss = 13.4504, kgLoss = 0.0782
2025-02-13 00:11:26.251136: Training Step 36/59: batchLoss = 2.9223, diffLoss = 14.2926, kgLoss = 0.0798
2025-02-13 00:11:27.176367: Training Step 37/59: batchLoss = 2.8481, diffLoss = 13.9103, kgLoss = 0.0825
2025-02-13 00:11:28.101286: Training Step 38/59: batchLoss = 2.7450, diffLoss = 13.4227, kgLoss = 0.0756
2025-02-13 00:11:29.021259: Training Step 39/59: batchLoss = 2.9780, diffLoss = 14.5245, kgLoss = 0.0914
2025-02-13 00:11:29.948717: Training Step 40/59: batchLoss = 2.9806, diffLoss = 14.5742, kgLoss = 0.0822
2025-02-13 00:11:30.872823: Training Step 41/59: batchLoss = 3.0221, diffLoss = 14.7576, kgLoss = 0.0882
2025-02-13 00:11:31.817863: Training Step 42/59: batchLoss = 2.9415, diffLoss = 14.3605, kgLoss = 0.0867
2025-02-13 00:11:32.753542: Training Step 43/59: batchLoss = 3.1209, diffLoss = 15.2370, kgLoss = 0.0919
2025-02-13 00:11:33.682039: Training Step 44/59: batchLoss = 2.6884, diffLoss = 13.1407, kgLoss = 0.0754
2025-02-13 00:11:34.625926: Training Step 45/59: batchLoss = 2.8550, diffLoss = 13.9563, kgLoss = 0.0797
2025-02-13 00:11:35.561174: Training Step 46/59: batchLoss = 2.8992, diffLoss = 14.1577, kgLoss = 0.0846
2025-02-13 00:11:36.498801: Training Step 47/59: batchLoss = 3.0979, diffLoss = 15.1541, kgLoss = 0.0838
2025-02-13 00:11:37.435601: Training Step 48/59: batchLoss = 3.1585, diffLoss = 15.4253, kgLoss = 0.0918
2025-02-13 00:11:38.371752: Training Step 49/59: batchLoss = 3.0418, diffLoss = 14.8611, kgLoss = 0.0869
2025-02-13 00:11:39.306209: Training Step 50/59: batchLoss = 3.1856, diffLoss = 15.5796, kgLoss = 0.0871
2025-02-13 00:11:40.243921: Training Step 51/59: batchLoss = 2.5957, diffLoss = 12.6405, kgLoss = 0.0846
2025-02-13 00:11:41.178342: Training Step 52/59: batchLoss = 2.6639, diffLoss = 12.9968, kgLoss = 0.0806
2025-02-13 00:11:42.106468: Training Step 53/59: batchLoss = 2.8494, diffLoss = 13.9175, kgLoss = 0.0824
2025-02-13 00:11:43.031068: Training Step 54/59: batchLoss = 3.1544, diffLoss = 15.4228, kgLoss = 0.0873
2025-02-13 00:11:43.952241: Training Step 55/59: batchLoss = 2.9734, diffLoss = 14.5349, kgLoss = 0.0831
2025-02-13 00:11:44.868057: Training Step 56/59: batchLoss = 3.3417, diffLoss = 16.3381, kgLoss = 0.0926
2025-02-13 00:11:45.702909: Training Step 57/59: batchLoss = 3.0664, diffLoss = 14.9852, kgLoss = 0.0867
2025-02-13 00:11:46.551264: Training Step 58/59: batchLoss = 3.4897, diffLoss = 17.0389, kgLoss = 0.1024
2025-02-13 00:11:46.648139: 
2025-02-13 00:11:46.648464: Epoch 67/1000, Train: epLoss = 0.4339, epDfLoss = 2.1188, epKgLoss = 0.0126  
2025-02-13 00:11:48.110678: Steps 0/47: batch_recall = 36.69, batch_ndcg = 47.11 
2025-02-13 00:11:49.396652: Steps 1/47: batch_recall = 36.68, batch_ndcg = 42.54 
2025-02-13 00:11:50.637787: Steps 2/47: batch_recall = 40.46, batch_ndcg = 46.60 
2025-02-13 00:11:51.897856: Steps 3/47: batch_recall = 44.04, batch_ndcg = 45.80 
2025-02-13 00:11:53.098830: Steps 4/47: batch_recall = 38.82, batch_ndcg = 45.50 
2025-02-13 00:11:54.317670: Steps 5/47: batch_recall = 33.79, batch_ndcg = 40.29 
2025-02-13 00:11:55.539516: Steps 6/47: batch_recall = 39.23, batch_ndcg = 42.09 
2025-02-13 00:11:56.718401: Steps 7/47: batch_recall = 42.79, batch_ndcg = 44.75 
2025-02-13 00:11:57.921048: Steps 8/47: batch_recall = 47.87, batch_ndcg = 51.70 
2025-02-13 00:11:59.078348: Steps 9/47: batch_recall = 47.85, batch_ndcg = 47.26 
2025-02-13 00:12:00.246825: Steps 10/47: batch_recall = 43.37, batch_ndcg = 43.29 
2025-02-13 00:12:01.386192: Steps 11/47: batch_recall = 54.99, batch_ndcg = 52.20 
2025-02-13 00:12:02.536699: Steps 12/47: batch_recall = 49.45, batch_ndcg = 48.97 
2025-02-13 00:12:03.673767: Steps 13/47: batch_recall = 49.61, batch_ndcg = 46.27 
2025-02-13 00:12:04.763201: Steps 14/47: batch_recall = 41.16, batch_ndcg = 42.19 
2025-02-13 00:12:05.860083: Steps 15/47: batch_recall = 56.44, batch_ndcg = 54.07 
2025-02-13 00:12:06.942216: Steps 16/47: batch_recall = 50.48, batch_ndcg = 45.70 
2025-02-13 00:12:07.979206: Steps 17/47: batch_recall = 57.30, batch_ndcg = 49.34 
2025-02-13 00:12:09.053044: Steps 18/47: batch_recall = 54.37, batch_ndcg = 49.78 
2025-02-13 00:12:10.118205: Steps 19/47: batch_recall = 60.49, batch_ndcg = 55.48 
2025-02-13 00:12:11.158400: Steps 20/47: batch_recall = 68.90, batch_ndcg = 62.76 
2025-02-13 00:12:12.207010: Steps 21/47: batch_recall = 64.65, batch_ndcg = 55.73 
2025-02-13 00:12:13.269083: Steps 22/47: batch_recall = 55.54, batch_ndcg = 51.51 
2025-02-13 00:12:14.327827: Steps 23/47: batch_recall = 62.84, batch_ndcg = 55.00 
2025-02-13 00:12:15.387238: Steps 24/47: batch_recall = 64.32, batch_ndcg = 57.75 
2025-02-13 00:12:16.423092: Steps 25/47: batch_recall = 65.81, batch_ndcg = 57.54 
2025-02-13 00:12:17.449711: Steps 26/47: batch_recall = 59.93, batch_ndcg = 54.42 
2025-02-13 00:12:18.491103: Steps 27/47: batch_recall = 62.62, batch_ndcg = 53.99 
2025-02-13 00:12:19.516791: Steps 28/47: batch_recall = 70.84, batch_ndcg = 59.71 
2025-02-13 00:12:20.529514: Steps 29/47: batch_recall = 69.95, batch_ndcg = 58.17 
2025-02-13 00:12:21.538688: Steps 30/47: batch_recall = 73.94, batch_ndcg = 64.30 
2025-02-13 00:12:22.551767: Steps 31/47: batch_recall = 68.37, batch_ndcg = 56.39 
2025-02-13 00:12:23.550104: Steps 32/47: batch_recall = 70.63, batch_ndcg = 64.34 
2025-02-13 00:12:24.559283: Steps 33/47: batch_recall = 79.70, batch_ndcg = 68.08 
2025-02-13 00:12:25.560142: Steps 34/47: batch_recall = 69.59, batch_ndcg = 56.79 
2025-02-13 00:12:26.534298: Steps 35/47: batch_recall = 76.84, batch_ndcg = 64.88 
2025-02-13 00:12:27.506588: Steps 36/47: batch_recall = 78.04, batch_ndcg = 63.69 
2025-02-13 00:12:28.463553: Steps 37/47: batch_recall = 84.31, batch_ndcg = 71.52 
2025-02-13 00:12:29.416602: Steps 38/47: batch_recall = 90.51, batch_ndcg = 72.80 
2025-02-13 00:12:30.367868: Steps 39/47: batch_recall = 87.35, batch_ndcg = 69.44 
2025-02-13 00:12:31.315669: Steps 40/47: batch_recall = 71.30, batch_ndcg = 62.05 
2025-02-13 00:12:32.269853: Steps 41/47: batch_recall = 87.34, batch_ndcg = 72.56 
2025-02-13 00:12:33.215344: Steps 42/47: batch_recall = 83.19, batch_ndcg = 64.71 
2025-02-13 00:12:34.166519: Steps 43/47: batch_recall = 91.53, batch_ndcg = 73.60 
2025-02-13 00:12:35.126568: Steps 44/47: batch_recall = 88.62, batch_ndcg = 69.60 
2025-02-13 00:12:36.077657: Steps 45/47: batch_recall = 93.48, batch_ndcg = 74.90 
2025-02-13 00:12:36.183108: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.53 
2025-02-13 00:12:36.183244: Epoch 67/1000, Test: Recall = 0.1217, NDCG = 0.1094  

2025-02-13 00:12:37.386020: Training Step 0/59: batchLoss = 3.1870, diffLoss = 15.5783, kgLoss = 0.0892
2025-02-13 00:12:38.317231: Training Step 1/59: batchLoss = 2.6735, diffLoss = 13.0346, kgLoss = 0.0832
2025-02-13 00:12:39.250734: Training Step 2/59: batchLoss = 2.9018, diffLoss = 14.1662, kgLoss = 0.0857
2025-02-13 00:12:40.178829: Training Step 3/59: batchLoss = 2.8099, diffLoss = 13.6990, kgLoss = 0.0876
2025-02-13 00:12:41.107262: Training Step 4/59: batchLoss = 2.9478, diffLoss = 14.3925, kgLoss = 0.0866
2025-02-13 00:12:42.044490: Training Step 5/59: batchLoss = 2.6134, diffLoss = 12.7498, kgLoss = 0.0793
2025-02-13 00:12:42.967426: Training Step 6/59: batchLoss = 3.0527, diffLoss = 14.9244, kgLoss = 0.0848
2025-02-13 00:12:43.897955: Training Step 7/59: batchLoss = 2.9098, diffLoss = 14.2196, kgLoss = 0.0823
2025-02-13 00:12:44.820732: Training Step 8/59: batchLoss = 2.8699, diffLoss = 13.9852, kgLoss = 0.0911
2025-02-13 00:12:45.746664: Training Step 9/59: batchLoss = 3.0155, diffLoss = 14.7465, kgLoss = 0.0827
2025-02-13 00:12:46.674528: Training Step 10/59: batchLoss = 2.8891, diffLoss = 14.1021, kgLoss = 0.0858
2025-02-13 00:12:47.598282: Training Step 11/59: batchLoss = 2.9836, diffLoss = 14.5466, kgLoss = 0.0928
2025-02-13 00:12:48.530607: Training Step 12/59: batchLoss = 2.6743, diffLoss = 13.0773, kgLoss = 0.0736
2025-02-13 00:12:49.461830: Training Step 13/59: batchLoss = 3.0053, diffLoss = 14.6439, kgLoss = 0.0957
2025-02-13 00:12:50.382539: Training Step 14/59: batchLoss = 3.2190, diffLoss = 15.7257, kgLoss = 0.0923
2025-02-13 00:12:51.309340: Training Step 15/59: batchLoss = 2.7366, diffLoss = 13.3513, kgLoss = 0.0829
2025-02-13 00:12:52.231431: Training Step 16/59: batchLoss = 2.9306, diffLoss = 14.2914, kgLoss = 0.0904
2025-02-13 00:12:53.148973: Training Step 17/59: batchLoss = 3.5432, diffLoss = 17.3099, kgLoss = 0.1016
2025-02-13 00:12:54.081428: Training Step 18/59: batchLoss = 2.7350, diffLoss = 13.3226, kgLoss = 0.0881
2025-02-13 00:12:55.011032: Training Step 19/59: batchLoss = 2.6903, diffLoss = 13.0940, kgLoss = 0.0893
2025-02-13 00:12:55.944659: Training Step 20/59: batchLoss = 2.9345, diffLoss = 14.3422, kgLoss = 0.0826
2025-02-13 00:12:56.886787: Training Step 21/59: batchLoss = 2.9341, diffLoss = 14.3436, kgLoss = 0.0817
2025-02-13 00:12:57.823482: Training Step 22/59: batchLoss = 2.8253, diffLoss = 13.8040, kgLoss = 0.0806
2025-02-13 00:12:58.759160: Training Step 23/59: batchLoss = 2.7331, diffLoss = 13.3202, kgLoss = 0.0863
2025-02-13 00:12:59.700405: Training Step 24/59: batchLoss = 2.6812, diffLoss = 13.0990, kgLoss = 0.0767
2025-02-13 00:13:00.637358: Training Step 25/59: batchLoss = 3.1905, diffLoss = 15.6023, kgLoss = 0.0875
2025-02-13 00:13:01.572328: Training Step 26/59: batchLoss = 2.4252, diffLoss = 11.8499, kgLoss = 0.0690
2025-02-13 00:13:02.501813: Training Step 27/59: batchLoss = 2.8774, diffLoss = 14.0623, kgLoss = 0.0811
2025-02-13 00:13:03.431611: Training Step 28/59: batchLoss = 2.8116, diffLoss = 13.7448, kgLoss = 0.0783
2025-02-13 00:13:04.360754: Training Step 29/59: batchLoss = 2.6290, diffLoss = 12.8577, kgLoss = 0.0719
2025-02-13 00:13:05.289741: Training Step 30/59: batchLoss = 3.0060, diffLoss = 14.6780, kgLoss = 0.0880
2025-02-13 00:13:06.221670: Training Step 31/59: batchLoss = 3.1659, diffLoss = 15.4567, kgLoss = 0.0932
2025-02-13 00:13:07.144636: Training Step 32/59: batchLoss = 3.0325, diffLoss = 14.8128, kgLoss = 0.0875
2025-02-13 00:13:08.071069: Training Step 33/59: batchLoss = 2.9724, diffLoss = 14.5103, kgLoss = 0.0879
2025-02-13 00:13:08.995191: Training Step 34/59: batchLoss = 3.0340, diffLoss = 14.8322, kgLoss = 0.0845
2025-02-13 00:13:09.916921: Training Step 35/59: batchLoss = 2.8458, diffLoss = 13.8835, kgLoss = 0.0863
2025-02-13 00:13:10.837713: Training Step 36/59: batchLoss = 2.9458, diffLoss = 14.3865, kgLoss = 0.0856
2025-02-13 00:13:11.762223: Training Step 37/59: batchLoss = 3.0097, diffLoss = 14.7114, kgLoss = 0.0843
2025-02-13 00:13:12.682817: Training Step 38/59: batchLoss = 2.8743, diffLoss = 14.0395, kgLoss = 0.0830
2025-02-13 00:13:13.605825: Training Step 39/59: batchLoss = 3.4873, diffLoss = 17.0410, kgLoss = 0.0989
2025-02-13 00:13:14.546916: Training Step 40/59: batchLoss = 3.1796, diffLoss = 15.5636, kgLoss = 0.0836
2025-02-13 00:13:15.473791: Training Step 41/59: batchLoss = 3.2232, diffLoss = 15.7355, kgLoss = 0.0952
2025-02-13 00:13:16.405890: Training Step 42/59: batchLoss = 3.3975, diffLoss = 16.6115, kgLoss = 0.0941
2025-02-13 00:13:17.342950: Training Step 43/59: batchLoss = 2.8992, diffLoss = 14.1716, kgLoss = 0.0811
2025-02-13 00:13:18.274323: Training Step 44/59: batchLoss = 3.1358, diffLoss = 15.3184, kgLoss = 0.0901
2025-02-13 00:13:19.208400: Training Step 45/59: batchLoss = 2.8965, diffLoss = 14.1490, kgLoss = 0.0834
2025-02-13 00:13:20.141634: Training Step 46/59: batchLoss = 3.4138, diffLoss = 16.6985, kgLoss = 0.0927
2025-02-13 00:13:21.078111: Training Step 47/59: batchLoss = 2.9943, diffLoss = 14.6333, kgLoss = 0.0845
2025-02-13 00:13:22.016687: Training Step 48/59: batchLoss = 2.4608, diffLoss = 12.0169, kgLoss = 0.0718
2025-02-13 00:13:22.959020: Training Step 49/59: batchLoss = 3.0458, diffLoss = 14.9054, kgLoss = 0.0809
2025-02-13 00:13:23.901792: Training Step 50/59: batchLoss = 3.0120, diffLoss = 14.7144, kgLoss = 0.0864
2025-02-13 00:13:24.836909: Training Step 51/59: batchLoss = 3.1379, diffLoss = 15.3372, kgLoss = 0.0881
2025-02-13 00:13:25.762743: Training Step 52/59: batchLoss = 3.0168, diffLoss = 14.7404, kgLoss = 0.0859
2025-02-13 00:13:26.687163: Training Step 53/59: batchLoss = 3.0512, diffLoss = 14.8992, kgLoss = 0.0892
2025-02-13 00:13:27.615312: Training Step 54/59: batchLoss = 2.9274, diffLoss = 14.2757, kgLoss = 0.0903
2025-02-13 00:13:28.540823: Training Step 55/59: batchLoss = 2.7704, diffLoss = 13.5243, kgLoss = 0.0820
2025-02-13 00:13:29.463256: Training Step 56/59: batchLoss = 3.0401, diffLoss = 14.8565, kgLoss = 0.0860
2025-02-13 00:13:30.309104: Training Step 57/59: batchLoss = 2.9028, diffLoss = 14.1805, kgLoss = 0.0833
2025-02-13 00:13:31.161663: Training Step 58/59: batchLoss = 2.9812, diffLoss = 14.5915, kgLoss = 0.0787
2025-02-13 00:13:31.257819: 
2025-02-13 00:13:31.258163: Epoch 68/1000, Train: epLoss = 0.4357, epDfLoss = 2.1282, epKgLoss = 0.0126  
2025-02-13 00:13:32.726699: Steps 0/47: batch_recall = 36.58, batch_ndcg = 47.56 
2025-02-13 00:13:34.019607: Steps 1/47: batch_recall = 37.25, batch_ndcg = 42.22 
2025-02-13 00:13:35.269062: Steps 2/47: batch_recall = 40.45, batch_ndcg = 46.36 
2025-02-13 00:13:36.531692: Steps 3/47: batch_recall = 43.79, batch_ndcg = 45.33 
2025-02-13 00:13:37.736776: Steps 4/47: batch_recall = 39.18, batch_ndcg = 45.68 
2025-02-13 00:13:38.954162: Steps 5/47: batch_recall = 33.72, batch_ndcg = 40.21 
2025-02-13 00:13:40.157527: Steps 6/47: batch_recall = 39.51, batch_ndcg = 41.81 
2025-02-13 00:13:41.342867: Steps 7/47: batch_recall = 43.65, batch_ndcg = 44.52 
2025-02-13 00:13:42.535482: Steps 8/47: batch_recall = 48.15, batch_ndcg = 53.00 
2025-02-13 00:13:43.684716: Steps 9/47: batch_recall = 46.38, batch_ndcg = 46.66 
2025-02-13 00:13:44.860648: Steps 10/47: batch_recall = 43.84, batch_ndcg = 43.89 
2025-02-13 00:13:46.008503: Steps 11/47: batch_recall = 54.39, batch_ndcg = 52.06 
2025-02-13 00:13:47.149208: Steps 12/47: batch_recall = 50.07, batch_ndcg = 48.42 
2025-02-13 00:13:48.264716: Steps 13/47: batch_recall = 49.94, batch_ndcg = 46.83 
2025-02-13 00:13:49.346562: Steps 14/47: batch_recall = 41.53, batch_ndcg = 41.52 
2025-02-13 00:13:50.434482: Steps 15/47: batch_recall = 57.65, batch_ndcg = 54.92 
2025-02-13 00:13:51.507388: Steps 16/47: batch_recall = 51.51, batch_ndcg = 47.32 
2025-02-13 00:13:52.545589: Steps 17/47: batch_recall = 57.54, batch_ndcg = 49.43 
2025-02-13 00:13:53.613987: Steps 18/47: batch_recall = 53.09, batch_ndcg = 49.24 
2025-02-13 00:13:54.664183: Steps 19/47: batch_recall = 60.50, batch_ndcg = 55.21 
2025-02-13 00:13:55.699644: Steps 20/47: batch_recall = 68.54, batch_ndcg = 62.15 
2025-02-13 00:13:56.736998: Steps 21/47: batch_recall = 63.54, batch_ndcg = 56.46 
2025-02-13 00:13:57.772038: Steps 22/47: batch_recall = 55.18, batch_ndcg = 51.24 
2025-02-13 00:13:58.810167: Steps 23/47: batch_recall = 63.23, batch_ndcg = 54.99 
2025-02-13 00:13:59.866860: Steps 24/47: batch_recall = 63.64, batch_ndcg = 57.20 
2025-02-13 00:14:00.893565: Steps 25/47: batch_recall = 66.58, batch_ndcg = 58.39 
2025-02-13 00:14:01.905810: Steps 26/47: batch_recall = 61.26, batch_ndcg = 55.33 
2025-02-13 00:14:02.925246: Steps 27/47: batch_recall = 64.54, batch_ndcg = 54.21 
2025-02-13 00:14:03.932775: Steps 28/47: batch_recall = 70.42, batch_ndcg = 59.36 
2025-02-13 00:14:04.943650: Steps 29/47: batch_recall = 70.60, batch_ndcg = 58.76 
2025-02-13 00:14:05.947303: Steps 30/47: batch_recall = 74.11, batch_ndcg = 64.59 
2025-02-13 00:14:06.986752: Steps 31/47: batch_recall = 67.35, batch_ndcg = 56.60 
2025-02-13 00:14:07.988997: Steps 32/47: batch_recall = 71.00, batch_ndcg = 65.17 
2025-02-13 00:14:08.998735: Steps 33/47: batch_recall = 80.24, batch_ndcg = 68.13 
2025-02-13 00:14:10.022187: Steps 34/47: batch_recall = 68.90, batch_ndcg = 56.69 
2025-02-13 00:14:11.008617: Steps 35/47: batch_recall = 77.71, batch_ndcg = 65.76 
2025-02-13 00:14:11.997913: Steps 36/47: batch_recall = 78.44, batch_ndcg = 63.68 
2025-02-13 00:14:12.983612: Steps 37/47: batch_recall = 83.47, batch_ndcg = 71.76 
2025-02-13 00:14:13.970590: Steps 38/47: batch_recall = 90.59, batch_ndcg = 73.04 
2025-02-13 00:14:14.954786: Steps 39/47: batch_recall = 88.03, batch_ndcg = 69.75 
2025-02-13 00:14:15.927482: Steps 40/47: batch_recall = 72.35, batch_ndcg = 63.14 
2025-02-13 00:14:16.897125: Steps 41/47: batch_recall = 86.38, batch_ndcg = 72.07 
2025-02-13 00:14:17.864974: Steps 42/47: batch_recall = 81.93, batch_ndcg = 65.01 
2025-02-13 00:14:18.822911: Steps 43/47: batch_recall = 90.45, batch_ndcg = 72.94 
2025-02-13 00:14:19.790214: Steps 44/47: batch_recall = 86.54, batch_ndcg = 69.53 
2025-02-13 00:14:20.720748: Steps 45/47: batch_recall = 92.31, batch_ndcg = 74.81 
2025-02-13 00:14:20.825946: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.54 
2025-02-13 00:14:20.826075: Epoch 68/1000, Test: Recall = 0.1217, NDCG = 0.1097  

2025-02-13 00:14:22.010954: Training Step 0/59: batchLoss = 2.6442, diffLoss = 12.8902, kgLoss = 0.0828
2025-02-13 00:14:22.941611: Training Step 1/59: batchLoss = 2.8400, diffLoss = 13.8270, kgLoss = 0.0932
2025-02-13 00:14:23.862755: Training Step 2/59: batchLoss = 2.9360, diffLoss = 14.3049, kgLoss = 0.0938
2025-02-13 00:14:24.780998: Training Step 3/59: batchLoss = 2.8525, diffLoss = 13.8944, kgLoss = 0.0920
2025-02-13 00:14:25.697994: Training Step 4/59: batchLoss = 2.9514, diffLoss = 14.4254, kgLoss = 0.0828
2025-02-13 00:14:26.617557: Training Step 5/59: batchLoss = 2.7598, diffLoss = 13.4725, kgLoss = 0.0816
2025-02-13 00:14:27.538791: Training Step 6/59: batchLoss = 2.9744, diffLoss = 14.5098, kgLoss = 0.0906
2025-02-13 00:14:28.466630: Training Step 7/59: batchLoss = 2.7245, diffLoss = 13.3188, kgLoss = 0.0759
2025-02-13 00:14:29.389671: Training Step 8/59: batchLoss = 2.5996, diffLoss = 12.7012, kgLoss = 0.0743
2025-02-13 00:14:30.316675: Training Step 9/59: batchLoss = 2.8486, diffLoss = 13.8718, kgLoss = 0.0929
2025-02-13 00:14:31.253653: Training Step 10/59: batchLoss = 2.6851, diffLoss = 13.1033, kgLoss = 0.0806
2025-02-13 00:14:32.183050: Training Step 11/59: batchLoss = 2.8733, diffLoss = 14.0115, kgLoss = 0.0888
2025-02-13 00:14:33.105179: Training Step 12/59: batchLoss = 3.1114, diffLoss = 15.1686, kgLoss = 0.0971
2025-02-13 00:14:34.024685: Training Step 13/59: batchLoss = 2.8234, diffLoss = 13.7633, kgLoss = 0.0885
2025-02-13 00:14:34.950066: Training Step 14/59: batchLoss = 2.9850, diffLoss = 14.5813, kgLoss = 0.0860
2025-02-13 00:14:35.882994: Training Step 15/59: batchLoss = 2.9219, diffLoss = 14.2910, kgLoss = 0.0796
2025-02-13 00:14:36.817914: Training Step 16/59: batchLoss = 2.6941, diffLoss = 13.1395, kgLoss = 0.0828
2025-02-13 00:14:37.747077: Training Step 17/59: batchLoss = 2.7275, diffLoss = 13.3075, kgLoss = 0.0825
2025-02-13 00:14:38.674437: Training Step 18/59: batchLoss = 2.7424, diffLoss = 13.3927, kgLoss = 0.0798
2025-02-13 00:14:39.604602: Training Step 19/59: batchLoss = 2.9243, diffLoss = 14.2591, kgLoss = 0.0906
2025-02-13 00:14:40.525999: Training Step 20/59: batchLoss = 3.0813, diffLoss = 15.0345, kgLoss = 0.0930
2025-02-13 00:14:41.447491: Training Step 21/59: batchLoss = 2.7115, diffLoss = 13.2172, kgLoss = 0.0851
2025-02-13 00:14:42.371878: Training Step 22/59: batchLoss = 2.8032, diffLoss = 13.7001, kgLoss = 0.0790
2025-02-13 00:14:43.291934: Training Step 23/59: batchLoss = 3.1053, diffLoss = 15.1616, kgLoss = 0.0912
2025-02-13 00:14:44.221002: Training Step 24/59: batchLoss = 3.1246, diffLoss = 15.2875, kgLoss = 0.0839
2025-02-13 00:14:45.143181: Training Step 25/59: batchLoss = 2.6174, diffLoss = 12.8007, kgLoss = 0.0716
2025-02-13 00:14:46.064969: Training Step 26/59: batchLoss = 3.1505, diffLoss = 15.3613, kgLoss = 0.0978
2025-02-13 00:14:46.997720: Training Step 27/59: batchLoss = 2.9473, diffLoss = 14.3936, kgLoss = 0.0857
2025-02-13 00:14:47.922801: Training Step 28/59: batchLoss = 3.0911, diffLoss = 15.1082, kgLoss = 0.0868
2025-02-13 00:14:48.852785: Training Step 29/59: batchLoss = 2.7787, diffLoss = 13.5301, kgLoss = 0.0908
2025-02-13 00:14:49.775058: Training Step 30/59: batchLoss = 3.0528, diffLoss = 14.9046, kgLoss = 0.0899
2025-02-13 00:14:50.708948: Training Step 31/59: batchLoss = 3.0327, diffLoss = 14.8316, kgLoss = 0.0830
2025-02-13 00:14:51.638327: Training Step 32/59: batchLoss = 3.1415, diffLoss = 15.3520, kgLoss = 0.0888
2025-02-13 00:14:52.569315: Training Step 33/59: batchLoss = 2.8601, diffLoss = 13.9855, kgLoss = 0.0788
2025-02-13 00:14:53.497861: Training Step 34/59: batchLoss = 3.0884, diffLoss = 15.0801, kgLoss = 0.0905
2025-02-13 00:14:54.433185: Training Step 35/59: batchLoss = 2.6972, diffLoss = 13.2048, kgLoss = 0.0703
2025-02-13 00:14:55.367192: Training Step 36/59: batchLoss = 2.6788, diffLoss = 13.0783, kgLoss = 0.0789
2025-02-13 00:14:56.301190: Training Step 37/59: batchLoss = 2.8709, diffLoss = 14.0317, kgLoss = 0.0807
2025-02-13 00:14:57.234987: Training Step 38/59: batchLoss = 2.8984, diffLoss = 14.1664, kgLoss = 0.0814
2025-02-13 00:14:58.173056: Training Step 39/59: batchLoss = 2.7805, diffLoss = 13.5918, kgLoss = 0.0776
2025-02-13 00:14:59.111433: Training Step 40/59: batchLoss = 3.0519, diffLoss = 14.8830, kgLoss = 0.0941
2025-02-13 00:15:00.042310: Training Step 41/59: batchLoss = 3.2980, diffLoss = 16.1150, kgLoss = 0.0938
2025-02-13 00:15:00.976423: Training Step 42/59: batchLoss = 3.2036, diffLoss = 15.6505, kgLoss = 0.0919
2025-02-13 00:15:01.898800: Training Step 43/59: batchLoss = 3.0483, diffLoss = 14.8601, kgLoss = 0.0953
2025-02-13 00:15:02.844524: Training Step 44/59: batchLoss = 2.9251, diffLoss = 14.2839, kgLoss = 0.0853
2025-02-13 00:15:03.763770: Training Step 45/59: batchLoss = 2.8036, diffLoss = 13.6950, kgLoss = 0.0808
2025-02-13 00:15:04.702380: Training Step 46/59: batchLoss = 3.3143, diffLoss = 16.1972, kgLoss = 0.0936
2025-02-13 00:15:05.630819: Training Step 47/59: batchLoss = 3.0096, diffLoss = 14.7092, kgLoss = 0.0848
2025-02-13 00:15:06.558432: Training Step 48/59: batchLoss = 3.0130, diffLoss = 14.7357, kgLoss = 0.0823
2025-02-13 00:15:07.473245: Training Step 49/59: batchLoss = 2.9861, diffLoss = 14.6056, kgLoss = 0.0812
2025-02-13 00:15:08.392540: Training Step 50/59: batchLoss = 2.8320, diffLoss = 13.8720, kgLoss = 0.0720
2025-02-13 00:15:09.312771: Training Step 51/59: batchLoss = 3.2693, diffLoss = 15.9629, kgLoss = 0.0958
2025-02-13 00:15:10.235510: Training Step 52/59: batchLoss = 2.9231, diffLoss = 14.2919, kgLoss = 0.0810
2025-02-13 00:15:11.153450: Training Step 53/59: batchLoss = 3.2797, diffLoss = 16.0067, kgLoss = 0.0980
2025-02-13 00:15:12.090869: Training Step 54/59: batchLoss = 2.9352, diffLoss = 14.3486, kgLoss = 0.0818
2025-02-13 00:15:13.026595: Training Step 55/59: batchLoss = 2.9214, diffLoss = 14.2939, kgLoss = 0.0783
2025-02-13 00:15:13.954307: Training Step 56/59: batchLoss = 3.0866, diffLoss = 15.0983, kgLoss = 0.0837
2025-02-13 00:15:14.805315: Training Step 57/59: batchLoss = 3.1025, diffLoss = 15.1461, kgLoss = 0.0916
2025-02-13 00:15:15.666570: Training Step 58/59: batchLoss = 2.8488, diffLoss = 13.9186, kgLoss = 0.0814
2025-02-13 00:15:15.760373: 
2025-02-13 00:15:15.760892: Epoch 69/1000, Train: epLoss = 0.4325, epDfLoss = 2.1118, epKgLoss = 0.0126  
2025-02-13 00:15:17.232941: Steps 0/47: batch_recall = 36.91, batch_ndcg = 46.71 
2025-02-13 00:15:18.536244: Steps 1/47: batch_recall = 37.62, batch_ndcg = 42.30 
2025-02-13 00:15:19.789677: Steps 2/47: batch_recall = 40.93, batch_ndcg = 46.50 
2025-02-13 00:15:21.054206: Steps 3/47: batch_recall = 43.98, batch_ndcg = 45.28 
2025-02-13 00:15:22.251658: Steps 4/47: batch_recall = 39.71, batch_ndcg = 45.79 
2025-02-13 00:15:23.454657: Steps 5/47: batch_recall = 33.37, batch_ndcg = 39.81 
2025-02-13 00:15:24.637709: Steps 6/47: batch_recall = 40.28, batch_ndcg = 42.88 
2025-02-13 00:15:25.799797: Steps 7/47: batch_recall = 43.53, batch_ndcg = 44.74 
2025-02-13 00:15:26.959749: Steps 8/47: batch_recall = 48.21, batch_ndcg = 52.49 
2025-02-13 00:15:28.076638: Steps 9/47: batch_recall = 45.69, batch_ndcg = 46.44 
2025-02-13 00:15:29.234303: Steps 10/47: batch_recall = 43.93, batch_ndcg = 43.88 
2025-02-13 00:15:30.364736: Steps 11/47: batch_recall = 54.55, batch_ndcg = 52.73 
2025-02-13 00:15:31.504844: Steps 12/47: batch_recall = 49.51, batch_ndcg = 48.77 
2025-02-13 00:15:32.631918: Steps 13/47: batch_recall = 50.14, batch_ndcg = 46.71 
2025-02-13 00:15:33.724680: Steps 14/47: batch_recall = 43.37, batch_ndcg = 41.67 
2025-02-13 00:15:34.815553: Steps 15/47: batch_recall = 57.43, batch_ndcg = 54.18 
2025-02-13 00:15:35.902318: Steps 16/47: batch_recall = 49.81, batch_ndcg = 46.17 
2025-02-13 00:15:36.963789: Steps 17/47: batch_recall = 58.05, batch_ndcg = 49.33 
2025-02-13 00:15:38.044230: Steps 18/47: batch_recall = 52.65, batch_ndcg = 49.25 
2025-02-13 00:15:39.114389: Steps 19/47: batch_recall = 60.83, batch_ndcg = 55.04 
2025-02-13 00:15:40.158857: Steps 20/47: batch_recall = 67.21, batch_ndcg = 61.58 
2025-02-13 00:15:41.204261: Steps 21/47: batch_recall = 63.83, batch_ndcg = 55.26 
2025-02-13 00:15:42.248596: Steps 22/47: batch_recall = 55.68, batch_ndcg = 51.25 
2025-02-13 00:15:43.300558: Steps 23/47: batch_recall = 63.17, batch_ndcg = 54.77 
2025-02-13 00:15:44.355430: Steps 24/47: batch_recall = 65.72, batch_ndcg = 58.49 
2025-02-13 00:15:45.381924: Steps 25/47: batch_recall = 67.00, batch_ndcg = 57.79 
2025-02-13 00:15:46.401935: Steps 26/47: batch_recall = 60.61, batch_ndcg = 54.00 
2025-02-13 00:15:47.433353: Steps 27/47: batch_recall = 63.90, batch_ndcg = 53.64 
2025-02-13 00:15:48.450368: Steps 28/47: batch_recall = 70.54, batch_ndcg = 60.18 
2025-02-13 00:15:49.464978: Steps 29/47: batch_recall = 70.94, batch_ndcg = 59.13 
2025-02-13 00:15:50.511295: Steps 30/47: batch_recall = 74.20, batch_ndcg = 64.83 
2025-02-13 00:15:51.534115: Steps 31/47: batch_recall = 66.57, batch_ndcg = 56.34 
2025-02-13 00:15:52.552977: Steps 32/47: batch_recall = 71.50, batch_ndcg = 65.43 
2025-02-13 00:15:53.575677: Steps 33/47: batch_recall = 79.40, batch_ndcg = 67.41 
2025-02-13 00:15:54.592352: Steps 34/47: batch_recall = 69.66, batch_ndcg = 57.62 
2025-02-13 00:15:55.588106: Steps 35/47: batch_recall = 77.27, batch_ndcg = 65.22 
2025-02-13 00:15:56.580500: Steps 36/47: batch_recall = 78.04, batch_ndcg = 64.46 
2025-02-13 00:15:57.565886: Steps 37/47: batch_recall = 83.48, batch_ndcg = 73.17 
2025-02-13 00:15:58.549139: Steps 38/47: batch_recall = 90.98, batch_ndcg = 72.85 
2025-02-13 00:15:59.527617: Steps 39/47: batch_recall = 88.59, batch_ndcg = 69.47 
2025-02-13 00:16:00.500904: Steps 40/47: batch_recall = 72.96, batch_ndcg = 62.95 
2025-02-13 00:16:01.465717: Steps 41/47: batch_recall = 87.79, batch_ndcg = 72.00 
2025-02-13 00:16:02.430022: Steps 42/47: batch_recall = 82.24, batch_ndcg = 65.09 
2025-02-13 00:16:03.390883: Steps 43/47: batch_recall = 90.29, batch_ndcg = 72.13 
2025-02-13 00:16:04.349587: Steps 44/47: batch_recall = 86.48, batch_ndcg = 69.17 
2025-02-13 00:16:05.283309: Steps 45/47: batch_recall = 92.50, batch_ndcg = 74.75 
2025-02-13 00:16:05.386500: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.58 
2025-02-13 00:16:05.386636: Epoch 69/1000, Test: Recall = 0.1219, NDCG = 0.1095  

2025-02-13 00:16:06.577482: Training Step 0/59: batchLoss = 3.0213, diffLoss = 14.7218, kgLoss = 0.0962
2025-02-13 00:16:07.509697: Training Step 1/59: batchLoss = 2.9894, diffLoss = 14.5613, kgLoss = 0.0964
2025-02-13 00:16:08.430940: Training Step 2/59: batchLoss = 2.8810, diffLoss = 14.0263, kgLoss = 0.0946
2025-02-13 00:16:09.359347: Training Step 3/59: batchLoss = 2.7635, diffLoss = 13.4882, kgLoss = 0.0823
2025-02-13 00:16:10.283446: Training Step 4/59: batchLoss = 2.8707, diffLoss = 14.0214, kgLoss = 0.0831
2025-02-13 00:16:11.212152: Training Step 5/59: batchLoss = 2.5182, diffLoss = 12.2433, kgLoss = 0.0870
2025-02-13 00:16:12.139691: Training Step 6/59: batchLoss = 2.8186, diffLoss = 13.7669, kgLoss = 0.0815
2025-02-13 00:16:13.064749: Training Step 7/59: batchLoss = 2.5387, diffLoss = 12.3869, kgLoss = 0.0766
2025-02-13 00:16:13.989348: Training Step 8/59: batchLoss = 2.9280, diffLoss = 14.2904, kgLoss = 0.0874
2025-02-13 00:16:14.921353: Training Step 9/59: batchLoss = 3.0011, diffLoss = 14.6340, kgLoss = 0.0929
2025-02-13 00:16:15.853843: Training Step 10/59: batchLoss = 2.6937, diffLoss = 13.1507, kgLoss = 0.0794
2025-02-13 00:16:16.793861: Training Step 11/59: batchLoss = 2.8321, diffLoss = 13.8160, kgLoss = 0.0861
2025-02-13 00:16:17.731013: Training Step 12/59: batchLoss = 2.8180, diffLoss = 13.7450, kgLoss = 0.0862
2025-02-13 00:16:18.668994: Training Step 13/59: batchLoss = 3.0044, diffLoss = 14.6782, kgLoss = 0.0860
2025-02-13 00:16:19.599653: Training Step 14/59: batchLoss = 2.8311, diffLoss = 13.8055, kgLoss = 0.0875
2025-02-13 00:16:20.530375: Training Step 15/59: batchLoss = 2.7393, diffLoss = 13.3663, kgLoss = 0.0826
2025-02-13 00:16:21.470566: Training Step 16/59: batchLoss = 2.7005, diffLoss = 13.1720, kgLoss = 0.0826
2025-02-13 00:16:22.400373: Training Step 17/59: batchLoss = 2.8356, diffLoss = 13.8825, kgLoss = 0.0739
2025-02-13 00:16:23.331864: Training Step 18/59: batchLoss = 2.8979, diffLoss = 14.1420, kgLoss = 0.0869
2025-02-13 00:16:24.262010: Training Step 19/59: batchLoss = 2.9954, diffLoss = 14.6296, kgLoss = 0.0868
2025-02-13 00:16:25.189425: Training Step 20/59: batchLoss = 3.0541, diffLoss = 14.8608, kgLoss = 0.1024
2025-02-13 00:16:26.118432: Training Step 21/59: batchLoss = 2.9517, diffLoss = 14.3888, kgLoss = 0.0924
2025-02-13 00:16:27.041432: Training Step 22/59: batchLoss = 2.9094, diffLoss = 14.1993, kgLoss = 0.0869
2025-02-13 00:16:27.976716: Training Step 23/59: batchLoss = 3.1211, diffLoss = 15.2600, kgLoss = 0.0864
2025-02-13 00:16:28.903563: Training Step 24/59: batchLoss = 3.1740, diffLoss = 15.5026, kgLoss = 0.0919
2025-02-13 00:16:29.827350: Training Step 25/59: batchLoss = 2.9795, diffLoss = 14.5603, kgLoss = 0.0843
2025-02-13 00:16:30.755558: Training Step 26/59: batchLoss = 3.1476, diffLoss = 15.3880, kgLoss = 0.0875
2025-02-13 00:16:31.686485: Training Step 27/59: batchLoss = 2.9543, diffLoss = 14.4294, kgLoss = 0.0855
2025-02-13 00:16:32.615536: Training Step 28/59: batchLoss = 2.8048, diffLoss = 13.6936, kgLoss = 0.0825
2025-02-13 00:16:33.541988: Training Step 29/59: batchLoss = 3.0945, diffLoss = 15.1330, kgLoss = 0.0848
2025-02-13 00:16:34.464020: Training Step 30/59: batchLoss = 2.9867, diffLoss = 14.6129, kgLoss = 0.0802
2025-02-13 00:16:35.407459: Training Step 31/59: batchLoss = 2.9388, diffLoss = 14.3507, kgLoss = 0.0859
2025-02-13 00:16:36.340512: Training Step 32/59: batchLoss = 2.9819, diffLoss = 14.5667, kgLoss = 0.0858
2025-02-13 00:16:37.277832: Training Step 33/59: batchLoss = 2.8162, diffLoss = 13.7756, kgLoss = 0.0764
2025-02-13 00:16:38.212724: Training Step 34/59: batchLoss = 2.9771, diffLoss = 14.5403, kgLoss = 0.0863
2025-02-13 00:16:39.154059: Training Step 35/59: batchLoss = 2.7940, diffLoss = 13.6607, kgLoss = 0.0774
2025-02-13 00:16:40.089065: Training Step 36/59: batchLoss = 3.2019, diffLoss = 15.6163, kgLoss = 0.0984
2025-02-13 00:16:41.037901: Training Step 37/59: batchLoss = 2.9989, diffLoss = 14.6410, kgLoss = 0.0883
2025-02-13 00:16:41.967451: Training Step 38/59: batchLoss = 2.8935, diffLoss = 14.1251, kgLoss = 0.0856
2025-02-13 00:16:42.900817: Training Step 39/59: batchLoss = 2.8452, diffLoss = 13.9060, kgLoss = 0.0800
2025-02-13 00:16:43.829757: Training Step 40/59: batchLoss = 2.7011, diffLoss = 13.1887, kgLoss = 0.0792
2025-02-13 00:16:44.761823: Training Step 41/59: batchLoss = 2.8343, diffLoss = 13.8710, kgLoss = 0.0752
2025-02-13 00:16:45.694535: Training Step 42/59: batchLoss = 2.8664, diffLoss = 14.0309, kgLoss = 0.0753
2025-02-13 00:16:46.627129: Training Step 43/59: batchLoss = 2.6120, diffLoss = 12.7620, kgLoss = 0.0745
2025-02-13 00:16:47.550162: Training Step 44/59: batchLoss = 3.0186, diffLoss = 14.7501, kgLoss = 0.0858
2025-02-13 00:16:48.473172: Training Step 45/59: batchLoss = 2.8591, diffLoss = 13.9687, kgLoss = 0.0816
2025-02-13 00:16:49.394688: Training Step 46/59: batchLoss = 3.2340, diffLoss = 15.7977, kgLoss = 0.0931
2025-02-13 00:16:50.307163: Training Step 47/59: batchLoss = 3.0084, diffLoss = 14.6832, kgLoss = 0.0897
2025-02-13 00:16:51.237427: Training Step 48/59: batchLoss = 3.1797, diffLoss = 15.5520, kgLoss = 0.0867
2025-02-13 00:16:52.157867: Training Step 49/59: batchLoss = 3.2419, diffLoss = 15.8267, kgLoss = 0.0958
2025-02-13 00:16:53.080581: Training Step 50/59: batchLoss = 2.7998, diffLoss = 13.6247, kgLoss = 0.0935
2025-02-13 00:16:54.005713: Training Step 51/59: batchLoss = 3.6900, diffLoss = 18.0311, kgLoss = 0.1047
2025-02-13 00:16:54.925431: Training Step 52/59: batchLoss = 2.7467, diffLoss = 13.4191, kgLoss = 0.0786
2025-02-13 00:16:55.851735: Training Step 53/59: batchLoss = 3.1886, diffLoss = 15.5920, kgLoss = 0.0877
2025-02-13 00:16:56.786869: Training Step 54/59: batchLoss = 2.7392, diffLoss = 13.3800, kgLoss = 0.0790
2025-02-13 00:16:57.715634: Training Step 55/59: batchLoss = 3.0321, diffLoss = 14.8250, kgLoss = 0.0838
2025-02-13 00:16:58.641815: Training Step 56/59: batchLoss = 2.6980, diffLoss = 13.1864, kgLoss = 0.0759
2025-02-13 00:16:59.489351: Training Step 57/59: batchLoss = 3.1816, diffLoss = 15.5770, kgLoss = 0.0828
2025-02-13 00:17:00.337505: Training Step 58/59: batchLoss = 2.7485, diffLoss = 13.4280, kgLoss = 0.0786
2025-02-13 00:17:00.430101: 
2025-02-13 00:17:00.430613: Epoch 70/1000, Train: epLoss = 0.4317, epDfLoss = 2.1081, epKgLoss = 0.0126  
2025-02-13 00:17:01.903330: Steps 0/47: batch_recall = 36.93, batch_ndcg = 47.43 
2025-02-13 00:17:03.213269: Steps 1/47: batch_recall = 36.98, batch_ndcg = 42.79 
2025-02-13 00:17:04.477894: Steps 2/47: batch_recall = 41.22, batch_ndcg = 47.41 
2025-02-13 00:17:05.745758: Steps 3/47: batch_recall = 43.91, batch_ndcg = 46.28 
2025-02-13 00:17:06.938046: Steps 4/47: batch_recall = 38.99, batch_ndcg = 45.71 
2025-02-13 00:17:08.149853: Steps 5/47: batch_recall = 32.37, batch_ndcg = 39.47 
2025-02-13 00:17:09.345863: Steps 6/47: batch_recall = 39.84, batch_ndcg = 42.69 
2025-02-13 00:17:10.513636: Steps 7/47: batch_recall = 43.73, batch_ndcg = 44.83 
2025-02-13 00:17:11.682624: Steps 8/47: batch_recall = 48.58, batch_ndcg = 52.37 
2025-02-13 00:17:12.810978: Steps 9/47: batch_recall = 45.70, batch_ndcg = 46.18 
2025-02-13 00:17:13.979972: Steps 10/47: batch_recall = 43.45, batch_ndcg = 43.52 
2025-02-13 00:17:15.116588: Steps 11/47: batch_recall = 54.98, batch_ndcg = 52.72 
2025-02-13 00:17:16.246683: Steps 12/47: batch_recall = 50.45, batch_ndcg = 48.59 
2025-02-13 00:17:17.376850: Steps 13/47: batch_recall = 48.64, batch_ndcg = 44.86 
2025-02-13 00:17:18.472757: Steps 14/47: batch_recall = 40.73, batch_ndcg = 40.77 
2025-02-13 00:17:19.578568: Steps 15/47: batch_recall = 57.42, batch_ndcg = 54.47 
2025-02-13 00:17:20.670148: Steps 16/47: batch_recall = 51.02, batch_ndcg = 47.84 
2025-02-13 00:17:21.725875: Steps 17/47: batch_recall = 58.75, batch_ndcg = 49.95 
2025-02-13 00:17:22.807645: Steps 18/47: batch_recall = 54.04, batch_ndcg = 49.43 
2025-02-13 00:17:23.879613: Steps 19/47: batch_recall = 60.46, batch_ndcg = 55.28 
2025-02-13 00:17:24.939797: Steps 20/47: batch_recall = 68.23, batch_ndcg = 63.09 
2025-02-13 00:17:25.993875: Steps 21/47: batch_recall = 63.84, batch_ndcg = 55.20 
2025-02-13 00:17:27.047563: Steps 22/47: batch_recall = 54.86, batch_ndcg = 51.44 
2025-02-13 00:17:28.096309: Steps 23/47: batch_recall = 63.20, batch_ndcg = 55.20 
2025-02-13 00:17:29.144144: Steps 24/47: batch_recall = 65.64, batch_ndcg = 58.46 
2025-02-13 00:17:30.180747: Steps 25/47: batch_recall = 65.67, batch_ndcg = 57.07 
2025-02-13 00:17:31.200384: Steps 26/47: batch_recall = 59.42, batch_ndcg = 53.32 
2025-02-13 00:17:32.225210: Steps 27/47: batch_recall = 63.37, batch_ndcg = 53.59 
2025-02-13 00:17:33.244650: Steps 28/47: batch_recall = 72.11, batch_ndcg = 61.13 
2025-02-13 00:17:34.259950: Steps 29/47: batch_recall = 70.73, batch_ndcg = 59.09 
2025-02-13 00:17:35.258910: Steps 30/47: batch_recall = 73.76, batch_ndcg = 64.69 
2025-02-13 00:17:36.281877: Steps 31/47: batch_recall = 67.58, batch_ndcg = 56.63 
2025-02-13 00:17:37.295577: Steps 32/47: batch_recall = 70.81, batch_ndcg = 65.26 
2025-02-13 00:17:38.301394: Steps 33/47: batch_recall = 80.68, batch_ndcg = 68.59 
2025-02-13 00:17:39.323074: Steps 34/47: batch_recall = 68.01, batch_ndcg = 56.89 
2025-02-13 00:17:40.316584: Steps 35/47: batch_recall = 77.53, batch_ndcg = 65.96 
2025-02-13 00:17:41.305508: Steps 36/47: batch_recall = 79.05, batch_ndcg = 64.28 
2025-02-13 00:17:42.282178: Steps 37/47: batch_recall = 84.47, batch_ndcg = 72.60 
2025-02-13 00:17:43.262750: Steps 38/47: batch_recall = 89.90, batch_ndcg = 72.76 
2025-02-13 00:17:44.240520: Steps 39/47: batch_recall = 87.96, batch_ndcg = 69.36 
2025-02-13 00:17:45.205646: Steps 40/47: batch_recall = 71.28, batch_ndcg = 62.80 
2025-02-13 00:17:46.173434: Steps 41/47: batch_recall = 86.94, batch_ndcg = 72.24 
2025-02-13 00:17:47.143624: Steps 42/47: batch_recall = 82.39, batch_ndcg = 64.94 
2025-02-13 00:17:48.112621: Steps 43/47: batch_recall = 91.27, batch_ndcg = 74.12 
2025-02-13 00:17:49.077644: Steps 44/47: batch_recall = 86.99, batch_ndcg = 69.08 
2025-02-13 00:17:50.008577: Steps 45/47: batch_recall = 93.89, batch_ndcg = 75.03 
2025-02-13 00:17:50.113823: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.69 
2025-02-13 00:17:50.113950: Epoch 70/1000, Test: Recall = 0.1218, NDCG = 0.1098  

2025-02-13 00:17:51.301396: Training Step 0/59: batchLoss = 2.6295, diffLoss = 12.8350, kgLoss = 0.0781
2025-02-13 00:17:52.222820: Training Step 1/59: batchLoss = 2.8138, diffLoss = 13.7455, kgLoss = 0.0809
2025-02-13 00:17:53.148372: Training Step 2/59: batchLoss = 2.9336, diffLoss = 14.3330, kgLoss = 0.0837
2025-02-13 00:17:54.078774: Training Step 3/59: batchLoss = 2.9609, diffLoss = 14.4693, kgLoss = 0.0838
2025-02-13 00:17:55.001005: Training Step 4/59: batchLoss = 2.8660, diffLoss = 13.9875, kgLoss = 0.0856
2025-02-13 00:17:55.931859: Training Step 5/59: batchLoss = 2.8337, diffLoss = 13.8116, kgLoss = 0.0893
2025-02-13 00:17:56.857755: Training Step 6/59: batchLoss = 2.8119, diffLoss = 13.7102, kgLoss = 0.0873
2025-02-13 00:17:57.782009: Training Step 7/59: batchLoss = 2.6279, diffLoss = 12.8130, kgLoss = 0.0816
2025-02-13 00:17:58.717123: Training Step 8/59: batchLoss = 2.9592, diffLoss = 14.4472, kgLoss = 0.0872
2025-02-13 00:17:59.650287: Training Step 9/59: batchLoss = 3.1488, diffLoss = 15.3678, kgLoss = 0.0940
2025-02-13 00:18:00.583336: Training Step 10/59: batchLoss = 2.7784, diffLoss = 13.5631, kgLoss = 0.0823
2025-02-13 00:18:01.515485: Training Step 11/59: batchLoss = 2.9335, diffLoss = 14.3390, kgLoss = 0.0821
2025-02-13 00:18:02.448738: Training Step 12/59: batchLoss = 2.8489, diffLoss = 13.9087, kgLoss = 0.0840
2025-02-13 00:18:03.394252: Training Step 13/59: batchLoss = 2.9204, diffLoss = 14.2690, kgLoss = 0.0833
2025-02-13 00:18:04.329679: Training Step 14/59: batchLoss = 2.8201, diffLoss = 13.7868, kgLoss = 0.0785
2025-02-13 00:18:05.263735: Training Step 15/59: batchLoss = 2.9088, diffLoss = 14.1939, kgLoss = 0.0875
2025-02-13 00:18:06.205833: Training Step 16/59: batchLoss = 2.7763, diffLoss = 13.5541, kgLoss = 0.0818
2025-02-13 00:18:07.141712: Training Step 17/59: batchLoss = 2.7863, diffLoss = 13.5868, kgLoss = 0.0861
2025-02-13 00:18:08.078722: Training Step 18/59: batchLoss = 3.1512, diffLoss = 15.3902, kgLoss = 0.0914
2025-02-13 00:18:09.003500: Training Step 19/59: batchLoss = 2.9335, diffLoss = 14.3372, kgLoss = 0.0826
2025-02-13 00:18:09.935477: Training Step 20/59: batchLoss = 2.6832, diffLoss = 13.0996, kgLoss = 0.0792
2025-02-13 00:18:10.855484: Training Step 21/59: batchLoss = 2.6313, diffLoss = 12.8605, kgLoss = 0.0740
2025-02-13 00:18:11.785764: Training Step 22/59: batchLoss = 3.0645, diffLoss = 14.9501, kgLoss = 0.0930
2025-02-13 00:18:12.706234: Training Step 23/59: batchLoss = 2.9206, diffLoss = 14.2593, kgLoss = 0.0859
2025-02-13 00:18:13.621443: Training Step 24/59: batchLoss = 2.6486, diffLoss = 12.9544, kgLoss = 0.0722
2025-02-13 00:18:14.547112: Training Step 25/59: batchLoss = 3.1241, diffLoss = 15.2803, kgLoss = 0.0851
2025-02-13 00:18:15.465431: Training Step 26/59: batchLoss = 2.7141, diffLoss = 13.2525, kgLoss = 0.0796
2025-02-13 00:18:16.393547: Training Step 27/59: batchLoss = 2.8653, diffLoss = 13.9961, kgLoss = 0.0826
2025-02-13 00:18:17.316573: Training Step 28/59: batchLoss = 3.0829, diffLoss = 15.0566, kgLoss = 0.0895
2025-02-13 00:18:18.239373: Training Step 29/59: batchLoss = 3.0777, diffLoss = 14.9860, kgLoss = 0.1007
2025-02-13 00:18:19.160760: Training Step 30/59: batchLoss = 2.9042, diffLoss = 14.1906, kgLoss = 0.0826
2025-02-13 00:18:20.079728: Training Step 31/59: batchLoss = 2.9616, diffLoss = 14.4728, kgLoss = 0.0838
2025-02-13 00:18:21.005687: Training Step 32/59: batchLoss = 2.9023, diffLoss = 14.1822, kgLoss = 0.0823
2025-02-13 00:18:21.930931: Training Step 33/59: batchLoss = 3.2183, diffLoss = 15.7189, kgLoss = 0.0932
2025-02-13 00:18:22.865036: Training Step 34/59: batchLoss = 3.1078, diffLoss = 15.1805, kgLoss = 0.0897
2025-02-13 00:18:23.797564: Training Step 35/59: batchLoss = 3.0828, diffLoss = 15.0899, kgLoss = 0.0810
2025-02-13 00:18:24.725029: Training Step 36/59: batchLoss = 2.8978, diffLoss = 14.1566, kgLoss = 0.0831
2025-02-13 00:18:25.653245: Training Step 37/59: batchLoss = 2.7999, diffLoss = 13.6533, kgLoss = 0.0866
2025-02-13 00:18:26.581646: Training Step 38/59: batchLoss = 2.7636, diffLoss = 13.4825, kgLoss = 0.0839
2025-02-13 00:18:27.509826: Training Step 39/59: batchLoss = 2.9505, diffLoss = 14.3868, kgLoss = 0.0914
2025-02-13 00:18:28.445511: Training Step 40/59: batchLoss = 3.1404, diffLoss = 15.3348, kgLoss = 0.0918
2025-02-13 00:18:29.374583: Training Step 41/59: batchLoss = 3.1376, diffLoss = 15.3115, kgLoss = 0.0941
2025-02-13 00:18:30.313046: Training Step 42/59: batchLoss = 2.9717, diffLoss = 14.5119, kgLoss = 0.0867
2025-02-13 00:18:31.240868: Training Step 43/59: batchLoss = 3.4018, diffLoss = 16.5953, kgLoss = 0.1034
2025-02-13 00:18:32.172186: Training Step 44/59: batchLoss = 3.2496, diffLoss = 15.8762, kgLoss = 0.0930
2025-02-13 00:18:33.101032: Training Step 45/59: batchLoss = 2.7397, diffLoss = 13.3797, kgLoss = 0.0797
2025-02-13 00:18:34.031623: Training Step 46/59: batchLoss = 2.7473, diffLoss = 13.4191, kgLoss = 0.0794
2025-02-13 00:18:34.956322: Training Step 47/59: batchLoss = 3.2317, diffLoss = 15.7738, kgLoss = 0.0962
2025-02-13 00:18:35.879125: Training Step 48/59: batchLoss = 2.9119, diffLoss = 14.2304, kgLoss = 0.0823
2025-02-13 00:18:36.807096: Training Step 49/59: batchLoss = 2.7337, diffLoss = 13.3740, kgLoss = 0.0736
2025-02-13 00:18:37.736578: Training Step 50/59: batchLoss = 3.2014, diffLoss = 15.6526, kgLoss = 0.0886
2025-02-13 00:18:38.665595: Training Step 51/59: batchLoss = 3.0250, diffLoss = 14.8039, kgLoss = 0.0803
2025-02-13 00:18:39.615091: Training Step 52/59: batchLoss = 2.7990, diffLoss = 13.6500, kgLoss = 0.0863
2025-02-13 00:18:40.532783: Training Step 53/59: batchLoss = 3.1455, diffLoss = 15.3528, kgLoss = 0.0936
2025-02-13 00:18:41.466633: Training Step 54/59: batchLoss = 2.7250, diffLoss = 13.2889, kgLoss = 0.0840
2025-02-13 00:18:42.401763: Training Step 55/59: batchLoss = 3.2260, diffLoss = 15.7651, kgLoss = 0.0912
2025-02-13 00:18:43.341121: Training Step 56/59: batchLoss = 3.0449, diffLoss = 14.8820, kgLoss = 0.0856
2025-02-13 00:18:44.190736: Training Step 57/59: batchLoss = 3.0308, diffLoss = 14.8241, kgLoss = 0.0825
2025-02-13 00:18:45.038575: Training Step 58/59: batchLoss = 2.9867, diffLoss = 14.5916, kgLoss = 0.0855
2025-02-13 00:18:45.132616: 
2025-02-13 00:18:45.132940: Epoch 71/1000, Train: epLoss = 0.4332, epDfLoss = 2.1157, epKgLoss = 0.0126  
2025-02-13 00:18:46.611140: Steps 0/47: batch_recall = 36.11, batch_ndcg = 47.26 
2025-02-13 00:18:47.905264: Steps 1/47: batch_recall = 37.33, batch_ndcg = 42.61 
2025-02-13 00:18:49.153158: Steps 2/47: batch_recall = 40.71, batch_ndcg = 47.27 
2025-02-13 00:18:50.409465: Steps 3/47: batch_recall = 43.38, batch_ndcg = 45.88 
2025-02-13 00:18:51.589983: Steps 4/47: batch_recall = 39.46, batch_ndcg = 46.62 
2025-02-13 00:18:52.786517: Steps 5/47: batch_recall = 33.39, batch_ndcg = 40.00 
2025-02-13 00:18:53.961313: Steps 6/47: batch_recall = 40.14, batch_ndcg = 42.93 
2025-02-13 00:18:55.118203: Steps 7/47: batch_recall = 42.50, batch_ndcg = 44.72 
2025-02-13 00:18:56.287695: Steps 8/47: batch_recall = 47.71, batch_ndcg = 53.22 
2025-02-13 00:18:57.420007: Steps 9/47: batch_recall = 46.67, batch_ndcg = 47.23 
2025-02-13 00:18:58.572945: Steps 10/47: batch_recall = 43.19, batch_ndcg = 43.46 
2025-02-13 00:18:59.711027: Steps 11/47: batch_recall = 55.25, batch_ndcg = 52.52 
2025-02-13 00:19:00.851801: Steps 12/47: batch_recall = 49.98, batch_ndcg = 48.86 
2025-02-13 00:19:01.984599: Steps 13/47: batch_recall = 49.28, batch_ndcg = 45.49 
2025-02-13 00:19:03.082943: Steps 14/47: batch_recall = 41.05, batch_ndcg = 41.34 
2025-02-13 00:19:04.180509: Steps 15/47: batch_recall = 57.29, batch_ndcg = 53.96 
2025-02-13 00:19:05.270263: Steps 16/47: batch_recall = 51.00, batch_ndcg = 46.89 
2025-02-13 00:19:06.326043: Steps 17/47: batch_recall = 58.37, batch_ndcg = 50.14 
2025-02-13 00:19:07.404534: Steps 18/47: batch_recall = 53.02, batch_ndcg = 49.02 
2025-02-13 00:19:08.479529: Steps 19/47: batch_recall = 61.71, batch_ndcg = 55.94 
2025-02-13 00:19:09.530983: Steps 20/47: batch_recall = 67.81, batch_ndcg = 62.41 
2025-02-13 00:19:10.587390: Steps 21/47: batch_recall = 65.02, batch_ndcg = 56.16 
2025-02-13 00:19:11.646255: Steps 22/47: batch_recall = 55.11, batch_ndcg = 51.14 
2025-02-13 00:19:12.685477: Steps 23/47: batch_recall = 63.44, batch_ndcg = 55.59 
2025-02-13 00:19:13.739301: Steps 24/47: batch_recall = 66.61, batch_ndcg = 58.60 
2025-02-13 00:19:14.772001: Steps 25/47: batch_recall = 64.83, batch_ndcg = 57.26 
2025-02-13 00:19:15.782252: Steps 26/47: batch_recall = 60.02, batch_ndcg = 53.67 
2025-02-13 00:19:16.812334: Steps 27/47: batch_recall = 63.26, batch_ndcg = 53.67 
2025-02-13 00:19:17.825935: Steps 28/47: batch_recall = 70.60, batch_ndcg = 60.23 
2025-02-13 00:19:18.833794: Steps 29/47: batch_recall = 72.08, batch_ndcg = 60.09 
2025-02-13 00:19:19.838992: Steps 30/47: batch_recall = 72.67, batch_ndcg = 64.16 
2025-02-13 00:19:20.868912: Steps 31/47: batch_recall = 66.94, batch_ndcg = 56.44 
2025-02-13 00:19:21.871786: Steps 32/47: batch_recall = 70.50, batch_ndcg = 64.90 
2025-02-13 00:19:22.874654: Steps 33/47: batch_recall = 80.73, batch_ndcg = 68.96 
2025-02-13 00:19:23.889478: Steps 34/47: batch_recall = 69.49, batch_ndcg = 56.99 
2025-02-13 00:19:24.879768: Steps 35/47: batch_recall = 75.87, batch_ndcg = 63.83 
2025-02-13 00:19:25.877985: Steps 36/47: batch_recall = 78.90, batch_ndcg = 64.41 
2025-02-13 00:19:26.855473: Steps 37/47: batch_recall = 85.20, batch_ndcg = 73.23 
2025-02-13 00:19:27.845002: Steps 38/47: batch_recall = 90.00, batch_ndcg = 72.35 
2025-02-13 00:19:28.828995: Steps 39/47: batch_recall = 88.54, batch_ndcg = 69.69 
2025-02-13 00:19:29.806060: Steps 40/47: batch_recall = 72.15, batch_ndcg = 61.78 
2025-02-13 00:19:30.774245: Steps 41/47: batch_recall = 87.49, batch_ndcg = 72.66 
2025-02-13 00:19:31.739258: Steps 42/47: batch_recall = 82.64, batch_ndcg = 65.39 
2025-02-13 00:19:32.696016: Steps 43/47: batch_recall = 90.04, batch_ndcg = 72.42 
2025-02-13 00:19:33.663258: Steps 44/47: batch_recall = 87.21, batch_ndcg = 68.75 
2025-02-13 00:19:34.586317: Steps 45/47: batch_recall = 92.55, batch_ndcg = 74.44 
2025-02-13 00:19:34.691681: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.62 
2025-02-13 00:19:34.691817: Epoch 71/1000, Test: Recall = 0.1218, NDCG = 0.1097  

2025-02-13 00:19:35.882801: Training Step 0/59: batchLoss = 2.8538, diffLoss = 13.8765, kgLoss = 0.0982
2025-02-13 00:19:36.810687: Training Step 1/59: batchLoss = 2.7779, diffLoss = 13.5375, kgLoss = 0.0880
2025-02-13 00:19:37.731964: Training Step 2/59: batchLoss = 2.9030, diffLoss = 14.1599, kgLoss = 0.0887
2025-02-13 00:19:38.661333: Training Step 3/59: batchLoss = 2.8784, diffLoss = 14.0414, kgLoss = 0.0876
2025-02-13 00:19:39.579801: Training Step 4/59: batchLoss = 2.5021, diffLoss = 12.1914, kgLoss = 0.0798
2025-02-13 00:19:40.508564: Training Step 5/59: batchLoss = 3.0015, diffLoss = 14.6200, kgLoss = 0.0969
2025-02-13 00:19:41.449671: Training Step 6/59: batchLoss = 2.6641, diffLoss = 12.9789, kgLoss = 0.0855
2025-02-13 00:19:42.376351: Training Step 7/59: batchLoss = 2.9864, diffLoss = 14.5781, kgLoss = 0.0885
2025-02-13 00:19:43.301250: Training Step 8/59: batchLoss = 3.0178, diffLoss = 14.7467, kgLoss = 0.0856
2025-02-13 00:19:44.222690: Training Step 9/59: batchLoss = 3.0626, diffLoss = 14.9137, kgLoss = 0.0999
2025-02-13 00:19:45.148593: Training Step 10/59: batchLoss = 2.5941, diffLoss = 12.6750, kgLoss = 0.0738
2025-02-13 00:19:46.075078: Training Step 11/59: batchLoss = 2.8106, diffLoss = 13.7408, kgLoss = 0.0780
2025-02-13 00:19:47.006682: Training Step 12/59: batchLoss = 2.8521, diffLoss = 13.9102, kgLoss = 0.0876
2025-02-13 00:19:47.945602: Training Step 13/59: batchLoss = 3.0631, diffLoss = 14.9886, kgLoss = 0.0817
2025-02-13 00:19:48.880353: Training Step 14/59: batchLoss = 2.9184, diffLoss = 14.2091, kgLoss = 0.0958
2025-02-13 00:19:49.819971: Training Step 15/59: batchLoss = 2.7878, diffLoss = 13.6296, kgLoss = 0.0774
2025-02-13 00:19:50.756830: Training Step 16/59: batchLoss = 3.1274, diffLoss = 15.2618, kgLoss = 0.0938
2025-02-13 00:19:51.694439: Training Step 17/59: batchLoss = 2.8021, diffLoss = 13.7143, kgLoss = 0.0740
2025-02-13 00:19:52.630415: Training Step 18/59: batchLoss = 3.1043, diffLoss = 15.1158, kgLoss = 0.1014
2025-02-13 00:19:53.564285: Training Step 19/59: batchLoss = 2.6376, diffLoss = 12.8607, kgLoss = 0.0819
2025-02-13 00:19:54.486724: Training Step 20/59: batchLoss = 2.9594, diffLoss = 14.4292, kgLoss = 0.0919
2025-02-13 00:19:55.427357: Training Step 21/59: batchLoss = 2.8041, diffLoss = 13.6841, kgLoss = 0.0841
2025-02-13 00:19:56.351170: Training Step 22/59: batchLoss = 2.7001, diffLoss = 13.1793, kgLoss = 0.0803
2025-02-13 00:19:57.290731: Training Step 23/59: batchLoss = 2.9251, diffLoss = 14.2846, kgLoss = 0.0852
2025-02-13 00:19:58.212263: Training Step 24/59: batchLoss = 3.0079, diffLoss = 14.6696, kgLoss = 0.0924
2025-02-13 00:19:59.142312: Training Step 25/59: batchLoss = 2.9546, diffLoss = 14.4135, kgLoss = 0.0898
2025-02-13 00:20:00.069495: Training Step 26/59: batchLoss = 3.1732, diffLoss = 15.4916, kgLoss = 0.0936
2025-02-13 00:20:00.994261: Training Step 27/59: batchLoss = 2.6574, diffLoss = 12.9744, kgLoss = 0.0782
2025-02-13 00:20:01.927444: Training Step 28/59: batchLoss = 2.9464, diffLoss = 14.4068, kgLoss = 0.0813
2025-02-13 00:20:02.854700: Training Step 29/59: batchLoss = 3.0374, diffLoss = 14.8353, kgLoss = 0.0880
2025-02-13 00:20:03.779079: Training Step 30/59: batchLoss = 2.9095, diffLoss = 14.1889, kgLoss = 0.0897
2025-02-13 00:20:04.701872: Training Step 31/59: batchLoss = 2.8581, diffLoss = 13.9507, kgLoss = 0.0850
2025-02-13 00:20:05.647562: Training Step 32/59: batchLoss = 3.0812, diffLoss = 15.0483, kgLoss = 0.0894
2025-02-13 00:20:06.586784: Training Step 33/59: batchLoss = 2.9295, diffLoss = 14.3207, kgLoss = 0.0817
2025-02-13 00:20:07.521184: Training Step 34/59: batchLoss = 3.3097, diffLoss = 16.1821, kgLoss = 0.0916
2025-02-13 00:20:08.462829: Training Step 35/59: batchLoss = 2.7160, diffLoss = 13.2885, kgLoss = 0.0728
2025-02-13 00:20:09.398289: Training Step 36/59: batchLoss = 2.6853, diffLoss = 13.1111, kgLoss = 0.0788
2025-02-13 00:20:10.327666: Training Step 37/59: batchLoss = 2.7391, diffLoss = 13.3901, kgLoss = 0.0764
2025-02-13 00:20:11.246195: Training Step 38/59: batchLoss = 2.9889, diffLoss = 14.6232, kgLoss = 0.0804
2025-02-13 00:20:12.184410: Training Step 39/59: batchLoss = 2.7585, diffLoss = 13.4946, kgLoss = 0.0744
2025-02-13 00:20:13.125190: Training Step 40/59: batchLoss = 2.9157, diffLoss = 14.2107, kgLoss = 0.0919
2025-02-13 00:20:14.062989: Training Step 41/59: batchLoss = 2.8573, diffLoss = 13.9628, kgLoss = 0.0809
2025-02-13 00:20:14.993813: Training Step 42/59: batchLoss = 2.8580, diffLoss = 13.9645, kgLoss = 0.0813
2025-02-13 00:20:15.924733: Training Step 43/59: batchLoss = 3.1724, diffLoss = 15.5154, kgLoss = 0.0867
2025-02-13 00:20:16.849169: Training Step 44/59: batchLoss = 3.1772, diffLoss = 15.5268, kgLoss = 0.0899
2025-02-13 00:20:17.775214: Training Step 45/59: batchLoss = 2.8210, diffLoss = 13.7752, kgLoss = 0.0825
2025-02-13 00:20:18.704587: Training Step 46/59: batchLoss = 3.0223, diffLoss = 14.7391, kgLoss = 0.0930
2025-02-13 00:20:19.641518: Training Step 47/59: batchLoss = 2.8958, diffLoss = 14.1625, kgLoss = 0.0791
2025-02-13 00:20:20.567929: Training Step 48/59: batchLoss = 3.0499, diffLoss = 14.9151, kgLoss = 0.0837
2025-02-13 00:20:21.495788: Training Step 49/59: batchLoss = 2.9878, diffLoss = 14.5862, kgLoss = 0.0882
2025-02-13 00:20:22.427279: Training Step 50/59: batchLoss = 3.0309, diffLoss = 14.8028, kgLoss = 0.0879
2025-02-13 00:20:23.350382: Training Step 51/59: batchLoss = 3.0047, diffLoss = 14.6997, kgLoss = 0.0810
2025-02-13 00:20:24.278384: Training Step 52/59: batchLoss = 3.0624, diffLoss = 14.9707, kgLoss = 0.0853
2025-02-13 00:20:25.200272: Training Step 53/59: batchLoss = 2.9091, diffLoss = 14.2167, kgLoss = 0.0823
2025-02-13 00:20:26.120499: Training Step 54/59: batchLoss = 2.7022, diffLoss = 13.1606, kgLoss = 0.0876
2025-02-13 00:20:27.050837: Training Step 55/59: batchLoss = 3.0653, diffLoss = 14.9736, kgLoss = 0.0882
2025-02-13 00:20:27.978348: Training Step 56/59: batchLoss = 3.1653, diffLoss = 15.4869, kgLoss = 0.0849
2025-02-13 00:20:28.826671: Training Step 57/59: batchLoss = 2.8529, diffLoss = 13.9386, kgLoss = 0.0815
2025-02-13 00:20:29.689342: Training Step 58/59: batchLoss = 3.0257, diffLoss = 14.7946, kgLoss = 0.0835
2025-02-13 00:20:29.789726: 
2025-02-13 00:20:29.790090: Epoch 72/1000, Train: epLoss = 0.4302, epDfLoss = 2.1003, epKgLoss = 0.0126  
2025-02-13 00:20:31.269286: Steps 0/47: batch_recall = 37.72, batch_ndcg = 47.38 
2025-02-13 00:20:32.566655: Steps 1/47: batch_recall = 36.77, batch_ndcg = 42.44 
2025-02-13 00:20:33.819765: Steps 2/47: batch_recall = 40.61, batch_ndcg = 46.76 
2025-02-13 00:20:35.083995: Steps 3/47: batch_recall = 44.01, batch_ndcg = 46.09 
2025-02-13 00:20:36.272884: Steps 4/47: batch_recall = 39.66, batch_ndcg = 46.10 
2025-02-13 00:20:37.481647: Steps 5/47: batch_recall = 33.13, batch_ndcg = 40.41 
2025-02-13 00:20:38.668254: Steps 6/47: batch_recall = 40.24, batch_ndcg = 42.96 
2025-02-13 00:20:39.838379: Steps 7/47: batch_recall = 43.84, batch_ndcg = 45.16 
2025-02-13 00:20:40.998930: Steps 8/47: batch_recall = 47.50, batch_ndcg = 51.64 
2025-02-13 00:20:42.134654: Steps 9/47: batch_recall = 46.15, batch_ndcg = 46.57 
2025-02-13 00:20:43.292380: Steps 10/47: batch_recall = 42.65, batch_ndcg = 42.84 
2025-02-13 00:20:44.418288: Steps 11/47: batch_recall = 54.63, batch_ndcg = 52.19 
2025-02-13 00:20:45.558066: Steps 12/47: batch_recall = 50.15, batch_ndcg = 48.13 
2025-02-13 00:20:46.691545: Steps 13/47: batch_recall = 49.08, batch_ndcg = 45.82 
2025-02-13 00:20:47.792463: Steps 14/47: batch_recall = 41.40, batch_ndcg = 41.73 
2025-02-13 00:20:48.900333: Steps 15/47: batch_recall = 57.22, batch_ndcg = 53.68 
2025-02-13 00:20:49.997842: Steps 16/47: batch_recall = 51.01, batch_ndcg = 46.99 
2025-02-13 00:20:51.054675: Steps 17/47: batch_recall = 58.81, batch_ndcg = 50.21 
2025-02-13 00:20:52.141397: Steps 18/47: batch_recall = 53.21, batch_ndcg = 49.95 
2025-02-13 00:20:53.208033: Steps 19/47: batch_recall = 59.46, batch_ndcg = 54.99 
2025-02-13 00:20:54.260781: Steps 20/47: batch_recall = 67.30, batch_ndcg = 61.51 
2025-02-13 00:20:55.318143: Steps 21/47: batch_recall = 63.49, batch_ndcg = 55.24 
2025-02-13 00:20:56.376425: Steps 22/47: batch_recall = 54.80, batch_ndcg = 50.80 
2025-02-13 00:20:57.415703: Steps 23/47: batch_recall = 63.26, batch_ndcg = 55.04 
2025-02-13 00:20:58.462952: Steps 24/47: batch_recall = 64.14, batch_ndcg = 57.62 
2025-02-13 00:20:59.501342: Steps 25/47: batch_recall = 66.56, batch_ndcg = 58.19 
2025-02-13 00:21:00.511769: Steps 26/47: batch_recall = 58.05, batch_ndcg = 53.37 
2025-02-13 00:21:01.540506: Steps 27/47: batch_recall = 63.15, batch_ndcg = 53.92 
2025-02-13 00:21:02.558040: Steps 28/47: batch_recall = 70.54, batch_ndcg = 59.68 
2025-02-13 00:21:03.564148: Steps 29/47: batch_recall = 70.61, batch_ndcg = 59.30 
2025-02-13 00:21:04.558938: Steps 30/47: batch_recall = 72.62, batch_ndcg = 63.33 
2025-02-13 00:21:05.587931: Steps 31/47: batch_recall = 66.55, batch_ndcg = 56.14 
2025-02-13 00:21:06.587032: Steps 32/47: batch_recall = 70.32, batch_ndcg = 63.70 
2025-02-13 00:21:07.595683: Steps 33/47: batch_recall = 80.54, batch_ndcg = 68.44 
2025-02-13 00:21:08.618575: Steps 34/47: batch_recall = 70.31, batch_ndcg = 57.93 
2025-02-13 00:21:09.611579: Steps 35/47: batch_recall = 76.15, batch_ndcg = 64.49 
2025-02-13 00:21:10.614400: Steps 36/47: batch_recall = 79.24, batch_ndcg = 64.55 
2025-02-13 00:21:11.595274: Steps 37/47: batch_recall = 83.53, batch_ndcg = 72.49 
2025-02-13 00:21:12.581593: Steps 38/47: batch_recall = 91.53, batch_ndcg = 73.72 
2025-02-13 00:21:13.564712: Steps 39/47: batch_recall = 87.94, batch_ndcg = 69.15 
2025-02-13 00:21:14.544914: Steps 40/47: batch_recall = 74.01, batch_ndcg = 62.72 
2025-02-13 00:21:15.511248: Steps 41/47: batch_recall = 86.79, batch_ndcg = 72.58 
2025-02-13 00:21:16.485454: Steps 42/47: batch_recall = 81.41, batch_ndcg = 64.18 
2025-02-13 00:21:17.444027: Steps 43/47: batch_recall = 90.37, batch_ndcg = 72.24 
2025-02-13 00:21:18.407475: Steps 44/47: batch_recall = 87.10, batch_ndcg = 69.79 
2025-02-13 00:21:19.337036: Steps 45/47: batch_recall = 93.55, batch_ndcg = 74.70 
2025-02-13 00:21:19.444582: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.61 
2025-02-13 00:21:19.444711: Epoch 72/1000, Test: Recall = 0.1215, NDCG = 0.1094  

2025-02-13 00:21:20.639537: Training Step 0/59: batchLoss = 2.8563, diffLoss = 13.9116, kgLoss = 0.0925
2025-02-13 00:21:21.561147: Training Step 1/59: batchLoss = 2.7094, diffLoss = 13.2102, kgLoss = 0.0842
2025-02-13 00:21:22.482270: Training Step 2/59: batchLoss = 2.7840, diffLoss = 13.5737, kgLoss = 0.0866
2025-02-13 00:21:23.412200: Training Step 3/59: batchLoss = 3.0056, diffLoss = 14.6658, kgLoss = 0.0906
2025-02-13 00:21:24.331052: Training Step 4/59: batchLoss = 2.9039, diffLoss = 14.1437, kgLoss = 0.0940
2025-02-13 00:21:25.246426: Training Step 5/59: batchLoss = 2.6602, diffLoss = 12.9727, kgLoss = 0.0821
2025-02-13 00:21:26.170715: Training Step 6/59: batchLoss = 2.6154, diffLoss = 12.7503, kgLoss = 0.0817
2025-02-13 00:21:27.087410: Training Step 7/59: batchLoss = 2.7123, diffLoss = 13.2548, kgLoss = 0.0767
2025-02-13 00:21:28.018659: Training Step 8/59: batchLoss = 2.5606, diffLoss = 12.4800, kgLoss = 0.0807
2025-02-13 00:21:28.952816: Training Step 9/59: batchLoss = 3.2819, diffLoss = 16.0308, kgLoss = 0.0947
2025-02-13 00:21:29.886836: Training Step 10/59: batchLoss = 2.5220, diffLoss = 12.3161, kgLoss = 0.0734
2025-02-13 00:21:30.820552: Training Step 11/59: batchLoss = 2.8436, diffLoss = 13.8940, kgLoss = 0.0810
2025-02-13 00:21:31.749531: Training Step 12/59: batchLoss = 2.8773, diffLoss = 14.0335, kgLoss = 0.0882
2025-02-13 00:21:32.695527: Training Step 13/59: batchLoss = 2.9459, diffLoss = 14.4009, kgLoss = 0.0821
2025-02-13 00:21:33.626205: Training Step 14/59: batchLoss = 2.9395, diffLoss = 14.3456, kgLoss = 0.0879
2025-02-13 00:21:34.565241: Training Step 15/59: batchLoss = 3.0307, diffLoss = 14.7926, kgLoss = 0.0902
2025-02-13 00:21:35.507439: Training Step 16/59: batchLoss = 3.1978, diffLoss = 15.6142, kgLoss = 0.0937
2025-02-13 00:21:36.445895: Training Step 17/59: batchLoss = 3.1313, diffLoss = 15.3132, kgLoss = 0.0858
2025-02-13 00:21:37.384651: Training Step 18/59: batchLoss = 2.6781, diffLoss = 13.0790, kgLoss = 0.0779
2025-02-13 00:21:38.321938: Training Step 19/59: batchLoss = 2.9458, diffLoss = 14.4029, kgLoss = 0.0815
2025-02-13 00:21:39.254667: Training Step 20/59: batchLoss = 2.8704, diffLoss = 14.0325, kgLoss = 0.0798
2025-02-13 00:21:40.183375: Training Step 21/59: batchLoss = 3.1423, diffLoss = 15.3447, kgLoss = 0.0917
2025-02-13 00:21:41.115941: Training Step 22/59: batchLoss = 3.0048, diffLoss = 14.7008, kgLoss = 0.0809
2025-02-13 00:21:42.045511: Training Step 23/59: batchLoss = 3.2050, diffLoss = 15.6305, kgLoss = 0.0986
2025-02-13 00:21:42.978106: Training Step 24/59: batchLoss = 2.7125, diffLoss = 13.2481, kgLoss = 0.0786
2025-02-13 00:21:43.904070: Training Step 25/59: batchLoss = 2.9899, diffLoss = 14.6011, kgLoss = 0.0871
2025-02-13 00:21:44.831719: Training Step 26/59: batchLoss = 2.8475, diffLoss = 13.9213, kgLoss = 0.0791
2025-02-13 00:21:45.756040: Training Step 27/59: batchLoss = 3.0898, diffLoss = 15.0947, kgLoss = 0.0885
2025-02-13 00:21:46.683586: Training Step 28/59: batchLoss = 3.1160, diffLoss = 15.2187, kgLoss = 0.0904
2025-02-13 00:21:47.607866: Training Step 29/59: batchLoss = 2.8648, diffLoss = 13.9794, kgLoss = 0.0861
2025-02-13 00:21:48.534550: Training Step 30/59: batchLoss = 2.6774, diffLoss = 13.0749, kgLoss = 0.0780
2025-02-13 00:21:49.461240: Training Step 31/59: batchLoss = 3.2475, diffLoss = 15.8548, kgLoss = 0.0957
2025-02-13 00:21:50.392895: Training Step 32/59: batchLoss = 2.7156, diffLoss = 13.2972, kgLoss = 0.0702
2025-02-13 00:21:51.330894: Training Step 33/59: batchLoss = 3.0509, diffLoss = 14.9194, kgLoss = 0.0837
2025-02-13 00:21:52.275208: Training Step 34/59: batchLoss = 2.9414, diffLoss = 14.3645, kgLoss = 0.0856
2025-02-13 00:21:53.222830: Training Step 35/59: batchLoss = 2.9257, diffLoss = 14.3001, kgLoss = 0.0820
2025-02-13 00:21:54.156486: Training Step 36/59: batchLoss = 2.9724, diffLoss = 14.5171, kgLoss = 0.0862
2025-02-13 00:21:55.088010: Training Step 37/59: batchLoss = 3.2161, diffLoss = 15.7065, kgLoss = 0.0935
2025-02-13 00:21:56.016237: Training Step 38/59: batchLoss = 3.5045, diffLoss = 17.1360, kgLoss = 0.0966
2025-02-13 00:21:56.946457: Training Step 39/59: batchLoss = 2.7308, diffLoss = 13.3458, kgLoss = 0.0771
2025-02-13 00:21:57.882701: Training Step 40/59: batchLoss = 2.7228, diffLoss = 13.3192, kgLoss = 0.0737
2025-02-13 00:21:58.815351: Training Step 41/59: batchLoss = 3.6574, diffLoss = 17.8938, kgLoss = 0.0983
2025-02-13 00:21:59.746074: Training Step 42/59: batchLoss = 3.0172, diffLoss = 14.7519, kgLoss = 0.0835
2025-02-13 00:22:00.677856: Training Step 43/59: batchLoss = 3.5111, diffLoss = 17.1786, kgLoss = 0.0942
2025-02-13 00:22:01.619479: Training Step 44/59: batchLoss = 3.0092, diffLoss = 14.7060, kgLoss = 0.0850
2025-02-13 00:22:02.547731: Training Step 45/59: batchLoss = 2.9881, diffLoss = 14.5827, kgLoss = 0.0895
2025-02-13 00:22:03.473061: Training Step 46/59: batchLoss = 3.0285, diffLoss = 14.7746, kgLoss = 0.0920
2025-02-13 00:22:04.390177: Training Step 47/59: batchLoss = 2.9672, diffLoss = 14.5024, kgLoss = 0.0834
2025-02-13 00:22:05.315210: Training Step 48/59: batchLoss = 2.8140, diffLoss = 13.7321, kgLoss = 0.0844
2025-02-13 00:22:06.244585: Training Step 49/59: batchLoss = 3.2164, diffLoss = 15.7062, kgLoss = 0.0940
2025-02-13 00:22:07.167990: Training Step 50/59: batchLoss = 2.9781, diffLoss = 14.5615, kgLoss = 0.0822
2025-02-13 00:22:08.086338: Training Step 51/59: batchLoss = 2.9147, diffLoss = 14.2355, kgLoss = 0.0845
2025-02-13 00:22:09.014165: Training Step 52/59: batchLoss = 2.8793, diffLoss = 14.0632, kgLoss = 0.0834
2025-02-13 00:22:09.941194: Training Step 53/59: batchLoss = 3.0621, diffLoss = 14.9387, kgLoss = 0.0929
2025-02-13 00:22:10.870040: Training Step 54/59: batchLoss = 3.0014, diffLoss = 14.6601, kgLoss = 0.0867
2025-02-13 00:22:11.805705: Training Step 55/59: batchLoss = 3.1048, diffLoss = 15.1755, kgLoss = 0.0872
2025-02-13 00:22:12.733045: Training Step 56/59: batchLoss = 2.9054, diffLoss = 14.2091, kgLoss = 0.0794
2025-02-13 00:22:13.575773: Training Step 57/59: batchLoss = 2.8515, diffLoss = 13.9414, kgLoss = 0.0790
2025-02-13 00:22:14.432529: Training Step 58/59: batchLoss = 2.6859, diffLoss = 13.1169, kgLoss = 0.0781
2025-02-13 00:22:14.528037: 
2025-02-13 00:22:14.528410: Epoch 73/1000, Train: epLoss = 0.4359, epDfLoss = 2.1288, epKgLoss = 0.0126  
2025-02-13 00:22:16.019615: Steps 0/47: batch_recall = 37.99, batch_ndcg = 48.54 
2025-02-13 00:22:17.323293: Steps 1/47: batch_recall = 37.30, batch_ndcg = 42.77 
2025-02-13 00:22:18.571225: Steps 2/47: batch_recall = 40.68, batch_ndcg = 46.47 
2025-02-13 00:22:19.838403: Steps 3/47: batch_recall = 43.83, batch_ndcg = 46.45 
2025-02-13 00:22:21.031522: Steps 4/47: batch_recall = 39.90, batch_ndcg = 46.48 
2025-02-13 00:22:22.239305: Steps 5/47: batch_recall = 34.62, batch_ndcg = 40.61 
2025-02-13 00:22:23.441797: Steps 6/47: batch_recall = 40.05, batch_ndcg = 42.85 
2025-02-13 00:22:24.608519: Steps 7/47: batch_recall = 43.62, batch_ndcg = 45.20 
2025-02-13 00:22:25.778021: Steps 8/47: batch_recall = 47.78, batch_ndcg = 52.37 
2025-02-13 00:22:26.901191: Steps 9/47: batch_recall = 47.01, batch_ndcg = 46.97 
2025-02-13 00:22:28.145348: Steps 10/47: batch_recall = 43.48, batch_ndcg = 43.18 
2025-02-13 00:22:29.271339: Steps 11/47: batch_recall = 54.30, batch_ndcg = 52.18 
2025-02-13 00:22:30.405516: Steps 12/47: batch_recall = 50.20, batch_ndcg = 49.25 
2025-02-13 00:22:31.536457: Steps 13/47: batch_recall = 48.92, batch_ndcg = 46.06 
2025-02-13 00:22:32.636857: Steps 14/47: batch_recall = 41.46, batch_ndcg = 41.92 
2025-02-13 00:22:33.734064: Steps 15/47: batch_recall = 57.12, batch_ndcg = 53.98 
2025-02-13 00:22:34.820594: Steps 16/47: batch_recall = 51.85, batch_ndcg = 48.40 
2025-02-13 00:22:35.871230: Steps 17/47: batch_recall = 58.87, batch_ndcg = 50.15 
2025-02-13 00:22:36.955850: Steps 18/47: batch_recall = 52.72, batch_ndcg = 49.04 
2025-02-13 00:22:38.036486: Steps 19/47: batch_recall = 60.47, batch_ndcg = 55.48 
2025-02-13 00:22:39.092395: Steps 20/47: batch_recall = 67.99, batch_ndcg = 61.89 
2025-02-13 00:22:40.152292: Steps 21/47: batch_recall = 62.88, batch_ndcg = 54.65 
2025-02-13 00:22:41.213884: Steps 22/47: batch_recall = 54.88, batch_ndcg = 51.24 
2025-02-13 00:22:42.265867: Steps 23/47: batch_recall = 64.84, batch_ndcg = 55.75 
2025-02-13 00:22:43.320532: Steps 24/47: batch_recall = 64.20, batch_ndcg = 57.91 
2025-02-13 00:22:44.364032: Steps 25/47: batch_recall = 65.67, batch_ndcg = 57.38 
2025-02-13 00:22:45.374054: Steps 26/47: batch_recall = 59.95, batch_ndcg = 54.64 
2025-02-13 00:22:46.400531: Steps 27/47: batch_recall = 61.06, batch_ndcg = 52.95 
2025-02-13 00:22:47.416910: Steps 28/47: batch_recall = 70.59, batch_ndcg = 60.05 
2025-02-13 00:22:48.417839: Steps 29/47: batch_recall = 71.59, batch_ndcg = 59.68 
2025-02-13 00:22:49.424139: Steps 30/47: batch_recall = 72.68, batch_ndcg = 64.01 
2025-02-13 00:22:50.447338: Steps 31/47: batch_recall = 67.30, batch_ndcg = 55.77 
2025-02-13 00:22:51.455592: Steps 32/47: batch_recall = 70.72, batch_ndcg = 64.72 
2025-02-13 00:22:52.457280: Steps 33/47: batch_recall = 79.85, batch_ndcg = 68.00 
2025-02-13 00:22:53.472461: Steps 34/47: batch_recall = 69.79, batch_ndcg = 58.12 
2025-02-13 00:22:54.455685: Steps 35/47: batch_recall = 77.49, batch_ndcg = 64.99 
2025-02-13 00:22:55.454270: Steps 36/47: batch_recall = 79.52, batch_ndcg = 64.73 
2025-02-13 00:22:56.440158: Steps 37/47: batch_recall = 84.37, batch_ndcg = 72.48 
2025-02-13 00:22:57.419917: Steps 38/47: batch_recall = 88.94, batch_ndcg = 72.92 
2025-02-13 00:22:58.396120: Steps 39/47: batch_recall = 88.18, batch_ndcg = 68.70 
2025-02-13 00:22:59.384962: Steps 40/47: batch_recall = 71.97, batch_ndcg = 62.40 
2025-02-13 00:23:00.353946: Steps 41/47: batch_recall = 87.26, batch_ndcg = 72.38 
2025-02-13 00:23:01.315602: Steps 42/47: batch_recall = 81.20, batch_ndcg = 64.84 
2025-02-13 00:23:02.296506: Steps 43/47: batch_recall = 89.05, batch_ndcg = 71.63 
2025-02-13 00:23:03.253037: Steps 44/47: batch_recall = 88.53, batch_ndcg = 69.40 
2025-02-13 00:23:04.184074: Steps 45/47: batch_recall = 93.20, batch_ndcg = 74.34 
2025-02-13 00:23:04.284645: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.64 
2025-02-13 00:23:04.284769: Epoch 73/1000, Test: Recall = 0.1217, NDCG = 0.1097  

2025-02-13 00:23:05.464065: Training Step 0/59: batchLoss = 2.9665, diffLoss = 14.4455, kgLoss = 0.0967
2025-02-13 00:23:06.382017: Training Step 1/59: batchLoss = 2.8085, diffLoss = 13.7034, kgLoss = 0.0848
2025-02-13 00:23:07.307999: Training Step 2/59: batchLoss = 2.8488, diffLoss = 13.9116, kgLoss = 0.0831
2025-02-13 00:23:08.227719: Training Step 3/59: batchLoss = 2.8579, diffLoss = 13.9484, kgLoss = 0.0853
2025-02-13 00:23:09.142060: Training Step 4/59: batchLoss = 2.6579, diffLoss = 12.9623, kgLoss = 0.0818
2025-02-13 00:23:10.063074: Training Step 5/59: batchLoss = 2.8017, diffLoss = 13.6512, kgLoss = 0.0893
2025-02-13 00:23:10.982886: Training Step 6/59: batchLoss = 2.6436, diffLoss = 12.9006, kgLoss = 0.0794
2025-02-13 00:23:11.904787: Training Step 7/59: batchLoss = 2.6649, diffLoss = 12.9940, kgLoss = 0.0827
2025-02-13 00:23:12.823847: Training Step 8/59: batchLoss = 3.1391, diffLoss = 15.3001, kgLoss = 0.0988
2025-02-13 00:23:13.747176: Training Step 9/59: batchLoss = 3.0035, diffLoss = 14.6600, kgLoss = 0.0894
2025-02-13 00:23:14.676384: Training Step 10/59: batchLoss = 2.8045, diffLoss = 13.7177, kgLoss = 0.0762
2025-02-13 00:23:15.613154: Training Step 11/59: batchLoss = 2.8947, diffLoss = 14.1524, kgLoss = 0.0803
2025-02-13 00:23:16.545003: Training Step 12/59: batchLoss = 2.9698, diffLoss = 14.4910, kgLoss = 0.0894
2025-02-13 00:23:17.474827: Training Step 13/59: batchLoss = 2.9890, diffLoss = 14.5787, kgLoss = 0.0916
2025-02-13 00:23:18.400737: Training Step 14/59: batchLoss = 2.6897, diffLoss = 13.1186, kgLoss = 0.0825
2025-02-13 00:23:19.337440: Training Step 15/59: batchLoss = 2.6126, diffLoss = 12.7259, kgLoss = 0.0843
2025-02-13 00:23:20.268836: Training Step 16/59: batchLoss = 2.9009, diffLoss = 14.1834, kgLoss = 0.0802
2025-02-13 00:23:21.210105: Training Step 17/59: batchLoss = 2.8621, diffLoss = 13.9954, kgLoss = 0.0788
2025-02-13 00:23:22.150855: Training Step 18/59: batchLoss = 3.2228, diffLoss = 15.7225, kgLoss = 0.0978
2025-02-13 00:23:23.083356: Training Step 19/59: batchLoss = 3.0764, diffLoss = 15.0251, kgLoss = 0.0892
2025-02-13 00:23:24.015282: Training Step 20/59: batchLoss = 3.1479, diffLoss = 15.3626, kgLoss = 0.0943
2025-02-13 00:23:24.945715: Training Step 21/59: batchLoss = 3.0268, diffLoss = 14.7783, kgLoss = 0.0890
2025-02-13 00:23:25.865681: Training Step 22/59: batchLoss = 2.8499, diffLoss = 13.9421, kgLoss = 0.0769
2025-02-13 00:23:26.786468: Training Step 23/59: batchLoss = 2.9981, diffLoss = 14.6415, kgLoss = 0.0872
2025-02-13 00:23:27.706339: Training Step 24/59: batchLoss = 3.1041, diffLoss = 15.1793, kgLoss = 0.0853
2025-02-13 00:23:28.630442: Training Step 25/59: batchLoss = 2.9500, diffLoss = 14.4029, kgLoss = 0.0868
2025-02-13 00:23:29.547907: Training Step 26/59: batchLoss = 3.0063, diffLoss = 14.6725, kgLoss = 0.0898
2025-02-13 00:23:30.470984: Training Step 27/59: batchLoss = 2.7956, diffLoss = 13.6721, kgLoss = 0.0764
2025-02-13 00:23:31.394318: Training Step 28/59: batchLoss = 3.0442, diffLoss = 14.8746, kgLoss = 0.0866
2025-02-13 00:23:32.318457: Training Step 29/59: batchLoss = 3.1209, diffLoss = 15.2494, kgLoss = 0.0888
2025-02-13 00:23:33.247581: Training Step 30/59: batchLoss = 3.1764, diffLoss = 15.5186, kgLoss = 0.0909
2025-02-13 00:23:34.166276: Training Step 31/59: batchLoss = 2.8808, diffLoss = 14.0838, kgLoss = 0.0801
2025-02-13 00:23:35.090537: Training Step 32/59: batchLoss = 3.0639, diffLoss = 14.9745, kgLoss = 0.0862
2025-02-13 00:23:36.017304: Training Step 33/59: batchLoss = 2.8707, diffLoss = 14.0441, kgLoss = 0.0774
2025-02-13 00:23:36.951035: Training Step 34/59: batchLoss = 2.8758, diffLoss = 14.0445, kgLoss = 0.0836
2025-02-13 00:23:37.879276: Training Step 35/59: batchLoss = 3.1920, diffLoss = 15.6025, kgLoss = 0.0894
2025-02-13 00:23:38.809974: Training Step 36/59: batchLoss = 2.8339, diffLoss = 13.8353, kgLoss = 0.0835
2025-02-13 00:23:39.745168: Training Step 37/59: batchLoss = 2.8346, diffLoss = 13.8357, kgLoss = 0.0843
2025-02-13 00:23:40.679982: Training Step 38/59: batchLoss = 3.0349, diffLoss = 14.8253, kgLoss = 0.0873
2025-02-13 00:23:41.615831: Training Step 39/59: batchLoss = 2.9011, diffLoss = 14.1843, kgLoss = 0.0803
2025-02-13 00:23:42.551585: Training Step 40/59: batchLoss = 2.7377, diffLoss = 13.3903, kgLoss = 0.0745
2025-02-13 00:23:43.486270: Training Step 41/59: batchLoss = 3.1458, diffLoss = 15.3657, kgLoss = 0.0908
2025-02-13 00:23:44.420458: Training Step 42/59: batchLoss = 3.3615, diffLoss = 16.4298, kgLoss = 0.0944
2025-02-13 00:23:45.347142: Training Step 43/59: batchLoss = 2.8706, diffLoss = 14.0360, kgLoss = 0.0793
2025-02-13 00:23:46.266099: Training Step 44/59: batchLoss = 2.9674, diffLoss = 14.4924, kgLoss = 0.0862
2025-02-13 00:23:47.190905: Training Step 45/59: batchLoss = 3.1064, diffLoss = 15.1967, kgLoss = 0.0839
2025-02-13 00:23:48.107659: Training Step 46/59: batchLoss = 3.0145, diffLoss = 14.7221, kgLoss = 0.0876
2025-02-13 00:23:49.037380: Training Step 47/59: batchLoss = 2.7378, diffLoss = 13.3646, kgLoss = 0.0811
2025-02-13 00:23:49.968460: Training Step 48/59: batchLoss = 2.8394, diffLoss = 13.8555, kgLoss = 0.0854
2025-02-13 00:23:50.896236: Training Step 49/59: batchLoss = 3.5239, diffLoss = 17.2047, kgLoss = 0.1037
2025-02-13 00:23:51.823766: Training Step 50/59: batchLoss = 3.3471, diffLoss = 16.3233, kgLoss = 0.1030
2025-02-13 00:23:52.747946: Training Step 51/59: batchLoss = 2.9477, diffLoss = 14.4106, kgLoss = 0.0819
2025-02-13 00:23:53.673720: Training Step 52/59: batchLoss = 2.8004, diffLoss = 13.6805, kgLoss = 0.0804
2025-02-13 00:23:54.589709: Training Step 53/59: batchLoss = 2.8102, diffLoss = 13.7469, kgLoss = 0.0761
2025-02-13 00:23:55.508872: Training Step 54/59: batchLoss = 3.0742, diffLoss = 15.0213, kgLoss = 0.0874
2025-02-13 00:23:56.439954: Training Step 55/59: batchLoss = 3.3766, diffLoss = 16.5170, kgLoss = 0.0915
2025-02-13 00:23:57.368261: Training Step 56/59: batchLoss = 2.6250, diffLoss = 12.8266, kgLoss = 0.0746
2025-02-13 00:23:58.213498: Training Step 57/59: batchLoss = 2.9031, diffLoss = 14.2061, kgLoss = 0.0774
2025-02-13 00:23:59.069504: Training Step 58/59: batchLoss = 3.0265, diffLoss = 14.7988, kgLoss = 0.0834
2025-02-13 00:23:59.161910: 
2025-02-13 00:23:59.162435: Epoch 74/1000, Train: epLoss = 0.4358, epDfLoss = 2.1288, epKgLoss = 0.0126  
2025-02-13 00:24:00.643727: Steps 0/47: batch_recall = 37.33, batch_ndcg = 47.60 
2025-02-13 00:24:01.952675: Steps 1/47: batch_recall = 36.79, batch_ndcg = 42.60 
2025-02-13 00:24:03.211607: Steps 2/47: batch_recall = 41.59, batch_ndcg = 47.51 
2025-02-13 00:24:04.482757: Steps 3/47: batch_recall = 43.57, batch_ndcg = 45.65 
2025-02-13 00:24:05.678039: Steps 4/47: batch_recall = 39.02, batch_ndcg = 46.10 
2025-02-13 00:24:06.885007: Steps 5/47: batch_recall = 33.50, batch_ndcg = 40.85 
2025-02-13 00:24:08.086504: Steps 6/47: batch_recall = 40.67, batch_ndcg = 42.95 
2025-02-13 00:24:09.253755: Steps 7/47: batch_recall = 43.23, batch_ndcg = 45.08 
2025-02-13 00:24:10.419747: Steps 8/47: batch_recall = 48.30, batch_ndcg = 52.69 
2025-02-13 00:24:11.552413: Steps 9/47: batch_recall = 47.10, batch_ndcg = 46.92 
2025-02-13 00:24:12.717595: Steps 10/47: batch_recall = 43.94, batch_ndcg = 44.43 
2025-02-13 00:24:13.854144: Steps 11/47: batch_recall = 55.31, batch_ndcg = 52.76 
2025-02-13 00:24:14.994025: Steps 12/47: batch_recall = 50.27, batch_ndcg = 48.78 
2025-02-13 00:24:16.120825: Steps 13/47: batch_recall = 49.32, batch_ndcg = 46.30 
2025-02-13 00:24:17.219775: Steps 14/47: batch_recall = 41.14, batch_ndcg = 41.70 
2025-02-13 00:24:18.320927: Steps 15/47: batch_recall = 58.48, batch_ndcg = 54.47 
2025-02-13 00:24:19.424991: Steps 16/47: batch_recall = 52.32, batch_ndcg = 47.88 
2025-02-13 00:24:20.490005: Steps 17/47: batch_recall = 59.63, batch_ndcg = 50.42 
2025-02-13 00:24:21.575846: Steps 18/47: batch_recall = 53.32, batch_ndcg = 49.66 
2025-02-13 00:24:22.660579: Steps 19/47: batch_recall = 61.43, batch_ndcg = 55.63 
2025-02-13 00:24:23.718484: Steps 20/47: batch_recall = 68.02, batch_ndcg = 61.86 
2025-02-13 00:24:24.784438: Steps 21/47: batch_recall = 64.07, batch_ndcg = 56.46 
2025-02-13 00:24:25.845501: Steps 22/47: batch_recall = 55.85, batch_ndcg = 51.34 
2025-02-13 00:24:26.897673: Steps 23/47: batch_recall = 64.44, batch_ndcg = 55.93 
2025-02-13 00:24:27.961742: Steps 24/47: batch_recall = 64.74, batch_ndcg = 57.96 
2025-02-13 00:24:29.006361: Steps 25/47: batch_recall = 66.73, batch_ndcg = 57.93 
2025-02-13 00:24:30.018536: Steps 26/47: batch_recall = 59.97, batch_ndcg = 53.58 
2025-02-13 00:24:31.049928: Steps 27/47: batch_recall = 62.96, batch_ndcg = 53.05 
2025-02-13 00:24:32.068663: Steps 28/47: batch_recall = 69.42, batch_ndcg = 59.11 
2025-02-13 00:24:33.075376: Steps 29/47: batch_recall = 69.61, batch_ndcg = 58.48 
2025-02-13 00:24:34.093234: Steps 30/47: batch_recall = 74.18, batch_ndcg = 65.31 
2025-02-13 00:24:35.121978: Steps 31/47: batch_recall = 67.28, batch_ndcg = 56.53 
2025-02-13 00:24:36.128680: Steps 32/47: batch_recall = 70.65, batch_ndcg = 64.54 
2025-02-13 00:24:37.147363: Steps 33/47: batch_recall = 81.69, batch_ndcg = 68.66 
2025-02-13 00:24:38.172341: Steps 34/47: batch_recall = 70.48, batch_ndcg = 58.18 
2025-02-13 00:24:39.173669: Steps 35/47: batch_recall = 77.70, batch_ndcg = 65.51 
2025-02-13 00:24:40.176643: Steps 36/47: batch_recall = 79.80, batch_ndcg = 64.81 
2025-02-13 00:24:41.151741: Steps 37/47: batch_recall = 85.24, batch_ndcg = 73.91 
2025-02-13 00:24:42.131402: Steps 38/47: batch_recall = 89.03, batch_ndcg = 72.47 
2025-02-13 00:24:43.111970: Steps 39/47: batch_recall = 88.04, batch_ndcg = 68.72 
2025-02-13 00:24:44.075469: Steps 40/47: batch_recall = 73.73, batch_ndcg = 62.66 
2025-02-13 00:24:45.037565: Steps 41/47: batch_recall = 87.15, batch_ndcg = 72.49 
2025-02-13 00:24:46.005533: Steps 42/47: batch_recall = 82.35, batch_ndcg = 65.46 
2025-02-13 00:24:46.957557: Steps 43/47: batch_recall = 92.50, batch_ndcg = 73.56 
2025-02-13 00:24:47.925840: Steps 44/47: batch_recall = 90.49, batch_ndcg = 71.10 
2025-02-13 00:24:48.861255: Steps 45/47: batch_recall = 94.41, batch_ndcg = 75.08 
2025-02-13 00:24:48.967637: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.57 
2025-02-13 00:24:48.967778: Epoch 74/1000, Test: Recall = 0.1226, NDCG = 0.1102  

2025-02-13 00:24:50.163357: Training Step 0/59: batchLoss = 3.0817, diffLoss = 15.0474, kgLoss = 0.0903
2025-02-13 00:24:51.082678: Training Step 1/59: batchLoss = 2.7166, diffLoss = 13.2526, kgLoss = 0.0826
2025-02-13 00:24:52.010167: Training Step 2/59: batchLoss = 2.8665, diffLoss = 13.9884, kgLoss = 0.0860
2025-02-13 00:24:52.933680: Training Step 3/59: batchLoss = 3.0643, diffLoss = 14.9624, kgLoss = 0.0898
2025-02-13 00:24:53.841461: Training Step 4/59: batchLoss = 2.7444, diffLoss = 13.3922, kgLoss = 0.0824
2025-02-13 00:24:54.761305: Training Step 5/59: batchLoss = 2.7918, diffLoss = 13.6322, kgLoss = 0.0817
2025-02-13 00:24:55.678116: Training Step 6/59: batchLoss = 2.5253, diffLoss = 12.3340, kgLoss = 0.0731
2025-02-13 00:24:56.596305: Training Step 7/59: batchLoss = 3.1442, diffLoss = 15.3417, kgLoss = 0.0948
2025-02-13 00:24:57.517590: Training Step 8/59: batchLoss = 2.8094, diffLoss = 13.6976, kgLoss = 0.0874
2025-02-13 00:24:58.435711: Training Step 9/59: batchLoss = 2.5115, diffLoss = 12.2382, kgLoss = 0.0799
2025-02-13 00:24:59.360984: Training Step 10/59: batchLoss = 2.6830, diffLoss = 13.0929, kgLoss = 0.0805
2025-02-13 00:25:00.280869: Training Step 11/59: batchLoss = 2.8096, diffLoss = 13.7322, kgLoss = 0.0789
2025-02-13 00:25:01.203845: Training Step 12/59: batchLoss = 2.8069, diffLoss = 13.7149, kgLoss = 0.0799
2025-02-13 00:25:02.128920: Training Step 13/59: batchLoss = 2.7550, diffLoss = 13.4740, kgLoss = 0.0753
2025-02-13 00:25:03.058341: Training Step 14/59: batchLoss = 2.9135, diffLoss = 14.1979, kgLoss = 0.0924
2025-02-13 00:25:03.982601: Training Step 15/59: batchLoss = 2.9980, diffLoss = 14.6112, kgLoss = 0.0947
2025-02-13 00:25:04.907603: Training Step 16/59: batchLoss = 2.9139, diffLoss = 14.2245, kgLoss = 0.0863
2025-02-13 00:25:05.838481: Training Step 17/59: batchLoss = 2.7983, diffLoss = 13.6793, kgLoss = 0.0780
2025-02-13 00:25:06.779582: Training Step 18/59: batchLoss = 2.9488, diffLoss = 14.3808, kgLoss = 0.0908
2025-02-13 00:25:07.704943: Training Step 19/59: batchLoss = 2.7792, diffLoss = 13.5836, kgLoss = 0.0781
2025-02-13 00:25:08.633597: Training Step 20/59: batchLoss = 2.6735, diffLoss = 13.0631, kgLoss = 0.0761
2025-02-13 00:25:09.552782: Training Step 21/59: batchLoss = 3.0492, diffLoss = 14.9010, kgLoss = 0.0862
2025-02-13 00:25:10.480681: Training Step 22/59: batchLoss = 2.9931, diffLoss = 14.6013, kgLoss = 0.0911
2025-02-13 00:25:11.403982: Training Step 23/59: batchLoss = 2.9635, diffLoss = 14.4710, kgLoss = 0.0867
2025-02-13 00:25:12.319391: Training Step 24/59: batchLoss = 2.9854, diffLoss = 14.6054, kgLoss = 0.0803
2025-02-13 00:25:13.237537: Training Step 25/59: batchLoss = 3.0523, diffLoss = 14.9188, kgLoss = 0.0857
2025-02-13 00:25:14.153963: Training Step 26/59: batchLoss = 3.0253, diffLoss = 14.8059, kgLoss = 0.0802
2025-02-13 00:25:15.077556: Training Step 27/59: batchLoss = 2.8812, diffLoss = 14.0559, kgLoss = 0.0876
2025-02-13 00:25:16.012393: Training Step 28/59: batchLoss = 3.0682, diffLoss = 14.9823, kgLoss = 0.0897
2025-02-13 00:25:16.932119: Training Step 29/59: batchLoss = 2.8501, diffLoss = 13.9311, kgLoss = 0.0799
2025-02-13 00:25:17.878147: Training Step 30/59: batchLoss = 2.8200, diffLoss = 13.7792, kgLoss = 0.0802
2025-02-13 00:25:18.797430: Training Step 31/59: batchLoss = 3.2451, diffLoss = 15.8339, kgLoss = 0.0978
2025-02-13 00:25:19.730813: Training Step 32/59: batchLoss = 3.0078, diffLoss = 14.6896, kgLoss = 0.0873
2025-02-13 00:25:20.669051: Training Step 33/59: batchLoss = 2.7210, diffLoss = 13.2509, kgLoss = 0.0886
2025-02-13 00:25:21.596965: Training Step 34/59: batchLoss = 2.8819, diffLoss = 14.0544, kgLoss = 0.0888
2025-02-13 00:25:22.530447: Training Step 35/59: batchLoss = 3.1540, diffLoss = 15.3986, kgLoss = 0.0929
2025-02-13 00:25:23.474468: Training Step 36/59: batchLoss = 2.8008, diffLoss = 13.6768, kgLoss = 0.0818
2025-02-13 00:25:24.402158: Training Step 37/59: batchLoss = 3.0710, diffLoss = 15.0087, kgLoss = 0.0865
2025-02-13 00:25:25.338909: Training Step 38/59: batchLoss = 3.2931, diffLoss = 16.0796, kgLoss = 0.0965
2025-02-13 00:25:26.285746: Training Step 39/59: batchLoss = 2.8419, diffLoss = 13.8828, kgLoss = 0.0817
2025-02-13 00:25:27.231201: Training Step 40/59: batchLoss = 2.9721, diffLoss = 14.5367, kgLoss = 0.0809
2025-02-13 00:25:28.165789: Training Step 41/59: batchLoss = 2.7160, diffLoss = 13.2859, kgLoss = 0.0735
2025-02-13 00:25:29.096376: Training Step 42/59: batchLoss = 3.0766, diffLoss = 15.0386, kgLoss = 0.0861
2025-02-13 00:25:30.017440: Training Step 43/59: batchLoss = 3.2534, diffLoss = 15.8762, kgLoss = 0.0977
2025-02-13 00:25:30.934943: Training Step 44/59: batchLoss = 3.0421, diffLoss = 14.8305, kgLoss = 0.0950
2025-02-13 00:25:31.856868: Training Step 45/59: batchLoss = 3.0801, diffLoss = 15.0495, kgLoss = 0.0878
2025-02-13 00:25:32.781556: Training Step 46/59: batchLoss = 3.1975, diffLoss = 15.6306, kgLoss = 0.0893
2025-02-13 00:25:33.704166: Training Step 47/59: batchLoss = 3.2462, diffLoss = 15.8410, kgLoss = 0.0974
2025-02-13 00:25:34.625725: Training Step 48/59: batchLoss = 3.1436, diffLoss = 15.3637, kgLoss = 0.0885
2025-02-13 00:25:35.556466: Training Step 49/59: batchLoss = 3.2284, diffLoss = 15.7826, kgLoss = 0.0898
2025-02-13 00:25:36.483286: Training Step 50/59: batchLoss = 3.0185, diffLoss = 14.7640, kgLoss = 0.0821
2025-02-13 00:25:37.404973: Training Step 51/59: batchLoss = 3.2165, diffLoss = 15.7347, kgLoss = 0.0869
2025-02-13 00:25:38.341125: Training Step 52/59: batchLoss = 3.2496, diffLoss = 15.8907, kgLoss = 0.0894
2025-02-13 00:25:39.261174: Training Step 53/59: batchLoss = 2.9825, diffLoss = 14.5585, kgLoss = 0.0884
2025-02-13 00:25:40.192604: Training Step 54/59: batchLoss = 2.8710, diffLoss = 14.0417, kgLoss = 0.0783
2025-02-13 00:25:41.121122: Training Step 55/59: batchLoss = 2.9943, diffLoss = 14.6442, kgLoss = 0.0818
2025-02-13 00:25:42.050367: Training Step 56/59: batchLoss = 2.5967, diffLoss = 12.6577, kgLoss = 0.0814
2025-02-13 00:25:42.899377: Training Step 57/59: batchLoss = 2.9914, diffLoss = 14.6097, kgLoss = 0.0868
2025-02-13 00:25:43.755253: Training Step 58/59: batchLoss = 2.9157, diffLoss = 14.2679, kgLoss = 0.0777
2025-02-13 00:25:43.854485: 
2025-02-13 00:25:43.855042: Epoch 75/1000, Train: epLoss = 0.4346, epDfLoss = 2.1224, epKgLoss = 0.0126  
2025-02-13 00:25:45.328892: Steps 0/47: batch_recall = 37.10, batch_ndcg = 47.63 
2025-02-13 00:25:46.636068: Steps 1/47: batch_recall = 37.27, batch_ndcg = 42.25 
2025-02-13 00:25:47.898496: Steps 2/47: batch_recall = 41.49, batch_ndcg = 47.09 
2025-02-13 00:25:49.171644: Steps 3/47: batch_recall = 42.90, batch_ndcg = 45.50 
2025-02-13 00:25:50.375395: Steps 4/47: batch_recall = 39.17, batch_ndcg = 45.83 
2025-02-13 00:25:51.587575: Steps 5/47: batch_recall = 33.80, batch_ndcg = 40.68 
2025-02-13 00:25:52.785422: Steps 6/47: batch_recall = 39.44, batch_ndcg = 41.79 
2025-02-13 00:25:53.952775: Steps 7/47: batch_recall = 42.29, batch_ndcg = 44.74 
2025-02-13 00:25:55.125883: Steps 8/47: batch_recall = 48.07, batch_ndcg = 51.68 
2025-02-13 00:25:56.270177: Steps 9/47: batch_recall = 45.96, batch_ndcg = 46.60 
2025-02-13 00:25:57.436443: Steps 10/47: batch_recall = 43.33, batch_ndcg = 43.40 
2025-02-13 00:25:58.571633: Steps 11/47: batch_recall = 55.46, batch_ndcg = 52.99 
2025-02-13 00:25:59.719012: Steps 12/47: batch_recall = 49.69, batch_ndcg = 49.21 
2025-02-13 00:26:00.851125: Steps 13/47: batch_recall = 48.22, batch_ndcg = 45.30 
2025-02-13 00:26:01.951917: Steps 14/47: batch_recall = 40.69, batch_ndcg = 41.43 
2025-02-13 00:26:03.051202: Steps 15/47: batch_recall = 57.72, batch_ndcg = 54.16 
2025-02-13 00:26:04.144810: Steps 16/47: batch_recall = 52.17, batch_ndcg = 47.82 
2025-02-13 00:26:05.205876: Steps 17/47: batch_recall = 57.61, batch_ndcg = 49.66 
2025-02-13 00:26:06.292933: Steps 18/47: batch_recall = 52.32, batch_ndcg = 49.31 
2025-02-13 00:26:07.360726: Steps 19/47: batch_recall = 59.46, batch_ndcg = 55.28 
2025-02-13 00:26:08.418419: Steps 20/47: batch_recall = 66.88, batch_ndcg = 61.12 
2025-02-13 00:26:09.481558: Steps 21/47: batch_recall = 65.91, batch_ndcg = 56.54 
2025-02-13 00:26:10.551076: Steps 22/47: batch_recall = 55.97, batch_ndcg = 51.42 
2025-02-13 00:26:11.596244: Steps 23/47: batch_recall = 64.39, batch_ndcg = 55.71 
2025-02-13 00:26:12.652905: Steps 24/47: batch_recall = 63.64, batch_ndcg = 57.18 
2025-02-13 00:26:13.699047: Steps 25/47: batch_recall = 66.49, batch_ndcg = 58.48 
2025-02-13 00:26:14.704509: Steps 26/47: batch_recall = 60.24, batch_ndcg = 54.33 
2025-02-13 00:26:15.745537: Steps 27/47: batch_recall = 63.02, batch_ndcg = 53.27 
2025-02-13 00:26:16.765773: Steps 28/47: batch_recall = 69.93, batch_ndcg = 59.52 
2025-02-13 00:26:17.790440: Steps 29/47: batch_recall = 69.60, batch_ndcg = 58.76 
2025-02-13 00:26:18.809979: Steps 30/47: batch_recall = 73.64, batch_ndcg = 64.65 
2025-02-13 00:26:19.834557: Steps 31/47: batch_recall = 66.83, batch_ndcg = 55.51 
2025-02-13 00:26:20.842405: Steps 32/47: batch_recall = 71.19, batch_ndcg = 64.87 
2025-02-13 00:26:21.846753: Steps 33/47: batch_recall = 80.78, batch_ndcg = 69.03 
2025-02-13 00:26:22.858067: Steps 34/47: batch_recall = 68.96, batch_ndcg = 57.34 
2025-02-13 00:26:23.853770: Steps 35/47: batch_recall = 76.98, batch_ndcg = 64.62 
2025-02-13 00:26:24.853570: Steps 36/47: batch_recall = 80.32, batch_ndcg = 65.15 
2025-02-13 00:26:25.839514: Steps 37/47: batch_recall = 84.18, batch_ndcg = 72.72 
2025-02-13 00:26:26.826017: Steps 38/47: batch_recall = 91.30, batch_ndcg = 73.65 
2025-02-13 00:26:27.802713: Steps 39/47: batch_recall = 87.71, batch_ndcg = 68.49 
2025-02-13 00:26:28.776525: Steps 40/47: batch_recall = 74.08, batch_ndcg = 63.08 
2025-02-13 00:26:29.744321: Steps 41/47: batch_recall = 86.81, batch_ndcg = 72.29 
2025-02-13 00:26:30.719184: Steps 42/47: batch_recall = 83.32, batch_ndcg = 65.55 
2025-02-13 00:26:31.691209: Steps 43/47: batch_recall = 89.81, batch_ndcg = 72.77 
2025-02-13 00:26:32.656075: Steps 44/47: batch_recall = 89.03, batch_ndcg = 71.59 
2025-02-13 00:26:33.593703: Steps 45/47: batch_recall = 94.17, batch_ndcg = 74.47 
2025-02-13 00:26:33.699078: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.59 
2025-02-13 00:26:33.699218: Epoch 75/1000, Test: Recall = 0.1218, NDCG = 0.1097  

2025-02-13 00:26:34.891348: Training Step 0/59: batchLoss = 2.9602, diffLoss = 14.4685, kgLoss = 0.0831
2025-02-13 00:26:35.815774: Training Step 1/59: batchLoss = 2.7887, diffLoss = 13.6242, kgLoss = 0.0798
2025-02-13 00:26:36.745258: Training Step 2/59: batchLoss = 2.7105, diffLoss = 13.2494, kgLoss = 0.0758
2025-02-13 00:26:37.668227: Training Step 3/59: batchLoss = 2.8170, diffLoss = 13.7605, kgLoss = 0.0812
2025-02-13 00:26:38.581605: Training Step 4/59: batchLoss = 2.6889, diffLoss = 13.1153, kgLoss = 0.0823
2025-02-13 00:26:39.504468: Training Step 5/59: batchLoss = 2.9283, diffLoss = 14.2775, kgLoss = 0.0911
2025-02-13 00:26:40.422638: Training Step 6/59: batchLoss = 2.6440, diffLoss = 12.8999, kgLoss = 0.0800
2025-02-13 00:26:41.353148: Training Step 7/59: batchLoss = 3.0009, diffLoss = 14.6429, kgLoss = 0.0904
2025-02-13 00:26:42.278051: Training Step 8/59: batchLoss = 2.8904, diffLoss = 14.0984, kgLoss = 0.0884
2025-02-13 00:26:43.211837: Training Step 9/59: batchLoss = 2.7360, diffLoss = 13.3322, kgLoss = 0.0870
2025-02-13 00:26:44.154392: Training Step 10/59: batchLoss = 2.9135, diffLoss = 14.1881, kgLoss = 0.0948
2025-02-13 00:26:45.078287: Training Step 11/59: batchLoss = 2.8670, diffLoss = 13.9992, kgLoss = 0.0839
2025-02-13 00:26:46.012875: Training Step 12/59: batchLoss = 2.7368, diffLoss = 13.3489, kgLoss = 0.0838
2025-02-13 00:26:46.943487: Training Step 13/59: batchLoss = 2.9607, diffLoss = 14.4388, kgLoss = 0.0912
2025-02-13 00:26:47.880718: Training Step 14/59: batchLoss = 2.5822, diffLoss = 12.5981, kgLoss = 0.0782
2025-02-13 00:26:48.816364: Training Step 15/59: batchLoss = 2.7364, diffLoss = 13.3792, kgLoss = 0.0757
2025-02-13 00:26:49.752174: Training Step 16/59: batchLoss = 2.8718, diffLoss = 14.0439, kgLoss = 0.0788
2025-02-13 00:26:50.690837: Training Step 17/59: batchLoss = 3.1586, diffLoss = 15.4223, kgLoss = 0.0926
2025-02-13 00:26:51.631187: Training Step 18/59: batchLoss = 2.7391, diffLoss = 13.3816, kgLoss = 0.0784
2025-02-13 00:26:52.565731: Training Step 19/59: batchLoss = 3.0219, diffLoss = 14.7334, kgLoss = 0.0940
2025-02-13 00:26:53.494321: Training Step 20/59: batchLoss = 2.9787, diffLoss = 14.5520, kgLoss = 0.0853
2025-02-13 00:26:54.425340: Training Step 21/59: batchLoss = 3.2336, diffLoss = 15.7930, kgLoss = 0.0938
2025-02-13 00:26:55.353350: Training Step 22/59: batchLoss = 2.7196, diffLoss = 13.2656, kgLoss = 0.0831
2025-02-13 00:26:56.280667: Training Step 23/59: batchLoss = 2.7562, diffLoss = 13.4443, kgLoss = 0.0841
2025-02-13 00:26:57.212321: Training Step 24/59: batchLoss = 3.2179, diffLoss = 15.7418, kgLoss = 0.0869
2025-02-13 00:26:58.135031: Training Step 25/59: batchLoss = 2.6937, diffLoss = 13.1531, kgLoss = 0.0789
2025-02-13 00:26:59.067586: Training Step 26/59: batchLoss = 3.0984, diffLoss = 15.1132, kgLoss = 0.0947
2025-02-13 00:26:59.984872: Training Step 27/59: batchLoss = 2.7259, diffLoss = 13.3171, kgLoss = 0.0781
2025-02-13 00:27:00.896956: Training Step 28/59: batchLoss = 2.8509, diffLoss = 13.9317, kgLoss = 0.0806
2025-02-13 00:27:01.824070: Training Step 29/59: batchLoss = 2.9359, diffLoss = 14.3410, kgLoss = 0.0846
2025-02-13 00:27:02.739547: Training Step 30/59: batchLoss = 2.8944, diffLoss = 14.1320, kgLoss = 0.0851
2025-02-13 00:27:03.670026: Training Step 31/59: batchLoss = 2.8500, diffLoss = 13.8600, kgLoss = 0.0975
2025-02-13 00:27:04.594223: Training Step 32/59: batchLoss = 2.7162, diffLoss = 13.2606, kgLoss = 0.0801
2025-02-13 00:27:05.518177: Training Step 33/59: batchLoss = 2.8750, diffLoss = 14.0552, kgLoss = 0.0800
2025-02-13 00:27:06.445298: Training Step 34/59: batchLoss = 3.0320, diffLoss = 14.8262, kgLoss = 0.0834
2025-02-13 00:27:07.369263: Training Step 35/59: batchLoss = 2.8408, diffLoss = 13.8869, kgLoss = 0.0793
2025-02-13 00:27:08.301185: Training Step 36/59: batchLoss = 3.2811, diffLoss = 16.0200, kgLoss = 0.0964
2025-02-13 00:27:09.239164: Training Step 37/59: batchLoss = 3.0129, diffLoss = 14.7261, kgLoss = 0.0846
2025-02-13 00:27:10.176414: Training Step 38/59: batchLoss = 2.9770, diffLoss = 14.5540, kgLoss = 0.0827
2025-02-13 00:27:11.110929: Training Step 39/59: batchLoss = 3.1433, diffLoss = 15.3676, kgLoss = 0.0872
2025-02-13 00:27:12.046876: Training Step 40/59: batchLoss = 3.2780, diffLoss = 15.9852, kgLoss = 0.1012
2025-02-13 00:27:12.982883: Training Step 41/59: batchLoss = 2.9774, diffLoss = 14.5325, kgLoss = 0.0887
2025-02-13 00:27:13.928686: Training Step 42/59: batchLoss = 2.8750, diffLoss = 14.0589, kgLoss = 0.0791
2025-02-13 00:27:14.856001: Training Step 43/59: batchLoss = 2.6704, diffLoss = 13.0296, kgLoss = 0.0806
2025-02-13 00:27:15.779930: Training Step 44/59: batchLoss = 2.8953, diffLoss = 14.1384, kgLoss = 0.0846
2025-02-13 00:27:16.710737: Training Step 45/59: batchLoss = 3.6612, diffLoss = 17.9014, kgLoss = 0.1011
2025-02-13 00:27:17.635788: Training Step 46/59: batchLoss = 3.1237, diffLoss = 15.2781, kgLoss = 0.0851
2025-02-13 00:27:18.564387: Training Step 47/59: batchLoss = 3.2271, diffLoss = 15.7751, kgLoss = 0.0901
2025-02-13 00:27:19.484294: Training Step 48/59: batchLoss = 3.2593, diffLoss = 15.8914, kgLoss = 0.1013
2025-02-13 00:27:20.411880: Training Step 49/59: batchLoss = 3.2417, diffLoss = 15.8492, kgLoss = 0.0898
2025-02-13 00:27:21.335296: Training Step 50/59: batchLoss = 2.6193, diffLoss = 12.7991, kgLoss = 0.0744
2025-02-13 00:27:22.259776: Training Step 51/59: batchLoss = 2.9203, diffLoss = 14.2667, kgLoss = 0.0837
2025-02-13 00:27:23.204830: Training Step 52/59: batchLoss = 2.9168, diffLoss = 14.2269, kgLoss = 0.0893
2025-02-13 00:27:24.127311: Training Step 53/59: batchLoss = 3.0761, diffLoss = 15.0309, kgLoss = 0.0874
2025-02-13 00:27:25.055697: Training Step 54/59: batchLoss = 2.9125, diffLoss = 14.2374, kgLoss = 0.0812
2025-02-13 00:27:25.986821: Training Step 55/59: batchLoss = 3.2833, diffLoss = 16.0571, kgLoss = 0.0899
2025-02-13 00:27:26.930578: Training Step 56/59: batchLoss = 2.7564, diffLoss = 13.4813, kgLoss = 0.0751
2025-02-13 00:27:27.777669: Training Step 57/59: batchLoss = 2.8690, diffLoss = 14.0581, kgLoss = 0.0717
2025-02-13 00:27:28.635181: Training Step 58/59: batchLoss = 3.2676, diffLoss = 15.9528, kgLoss = 0.0963
2025-02-13 00:27:28.731289: 
2025-02-13 00:27:28.731759: Epoch 76/1000, Train: epLoss = 0.4335, epDfLoss = 2.1172, epKgLoss = 0.0126  
2025-02-13 00:27:30.219887: Steps 0/47: batch_recall = 37.27, batch_ndcg = 47.16 
2025-02-13 00:27:31.520698: Steps 1/47: batch_recall = 36.60, batch_ndcg = 41.89 
2025-02-13 00:27:32.781457: Steps 2/47: batch_recall = 43.17, batch_ndcg = 48.11 
2025-02-13 00:27:34.049216: Steps 3/47: batch_recall = 43.94, batch_ndcg = 46.30 
2025-02-13 00:27:35.254727: Steps 4/47: batch_recall = 38.47, batch_ndcg = 45.65 
2025-02-13 00:27:36.459445: Steps 5/47: batch_recall = 33.57, batch_ndcg = 40.63 
2025-02-13 00:27:37.640109: Steps 6/47: batch_recall = 39.83, batch_ndcg = 42.13 
2025-02-13 00:27:38.796268: Steps 7/47: batch_recall = 43.85, batch_ndcg = 45.37 
2025-02-13 00:27:39.966062: Steps 8/47: batch_recall = 48.32, batch_ndcg = 52.55 
2025-02-13 00:27:41.099624: Steps 9/47: batch_recall = 47.08, batch_ndcg = 46.90 
2025-02-13 00:27:42.265840: Steps 10/47: batch_recall = 43.67, batch_ndcg = 43.50 
2025-02-13 00:27:43.392153: Steps 11/47: batch_recall = 55.51, batch_ndcg = 53.13 
2025-02-13 00:27:44.519631: Steps 12/47: batch_recall = 49.15, batch_ndcg = 48.43 
2025-02-13 00:27:45.657183: Steps 13/47: batch_recall = 48.66, batch_ndcg = 45.55 
2025-02-13 00:27:46.756754: Steps 14/47: batch_recall = 40.96, batch_ndcg = 41.28 
2025-02-13 00:27:47.859418: Steps 15/47: batch_recall = 57.57, batch_ndcg = 54.24 
2025-02-13 00:27:48.954372: Steps 16/47: batch_recall = 51.53, batch_ndcg = 47.66 
2025-02-13 00:27:50.014458: Steps 17/47: batch_recall = 57.73, batch_ndcg = 49.88 
2025-02-13 00:27:51.104053: Steps 18/47: batch_recall = 52.28, batch_ndcg = 49.19 
2025-02-13 00:27:52.180234: Steps 19/47: batch_recall = 60.43, batch_ndcg = 56.11 
2025-02-13 00:27:53.235546: Steps 20/47: batch_recall = 67.84, batch_ndcg = 61.25 
2025-02-13 00:27:54.289840: Steps 21/47: batch_recall = 64.98, batch_ndcg = 55.77 
2025-02-13 00:27:55.357471: Steps 22/47: batch_recall = 55.83, batch_ndcg = 51.38 
2025-02-13 00:27:56.407766: Steps 23/47: batch_recall = 64.64, batch_ndcg = 56.18 
2025-02-13 00:27:57.461902: Steps 24/47: batch_recall = 64.77, batch_ndcg = 58.06 
2025-02-13 00:27:58.510311: Steps 25/47: batch_recall = 65.07, batch_ndcg = 57.61 
2025-02-13 00:27:59.517279: Steps 26/47: batch_recall = 59.85, batch_ndcg = 53.72 
2025-02-13 00:28:00.538418: Steps 27/47: batch_recall = 62.99, batch_ndcg = 53.47 
2025-02-13 00:28:01.548606: Steps 28/47: batch_recall = 70.73, batch_ndcg = 59.34 
2025-02-13 00:28:02.554380: Steps 29/47: batch_recall = 69.81, batch_ndcg = 59.06 
2025-02-13 00:28:03.553840: Steps 30/47: batch_recall = 72.48, batch_ndcg = 64.20 
2025-02-13 00:28:04.588186: Steps 31/47: batch_recall = 68.70, batch_ndcg = 57.07 
2025-02-13 00:28:05.590096: Steps 32/47: batch_recall = 70.80, batch_ndcg = 64.22 
2025-02-13 00:28:06.603155: Steps 33/47: batch_recall = 80.49, batch_ndcg = 68.71 
2025-02-13 00:28:07.617756: Steps 34/47: batch_recall = 67.62, batch_ndcg = 56.17 
2025-02-13 00:28:08.601708: Steps 35/47: batch_recall = 76.53, batch_ndcg = 64.46 
2025-02-13 00:28:09.595378: Steps 36/47: batch_recall = 79.39, batch_ndcg = 64.99 
2025-02-13 00:28:10.564976: Steps 37/47: batch_recall = 82.80, batch_ndcg = 72.22 
2025-02-13 00:28:11.537687: Steps 38/47: batch_recall = 91.78, batch_ndcg = 73.78 
2025-02-13 00:28:12.508490: Steps 39/47: batch_recall = 85.58, batch_ndcg = 67.96 
2025-02-13 00:28:13.464979: Steps 40/47: batch_recall = 72.35, batch_ndcg = 62.13 
2025-02-13 00:28:14.413232: Steps 41/47: batch_recall = 87.52, batch_ndcg = 73.13 
2025-02-13 00:28:15.363776: Steps 42/47: batch_recall = 81.99, batch_ndcg = 65.36 
2025-02-13 00:28:16.313453: Steps 43/47: batch_recall = 91.53, batch_ndcg = 72.86 
2025-02-13 00:28:17.280077: Steps 44/47: batch_recall = 86.90, batch_ndcg = 69.82 
2025-02-13 00:28:18.215465: Steps 45/47: batch_recall = 93.77, batch_ndcg = 74.47 
2025-02-13 00:28:18.319505: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.71 
2025-02-13 00:28:18.319629: Epoch 76/1000, Test: Recall = 0.1217, NDCG = 0.1097  

2025-02-13 00:28:19.524470: Training Step 0/59: batchLoss = 2.7930, diffLoss = 13.6038, kgLoss = 0.0903
2025-02-13 00:28:20.461754: Training Step 1/59: batchLoss = 2.8625, diffLoss = 13.9734, kgLoss = 0.0848
2025-02-13 00:28:21.385166: Training Step 2/59: batchLoss = 2.7557, diffLoss = 13.4464, kgLoss = 0.0830
2025-02-13 00:28:22.313312: Training Step 3/59: batchLoss = 3.0111, diffLoss = 14.7000, kgLoss = 0.0889
2025-02-13 00:28:23.239008: Training Step 4/59: batchLoss = 2.9502, diffLoss = 14.3857, kgLoss = 0.0913
2025-02-13 00:28:24.163072: Training Step 5/59: batchLoss = 2.8314, diffLoss = 13.8074, kgLoss = 0.0874
2025-02-13 00:28:25.080662: Training Step 6/59: batchLoss = 2.8202, diffLoss = 13.7552, kgLoss = 0.0864
2025-02-13 00:28:25.998274: Training Step 7/59: batchLoss = 2.9718, diffLoss = 14.5217, kgLoss = 0.0844
2025-02-13 00:28:26.916007: Training Step 8/59: batchLoss = 3.1265, diffLoss = 15.2451, kgLoss = 0.0969
2025-02-13 00:28:27.839102: Training Step 9/59: batchLoss = 2.4902, diffLoss = 12.1590, kgLoss = 0.0730
2025-02-13 00:28:28.760602: Training Step 10/59: batchLoss = 2.5165, diffLoss = 12.2785, kgLoss = 0.0761
2025-02-13 00:28:29.679981: Training Step 11/59: batchLoss = 2.6279, diffLoss = 12.8086, kgLoss = 0.0827
2025-02-13 00:28:30.614956: Training Step 12/59: batchLoss = 2.8965, diffLoss = 14.1429, kgLoss = 0.0848
2025-02-13 00:28:31.558456: Training Step 13/59: batchLoss = 2.8435, diffLoss = 13.8819, kgLoss = 0.0839
2025-02-13 00:28:32.490530: Training Step 14/59: batchLoss = 2.9876, diffLoss = 14.6126, kgLoss = 0.0813
2025-02-13 00:28:33.424701: Training Step 15/59: batchLoss = 2.8720, diffLoss = 14.0078, kgLoss = 0.0881
2025-02-13 00:28:34.362885: Training Step 16/59: batchLoss = 2.9907, diffLoss = 14.5983, kgLoss = 0.0888
2025-02-13 00:28:35.303002: Training Step 17/59: batchLoss = 2.9733, diffLoss = 14.5249, kgLoss = 0.0854
2025-02-13 00:28:36.240483: Training Step 18/59: batchLoss = 2.7859, diffLoss = 13.5999, kgLoss = 0.0823
2025-02-13 00:28:37.176369: Training Step 19/59: batchLoss = 2.7618, diffLoss = 13.4826, kgLoss = 0.0817
2025-02-13 00:28:38.120324: Training Step 20/59: batchLoss = 2.8417, diffLoss = 13.9050, kgLoss = 0.0759
2025-02-13 00:28:39.057484: Training Step 21/59: batchLoss = 2.8001, diffLoss = 13.6658, kgLoss = 0.0837
2025-02-13 00:28:39.984026: Training Step 22/59: batchLoss = 2.6547, diffLoss = 12.9497, kgLoss = 0.0810
2025-02-13 00:28:40.912183: Training Step 23/59: batchLoss = 2.8874, diffLoss = 14.1004, kgLoss = 0.0842
2025-02-13 00:28:41.837352: Training Step 24/59: batchLoss = 2.9195, diffLoss = 14.2610, kgLoss = 0.0841
2025-02-13 00:28:42.754708: Training Step 25/59: batchLoss = 3.0172, diffLoss = 14.7344, kgLoss = 0.0878
2025-02-13 00:28:43.679953: Training Step 26/59: batchLoss = 2.9288, diffLoss = 14.2912, kgLoss = 0.0882
2025-02-13 00:28:44.598476: Training Step 27/59: batchLoss = 2.8370, diffLoss = 13.8381, kgLoss = 0.0867
2025-02-13 00:28:45.525862: Training Step 28/59: batchLoss = 3.0184, diffLoss = 14.7120, kgLoss = 0.0950
2025-02-13 00:28:46.448654: Training Step 29/59: batchLoss = 2.7977, diffLoss = 13.6471, kgLoss = 0.0853
2025-02-13 00:28:47.372278: Training Step 30/59: batchLoss = 2.9676, diffLoss = 14.5214, kgLoss = 0.0792
2025-02-13 00:28:48.302320: Training Step 31/59: batchLoss = 3.1823, diffLoss = 15.5244, kgLoss = 0.0967
2025-02-13 00:28:49.234628: Training Step 32/59: batchLoss = 3.0528, diffLoss = 14.9313, kgLoss = 0.0832
2025-02-13 00:28:50.171149: Training Step 33/59: batchLoss = 3.1841, diffLoss = 15.5735, kgLoss = 0.0868
2025-02-13 00:28:51.106488: Training Step 34/59: batchLoss = 2.8016, diffLoss = 13.6749, kgLoss = 0.0832
2025-02-13 00:28:52.042336: Training Step 35/59: batchLoss = 3.1995, diffLoss = 15.6428, kgLoss = 0.0886
2025-02-13 00:28:52.979785: Training Step 36/59: batchLoss = 3.0937, diffLoss = 15.0935, kgLoss = 0.0938
2025-02-13 00:28:53.924537: Training Step 37/59: batchLoss = 2.6368, diffLoss = 12.8876, kgLoss = 0.0741
2025-02-13 00:28:54.858555: Training Step 38/59: batchLoss = 2.7398, diffLoss = 13.3893, kgLoss = 0.0774
2025-02-13 00:28:55.794140: Training Step 39/59: batchLoss = 2.9601, diffLoss = 14.4791, kgLoss = 0.0803
2025-02-13 00:28:56.731749: Training Step 40/59: batchLoss = 2.9342, diffLoss = 14.3228, kgLoss = 0.0870
2025-02-13 00:28:57.663922: Training Step 41/59: batchLoss = 2.8068, diffLoss = 13.7098, kgLoss = 0.0811
2025-02-13 00:28:58.598440: Training Step 42/59: batchLoss = 2.9649, diffLoss = 14.5074, kgLoss = 0.0793
2025-02-13 00:28:59.524020: Training Step 43/59: batchLoss = 2.9090, diffLoss = 14.2191, kgLoss = 0.0814
2025-02-13 00:29:00.445706: Training Step 44/59: batchLoss = 3.2388, diffLoss = 15.8043, kgLoss = 0.0975
2025-02-13 00:29:01.370631: Training Step 45/59: batchLoss = 2.7619, diffLoss = 13.5107, kgLoss = 0.0747
2025-02-13 00:29:02.300161: Training Step 46/59: batchLoss = 2.9009, diffLoss = 14.1822, kgLoss = 0.0806
2025-02-13 00:29:03.232611: Training Step 47/59: batchLoss = 3.0320, diffLoss = 14.8327, kgLoss = 0.0819
2025-02-13 00:29:04.158525: Training Step 48/59: batchLoss = 3.4120, diffLoss = 16.6563, kgLoss = 0.1009
2025-02-13 00:29:05.084118: Training Step 49/59: batchLoss = 3.1748, diffLoss = 15.5157, kgLoss = 0.0896
2025-02-13 00:29:06.017510: Training Step 50/59: batchLoss = 2.8199, diffLoss = 13.7767, kgLoss = 0.0806
2025-02-13 00:29:06.948771: Training Step 51/59: batchLoss = 3.2229, diffLoss = 15.7547, kgLoss = 0.0900
2025-02-13 00:29:07.877723: Training Step 52/59: batchLoss = 3.1064, diffLoss = 15.1905, kgLoss = 0.0854
2025-02-13 00:29:08.809949: Training Step 53/59: batchLoss = 3.1864, diffLoss = 15.5901, kgLoss = 0.0854
2025-02-13 00:29:09.765025: Training Step 54/59: batchLoss = 3.2994, diffLoss = 16.1151, kgLoss = 0.0955
2025-02-13 00:29:10.698543: Training Step 55/59: batchLoss = 3.2662, diffLoss = 15.9553, kgLoss = 0.0939
2025-02-13 00:29:11.628607: Training Step 56/59: batchLoss = 3.1269, diffLoss = 15.2862, kgLoss = 0.0871
2025-02-13 00:29:12.479353: Training Step 57/59: batchLoss = 2.9909, diffLoss = 14.6045, kgLoss = 0.0875
2025-02-13 00:29:13.335875: Training Step 58/59: batchLoss = 3.3579, diffLoss = 16.4220, kgLoss = 0.0919
2025-02-13 00:29:13.436202: 
2025-02-13 00:29:13.436537: Epoch 77/1000, Train: epLoss = 0.4348, epDfLoss = 2.1233, epKgLoss = 0.0126  
2025-02-13 00:29:14.917891: Steps 0/47: batch_recall = 36.89, batch_ndcg = 47.70 
2025-02-13 00:29:16.226224: Steps 1/47: batch_recall = 36.57, batch_ndcg = 42.49 
2025-02-13 00:29:17.474717: Steps 2/47: batch_recall = 42.33, batch_ndcg = 47.86 
2025-02-13 00:29:18.735335: Steps 3/47: batch_recall = 44.39, batch_ndcg = 47.39 
2025-02-13 00:29:19.925938: Steps 4/47: batch_recall = 39.53, batch_ndcg = 45.56 
2025-02-13 00:29:21.134980: Steps 5/47: batch_recall = 33.45, batch_ndcg = 40.23 
2025-02-13 00:29:22.324533: Steps 6/47: batch_recall = 39.37, batch_ndcg = 41.82 
2025-02-13 00:29:23.496770: Steps 7/47: batch_recall = 41.23, batch_ndcg = 44.13 
2025-02-13 00:29:24.664299: Steps 8/47: batch_recall = 47.94, batch_ndcg = 51.74 
2025-02-13 00:29:25.798667: Steps 9/47: batch_recall = 45.79, batch_ndcg = 46.17 
2025-02-13 00:29:26.959728: Steps 10/47: batch_recall = 44.39, batch_ndcg = 43.83 
2025-02-13 00:29:28.105868: Steps 11/47: batch_recall = 55.17, batch_ndcg = 52.57 
2025-02-13 00:29:29.264959: Steps 12/47: batch_recall = 49.74, batch_ndcg = 49.22 
2025-02-13 00:29:30.408386: Steps 13/47: batch_recall = 48.67, batch_ndcg = 45.35 
2025-02-13 00:29:31.509090: Steps 14/47: batch_recall = 41.27, batch_ndcg = 41.36 
2025-02-13 00:29:32.621552: Steps 15/47: batch_recall = 57.68, batch_ndcg = 54.09 
2025-02-13 00:29:33.718416: Steps 16/47: batch_recall = 51.64, batch_ndcg = 48.63 
2025-02-13 00:29:34.780781: Steps 17/47: batch_recall = 58.10, batch_ndcg = 49.64 
2025-02-13 00:29:35.938446: Steps 18/47: batch_recall = 52.20, batch_ndcg = 49.11 
2025-02-13 00:29:37.012813: Steps 19/47: batch_recall = 60.28, batch_ndcg = 55.84 
2025-02-13 00:29:38.065438: Steps 20/47: batch_recall = 68.89, batch_ndcg = 61.84 
2025-02-13 00:29:39.112428: Steps 21/47: batch_recall = 65.20, batch_ndcg = 55.57 
2025-02-13 00:29:40.158478: Steps 22/47: batch_recall = 55.82, batch_ndcg = 51.44 
2025-02-13 00:29:41.207781: Steps 23/47: batch_recall = 64.85, batch_ndcg = 56.57 
2025-02-13 00:29:42.270164: Steps 24/47: batch_recall = 63.46, batch_ndcg = 57.41 
2025-02-13 00:29:43.305555: Steps 25/47: batch_recall = 65.59, batch_ndcg = 58.03 
2025-02-13 00:29:44.321561: Steps 26/47: batch_recall = 60.80, batch_ndcg = 53.73 
2025-02-13 00:29:45.353782: Steps 27/47: batch_recall = 62.71, batch_ndcg = 53.33 
2025-02-13 00:29:46.376911: Steps 28/47: batch_recall = 70.91, batch_ndcg = 59.19 
2025-02-13 00:29:47.407756: Steps 29/47: batch_recall = 69.66, batch_ndcg = 57.87 
2025-02-13 00:29:48.435590: Steps 30/47: batch_recall = 72.54, batch_ndcg = 64.33 
2025-02-13 00:29:49.475622: Steps 31/47: batch_recall = 67.38, batch_ndcg = 56.45 
2025-02-13 00:29:50.505329: Steps 32/47: batch_recall = 71.23, batch_ndcg = 64.91 
2025-02-13 00:29:51.531045: Steps 33/47: batch_recall = 80.76, batch_ndcg = 68.15 
2025-02-13 00:29:52.560380: Steps 34/47: batch_recall = 69.86, batch_ndcg = 56.28 
2025-02-13 00:29:53.559781: Steps 35/47: batch_recall = 76.14, batch_ndcg = 65.17 
2025-02-13 00:29:54.561823: Steps 36/47: batch_recall = 79.03, batch_ndcg = 64.79 
2025-02-13 00:29:55.551499: Steps 37/47: batch_recall = 81.84, batch_ndcg = 72.08 
2025-02-13 00:29:56.531180: Steps 38/47: batch_recall = 89.04, batch_ndcg = 72.91 
2025-02-13 00:29:57.509370: Steps 39/47: batch_recall = 86.93, batch_ndcg = 68.57 
2025-02-13 00:29:58.465908: Steps 40/47: batch_recall = 71.55, batch_ndcg = 62.72 
2025-02-13 00:29:59.421778: Steps 41/47: batch_recall = 87.12, batch_ndcg = 72.44 
2025-02-13 00:30:00.379517: Steps 42/47: batch_recall = 82.90, batch_ndcg = 65.38 
2025-02-13 00:30:01.332946: Steps 43/47: batch_recall = 90.92, batch_ndcg = 72.72 
2025-02-13 00:30:02.291120: Steps 44/47: batch_recall = 87.33, batch_ndcg = 70.50 
2025-02-13 00:30:03.222805: Steps 45/47: batch_recall = 93.73, batch_ndcg = 74.96 
2025-02-13 00:30:03.324941: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.70 
2025-02-13 00:30:03.325067: Epoch 77/1000, Test: Recall = 0.1216, NDCG = 0.1096  

2025-02-13 00:30:04.509491: Training Step 0/59: batchLoss = 2.9606, diffLoss = 14.4485, kgLoss = 0.0887
2025-02-13 00:30:05.446459: Training Step 1/59: batchLoss = 2.7188, diffLoss = 13.2853, kgLoss = 0.0772
2025-02-13 00:30:06.381228: Training Step 2/59: batchLoss = 2.9349, diffLoss = 14.3349, kgLoss = 0.0849
2025-02-13 00:30:07.316272: Training Step 3/59: batchLoss = 2.8623, diffLoss = 13.9676, kgLoss = 0.0860
2025-02-13 00:30:08.248234: Training Step 4/59: batchLoss = 2.7174, diffLoss = 13.2582, kgLoss = 0.0822
2025-02-13 00:30:09.183374: Training Step 5/59: batchLoss = 3.2346, diffLoss = 15.7898, kgLoss = 0.0958
2025-02-13 00:30:10.117950: Training Step 6/59: batchLoss = 2.8106, diffLoss = 13.7219, kgLoss = 0.0828
2025-02-13 00:30:11.051028: Training Step 7/59: batchLoss = 2.8731, diffLoss = 14.0164, kgLoss = 0.0873
2025-02-13 00:30:11.989627: Training Step 8/59: batchLoss = 2.7455, diffLoss = 13.4208, kgLoss = 0.0767
2025-02-13 00:30:12.924077: Training Step 9/59: batchLoss = 3.0544, diffLoss = 14.9254, kgLoss = 0.0867
2025-02-13 00:30:13.859349: Training Step 10/59: batchLoss = 3.1090, diffLoss = 15.1742, kgLoss = 0.0927
2025-02-13 00:30:14.787450: Training Step 11/59: batchLoss = 2.6767, diffLoss = 13.0530, kgLoss = 0.0827
2025-02-13 00:30:15.709751: Training Step 12/59: batchLoss = 2.6515, diffLoss = 12.9388, kgLoss = 0.0796
2025-02-13 00:30:16.643852: Training Step 13/59: batchLoss = 3.1802, diffLoss = 15.4955, kgLoss = 0.1014
2025-02-13 00:30:17.570579: Training Step 14/59: batchLoss = 2.6860, diffLoss = 13.1165, kgLoss = 0.0783
2025-02-13 00:30:18.496041: Training Step 15/59: batchLoss = 3.0130, diffLoss = 14.7036, kgLoss = 0.0903
2025-02-13 00:30:19.421198: Training Step 16/59: batchLoss = 3.0938, diffLoss = 15.0956, kgLoss = 0.0934
2025-02-13 00:30:20.346998: Training Step 17/59: batchLoss = 3.0569, diffLoss = 14.9348, kgLoss = 0.0874
2025-02-13 00:30:21.273252: Training Step 18/59: batchLoss = 2.8749, diffLoss = 14.0217, kgLoss = 0.0883
2025-02-13 00:30:22.197588: Training Step 19/59: batchLoss = 2.9286, diffLoss = 14.3119, kgLoss = 0.0827
2025-02-13 00:30:23.125313: Training Step 20/59: batchLoss = 2.8685, diffLoss = 14.0314, kgLoss = 0.0777
2025-02-13 00:30:24.056767: Training Step 21/59: batchLoss = 2.9571, diffLoss = 14.4488, kgLoss = 0.0841
2025-02-13 00:30:24.990054: Training Step 22/59: batchLoss = 3.0231, diffLoss = 14.7743, kgLoss = 0.0853
2025-02-13 00:30:25.923645: Training Step 23/59: batchLoss = 3.2526, diffLoss = 15.8710, kgLoss = 0.0980
2025-02-13 00:30:26.856978: Training Step 24/59: batchLoss = 3.1539, diffLoss = 15.4312, kgLoss = 0.0845
2025-02-13 00:30:27.796511: Training Step 25/59: batchLoss = 3.0322, diffLoss = 14.8087, kgLoss = 0.0881
2025-02-13 00:30:28.732903: Training Step 26/59: batchLoss = 2.9084, diffLoss = 14.2090, kgLoss = 0.0832
2025-02-13 00:30:29.672784: Training Step 27/59: batchLoss = 2.6305, diffLoss = 12.8319, kgLoss = 0.0802
2025-02-13 00:30:30.608622: Training Step 28/59: batchLoss = 2.8890, diffLoss = 14.0816, kgLoss = 0.0908
2025-02-13 00:30:31.564146: Training Step 29/59: batchLoss = 2.8239, diffLoss = 13.7941, kgLoss = 0.0814
2025-02-13 00:30:32.497176: Training Step 30/59: batchLoss = 2.4468, diffLoss = 11.9584, kgLoss = 0.0689
2025-02-13 00:30:33.420725: Training Step 31/59: batchLoss = 2.6341, diffLoss = 12.8703, kgLoss = 0.0751
2025-02-13 00:30:34.347597: Training Step 32/59: batchLoss = 2.9437, diffLoss = 14.3316, kgLoss = 0.0967
2025-02-13 00:30:35.272026: Training Step 33/59: batchLoss = 3.1544, diffLoss = 15.3912, kgLoss = 0.0952
2025-02-13 00:30:36.194725: Training Step 34/59: batchLoss = 3.1763, diffLoss = 15.4999, kgLoss = 0.0954
2025-02-13 00:30:37.117461: Training Step 35/59: batchLoss = 2.5685, diffLoss = 12.5599, kgLoss = 0.0707
2025-02-13 00:30:38.039261: Training Step 36/59: batchLoss = 2.8748, diffLoss = 14.0447, kgLoss = 0.0823
2025-02-13 00:30:38.976353: Training Step 37/59: batchLoss = 2.9988, diffLoss = 14.6221, kgLoss = 0.0930
2025-02-13 00:30:39.893040: Training Step 38/59: batchLoss = 2.9240, diffLoss = 14.2820, kgLoss = 0.0845
2025-02-13 00:30:40.817183: Training Step 39/59: batchLoss = 2.8973, diffLoss = 14.1460, kgLoss = 0.0852
2025-02-13 00:30:41.743177: Training Step 40/59: batchLoss = 2.5014, diffLoss = 12.2211, kgLoss = 0.0715
2025-02-13 00:30:42.661890: Training Step 41/59: batchLoss = 3.1564, diffLoss = 15.4310, kgLoss = 0.0878
2025-02-13 00:30:43.595062: Training Step 42/59: batchLoss = 3.2009, diffLoss = 15.6412, kgLoss = 0.0908
2025-02-13 00:30:44.509805: Training Step 43/59: batchLoss = 3.1341, diffLoss = 15.3007, kgLoss = 0.0925
2025-02-13 00:30:45.435279: Training Step 44/59: batchLoss = 2.8698, diffLoss = 14.0270, kgLoss = 0.0805
2025-02-13 00:30:46.362131: Training Step 45/59: batchLoss = 3.0910, diffLoss = 15.0983, kgLoss = 0.0891
2025-02-13 00:30:47.285848: Training Step 46/59: batchLoss = 2.8309, diffLoss = 13.8408, kgLoss = 0.0784
2025-02-13 00:30:48.217528: Training Step 47/59: batchLoss = 2.7950, diffLoss = 13.6558, kgLoss = 0.0798
2025-02-13 00:30:49.136707: Training Step 48/59: batchLoss = 3.0295, diffLoss = 14.7950, kgLoss = 0.0881
2025-02-13 00:30:50.057489: Training Step 49/59: batchLoss = 3.1086, diffLoss = 15.1940, kgLoss = 0.0872
2025-02-13 00:30:50.977456: Training Step 50/59: batchLoss = 2.8942, diffLoss = 14.1334, kgLoss = 0.0844
2025-02-13 00:30:51.913185: Training Step 51/59: batchLoss = 3.2154, diffLoss = 15.7144, kgLoss = 0.0906
2025-02-13 00:30:52.838172: Training Step 52/59: batchLoss = 3.3261, diffLoss = 16.2515, kgLoss = 0.0948
2025-02-13 00:30:53.764665: Training Step 53/59: batchLoss = 3.1505, diffLoss = 15.4230, kgLoss = 0.0823
2025-02-13 00:30:54.693861: Training Step 54/59: batchLoss = 3.2578, diffLoss = 15.9058, kgLoss = 0.0958
2025-02-13 00:30:55.618863: Training Step 55/59: batchLoss = 2.7816, diffLoss = 13.5978, kgLoss = 0.0775
2025-02-13 00:30:56.539648: Training Step 56/59: batchLoss = 2.9393, diffLoss = 14.3625, kgLoss = 0.0835
2025-02-13 00:30:57.380419: Training Step 57/59: batchLoss = 3.1459, diffLoss = 15.3868, kgLoss = 0.0857
2025-02-13 00:30:58.235207: Training Step 58/59: batchLoss = 2.8999, diffLoss = 14.1737, kgLoss = 0.0814
2025-02-13 00:30:58.330610: 
2025-02-13 00:30:58.331182: Epoch 78/1000, Train: epLoss = 0.4342, epDfLoss = 2.1204, epKgLoss = 0.0126  
2025-02-13 00:30:59.818739: Steps 0/47: batch_recall = 36.16, batch_ndcg = 46.94 
2025-02-13 00:31:01.113383: Steps 1/47: batch_recall = 37.01, batch_ndcg = 42.57 
2025-02-13 00:31:02.370513: Steps 2/47: batch_recall = 42.57, batch_ndcg = 47.62 
2025-02-13 00:31:03.631370: Steps 3/47: batch_recall = 43.84, batch_ndcg = 46.67 
2025-02-13 00:31:04.848699: Steps 4/47: batch_recall = 39.60, batch_ndcg = 45.50 
2025-02-13 00:31:06.064984: Steps 5/47: batch_recall = 33.16, batch_ndcg = 39.82 
2025-02-13 00:31:07.272517: Steps 6/47: batch_recall = 40.30, batch_ndcg = 42.44 
2025-02-13 00:31:08.445558: Steps 7/47: batch_recall = 43.14, batch_ndcg = 44.50 
2025-02-13 00:31:09.628184: Steps 8/47: batch_recall = 48.29, batch_ndcg = 51.90 
2025-02-13 00:31:10.765810: Steps 9/47: batch_recall = 45.59, batch_ndcg = 45.80 
2025-02-13 00:31:11.927931: Steps 10/47: batch_recall = 45.04, batch_ndcg = 44.14 
2025-02-13 00:31:13.062268: Steps 11/47: batch_recall = 55.20, batch_ndcg = 52.92 
2025-02-13 00:31:14.199433: Steps 12/47: batch_recall = 50.63, batch_ndcg = 49.84 
2025-02-13 00:31:15.330297: Steps 13/47: batch_recall = 48.36, batch_ndcg = 45.22 
2025-02-13 00:31:16.416669: Steps 14/47: batch_recall = 40.46, batch_ndcg = 41.46 
2025-02-13 00:31:17.501073: Steps 15/47: batch_recall = 57.24, batch_ndcg = 54.85 
2025-02-13 00:31:18.588319: Steps 16/47: batch_recall = 50.95, batch_ndcg = 47.32 
2025-02-13 00:31:19.644857: Steps 17/47: batch_recall = 57.82, batch_ndcg = 49.55 
2025-02-13 00:31:20.741230: Steps 18/47: batch_recall = 53.30, batch_ndcg = 49.21 
2025-02-13 00:31:21.835007: Steps 19/47: batch_recall = 60.97, batch_ndcg = 55.95 
2025-02-13 00:31:22.894658: Steps 20/47: batch_recall = 66.63, batch_ndcg = 61.00 
2025-02-13 00:31:23.935267: Steps 21/47: batch_recall = 65.19, batch_ndcg = 55.70 
2025-02-13 00:31:25.001376: Steps 22/47: batch_recall = 55.53, batch_ndcg = 51.70 
2025-02-13 00:31:26.066415: Steps 23/47: batch_recall = 65.38, batch_ndcg = 55.78 
2025-02-13 00:31:27.147943: Steps 24/47: batch_recall = 64.32, batch_ndcg = 58.24 
2025-02-13 00:31:28.175738: Steps 25/47: batch_recall = 63.91, batch_ndcg = 56.77 
2025-02-13 00:31:29.177158: Steps 26/47: batch_recall = 59.07, batch_ndcg = 53.26 
2025-02-13 00:31:30.195545: Steps 27/47: batch_recall = 63.64, batch_ndcg = 53.55 
2025-02-13 00:31:31.207854: Steps 28/47: batch_recall = 70.56, batch_ndcg = 58.72 
2025-02-13 00:31:32.213548: Steps 29/47: batch_recall = 68.40, batch_ndcg = 57.51 
2025-02-13 00:31:33.218251: Steps 30/47: batch_recall = 72.74, batch_ndcg = 64.30 
2025-02-13 00:31:34.237695: Steps 31/47: batch_recall = 66.25, batch_ndcg = 55.86 
2025-02-13 00:31:35.240043: Steps 32/47: batch_recall = 70.18, batch_ndcg = 64.80 
2025-02-13 00:31:36.256473: Steps 33/47: batch_recall = 79.75, batch_ndcg = 68.16 
2025-02-13 00:31:37.261701: Steps 34/47: batch_recall = 68.59, batch_ndcg = 55.83 
2025-02-13 00:31:38.246710: Steps 35/47: batch_recall = 76.89, batch_ndcg = 65.66 
2025-02-13 00:31:39.243586: Steps 36/47: batch_recall = 78.06, batch_ndcg = 65.08 
2025-02-13 00:31:40.231185: Steps 37/47: batch_recall = 83.41, batch_ndcg = 72.61 
2025-02-13 00:31:41.218691: Steps 38/47: batch_recall = 89.21, batch_ndcg = 73.05 
2025-02-13 00:31:42.194051: Steps 39/47: batch_recall = 88.69, batch_ndcg = 69.94 
2025-02-13 00:31:43.166891: Steps 40/47: batch_recall = 72.08, batch_ndcg = 63.07 
2025-02-13 00:31:44.135496: Steps 41/47: batch_recall = 88.33, batch_ndcg = 72.85 
2025-02-13 00:31:45.096794: Steps 42/47: batch_recall = 81.84, batch_ndcg = 64.77 
2025-02-13 00:31:46.060140: Steps 43/47: batch_recall = 90.96, batch_ndcg = 72.92 
2025-02-13 00:31:47.034656: Steps 44/47: batch_recall = 88.71, batch_ndcg = 71.10 
2025-02-13 00:31:47.972863: Steps 45/47: batch_recall = 94.86, batch_ndcg = 75.21 
2025-02-13 00:31:48.078242: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.73 
2025-02-13 00:31:48.078380: Epoch 78/1000, Test: Recall = 0.1216, NDCG = 0.1096  

2025-02-13 00:31:49.281654: Training Step 0/59: batchLoss = 3.1319, diffLoss = 15.2709, kgLoss = 0.0972
2025-02-13 00:31:50.208888: Training Step 1/59: batchLoss = 2.9048, diffLoss = 14.1920, kgLoss = 0.0830
2025-02-13 00:31:51.130789: Training Step 2/59: batchLoss = 2.7921, diffLoss = 13.6021, kgLoss = 0.0896
2025-02-13 00:31:52.053129: Training Step 3/59: batchLoss = 3.0534, diffLoss = 14.8893, kgLoss = 0.0944
2025-02-13 00:31:52.979526: Training Step 4/59: batchLoss = 2.8255, diffLoss = 13.7742, kgLoss = 0.0883
2025-02-13 00:31:53.902492: Training Step 5/59: batchLoss = 2.8431, diffLoss = 13.8696, kgLoss = 0.0865
2025-02-13 00:31:54.823664: Training Step 6/59: batchLoss = 2.8477, diffLoss = 13.9159, kgLoss = 0.0807
2025-02-13 00:31:55.745459: Training Step 7/59: batchLoss = 2.9777, diffLoss = 14.5192, kgLoss = 0.0924
2025-02-13 00:31:56.667829: Training Step 8/59: batchLoss = 2.8574, diffLoss = 13.9549, kgLoss = 0.0831
2025-02-13 00:31:57.593095: Training Step 9/59: batchLoss = 3.0798, diffLoss = 15.0452, kgLoss = 0.0884
2025-02-13 00:31:58.515175: Training Step 10/59: batchLoss = 2.6223, diffLoss = 12.8197, kgLoss = 0.0730
2025-02-13 00:31:59.440734: Training Step 11/59: batchLoss = 2.8672, diffLoss = 13.9916, kgLoss = 0.0861
2025-02-13 00:32:00.375963: Training Step 12/59: batchLoss = 2.9535, diffLoss = 14.4216, kgLoss = 0.0864
2025-02-13 00:32:01.310048: Training Step 13/59: batchLoss = 3.1508, diffLoss = 15.3691, kgLoss = 0.0962
2025-02-13 00:32:02.240848: Training Step 14/59: batchLoss = 2.9235, diffLoss = 14.2677, kgLoss = 0.0875
2025-02-13 00:32:03.177839: Training Step 15/59: batchLoss = 2.9314, diffLoss = 14.3112, kgLoss = 0.0865
2025-02-13 00:32:04.111609: Training Step 16/59: batchLoss = 2.8120, diffLoss = 13.7471, kgLoss = 0.0782
2025-02-13 00:32:05.041914: Training Step 17/59: batchLoss = 2.9706, diffLoss = 14.4692, kgLoss = 0.0959
2025-02-13 00:32:05.968649: Training Step 18/59: batchLoss = 2.7441, diffLoss = 13.3887, kgLoss = 0.0829
2025-02-13 00:32:06.897882: Training Step 19/59: batchLoss = 2.9889, diffLoss = 14.5920, kgLoss = 0.0882
2025-02-13 00:32:07.830528: Training Step 20/59: batchLoss = 3.0290, diffLoss = 14.7622, kgLoss = 0.0958
2025-02-13 00:32:08.769049: Training Step 21/59: batchLoss = 2.9179, diffLoss = 14.2564, kgLoss = 0.0833
2025-02-13 00:32:09.692860: Training Step 22/59: batchLoss = 2.9935, diffLoss = 14.6222, kgLoss = 0.0863
2025-02-13 00:32:10.626803: Training Step 23/59: batchLoss = 2.7574, diffLoss = 13.4429, kgLoss = 0.0860
2025-02-13 00:32:11.548891: Training Step 24/59: batchLoss = 2.6187, diffLoss = 12.7789, kgLoss = 0.0787
2025-02-13 00:32:12.477892: Training Step 25/59: batchLoss = 3.4146, diffLoss = 16.6765, kgLoss = 0.0992
2025-02-13 00:32:13.402707: Training Step 26/59: batchLoss = 2.6155, diffLoss = 12.7938, kgLoss = 0.0710
2025-02-13 00:32:14.325278: Training Step 27/59: batchLoss = 2.7420, diffLoss = 13.3786, kgLoss = 0.0828
2025-02-13 00:32:15.251170: Training Step 28/59: batchLoss = 3.0788, diffLoss = 15.0182, kgLoss = 0.0940
2025-02-13 00:32:16.172984: Training Step 29/59: batchLoss = 2.7992, diffLoss = 13.6572, kgLoss = 0.0847
2025-02-13 00:32:17.098142: Training Step 30/59: batchLoss = 2.6446, diffLoss = 12.9155, kgLoss = 0.0769
2025-02-13 00:32:18.022754: Training Step 31/59: batchLoss = 2.8702, diffLoss = 14.0124, kgLoss = 0.0847
2025-02-13 00:32:18.939632: Training Step 32/59: batchLoss = 2.8249, diffLoss = 13.8129, kgLoss = 0.0778
2025-02-13 00:32:19.856902: Training Step 33/59: batchLoss = 3.0361, diffLoss = 14.8510, kgLoss = 0.0824
2025-02-13 00:32:20.772617: Training Step 34/59: batchLoss = 2.9708, diffLoss = 14.5226, kgLoss = 0.0828
2025-02-13 00:32:21.701147: Training Step 35/59: batchLoss = 2.9829, diffLoss = 14.5589, kgLoss = 0.0889
2025-02-13 00:32:22.628494: Training Step 36/59: batchLoss = 2.9467, diffLoss = 14.3811, kgLoss = 0.0881
2025-02-13 00:32:23.565897: Training Step 37/59: batchLoss = 3.2615, diffLoss = 15.9438, kgLoss = 0.0909
2025-02-13 00:32:24.502687: Training Step 38/59: batchLoss = 2.9539, diffLoss = 14.4467, kgLoss = 0.0807
2025-02-13 00:32:25.437395: Training Step 39/59: batchLoss = 2.9887, diffLoss = 14.6217, kgLoss = 0.0805
2025-02-13 00:32:26.371900: Training Step 40/59: batchLoss = 2.8454, diffLoss = 13.9034, kgLoss = 0.0809
2025-02-13 00:32:27.309198: Training Step 41/59: batchLoss = 3.0332, diffLoss = 14.8369, kgLoss = 0.0823
2025-02-13 00:32:28.238389: Training Step 42/59: batchLoss = 2.6532, diffLoss = 12.9651, kgLoss = 0.0753
2025-02-13 00:32:29.188950: Training Step 43/59: batchLoss = 3.2099, diffLoss = 15.7075, kgLoss = 0.0855
2025-02-13 00:32:30.128659: Training Step 44/59: batchLoss = 3.3201, diffLoss = 16.2110, kgLoss = 0.0974
2025-02-13 00:32:31.053224: Training Step 45/59: batchLoss = 2.8104, diffLoss = 13.7274, kgLoss = 0.0811
2025-02-13 00:32:31.979176: Training Step 46/59: batchLoss = 2.7826, diffLoss = 13.6034, kgLoss = 0.0774
2025-02-13 00:32:32.907197: Training Step 47/59: batchLoss = 3.0056, diffLoss = 14.6251, kgLoss = 0.1007
2025-02-13 00:32:33.834396: Training Step 48/59: batchLoss = 2.7952, diffLoss = 13.6400, kgLoss = 0.0840
2025-02-13 00:32:34.770176: Training Step 49/59: batchLoss = 3.0175, diffLoss = 14.7327, kgLoss = 0.0887
2025-02-13 00:32:35.693653: Training Step 50/59: batchLoss = 2.4333, diffLoss = 11.8992, kgLoss = 0.0668
2025-02-13 00:32:36.621126: Training Step 51/59: batchLoss = 2.7216, diffLoss = 13.3068, kgLoss = 0.0752
2025-02-13 00:32:37.551081: Training Step 52/59: batchLoss = 3.1157, diffLoss = 15.2402, kgLoss = 0.0846
2025-02-13 00:32:38.488064: Training Step 53/59: batchLoss = 3.2210, diffLoss = 15.7422, kgLoss = 0.0907
2025-02-13 00:32:39.421524: Training Step 54/59: batchLoss = 2.8986, diffLoss = 14.1673, kgLoss = 0.0814
2025-02-13 00:32:40.347824: Training Step 55/59: batchLoss = 2.8120, diffLoss = 13.7475, kgLoss = 0.0781
2025-02-13 00:32:41.275310: Training Step 56/59: batchLoss = 3.1851, diffLoss = 15.5765, kgLoss = 0.0872
2025-02-13 00:32:42.126292: Training Step 57/59: batchLoss = 3.2734, diffLoss = 15.9869, kgLoss = 0.0950
2025-02-13 00:32:42.981591: Training Step 58/59: batchLoss = 3.3570, diffLoss = 16.4064, kgLoss = 0.0947
2025-02-13 00:32:43.083559: 
2025-02-13 00:32:43.083894: Epoch 79/1000, Train: epLoss = 0.4330, epDfLoss = 2.1147, epKgLoss = 0.0126  
2025-02-13 00:32:44.567395: Steps 0/47: batch_recall = 36.77, batch_ndcg = 47.41 
2025-02-13 00:32:45.887408: Steps 1/47: batch_recall = 36.75, batch_ndcg = 42.60 
2025-02-13 00:32:47.151533: Steps 2/47: batch_recall = 42.53, batch_ndcg = 47.73 
2025-02-13 00:32:48.419797: Steps 3/47: batch_recall = 44.25, batch_ndcg = 47.53 
2025-02-13 00:32:49.626209: Steps 4/47: batch_recall = 39.62, batch_ndcg = 45.54 
2025-02-13 00:32:50.851500: Steps 5/47: batch_recall = 33.61, batch_ndcg = 40.26 
2025-02-13 00:32:52.044731: Steps 6/47: batch_recall = 40.20, batch_ndcg = 42.66 
2025-02-13 00:32:53.201614: Steps 7/47: batch_recall = 43.89, batch_ndcg = 45.10 
2025-02-13 00:32:54.361261: Steps 8/47: batch_recall = 48.06, batch_ndcg = 51.83 
2025-02-13 00:32:55.492676: Steps 9/47: batch_recall = 45.89, batch_ndcg = 46.17 
2025-02-13 00:32:56.645629: Steps 10/47: batch_recall = 43.23, batch_ndcg = 43.56 
2025-02-13 00:32:57.773636: Steps 11/47: batch_recall = 54.03, batch_ndcg = 52.16 
2025-02-13 00:32:58.912779: Steps 12/47: batch_recall = 51.22, batch_ndcg = 49.36 
2025-02-13 00:33:00.040174: Steps 13/47: batch_recall = 49.73, batch_ndcg = 45.91 
2025-02-13 00:33:01.124546: Steps 14/47: batch_recall = 42.39, batch_ndcg = 41.85 
2025-02-13 00:33:02.219902: Steps 15/47: batch_recall = 56.78, batch_ndcg = 54.15 
2025-02-13 00:33:03.313998: Steps 16/47: batch_recall = 51.97, batch_ndcg = 47.91 
2025-02-13 00:33:04.378798: Steps 17/47: batch_recall = 58.16, batch_ndcg = 49.89 
2025-02-13 00:33:05.469959: Steps 18/47: batch_recall = 52.96, batch_ndcg = 49.80 
2025-02-13 00:33:06.546444: Steps 19/47: batch_recall = 60.23, batch_ndcg = 55.40 
2025-02-13 00:33:07.597867: Steps 20/47: batch_recall = 67.01, batch_ndcg = 62.20 
2025-02-13 00:33:08.665202: Steps 21/47: batch_recall = 65.22, batch_ndcg = 56.31 
2025-02-13 00:33:09.724183: Steps 22/47: batch_recall = 56.63, batch_ndcg = 51.97 
2025-02-13 00:33:10.786394: Steps 23/47: batch_recall = 65.64, batch_ndcg = 56.27 
2025-02-13 00:33:11.860880: Steps 24/47: batch_recall = 65.07, batch_ndcg = 57.33 
2025-02-13 00:33:12.899054: Steps 25/47: batch_recall = 65.70, batch_ndcg = 57.71 
2025-02-13 00:33:13.910203: Steps 26/47: batch_recall = 60.32, batch_ndcg = 53.23 
2025-02-13 00:33:14.947808: Steps 27/47: batch_recall = 63.08, batch_ndcg = 53.66 
2025-02-13 00:33:15.969738: Steps 28/47: batch_recall = 70.67, batch_ndcg = 59.88 
2025-02-13 00:33:16.973247: Steps 29/47: batch_recall = 70.16, batch_ndcg = 58.87 
2025-02-13 00:33:17.994307: Steps 30/47: batch_recall = 72.57, batch_ndcg = 63.57 
2025-02-13 00:33:19.014441: Steps 31/47: batch_recall = 66.52, batch_ndcg = 56.06 
2025-02-13 00:33:20.021754: Steps 32/47: batch_recall = 70.80, batch_ndcg = 64.79 
2025-02-13 00:33:21.027701: Steps 33/47: batch_recall = 79.03, batch_ndcg = 67.10 
2025-02-13 00:33:22.033153: Steps 34/47: batch_recall = 66.88, batch_ndcg = 56.24 
2025-02-13 00:33:23.021156: Steps 35/47: batch_recall = 75.08, batch_ndcg = 64.97 
2025-02-13 00:33:24.013689: Steps 36/47: batch_recall = 79.40, batch_ndcg = 65.47 
2025-02-13 00:33:24.992760: Steps 37/47: batch_recall = 84.30, batch_ndcg = 73.55 
2025-02-13 00:33:25.978064: Steps 38/47: batch_recall = 90.33, batch_ndcg = 72.79 
2025-02-13 00:33:26.962158: Steps 39/47: batch_recall = 87.38, batch_ndcg = 68.76 
2025-02-13 00:33:27.934607: Steps 40/47: batch_recall = 71.99, batch_ndcg = 62.98 
2025-02-13 00:33:28.908004: Steps 41/47: batch_recall = 87.92, batch_ndcg = 72.04 
2025-02-13 00:33:29.879234: Steps 42/47: batch_recall = 81.76, batch_ndcg = 65.26 
2025-02-13 00:33:30.844802: Steps 43/47: batch_recall = 89.82, batch_ndcg = 72.65 
2025-02-13 00:33:31.808836: Steps 44/47: batch_recall = 87.27, batch_ndcg = 69.16 
2025-02-13 00:33:32.766735: Steps 45/47: batch_recall = 93.84, batch_ndcg = 75.30 
2025-02-13 00:33:32.875646: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.67 
2025-02-13 00:33:32.875975: Epoch 79/1000, Test: Recall = 0.1217, NDCG = 0.1098  

2025-02-13 00:33:34.060444: Training Step 0/59: batchLoss = 2.5906, diffLoss = 12.6395, kgLoss = 0.0784
2025-02-13 00:33:34.980397: Training Step 1/59: batchLoss = 3.0615, diffLoss = 14.9408, kgLoss = 0.0916
2025-02-13 00:33:35.899782: Training Step 2/59: batchLoss = 2.9497, diffLoss = 14.3591, kgLoss = 0.0974
2025-02-13 00:33:36.819669: Training Step 3/59: batchLoss = 3.0068, diffLoss = 14.6702, kgLoss = 0.0910
2025-02-13 00:33:37.743968: Training Step 4/59: batchLoss = 2.7765, diffLoss = 13.5459, kgLoss = 0.0841
2025-02-13 00:33:38.665960: Training Step 5/59: batchLoss = 2.9943, diffLoss = 14.5984, kgLoss = 0.0933
2025-02-13 00:33:39.582265: Training Step 6/59: batchLoss = 2.8087, diffLoss = 13.6936, kgLoss = 0.0874
2025-02-13 00:33:40.511979: Training Step 7/59: batchLoss = 2.5785, diffLoss = 12.5525, kgLoss = 0.0850
2025-02-13 00:33:41.428913: Training Step 8/59: batchLoss = 2.7783, diffLoss = 13.5605, kgLoss = 0.0827
2025-02-13 00:33:42.355118: Training Step 9/59: batchLoss = 2.8865, diffLoss = 14.1043, kgLoss = 0.0820
2025-02-13 00:33:43.272512: Training Step 10/59: batchLoss = 2.6196, diffLoss = 12.7745, kgLoss = 0.0809
2025-02-13 00:33:44.207624: Training Step 11/59: batchLoss = 2.8665, diffLoss = 13.9713, kgLoss = 0.0903
2025-02-13 00:33:45.144590: Training Step 12/59: batchLoss = 2.7029, diffLoss = 13.1888, kgLoss = 0.0814
2025-02-13 00:33:46.083831: Training Step 13/59: batchLoss = 3.0875, diffLoss = 15.0981, kgLoss = 0.0849
2025-02-13 00:33:47.016163: Training Step 14/59: batchLoss = 2.8177, diffLoss = 13.7574, kgLoss = 0.0828
2025-02-13 00:33:47.942927: Training Step 15/59: batchLoss = 3.3132, diffLoss = 16.1882, kgLoss = 0.0945
2025-02-13 00:33:48.878105: Training Step 16/59: batchLoss = 2.8006, diffLoss = 13.6495, kgLoss = 0.0884
2025-02-13 00:33:49.803457: Training Step 17/59: batchLoss = 2.6407, diffLoss = 12.8839, kgLoss = 0.0799
2025-02-13 00:33:50.745025: Training Step 18/59: batchLoss = 2.7759, diffLoss = 13.5488, kgLoss = 0.0827
2025-02-13 00:33:51.678037: Training Step 19/59: batchLoss = 2.9191, diffLoss = 14.2399, kgLoss = 0.0889
2025-02-13 00:33:52.612623: Training Step 20/59: batchLoss = 2.8240, diffLoss = 13.8012, kgLoss = 0.0797
2025-02-13 00:33:53.547629: Training Step 21/59: batchLoss = 2.7686, diffLoss = 13.5202, kgLoss = 0.0807
2025-02-13 00:33:54.472455: Training Step 22/59: batchLoss = 2.9918, diffLoss = 14.6011, kgLoss = 0.0895
2025-02-13 00:33:55.414241: Training Step 23/59: batchLoss = 2.8454, diffLoss = 13.8899, kgLoss = 0.0843
2025-02-13 00:33:56.343672: Training Step 24/59: batchLoss = 2.7564, diffLoss = 13.4569, kgLoss = 0.0813
2025-02-13 00:33:57.264519: Training Step 25/59: batchLoss = 2.9694, diffLoss = 14.4660, kgLoss = 0.0953
2025-02-13 00:33:58.196786: Training Step 26/59: batchLoss = 2.7361, diffLoss = 13.3534, kgLoss = 0.0818
2025-02-13 00:33:59.123532: Training Step 27/59: batchLoss = 2.9256, diffLoss = 14.2327, kgLoss = 0.0988
2025-02-13 00:34:00.053494: Training Step 28/59: batchLoss = 2.9282, diffLoss = 14.2909, kgLoss = 0.0876
2025-02-13 00:34:00.975068: Training Step 29/59: batchLoss = 3.0272, diffLoss = 14.7936, kgLoss = 0.0856
2025-02-13 00:34:01.899319: Training Step 30/59: batchLoss = 3.1732, diffLoss = 15.4810, kgLoss = 0.0962
2025-02-13 00:34:02.827908: Training Step 31/59: batchLoss = 2.7965, diffLoss = 13.6643, kgLoss = 0.0795
2025-02-13 00:34:03.763339: Training Step 32/59: batchLoss = 2.6961, diffLoss = 13.1727, kgLoss = 0.0770
2025-02-13 00:34:04.688673: Training Step 33/59: batchLoss = 3.1578, diffLoss = 15.4229, kgLoss = 0.0916
2025-02-13 00:34:05.619488: Training Step 34/59: batchLoss = 3.0356, diffLoss = 14.8146, kgLoss = 0.0908
2025-02-13 00:34:06.549990: Training Step 35/59: batchLoss = 3.0246, diffLoss = 14.7678, kgLoss = 0.0888
2025-02-13 00:34:07.477223: Training Step 36/59: batchLoss = 2.9810, diffLoss = 14.5681, kgLoss = 0.0842
2025-02-13 00:34:08.412571: Training Step 37/59: batchLoss = 2.6619, diffLoss = 12.9887, kgLoss = 0.0801
2025-02-13 00:34:09.344602: Training Step 38/59: batchLoss = 3.0717, diffLoss = 14.9994, kgLoss = 0.0898
2025-02-13 00:34:10.275562: Training Step 39/59: batchLoss = 3.0569, diffLoss = 14.9241, kgLoss = 0.0901
2025-02-13 00:34:11.209976: Training Step 40/59: batchLoss = 3.0094, diffLoss = 14.6964, kgLoss = 0.0877
2025-02-13 00:34:12.147169: Training Step 41/59: batchLoss = 2.8044, diffLoss = 13.7262, kgLoss = 0.0740
2025-02-13 00:34:13.084575: Training Step 42/59: batchLoss = 2.8224, diffLoss = 13.7867, kgLoss = 0.0814
2025-02-13 00:34:14.019942: Training Step 43/59: batchLoss = 3.0012, diffLoss = 14.6402, kgLoss = 0.0914
2025-02-13 00:34:14.948922: Training Step 44/59: batchLoss = 2.8948, diffLoss = 14.1383, kgLoss = 0.0840
2025-02-13 00:34:15.876747: Training Step 45/59: batchLoss = 2.8722, diffLoss = 14.0434, kgLoss = 0.0794
2025-02-13 00:34:16.795001: Training Step 46/59: batchLoss = 2.9804, diffLoss = 14.5710, kgLoss = 0.0828
2025-02-13 00:34:17.712696: Training Step 47/59: batchLoss = 3.1998, diffLoss = 15.6287, kgLoss = 0.0926
2025-02-13 00:34:18.632297: Training Step 48/59: batchLoss = 2.8816, diffLoss = 14.0814, kgLoss = 0.0816
2025-02-13 00:34:19.552418: Training Step 49/59: batchLoss = 3.0265, diffLoss = 14.7887, kgLoss = 0.0859
2025-02-13 00:34:20.470782: Training Step 50/59: batchLoss = 3.1181, diffLoss = 15.2188, kgLoss = 0.0929
2025-02-13 00:34:21.397203: Training Step 51/59: batchLoss = 2.8973, diffLoss = 14.1467, kgLoss = 0.0850
2025-02-13 00:34:22.319095: Training Step 52/59: batchLoss = 2.8063, diffLoss = 13.7385, kgLoss = 0.0733
2025-02-13 00:34:23.234671: Training Step 53/59: batchLoss = 3.0562, diffLoss = 14.9473, kgLoss = 0.0834
2025-02-13 00:34:24.161263: Training Step 54/59: batchLoss = 2.9477, diffLoss = 14.4212, kgLoss = 0.0793
2025-02-13 00:34:25.078425: Training Step 55/59: batchLoss = 2.9503, diffLoss = 14.4281, kgLoss = 0.0809
2025-02-13 00:34:25.999893: Training Step 56/59: batchLoss = 2.8643, diffLoss = 13.9979, kgLoss = 0.0809
2025-02-13 00:34:26.844519: Training Step 57/59: batchLoss = 3.2029, diffLoss = 15.6641, kgLoss = 0.0876
2025-02-13 00:34:27.699798: Training Step 58/59: batchLoss = 2.9766, diffLoss = 14.5524, kgLoss = 0.0826
2025-02-13 00:34:27.800198: 
2025-02-13 00:34:27.800712: Epoch 80/1000, Train: epLoss = 0.4293, epDfLoss = 2.0960, epKgLoss = 0.0126  
2025-02-13 00:34:29.286594: Steps 0/47: batch_recall = 35.83, batch_ndcg = 46.97 
2025-02-13 00:34:30.593379: Steps 1/47: batch_recall = 37.32, batch_ndcg = 42.40 
2025-02-13 00:34:31.856010: Steps 2/47: batch_recall = 41.30, batch_ndcg = 47.56 
2025-02-13 00:34:33.125510: Steps 3/47: batch_recall = 44.34, batch_ndcg = 47.27 
2025-02-13 00:34:34.340802: Steps 4/47: batch_recall = 39.61, batch_ndcg = 46.03 
2025-02-13 00:34:35.563528: Steps 5/47: batch_recall = 33.63, batch_ndcg = 40.46 
2025-02-13 00:34:36.756408: Steps 6/47: batch_recall = 39.88, batch_ndcg = 42.07 
2025-02-13 00:34:37.927733: Steps 7/47: batch_recall = 44.49, batch_ndcg = 45.81 
2025-02-13 00:34:39.102019: Steps 8/47: batch_recall = 48.93, batch_ndcg = 52.88 
2025-02-13 00:34:40.228140: Steps 9/47: batch_recall = 44.90, batch_ndcg = 46.24 
2025-02-13 00:34:41.376788: Steps 10/47: batch_recall = 43.01, batch_ndcg = 43.33 
2025-02-13 00:34:42.508416: Steps 11/47: batch_recall = 54.74, batch_ndcg = 52.40 
2025-02-13 00:34:43.648215: Steps 12/47: batch_recall = 50.42, batch_ndcg = 48.96 
2025-02-13 00:34:44.767024: Steps 13/47: batch_recall = 49.27, batch_ndcg = 45.82 
2025-02-13 00:34:45.860516: Steps 14/47: batch_recall = 40.35, batch_ndcg = 40.80 
2025-02-13 00:34:46.950849: Steps 15/47: batch_recall = 57.91, batch_ndcg = 54.37 
2025-02-13 00:34:48.036045: Steps 16/47: batch_recall = 50.09, batch_ndcg = 47.75 
2025-02-13 00:34:49.086472: Steps 17/47: batch_recall = 58.27, batch_ndcg = 50.29 
2025-02-13 00:34:50.186407: Steps 18/47: batch_recall = 53.93, batch_ndcg = 50.39 
2025-02-13 00:34:51.250032: Steps 19/47: batch_recall = 60.62, batch_ndcg = 54.88 
2025-02-13 00:34:52.305646: Steps 20/47: batch_recall = 66.07, batch_ndcg = 60.93 
2025-02-13 00:34:53.371712: Steps 21/47: batch_recall = 64.35, batch_ndcg = 55.11 
2025-02-13 00:34:54.426445: Steps 22/47: batch_recall = 56.67, batch_ndcg = 52.12 
2025-02-13 00:34:55.477299: Steps 23/47: batch_recall = 63.39, batch_ndcg = 55.65 
2025-02-13 00:34:56.545646: Steps 24/47: batch_recall = 64.52, batch_ndcg = 57.44 
2025-02-13 00:34:57.580473: Steps 25/47: batch_recall = 64.65, batch_ndcg = 56.68 
2025-02-13 00:34:58.586281: Steps 26/47: batch_recall = 62.26, batch_ndcg = 54.06 
2025-02-13 00:34:59.625936: Steps 27/47: batch_recall = 61.90, batch_ndcg = 53.09 
2025-02-13 00:35:00.641122: Steps 28/47: batch_recall = 71.07, batch_ndcg = 59.14 
2025-02-13 00:35:01.651399: Steps 29/47: batch_recall = 68.84, batch_ndcg = 58.19 
2025-02-13 00:35:02.657521: Steps 30/47: batch_recall = 74.02, batch_ndcg = 64.26 
2025-02-13 00:35:03.676862: Steps 31/47: batch_recall = 67.05, batch_ndcg = 55.99 
2025-02-13 00:35:04.688858: Steps 32/47: batch_recall = 70.65, batch_ndcg = 65.34 
2025-02-13 00:35:05.692442: Steps 33/47: batch_recall = 78.87, batch_ndcg = 67.11 
2025-02-13 00:35:06.693982: Steps 34/47: batch_recall = 68.37, batch_ndcg = 56.33 
2025-02-13 00:35:07.680135: Steps 35/47: batch_recall = 77.75, batch_ndcg = 66.12 
2025-02-13 00:35:08.670790: Steps 36/47: batch_recall = 79.14, batch_ndcg = 65.39 
2025-02-13 00:35:09.651999: Steps 37/47: batch_recall = 85.21, batch_ndcg = 72.84 
2025-02-13 00:35:10.633098: Steps 38/47: batch_recall = 88.48, batch_ndcg = 72.42 
2025-02-13 00:35:11.614421: Steps 39/47: batch_recall = 88.97, batch_ndcg = 69.47 
2025-02-13 00:35:12.585370: Steps 40/47: batch_recall = 73.44, batch_ndcg = 63.65 
2025-02-13 00:35:13.557244: Steps 41/47: batch_recall = 87.53, batch_ndcg = 72.20 
2025-02-13 00:35:14.523742: Steps 42/47: batch_recall = 83.39, batch_ndcg = 65.61 
2025-02-13 00:35:15.496838: Steps 43/47: batch_recall = 90.07, batch_ndcg = 72.58 
2025-02-13 00:35:16.474475: Steps 44/47: batch_recall = 86.47, batch_ndcg = 70.06 
2025-02-13 00:35:17.424567: Steps 45/47: batch_recall = 92.81, batch_ndcg = 73.68 
2025-02-13 00:35:17.527593: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.72 
2025-02-13 00:35:17.527727: Epoch 80/1000, Test: Recall = 0.1216, NDCG = 0.1096  

2025-02-13 00:35:18.724753: Training Step 0/59: batchLoss = 3.1621, diffLoss = 15.4289, kgLoss = 0.0954
2025-02-13 00:35:19.652077: Training Step 1/59: batchLoss = 2.6882, diffLoss = 13.1095, kgLoss = 0.0829
2025-02-13 00:35:20.575945: Training Step 2/59: batchLoss = 2.6730, diffLoss = 13.0235, kgLoss = 0.0854
2025-02-13 00:35:21.501618: Training Step 3/59: batchLoss = 2.7885, diffLoss = 13.6250, kgLoss = 0.0794
2025-02-13 00:35:22.428368: Training Step 4/59: batchLoss = 2.7971, diffLoss = 13.6349, kgLoss = 0.0877
2025-02-13 00:35:23.346580: Training Step 5/59: batchLoss = 3.1387, diffLoss = 15.2885, kgLoss = 0.1013
2025-02-13 00:35:24.263613: Training Step 6/59: batchLoss = 3.0314, diffLoss = 14.7827, kgLoss = 0.0935
2025-02-13 00:35:25.182104: Training Step 7/59: batchLoss = 2.6514, diffLoss = 12.9196, kgLoss = 0.0844
2025-02-13 00:35:26.103431: Training Step 8/59: batchLoss = 2.8796, diffLoss = 14.0799, kgLoss = 0.0795
2025-02-13 00:35:27.028482: Training Step 9/59: batchLoss = 2.4621, diffLoss = 11.9994, kgLoss = 0.0778
2025-02-13 00:35:27.948892: Training Step 10/59: batchLoss = 2.7854, diffLoss = 13.5948, kgLoss = 0.0830
2025-02-13 00:35:28.872733: Training Step 11/59: batchLoss = 3.0243, diffLoss = 14.7758, kgLoss = 0.0864
2025-02-13 00:35:29.797602: Training Step 12/59: batchLoss = 2.8961, diffLoss = 14.1498, kgLoss = 0.0827
2025-02-13 00:35:30.729619: Training Step 13/59: batchLoss = 2.9740, diffLoss = 14.5091, kgLoss = 0.0902
2025-02-13 00:35:31.668191: Training Step 14/59: batchLoss = 3.3551, diffLoss = 16.3553, kgLoss = 0.1050
2025-02-13 00:35:32.606848: Training Step 15/59: batchLoss = 3.0617, diffLoss = 14.9664, kgLoss = 0.0855
2025-02-13 00:35:33.545436: Training Step 16/59: batchLoss = 2.6504, diffLoss = 12.9413, kgLoss = 0.0776
2025-02-13 00:35:34.475797: Training Step 17/59: batchLoss = 2.7645, diffLoss = 13.5290, kgLoss = 0.0734
2025-02-13 00:35:35.405764: Training Step 18/59: batchLoss = 2.5293, diffLoss = 12.3309, kgLoss = 0.0789
2025-02-13 00:35:36.339008: Training Step 19/59: batchLoss = 3.0475, diffLoss = 14.9075, kgLoss = 0.0825
2025-02-13 00:35:37.275755: Training Step 20/59: batchLoss = 2.8239, diffLoss = 13.8094, kgLoss = 0.0775
2025-02-13 00:35:38.210494: Training Step 21/59: batchLoss = 3.0800, diffLoss = 15.0607, kgLoss = 0.0848
2025-02-13 00:35:39.133532: Training Step 22/59: batchLoss = 3.0707, diffLoss = 14.9869, kgLoss = 0.0917
2025-02-13 00:35:40.058935: Training Step 23/59: batchLoss = 2.7625, diffLoss = 13.4811, kgLoss = 0.0829
2025-02-13 00:35:40.979288: Training Step 24/59: batchLoss = 2.8247, diffLoss = 13.8033, kgLoss = 0.0800
2025-02-13 00:35:41.910613: Training Step 25/59: batchLoss = 2.9314, diffLoss = 14.3312, kgLoss = 0.0814
2025-02-13 00:35:42.840551: Training Step 26/59: batchLoss = 3.0379, diffLoss = 14.8367, kgLoss = 0.0882
2025-02-13 00:35:43.770176: Training Step 27/59: batchLoss = 2.8199, diffLoss = 13.7551, kgLoss = 0.0861
2025-02-13 00:35:44.699002: Training Step 28/59: batchLoss = 3.0143, diffLoss = 14.7517, kgLoss = 0.0799
2025-02-13 00:35:45.623604: Training Step 29/59: batchLoss = 3.3246, diffLoss = 16.2283, kgLoss = 0.0987
2025-02-13 00:35:46.552075: Training Step 30/59: batchLoss = 2.8149, diffLoss = 13.7646, kgLoss = 0.0774
2025-02-13 00:35:47.479191: Training Step 31/59: batchLoss = 2.9473, diffLoss = 14.4236, kgLoss = 0.0782
2025-02-13 00:35:48.402004: Training Step 32/59: batchLoss = 2.8836, diffLoss = 14.0994, kgLoss = 0.0797
2025-02-13 00:35:49.318892: Training Step 33/59: batchLoss = 3.1959, diffLoss = 15.5890, kgLoss = 0.0977
2025-02-13 00:35:50.239290: Training Step 34/59: batchLoss = 3.1545, diffLoss = 15.4222, kgLoss = 0.0876
2025-02-13 00:35:51.173509: Training Step 35/59: batchLoss = 2.8742, diffLoss = 14.0568, kgLoss = 0.0786
2025-02-13 00:35:52.101637: Training Step 36/59: batchLoss = 3.1463, diffLoss = 15.3623, kgLoss = 0.0922
2025-02-13 00:35:53.043076: Training Step 37/59: batchLoss = 2.8595, diffLoss = 13.9681, kgLoss = 0.0823
2025-02-13 00:35:53.980535: Training Step 38/59: batchLoss = 2.9824, diffLoss = 14.5500, kgLoss = 0.0905
2025-02-13 00:35:54.924759: Training Step 39/59: batchLoss = 3.3696, diffLoss = 16.4725, kgLoss = 0.0939
2025-02-13 00:35:55.857528: Training Step 40/59: batchLoss = 3.0306, diffLoss = 14.8153, kgLoss = 0.0844
2025-02-13 00:35:56.791784: Training Step 41/59: batchLoss = 3.0111, diffLoss = 14.7409, kgLoss = 0.0787
2025-02-13 00:35:57.739049: Training Step 42/59: batchLoss = 3.0828, diffLoss = 15.0828, kgLoss = 0.0827
2025-02-13 00:35:58.670786: Training Step 43/59: batchLoss = 3.0204, diffLoss = 14.7605, kgLoss = 0.0854
2025-02-13 00:35:59.610114: Training Step 44/59: batchLoss = 3.0809, diffLoss = 15.0274, kgLoss = 0.0943
2025-02-13 00:36:00.537170: Training Step 45/59: batchLoss = 3.2879, diffLoss = 16.0589, kgLoss = 0.0952
2025-02-13 00:36:01.459217: Training Step 46/59: batchLoss = 3.1146, diffLoss = 15.2071, kgLoss = 0.0914
2025-02-13 00:36:02.382599: Training Step 47/59: batchLoss = 3.1731, diffLoss = 15.4965, kgLoss = 0.0923
2025-02-13 00:36:03.307447: Training Step 48/59: batchLoss = 2.8976, diffLoss = 14.1468, kgLoss = 0.0853
2025-02-13 00:36:04.251009: Training Step 49/59: batchLoss = 2.9767, diffLoss = 14.5309, kgLoss = 0.0882
2025-02-13 00:36:05.179534: Training Step 50/59: batchLoss = 2.7500, diffLoss = 13.4432, kgLoss = 0.0767
2025-02-13 00:36:06.114572: Training Step 51/59: batchLoss = 2.6800, diffLoss = 13.0611, kgLoss = 0.0847
2025-02-13 00:36:07.038659: Training Step 52/59: batchLoss = 2.8653, diffLoss = 13.9990, kgLoss = 0.0819
2025-02-13 00:36:07.959175: Training Step 53/59: batchLoss = 3.3308, diffLoss = 16.2883, kgLoss = 0.0914
2025-02-13 00:36:08.889100: Training Step 54/59: batchLoss = 2.8726, diffLoss = 14.0565, kgLoss = 0.0766
2025-02-13 00:36:09.815715: Training Step 55/59: batchLoss = 2.9545, diffLoss = 14.4330, kgLoss = 0.0849
2025-02-13 00:36:10.737666: Training Step 56/59: batchLoss = 3.0067, diffLoss = 14.7086, kgLoss = 0.0812
2025-02-13 00:36:11.579782: Training Step 57/59: batchLoss = 3.1773, diffLoss = 15.5193, kgLoss = 0.0917
2025-02-13 00:36:12.437759: Training Step 58/59: batchLoss = 2.9225, diffLoss = 14.3057, kgLoss = 0.0768
2025-02-13 00:36:12.531773: 
2025-02-13 00:36:12.532315: Epoch 81/1000, Train: epLoss = 0.4363, epDfLoss = 2.1310, epKgLoss = 0.0126  
2025-02-13 00:36:14.009104: Steps 0/47: batch_recall = 36.96, batch_ndcg = 47.39 
2025-02-13 00:36:15.310274: Steps 1/47: batch_recall = 37.10, batch_ndcg = 42.70 
2025-02-13 00:36:16.584302: Steps 2/47: batch_recall = 41.41, batch_ndcg = 48.04 
2025-02-13 00:36:17.852137: Steps 3/47: batch_recall = 44.08, batch_ndcg = 46.03 
2025-02-13 00:36:19.061908: Steps 4/47: batch_recall = 39.92, batch_ndcg = 46.14 
2025-02-13 00:36:20.284589: Steps 5/47: batch_recall = 33.64, batch_ndcg = 41.00 
2025-02-13 00:36:21.488158: Steps 6/47: batch_recall = 39.28, batch_ndcg = 41.97 
2025-02-13 00:36:22.649382: Steps 7/47: batch_recall = 42.92, batch_ndcg = 45.07 
2025-02-13 00:36:23.823824: Steps 8/47: batch_recall = 48.46, batch_ndcg = 52.63 
2025-02-13 00:36:24.961480: Steps 9/47: batch_recall = 45.75, batch_ndcg = 46.69 
2025-02-13 00:36:26.117347: Steps 10/47: batch_recall = 42.74, batch_ndcg = 43.06 
2025-02-13 00:36:27.249636: Steps 11/47: batch_recall = 54.10, batch_ndcg = 52.31 
2025-02-13 00:36:28.391561: Steps 12/47: batch_recall = 50.91, batch_ndcg = 50.38 
2025-02-13 00:36:29.521586: Steps 13/47: batch_recall = 49.85, batch_ndcg = 45.72 
2025-02-13 00:36:30.611089: Steps 14/47: batch_recall = 39.92, batch_ndcg = 40.60 
2025-02-13 00:36:31.705647: Steps 15/47: batch_recall = 57.05, batch_ndcg = 53.37 
2025-02-13 00:36:32.801327: Steps 16/47: batch_recall = 52.17, batch_ndcg = 47.68 
2025-02-13 00:36:33.870711: Steps 17/47: batch_recall = 58.50, batch_ndcg = 49.94 
2025-02-13 00:36:34.955969: Steps 18/47: batch_recall = 52.95, batch_ndcg = 50.23 
2025-02-13 00:36:36.035207: Steps 19/47: batch_recall = 60.42, batch_ndcg = 55.81 
2025-02-13 00:36:37.091538: Steps 20/47: batch_recall = 67.54, batch_ndcg = 62.54 
2025-02-13 00:36:38.152177: Steps 21/47: batch_recall = 65.27, batch_ndcg = 56.18 
2025-02-13 00:36:39.211550: Steps 22/47: batch_recall = 55.18, batch_ndcg = 51.18 
2025-02-13 00:36:40.277112: Steps 23/47: batch_recall = 64.62, batch_ndcg = 55.64 
2025-02-13 00:36:41.343947: Steps 24/47: batch_recall = 65.54, batch_ndcg = 58.70 
2025-02-13 00:36:42.383273: Steps 25/47: batch_recall = 65.88, batch_ndcg = 57.91 
2025-02-13 00:36:43.396752: Steps 26/47: batch_recall = 61.73, batch_ndcg = 53.96 
2025-02-13 00:36:44.424389: Steps 27/47: batch_recall = 63.21, batch_ndcg = 53.52 
2025-02-13 00:36:45.445100: Steps 28/47: batch_recall = 71.61, batch_ndcg = 59.96 
2025-02-13 00:36:46.451028: Steps 29/47: batch_recall = 69.03, batch_ndcg = 58.89 
2025-02-13 00:36:47.465678: Steps 30/47: batch_recall = 73.90, batch_ndcg = 64.21 
2025-02-13 00:36:48.488134: Steps 31/47: batch_recall = 66.19, batch_ndcg = 55.94 
2025-02-13 00:36:49.482101: Steps 32/47: batch_recall = 71.04, batch_ndcg = 65.42 
2025-02-13 00:36:50.500293: Steps 33/47: batch_recall = 79.49, batch_ndcg = 67.62 
2025-02-13 00:36:51.502661: Steps 34/47: batch_recall = 67.43, batch_ndcg = 56.59 
2025-02-13 00:36:52.506874: Steps 35/47: batch_recall = 76.73, batch_ndcg = 65.96 
2025-02-13 00:36:53.501595: Steps 36/47: batch_recall = 78.68, batch_ndcg = 64.57 
2025-02-13 00:36:54.483719: Steps 37/47: batch_recall = 85.56, batch_ndcg = 73.79 
2025-02-13 00:36:55.470885: Steps 38/47: batch_recall = 90.86, batch_ndcg = 73.08 
2025-02-13 00:36:56.446841: Steps 39/47: batch_recall = 88.33, batch_ndcg = 68.76 
2025-02-13 00:36:57.429701: Steps 40/47: batch_recall = 72.29, batch_ndcg = 63.35 
2025-02-13 00:36:58.400770: Steps 41/47: batch_recall = 86.98, batch_ndcg = 72.82 
2025-02-13 00:36:59.376402: Steps 42/47: batch_recall = 84.08, batch_ndcg = 66.14 
2025-02-13 00:37:00.351812: Steps 43/47: batch_recall = 89.93, batch_ndcg = 72.08 
2025-02-13 00:37:01.340416: Steps 44/47: batch_recall = 87.78, batch_ndcg = 70.44 
2025-02-13 00:37:02.290967: Steps 45/47: batch_recall = 90.58, batch_ndcg = 74.09 
2025-02-13 00:37:02.395790: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.60 
2025-02-13 00:37:02.395918: Epoch 81/1000, Test: Recall = 0.1218, NDCG = 0.1100  

2025-02-13 00:37:03.592562: Training Step 0/59: batchLoss = 2.8029, diffLoss = 13.6513, kgLoss = 0.0908
2025-02-13 00:37:04.515935: Training Step 1/59: batchLoss = 3.0505, diffLoss = 14.8820, kgLoss = 0.0927
2025-02-13 00:37:05.446848: Training Step 2/59: batchLoss = 2.7151, diffLoss = 13.2446, kgLoss = 0.0827
2025-02-13 00:37:06.367393: Training Step 3/59: batchLoss = 2.7161, diffLoss = 13.2778, kgLoss = 0.0756
2025-02-13 00:37:07.297258: Training Step 4/59: batchLoss = 2.9731, diffLoss = 14.5021, kgLoss = 0.0908
2025-02-13 00:37:08.223857: Training Step 5/59: batchLoss = 3.0673, diffLoss = 14.9748, kgLoss = 0.0904
2025-02-13 00:37:09.156448: Training Step 6/59: batchLoss = 2.5547, diffLoss = 12.4520, kgLoss = 0.0804
2025-02-13 00:37:10.083004: Training Step 7/59: batchLoss = 2.7383, diffLoss = 13.3747, kgLoss = 0.0792
2025-02-13 00:37:11.011586: Training Step 8/59: batchLoss = 2.7743, diffLoss = 13.5159, kgLoss = 0.0889
2025-02-13 00:37:11.941041: Training Step 9/59: batchLoss = 2.6604, diffLoss = 12.9945, kgLoss = 0.0769
2025-02-13 00:37:12.865048: Training Step 10/59: batchLoss = 2.8230, diffLoss = 13.7726, kgLoss = 0.0856
2025-02-13 00:37:13.797073: Training Step 11/59: batchLoss = 3.3040, diffLoss = 16.1059, kgLoss = 0.1035
2025-02-13 00:37:14.736465: Training Step 12/59: batchLoss = 3.0693, diffLoss = 14.9632, kgLoss = 0.0958
2025-02-13 00:37:15.675382: Training Step 13/59: batchLoss = 2.7841, diffLoss = 13.5676, kgLoss = 0.0882
2025-02-13 00:37:16.615228: Training Step 14/59: batchLoss = 3.0278, diffLoss = 14.7590, kgLoss = 0.0949
2025-02-13 00:37:17.551931: Training Step 15/59: batchLoss = 2.8118, diffLoss = 13.7313, kgLoss = 0.0819
2025-02-13 00:37:18.492042: Training Step 16/59: batchLoss = 2.7594, diffLoss = 13.4633, kgLoss = 0.0834
2025-02-13 00:37:19.433449: Training Step 17/59: batchLoss = 2.7103, diffLoss = 13.2477, kgLoss = 0.0760
2025-02-13 00:37:20.369163: Training Step 18/59: batchLoss = 2.9653, diffLoss = 14.4550, kgLoss = 0.0929
2025-02-13 00:37:21.313103: Training Step 19/59: batchLoss = 2.5824, diffLoss = 12.5977, kgLoss = 0.0786
2025-02-13 00:37:22.246840: Training Step 20/59: batchLoss = 2.7290, diffLoss = 13.3144, kgLoss = 0.0826
2025-02-13 00:37:23.179442: Training Step 21/59: batchLoss = 2.8807, diffLoss = 14.0812, kgLoss = 0.0806
2025-02-13 00:37:24.105713: Training Step 22/59: batchLoss = 2.8417, diffLoss = 13.8746, kgLoss = 0.0835
2025-02-13 00:37:25.029660: Training Step 23/59: batchLoss = 2.6703, diffLoss = 13.0334, kgLoss = 0.0796
2025-02-13 00:37:25.952036: Training Step 24/59: batchLoss = 3.1768, diffLoss = 15.5023, kgLoss = 0.0954
2025-02-13 00:37:26.873060: Training Step 25/59: batchLoss = 3.0607, diffLoss = 14.9304, kgLoss = 0.0932
2025-02-13 00:37:27.790446: Training Step 26/59: batchLoss = 3.1268, diffLoss = 15.2840, kgLoss = 0.0875
2025-02-13 00:37:28.712959: Training Step 27/59: batchLoss = 2.9323, diffLoss = 14.3177, kgLoss = 0.0860
2025-02-13 00:37:29.629972: Training Step 28/59: batchLoss = 2.8136, diffLoss = 13.7356, kgLoss = 0.0830
2025-02-13 00:37:30.547447: Training Step 29/59: batchLoss = 2.9193, diffLoss = 14.2643, kgLoss = 0.0831
2025-02-13 00:37:31.466486: Training Step 30/59: batchLoss = 3.0514, diffLoss = 14.8980, kgLoss = 0.0898
2025-02-13 00:37:32.402762: Training Step 31/59: batchLoss = 2.9968, diffLoss = 14.6735, kgLoss = 0.0777
2025-02-13 00:37:33.310504: Training Step 32/59: batchLoss = 2.7909, diffLoss = 13.6052, kgLoss = 0.0873
2025-02-13 00:37:34.223422: Training Step 33/59: batchLoss = 2.8562, diffLoss = 13.9599, kgLoss = 0.0802
2025-02-13 00:37:35.138768: Training Step 34/59: batchLoss = 2.7397, diffLoss = 13.3927, kgLoss = 0.0765
2025-02-13 00:37:36.057061: Training Step 35/59: batchLoss = 2.9413, diffLoss = 14.3607, kgLoss = 0.0865
2025-02-13 00:37:36.987665: Training Step 36/59: batchLoss = 3.1287, diffLoss = 15.2909, kgLoss = 0.0881
2025-02-13 00:37:37.920705: Training Step 37/59: batchLoss = 3.0271, diffLoss = 14.7762, kgLoss = 0.0899
2025-02-13 00:37:38.853619: Training Step 38/59: batchLoss = 3.3473, diffLoss = 16.3649, kgLoss = 0.0929
2025-02-13 00:37:39.776689: Training Step 39/59: batchLoss = 2.8468, diffLoss = 13.8861, kgLoss = 0.0870
2025-02-13 00:37:40.707195: Training Step 40/59: batchLoss = 3.1238, diffLoss = 15.2546, kgLoss = 0.0911
2025-02-13 00:37:41.636444: Training Step 41/59: batchLoss = 3.1380, diffLoss = 15.3131, kgLoss = 0.0942
2025-02-13 00:37:42.568242: Training Step 42/59: batchLoss = 2.9111, diffLoss = 14.2108, kgLoss = 0.0862
2025-02-13 00:37:43.494215: Training Step 43/59: batchLoss = 2.8054, diffLoss = 13.6964, kgLoss = 0.0827
2025-02-13 00:37:44.416103: Training Step 44/59: batchLoss = 2.8582, diffLoss = 13.9608, kgLoss = 0.0825
2025-02-13 00:37:45.345738: Training Step 45/59: batchLoss = 2.9659, diffLoss = 14.4744, kgLoss = 0.0888
2025-02-13 00:37:46.256218: Training Step 46/59: batchLoss = 2.7018, diffLoss = 13.2028, kgLoss = 0.0765
2025-02-13 00:37:47.183591: Training Step 47/59: batchLoss = 3.1208, diffLoss = 15.2674, kgLoss = 0.0841
2025-02-13 00:37:48.101296: Training Step 48/59: batchLoss = 2.9028, diffLoss = 14.1931, kgLoss = 0.0802
2025-02-13 00:37:49.027058: Training Step 49/59: batchLoss = 2.7829, diffLoss = 13.6245, kgLoss = 0.0725
2025-02-13 00:37:49.950419: Training Step 50/59: batchLoss = 3.3932, diffLoss = 16.6019, kgLoss = 0.0910
2025-02-13 00:37:50.876871: Training Step 51/59: batchLoss = 2.7233, diffLoss = 13.2903, kgLoss = 0.0815
2025-02-13 00:37:51.804804: Training Step 52/59: batchLoss = 3.0773, diffLoss = 15.0399, kgLoss = 0.0866
2025-02-13 00:37:52.725802: Training Step 53/59: batchLoss = 3.2850, diffLoss = 16.0798, kgLoss = 0.0863
2025-02-13 00:37:53.652141: Training Step 54/59: batchLoss = 3.0434, diffLoss = 14.8799, kgLoss = 0.0843
2025-02-13 00:37:54.573360: Training Step 55/59: batchLoss = 3.1090, diffLoss = 15.2131, kgLoss = 0.0830
2025-02-13 00:37:55.494398: Training Step 56/59: batchLoss = 3.2709, diffLoss = 16.0025, kgLoss = 0.0880
2025-02-13 00:37:56.332690: Training Step 57/59: batchLoss = 2.9959, diffLoss = 14.6309, kgLoss = 0.0872
2025-02-13 00:37:57.183332: Training Step 58/59: batchLoss = 2.9863, diffLoss = 14.6241, kgLoss = 0.0769
2025-02-13 00:37:57.282789: 
2025-02-13 00:37:57.283343: Epoch 82/1000, Train: epLoss = 0.4324, epDfLoss = 2.1116, epKgLoss = 0.0126  
2025-02-13 00:37:58.756441: Steps 0/47: batch_recall = 37.32, batch_ndcg = 47.57 
2025-02-13 00:38:00.067373: Steps 1/47: batch_recall = 37.12, batch_ndcg = 42.87 
2025-02-13 00:38:01.331098: Steps 2/47: batch_recall = 40.90, batch_ndcg = 46.48 
2025-02-13 00:38:02.599192: Steps 3/47: batch_recall = 44.75, batch_ndcg = 46.90 
2025-02-13 00:38:03.793528: Steps 4/47: batch_recall = 39.58, batch_ndcg = 46.34 
2025-02-13 00:38:05.014674: Steps 5/47: batch_recall = 32.47, batch_ndcg = 39.72 
2025-02-13 00:38:06.221408: Steps 6/47: batch_recall = 38.60, batch_ndcg = 41.88 
2025-02-13 00:38:07.402158: Steps 7/47: batch_recall = 44.66, batch_ndcg = 46.34 
2025-02-13 00:38:08.575478: Steps 8/47: batch_recall = 48.62, batch_ndcg = 52.23 
2025-02-13 00:38:09.706903: Steps 9/47: batch_recall = 46.07, batch_ndcg = 46.57 
2025-02-13 00:38:10.870399: Steps 10/47: batch_recall = 43.53, batch_ndcg = 43.85 
2025-02-13 00:38:12.007014: Steps 11/47: batch_recall = 53.98, batch_ndcg = 52.00 
2025-02-13 00:38:13.144859: Steps 12/47: batch_recall = 50.68, batch_ndcg = 49.67 
2025-02-13 00:38:14.270383: Steps 13/47: batch_recall = 49.94, batch_ndcg = 46.28 
2025-02-13 00:38:15.362808: Steps 14/47: batch_recall = 40.47, batch_ndcg = 41.04 
2025-02-13 00:38:16.455711: Steps 15/47: batch_recall = 57.26, batch_ndcg = 53.93 
2025-02-13 00:38:17.536453: Steps 16/47: batch_recall = 51.14, batch_ndcg = 46.94 
2025-02-13 00:38:18.597915: Steps 17/47: batch_recall = 58.31, batch_ndcg = 49.65 
2025-02-13 00:38:19.683873: Steps 18/47: batch_recall = 53.59, batch_ndcg = 50.07 
2025-02-13 00:38:20.758888: Steps 19/47: batch_recall = 61.04, batch_ndcg = 56.56 
2025-02-13 00:38:21.820716: Steps 20/47: batch_recall = 67.36, batch_ndcg = 62.33 
2025-02-13 00:38:22.875112: Steps 21/47: batch_recall = 65.26, batch_ndcg = 55.93 
2025-02-13 00:38:23.938994: Steps 22/47: batch_recall = 57.36, batch_ndcg = 53.33 
2025-02-13 00:38:24.983465: Steps 23/47: batch_recall = 63.78, batch_ndcg = 54.67 
2025-02-13 00:38:26.039561: Steps 24/47: batch_recall = 64.68, batch_ndcg = 58.07 
2025-02-13 00:38:27.080035: Steps 25/47: batch_recall = 66.42, batch_ndcg = 58.21 
2025-02-13 00:38:28.100098: Steps 26/47: batch_recall = 60.01, batch_ndcg = 53.27 
2025-02-13 00:38:29.134014: Steps 27/47: batch_recall = 63.39, batch_ndcg = 53.17 
2025-02-13 00:38:30.143785: Steps 28/47: batch_recall = 71.01, batch_ndcg = 59.27 
2025-02-13 00:38:31.147418: Steps 29/47: batch_recall = 70.60, batch_ndcg = 58.86 
2025-02-13 00:38:32.145785: Steps 30/47: batch_recall = 74.07, batch_ndcg = 65.09 
2025-02-13 00:38:33.158118: Steps 31/47: batch_recall = 67.36, batch_ndcg = 56.24 
2025-02-13 00:38:34.166535: Steps 32/47: batch_recall = 70.85, batch_ndcg = 65.90 
2025-02-13 00:38:35.168783: Steps 33/47: batch_recall = 81.07, batch_ndcg = 68.57 
2025-02-13 00:38:36.165048: Steps 34/47: batch_recall = 67.50, batch_ndcg = 56.02 
2025-02-13 00:38:37.138079: Steps 35/47: batch_recall = 77.59, batch_ndcg = 65.58 
2025-02-13 00:38:38.121680: Steps 36/47: batch_recall = 79.41, batch_ndcg = 65.59 
2025-02-13 00:38:39.101671: Steps 37/47: batch_recall = 84.04, batch_ndcg = 72.25 
2025-02-13 00:38:40.074827: Steps 38/47: batch_recall = 90.65, batch_ndcg = 73.42 
2025-02-13 00:38:41.044627: Steps 39/47: batch_recall = 87.22, batch_ndcg = 68.65 
2025-02-13 00:38:42.021803: Steps 40/47: batch_recall = 72.25, batch_ndcg = 63.53 
2025-02-13 00:38:42.995528: Steps 41/47: batch_recall = 86.28, batch_ndcg = 71.65 
2025-02-13 00:38:43.971081: Steps 42/47: batch_recall = 82.53, batch_ndcg = 65.66 
2025-02-13 00:38:44.938943: Steps 43/47: batch_recall = 90.43, batch_ndcg = 73.28 
2025-02-13 00:38:45.936430: Steps 44/47: batch_recall = 88.85, batch_ndcg = 70.45 
2025-02-13 00:38:46.883722: Steps 45/47: batch_recall = 92.02, batch_ndcg = 73.51 
2025-02-13 00:38:46.987351: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.64 
2025-02-13 00:38:46.987484: Epoch 82/1000, Test: Recall = 0.1220, NDCG = 0.1099  

2025-02-13 00:38:48.180297: Training Step 0/59: batchLoss = 2.9275, diffLoss = 14.3199, kgLoss = 0.0794
2025-02-13 00:38:49.104475: Training Step 1/59: batchLoss = 2.8724, diffLoss = 14.0041, kgLoss = 0.0895
2025-02-13 00:38:50.021576: Training Step 2/59: batchLoss = 2.5210, diffLoss = 12.3252, kgLoss = 0.0699
2025-02-13 00:38:50.932737: Training Step 3/59: batchLoss = 2.8170, diffLoss = 13.7726, kgLoss = 0.0781
2025-02-13 00:38:51.856526: Training Step 4/59: batchLoss = 2.6016, diffLoss = 12.7207, kgLoss = 0.0719
2025-02-13 00:38:52.777780: Training Step 5/59: batchLoss = 2.9450, diffLoss = 14.3742, kgLoss = 0.0877
2025-02-13 00:38:53.710372: Training Step 6/59: batchLoss = 2.8104, diffLoss = 13.6917, kgLoss = 0.0901
2025-02-13 00:38:54.634813: Training Step 7/59: batchLoss = 2.8627, diffLoss = 13.9854, kgLoss = 0.0821
2025-02-13 00:38:55.567039: Training Step 8/59: batchLoss = 2.7724, diffLoss = 13.5274, kgLoss = 0.0836
2025-02-13 00:38:56.497343: Training Step 9/59: batchLoss = 3.2002, diffLoss = 15.6066, kgLoss = 0.0987
2025-02-13 00:38:57.414191: Training Step 10/59: batchLoss = 2.7545, diffLoss = 13.4514, kgLoss = 0.0803
2025-02-13 00:38:58.338836: Training Step 11/59: batchLoss = 2.5812, diffLoss = 12.6271, kgLoss = 0.0698
2025-02-13 00:38:59.264106: Training Step 12/59: batchLoss = 2.7741, diffLoss = 13.5356, kgLoss = 0.0837
2025-02-13 00:39:00.195537: Training Step 13/59: batchLoss = 2.7802, diffLoss = 13.5606, kgLoss = 0.0851
2025-02-13 00:39:01.123481: Training Step 14/59: batchLoss = 2.8142, diffLoss = 13.7539, kgLoss = 0.0793
2025-02-13 00:39:02.058324: Training Step 15/59: batchLoss = 2.8931, diffLoss = 14.1260, kgLoss = 0.0849
2025-02-13 00:39:02.999901: Training Step 16/59: batchLoss = 2.8208, diffLoss = 13.7609, kgLoss = 0.0858
2025-02-13 00:39:03.937960: Training Step 17/59: batchLoss = 2.8370, diffLoss = 13.8796, kgLoss = 0.0763
2025-02-13 00:39:04.869077: Training Step 18/59: batchLoss = 2.9008, diffLoss = 14.1486, kgLoss = 0.0889
2025-02-13 00:39:05.807511: Training Step 19/59: batchLoss = 2.7826, diffLoss = 13.5811, kgLoss = 0.0829
2025-02-13 00:39:06.744045: Training Step 20/59: batchLoss = 3.5989, diffLoss = 17.5612, kgLoss = 0.1083
2025-02-13 00:39:07.675267: Training Step 21/59: batchLoss = 2.8432, diffLoss = 13.8830, kgLoss = 0.0833
2025-02-13 00:39:08.615138: Training Step 22/59: batchLoss = 2.8873, diffLoss = 14.0992, kgLoss = 0.0843
2025-02-13 00:39:09.554432: Training Step 23/59: batchLoss = 2.7409, diffLoss = 13.3364, kgLoss = 0.0920
2025-02-13 00:39:10.480375: Training Step 24/59: batchLoss = 3.3058, diffLoss = 16.1570, kgLoss = 0.0931
2025-02-13 00:39:11.413480: Training Step 25/59: batchLoss = 2.9963, diffLoss = 14.6605, kgLoss = 0.0803
2025-02-13 00:39:12.337931: Training Step 26/59: batchLoss = 3.3789, diffLoss = 16.5070, kgLoss = 0.0969
2025-02-13 00:39:13.275359: Training Step 27/59: batchLoss = 2.6641, diffLoss = 13.0021, kgLoss = 0.0797
2025-02-13 00:39:14.195462: Training Step 28/59: batchLoss = 2.7240, diffLoss = 13.3041, kgLoss = 0.0790
2025-02-13 00:39:15.119219: Training Step 29/59: batchLoss = 2.7951, diffLoss = 13.6412, kgLoss = 0.0836
2025-02-13 00:39:16.049885: Training Step 30/59: batchLoss = 3.3725, diffLoss = 16.4967, kgLoss = 0.0914
2025-02-13 00:39:16.978936: Training Step 31/59: batchLoss = 3.2260, diffLoss = 15.7541, kgLoss = 0.0939
2025-02-13 00:39:17.920374: Training Step 32/59: batchLoss = 2.8800, diffLoss = 14.0685, kgLoss = 0.0828
2025-02-13 00:39:18.858532: Training Step 33/59: batchLoss = 2.9629, diffLoss = 14.4585, kgLoss = 0.0890
2025-02-13 00:39:19.787806: Training Step 34/59: batchLoss = 2.9123, diffLoss = 14.2065, kgLoss = 0.0888
2025-02-13 00:39:20.708499: Training Step 35/59: batchLoss = 2.9560, diffLoss = 14.4377, kgLoss = 0.0856
2025-02-13 00:39:21.644967: Training Step 36/59: batchLoss = 2.9272, diffLoss = 14.3045, kgLoss = 0.0829
2025-02-13 00:39:22.578952: Training Step 37/59: batchLoss = 2.6742, diffLoss = 13.0793, kgLoss = 0.0729
2025-02-13 00:39:23.509406: Training Step 38/59: batchLoss = 3.0991, diffLoss = 15.1323, kgLoss = 0.0908
2025-02-13 00:39:24.445232: Training Step 39/59: batchLoss = 3.3248, diffLoss = 16.2008, kgLoss = 0.1058
2025-02-13 00:39:25.377553: Training Step 40/59: batchLoss = 2.9580, diffLoss = 14.4225, kgLoss = 0.0918
2025-02-13 00:39:26.310080: Training Step 41/59: batchLoss = 2.7705, diffLoss = 13.5179, kgLoss = 0.0837
2025-02-13 00:39:27.262194: Training Step 42/59: batchLoss = 2.7819, diffLoss = 13.5938, kgLoss = 0.0790
2025-02-13 00:39:28.197662: Training Step 43/59: batchLoss = 2.8580, diffLoss = 13.9574, kgLoss = 0.0831
2025-02-13 00:39:29.139317: Training Step 44/59: batchLoss = 2.9980, diffLoss = 14.6547, kgLoss = 0.0838
2025-02-13 00:39:30.080167: Training Step 45/59: batchLoss = 2.8723, diffLoss = 14.0532, kgLoss = 0.0770
2025-02-13 00:39:31.007845: Training Step 46/59: batchLoss = 2.8278, diffLoss = 13.8087, kgLoss = 0.0826
2025-02-13 00:39:31.933624: Training Step 47/59: batchLoss = 2.8706, diffLoss = 14.0140, kgLoss = 0.0847
2025-02-13 00:39:32.858387: Training Step 48/59: batchLoss = 2.8481, diffLoss = 13.8994, kgLoss = 0.0853
2025-02-13 00:39:33.777953: Training Step 49/59: batchLoss = 2.8436, diffLoss = 13.8870, kgLoss = 0.0827
2025-02-13 00:39:34.698105: Training Step 50/59: batchLoss = 3.0645, diffLoss = 14.9712, kgLoss = 0.0878
2025-02-13 00:39:35.628828: Training Step 51/59: batchLoss = 2.8034, diffLoss = 13.7134, kgLoss = 0.0759
2025-02-13 00:39:36.549253: Training Step 52/59: batchLoss = 3.1714, diffLoss = 15.4841, kgLoss = 0.0933
2025-02-13 00:39:37.466011: Training Step 53/59: batchLoss = 3.4211, diffLoss = 16.7169, kgLoss = 0.0972
2025-02-13 00:39:38.389324: Training Step 54/59: batchLoss = 2.7145, diffLoss = 13.2290, kgLoss = 0.0859
2025-02-13 00:39:39.315034: Training Step 55/59: batchLoss = 3.1135, diffLoss = 15.2129, kgLoss = 0.0887
2025-02-13 00:39:40.249779: Training Step 56/59: batchLoss = 2.8897, diffLoss = 14.1387, kgLoss = 0.0775
2025-02-13 00:39:41.087836: Training Step 57/59: batchLoss = 3.1606, diffLoss = 15.4041, kgLoss = 0.0997
2025-02-13 00:39:41.936256: Training Step 58/59: batchLoss = 3.2174, diffLoss = 15.7073, kgLoss = 0.0949
2025-02-13 00:39:42.036511: 
2025-02-13 00:39:42.036869: Epoch 83/1000, Train: epLoss = 0.4318, epDfLoss = 2.1086, epKgLoss = 0.0126  
2025-02-13 00:39:43.530160: Steps 0/47: batch_recall = 36.78, batch_ndcg = 47.90 
2025-02-13 00:39:44.842150: Steps 1/47: batch_recall = 36.32, batch_ndcg = 41.93 
2025-02-13 00:39:46.101971: Steps 2/47: batch_recall = 41.69, batch_ndcg = 47.06 
2025-02-13 00:39:47.373182: Steps 3/47: batch_recall = 44.16, batch_ndcg = 46.44 
2025-02-13 00:39:48.579365: Steps 4/47: batch_recall = 39.58, batch_ndcg = 45.94 
2025-02-13 00:39:49.801130: Steps 5/47: batch_recall = 33.75, batch_ndcg = 40.11 
2025-02-13 00:39:51.009446: Steps 6/47: batch_recall = 38.62, batch_ndcg = 42.02 
2025-02-13 00:39:52.180792: Steps 7/47: batch_recall = 44.32, batch_ndcg = 45.73 
2025-02-13 00:39:53.351256: Steps 8/47: batch_recall = 49.22, batch_ndcg = 52.69 
2025-02-13 00:39:54.491668: Steps 9/47: batch_recall = 46.74, batch_ndcg = 46.71 
2025-02-13 00:39:55.657443: Steps 10/47: batch_recall = 43.62, batch_ndcg = 43.62 
2025-02-13 00:39:56.802834: Steps 11/47: batch_recall = 55.69, batch_ndcg = 52.86 
2025-02-13 00:39:57.946162: Steps 12/47: batch_recall = 50.53, batch_ndcg = 49.72 
2025-02-13 00:39:59.070643: Steps 13/47: batch_recall = 50.50, batch_ndcg = 46.02 
2025-02-13 00:40:00.166111: Steps 14/47: batch_recall = 42.07, batch_ndcg = 42.00 
2025-02-13 00:40:01.250527: Steps 15/47: batch_recall = 57.54, batch_ndcg = 54.19 
2025-02-13 00:40:02.316070: Steps 16/47: batch_recall = 51.26, batch_ndcg = 46.79 
2025-02-13 00:40:03.355560: Steps 17/47: batch_recall = 57.93, batch_ndcg = 49.78 
2025-02-13 00:40:04.431906: Steps 18/47: batch_recall = 53.07, batch_ndcg = 49.91 
2025-02-13 00:40:05.513957: Steps 19/47: batch_recall = 59.53, batch_ndcg = 55.18 
2025-02-13 00:40:06.560921: Steps 20/47: batch_recall = 68.05, batch_ndcg = 62.06 
2025-02-13 00:40:07.616962: Steps 21/47: batch_recall = 65.34, batch_ndcg = 55.91 
2025-02-13 00:40:08.670881: Steps 22/47: batch_recall = 56.19, batch_ndcg = 51.37 
2025-02-13 00:40:09.728717: Steps 23/47: batch_recall = 65.60, batch_ndcg = 55.18 
2025-02-13 00:40:10.799407: Steps 24/47: batch_recall = 67.03, batch_ndcg = 59.10 
2025-02-13 00:40:11.853424: Steps 25/47: batch_recall = 65.93, batch_ndcg = 57.87 
2025-02-13 00:40:12.874681: Steps 26/47: batch_recall = 60.39, batch_ndcg = 52.58 
2025-02-13 00:40:13.900327: Steps 27/47: batch_recall = 60.58, batch_ndcg = 52.45 
2025-02-13 00:40:14.913694: Steps 28/47: batch_recall = 70.55, batch_ndcg = 59.92 
2025-02-13 00:40:15.922851: Steps 29/47: batch_recall = 69.63, batch_ndcg = 59.11 
2025-02-13 00:40:16.929041: Steps 30/47: batch_recall = 72.56, batch_ndcg = 64.05 
2025-02-13 00:40:17.952309: Steps 31/47: batch_recall = 68.90, batch_ndcg = 56.66 
2025-02-13 00:40:18.952568: Steps 32/47: batch_recall = 70.50, batch_ndcg = 65.52 
2025-02-13 00:40:19.955586: Steps 33/47: batch_recall = 80.70, batch_ndcg = 68.72 
2025-02-13 00:40:20.958470: Steps 34/47: batch_recall = 67.48, batch_ndcg = 56.12 
2025-02-13 00:40:21.941655: Steps 35/47: batch_recall = 77.53, batch_ndcg = 65.04 
2025-02-13 00:40:22.912182: Steps 36/47: batch_recall = 78.48, batch_ndcg = 64.80 
2025-02-13 00:40:23.882690: Steps 37/47: batch_recall = 83.25, batch_ndcg = 73.09 
2025-02-13 00:40:24.857234: Steps 38/47: batch_recall = 91.52, batch_ndcg = 73.72 
2025-02-13 00:40:25.828392: Steps 39/47: batch_recall = 85.99, batch_ndcg = 68.14 
2025-02-13 00:40:26.813707: Steps 40/47: batch_recall = 73.50, batch_ndcg = 63.55 
2025-02-13 00:40:27.777382: Steps 41/47: batch_recall = 87.37, batch_ndcg = 72.50 
2025-02-13 00:40:28.736895: Steps 42/47: batch_recall = 81.39, batch_ndcg = 64.54 
2025-02-13 00:40:29.701636: Steps 43/47: batch_recall = 89.96, batch_ndcg = 72.65 
2025-02-13 00:40:30.677724: Steps 44/47: batch_recall = 88.23, batch_ndcg = 70.49 
2025-02-13 00:40:31.623718: Steps 45/47: batch_recall = 92.92, batch_ndcg = 74.25 
2025-02-13 00:40:31.729054: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.60 
2025-02-13 00:40:31.729177: Epoch 83/1000, Test: Recall = 0.1220, NDCG = 0.1098  

2025-02-13 00:40:32.939351: Training Step 0/59: batchLoss = 2.5704, diffLoss = 12.5191, kgLoss = 0.0833
2025-02-13 00:40:33.870913: Training Step 1/59: batchLoss = 3.0570, diffLoss = 14.9176, kgLoss = 0.0918
2025-02-13 00:40:34.795346: Training Step 2/59: batchLoss = 2.8718, diffLoss = 13.9971, kgLoss = 0.0905
2025-02-13 00:40:35.718020: Training Step 3/59: batchLoss = 2.6735, diffLoss = 13.0522, kgLoss = 0.0789
2025-02-13 00:40:36.650778: Training Step 4/59: batchLoss = 2.9700, diffLoss = 14.5076, kgLoss = 0.0856
2025-02-13 00:40:37.569513: Training Step 5/59: batchLoss = 2.8437, diffLoss = 13.8908, kgLoss = 0.0819
2025-02-13 00:40:38.494267: Training Step 6/59: batchLoss = 2.5360, diffLoss = 12.3818, kgLoss = 0.0746
2025-02-13 00:40:39.417569: Training Step 7/59: batchLoss = 2.8044, diffLoss = 13.6957, kgLoss = 0.0816
2025-02-13 00:40:40.342712: Training Step 8/59: batchLoss = 2.8318, diffLoss = 13.8152, kgLoss = 0.0859
2025-02-13 00:40:41.269500: Training Step 9/59: batchLoss = 2.8528, diffLoss = 13.9146, kgLoss = 0.0874
2025-02-13 00:40:42.191602: Training Step 10/59: batchLoss = 2.5078, diffLoss = 12.2233, kgLoss = 0.0790
2025-02-13 00:40:43.120532: Training Step 11/59: batchLoss = 2.9058, diffLoss = 14.1903, kgLoss = 0.0846
2025-02-13 00:40:44.049855: Training Step 12/59: batchLoss = 3.2541, diffLoss = 15.8834, kgLoss = 0.0968
2025-02-13 00:40:44.977783: Training Step 13/59: batchLoss = 3.0107, diffLoss = 14.7081, kgLoss = 0.0864
2025-02-13 00:40:45.910300: Training Step 14/59: batchLoss = 2.7551, diffLoss = 13.4876, kgLoss = 0.0720
2025-02-13 00:40:46.851560: Training Step 15/59: batchLoss = 2.7884, diffLoss = 13.6120, kgLoss = 0.0825
2025-02-13 00:40:47.787649: Training Step 16/59: batchLoss = 2.8847, diffLoss = 14.0926, kgLoss = 0.0827
2025-02-13 00:40:48.726021: Training Step 17/59: batchLoss = 2.8484, diffLoss = 13.8989, kgLoss = 0.0858
2025-02-13 00:40:49.659755: Training Step 18/59: batchLoss = 2.9152, diffLoss = 14.2129, kgLoss = 0.0907
2025-02-13 00:40:50.600568: Training Step 19/59: batchLoss = 2.8235, diffLoss = 13.7910, kgLoss = 0.0817
2025-02-13 00:40:51.536343: Training Step 20/59: batchLoss = 3.0348, diffLoss = 14.8195, kgLoss = 0.0887
2025-02-13 00:40:52.473602: Training Step 21/59: batchLoss = 2.4855, diffLoss = 12.1292, kgLoss = 0.0746
2025-02-13 00:40:53.405777: Training Step 22/59: batchLoss = 3.3451, diffLoss = 16.3314, kgLoss = 0.0985
2025-02-13 00:40:54.341849: Training Step 23/59: batchLoss = 3.0810, diffLoss = 15.0570, kgLoss = 0.0870
2025-02-13 00:40:55.271059: Training Step 24/59: batchLoss = 2.7757, diffLoss = 13.5727, kgLoss = 0.0764
2025-02-13 00:40:56.199798: Training Step 25/59: batchLoss = 2.8166, diffLoss = 13.7704, kgLoss = 0.0781
2025-02-13 00:40:57.128822: Training Step 26/59: batchLoss = 2.7743, diffLoss = 13.5579, kgLoss = 0.0784
2025-02-13 00:40:58.056050: Training Step 27/59: batchLoss = 2.8657, diffLoss = 14.0059, kgLoss = 0.0806
2025-02-13 00:40:58.992049: Training Step 28/59: batchLoss = 3.0294, diffLoss = 14.7618, kgLoss = 0.0964
2025-02-13 00:40:59.912208: Training Step 29/59: batchLoss = 2.9193, diffLoss = 14.2574, kgLoss = 0.0848
2025-02-13 00:41:00.835669: Training Step 30/59: batchLoss = 2.6391, diffLoss = 12.8859, kgLoss = 0.0774
2025-02-13 00:41:01.759228: Training Step 31/59: batchLoss = 3.0332, diffLoss = 14.8165, kgLoss = 0.0874
2025-02-13 00:41:02.688924: Training Step 32/59: batchLoss = 3.0658, diffLoss = 14.9630, kgLoss = 0.0914
2025-02-13 00:41:03.615115: Training Step 33/59: batchLoss = 2.9765, diffLoss = 14.5345, kgLoss = 0.0870
2025-02-13 00:41:04.533933: Training Step 34/59: batchLoss = 2.5443, diffLoss = 12.4337, kgLoss = 0.0720
2025-02-13 00:41:05.465331: Training Step 35/59: batchLoss = 3.2576, diffLoss = 15.8928, kgLoss = 0.0988
2025-02-13 00:41:06.386849: Training Step 36/59: batchLoss = 3.0312, diffLoss = 14.8202, kgLoss = 0.0840
2025-02-13 00:41:07.316636: Training Step 37/59: batchLoss = 2.9785, diffLoss = 14.5422, kgLoss = 0.0875
2025-02-13 00:41:08.252616: Training Step 38/59: batchLoss = 3.7032, diffLoss = 18.0721, kgLoss = 0.1110
2025-02-13 00:41:09.197090: Training Step 39/59: batchLoss = 3.0712, diffLoss = 14.9913, kgLoss = 0.0911
2025-02-13 00:41:10.133557: Training Step 40/59: batchLoss = 2.8848, diffLoss = 14.1050, kgLoss = 0.0797
2025-02-13 00:41:11.076148: Training Step 41/59: batchLoss = 3.0705, diffLoss = 14.9664, kgLoss = 0.0965
2025-02-13 00:41:12.007072: Training Step 42/59: batchLoss = 2.8946, diffLoss = 14.1394, kgLoss = 0.0834
2025-02-13 00:41:12.950764: Training Step 43/59: batchLoss = 2.9388, diffLoss = 14.3365, kgLoss = 0.0893
2025-02-13 00:41:13.886181: Training Step 44/59: batchLoss = 3.1290, diffLoss = 15.3093, kgLoss = 0.0840
2025-02-13 00:41:14.816314: Training Step 45/59: batchLoss = 3.2530, diffLoss = 15.8796, kgLoss = 0.0963
2025-02-13 00:41:15.751928: Training Step 46/59: batchLoss = 2.9696, diffLoss = 14.5153, kgLoss = 0.0832
2025-02-13 00:41:16.669730: Training Step 47/59: batchLoss = 2.9373, diffLoss = 14.3549, kgLoss = 0.0829
2025-02-13 00:41:17.591405: Training Step 48/59: batchLoss = 3.0689, diffLoss = 14.9883, kgLoss = 0.0890
2025-02-13 00:41:18.514808: Training Step 49/59: batchLoss = 2.6507, diffLoss = 12.9561, kgLoss = 0.0744
2025-02-13 00:41:19.433820: Training Step 50/59: batchLoss = 3.0835, diffLoss = 15.0511, kgLoss = 0.0916
2025-02-13 00:41:20.358971: Training Step 51/59: batchLoss = 2.9200, diffLoss = 14.2486, kgLoss = 0.0879
2025-02-13 00:41:21.271016: Training Step 52/59: batchLoss = 2.9379, diffLoss = 14.3549, kgLoss = 0.0837
2025-02-13 00:41:22.195812: Training Step 53/59: batchLoss = 2.9756, diffLoss = 14.5415, kgLoss = 0.0842
2025-02-13 00:41:23.137259: Training Step 54/59: batchLoss = 2.9429, diffLoss = 14.3751, kgLoss = 0.0848
2025-02-13 00:41:24.060225: Training Step 55/59: batchLoss = 3.2580, diffLoss = 15.9253, kgLoss = 0.0912
2025-02-13 00:41:24.978279: Training Step 56/59: batchLoss = 2.8423, diffLoss = 13.9027, kgLoss = 0.0772
2025-02-13 00:41:25.820721: Training Step 57/59: batchLoss = 2.7714, diffLoss = 13.5171, kgLoss = 0.0850
2025-02-13 00:41:26.670038: Training Step 58/59: batchLoss = 3.0571, diffLoss = 14.9390, kgLoss = 0.0866
2025-02-13 00:41:26.768386: 
2025-02-13 00:41:26.768902: Epoch 84/1000, Train: epLoss = 0.4318, epDfLoss = 2.1085, epKgLoss = 0.0126  
2025-02-13 00:41:28.252031: Steps 0/47: batch_recall = 37.06, batch_ndcg = 48.08 
2025-02-13 00:41:29.558193: Steps 1/47: batch_recall = 36.14, batch_ndcg = 42.18 
2025-02-13 00:41:30.818609: Steps 2/47: batch_recall = 40.23, batch_ndcg = 46.16 
2025-02-13 00:41:32.084654: Steps 3/47: batch_recall = 44.48, batch_ndcg = 45.86 
2025-02-13 00:41:33.291651: Steps 4/47: batch_recall = 39.93, batch_ndcg = 46.51 
2025-02-13 00:41:34.505337: Steps 5/47: batch_recall = 33.32, batch_ndcg = 40.07 
2025-02-13 00:41:35.728052: Steps 6/47: batch_recall = 39.13, batch_ndcg = 41.61 
2025-02-13 00:41:36.907708: Steps 7/47: batch_recall = 44.60, batch_ndcg = 45.94 
2025-02-13 00:41:38.085421: Steps 8/47: batch_recall = 48.39, batch_ndcg = 52.82 
2025-02-13 00:41:39.217697: Steps 9/47: batch_recall = 46.27, batch_ndcg = 46.59 
2025-02-13 00:41:40.372730: Steps 10/47: batch_recall = 43.89, batch_ndcg = 43.54 
2025-02-13 00:41:41.521104: Steps 11/47: batch_recall = 54.57, batch_ndcg = 52.67 
2025-02-13 00:41:42.654290: Steps 12/47: batch_recall = 49.75, batch_ndcg = 49.32 
2025-02-13 00:41:43.768671: Steps 13/47: batch_recall = 49.74, batch_ndcg = 45.48 
2025-02-13 00:41:44.854068: Steps 14/47: batch_recall = 42.59, batch_ndcg = 41.45 
2025-02-13 00:41:45.945363: Steps 15/47: batch_recall = 57.49, batch_ndcg = 54.44 
2025-02-13 00:41:47.020490: Steps 16/47: batch_recall = 51.08, batch_ndcg = 48.17 
2025-02-13 00:41:48.078506: Steps 17/47: batch_recall = 57.95, batch_ndcg = 49.69 
2025-02-13 00:41:49.173714: Steps 18/47: batch_recall = 54.06, batch_ndcg = 50.36 
2025-02-13 00:41:50.250473: Steps 19/47: batch_recall = 60.64, batch_ndcg = 55.63 
2025-02-13 00:41:51.312175: Steps 20/47: batch_recall = 68.28, batch_ndcg = 62.15 
2025-02-13 00:41:52.366051: Steps 21/47: batch_recall = 66.33, batch_ndcg = 56.20 
2025-02-13 00:41:53.428741: Steps 22/47: batch_recall = 57.09, batch_ndcg = 52.25 
2025-02-13 00:41:54.486217: Steps 23/47: batch_recall = 64.10, batch_ndcg = 54.92 
2025-02-13 00:41:55.555492: Steps 24/47: batch_recall = 66.97, batch_ndcg = 59.64 
2025-02-13 00:41:56.617298: Steps 25/47: batch_recall = 66.82, batch_ndcg = 57.59 
2025-02-13 00:41:57.639426: Steps 26/47: batch_recall = 60.60, batch_ndcg = 52.71 
2025-02-13 00:41:58.665205: Steps 27/47: batch_recall = 61.37, batch_ndcg = 53.48 
2025-02-13 00:41:59.678685: Steps 28/47: batch_recall = 70.53, batch_ndcg = 59.68 
2025-02-13 00:42:00.691731: Steps 29/47: batch_recall = 69.55, batch_ndcg = 58.22 
2025-02-13 00:42:01.698386: Steps 30/47: batch_recall = 74.11, batch_ndcg = 63.89 
2025-02-13 00:42:02.717999: Steps 31/47: batch_recall = 66.79, batch_ndcg = 55.81 
2025-02-13 00:42:03.720109: Steps 32/47: batch_recall = 70.96, batch_ndcg = 65.03 
2025-02-13 00:42:04.717639: Steps 33/47: batch_recall = 79.85, batch_ndcg = 69.22 
2025-02-13 00:42:05.711729: Steps 34/47: batch_recall = 67.04, batch_ndcg = 55.79 
2025-02-13 00:42:06.696383: Steps 35/47: batch_recall = 78.33, batch_ndcg = 65.50 
2025-02-13 00:42:07.686311: Steps 36/47: batch_recall = 77.88, batch_ndcg = 64.90 
2025-02-13 00:42:08.659394: Steps 37/47: batch_recall = 85.07, batch_ndcg = 72.97 
2025-02-13 00:42:09.645865: Steps 38/47: batch_recall = 89.79, batch_ndcg = 72.95 
2025-02-13 00:42:10.622208: Steps 39/47: batch_recall = 86.87, batch_ndcg = 68.34 
2025-02-13 00:42:11.593800: Steps 40/47: batch_recall = 70.33, batch_ndcg = 61.49 
2025-02-13 00:42:12.562247: Steps 41/47: batch_recall = 85.55, batch_ndcg = 71.49 
2025-02-13 00:42:13.534836: Steps 42/47: batch_recall = 81.13, batch_ndcg = 63.86 
2025-02-13 00:42:14.510944: Steps 43/47: batch_recall = 91.21, batch_ndcg = 73.02 
2025-02-13 00:42:15.484334: Steps 44/47: batch_recall = 86.46, batch_ndcg = 70.49 
2025-02-13 00:42:16.431207: Steps 45/47: batch_recall = 93.30, batch_ndcg = 74.67 
2025-02-13 00:42:16.536392: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.71 
2025-02-13 00:42:16.536725: Epoch 84/1000, Test: Recall = 0.1218, NDCG = 0.1097  

2025-02-13 00:42:17.731525: Training Step 0/59: batchLoss = 2.8094, diffLoss = 13.7237, kgLoss = 0.0808
2025-02-13 00:42:18.656456: Training Step 1/59: batchLoss = 3.2331, diffLoss = 15.7816, kgLoss = 0.0959
2025-02-13 00:42:19.588229: Training Step 2/59: batchLoss = 2.8989, diffLoss = 14.1395, kgLoss = 0.0888
2025-02-13 00:42:20.507301: Training Step 3/59: batchLoss = 2.9581, diffLoss = 14.4551, kgLoss = 0.0838
2025-02-13 00:42:21.428499: Training Step 4/59: batchLoss = 2.7103, diffLoss = 13.2091, kgLoss = 0.0856
2025-02-13 00:42:22.355709: Training Step 5/59: batchLoss = 2.6706, diffLoss = 13.0580, kgLoss = 0.0737
2025-02-13 00:42:23.276908: Training Step 6/59: batchLoss = 2.6554, diffLoss = 12.9436, kgLoss = 0.0833
2025-02-13 00:42:24.205218: Training Step 7/59: batchLoss = 2.6805, diffLoss = 13.0848, kgLoss = 0.0794
2025-02-13 00:42:25.128964: Training Step 8/59: batchLoss = 2.6506, diffLoss = 12.9208, kgLoss = 0.0830
2025-02-13 00:42:26.059118: Training Step 9/59: batchLoss = 3.1126, diffLoss = 15.2114, kgLoss = 0.0879
2025-02-13 00:42:26.984530: Training Step 10/59: batchLoss = 2.8291, diffLoss = 13.7896, kgLoss = 0.0890
2025-02-13 00:42:27.905444: Training Step 11/59: batchLoss = 3.1213, diffLoss = 15.2160, kgLoss = 0.0977
2025-02-13 00:42:28.825325: Training Step 12/59: batchLoss = 2.7302, diffLoss = 13.3407, kgLoss = 0.0776
2025-02-13 00:42:29.754146: Training Step 13/59: batchLoss = 2.9384, diffLoss = 14.3440, kgLoss = 0.0870
2025-02-13 00:42:30.683186: Training Step 14/59: batchLoss = 2.6741, diffLoss = 13.0402, kgLoss = 0.0826
2025-02-13 00:42:31.618581: Training Step 15/59: batchLoss = 3.0473, diffLoss = 14.8695, kgLoss = 0.0917
2025-02-13 00:42:32.559659: Training Step 16/59: batchLoss = 2.8983, diffLoss = 14.1407, kgLoss = 0.0877
2025-02-13 00:42:33.496273: Training Step 17/59: batchLoss = 2.9458, diffLoss = 14.3553, kgLoss = 0.0935
2025-02-13 00:42:34.433663: Training Step 18/59: batchLoss = 2.7354, diffLoss = 13.3705, kgLoss = 0.0766
2025-02-13 00:42:35.370497: Training Step 19/59: batchLoss = 3.0527, diffLoss = 14.8856, kgLoss = 0.0944
2025-02-13 00:42:36.301909: Training Step 20/59: batchLoss = 2.9362, diffLoss = 14.3561, kgLoss = 0.0813
2025-02-13 00:42:37.232543: Training Step 21/59: batchLoss = 3.2566, diffLoss = 15.9193, kgLoss = 0.0909
2025-02-13 00:42:38.164329: Training Step 22/59: batchLoss = 2.8407, diffLoss = 13.8685, kgLoss = 0.0837
2025-02-13 00:42:39.094622: Training Step 23/59: batchLoss = 2.8979, diffLoss = 14.1401, kgLoss = 0.0873
2025-02-13 00:42:40.018697: Training Step 24/59: batchLoss = 2.8388, diffLoss = 13.8797, kgLoss = 0.0786
2025-02-13 00:42:40.936544: Training Step 25/59: batchLoss = 3.3725, diffLoss = 16.4919, kgLoss = 0.0927
2025-02-13 00:42:41.851282: Training Step 26/59: batchLoss = 3.1455, diffLoss = 15.3476, kgLoss = 0.0950
2025-02-13 00:42:42.777272: Training Step 27/59: batchLoss = 3.0643, diffLoss = 14.9455, kgLoss = 0.0940
2025-02-13 00:42:43.712143: Training Step 28/59: batchLoss = 2.6629, diffLoss = 13.0022, kgLoss = 0.0781
2025-02-13 00:42:44.639283: Training Step 29/59: batchLoss = 2.8550, diffLoss = 13.9293, kgLoss = 0.0864
2025-02-13 00:42:45.563650: Training Step 30/59: batchLoss = 2.6573, diffLoss = 12.9767, kgLoss = 0.0774
2025-02-13 00:42:46.485160: Training Step 31/59: batchLoss = 3.1140, diffLoss = 15.2120, kgLoss = 0.0895
2025-02-13 00:42:47.414658: Training Step 32/59: batchLoss = 3.0046, diffLoss = 14.6728, kgLoss = 0.0876
2025-02-13 00:42:48.339272: Training Step 33/59: batchLoss = 2.7629, diffLoss = 13.5104, kgLoss = 0.0760
2025-02-13 00:42:49.263040: Training Step 34/59: batchLoss = 3.0187, diffLoss = 14.7442, kgLoss = 0.0873
2025-02-13 00:42:50.185258: Training Step 35/59: batchLoss = 2.7961, diffLoss = 13.6619, kgLoss = 0.0797
2025-02-13 00:42:51.126620: Training Step 36/59: batchLoss = 2.9515, diffLoss = 14.3979, kgLoss = 0.0898
2025-02-13 00:42:52.062058: Training Step 37/59: batchLoss = 2.8262, diffLoss = 13.8176, kgLoss = 0.0783
2025-02-13 00:42:52.990945: Training Step 38/59: batchLoss = 2.8030, diffLoss = 13.6726, kgLoss = 0.0856
2025-02-13 00:42:53.924271: Training Step 39/59: batchLoss = 3.2700, diffLoss = 15.9763, kgLoss = 0.0934
2025-02-13 00:42:54.864994: Training Step 40/59: batchLoss = 3.0237, diffLoss = 14.7344, kgLoss = 0.0960
2025-02-13 00:42:55.798650: Training Step 41/59: batchLoss = 2.9866, diffLoss = 14.5897, kgLoss = 0.0859
2025-02-13 00:42:56.728957: Training Step 42/59: batchLoss = 2.4460, diffLoss = 11.9258, kgLoss = 0.0761
2025-02-13 00:42:57.666538: Training Step 43/59: batchLoss = 2.7940, diffLoss = 13.6350, kgLoss = 0.0837
2025-02-13 00:42:58.604726: Training Step 44/59: batchLoss = 2.9782, diffLoss = 14.5513, kgLoss = 0.0849
2025-02-13 00:42:59.538373: Training Step 45/59: batchLoss = 2.8569, diffLoss = 13.9661, kgLoss = 0.0796
2025-02-13 00:43:00.471744: Training Step 46/59: batchLoss = 3.0868, diffLoss = 15.0759, kgLoss = 0.0895
2025-02-13 00:43:01.397442: Training Step 47/59: batchLoss = 2.8925, diffLoss = 14.1402, kgLoss = 0.0806
2025-02-13 00:43:02.313354: Training Step 48/59: batchLoss = 2.8613, diffLoss = 13.9900, kgLoss = 0.0791
2025-02-13 00:43:03.236939: Training Step 49/59: batchLoss = 3.3595, diffLoss = 16.4422, kgLoss = 0.0889
2025-02-13 00:43:04.149886: Training Step 50/59: batchLoss = 2.9445, diffLoss = 14.3848, kgLoss = 0.0845
2025-02-13 00:43:05.070822: Training Step 51/59: batchLoss = 2.8945, diffLoss = 14.0890, kgLoss = 0.0959
2025-02-13 00:43:05.991544: Training Step 52/59: batchLoss = 2.8740, diffLoss = 14.0553, kgLoss = 0.0786
2025-02-13 00:43:06.919172: Training Step 53/59: batchLoss = 2.9335, diffLoss = 14.3589, kgLoss = 0.0772
2025-02-13 00:43:07.846324: Training Step 54/59: batchLoss = 3.4087, diffLoss = 16.6211, kgLoss = 0.1056
2025-02-13 00:43:08.773545: Training Step 55/59: batchLoss = 2.7803, diffLoss = 13.5784, kgLoss = 0.0807
2025-02-13 00:43:09.696347: Training Step 56/59: batchLoss = 3.1029, diffLoss = 15.1760, kgLoss = 0.0846
2025-02-13 00:43:10.537656: Training Step 57/59: batchLoss = 2.8074, diffLoss = 13.7159, kgLoss = 0.0803
2025-02-13 00:43:11.388622: Training Step 58/59: batchLoss = 2.9253, diffLoss = 14.2987, kgLoss = 0.0819
2025-02-13 00:43:11.485982: 
2025-02-13 00:43:11.486513: Epoch 85/1000, Train: epLoss = 0.4310, epDfLoss = 2.1043, epKgLoss = 0.0126  
2025-02-13 00:43:12.965264: Steps 0/47: batch_recall = 36.96, batch_ndcg = 47.84 
2025-02-13 00:43:14.283528: Steps 1/47: batch_recall = 36.60, batch_ndcg = 41.93 
2025-02-13 00:43:15.549645: Steps 2/47: batch_recall = 41.35, batch_ndcg = 47.53 
2025-02-13 00:43:16.816616: Steps 3/47: batch_recall = 44.73, batch_ndcg = 46.45 
2025-02-13 00:43:18.024729: Steps 4/47: batch_recall = 39.80, batch_ndcg = 46.21 
2025-02-13 00:43:19.254169: Steps 5/47: batch_recall = 33.40, batch_ndcg = 40.74 
2025-02-13 00:43:20.461180: Steps 6/47: batch_recall = 39.11, batch_ndcg = 42.19 
2025-02-13 00:43:21.643700: Steps 7/47: batch_recall = 44.95, batch_ndcg = 46.10 
2025-02-13 00:43:22.818032: Steps 8/47: batch_recall = 48.74, batch_ndcg = 53.13 
2025-02-13 00:43:23.958282: Steps 9/47: batch_recall = 47.03, batch_ndcg = 47.03 
2025-02-13 00:43:25.124923: Steps 10/47: batch_recall = 42.98, batch_ndcg = 43.48 
2025-02-13 00:43:26.257404: Steps 11/47: batch_recall = 54.94, batch_ndcg = 52.67 
2025-02-13 00:43:27.393921: Steps 12/47: batch_recall = 48.86, batch_ndcg = 48.35 
2025-02-13 00:43:28.516566: Steps 13/47: batch_recall = 49.39, batch_ndcg = 45.38 
2025-02-13 00:43:29.611874: Steps 14/47: batch_recall = 41.84, batch_ndcg = 41.40 
2025-02-13 00:43:30.703791: Steps 15/47: batch_recall = 57.01, batch_ndcg = 53.61 
2025-02-13 00:43:31.778618: Steps 16/47: batch_recall = 50.96, batch_ndcg = 47.88 
2025-02-13 00:43:32.816863: Steps 17/47: batch_recall = 58.57, batch_ndcg = 49.94 
2025-02-13 00:43:33.899848: Steps 18/47: batch_recall = 53.07, batch_ndcg = 50.30 
2025-02-13 00:43:34.965347: Steps 19/47: batch_recall = 61.15, batch_ndcg = 56.08 
2025-02-13 00:43:36.022583: Steps 20/47: batch_recall = 67.22, batch_ndcg = 61.50 
2025-02-13 00:43:37.071235: Steps 21/47: batch_recall = 65.87, batch_ndcg = 55.62 
2025-02-13 00:43:38.130735: Steps 22/47: batch_recall = 55.78, batch_ndcg = 51.39 
2025-02-13 00:43:39.181778: Steps 23/47: batch_recall = 65.18, batch_ndcg = 56.16 
2025-02-13 00:43:40.242572: Steps 24/47: batch_recall = 64.75, batch_ndcg = 58.52 
2025-02-13 00:43:41.301727: Steps 25/47: batch_recall = 65.41, batch_ndcg = 57.67 
2025-02-13 00:43:42.323929: Steps 26/47: batch_recall = 60.67, batch_ndcg = 53.20 
2025-02-13 00:43:43.356674: Steps 27/47: batch_recall = 62.23, batch_ndcg = 52.39 
2025-02-13 00:43:44.365255: Steps 28/47: batch_recall = 71.29, batch_ndcg = 60.20 
2025-02-13 00:43:45.379973: Steps 29/47: batch_recall = 70.87, batch_ndcg = 59.41 
2025-02-13 00:43:46.390113: Steps 30/47: batch_recall = 74.06, batch_ndcg = 64.37 
2025-02-13 00:43:47.422167: Steps 31/47: batch_recall = 67.28, batch_ndcg = 56.07 
2025-02-13 00:43:48.431466: Steps 32/47: batch_recall = 71.13, batch_ndcg = 64.82 
2025-02-13 00:43:49.443216: Steps 33/47: batch_recall = 80.85, batch_ndcg = 69.07 
2025-02-13 00:43:50.452877: Steps 34/47: batch_recall = 69.01, batch_ndcg = 57.35 
2025-02-13 00:43:51.433101: Steps 35/47: batch_recall = 77.70, batch_ndcg = 65.91 
2025-02-13 00:43:52.407767: Steps 36/47: batch_recall = 79.25, batch_ndcg = 64.35 
2025-02-13 00:43:53.378364: Steps 37/47: batch_recall = 84.15, batch_ndcg = 73.47 
2025-02-13 00:43:54.350295: Steps 38/47: batch_recall = 89.59, batch_ndcg = 72.56 
2025-02-13 00:43:55.333616: Steps 39/47: batch_recall = 87.23, batch_ndcg = 68.76 
2025-02-13 00:43:56.304569: Steps 40/47: batch_recall = 72.60, batch_ndcg = 63.26 
2025-02-13 00:43:57.283141: Steps 41/47: batch_recall = 87.33, batch_ndcg = 71.93 
2025-02-13 00:43:58.259487: Steps 42/47: batch_recall = 81.71, batch_ndcg = 64.45 
2025-02-13 00:43:59.230743: Steps 43/47: batch_recall = 91.46, batch_ndcg = 72.65 
2025-02-13 00:44:00.213203: Steps 44/47: batch_recall = 86.90, batch_ndcg = 70.36 
2025-02-13 00:44:01.163212: Steps 45/47: batch_recall = 92.34, batch_ndcg = 75.16 
2025-02-13 00:44:01.270224: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.69 
2025-02-13 00:44:01.270360: Epoch 85/1000, Test: Recall = 0.1220, NDCG = 0.1099  

2025-02-13 00:44:02.468183: Training Step 0/59: batchLoss = 2.9869, diffLoss = 14.5655, kgLoss = 0.0923
2025-02-13 00:44:03.397884: Training Step 1/59: batchLoss = 2.7041, diffLoss = 13.1827, kgLoss = 0.0844
2025-02-13 00:44:04.316898: Training Step 2/59: batchLoss = 2.7189, diffLoss = 13.2817, kgLoss = 0.0782
2025-02-13 00:44:05.231594: Training Step 3/59: batchLoss = 2.6913, diffLoss = 13.1210, kgLoss = 0.0839
2025-02-13 00:44:06.151494: Training Step 4/59: batchLoss = 2.9356, diffLoss = 14.3088, kgLoss = 0.0923
2025-02-13 00:44:07.076962: Training Step 5/59: batchLoss = 3.0226, diffLoss = 14.7417, kgLoss = 0.0929
2025-02-13 00:44:07.999896: Training Step 6/59: batchLoss = 3.1310, diffLoss = 15.2943, kgLoss = 0.0902
2025-02-13 00:44:08.916661: Training Step 7/59: batchLoss = 2.7343, diffLoss = 13.3052, kgLoss = 0.0916
2025-02-13 00:44:09.837053: Training Step 8/59: batchLoss = 2.6981, diffLoss = 13.1847, kgLoss = 0.0765
2025-02-13 00:44:10.754470: Training Step 9/59: batchLoss = 2.8107, diffLoss = 13.7235, kgLoss = 0.0825
2025-02-13 00:44:11.670322: Training Step 10/59: batchLoss = 2.9586, diffLoss = 14.4404, kgLoss = 0.0882
2025-02-13 00:44:12.579859: Training Step 11/59: batchLoss = 2.9750, diffLoss = 14.5344, kgLoss = 0.0851
2025-02-13 00:44:13.489976: Training Step 12/59: batchLoss = 3.0048, diffLoss = 14.6187, kgLoss = 0.1014
2025-02-13 00:44:14.411816: Training Step 13/59: batchLoss = 2.6225, diffLoss = 12.8094, kgLoss = 0.0757
2025-02-13 00:44:15.326904: Training Step 14/59: batchLoss = 2.8930, diffLoss = 14.1189, kgLoss = 0.0865
2025-02-13 00:44:16.241975: Training Step 15/59: batchLoss = 2.9299, diffLoss = 14.3113, kgLoss = 0.0845
2025-02-13 00:44:17.160293: Training Step 16/59: batchLoss = 3.0744, diffLoss = 15.0249, kgLoss = 0.0868
2025-02-13 00:44:18.070106: Training Step 17/59: batchLoss = 3.1224, diffLoss = 15.2520, kgLoss = 0.0900
2025-02-13 00:44:18.993823: Training Step 18/59: batchLoss = 3.1723, diffLoss = 15.4499, kgLoss = 0.1029
2025-02-13 00:44:19.907489: Training Step 19/59: batchLoss = 3.1153, diffLoss = 15.2285, kgLoss = 0.0870
2025-02-13 00:44:20.824566: Training Step 20/59: batchLoss = 3.0407, diffLoss = 14.8490, kgLoss = 0.0887
2025-02-13 00:44:21.736950: Training Step 21/59: batchLoss = 2.8878, diffLoss = 14.1300, kgLoss = 0.0773
2025-02-13 00:44:22.653991: Training Step 22/59: batchLoss = 2.8962, diffLoss = 14.1340, kgLoss = 0.0868
2025-02-13 00:44:23.572706: Training Step 23/59: batchLoss = 2.5143, diffLoss = 12.2678, kgLoss = 0.0759
2025-02-13 00:44:24.493750: Training Step 24/59: batchLoss = 2.8128, diffLoss = 13.7291, kgLoss = 0.0837
2025-02-13 00:44:25.422312: Training Step 25/59: batchLoss = 2.7172, diffLoss = 13.2655, kgLoss = 0.0802
2025-02-13 00:44:26.347976: Training Step 26/59: batchLoss = 2.9278, diffLoss = 14.2887, kgLoss = 0.0876
2025-02-13 00:44:27.283782: Training Step 27/59: batchLoss = 2.7243, diffLoss = 13.3090, kgLoss = 0.0782
2025-02-13 00:44:28.216992: Training Step 28/59: batchLoss = 2.7476, diffLoss = 13.3901, kgLoss = 0.0869
2025-02-13 00:44:29.152277: Training Step 29/59: batchLoss = 3.2990, diffLoss = 16.0801, kgLoss = 0.1037
2025-02-13 00:44:30.079932: Training Step 30/59: batchLoss = 2.7380, diffLoss = 13.3738, kgLoss = 0.0790
2025-02-13 00:44:31.008811: Training Step 31/59: batchLoss = 2.8763, diffLoss = 14.0540, kgLoss = 0.0818
2025-02-13 00:44:31.938860: Training Step 32/59: batchLoss = 3.0565, diffLoss = 14.9332, kgLoss = 0.0874
2025-02-13 00:44:32.871165: Training Step 33/59: batchLoss = 2.9370, diffLoss = 14.3462, kgLoss = 0.0847
2025-02-13 00:44:33.803356: Training Step 34/59: batchLoss = 3.0082, diffLoss = 14.7001, kgLoss = 0.0852
2025-02-13 00:44:34.727830: Training Step 35/59: batchLoss = 2.9001, diffLoss = 14.1685, kgLoss = 0.0830
2025-02-13 00:44:35.660292: Training Step 36/59: batchLoss = 2.7377, diffLoss = 13.3437, kgLoss = 0.0862
2025-02-13 00:44:36.587177: Training Step 37/59: batchLoss = 2.9009, diffLoss = 14.1738, kgLoss = 0.0827
2025-02-13 00:44:37.512061: Training Step 38/59: batchLoss = 2.6691, diffLoss = 13.0459, kgLoss = 0.0749
2025-02-13 00:44:38.435234: Training Step 39/59: batchLoss = 2.6739, diffLoss = 13.0771, kgLoss = 0.0731
2025-02-13 00:44:39.362291: Training Step 40/59: batchLoss = 3.0098, diffLoss = 14.7162, kgLoss = 0.0832
2025-02-13 00:44:40.285461: Training Step 41/59: batchLoss = 3.1830, diffLoss = 15.5758, kgLoss = 0.0849
2025-02-13 00:44:41.209012: Training Step 42/59: batchLoss = 3.2379, diffLoss = 15.7994, kgLoss = 0.0975
2025-02-13 00:44:42.134182: Training Step 43/59: batchLoss = 2.8736, diffLoss = 14.0568, kgLoss = 0.0778
2025-02-13 00:44:43.058322: Training Step 44/59: batchLoss = 2.8109, diffLoss = 13.7304, kgLoss = 0.0810
2025-02-13 00:44:43.977036: Training Step 45/59: batchLoss = 2.9888, diffLoss = 14.5787, kgLoss = 0.0913
2025-02-13 00:44:44.893955: Training Step 46/59: batchLoss = 3.0423, diffLoss = 14.8667, kgLoss = 0.0862
2025-02-13 00:44:45.824309: Training Step 47/59: batchLoss = 2.9370, diffLoss = 14.3338, kgLoss = 0.0879
2025-02-13 00:44:46.746398: Training Step 48/59: batchLoss = 2.8358, diffLoss = 13.8574, kgLoss = 0.0804
2025-02-13 00:44:47.683822: Training Step 49/59: batchLoss = 2.9436, diffLoss = 14.3575, kgLoss = 0.0902
2025-02-13 00:44:48.619347: Training Step 50/59: batchLoss = 2.9519, diffLoss = 14.4432, kgLoss = 0.0791
2025-02-13 00:44:49.555761: Training Step 51/59: batchLoss = 3.3477, diffLoss = 16.3613, kgLoss = 0.0942
2025-02-13 00:44:50.496159: Training Step 52/59: batchLoss = 2.9686, diffLoss = 14.4947, kgLoss = 0.0871
2025-02-13 00:44:51.433042: Training Step 53/59: batchLoss = 3.0463, diffLoss = 14.8808, kgLoss = 0.0876
2025-02-13 00:44:52.367365: Training Step 54/59: batchLoss = 3.0490, diffLoss = 14.8844, kgLoss = 0.0902
2025-02-13 00:44:53.310642: Training Step 55/59: batchLoss = 2.8005, diffLoss = 13.6878, kgLoss = 0.0786
2025-02-13 00:44:54.253840: Training Step 56/59: batchLoss = 3.0077, diffLoss = 14.7028, kgLoss = 0.0839
2025-02-13 00:44:55.106923: Training Step 57/59: batchLoss = 3.0429, diffLoss = 14.8623, kgLoss = 0.0880
2025-02-13 00:44:55.969819: Training Step 58/59: batchLoss = 2.8947, diffLoss = 14.1710, kgLoss = 0.0757
2025-02-13 00:44:56.068416: 
2025-02-13 00:44:56.068930: Epoch 86/1000, Train: epLoss = 0.4308, epDfLoss = 2.1038, epKgLoss = 0.0126  
2025-02-13 00:44:57.537460: Steps 0/47: batch_recall = 36.80, batch_ndcg = 47.61 
2025-02-13 00:44:58.826683: Steps 1/47: batch_recall = 37.51, batch_ndcg = 42.64 
2025-02-13 00:45:00.078815: Steps 2/47: batch_recall = 41.09, batch_ndcg = 46.94 
2025-02-13 00:45:01.342537: Steps 3/47: batch_recall = 45.26, batch_ndcg = 47.24 
2025-02-13 00:45:02.532755: Steps 4/47: batch_recall = 39.67, batch_ndcg = 45.88 
2025-02-13 00:45:03.734142: Steps 5/47: batch_recall = 32.58, batch_ndcg = 41.00 
2025-02-13 00:45:04.916780: Steps 6/47: batch_recall = 39.47, batch_ndcg = 42.43 
2025-02-13 00:45:06.088586: Steps 7/47: batch_recall = 44.15, batch_ndcg = 45.85 
2025-02-13 00:45:07.249476: Steps 8/47: batch_recall = 48.55, batch_ndcg = 51.77 
2025-02-13 00:45:08.390205: Steps 9/47: batch_recall = 46.69, batch_ndcg = 47.42 
2025-02-13 00:45:09.566265: Steps 10/47: batch_recall = 42.95, batch_ndcg = 43.02 
2025-02-13 00:45:10.717690: Steps 11/47: batch_recall = 55.47, batch_ndcg = 52.62 
2025-02-13 00:45:11.875286: Steps 12/47: batch_recall = 50.59, batch_ndcg = 49.92 
2025-02-13 00:45:13.012511: Steps 13/47: batch_recall = 50.72, batch_ndcg = 46.37 
2025-02-13 00:45:14.104695: Steps 14/47: batch_recall = 41.56, batch_ndcg = 41.84 
2025-02-13 00:45:15.196009: Steps 15/47: batch_recall = 57.32, batch_ndcg = 53.88 
2025-02-13 00:45:16.286782: Steps 16/47: batch_recall = 51.39, batch_ndcg = 47.62 
2025-02-13 00:45:17.343840: Steps 17/47: batch_recall = 58.22, batch_ndcg = 49.94 
2025-02-13 00:45:18.411886: Steps 18/47: batch_recall = 53.22, batch_ndcg = 49.26 
2025-02-13 00:45:19.462006: Steps 19/47: batch_recall = 59.51, batch_ndcg = 54.49 
2025-02-13 00:45:20.501142: Steps 20/47: batch_recall = 68.48, batch_ndcg = 62.37 
2025-02-13 00:45:21.538770: Steps 21/47: batch_recall = 67.05, batch_ndcg = 56.57 
2025-02-13 00:45:22.570423: Steps 22/47: batch_recall = 54.89, batch_ndcg = 51.44 
2025-02-13 00:45:23.612328: Steps 23/47: batch_recall = 64.64, batch_ndcg = 55.80 
2025-02-13 00:45:24.663466: Steps 24/47: batch_recall = 66.66, batch_ndcg = 59.63 
2025-02-13 00:45:25.696756: Steps 25/47: batch_recall = 66.56, batch_ndcg = 58.52 
2025-02-13 00:45:26.711609: Steps 26/47: batch_recall = 59.67, batch_ndcg = 53.32 
2025-02-13 00:45:27.746097: Steps 27/47: batch_recall = 63.65, batch_ndcg = 53.61 
2025-02-13 00:45:28.785306: Steps 28/47: batch_recall = 71.05, batch_ndcg = 60.35 
2025-02-13 00:45:29.810885: Steps 29/47: batch_recall = 69.03, batch_ndcg = 58.14 
2025-02-13 00:45:30.825249: Steps 30/47: batch_recall = 73.47, batch_ndcg = 64.41 
2025-02-13 00:45:31.862808: Steps 31/47: batch_recall = 68.01, batch_ndcg = 56.67 
2025-02-13 00:45:32.883439: Steps 32/47: batch_recall = 71.47, batch_ndcg = 65.65 
2025-02-13 00:45:33.908216: Steps 33/47: batch_recall = 79.63, batch_ndcg = 69.08 
2025-02-13 00:45:34.924998: Steps 34/47: batch_recall = 69.62, batch_ndcg = 57.19 
2025-02-13 00:45:35.922896: Steps 35/47: batch_recall = 76.88, batch_ndcg = 65.64 
2025-02-13 00:45:36.924620: Steps 36/47: batch_recall = 78.95, batch_ndcg = 63.98 
2025-02-13 00:45:37.908060: Steps 37/47: batch_recall = 84.36, batch_ndcg = 74.07 
2025-02-13 00:45:38.881926: Steps 38/47: batch_recall = 89.25, batch_ndcg = 72.86 
2025-02-13 00:45:39.857369: Steps 39/47: batch_recall = 87.98, batch_ndcg = 68.83 
2025-02-13 00:45:40.816110: Steps 40/47: batch_recall = 71.56, batch_ndcg = 62.07 
2025-02-13 00:45:41.777782: Steps 41/47: batch_recall = 86.79, batch_ndcg = 72.10 
2025-02-13 00:45:42.749221: Steps 42/47: batch_recall = 81.93, batch_ndcg = 65.13 
2025-02-13 00:45:43.706066: Steps 43/47: batch_recall = 90.74, batch_ndcg = 72.73 
2025-02-13 00:45:44.670741: Steps 44/47: batch_recall = 88.00, batch_ndcg = 69.74 
2025-02-13 00:45:45.605479: Steps 45/47: batch_recall = 93.98, batch_ndcg = 74.94 
2025-02-13 00:45:45.708322: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 00:45:45.708463: Epoch 86/1000, Test: Recall = 0.1222, NDCG = 0.1101  

2025-02-13 00:45:46.899341: Training Step 0/59: batchLoss = 2.8495, diffLoss = 13.9151, kgLoss = 0.0831
2025-02-13 00:45:47.826529: Training Step 1/59: batchLoss = 2.8479, diffLoss = 13.8776, kgLoss = 0.0904
2025-02-13 00:45:48.756071: Training Step 2/59: batchLoss = 2.9428, diffLoss = 14.3553, kgLoss = 0.0896
2025-02-13 00:45:49.681591: Training Step 3/59: batchLoss = 2.7113, diffLoss = 13.2451, kgLoss = 0.0779
2025-02-13 00:45:50.613559: Training Step 4/59: batchLoss = 2.8365, diffLoss = 13.8552, kgLoss = 0.0819
2025-02-13 00:45:51.553693: Training Step 5/59: batchLoss = 2.6745, diffLoss = 13.0767, kgLoss = 0.0739
2025-02-13 00:45:52.482843: Training Step 6/59: batchLoss = 2.9978, diffLoss = 14.6286, kgLoss = 0.0901
2025-02-13 00:45:53.415134: Training Step 7/59: batchLoss = 2.7579, diffLoss = 13.4847, kgLoss = 0.0762
2025-02-13 00:45:54.350938: Training Step 8/59: batchLoss = 2.7281, diffLoss = 13.3308, kgLoss = 0.0775
2025-02-13 00:45:55.289450: Training Step 9/59: batchLoss = 2.9031, diffLoss = 14.1694, kgLoss = 0.0865
2025-02-13 00:45:56.220174: Training Step 10/59: batchLoss = 3.0970, diffLoss = 15.1150, kgLoss = 0.0925
2025-02-13 00:45:57.159074: Training Step 11/59: batchLoss = 2.6925, diffLoss = 13.1441, kgLoss = 0.0796
2025-02-13 00:45:58.096353: Training Step 12/59: batchLoss = 2.6810, diffLoss = 13.1126, kgLoss = 0.0731
2025-02-13 00:45:59.033888: Training Step 13/59: batchLoss = 2.5589, diffLoss = 12.4986, kgLoss = 0.0740
2025-02-13 00:45:59.967742: Training Step 14/59: batchLoss = 2.9699, diffLoss = 14.5148, kgLoss = 0.0837
2025-02-13 00:46:00.900270: Training Step 15/59: batchLoss = 2.8528, diffLoss = 13.9454, kgLoss = 0.0797
2025-02-13 00:46:01.823964: Training Step 16/59: batchLoss = 2.6696, diffLoss = 13.0357, kgLoss = 0.0781
2025-02-13 00:46:02.757727: Training Step 17/59: batchLoss = 2.9158, diffLoss = 14.2356, kgLoss = 0.0859
2025-02-13 00:46:03.685986: Training Step 18/59: batchLoss = 2.5922, diffLoss = 12.6548, kgLoss = 0.0765
2025-02-13 00:46:04.614123: Training Step 19/59: batchLoss = 2.9034, diffLoss = 14.1548, kgLoss = 0.0906
2025-02-13 00:46:05.539868: Training Step 20/59: batchLoss = 3.1881, diffLoss = 15.5766, kgLoss = 0.0910
2025-02-13 00:46:06.467484: Training Step 21/59: batchLoss = 2.6375, diffLoss = 12.8692, kgLoss = 0.0796
2025-02-13 00:46:07.395223: Training Step 22/59: batchLoss = 2.8208, diffLoss = 13.7692, kgLoss = 0.0837
2025-02-13 00:46:08.319008: Training Step 23/59: batchLoss = 3.0289, diffLoss = 14.7869, kgLoss = 0.0894
2025-02-13 00:46:09.258268: Training Step 24/59: batchLoss = 2.9944, diffLoss = 14.6140, kgLoss = 0.0895
2025-02-13 00:46:10.182415: Training Step 25/59: batchLoss = 2.7042, diffLoss = 13.1855, kgLoss = 0.0839
2025-02-13 00:46:11.105697: Training Step 26/59: batchLoss = 2.8442, diffLoss = 13.8720, kgLoss = 0.0873
2025-02-13 00:46:12.044686: Training Step 27/59: batchLoss = 3.3021, diffLoss = 16.1498, kgLoss = 0.0901
2025-02-13 00:46:12.979676: Training Step 28/59: batchLoss = 2.8071, diffLoss = 13.6989, kgLoss = 0.0842
2025-02-13 00:46:13.908348: Training Step 29/59: batchLoss = 2.9311, diffLoss = 14.2916, kgLoss = 0.0909
2025-02-13 00:46:14.849167: Training Step 30/59: batchLoss = 3.2580, diffLoss = 15.9112, kgLoss = 0.0947
2025-02-13 00:46:15.785275: Training Step 31/59: batchLoss = 3.1161, diffLoss = 15.2085, kgLoss = 0.0929
2025-02-13 00:46:16.713406: Training Step 32/59: batchLoss = 3.0038, diffLoss = 14.6512, kgLoss = 0.0920
2025-02-13 00:46:17.650206: Training Step 33/59: batchLoss = 3.0423, diffLoss = 14.8431, kgLoss = 0.0921
2025-02-13 00:46:18.587317: Training Step 34/59: batchLoss = 3.1124, diffLoss = 15.2111, kgLoss = 0.0878
2025-02-13 00:46:19.534904: Training Step 35/59: batchLoss = 3.3030, diffLoss = 16.1555, kgLoss = 0.0899
2025-02-13 00:46:20.470705: Training Step 36/59: batchLoss = 3.0422, diffLoss = 14.8822, kgLoss = 0.0822
2025-02-13 00:46:21.396108: Training Step 37/59: batchLoss = 3.3323, diffLoss = 16.2670, kgLoss = 0.0987
2025-02-13 00:46:22.324506: Training Step 38/59: batchLoss = 2.9001, diffLoss = 14.1934, kgLoss = 0.0768
2025-02-13 00:46:23.245706: Training Step 39/59: batchLoss = 2.9291, diffLoss = 14.2862, kgLoss = 0.0898
2025-02-13 00:46:24.166554: Training Step 40/59: batchLoss = 2.7787, diffLoss = 13.5292, kgLoss = 0.0910
2025-02-13 00:46:25.096842: Training Step 41/59: batchLoss = 2.7846, diffLoss = 13.5907, kgLoss = 0.0831
2025-02-13 00:46:26.014169: Training Step 42/59: batchLoss = 2.8320, diffLoss = 13.8120, kgLoss = 0.0870
2025-02-13 00:46:26.944342: Training Step 43/59: batchLoss = 2.8719, diffLoss = 14.0180, kgLoss = 0.0854
2025-02-13 00:46:27.862987: Training Step 44/59: batchLoss = 3.0132, diffLoss = 14.6995, kgLoss = 0.0917
2025-02-13 00:46:28.796382: Training Step 45/59: batchLoss = 2.9285, diffLoss = 14.3227, kgLoss = 0.0799
2025-02-13 00:46:29.716780: Training Step 46/59: batchLoss = 3.1241, diffLoss = 15.2782, kgLoss = 0.0856
2025-02-13 00:46:30.642425: Training Step 47/59: batchLoss = 3.4950, diffLoss = 17.0932, kgLoss = 0.0955
2025-02-13 00:46:31.572179: Training Step 48/59: batchLoss = 2.9286, diffLoss = 14.3235, kgLoss = 0.0799
2025-02-13 00:46:32.505879: Training Step 49/59: batchLoss = 2.9395, diffLoss = 14.3717, kgLoss = 0.0815
2025-02-13 00:46:33.441343: Training Step 50/59: batchLoss = 2.9615, diffLoss = 14.4766, kgLoss = 0.0827
2025-02-13 00:46:34.382340: Training Step 51/59: batchLoss = 2.8259, diffLoss = 13.8021, kgLoss = 0.0819
2025-02-13 00:46:35.318170: Training Step 52/59: batchLoss = 3.1149, diffLoss = 15.2191, kgLoss = 0.0889
2025-02-13 00:46:36.252874: Training Step 53/59: batchLoss = 3.0328, diffLoss = 14.8364, kgLoss = 0.0819
2025-02-13 00:46:37.189767: Training Step 54/59: batchLoss = 3.3296, diffLoss = 16.2639, kgLoss = 0.0961
2025-02-13 00:46:38.124273: Training Step 55/59: batchLoss = 3.2384, diffLoss = 15.8394, kgLoss = 0.0881
2025-02-13 00:46:39.052189: Training Step 56/59: batchLoss = 3.2108, diffLoss = 15.7094, kgLoss = 0.0861
2025-02-13 00:46:39.903264: Training Step 57/59: batchLoss = 3.1184, diffLoss = 15.2446, kgLoss = 0.0869
2025-02-13 00:46:40.754812: Training Step 58/59: batchLoss = 3.0351, diffLoss = 14.8367, kgLoss = 0.0847
2025-02-13 00:46:40.849381: 
2025-02-13 00:46:40.849750: Epoch 87/1000, Train: epLoss = 0.4351, epDfLoss = 2.1251, epKgLoss = 0.0126  
2025-02-13 00:46:42.317551: Steps 0/47: batch_recall = 36.18, batch_ndcg = 47.07 
2025-02-13 00:46:43.595584: Steps 1/47: batch_recall = 37.26, batch_ndcg = 43.10 
2025-02-13 00:46:44.838389: Steps 2/47: batch_recall = 40.26, batch_ndcg = 46.29 
2025-02-13 00:46:46.093370: Steps 3/47: batch_recall = 43.34, batch_ndcg = 46.46 
2025-02-13 00:46:47.284851: Steps 4/47: batch_recall = 39.27, batch_ndcg = 45.64 
2025-02-13 00:46:48.499868: Steps 5/47: batch_recall = 33.67, batch_ndcg = 40.67 
2025-02-13 00:46:49.687877: Steps 6/47: batch_recall = 38.92, batch_ndcg = 42.94 
2025-02-13 00:46:50.856780: Steps 7/47: batch_recall = 44.91, batch_ndcg = 45.48 
2025-02-13 00:46:52.033010: Steps 8/47: batch_recall = 48.59, batch_ndcg = 52.28 
2025-02-13 00:46:53.178521: Steps 9/47: batch_recall = 47.45, batch_ndcg = 46.57 
2025-02-13 00:46:54.345948: Steps 10/47: batch_recall = 44.83, batch_ndcg = 44.22 
2025-02-13 00:46:55.496494: Steps 11/47: batch_recall = 55.61, batch_ndcg = 52.56 
2025-02-13 00:46:56.650547: Steps 12/47: batch_recall = 50.91, batch_ndcg = 49.97 
2025-02-13 00:46:57.795836: Steps 13/47: batch_recall = 48.30, batch_ndcg = 45.85 
2025-02-13 00:46:58.899055: Steps 14/47: batch_recall = 40.86, batch_ndcg = 41.33 
2025-02-13 00:46:59.994972: Steps 15/47: batch_recall = 56.58, batch_ndcg = 53.78 
2025-02-13 00:47:01.084277: Steps 16/47: batch_recall = 51.40, batch_ndcg = 48.05 
2025-02-13 00:47:02.146060: Steps 17/47: batch_recall = 58.50, batch_ndcg = 49.94 
2025-02-13 00:47:03.224575: Steps 18/47: batch_recall = 52.33, batch_ndcg = 48.96 
2025-02-13 00:47:04.282467: Steps 19/47: batch_recall = 60.02, batch_ndcg = 55.25 
2025-02-13 00:47:05.327965: Steps 20/47: batch_recall = 67.41, batch_ndcg = 61.98 
2025-02-13 00:47:06.375923: Steps 21/47: batch_recall = 66.99, batch_ndcg = 57.18 
2025-02-13 00:47:07.420968: Steps 22/47: batch_recall = 56.79, batch_ndcg = 51.75 
2025-02-13 00:47:08.464839: Steps 23/47: batch_recall = 65.24, batch_ndcg = 56.57 
2025-02-13 00:47:09.529249: Steps 24/47: batch_recall = 64.46, batch_ndcg = 59.13 
2025-02-13 00:47:10.561821: Steps 25/47: batch_recall = 66.10, batch_ndcg = 57.49 
2025-02-13 00:47:11.586386: Steps 26/47: batch_recall = 61.54, batch_ndcg = 54.50 
2025-02-13 00:47:12.624649: Steps 27/47: batch_recall = 62.71, batch_ndcg = 54.04 
2025-02-13 00:47:13.636860: Steps 28/47: batch_recall = 70.02, batch_ndcg = 59.31 
2025-02-13 00:47:14.654633: Steps 29/47: batch_recall = 69.45, batch_ndcg = 58.38 
2025-02-13 00:47:15.677036: Steps 30/47: batch_recall = 73.52, batch_ndcg = 63.86 
2025-02-13 00:47:16.715224: Steps 31/47: batch_recall = 68.20, batch_ndcg = 56.61 
2025-02-13 00:47:17.735560: Steps 32/47: batch_recall = 71.51, batch_ndcg = 65.61 
2025-02-13 00:47:18.757245: Steps 33/47: batch_recall = 79.75, batch_ndcg = 68.11 
2025-02-13 00:47:19.776998: Steps 34/47: batch_recall = 68.98, batch_ndcg = 56.70 
2025-02-13 00:47:20.770548: Steps 35/47: batch_recall = 76.91, batch_ndcg = 66.00 
2025-02-13 00:47:21.761070: Steps 36/47: batch_recall = 77.27, batch_ndcg = 63.90 
2025-02-13 00:47:22.739540: Steps 37/47: batch_recall = 83.98, batch_ndcg = 72.56 
2025-02-13 00:47:23.703186: Steps 38/47: batch_recall = 88.87, batch_ndcg = 73.28 
2025-02-13 00:47:24.684657: Steps 39/47: batch_recall = 88.79, batch_ndcg = 69.17 
2025-02-13 00:47:25.637525: Steps 40/47: batch_recall = 72.34, batch_ndcg = 62.13 
2025-02-13 00:47:26.579234: Steps 41/47: batch_recall = 86.92, batch_ndcg = 72.23 
2025-02-13 00:47:27.538764: Steps 42/47: batch_recall = 81.40, batch_ndcg = 64.48 
2025-02-13 00:47:28.484467: Steps 43/47: batch_recall = 91.56, batch_ndcg = 73.07 
2025-02-13 00:47:29.447376: Steps 44/47: batch_recall = 87.36, batch_ndcg = 69.56 
2025-02-13 00:47:30.377054: Steps 45/47: batch_recall = 93.44, batch_ndcg = 75.21 
2025-02-13 00:47:30.479925: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-13 00:47:30.480061: Epoch 87/1000, Test: Recall = 0.1219, NDCG = 0.1099  

2025-02-13 00:47:31.667825: Training Step 0/59: batchLoss = 2.7872, diffLoss = 13.5910, kgLoss = 0.0862
2025-02-13 00:47:32.595532: Training Step 1/59: batchLoss = 2.8198, diffLoss = 13.7327, kgLoss = 0.0916
2025-02-13 00:47:33.514959: Training Step 2/59: batchLoss = 2.8846, diffLoss = 14.0605, kgLoss = 0.0907
2025-02-13 00:47:34.441956: Training Step 3/59: batchLoss = 2.5199, diffLoss = 12.2956, kgLoss = 0.0760
2025-02-13 00:47:35.374716: Training Step 4/59: batchLoss = 2.8795, diffLoss = 14.0866, kgLoss = 0.0777
2025-02-13 00:47:36.316781: Training Step 5/59: batchLoss = 2.7118, diffLoss = 13.2315, kgLoss = 0.0819
2025-02-13 00:47:37.251276: Training Step 6/59: batchLoss = 3.2063, diffLoss = 15.6604, kgLoss = 0.0928
2025-02-13 00:47:38.192270: Training Step 7/59: batchLoss = 2.8351, diffLoss = 13.8308, kgLoss = 0.0861
2025-02-13 00:47:39.121848: Training Step 8/59: batchLoss = 2.7480, diffLoss = 13.3986, kgLoss = 0.0853
2025-02-13 00:47:40.059703: Training Step 9/59: batchLoss = 2.7432, diffLoss = 13.3950, kgLoss = 0.0802
2025-02-13 00:47:40.998485: Training Step 10/59: batchLoss = 2.8982, diffLoss = 14.1469, kgLoss = 0.0861
2025-02-13 00:47:41.930283: Training Step 11/59: batchLoss = 3.1794, diffLoss = 15.5267, kgLoss = 0.0925
2025-02-13 00:47:42.865533: Training Step 12/59: batchLoss = 2.6545, diffLoss = 12.9552, kgLoss = 0.0793
2025-02-13 00:47:43.797685: Training Step 13/59: batchLoss = 2.8211, diffLoss = 13.7668, kgLoss = 0.0846
2025-02-13 00:47:44.721262: Training Step 14/59: batchLoss = 2.6889, diffLoss = 13.0962, kgLoss = 0.0871
2025-02-13 00:47:45.645333: Training Step 15/59: batchLoss = 3.0533, diffLoss = 14.9310, kgLoss = 0.0839
2025-02-13 00:47:46.569161: Training Step 16/59: batchLoss = 2.9897, diffLoss = 14.5834, kgLoss = 0.0912
2025-02-13 00:47:47.501949: Training Step 17/59: batchLoss = 2.8790, diffLoss = 14.0657, kgLoss = 0.0824
2025-02-13 00:47:48.426730: Training Step 18/59: batchLoss = 3.3683, diffLoss = 16.4637, kgLoss = 0.0945
2025-02-13 00:47:49.363700: Training Step 19/59: batchLoss = 2.9851, diffLoss = 14.5714, kgLoss = 0.0886
2025-02-13 00:47:50.289003: Training Step 20/59: batchLoss = 3.0562, diffLoss = 14.9210, kgLoss = 0.0900
2025-02-13 00:47:51.214411: Training Step 21/59: batchLoss = 2.8699, diffLoss = 14.0140, kgLoss = 0.0838
2025-02-13 00:47:52.134250: Training Step 22/59: batchLoss = 2.9061, diffLoss = 14.1870, kgLoss = 0.0859
2025-02-13 00:47:53.053187: Training Step 23/59: batchLoss = 2.9361, diffLoss = 14.3184, kgLoss = 0.0906
2025-02-13 00:47:53.971369: Training Step 24/59: batchLoss = 2.9244, diffLoss = 14.2708, kgLoss = 0.0878
2025-02-13 00:47:54.896350: Training Step 25/59: batchLoss = 2.8090, diffLoss = 13.7124, kgLoss = 0.0831
2025-02-13 00:47:55.827470: Training Step 26/59: batchLoss = 3.1253, diffLoss = 15.2365, kgLoss = 0.0974
2025-02-13 00:47:56.757019: Training Step 27/59: batchLoss = 2.6345, diffLoss = 12.8699, kgLoss = 0.0757
2025-02-13 00:47:57.695617: Training Step 28/59: batchLoss = 2.6019, diffLoss = 12.7177, kgLoss = 0.0730
2025-02-13 00:47:58.639786: Training Step 29/59: batchLoss = 2.7277, diffLoss = 13.3198, kgLoss = 0.0797
2025-02-13 00:47:59.576165: Training Step 30/59: batchLoss = 3.1604, diffLoss = 15.4253, kgLoss = 0.0942
2025-02-13 00:48:00.512807: Training Step 31/59: batchLoss = 3.0477, diffLoss = 14.8638, kgLoss = 0.0937
2025-02-13 00:48:01.447905: Training Step 32/59: batchLoss = 3.4286, diffLoss = 16.7366, kgLoss = 0.1016
2025-02-13 00:48:02.389965: Training Step 33/59: batchLoss = 3.2226, diffLoss = 15.7373, kgLoss = 0.0939
2025-02-13 00:48:03.324847: Training Step 34/59: batchLoss = 3.3965, diffLoss = 16.6012, kgLoss = 0.0953
2025-02-13 00:48:04.266828: Training Step 35/59: batchLoss = 2.5717, diffLoss = 12.5639, kgLoss = 0.0737
2025-02-13 00:48:05.204505: Training Step 36/59: batchLoss = 3.0446, diffLoss = 14.8593, kgLoss = 0.0910
2025-02-13 00:48:06.128173: Training Step 37/59: batchLoss = 2.7867, diffLoss = 13.6305, kgLoss = 0.0757
2025-02-13 00:48:07.057664: Training Step 38/59: batchLoss = 2.8256, diffLoss = 13.7921, kgLoss = 0.0839
2025-02-13 00:48:08.004503: Training Step 39/59: batchLoss = 3.0546, diffLoss = 14.9337, kgLoss = 0.0848
2025-02-13 00:48:08.928051: Training Step 40/59: batchLoss = 3.1119, diffLoss = 15.2105, kgLoss = 0.0872
2025-02-13 00:48:09.865666: Training Step 41/59: batchLoss = 2.8490, diffLoss = 13.9246, kgLoss = 0.0801
2025-02-13 00:48:10.788064: Training Step 42/59: batchLoss = 2.8134, diffLoss = 13.7600, kgLoss = 0.0767
2025-02-13 00:48:11.717883: Training Step 43/59: batchLoss = 2.9195, diffLoss = 14.2722, kgLoss = 0.0814
2025-02-13 00:48:12.641127: Training Step 44/59: batchLoss = 3.1477, diffLoss = 15.3708, kgLoss = 0.0919
2025-02-13 00:48:13.565038: Training Step 45/59: batchLoss = 3.0005, diffLoss = 14.6419, kgLoss = 0.0901
2025-02-13 00:48:14.489021: Training Step 46/59: batchLoss = 2.6622, diffLoss = 13.0044, kgLoss = 0.0767
2025-02-13 00:48:15.413973: Training Step 47/59: batchLoss = 2.8984, diffLoss = 14.1543, kgLoss = 0.0844
2025-02-13 00:48:16.361297: Training Step 48/59: batchLoss = 3.1151, diffLoss = 15.2329, kgLoss = 0.0857
2025-02-13 00:48:17.299012: Training Step 49/59: batchLoss = 2.8763, diffLoss = 14.0719, kgLoss = 0.0774
2025-02-13 00:48:18.232534: Training Step 50/59: batchLoss = 2.6412, diffLoss = 12.9077, kgLoss = 0.0745
2025-02-13 00:48:19.172161: Training Step 51/59: batchLoss = 3.6310, diffLoss = 17.7626, kgLoss = 0.0981
2025-02-13 00:48:20.111597: Training Step 52/59: batchLoss = 3.0352, diffLoss = 14.8348, kgLoss = 0.0853
2025-02-13 00:48:21.052237: Training Step 53/59: batchLoss = 3.2013, diffLoss = 15.6497, kgLoss = 0.0892
2025-02-13 00:48:21.988497: Training Step 54/59: batchLoss = 2.9809, diffLoss = 14.5708, kgLoss = 0.0834
2025-02-13 00:48:22.932323: Training Step 55/59: batchLoss = 3.0976, diffLoss = 15.1627, kgLoss = 0.0813
2025-02-13 00:48:23.866389: Training Step 56/59: batchLoss = 2.9352, diffLoss = 14.3763, kgLoss = 0.0749
2025-02-13 00:48:24.715527: Training Step 57/59: batchLoss = 3.1922, diffLoss = 15.6180, kgLoss = 0.0858
2025-02-13 00:48:25.583604: Training Step 58/59: batchLoss = 3.0751, diffLoss = 15.0212, kgLoss = 0.0886
2025-02-13 00:48:25.679123: 
2025-02-13 00:48:25.679634: Epoch 88/1000, Train: epLoss = 0.4349, epDfLoss = 2.1241, epKgLoss = 0.0126  
2025-02-13 00:48:27.154384: Steps 0/47: batch_recall = 36.43, batch_ndcg = 47.38 
2025-02-13 00:48:28.441695: Steps 1/47: batch_recall = 37.45, batch_ndcg = 42.70 
2025-02-13 00:48:29.683661: Steps 2/47: batch_recall = 41.66, batch_ndcg = 47.27 
2025-02-13 00:48:30.931411: Steps 3/47: batch_recall = 44.69, batch_ndcg = 47.35 
2025-02-13 00:48:32.115438: Steps 4/47: batch_recall = 39.22, batch_ndcg = 45.78 
2025-02-13 00:48:33.333207: Steps 5/47: batch_recall = 33.86, batch_ndcg = 40.46 
2025-02-13 00:48:34.527349: Steps 6/47: batch_recall = 38.66, batch_ndcg = 42.00 
2025-02-13 00:48:35.699411: Steps 7/47: batch_recall = 45.26, batch_ndcg = 46.06 
2025-02-13 00:48:36.878012: Steps 8/47: batch_recall = 48.59, batch_ndcg = 52.68 
2025-02-13 00:48:38.012392: Steps 9/47: batch_recall = 47.27, batch_ndcg = 46.99 
2025-02-13 00:48:39.184359: Steps 10/47: batch_recall = 44.08, batch_ndcg = 43.79 
2025-02-13 00:48:40.329014: Steps 11/47: batch_recall = 54.95, batch_ndcg = 52.28 
2025-02-13 00:48:41.485476: Steps 12/47: batch_recall = 50.89, batch_ndcg = 49.34 
2025-02-13 00:48:42.631602: Steps 13/47: batch_recall = 49.48, batch_ndcg = 46.05 
2025-02-13 00:48:43.742025: Steps 14/47: batch_recall = 41.19, batch_ndcg = 42.01 
2025-02-13 00:48:44.849609: Steps 15/47: batch_recall = 56.71, batch_ndcg = 54.20 
2025-02-13 00:48:45.940211: Steps 16/47: batch_recall = 51.81, batch_ndcg = 48.44 
2025-02-13 00:48:47.006207: Steps 17/47: batch_recall = 58.46, batch_ndcg = 50.17 
2025-02-13 00:48:48.087299: Steps 18/47: batch_recall = 52.95, batch_ndcg = 49.46 
2025-02-13 00:48:49.145531: Steps 19/47: batch_recall = 60.06, batch_ndcg = 55.53 
2025-02-13 00:48:50.189612: Steps 20/47: batch_recall = 68.36, batch_ndcg = 61.91 
2025-02-13 00:48:51.234211: Steps 21/47: batch_recall = 65.37, batch_ndcg = 56.28 
2025-02-13 00:48:52.277826: Steps 22/47: batch_recall = 55.75, batch_ndcg = 51.35 
2025-02-13 00:48:53.318652: Steps 23/47: batch_recall = 64.21, batch_ndcg = 55.45 
2025-02-13 00:48:54.372265: Steps 24/47: batch_recall = 65.45, batch_ndcg = 59.08 
2025-02-13 00:48:55.409373: Steps 25/47: batch_recall = 64.83, batch_ndcg = 56.52 
2025-02-13 00:48:56.421853: Steps 26/47: batch_recall = 60.95, batch_ndcg = 53.35 
2025-02-13 00:48:57.457351: Steps 27/47: batch_recall = 61.87, batch_ndcg = 53.42 
2025-02-13 00:48:58.480401: Steps 28/47: batch_recall = 69.79, batch_ndcg = 59.60 
2025-02-13 00:48:59.492865: Steps 29/47: batch_recall = 70.44, batch_ndcg = 59.31 
2025-02-13 00:49:00.513316: Steps 30/47: batch_recall = 73.23, batch_ndcg = 63.70 
2025-02-13 00:49:01.552911: Steps 31/47: batch_recall = 67.72, batch_ndcg = 56.09 
2025-02-13 00:49:02.573905: Steps 32/47: batch_recall = 71.90, batch_ndcg = 65.85 
2025-02-13 00:49:03.594008: Steps 33/47: batch_recall = 79.79, batch_ndcg = 68.00 
2025-02-13 00:49:04.611044: Steps 34/47: batch_recall = 69.32, batch_ndcg = 57.20 
2025-02-13 00:49:05.606354: Steps 35/47: batch_recall = 77.44, batch_ndcg = 65.84 
2025-02-13 00:49:06.606160: Steps 36/47: batch_recall = 78.36, batch_ndcg = 63.71 
2025-02-13 00:49:07.590285: Steps 37/47: batch_recall = 83.87, batch_ndcg = 73.01 
2025-02-13 00:49:08.574384: Steps 38/47: batch_recall = 88.74, batch_ndcg = 72.37 
2025-02-13 00:49:09.545555: Steps 39/47: batch_recall = 88.76, batch_ndcg = 69.43 
2025-02-13 00:49:10.521316: Steps 40/47: batch_recall = 72.48, batch_ndcg = 63.15 
2025-02-13 00:49:11.483029: Steps 41/47: batch_recall = 87.59, batch_ndcg = 71.66 
2025-02-13 00:49:12.439532: Steps 42/47: batch_recall = 81.36, batch_ndcg = 65.53 
2025-02-13 00:49:13.399139: Steps 43/47: batch_recall = 89.11, batch_ndcg = 71.70 
2025-02-13 00:49:14.349594: Steps 44/47: batch_recall = 87.29, batch_ndcg = 69.50 
2025-02-13 00:49:15.289856: Steps 45/47: batch_recall = 92.35, batch_ndcg = 74.82 
2025-02-13 00:49:15.394021: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 00:49:15.394369: Epoch 88/1000, Test: Recall = 0.1219, NDCG = 0.1099  

2025-02-13 00:49:16.572897: Training Step 0/59: batchLoss = 3.0237, diffLoss = 14.7692, kgLoss = 0.0873
2025-02-13 00:49:17.488393: Training Step 1/59: batchLoss = 2.9080, diffLoss = 14.1610, kgLoss = 0.0948
2025-02-13 00:49:18.410350: Training Step 2/59: batchLoss = 2.7691, diffLoss = 13.5018, kgLoss = 0.0859
2025-02-13 00:49:19.326766: Training Step 3/59: batchLoss = 2.5789, diffLoss = 12.5842, kgLoss = 0.0776
2025-02-13 00:49:20.244388: Training Step 4/59: batchLoss = 3.2725, diffLoss = 15.9322, kgLoss = 0.1075
2025-02-13 00:49:21.170368: Training Step 5/59: batchLoss = 2.8364, diffLoss = 13.8695, kgLoss = 0.0781
2025-02-13 00:49:22.108744: Training Step 6/59: batchLoss = 2.9376, diffLoss = 14.3018, kgLoss = 0.0966
2025-02-13 00:49:23.047669: Training Step 7/59: batchLoss = 2.4320, diffLoss = 11.8689, kgLoss = 0.0728
2025-02-13 00:49:23.989244: Training Step 8/59: batchLoss = 2.6192, diffLoss = 12.7796, kgLoss = 0.0791
2025-02-13 00:49:24.921972: Training Step 9/59: batchLoss = 2.8104, diffLoss = 13.7287, kgLoss = 0.0808
2025-02-13 00:49:25.854959: Training Step 10/59: batchLoss = 2.8063, diffLoss = 13.7084, kgLoss = 0.0807
2025-02-13 00:49:26.789075: Training Step 11/59: batchLoss = 2.6697, diffLoss = 13.0149, kgLoss = 0.0834
2025-02-13 00:49:27.723520: Training Step 12/59: batchLoss = 2.9069, diffLoss = 14.1971, kgLoss = 0.0844
2025-02-13 00:49:28.658196: Training Step 13/59: batchLoss = 2.8692, diffLoss = 14.0070, kgLoss = 0.0847
2025-02-13 00:49:29.591448: Training Step 14/59: batchLoss = 2.6081, diffLoss = 12.7273, kgLoss = 0.0782
2025-02-13 00:49:30.517352: Training Step 15/59: batchLoss = 3.0675, diffLoss = 14.9484, kgLoss = 0.0972
2025-02-13 00:49:31.450596: Training Step 16/59: batchLoss = 2.6033, diffLoss = 12.6972, kgLoss = 0.0798
2025-02-13 00:49:32.372579: Training Step 17/59: batchLoss = 2.5918, diffLoss = 12.6581, kgLoss = 0.0753
2025-02-13 00:49:33.294450: Training Step 18/59: batchLoss = 2.9621, diffLoss = 14.4827, kgLoss = 0.0819
2025-02-13 00:49:34.225347: Training Step 19/59: batchLoss = 2.7891, diffLoss = 13.6297, kgLoss = 0.0789
2025-02-13 00:49:35.154782: Training Step 20/59: batchLoss = 2.8362, diffLoss = 13.8739, kgLoss = 0.0767
2025-02-13 00:49:36.084904: Training Step 21/59: batchLoss = 2.8530, diffLoss = 13.9200, kgLoss = 0.0862
2025-02-13 00:49:37.003163: Training Step 22/59: batchLoss = 2.8054, diffLoss = 13.6982, kgLoss = 0.0822
2025-02-13 00:49:37.945724: Training Step 23/59: batchLoss = 2.8647, diffLoss = 13.9884, kgLoss = 0.0838
2025-02-13 00:49:38.866555: Training Step 24/59: batchLoss = 3.1636, diffLoss = 15.4264, kgLoss = 0.0979
2025-02-13 00:49:39.782841: Training Step 25/59: batchLoss = 2.7847, diffLoss = 13.5913, kgLoss = 0.0831
2025-02-13 00:49:40.705222: Training Step 26/59: batchLoss = 2.7963, diffLoss = 13.6897, kgLoss = 0.0730
2025-02-13 00:49:41.630748: Training Step 27/59: batchLoss = 2.7913, diffLoss = 13.6419, kgLoss = 0.0787
2025-02-13 00:49:42.563832: Training Step 28/59: batchLoss = 2.8622, diffLoss = 13.9786, kgLoss = 0.0830
2025-02-13 00:49:43.498613: Training Step 29/59: batchLoss = 2.8706, diffLoss = 13.9993, kgLoss = 0.0884
2025-02-13 00:49:44.432374: Training Step 30/59: batchLoss = 2.7498, diffLoss = 13.4297, kgLoss = 0.0799
2025-02-13 00:49:45.363378: Training Step 31/59: batchLoss = 2.7345, diffLoss = 13.3760, kgLoss = 0.0741
2025-02-13 00:49:46.299412: Training Step 32/59: batchLoss = 2.9247, diffLoss = 14.2608, kgLoss = 0.0906
2025-02-13 00:49:47.231480: Training Step 33/59: batchLoss = 3.2679, diffLoss = 15.9672, kgLoss = 0.0930
2025-02-13 00:49:48.168561: Training Step 34/59: batchLoss = 3.0804, diffLoss = 15.0640, kgLoss = 0.0845
2025-02-13 00:49:49.104219: Training Step 35/59: batchLoss = 3.0560, diffLoss = 14.9504, kgLoss = 0.0824
2025-02-13 00:49:50.036750: Training Step 36/59: batchLoss = 3.2998, diffLoss = 16.1173, kgLoss = 0.0955
2025-02-13 00:49:50.968492: Training Step 37/59: batchLoss = 3.3272, diffLoss = 16.2171, kgLoss = 0.1048
2025-02-13 00:49:51.903792: Training Step 38/59: batchLoss = 3.0988, diffLoss = 15.1105, kgLoss = 0.0958
2025-02-13 00:49:52.828499: Training Step 39/59: batchLoss = 2.8301, diffLoss = 13.8168, kgLoss = 0.0834
2025-02-13 00:49:53.751485: Training Step 40/59: batchLoss = 3.0330, diffLoss = 14.8261, kgLoss = 0.0848
2025-02-13 00:49:54.672499: Training Step 41/59: batchLoss = 2.8718, diffLoss = 14.0350, kgLoss = 0.0810
2025-02-13 00:49:55.606630: Training Step 42/59: batchLoss = 3.0365, diffLoss = 14.8425, kgLoss = 0.0849
2025-02-13 00:49:56.530478: Training Step 43/59: batchLoss = 2.9799, diffLoss = 14.5658, kgLoss = 0.0834
2025-02-13 00:49:57.466877: Training Step 44/59: batchLoss = 3.3681, diffLoss = 16.4442, kgLoss = 0.0990
2025-02-13 00:49:58.385185: Training Step 45/59: batchLoss = 3.2220, diffLoss = 15.7360, kgLoss = 0.0935
2025-02-13 00:49:59.316611: Training Step 46/59: batchLoss = 3.0529, diffLoss = 14.9216, kgLoss = 0.0857
2025-02-13 00:50:00.241468: Training Step 47/59: batchLoss = 2.8669, diffLoss = 13.9930, kgLoss = 0.0854
2025-02-13 00:50:01.170232: Training Step 48/59: batchLoss = 3.5084, diffLoss = 17.1506, kgLoss = 0.0979
2025-02-13 00:50:02.089805: Training Step 49/59: batchLoss = 3.1181, diffLoss = 15.2435, kgLoss = 0.0868
2025-02-13 00:50:03.020341: Training Step 50/59: batchLoss = 2.8266, diffLoss = 13.8211, kgLoss = 0.0780
2025-02-13 00:50:03.956148: Training Step 51/59: batchLoss = 3.1962, diffLoss = 15.6268, kgLoss = 0.0885
2025-02-13 00:50:04.883318: Training Step 52/59: batchLoss = 2.9822, diffLoss = 14.5847, kgLoss = 0.0816
2025-02-13 00:50:05.825506: Training Step 53/59: batchLoss = 3.5200, diffLoss = 17.2147, kgLoss = 0.0963
2025-02-13 00:50:06.756740: Training Step 54/59: batchLoss = 2.9576, diffLoss = 14.4608, kgLoss = 0.0817
2025-02-13 00:50:07.692901: Training Step 55/59: batchLoss = 3.1591, diffLoss = 15.4452, kgLoss = 0.0876
2025-02-13 00:50:08.617049: Training Step 56/59: batchLoss = 2.9898, diffLoss = 14.6073, kgLoss = 0.0854
2025-02-13 00:50:09.460115: Training Step 57/59: batchLoss = 2.6810, diffLoss = 13.1139, kgLoss = 0.0727
2025-02-13 00:50:10.322635: Training Step 58/59: batchLoss = 3.2655, diffLoss = 15.9712, kgLoss = 0.0890
2025-02-13 00:50:10.420615: 
2025-02-13 00:50:10.421376: Epoch 89/1000, Train: epLoss = 0.4342, epDfLoss = 2.1207, epKgLoss = 0.0126  
2025-02-13 00:50:11.897167: Steps 0/47: batch_recall = 37.35, batch_ndcg = 47.92 
2025-02-13 00:50:13.186386: Steps 1/47: batch_recall = 37.24, batch_ndcg = 43.29 
2025-02-13 00:50:14.433120: Steps 2/47: batch_recall = 40.84, batch_ndcg = 47.23 
2025-02-13 00:50:15.696130: Steps 3/47: batch_recall = 43.83, batch_ndcg = 45.78 
2025-02-13 00:50:16.884511: Steps 4/47: batch_recall = 39.86, batch_ndcg = 46.08 
2025-02-13 00:50:18.083407: Steps 5/47: batch_recall = 33.97, batch_ndcg = 40.74 
2025-02-13 00:50:19.279974: Steps 6/47: batch_recall = 37.66, batch_ndcg = 41.26 
2025-02-13 00:50:20.440403: Steps 7/47: batch_recall = 44.85, batch_ndcg = 45.88 
2025-02-13 00:50:21.601732: Steps 8/47: batch_recall = 48.89, batch_ndcg = 52.69 
2025-02-13 00:50:22.725023: Steps 9/47: batch_recall = 46.37, batch_ndcg = 46.76 
2025-02-13 00:50:23.892577: Steps 10/47: batch_recall = 44.62, batch_ndcg = 44.10 
2025-02-13 00:50:25.033441: Steps 11/47: batch_recall = 56.10, batch_ndcg = 53.00 
2025-02-13 00:50:26.179759: Steps 12/47: batch_recall = 50.61, batch_ndcg = 50.20 
2025-02-13 00:50:27.330058: Steps 13/47: batch_recall = 49.61, batch_ndcg = 46.07 
2025-02-13 00:50:28.430439: Steps 14/47: batch_recall = 43.32, batch_ndcg = 42.06 
2025-02-13 00:50:29.531141: Steps 15/47: batch_recall = 56.51, batch_ndcg = 53.54 
2025-02-13 00:50:30.619092: Steps 16/47: batch_recall = 51.12, batch_ndcg = 48.44 
2025-02-13 00:50:31.678779: Steps 17/47: batch_recall = 59.38, batch_ndcg = 49.97 
2025-02-13 00:50:32.768507: Steps 18/47: batch_recall = 52.97, batch_ndcg = 49.81 
2025-02-13 00:50:33.832381: Steps 19/47: batch_recall = 59.99, batch_ndcg = 55.56 
2025-02-13 00:50:34.883819: Steps 20/47: batch_recall = 68.80, batch_ndcg = 62.71 
2025-02-13 00:50:35.921509: Steps 21/47: batch_recall = 67.04, batch_ndcg = 57.23 
2025-02-13 00:50:36.975800: Steps 22/47: batch_recall = 57.91, batch_ndcg = 52.38 
2025-02-13 00:50:38.020151: Steps 23/47: batch_recall = 63.87, batch_ndcg = 55.47 
2025-02-13 00:50:39.060754: Steps 24/47: batch_recall = 65.43, batch_ndcg = 58.23 
2025-02-13 00:50:40.094248: Steps 25/47: batch_recall = 65.73, batch_ndcg = 57.41 
2025-02-13 00:50:41.108418: Steps 26/47: batch_recall = 62.46, batch_ndcg = 54.50 
2025-02-13 00:50:42.133982: Steps 27/47: batch_recall = 61.95, batch_ndcg = 53.66 
2025-02-13 00:50:43.146919: Steps 28/47: batch_recall = 70.92, batch_ndcg = 59.88 
2025-02-13 00:50:44.147554: Steps 29/47: batch_recall = 69.38, batch_ndcg = 58.70 
2025-02-13 00:50:45.149232: Steps 30/47: batch_recall = 73.81, batch_ndcg = 64.09 
2025-02-13 00:50:46.172882: Steps 31/47: batch_recall = 67.40, batch_ndcg = 56.64 
2025-02-13 00:50:47.183892: Steps 32/47: batch_recall = 70.60, batch_ndcg = 65.74 
2025-02-13 00:50:48.197376: Steps 33/47: batch_recall = 80.57, batch_ndcg = 68.77 
2025-02-13 00:50:49.209344: Steps 34/47: batch_recall = 69.14, batch_ndcg = 56.89 
2025-02-13 00:50:50.195981: Steps 35/47: batch_recall = 79.11, batch_ndcg = 66.74 
2025-02-13 00:50:51.189010: Steps 36/47: batch_recall = 78.76, batch_ndcg = 64.33 
2025-02-13 00:50:52.164698: Steps 37/47: batch_recall = 84.03, batch_ndcg = 73.41 
2025-02-13 00:50:53.138005: Steps 38/47: batch_recall = 91.17, batch_ndcg = 74.15 
2025-02-13 00:50:54.111973: Steps 39/47: batch_recall = 87.88, batch_ndcg = 69.31 
2025-02-13 00:50:55.069390: Steps 40/47: batch_recall = 71.60, batch_ndcg = 63.14 
2025-02-13 00:50:56.028949: Steps 41/47: batch_recall = 88.47, batch_ndcg = 72.97 
2025-02-13 00:50:56.981782: Steps 42/47: batch_recall = 82.16, batch_ndcg = 64.89 
2025-02-13 00:50:57.940466: Steps 43/47: batch_recall = 91.13, batch_ndcg = 72.92 
2025-02-13 00:50:58.902277: Steps 44/47: batch_recall = 89.68, batch_ndcg = 71.25 
2025-02-13 00:50:59.839256: Steps 45/47: batch_recall = 93.61, batch_ndcg = 74.59 
2025-02-13 00:50:59.944038: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.56 
2025-02-13 00:50:59.944375: Epoch 89/1000, Test: Recall = 0.1226, NDCG = 0.1104  

2025-02-13 00:51:01.136289: Training Step 0/59: batchLoss = 2.7410, diffLoss = 13.3914, kgLoss = 0.0784
2025-02-13 00:51:02.057230: Training Step 1/59: batchLoss = 2.7788, diffLoss = 13.5358, kgLoss = 0.0896
2025-02-13 00:51:02.976186: Training Step 2/59: batchLoss = 2.7351, diffLoss = 13.3519, kgLoss = 0.0809
2025-02-13 00:51:03.891629: Training Step 3/59: batchLoss = 2.6546, diffLoss = 12.9787, kgLoss = 0.0736
2025-02-13 00:51:04.804047: Training Step 4/59: batchLoss = 2.5267, diffLoss = 12.3222, kgLoss = 0.0778
2025-02-13 00:51:05.727854: Training Step 5/59: batchLoss = 2.9080, diffLoss = 14.1759, kgLoss = 0.0910
2025-02-13 00:51:06.657498: Training Step 6/59: batchLoss = 2.9602, diffLoss = 14.4482, kgLoss = 0.0882
2025-02-13 00:51:07.587373: Training Step 7/59: batchLoss = 2.9683, diffLoss = 14.4827, kgLoss = 0.0897
2025-02-13 00:51:08.528461: Training Step 8/59: batchLoss = 2.8555, diffLoss = 13.9142, kgLoss = 0.0908
2025-02-13 00:51:09.466245: Training Step 9/59: batchLoss = 2.7864, diffLoss = 13.6079, kgLoss = 0.0810
2025-02-13 00:51:10.410775: Training Step 10/59: batchLoss = 2.8850, diffLoss = 14.1064, kgLoss = 0.0796
2025-02-13 00:51:11.344058: Training Step 11/59: batchLoss = 2.7035, diffLoss = 13.2045, kgLoss = 0.0783
2025-02-13 00:51:12.278810: Training Step 12/59: batchLoss = 3.0059, diffLoss = 14.6518, kgLoss = 0.0945
2025-02-13 00:51:13.216616: Training Step 13/59: batchLoss = 2.8120, diffLoss = 13.7267, kgLoss = 0.0833
2025-02-13 00:51:14.152952: Training Step 14/59: batchLoss = 2.8846, diffLoss = 14.0746, kgLoss = 0.0871
2025-02-13 00:51:15.092603: Training Step 15/59: batchLoss = 3.0123, diffLoss = 14.6864, kgLoss = 0.0938
2025-02-13 00:51:16.028814: Training Step 16/59: batchLoss = 3.3213, diffLoss = 16.2086, kgLoss = 0.0995
2025-02-13 00:51:16.954921: Training Step 17/59: batchLoss = 3.0082, diffLoss = 14.7022, kgLoss = 0.0847
2025-02-13 00:51:17.890791: Training Step 18/59: batchLoss = 2.7343, diffLoss = 13.3577, kgLoss = 0.0784
2025-02-13 00:51:18.811793: Training Step 19/59: batchLoss = 2.8014, diffLoss = 13.6966, kgLoss = 0.0776
2025-02-13 00:51:19.741246: Training Step 20/59: batchLoss = 3.0656, diffLoss = 14.9762, kgLoss = 0.0880
2025-02-13 00:51:20.664827: Training Step 21/59: batchLoss = 2.8698, diffLoss = 14.0213, kgLoss = 0.0820
2025-02-13 00:51:21.587735: Training Step 22/59: batchLoss = 3.0429, diffLoss = 14.8745, kgLoss = 0.0850
2025-02-13 00:51:22.518243: Training Step 23/59: batchLoss = 2.8804, diffLoss = 14.0592, kgLoss = 0.0857
2025-02-13 00:51:23.435836: Training Step 24/59: batchLoss = 2.6774, diffLoss = 13.0925, kgLoss = 0.0736
2025-02-13 00:51:24.364399: Training Step 25/59: batchLoss = 2.7781, diffLoss = 13.5436, kgLoss = 0.0868
2025-02-13 00:51:25.290609: Training Step 26/59: batchLoss = 2.7595, diffLoss = 13.4520, kgLoss = 0.0864
2025-02-13 00:51:26.215574: Training Step 27/59: batchLoss = 3.3593, diffLoss = 16.3756, kgLoss = 0.1053
2025-02-13 00:51:27.145947: Training Step 28/59: batchLoss = 2.6294, diffLoss = 12.8330, kgLoss = 0.0785
2025-02-13 00:51:28.071557: Training Step 29/59: batchLoss = 2.9778, diffLoss = 14.5572, kgLoss = 0.0829
2025-02-13 00:51:28.998157: Training Step 30/59: batchLoss = 3.1819, diffLoss = 15.5555, kgLoss = 0.0886
2025-02-13 00:51:29.933928: Training Step 31/59: batchLoss = 2.8062, diffLoss = 13.6602, kgLoss = 0.0927
2025-02-13 00:51:30.869056: Training Step 32/59: batchLoss = 2.7863, diffLoss = 13.6235, kgLoss = 0.0770
2025-02-13 00:51:31.807931: Training Step 33/59: batchLoss = 3.0923, diffLoss = 15.1151, kgLoss = 0.0867
2025-02-13 00:51:32.756450: Training Step 34/59: batchLoss = 2.9233, diffLoss = 14.2813, kgLoss = 0.0838
2025-02-13 00:51:33.687610: Training Step 35/59: batchLoss = 3.2147, diffLoss = 15.7297, kgLoss = 0.0859
2025-02-13 00:51:34.628347: Training Step 36/59: batchLoss = 2.8148, diffLoss = 13.7416, kgLoss = 0.0831
2025-02-13 00:51:35.575443: Training Step 37/59: batchLoss = 2.9052, diffLoss = 14.1956, kgLoss = 0.0826
2025-02-13 00:51:36.514154: Training Step 38/59: batchLoss = 2.5123, diffLoss = 12.2916, kgLoss = 0.0674
2025-02-13 00:51:37.444663: Training Step 39/59: batchLoss = 2.9926, diffLoss = 14.6192, kgLoss = 0.0860
2025-02-13 00:51:38.372684: Training Step 40/59: batchLoss = 2.7604, diffLoss = 13.4913, kgLoss = 0.0777
2025-02-13 00:51:39.298949: Training Step 41/59: batchLoss = 2.9310, diffLoss = 14.3213, kgLoss = 0.0834
2025-02-13 00:51:40.244424: Training Step 42/59: batchLoss = 2.9490, diffLoss = 14.3852, kgLoss = 0.0900
2025-02-13 00:51:41.163494: Training Step 43/59: batchLoss = 2.9591, diffLoss = 14.4546, kgLoss = 0.0852
2025-02-13 00:51:42.090726: Training Step 44/59: batchLoss = 3.0411, diffLoss = 14.8650, kgLoss = 0.0852
2025-02-13 00:51:43.009967: Training Step 45/59: batchLoss = 2.8464, diffLoss = 13.9221, kgLoss = 0.0775
2025-02-13 00:51:43.931611: Training Step 46/59: batchLoss = 3.0091, diffLoss = 14.6863, kgLoss = 0.0898
2025-02-13 00:51:44.856357: Training Step 47/59: batchLoss = 2.8957, diffLoss = 14.1184, kgLoss = 0.0900
2025-02-13 00:51:45.778526: Training Step 48/59: batchLoss = 3.1780, diffLoss = 15.5225, kgLoss = 0.0919
2025-02-13 00:51:46.709737: Training Step 49/59: batchLoss = 3.1377, diffLoss = 15.3475, kgLoss = 0.0853
2025-02-13 00:51:47.634504: Training Step 50/59: batchLoss = 2.8830, diffLoss = 14.0655, kgLoss = 0.0873
2025-02-13 00:51:48.564210: Training Step 51/59: batchLoss = 3.1213, diffLoss = 15.2771, kgLoss = 0.0824
2025-02-13 00:51:49.498506: Training Step 52/59: batchLoss = 2.8363, diffLoss = 13.8352, kgLoss = 0.0866
2025-02-13 00:51:50.440231: Training Step 53/59: batchLoss = 2.9896, diffLoss = 14.6158, kgLoss = 0.0831
2025-02-13 00:51:51.377629: Training Step 54/59: batchLoss = 3.5602, diffLoss = 17.4086, kgLoss = 0.0981
2025-02-13 00:51:52.315935: Training Step 55/59: batchLoss = 3.3052, diffLoss = 16.1639, kgLoss = 0.0905
2025-02-13 00:51:53.246765: Training Step 56/59: batchLoss = 2.9613, diffLoss = 14.4444, kgLoss = 0.0905
2025-02-13 00:51:54.098337: Training Step 57/59: batchLoss = 3.3339, diffLoss = 16.3081, kgLoss = 0.0904
2025-02-13 00:51:54.955949: Training Step 58/59: batchLoss = 3.2827, diffLoss = 16.0391, kgLoss = 0.0936
2025-02-13 00:51:55.049220: 
2025-02-13 00:51:55.049712: Epoch 90/1000, Train: epLoss = 0.4333, epDfLoss = 2.1162, epKgLoss = 0.0126  
2025-02-13 00:51:56.532191: Steps 0/47: batch_recall = 37.68, batch_ndcg = 47.93 
2025-02-13 00:51:57.840297: Steps 1/47: batch_recall = 37.01, batch_ndcg = 43.07 
2025-02-13 00:51:59.097254: Steps 2/47: batch_recall = 40.30, batch_ndcg = 46.47 
2025-02-13 00:52:00.358023: Steps 3/47: batch_recall = 44.12, batch_ndcg = 46.28 
2025-02-13 00:52:01.553370: Steps 4/47: batch_recall = 40.28, batch_ndcg = 46.60 
2025-02-13 00:52:02.779203: Steps 5/47: batch_recall = 34.53, batch_ndcg = 40.80 
2025-02-13 00:52:03.980584: Steps 6/47: batch_recall = 38.62, batch_ndcg = 41.59 
2025-02-13 00:52:05.156206: Steps 7/47: batch_recall = 44.92, batch_ndcg = 46.36 
2025-02-13 00:52:06.322091: Steps 8/47: batch_recall = 48.71, batch_ndcg = 53.04 
2025-02-13 00:52:07.472406: Steps 9/47: batch_recall = 46.00, batch_ndcg = 46.36 
2025-02-13 00:52:08.639559: Steps 10/47: batch_recall = 44.62, batch_ndcg = 44.65 
2025-02-13 00:52:09.786226: Steps 11/47: batch_recall = 55.28, batch_ndcg = 52.19 
2025-02-13 00:52:10.936241: Steps 12/47: batch_recall = 50.15, batch_ndcg = 48.65 
2025-02-13 00:52:12.082114: Steps 13/47: batch_recall = 50.09, batch_ndcg = 45.99 
2025-02-13 00:52:13.182727: Steps 14/47: batch_recall = 41.93, batch_ndcg = 41.64 
2025-02-13 00:52:14.284492: Steps 15/47: batch_recall = 55.80, batch_ndcg = 53.22 
2025-02-13 00:52:15.372845: Steps 16/47: batch_recall = 52.62, batch_ndcg = 48.02 
2025-02-13 00:52:16.438514: Steps 17/47: batch_recall = 57.17, batch_ndcg = 49.27 
2025-02-13 00:52:17.532781: Steps 18/47: batch_recall = 53.74, batch_ndcg = 49.75 
2025-02-13 00:52:18.617737: Steps 19/47: batch_recall = 60.39, batch_ndcg = 55.47 
2025-02-13 00:52:19.655243: Steps 20/47: batch_recall = 69.39, batch_ndcg = 62.24 
2025-02-13 00:52:20.697654: Steps 21/47: batch_recall = 67.30, batch_ndcg = 56.13 
2025-02-13 00:52:21.741349: Steps 22/47: batch_recall = 54.45, batch_ndcg = 51.20 
2025-02-13 00:52:22.788656: Steps 23/47: batch_recall = 62.73, batch_ndcg = 55.12 
2025-02-13 00:52:23.844036: Steps 24/47: batch_recall = 66.08, batch_ndcg = 58.35 
2025-02-13 00:52:24.889828: Steps 25/47: batch_recall = 66.64, batch_ndcg = 57.74 
2025-02-13 00:52:25.905655: Steps 26/47: batch_recall = 63.05, batch_ndcg = 54.65 
2025-02-13 00:52:26.928763: Steps 27/47: batch_recall = 63.15, batch_ndcg = 53.38 
2025-02-13 00:52:27.949072: Steps 28/47: batch_recall = 70.48, batch_ndcg = 59.71 
2025-02-13 00:52:28.962736: Steps 29/47: batch_recall = 69.30, batch_ndcg = 58.01 
2025-02-13 00:52:29.982450: Steps 30/47: batch_recall = 72.82, batch_ndcg = 63.68 
2025-02-13 00:52:31.029754: Steps 31/47: batch_recall = 65.99, batch_ndcg = 55.74 
2025-02-13 00:52:32.045817: Steps 32/47: batch_recall = 72.24, batch_ndcg = 65.83 
2025-02-13 00:52:33.073394: Steps 33/47: batch_recall = 79.35, batch_ndcg = 68.06 
2025-02-13 00:52:34.100145: Steps 34/47: batch_recall = 68.64, batch_ndcg = 56.82 
2025-02-13 00:52:35.092469: Steps 35/47: batch_recall = 79.98, batch_ndcg = 67.04 
2025-02-13 00:52:36.087461: Steps 36/47: batch_recall = 79.19, batch_ndcg = 64.48 
2025-02-13 00:52:37.075871: Steps 37/47: batch_recall = 84.34, batch_ndcg = 73.43 
2025-02-13 00:52:38.085441: Steps 38/47: batch_recall = 89.61, batch_ndcg = 73.17 
2025-02-13 00:52:39.053681: Steps 39/47: batch_recall = 87.44, batch_ndcg = 69.11 
2025-02-13 00:52:40.026604: Steps 40/47: batch_recall = 72.44, batch_ndcg = 62.62 
2025-02-13 00:52:40.994168: Steps 41/47: batch_recall = 88.60, batch_ndcg = 72.63 
2025-02-13 00:52:41.956510: Steps 42/47: batch_recall = 81.43, batch_ndcg = 64.37 
2025-02-13 00:52:42.920967: Steps 43/47: batch_recall = 90.65, batch_ndcg = 72.96 
2025-02-13 00:52:43.888293: Steps 44/47: batch_recall = 87.71, batch_ndcg = 69.48 
2025-02-13 00:52:44.823928: Steps 45/47: batch_recall = 93.53, batch_ndcg = 75.35 
2025-02-13 00:52:44.928226: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 00:52:44.928367: Epoch 90/1000, Test: Recall = 0.1223, NDCG = 0.1099  

2025-02-13 00:52:46.116021: Training Step 0/59: batchLoss = 2.5450, diffLoss = 12.3976, kgLoss = 0.0818
2025-02-13 00:52:47.047762: Training Step 1/59: batchLoss = 2.9627, diffLoss = 14.4553, kgLoss = 0.0895
2025-02-13 00:52:47.965949: Training Step 2/59: batchLoss = 2.9162, diffLoss = 14.2180, kgLoss = 0.0908
2025-02-13 00:52:48.898980: Training Step 3/59: batchLoss = 2.9262, diffLoss = 14.2858, kgLoss = 0.0863
2025-02-13 00:52:49.819982: Training Step 4/59: batchLoss = 2.7534, diffLoss = 13.4584, kgLoss = 0.0772
2025-02-13 00:52:50.740436: Training Step 5/59: batchLoss = 2.9162, diffLoss = 14.2322, kgLoss = 0.0871
2025-02-13 00:52:51.665772: Training Step 6/59: batchLoss = 2.7338, diffLoss = 13.3278, kgLoss = 0.0853
2025-02-13 00:52:52.596629: Training Step 7/59: batchLoss = 2.7666, diffLoss = 13.5138, kgLoss = 0.0798
2025-02-13 00:52:53.531259: Training Step 8/59: batchLoss = 2.7244, diffLoss = 13.2832, kgLoss = 0.0847
2025-02-13 00:52:54.462165: Training Step 9/59: batchLoss = 2.8669, diffLoss = 14.0114, kgLoss = 0.0807
2025-02-13 00:52:55.399158: Training Step 10/59: batchLoss = 2.9591, diffLoss = 14.4280, kgLoss = 0.0919
2025-02-13 00:52:56.326607: Training Step 11/59: batchLoss = 2.7876, diffLoss = 13.6229, kgLoss = 0.0787
2025-02-13 00:52:57.254104: Training Step 12/59: batchLoss = 2.9384, diffLoss = 14.3403, kgLoss = 0.0879
2025-02-13 00:52:58.186172: Training Step 13/59: batchLoss = 2.5666, diffLoss = 12.5448, kgLoss = 0.0721
2025-02-13 00:52:59.109876: Training Step 14/59: batchLoss = 2.8247, diffLoss = 13.7895, kgLoss = 0.0835
2025-02-13 00:53:00.038209: Training Step 15/59: batchLoss = 2.9030, diffLoss = 14.1913, kgLoss = 0.0810
2025-02-13 00:53:00.958870: Training Step 16/59: batchLoss = 2.6026, diffLoss = 12.7026, kgLoss = 0.0776
2025-02-13 00:53:01.886567: Training Step 17/59: batchLoss = 3.0932, diffLoss = 15.1341, kgLoss = 0.0830
2025-02-13 00:53:02.805499: Training Step 18/59: batchLoss = 2.9401, diffLoss = 14.3489, kgLoss = 0.0879
2025-02-13 00:53:03.732573: Training Step 19/59: batchLoss = 3.0777, diffLoss = 15.0334, kgLoss = 0.0887
2025-02-13 00:53:04.653776: Training Step 20/59: batchLoss = 2.8852, diffLoss = 14.1035, kgLoss = 0.0806
2025-02-13 00:53:05.581243: Training Step 21/59: batchLoss = 2.6761, diffLoss = 13.0847, kgLoss = 0.0739
2025-02-13 00:53:06.508162: Training Step 22/59: batchLoss = 3.1650, diffLoss = 15.4673, kgLoss = 0.0894
2025-02-13 00:53:07.431697: Training Step 23/59: batchLoss = 2.8562, diffLoss = 13.9403, kgLoss = 0.0852
2025-02-13 00:53:08.371405: Training Step 24/59: batchLoss = 3.0656, diffLoss = 14.9479, kgLoss = 0.0950
2025-02-13 00:53:09.298659: Training Step 25/59: batchLoss = 2.9313, diffLoss = 14.3242, kgLoss = 0.0831
2025-02-13 00:53:10.230978: Training Step 26/59: batchLoss = 2.9932, diffLoss = 14.6291, kgLoss = 0.0842
2025-02-13 00:53:11.154333: Training Step 27/59: batchLoss = 2.7195, diffLoss = 13.2803, kgLoss = 0.0793
2025-02-13 00:53:12.078160: Training Step 28/59: batchLoss = 2.8627, diffLoss = 13.9862, kgLoss = 0.0818
2025-02-13 00:53:13.015201: Training Step 29/59: batchLoss = 3.1683, diffLoss = 15.4825, kgLoss = 0.0897
2025-02-13 00:53:13.942386: Training Step 30/59: batchLoss = 3.0835, diffLoss = 15.0882, kgLoss = 0.0823
2025-02-13 00:53:14.875598: Training Step 31/59: batchLoss = 3.2797, diffLoss = 16.0250, kgLoss = 0.0933
2025-02-13 00:53:15.811371: Training Step 32/59: batchLoss = 2.9809, diffLoss = 14.5495, kgLoss = 0.0887
2025-02-13 00:53:16.748917: Training Step 33/59: batchLoss = 2.8439, diffLoss = 13.8872, kgLoss = 0.0831
2025-02-13 00:53:17.683711: Training Step 34/59: batchLoss = 3.0211, diffLoss = 14.7634, kgLoss = 0.0855
2025-02-13 00:53:18.608180: Training Step 35/59: batchLoss = 2.7989, diffLoss = 13.6522, kgLoss = 0.0856
2025-02-13 00:53:19.543928: Training Step 36/59: batchLoss = 2.8836, diffLoss = 14.0811, kgLoss = 0.0843
2025-02-13 00:53:20.477164: Training Step 37/59: batchLoss = 2.9315, diffLoss = 14.3102, kgLoss = 0.0868
2025-02-13 00:53:21.407819: Training Step 38/59: batchLoss = 2.6789, diffLoss = 13.0808, kgLoss = 0.0785
2025-02-13 00:53:22.330318: Training Step 39/59: batchLoss = 2.8864, diffLoss = 14.1040, kgLoss = 0.0821
2025-02-13 00:53:23.254377: Training Step 40/59: batchLoss = 2.9653, diffLoss = 14.4886, kgLoss = 0.0844
2025-02-13 00:53:24.175356: Training Step 41/59: batchLoss = 3.1190, diffLoss = 15.2311, kgLoss = 0.0910
2025-02-13 00:53:25.092184: Training Step 42/59: batchLoss = 2.7676, diffLoss = 13.5252, kgLoss = 0.0782
2025-02-13 00:53:26.014832: Training Step 43/59: batchLoss = 3.3688, diffLoss = 16.4360, kgLoss = 0.1020
2025-02-13 00:53:26.934487: Training Step 44/59: batchLoss = 3.0382, diffLoss = 14.8176, kgLoss = 0.0934
2025-02-13 00:53:27.860778: Training Step 45/59: batchLoss = 2.8287, diffLoss = 13.8336, kgLoss = 0.0775
2025-02-13 00:53:28.797714: Training Step 46/59: batchLoss = 3.2750, diffLoss = 16.0247, kgLoss = 0.0876
2025-02-13 00:53:29.719007: Training Step 47/59: batchLoss = 2.8683, diffLoss = 14.0360, kgLoss = 0.0763
2025-02-13 00:53:30.651589: Training Step 48/59: batchLoss = 3.2224, diffLoss = 15.7360, kgLoss = 0.0940
2025-02-13 00:53:31.569717: Training Step 49/59: batchLoss = 2.9415, diffLoss = 14.4015, kgLoss = 0.0765
2025-02-13 00:53:32.493617: Training Step 50/59: batchLoss = 3.0825, diffLoss = 15.0658, kgLoss = 0.0867
2025-02-13 00:53:33.423690: Training Step 51/59: batchLoss = 3.1193, diffLoss = 15.2456, kgLoss = 0.0877
2025-02-13 00:53:34.353573: Training Step 52/59: batchLoss = 3.6505, diffLoss = 17.8349, kgLoss = 0.1044
2025-02-13 00:53:35.283967: Training Step 53/59: batchLoss = 2.9827, diffLoss = 14.5606, kgLoss = 0.0882
2025-02-13 00:53:36.214677: Training Step 54/59: batchLoss = 3.1915, diffLoss = 15.6092, kgLoss = 0.0871
2025-02-13 00:53:37.149931: Training Step 55/59: batchLoss = 3.2952, diffLoss = 16.1097, kgLoss = 0.0916
2025-02-13 00:53:38.083494: Training Step 56/59: batchLoss = 3.5372, diffLoss = 17.2923, kgLoss = 0.0984
2025-02-13 00:53:38.935980: Training Step 57/59: batchLoss = 3.0611, diffLoss = 14.9690, kgLoss = 0.0841
2025-02-13 00:53:39.794017: Training Step 58/59: batchLoss = 3.0085, diffLoss = 14.6947, kgLoss = 0.0869
2025-02-13 00:53:39.892250: 
2025-02-13 00:53:39.892615: Epoch 91/1000, Train: epLoss = 0.4371, epDfLoss = 2.1350, epKgLoss = 0.0126  
2025-02-13 00:53:41.380948: Steps 0/47: batch_recall = 38.11, batch_ndcg = 47.98 
2025-02-13 00:53:42.685297: Steps 1/47: batch_recall = 36.65, batch_ndcg = 42.68 
2025-02-13 00:53:43.931563: Steps 2/47: batch_recall = 40.41, batch_ndcg = 46.38 
2025-02-13 00:53:45.194965: Steps 3/47: batch_recall = 44.50, batch_ndcg = 46.62 
2025-02-13 00:53:46.392660: Steps 4/47: batch_recall = 39.62, batch_ndcg = 45.77 
2025-02-13 00:53:47.604004: Steps 5/47: batch_recall = 33.69, batch_ndcg = 39.90 
2025-02-13 00:53:48.810003: Steps 6/47: batch_recall = 38.30, batch_ndcg = 41.17 
2025-02-13 00:53:49.973018: Steps 7/47: batch_recall = 44.27, batch_ndcg = 46.50 
2025-02-13 00:53:51.158002: Steps 8/47: batch_recall = 48.25, batch_ndcg = 52.10 
2025-02-13 00:53:52.298272: Steps 9/47: batch_recall = 46.46, batch_ndcg = 46.98 
2025-02-13 00:53:53.459634: Steps 10/47: batch_recall = 44.54, batch_ndcg = 44.63 
2025-02-13 00:53:54.612908: Steps 11/47: batch_recall = 55.52, batch_ndcg = 52.23 
2025-02-13 00:53:55.771006: Steps 12/47: batch_recall = 49.78, batch_ndcg = 48.95 
2025-02-13 00:53:56.913362: Steps 13/47: batch_recall = 48.85, batch_ndcg = 45.90 
2025-02-13 00:53:58.024816: Steps 14/47: batch_recall = 41.59, batch_ndcg = 41.74 
2025-02-13 00:53:59.126426: Steps 15/47: batch_recall = 57.46, batch_ndcg = 53.96 
2025-02-13 00:54:00.218615: Steps 16/47: batch_recall = 51.14, batch_ndcg = 46.99 
2025-02-13 00:54:01.279575: Steps 17/47: batch_recall = 58.23, batch_ndcg = 50.17 
2025-02-13 00:54:02.368047: Steps 18/47: batch_recall = 53.75, batch_ndcg = 50.27 
2025-02-13 00:54:03.437040: Steps 19/47: batch_recall = 59.84, batch_ndcg = 55.05 
2025-02-13 00:54:04.488838: Steps 20/47: batch_recall = 68.95, batch_ndcg = 62.29 
2025-02-13 00:54:05.524574: Steps 21/47: batch_recall = 66.28, batch_ndcg = 56.30 
2025-02-13 00:54:06.576907: Steps 22/47: batch_recall = 55.40, batch_ndcg = 51.63 
2025-02-13 00:54:07.619032: Steps 23/47: batch_recall = 64.45, batch_ndcg = 55.71 
2025-02-13 00:54:08.678110: Steps 24/47: batch_recall = 64.14, batch_ndcg = 57.36 
2025-02-13 00:54:09.715185: Steps 25/47: batch_recall = 65.54, batch_ndcg = 57.42 
2025-02-13 00:54:10.717210: Steps 26/47: batch_recall = 62.87, batch_ndcg = 54.73 
2025-02-13 00:54:11.743205: Steps 27/47: batch_recall = 63.46, batch_ndcg = 53.52 
2025-02-13 00:54:12.749874: Steps 28/47: batch_recall = 70.59, batch_ndcg = 59.74 
2025-02-13 00:54:13.745417: Steps 29/47: batch_recall = 69.97, batch_ndcg = 57.93 
2025-02-13 00:54:14.747602: Steps 30/47: batch_recall = 72.84, batch_ndcg = 63.94 
2025-02-13 00:54:15.781777: Steps 31/47: batch_recall = 66.09, batch_ndcg = 56.03 
2025-02-13 00:54:16.794933: Steps 32/47: batch_recall = 71.45, batch_ndcg = 66.33 
2025-02-13 00:54:17.817195: Steps 33/47: batch_recall = 79.56, batch_ndcg = 67.55 
2025-02-13 00:54:18.834790: Steps 34/47: batch_recall = 68.35, batch_ndcg = 56.72 
2025-02-13 00:54:19.825664: Steps 35/47: batch_recall = 77.72, batch_ndcg = 66.66 
2025-02-13 00:54:20.812737: Steps 36/47: batch_recall = 77.96, batch_ndcg = 64.16 
2025-02-13 00:54:21.790632: Steps 37/47: batch_recall = 84.15, batch_ndcg = 72.83 
2025-02-13 00:54:22.774584: Steps 38/47: batch_recall = 89.35, batch_ndcg = 72.33 
2025-02-13 00:54:23.754579: Steps 39/47: batch_recall = 86.89, batch_ndcg = 69.78 
2025-02-13 00:54:24.725903: Steps 40/47: batch_recall = 72.86, batch_ndcg = 62.41 
2025-02-13 00:54:25.688879: Steps 41/47: batch_recall = 86.50, batch_ndcg = 72.32 
2025-02-13 00:54:26.641636: Steps 42/47: batch_recall = 81.65, batch_ndcg = 64.89 
2025-02-13 00:54:27.601147: Steps 43/47: batch_recall = 90.62, batch_ndcg = 73.42 
2025-02-13 00:54:28.551288: Steps 44/47: batch_recall = 86.68, batch_ndcg = 69.98 
2025-02-13 00:54:29.485154: Steps 45/47: batch_recall = 92.85, batch_ndcg = 75.69 
2025-02-13 00:54:29.590411: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.67 
2025-02-13 00:54:29.590545: Epoch 91/1000, Test: Recall = 0.1218, NDCG = 0.1099  

2025-02-13 00:54:30.778279: Training Step 0/59: batchLoss = 2.9868, diffLoss = 14.5973, kgLoss = 0.0842
2025-02-13 00:54:31.697571: Training Step 1/59: batchLoss = 2.8255, diffLoss = 13.7550, kgLoss = 0.0931
2025-02-13 00:54:32.619101: Training Step 2/59: batchLoss = 2.7502, diffLoss = 13.4350, kgLoss = 0.0790
2025-02-13 00:54:33.543719: Training Step 3/59: batchLoss = 2.9294, diffLoss = 14.2904, kgLoss = 0.0892
2025-02-13 00:54:34.464501: Training Step 4/59: batchLoss = 2.8476, diffLoss = 13.9113, kgLoss = 0.0817
2025-02-13 00:54:35.381142: Training Step 5/59: batchLoss = 2.9902, diffLoss = 14.5970, kgLoss = 0.0886
2025-02-13 00:54:36.317809: Training Step 6/59: batchLoss = 2.8068, diffLoss = 13.7157, kgLoss = 0.0796
2025-02-13 00:54:37.258759: Training Step 7/59: batchLoss = 2.8355, diffLoss = 13.8327, kgLoss = 0.0862
2025-02-13 00:54:38.190718: Training Step 8/59: batchLoss = 2.9131, diffLoss = 14.2190, kgLoss = 0.0866
2025-02-13 00:54:39.117670: Training Step 9/59: batchLoss = 2.9321, diffLoss = 14.2933, kgLoss = 0.0918
2025-02-13 00:54:40.053631: Training Step 10/59: batchLoss = 2.7764, diffLoss = 13.5523, kgLoss = 0.0825
2025-02-13 00:54:40.988252: Training Step 11/59: batchLoss = 2.9791, diffLoss = 14.5039, kgLoss = 0.0979
2025-02-13 00:54:41.919953: Training Step 12/59: batchLoss = 2.9855, diffLoss = 14.5733, kgLoss = 0.0886
2025-02-13 00:54:42.860245: Training Step 13/59: batchLoss = 2.8085, diffLoss = 13.7044, kgLoss = 0.0846
2025-02-13 00:54:43.797239: Training Step 14/59: batchLoss = 2.8489, diffLoss = 13.8891, kgLoss = 0.0889
2025-02-13 00:54:44.736436: Training Step 15/59: batchLoss = 2.9403, diffLoss = 14.3654, kgLoss = 0.0840
2025-02-13 00:54:45.673090: Training Step 16/59: batchLoss = 2.8428, diffLoss = 13.8692, kgLoss = 0.0862
2025-02-13 00:54:46.599709: Training Step 17/59: batchLoss = 3.2021, diffLoss = 15.6324, kgLoss = 0.0945
2025-02-13 00:54:47.531191: Training Step 18/59: batchLoss = 2.8352, diffLoss = 13.8239, kgLoss = 0.0880
2025-02-13 00:54:48.457939: Training Step 19/59: batchLoss = 3.0186, diffLoss = 14.7134, kgLoss = 0.0948
2025-02-13 00:54:49.389465: Training Step 20/59: batchLoss = 2.9540, diffLoss = 14.4166, kgLoss = 0.0884
2025-02-13 00:54:50.326236: Training Step 21/59: batchLoss = 3.1252, diffLoss = 15.2857, kgLoss = 0.0851
2025-02-13 00:54:51.250682: Training Step 22/59: batchLoss = 2.7228, diffLoss = 13.2958, kgLoss = 0.0795
2025-02-13 00:54:52.180941: Training Step 23/59: batchLoss = 2.8402, diffLoss = 13.8616, kgLoss = 0.0848
2025-02-13 00:54:53.101899: Training Step 24/59: batchLoss = 3.1211, diffLoss = 15.2291, kgLoss = 0.0941
2025-02-13 00:54:54.043405: Training Step 25/59: batchLoss = 2.6719, diffLoss = 13.0735, kgLoss = 0.0715
2025-02-13 00:54:54.967195: Training Step 26/59: batchLoss = 2.9888, diffLoss = 14.6130, kgLoss = 0.0827
2025-02-13 00:54:55.889887: Training Step 27/59: batchLoss = 2.9041, diffLoss = 14.2121, kgLoss = 0.0772
2025-02-13 00:54:56.821580: Training Step 28/59: batchLoss = 3.2702, diffLoss = 15.9856, kgLoss = 0.0913
2025-02-13 00:54:57.754360: Training Step 29/59: batchLoss = 2.9593, diffLoss = 14.4526, kgLoss = 0.0860
2025-02-13 00:54:58.691443: Training Step 30/59: batchLoss = 3.2626, diffLoss = 15.9256, kgLoss = 0.0968
2025-02-13 00:54:59.632730: Training Step 31/59: batchLoss = 2.6209, diffLoss = 12.8026, kgLoss = 0.0755
2025-02-13 00:55:00.568980: Training Step 32/59: batchLoss = 3.1701, diffLoss = 15.5014, kgLoss = 0.0872
2025-02-13 00:55:01.499023: Training Step 33/59: batchLoss = 2.9618, diffLoss = 14.4794, kgLoss = 0.0824
2025-02-13 00:55:02.451831: Training Step 34/59: batchLoss = 2.9265, diffLoss = 14.2853, kgLoss = 0.0868
2025-02-13 00:55:03.390850: Training Step 35/59: batchLoss = 3.1686, diffLoss = 15.4888, kgLoss = 0.0886
2025-02-13 00:55:04.329776: Training Step 36/59: batchLoss = 2.9708, diffLoss = 14.5201, kgLoss = 0.0835
2025-02-13 00:55:05.265399: Training Step 37/59: batchLoss = 3.0268, diffLoss = 14.7899, kgLoss = 0.0860
2025-02-13 00:55:06.193959: Training Step 38/59: batchLoss = 2.7303, diffLoss = 13.3284, kgLoss = 0.0808
2025-02-13 00:55:07.120957: Training Step 39/59: batchLoss = 2.6456, diffLoss = 12.9300, kgLoss = 0.0744
2025-02-13 00:55:08.052233: Training Step 40/59: batchLoss = 2.9824, diffLoss = 14.5789, kgLoss = 0.0832
2025-02-13 00:55:08.970710: Training Step 41/59: batchLoss = 3.0189, diffLoss = 14.7545, kgLoss = 0.0850
2025-02-13 00:55:09.891164: Training Step 42/59: batchLoss = 2.7930, diffLoss = 13.6561, kgLoss = 0.0772
2025-02-13 00:55:10.816204: Training Step 43/59: batchLoss = 3.0296, diffLoss = 14.7556, kgLoss = 0.0981
2025-02-13 00:55:11.750585: Training Step 44/59: batchLoss = 2.9024, diffLoss = 14.1632, kgLoss = 0.0872
2025-02-13 00:55:12.676645: Training Step 45/59: batchLoss = 2.8626, diffLoss = 13.9996, kgLoss = 0.0783
2025-02-13 00:55:13.612548: Training Step 46/59: batchLoss = 3.3259, diffLoss = 16.2405, kgLoss = 0.0972
2025-02-13 00:55:14.536604: Training Step 47/59: batchLoss = 2.8125, diffLoss = 13.7214, kgLoss = 0.0853
2025-02-13 00:55:15.462565: Training Step 48/59: batchLoss = 3.1915, diffLoss = 15.5735, kgLoss = 0.0960
2025-02-13 00:55:16.393765: Training Step 49/59: batchLoss = 3.1285, diffLoss = 15.3036, kgLoss = 0.0847
2025-02-13 00:55:17.311342: Training Step 50/59: batchLoss = 3.0546, diffLoss = 14.9227, kgLoss = 0.0876
2025-02-13 00:55:18.236028: Training Step 51/59: batchLoss = 3.1234, diffLoss = 15.2608, kgLoss = 0.0891
2025-02-13 00:55:19.179475: Training Step 52/59: batchLoss = 3.2890, diffLoss = 16.0870, kgLoss = 0.0895
2025-02-13 00:55:20.112804: Training Step 53/59: batchLoss = 2.7505, diffLoss = 13.4375, kgLoss = 0.0788
2025-02-13 00:55:21.050677: Training Step 54/59: batchLoss = 3.1307, diffLoss = 15.2893, kgLoss = 0.0911
2025-02-13 00:55:21.992829: Training Step 55/59: batchLoss = 2.8631, diffLoss = 14.0182, kgLoss = 0.0743
2025-02-13 00:55:22.924480: Training Step 56/59: batchLoss = 2.5896, diffLoss = 12.6522, kgLoss = 0.0740
2025-02-13 00:55:23.776531: Training Step 57/59: batchLoss = 2.7755, diffLoss = 13.5739, kgLoss = 0.0758
2025-02-13 00:55:24.639790: Training Step 58/59: batchLoss = 2.9071, diffLoss = 14.2050, kgLoss = 0.0826
2025-02-13 00:55:24.737643: 
2025-02-13 00:55:24.738215: Epoch 92/1000, Train: epLoss = 0.4341, epDfLoss = 2.1199, epKgLoss = 0.0126  
2025-02-13 00:55:26.219876: Steps 0/47: batch_recall = 36.86, batch_ndcg = 47.69 
2025-02-13 00:55:27.539529: Steps 1/47: batch_recall = 36.17, batch_ndcg = 42.56 
2025-02-13 00:55:28.785255: Steps 2/47: batch_recall = 40.31, batch_ndcg = 46.37 
2025-02-13 00:55:30.025175: Steps 3/47: batch_recall = 44.28, batch_ndcg = 46.99 
2025-02-13 00:55:31.217335: Steps 4/47: batch_recall = 39.67, batch_ndcg = 45.83 
2025-02-13 00:55:32.420976: Steps 5/47: batch_recall = 33.20, batch_ndcg = 39.73 
2025-02-13 00:55:33.604735: Steps 6/47: batch_recall = 38.53, batch_ndcg = 41.93 
2025-02-13 00:55:34.768425: Steps 7/47: batch_recall = 44.58, batch_ndcg = 45.92 
2025-02-13 00:55:35.933204: Steps 8/47: batch_recall = 47.85, batch_ndcg = 52.51 
2025-02-13 00:55:37.063416: Steps 9/47: batch_recall = 48.17, batch_ndcg = 47.35 
2025-02-13 00:55:38.205875: Steps 10/47: batch_recall = 43.38, batch_ndcg = 44.13 
2025-02-13 00:55:39.343134: Steps 11/47: batch_recall = 54.70, batch_ndcg = 52.63 
2025-02-13 00:55:40.484659: Steps 12/47: batch_recall = 49.73, batch_ndcg = 48.53 
2025-02-13 00:55:41.625882: Steps 13/47: batch_recall = 49.58, batch_ndcg = 45.73 
2025-02-13 00:55:42.724163: Steps 14/47: batch_recall = 41.40, batch_ndcg = 41.21 
2025-02-13 00:55:43.822771: Steps 15/47: batch_recall = 57.14, batch_ndcg = 54.49 
2025-02-13 00:55:44.914297: Steps 16/47: batch_recall = 52.33, batch_ndcg = 48.36 
2025-02-13 00:55:45.969664: Steps 17/47: batch_recall = 57.92, batch_ndcg = 49.51 
2025-02-13 00:55:47.055732: Steps 18/47: batch_recall = 52.35, batch_ndcg = 49.63 
2025-02-13 00:55:48.120911: Steps 19/47: batch_recall = 60.35, batch_ndcg = 54.83 
2025-02-13 00:55:49.168579: Steps 20/47: batch_recall = 67.61, batch_ndcg = 61.98 
2025-02-13 00:55:50.213916: Steps 21/47: batch_recall = 66.14, batch_ndcg = 56.16 
2025-02-13 00:55:51.251147: Steps 22/47: batch_recall = 55.55, batch_ndcg = 51.63 
2025-02-13 00:55:52.277199: Steps 23/47: batch_recall = 64.48, batch_ndcg = 55.81 
2025-02-13 00:55:53.328000: Steps 24/47: batch_recall = 64.81, batch_ndcg = 57.70 
2025-02-13 00:55:54.361644: Steps 25/47: batch_recall = 65.62, batch_ndcg = 57.19 
2025-02-13 00:55:55.368881: Steps 26/47: batch_recall = 60.30, batch_ndcg = 53.45 
2025-02-13 00:55:56.391771: Steps 27/47: batch_recall = 61.81, batch_ndcg = 52.62 
2025-02-13 00:55:57.404222: Steps 28/47: batch_recall = 71.58, batch_ndcg = 60.91 
2025-02-13 00:55:58.410068: Steps 29/47: batch_recall = 67.79, batch_ndcg = 58.07 
2025-02-13 00:55:59.417714: Steps 30/47: batch_recall = 74.13, batch_ndcg = 64.80 
2025-02-13 00:56:00.453678: Steps 31/47: batch_recall = 67.41, batch_ndcg = 56.26 
2025-02-13 00:56:01.461302: Steps 32/47: batch_recall = 71.84, batch_ndcg = 65.40 
2025-02-13 00:56:02.484524: Steps 33/47: batch_recall = 80.49, batch_ndcg = 68.48 
2025-02-13 00:56:03.500078: Steps 34/47: batch_recall = 67.52, batch_ndcg = 56.52 
2025-02-13 00:56:04.486509: Steps 35/47: batch_recall = 76.88, batch_ndcg = 65.71 
2025-02-13 00:56:05.479738: Steps 36/47: batch_recall = 77.43, batch_ndcg = 62.77 
2025-02-13 00:56:06.462067: Steps 37/47: batch_recall = 84.55, batch_ndcg = 72.96 
2025-02-13 00:56:07.442767: Steps 38/47: batch_recall = 89.89, batch_ndcg = 73.21 
2025-02-13 00:56:08.421102: Steps 39/47: batch_recall = 88.04, batch_ndcg = 69.86 
2025-02-13 00:56:09.389474: Steps 40/47: batch_recall = 71.94, batch_ndcg = 62.61 
2025-02-13 00:56:10.343478: Steps 41/47: batch_recall = 89.56, batch_ndcg = 73.36 
2025-02-13 00:56:11.299337: Steps 42/47: batch_recall = 81.73, batch_ndcg = 64.70 
2025-02-13 00:56:12.249245: Steps 43/47: batch_recall = 90.33, batch_ndcg = 73.36 
2025-02-13 00:56:13.194938: Steps 44/47: batch_recall = 88.75, batch_ndcg = 70.98 
2025-02-13 00:56:14.126980: Steps 45/47: batch_recall = 92.23, batch_ndcg = 74.63 
2025-02-13 00:56:14.229384: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 00:56:14.229516: Epoch 92/1000, Test: Recall = 0.1217, NDCG = 0.1098  

2025-02-13 00:56:15.401239: Training Step 0/59: batchLoss = 2.8895, diffLoss = 14.1124, kgLoss = 0.0837
2025-02-13 00:56:16.317025: Training Step 1/59: batchLoss = 2.8053, diffLoss = 13.6918, kgLoss = 0.0836
2025-02-13 00:56:17.240204: Training Step 2/59: batchLoss = 2.6782, diffLoss = 13.0681, kgLoss = 0.0807
2025-02-13 00:56:18.164818: Training Step 3/59: batchLoss = 2.3886, diffLoss = 11.6526, kgLoss = 0.0726
2025-02-13 00:56:19.091753: Training Step 4/59: batchLoss = 3.1308, diffLoss = 15.2440, kgLoss = 0.1025
2025-02-13 00:56:20.015357: Training Step 5/59: batchLoss = 2.8103, diffLoss = 13.7341, kgLoss = 0.0793
2025-02-13 00:56:20.942642: Training Step 6/59: batchLoss = 2.6225, diffLoss = 12.8085, kgLoss = 0.0760
2025-02-13 00:56:21.885534: Training Step 7/59: batchLoss = 2.7740, diffLoss = 13.5040, kgLoss = 0.0915
2025-02-13 00:56:22.823060: Training Step 8/59: batchLoss = 2.7878, diffLoss = 13.6022, kgLoss = 0.0842
2025-02-13 00:56:23.762474: Training Step 9/59: batchLoss = 2.9024, diffLoss = 14.1643, kgLoss = 0.0870
2025-02-13 00:56:24.700513: Training Step 10/59: batchLoss = 2.7787, diffLoss = 13.5705, kgLoss = 0.0807
2025-02-13 00:56:25.633699: Training Step 11/59: batchLoss = 2.7123, diffLoss = 13.2190, kgLoss = 0.0856
2025-02-13 00:56:26.564168: Training Step 12/59: batchLoss = 2.6505, diffLoss = 12.9440, kgLoss = 0.0772
2025-02-13 00:56:27.503534: Training Step 13/59: batchLoss = 2.7528, diffLoss = 13.4455, kgLoss = 0.0797
2025-02-13 00:56:28.434311: Training Step 14/59: batchLoss = 2.8721, diffLoss = 14.0266, kgLoss = 0.0834
2025-02-13 00:56:29.378160: Training Step 15/59: batchLoss = 2.7676, diffLoss = 13.5532, kgLoss = 0.0712
2025-02-13 00:56:30.312742: Training Step 16/59: batchLoss = 2.8800, diffLoss = 14.0470, kgLoss = 0.0882
2025-02-13 00:56:31.238967: Training Step 17/59: batchLoss = 3.1293, diffLoss = 15.2700, kgLoss = 0.0941
2025-02-13 00:56:32.172979: Training Step 18/59: batchLoss = 2.8700, diffLoss = 14.0017, kgLoss = 0.0871
2025-02-13 00:56:33.101333: Training Step 19/59: batchLoss = 2.5791, diffLoss = 12.5681, kgLoss = 0.0818
2025-02-13 00:56:34.029952: Training Step 20/59: batchLoss = 2.9656, diffLoss = 14.4738, kgLoss = 0.0886
2025-02-13 00:56:34.952434: Training Step 21/59: batchLoss = 2.8652, diffLoss = 13.9944, kgLoss = 0.0830
2025-02-13 00:56:35.888556: Training Step 22/59: batchLoss = 2.9341, diffLoss = 14.3674, kgLoss = 0.0758
2025-02-13 00:56:36.811235: Training Step 23/59: batchLoss = 3.2088, diffLoss = 15.6880, kgLoss = 0.0890
2025-02-13 00:56:37.732885: Training Step 24/59: batchLoss = 2.6879, diffLoss = 13.1416, kgLoss = 0.0745
2025-02-13 00:56:38.663233: Training Step 25/59: batchLoss = 2.9425, diffLoss = 14.3535, kgLoss = 0.0897
2025-02-13 00:56:39.580173: Training Step 26/59: batchLoss = 2.6036, diffLoss = 12.6929, kgLoss = 0.0813
2025-02-13 00:56:40.504616: Training Step 27/59: batchLoss = 2.8482, diffLoss = 13.9206, kgLoss = 0.0801
2025-02-13 00:56:41.422862: Training Step 28/59: batchLoss = 3.1925, diffLoss = 15.6121, kgLoss = 0.0877
2025-02-13 00:56:42.358778: Training Step 29/59: batchLoss = 2.9437, diffLoss = 14.3857, kgLoss = 0.0832
2025-02-13 00:56:43.292379: Training Step 30/59: batchLoss = 3.1223, diffLoss = 15.2331, kgLoss = 0.0946
2025-02-13 00:56:44.224223: Training Step 31/59: batchLoss = 2.9947, diffLoss = 14.6143, kgLoss = 0.0898
2025-02-13 00:56:45.166438: Training Step 32/59: batchLoss = 2.8739, diffLoss = 14.0554, kgLoss = 0.0785
2025-02-13 00:56:46.101882: Training Step 33/59: batchLoss = 2.9769, diffLoss = 14.5375, kgLoss = 0.0868
2025-02-13 00:56:47.048174: Training Step 34/59: batchLoss = 3.2631, diffLoss = 15.9342, kgLoss = 0.0953
2025-02-13 00:56:47.983818: Training Step 35/59: batchLoss = 3.0819, diffLoss = 15.0811, kgLoss = 0.0821
2025-02-13 00:56:48.920881: Training Step 36/59: batchLoss = 2.8440, diffLoss = 13.8931, kgLoss = 0.0817
2025-02-13 00:56:49.857905: Training Step 37/59: batchLoss = 3.1209, diffLoss = 15.2552, kgLoss = 0.0873
2025-02-13 00:56:50.797590: Training Step 38/59: batchLoss = 3.2755, diffLoss = 15.9921, kgLoss = 0.0964
2025-02-13 00:56:51.748478: Training Step 39/59: batchLoss = 2.9753, diffLoss = 14.5274, kgLoss = 0.0872
2025-02-13 00:56:52.678959: Training Step 40/59: batchLoss = 3.0271, diffLoss = 14.7543, kgLoss = 0.0953
2025-02-13 00:56:53.613599: Training Step 41/59: batchLoss = 3.4237, diffLoss = 16.7343, kgLoss = 0.0961
2025-02-13 00:56:54.542362: Training Step 42/59: batchLoss = 2.7986, diffLoss = 13.6946, kgLoss = 0.0746
2025-02-13 00:56:55.481912: Training Step 43/59: batchLoss = 3.1729, diffLoss = 15.5029, kgLoss = 0.0904
2025-02-13 00:56:56.407940: Training Step 44/59: batchLoss = 2.9198, diffLoss = 14.2798, kgLoss = 0.0798
2025-02-13 00:56:57.344363: Training Step 45/59: batchLoss = 2.6331, diffLoss = 12.8362, kgLoss = 0.0824
2025-02-13 00:56:58.276013: Training Step 46/59: batchLoss = 3.1921, diffLoss = 15.6049, kgLoss = 0.0889
2025-02-13 00:56:59.206675: Training Step 47/59: batchLoss = 3.3586, diffLoss = 16.4066, kgLoss = 0.0966
2025-02-13 00:57:00.150956: Training Step 48/59: batchLoss = 2.9878, diffLoss = 14.6227, kgLoss = 0.0790
2025-02-13 00:57:01.081572: Training Step 49/59: batchLoss = 3.2596, diffLoss = 15.9226, kgLoss = 0.0938
2025-02-13 00:57:02.004141: Training Step 50/59: batchLoss = 3.2253, diffLoss = 15.7683, kgLoss = 0.0895
2025-02-13 00:57:02.932158: Training Step 51/59: batchLoss = 2.9254, diffLoss = 14.2859, kgLoss = 0.0852
2025-02-13 00:57:03.855573: Training Step 52/59: batchLoss = 2.8348, diffLoss = 13.8705, kgLoss = 0.0759
2025-02-13 00:57:04.788749: Training Step 53/59: batchLoss = 2.7329, diffLoss = 13.3241, kgLoss = 0.0851
2025-02-13 00:57:05.728527: Training Step 54/59: batchLoss = 2.9406, diffLoss = 14.3806, kgLoss = 0.0806
2025-02-13 00:57:06.665150: Training Step 55/59: batchLoss = 3.1137, diffLoss = 15.1813, kgLoss = 0.0968
2025-02-13 00:57:07.594978: Training Step 56/59: batchLoss = 3.1870, diffLoss = 15.5470, kgLoss = 0.0970
2025-02-13 00:57:08.439142: Training Step 57/59: batchLoss = 2.9160, diffLoss = 14.2457, kgLoss = 0.0836
2025-02-13 00:57:09.288521: Training Step 58/59: batchLoss = 3.3697, diffLoss = 16.4829, kgLoss = 0.0914
2025-02-13 00:57:09.384945: 
2025-02-13 00:57:09.385597: Epoch 93/1000, Train: epLoss = 0.4338, epDfLoss = 2.1186, epKgLoss = 0.0126  
2025-02-13 00:57:10.870927: Steps 0/47: batch_recall = 37.08, batch_ndcg = 47.87 
2025-02-13 00:57:12.171798: Steps 1/47: batch_recall = 37.49, batch_ndcg = 43.56 
2025-02-13 00:57:13.427430: Steps 2/47: batch_recall = 40.50, batch_ndcg = 46.97 
2025-02-13 00:57:14.682115: Steps 3/47: batch_recall = 44.22, batch_ndcg = 46.77 
2025-02-13 00:57:15.884614: Steps 4/47: batch_recall = 39.50, batch_ndcg = 45.46 
2025-02-13 00:57:17.092164: Steps 5/47: batch_recall = 33.24, batch_ndcg = 39.90 
2025-02-13 00:57:18.303949: Steps 6/47: batch_recall = 38.56, batch_ndcg = 41.94 
2025-02-13 00:57:19.473426: Steps 7/47: batch_recall = 45.13, batch_ndcg = 46.49 
2025-02-13 00:57:20.635801: Steps 8/47: batch_recall = 47.50, batch_ndcg = 51.12 
2025-02-13 00:57:21.766072: Steps 9/47: batch_recall = 47.96, batch_ndcg = 46.75 
2025-02-13 00:57:22.937230: Steps 10/47: batch_recall = 43.92, batch_ndcg = 43.61 
2025-02-13 00:57:24.076618: Steps 11/47: batch_recall = 55.91, batch_ndcg = 53.16 
2025-02-13 00:57:25.230180: Steps 12/47: batch_recall = 49.01, batch_ndcg = 49.40 
2025-02-13 00:57:26.376687: Steps 13/47: batch_recall = 49.27, batch_ndcg = 45.73 
2025-02-13 00:57:27.488096: Steps 14/47: batch_recall = 41.13, batch_ndcg = 41.09 
2025-02-13 00:57:28.594970: Steps 15/47: batch_recall = 56.99, batch_ndcg = 54.33 
2025-02-13 00:57:29.692448: Steps 16/47: batch_recall = 51.83, batch_ndcg = 48.98 
2025-02-13 00:57:30.746177: Steps 17/47: batch_recall = 58.73, batch_ndcg = 50.36 
2025-02-13 00:57:31.833947: Steps 18/47: batch_recall = 52.79, batch_ndcg = 49.47 
2025-02-13 00:57:32.908231: Steps 19/47: batch_recall = 59.29, batch_ndcg = 53.84 
2025-02-13 00:57:33.959287: Steps 20/47: batch_recall = 67.21, batch_ndcg = 60.67 
2025-02-13 00:57:35.014559: Steps 21/47: batch_recall = 64.95, batch_ndcg = 56.86 
2025-02-13 00:57:36.070223: Steps 22/47: batch_recall = 56.93, batch_ndcg = 51.98 
2025-02-13 00:57:37.119062: Steps 23/47: batch_recall = 63.00, batch_ndcg = 55.92 
2025-02-13 00:57:38.176847: Steps 24/47: batch_recall = 64.17, batch_ndcg = 57.89 
2025-02-13 00:57:39.224619: Steps 25/47: batch_recall = 65.43, batch_ndcg = 57.01 
2025-02-13 00:57:40.249794: Steps 26/47: batch_recall = 59.81, batch_ndcg = 53.57 
2025-02-13 00:57:41.279590: Steps 27/47: batch_recall = 61.32, batch_ndcg = 52.80 
2025-02-13 00:57:42.299913: Steps 28/47: batch_recall = 71.34, batch_ndcg = 59.53 
2025-02-13 00:57:43.313821: Steps 29/47: batch_recall = 69.22, batch_ndcg = 58.32 
2025-02-13 00:57:44.324502: Steps 30/47: batch_recall = 72.74, batch_ndcg = 63.70 
2025-02-13 00:57:45.359270: Steps 31/47: batch_recall = 67.37, batch_ndcg = 56.82 
2025-02-13 00:57:46.382366: Steps 32/47: batch_recall = 70.48, batch_ndcg = 65.24 
2025-02-13 00:57:47.405055: Steps 33/47: batch_recall = 80.12, batch_ndcg = 69.01 
2025-02-13 00:57:48.431961: Steps 34/47: batch_recall = 69.79, batch_ndcg = 57.88 
2025-02-13 00:57:49.426341: Steps 35/47: batch_recall = 76.67, batch_ndcg = 64.80 
2025-02-13 00:57:50.435775: Steps 36/47: batch_recall = 78.16, batch_ndcg = 63.87 
2025-02-13 00:57:51.434162: Steps 37/47: batch_recall = 85.29, batch_ndcg = 73.32 
2025-02-13 00:57:52.424232: Steps 38/47: batch_recall = 91.13, batch_ndcg = 73.24 
2025-02-13 00:57:53.405273: Steps 39/47: batch_recall = 88.67, batch_ndcg = 69.22 
2025-02-13 00:57:54.381914: Steps 40/47: batch_recall = 73.06, batch_ndcg = 62.91 
2025-02-13 00:57:55.361045: Steps 41/47: batch_recall = 89.47, batch_ndcg = 72.96 
2025-02-13 00:57:56.321078: Steps 42/47: batch_recall = 81.83, batch_ndcg = 64.82 
2025-02-13 00:57:57.281558: Steps 43/47: batch_recall = 91.69, batch_ndcg = 73.20 
2025-02-13 00:57:58.261802: Steps 44/47: batch_recall = 86.23, batch_ndcg = 69.50 
2025-02-13 00:57:59.195098: Steps 45/47: batch_recall = 92.52, batch_ndcg = 75.12 
2025-02-13 00:57:59.299194: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.63 
2025-02-13 00:57:59.299296: Epoch 93/1000, Test: Recall = 0.1218, NDCG = 0.1098  

2025-02-13 00:58:00.487159: Training Step 0/59: batchLoss = 2.9273, diffLoss = 14.3168, kgLoss = 0.0799
2025-02-13 00:58:01.411734: Training Step 1/59: batchLoss = 3.0637, diffLoss = 14.9565, kgLoss = 0.0905
2025-02-13 00:58:02.331836: Training Step 2/59: batchLoss = 2.8144, diffLoss = 13.7381, kgLoss = 0.0835
2025-02-13 00:58:03.265017: Training Step 3/59: batchLoss = 2.7432, diffLoss = 13.3625, kgLoss = 0.0883
2025-02-13 00:58:04.190998: Training Step 4/59: batchLoss = 2.7025, diffLoss = 13.1638, kgLoss = 0.0872
2025-02-13 00:58:05.130873: Training Step 5/59: batchLoss = 2.8492, diffLoss = 13.8633, kgLoss = 0.0956
2025-02-13 00:58:06.056856: Training Step 6/59: batchLoss = 2.9953, diffLoss = 14.6125, kgLoss = 0.0910
2025-02-13 00:58:06.990824: Training Step 7/59: batchLoss = 2.8190, diffLoss = 13.7420, kgLoss = 0.0882
2025-02-13 00:58:07.925407: Training Step 8/59: batchLoss = 2.7853, diffLoss = 13.5837, kgLoss = 0.0857
2025-02-13 00:58:08.867536: Training Step 9/59: batchLoss = 2.8461, diffLoss = 13.8662, kgLoss = 0.0910
2025-02-13 00:58:09.800558: Training Step 10/59: batchLoss = 2.6676, diffLoss = 12.9873, kgLoss = 0.0877
2025-02-13 00:58:10.740781: Training Step 11/59: batchLoss = 2.7869, diffLoss = 13.5761, kgLoss = 0.0896
2025-02-13 00:58:11.675265: Training Step 12/59: batchLoss = 3.0669, diffLoss = 14.9599, kgLoss = 0.0937
2025-02-13 00:58:12.611054: Training Step 13/59: batchLoss = 2.9766, diffLoss = 14.5239, kgLoss = 0.0898
2025-02-13 00:58:13.550904: Training Step 14/59: batchLoss = 2.8475, diffLoss = 13.8925, kgLoss = 0.0863
2025-02-13 00:58:14.490162: Training Step 15/59: batchLoss = 2.7010, diffLoss = 13.1425, kgLoss = 0.0906
2025-02-13 00:58:15.429812: Training Step 16/59: batchLoss = 2.9061, diffLoss = 14.1718, kgLoss = 0.0897
2025-02-13 00:58:16.372183: Training Step 17/59: batchLoss = 2.7996, diffLoss = 13.6638, kgLoss = 0.0835
2025-02-13 00:58:17.296179: Training Step 18/59: batchLoss = 2.9277, diffLoss = 14.2961, kgLoss = 0.0856
2025-02-13 00:58:18.225846: Training Step 19/59: batchLoss = 3.0734, diffLoss = 14.9979, kgLoss = 0.0923
2025-02-13 00:58:19.155544: Training Step 20/59: batchLoss = 2.9534, diffLoss = 14.4375, kgLoss = 0.0823
2025-02-13 00:58:20.081590: Training Step 21/59: batchLoss = 2.7145, diffLoss = 13.2677, kgLoss = 0.0762
2025-02-13 00:58:21.011421: Training Step 22/59: batchLoss = 2.7354, diffLoss = 13.3820, kgLoss = 0.0738
2025-02-13 00:58:21.938384: Training Step 23/59: batchLoss = 2.9832, diffLoss = 14.5770, kgLoss = 0.0847
2025-02-13 00:58:22.873333: Training Step 24/59: batchLoss = 2.8709, diffLoss = 14.0480, kgLoss = 0.0766
2025-02-13 00:58:23.802154: Training Step 25/59: batchLoss = 3.0324, diffLoss = 14.8090, kgLoss = 0.0882
2025-02-13 00:58:24.726185: Training Step 26/59: batchLoss = 3.0228, diffLoss = 14.7581, kgLoss = 0.0890
2025-02-13 00:58:25.659244: Training Step 27/59: batchLoss = 2.9009, diffLoss = 14.1899, kgLoss = 0.0787
2025-02-13 00:58:26.584907: Training Step 28/59: batchLoss = 3.0473, diffLoss = 14.8909, kgLoss = 0.0864
2025-02-13 00:58:27.521040: Training Step 29/59: batchLoss = 2.9278, diffLoss = 14.2596, kgLoss = 0.0948
2025-02-13 00:58:28.457448: Training Step 30/59: batchLoss = 2.8731, diffLoss = 14.0622, kgLoss = 0.0758
2025-02-13 00:58:29.393522: Training Step 31/59: batchLoss = 3.2853, diffLoss = 16.0759, kgLoss = 0.0877
2025-02-13 00:58:30.327536: Training Step 32/59: batchLoss = 2.9628, diffLoss = 14.4428, kgLoss = 0.0929
2025-02-13 00:58:31.254603: Training Step 33/59: batchLoss = 2.8852, diffLoss = 14.0774, kgLoss = 0.0872
2025-02-13 00:58:32.193223: Training Step 34/59: batchLoss = 2.8453, diffLoss = 13.9160, kgLoss = 0.0776
2025-02-13 00:58:33.124784: Training Step 35/59: batchLoss = 2.5386, diffLoss = 12.4073, kgLoss = 0.0714
2025-02-13 00:58:34.061488: Training Step 36/59: batchLoss = 3.1285, diffLoss = 15.3220, kgLoss = 0.0801
2025-02-13 00:58:34.999108: Training Step 37/59: batchLoss = 2.8625, diffLoss = 13.9719, kgLoss = 0.0852
2025-02-13 00:58:35.938683: Training Step 38/59: batchLoss = 2.6338, diffLoss = 12.8728, kgLoss = 0.0740
2025-02-13 00:58:36.876403: Training Step 39/59: batchLoss = 3.0181, diffLoss = 14.7654, kgLoss = 0.0812
2025-02-13 00:58:37.813564: Training Step 40/59: batchLoss = 3.0761, diffLoss = 15.0132, kgLoss = 0.0918
2025-02-13 00:58:38.746261: Training Step 41/59: batchLoss = 2.7283, diffLoss = 13.3411, kgLoss = 0.0752
2025-02-13 00:58:39.670326: Training Step 42/59: batchLoss = 2.8775, diffLoss = 14.0742, kgLoss = 0.0783
2025-02-13 00:58:40.600223: Training Step 43/59: batchLoss = 3.0152, diffLoss = 14.6892, kgLoss = 0.0967
2025-02-13 00:58:41.519894: Training Step 44/59: batchLoss = 3.0166, diffLoss = 14.7248, kgLoss = 0.0896
2025-02-13 00:58:42.445237: Training Step 45/59: batchLoss = 3.1099, diffLoss = 15.2006, kgLoss = 0.0873
2025-02-13 00:58:43.371218: Training Step 46/59: batchLoss = 2.5218, diffLoss = 12.3335, kgLoss = 0.0689
2025-02-13 00:58:44.298459: Training Step 47/59: batchLoss = 2.7762, diffLoss = 13.5269, kgLoss = 0.0886
2025-02-13 00:58:45.232494: Training Step 48/59: batchLoss = 2.7294, diffLoss = 13.3505, kgLoss = 0.0741
2025-02-13 00:58:46.157983: Training Step 49/59: batchLoss = 3.2467, diffLoss = 15.8504, kgLoss = 0.0958
2025-02-13 00:58:47.089964: Training Step 50/59: batchLoss = 2.7157, diffLoss = 13.2776, kgLoss = 0.0752
2025-02-13 00:58:48.017966: Training Step 51/59: batchLoss = 3.2574, diffLoss = 15.9313, kgLoss = 0.0889
2025-02-13 00:58:48.951151: Training Step 52/59: batchLoss = 2.8289, diffLoss = 13.8069, kgLoss = 0.0844
2025-02-13 00:58:49.896988: Training Step 53/59: batchLoss = 3.1806, diffLoss = 15.5294, kgLoss = 0.0934
2025-02-13 00:58:50.831600: Training Step 54/59: batchLoss = 2.7986, diffLoss = 13.6839, kgLoss = 0.0773
2025-02-13 00:58:51.769425: Training Step 55/59: batchLoss = 3.3227, diffLoss = 16.2266, kgLoss = 0.0967
2025-02-13 00:58:52.703927: Training Step 56/59: batchLoss = 3.0065, diffLoss = 14.6838, kgLoss = 0.0871
2025-02-13 00:58:53.550450: Training Step 57/59: batchLoss = 2.8334, diffLoss = 13.8376, kgLoss = 0.0823
2025-02-13 00:58:54.407872: Training Step 58/59: batchLoss = 3.5342, diffLoss = 17.2644, kgLoss = 0.1016
2025-02-13 00:58:54.503421: 
2025-02-13 00:58:54.503975: Epoch 94/1000, Train: epLoss = 0.4302, epDfLoss = 2.1007, epKgLoss = 0.0126  
2025-02-13 00:58:55.986860: Steps 0/47: batch_recall = 38.33, batch_ndcg = 47.76 
2025-02-13 00:58:57.295326: Steps 1/47: batch_recall = 36.26, batch_ndcg = 42.42 
2025-02-13 00:58:58.554056: Steps 2/47: batch_recall = 40.53, batch_ndcg = 46.74 
2025-02-13 00:58:59.806233: Steps 3/47: batch_recall = 43.99, batch_ndcg = 46.76 
2025-02-13 00:59:01.002901: Steps 4/47: batch_recall = 39.81, batch_ndcg = 45.95 
2025-02-13 00:59:02.199241: Steps 5/47: batch_recall = 33.06, batch_ndcg = 40.27 
2025-02-13 00:59:03.391160: Steps 6/47: batch_recall = 38.13, batch_ndcg = 41.22 
2025-02-13 00:59:04.551136: Steps 7/47: batch_recall = 44.39, batch_ndcg = 46.60 
2025-02-13 00:59:05.719520: Steps 8/47: batch_recall = 48.09, batch_ndcg = 52.75 
2025-02-13 00:59:06.862508: Steps 9/47: batch_recall = 47.03, batch_ndcg = 46.87 
2025-02-13 00:59:08.023925: Steps 10/47: batch_recall = 43.77, batch_ndcg = 43.73 
2025-02-13 00:59:09.163423: Steps 11/47: batch_recall = 55.92, batch_ndcg = 53.28 
2025-02-13 00:59:10.306301: Steps 12/47: batch_recall = 49.20, batch_ndcg = 48.59 
2025-02-13 00:59:11.449356: Steps 13/47: batch_recall = 49.51, batch_ndcg = 45.90 
2025-02-13 00:59:12.555405: Steps 14/47: batch_recall = 40.92, batch_ndcg = 40.86 
2025-02-13 00:59:13.650255: Steps 15/47: batch_recall = 56.52, batch_ndcg = 54.18 
2025-02-13 00:59:14.746313: Steps 16/47: batch_recall = 51.96, batch_ndcg = 48.39 
2025-02-13 00:59:15.812264: Steps 17/47: batch_recall = 59.08, batch_ndcg = 50.41 
2025-02-13 00:59:16.900427: Steps 18/47: batch_recall = 53.78, batch_ndcg = 49.83 
2025-02-13 00:59:17.975816: Steps 19/47: batch_recall = 57.47, batch_ndcg = 52.96 
2025-02-13 00:59:19.044012: Steps 20/47: batch_recall = 68.43, batch_ndcg = 62.42 
2025-02-13 00:59:20.088364: Steps 21/47: batch_recall = 65.01, batch_ndcg = 55.90 
2025-02-13 00:59:21.137012: Steps 22/47: batch_recall = 55.87, batch_ndcg = 50.95 
2025-02-13 00:59:22.184532: Steps 23/47: batch_recall = 63.92, batch_ndcg = 56.19 
2025-02-13 00:59:23.241494: Steps 24/47: batch_recall = 66.44, batch_ndcg = 58.36 
2025-02-13 00:59:24.274784: Steps 25/47: batch_recall = 64.93, batch_ndcg = 56.80 
2025-02-13 00:59:25.293269: Steps 26/47: batch_recall = 60.31, batch_ndcg = 53.86 
2025-02-13 00:59:26.319752: Steps 27/47: batch_recall = 60.69, batch_ndcg = 52.61 
2025-02-13 00:59:27.335967: Steps 28/47: batch_recall = 70.76, batch_ndcg = 59.64 
2025-02-13 00:59:28.345777: Steps 29/47: batch_recall = 70.42, batch_ndcg = 58.45 
2025-02-13 00:59:29.352443: Steps 30/47: batch_recall = 73.21, batch_ndcg = 64.25 
2025-02-13 00:59:30.382278: Steps 31/47: batch_recall = 65.91, batch_ndcg = 55.80 
2025-02-13 00:59:31.389647: Steps 32/47: batch_recall = 71.38, batch_ndcg = 65.03 
2025-02-13 00:59:32.390980: Steps 33/47: batch_recall = 81.18, batch_ndcg = 69.09 
2025-02-13 00:59:33.393742: Steps 34/47: batch_recall = 70.16, batch_ndcg = 57.11 
2025-02-13 00:59:34.383876: Steps 35/47: batch_recall = 76.57, batch_ndcg = 64.52 
2025-02-13 00:59:35.372091: Steps 36/47: batch_recall = 77.47, batch_ndcg = 63.83 
2025-02-13 00:59:36.337506: Steps 37/47: batch_recall = 85.05, batch_ndcg = 73.28 
2025-02-13 00:59:37.307967: Steps 38/47: batch_recall = 89.20, batch_ndcg = 72.31 
2025-02-13 00:59:38.279845: Steps 39/47: batch_recall = 88.00, batch_ndcg = 68.54 
2025-02-13 00:59:39.247870: Steps 40/47: batch_recall = 73.41, batch_ndcg = 63.45 
2025-02-13 00:59:40.209632: Steps 41/47: batch_recall = 88.11, batch_ndcg = 72.50 
2025-02-13 00:59:41.169874: Steps 42/47: batch_recall = 83.20, batch_ndcg = 65.87 
2025-02-13 00:59:42.137727: Steps 43/47: batch_recall = 91.31, batch_ndcg = 73.28 
2025-02-13 00:59:43.115420: Steps 44/47: batch_recall = 87.19, batch_ndcg = 69.38 
2025-02-13 00:59:44.059491: Steps 45/47: batch_recall = 93.65, batch_ndcg = 74.22 
2025-02-13 00:59:44.163026: Steps 46/47: batch_recall = 1.75, batch_ndcg = 1.53 
2025-02-13 00:59:44.163166: Epoch 94/1000, Test: Recall = 0.1218, NDCG = 0.1097  

2025-02-13 00:59:45.356206: Training Step 0/59: batchLoss = 2.7178, diffLoss = 13.2523, kgLoss = 0.0841
2025-02-13 00:59:46.292157: Training Step 1/59: batchLoss = 3.0844, diffLoss = 15.0530, kgLoss = 0.0922
2025-02-13 00:59:47.229904: Training Step 2/59: batchLoss = 2.8924, diffLoss = 14.1106, kgLoss = 0.0878
2025-02-13 00:59:48.164520: Training Step 3/59: batchLoss = 2.6608, diffLoss = 12.9431, kgLoss = 0.0903
2025-02-13 00:59:49.100312: Training Step 4/59: batchLoss = 3.0735, diffLoss = 14.9930, kgLoss = 0.0937
2025-02-13 00:59:50.036413: Training Step 5/59: batchLoss = 2.6833, diffLoss = 13.0999, kgLoss = 0.0792
2025-02-13 00:59:50.977636: Training Step 6/59: batchLoss = 2.6534, diffLoss = 12.9062, kgLoss = 0.0902
2025-02-13 00:59:51.907573: Training Step 7/59: batchLoss = 2.6777, diffLoss = 13.0693, kgLoss = 0.0798
2025-02-13 00:59:52.828752: Training Step 8/59: batchLoss = 3.1967, diffLoss = 15.6012, kgLoss = 0.0955
2025-02-13 00:59:53.761660: Training Step 9/59: batchLoss = 3.0794, diffLoss = 15.0196, kgLoss = 0.0944
2025-02-13 00:59:54.688566: Training Step 10/59: batchLoss = 2.8197, diffLoss = 13.7795, kgLoss = 0.0798
2025-02-13 00:59:55.615591: Training Step 11/59: batchLoss = 2.9761, diffLoss = 14.5391, kgLoss = 0.0854
2025-02-13 00:59:56.543536: Training Step 12/59: batchLoss = 2.9377, diffLoss = 14.3544, kgLoss = 0.0835
2025-02-13 00:59:57.471176: Training Step 13/59: batchLoss = 2.7881, diffLoss = 13.6154, kgLoss = 0.0812
2025-02-13 00:59:58.401550: Training Step 14/59: batchLoss = 3.1183, diffLoss = 15.2461, kgLoss = 0.0863
2025-02-13 00:59:59.327104: Training Step 15/59: batchLoss = 2.7429, diffLoss = 13.3945, kgLoss = 0.0799
2025-02-13 01:00:00.264061: Training Step 16/59: batchLoss = 2.6709, diffLoss = 13.0366, kgLoss = 0.0794
2025-02-13 01:00:01.183402: Training Step 17/59: batchLoss = 2.8831, diffLoss = 14.0491, kgLoss = 0.0915
2025-02-13 01:00:02.107696: Training Step 18/59: batchLoss = 3.2968, diffLoss = 16.1175, kgLoss = 0.0917
2025-02-13 01:00:03.034331: Training Step 19/59: batchLoss = 2.7824, diffLoss = 13.5651, kgLoss = 0.0867
2025-02-13 01:00:03.972428: Training Step 20/59: batchLoss = 2.8741, diffLoss = 14.0252, kgLoss = 0.0864
2025-02-13 01:00:04.907991: Training Step 21/59: batchLoss = 2.8898, diffLoss = 14.1154, kgLoss = 0.0834
2025-02-13 01:00:05.839707: Training Step 22/59: batchLoss = 3.0607, diffLoss = 14.9436, kgLoss = 0.0900
2025-02-13 01:00:06.775712: Training Step 23/59: batchLoss = 3.1314, diffLoss = 15.3056, kgLoss = 0.0879
2025-02-13 01:00:07.717664: Training Step 24/59: batchLoss = 2.8615, diffLoss = 13.9658, kgLoss = 0.0855
2025-02-13 01:00:08.662156: Training Step 25/59: batchLoss = 2.9707, diffLoss = 14.5093, kgLoss = 0.0861
2025-02-13 01:00:09.596802: Training Step 26/59: batchLoss = 2.7765, diffLoss = 13.5619, kgLoss = 0.0801
2025-02-13 01:00:10.543558: Training Step 27/59: batchLoss = 2.9800, diffLoss = 14.5503, kgLoss = 0.0875
2025-02-13 01:00:11.480336: Training Step 28/59: batchLoss = 3.0824, diffLoss = 15.0583, kgLoss = 0.0884
2025-02-13 01:00:12.410281: Training Step 29/59: batchLoss = 2.9479, diffLoss = 14.4314, kgLoss = 0.0770
2025-02-13 01:00:13.335914: Training Step 30/59: batchLoss = 3.3248, diffLoss = 16.2377, kgLoss = 0.0966
2025-02-13 01:00:14.279064: Training Step 31/59: batchLoss = 2.7722, diffLoss = 13.5308, kgLoss = 0.0825
2025-02-13 01:00:15.208347: Training Step 32/59: batchLoss = 2.8601, diffLoss = 13.9847, kgLoss = 0.0790
2025-02-13 01:00:16.150883: Training Step 33/59: batchLoss = 3.0902, diffLoss = 15.1039, kgLoss = 0.0868
2025-02-13 01:00:17.085733: Training Step 34/59: batchLoss = 2.8621, diffLoss = 13.9667, kgLoss = 0.0859
2025-02-13 01:00:18.010830: Training Step 35/59: batchLoss = 3.1069, diffLoss = 15.1786, kgLoss = 0.0889
2025-02-13 01:00:18.941499: Training Step 36/59: batchLoss = 3.0537, diffLoss = 14.9094, kgLoss = 0.0898
2025-02-13 01:00:19.877730: Training Step 37/59: batchLoss = 2.8939, diffLoss = 14.1610, kgLoss = 0.0771
2025-02-13 01:00:20.800225: Training Step 38/59: batchLoss = 2.8806, diffLoss = 14.0677, kgLoss = 0.0838
2025-02-13 01:00:21.739739: Training Step 39/59: batchLoss = 2.6263, diffLoss = 12.7947, kgLoss = 0.0842
2025-02-13 01:00:22.668958: Training Step 40/59: batchLoss = 3.0720, diffLoss = 15.0098, kgLoss = 0.0875
2025-02-13 01:00:23.601178: Training Step 41/59: batchLoss = 3.0887, diffLoss = 15.1050, kgLoss = 0.0847
2025-02-13 01:00:24.543291: Training Step 42/59: batchLoss = 3.0287, diffLoss = 14.8124, kgLoss = 0.0827
2025-02-13 01:00:25.478571: Training Step 43/59: batchLoss = 2.8793, diffLoss = 14.0970, kgLoss = 0.0749
2025-02-13 01:00:26.407563: Training Step 44/59: batchLoss = 2.9271, diffLoss = 14.2875, kgLoss = 0.0870
2025-02-13 01:00:27.337534: Training Step 45/59: batchLoss = 2.7331, diffLoss = 13.3444, kgLoss = 0.0803
2025-02-13 01:00:28.267661: Training Step 46/59: batchLoss = 3.0855, diffLoss = 15.0754, kgLoss = 0.0881
2025-02-13 01:00:29.203144: Training Step 47/59: batchLoss = 2.5760, diffLoss = 12.5838, kgLoss = 0.0741
2025-02-13 01:00:30.131888: Training Step 48/59: batchLoss = 3.0700, diffLoss = 15.0034, kgLoss = 0.0866
2025-02-13 01:00:31.064855: Training Step 49/59: batchLoss = 2.8736, diffLoss = 14.0417, kgLoss = 0.0816
2025-02-13 01:00:32.002923: Training Step 50/59: batchLoss = 3.0482, diffLoss = 14.8914, kgLoss = 0.0873
2025-02-13 01:00:32.933668: Training Step 51/59: batchLoss = 3.0040, diffLoss = 14.6355, kgLoss = 0.0961
2025-02-13 01:00:33.876909: Training Step 52/59: batchLoss = 3.0883, diffLoss = 15.1198, kgLoss = 0.0805
2025-02-13 01:00:34.805486: Training Step 53/59: batchLoss = 3.1630, diffLoss = 15.4475, kgLoss = 0.0919
2025-02-13 01:00:35.737382: Training Step 54/59: batchLoss = 2.9177, diffLoss = 14.2639, kgLoss = 0.0811
2025-02-13 01:00:36.665094: Training Step 55/59: batchLoss = 3.0271, diffLoss = 14.7679, kgLoss = 0.0919
2025-02-13 01:00:37.592885: Training Step 56/59: batchLoss = 2.7531, diffLoss = 13.4618, kgLoss = 0.0759
2025-02-13 01:00:38.429983: Training Step 57/59: batchLoss = 3.3357, diffLoss = 16.3148, kgLoss = 0.0909
2025-02-13 01:00:39.280103: Training Step 58/59: batchLoss = 2.9829, diffLoss = 14.5898, kgLoss = 0.0812
2025-02-13 01:00:39.370204: 
2025-02-13 01:00:39.370717: Epoch 95/1000, Train: epLoss = 0.4336, epDfLoss = 2.1175, epKgLoss = 0.0126  
2025-02-13 01:00:40.834874: Steps 0/47: batch_recall = 36.81, batch_ndcg = 47.03 
2025-02-13 01:00:42.126665: Steps 1/47: batch_recall = 36.77, batch_ndcg = 42.59 
2025-02-13 01:00:43.375855: Steps 2/47: batch_recall = 40.60, batch_ndcg = 46.64 
2025-02-13 01:00:44.636464: Steps 3/47: batch_recall = 43.52, batch_ndcg = 46.51 
2025-02-13 01:00:45.842255: Steps 4/47: batch_recall = 39.59, batch_ndcg = 45.88 
2025-02-13 01:00:47.074357: Steps 5/47: batch_recall = 32.97, batch_ndcg = 39.52 
2025-02-13 01:00:48.279443: Steps 6/47: batch_recall = 38.60, batch_ndcg = 41.96 
2025-02-13 01:00:49.452511: Steps 7/47: batch_recall = 43.87, batch_ndcg = 45.98 
2025-02-13 01:00:50.634194: Steps 8/47: batch_recall = 48.16, batch_ndcg = 52.04 
2025-02-13 01:00:51.789340: Steps 9/47: batch_recall = 47.57, batch_ndcg = 47.30 
2025-02-13 01:00:52.966599: Steps 10/47: batch_recall = 43.13, batch_ndcg = 42.78 
2025-02-13 01:00:54.112079: Steps 11/47: batch_recall = 57.23, batch_ndcg = 53.51 
2025-02-13 01:00:55.257478: Steps 12/47: batch_recall = 49.05, batch_ndcg = 47.76 
2025-02-13 01:00:56.384958: Steps 13/47: batch_recall = 48.65, batch_ndcg = 45.21 
2025-02-13 01:00:57.471843: Steps 14/47: batch_recall = 41.37, batch_ndcg = 41.00 
2025-02-13 01:00:58.568463: Steps 15/47: batch_recall = 56.95, batch_ndcg = 54.07 
2025-02-13 01:00:59.645730: Steps 16/47: batch_recall = 52.08, batch_ndcg = 48.09 
2025-02-13 01:01:00.688842: Steps 17/47: batch_recall = 58.62, batch_ndcg = 50.18 
2025-02-13 01:01:01.771499: Steps 18/47: batch_recall = 53.33, batch_ndcg = 50.42 
2025-02-13 01:01:02.831229: Steps 19/47: batch_recall = 59.24, batch_ndcg = 53.99 
2025-02-13 01:01:03.872773: Steps 20/47: batch_recall = 69.49, batch_ndcg = 62.19 
2025-02-13 01:01:04.922378: Steps 21/47: batch_recall = 63.94, batch_ndcg = 55.19 
2025-02-13 01:01:05.982002: Steps 22/47: batch_recall = 57.45, batch_ndcg = 52.32 
2025-02-13 01:01:07.042130: Steps 23/47: batch_recall = 63.58, batch_ndcg = 55.56 
2025-02-13 01:01:08.113866: Steps 24/47: batch_recall = 66.62, batch_ndcg = 58.78 
2025-02-13 01:01:09.176595: Steps 25/47: batch_recall = 64.56, batch_ndcg = 57.16 
2025-02-13 01:01:10.201438: Steps 26/47: batch_recall = 61.71, batch_ndcg = 54.37 
2025-02-13 01:01:11.255368: Steps 27/47: batch_recall = 62.00, batch_ndcg = 52.92 
2025-02-13 01:01:12.289460: Steps 28/47: batch_recall = 71.45, batch_ndcg = 59.85 
2025-02-13 01:01:13.312449: Steps 29/47: batch_recall = 71.36, batch_ndcg = 59.33 
2025-02-13 01:01:14.327609: Steps 30/47: batch_recall = 73.18, batch_ndcg = 64.47 
2025-02-13 01:01:15.360178: Steps 31/47: batch_recall = 66.51, batch_ndcg = 56.12 
2025-02-13 01:01:16.373812: Steps 32/47: batch_recall = 72.11, batch_ndcg = 65.80 
2025-02-13 01:01:17.388146: Steps 33/47: batch_recall = 80.63, batch_ndcg = 68.28 
2025-02-13 01:01:18.395179: Steps 34/47: batch_recall = 68.77, batch_ndcg = 57.11 
2025-02-13 01:01:19.380999: Steps 35/47: batch_recall = 77.60, batch_ndcg = 65.11 
2025-02-13 01:01:20.362208: Steps 36/47: batch_recall = 78.94, batch_ndcg = 64.28 
2025-02-13 01:01:21.321821: Steps 37/47: batch_recall = 85.18, batch_ndcg = 73.16 
2025-02-13 01:01:22.295191: Steps 38/47: batch_recall = 89.97, batch_ndcg = 72.47 
2025-02-13 01:01:23.258003: Steps 39/47: batch_recall = 88.55, batch_ndcg = 68.68 
2025-02-13 01:01:24.213445: Steps 40/47: batch_recall = 73.64, batch_ndcg = 62.93 
2025-02-13 01:01:25.168448: Steps 41/47: batch_recall = 87.09, batch_ndcg = 72.57 
2025-02-13 01:01:26.130854: Steps 42/47: batch_recall = 82.09, batch_ndcg = 65.66 
2025-02-13 01:01:27.092866: Steps 43/47: batch_recall = 91.22, batch_ndcg = 73.14 
2025-02-13 01:01:28.065775: Steps 44/47: batch_recall = 87.03, batch_ndcg = 69.65 
2025-02-13 01:01:29.008460: Steps 45/47: batch_recall = 93.26, batch_ndcg = 74.56 
2025-02-13 01:01:29.114716: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.82 
2025-02-13 01:01:29.114852: Epoch 95/1000, Test: Recall = 0.1221, NDCG = 0.1097  

2025-02-13 01:01:30.318201: Training Step 0/59: batchLoss = 2.7997, diffLoss = 13.6691, kgLoss = 0.0824
2025-02-13 01:01:31.250395: Training Step 1/59: batchLoss = 2.8614, diffLoss = 13.9553, kgLoss = 0.0879
2025-02-13 01:01:32.192722: Training Step 2/59: batchLoss = 2.6128, diffLoss = 12.7523, kgLoss = 0.0779
2025-02-13 01:01:33.129449: Training Step 3/59: batchLoss = 2.4541, diffLoss = 11.9543, kgLoss = 0.0790
2025-02-13 01:01:34.071551: Training Step 4/59: batchLoss = 2.8628, diffLoss = 13.9762, kgLoss = 0.0844
2025-02-13 01:01:35.001992: Training Step 5/59: batchLoss = 2.9935, diffLoss = 14.6332, kgLoss = 0.0836
2025-02-13 01:01:35.923604: Training Step 6/59: batchLoss = 3.0401, diffLoss = 14.8419, kgLoss = 0.0897
2025-02-13 01:01:36.857907: Training Step 7/59: batchLoss = 2.7875, diffLoss = 13.5508, kgLoss = 0.0966
2025-02-13 01:01:37.785937: Training Step 8/59: batchLoss = 3.0247, diffLoss = 14.7744, kgLoss = 0.0873
2025-02-13 01:01:38.716663: Training Step 9/59: batchLoss = 2.6193, diffLoss = 12.8009, kgLoss = 0.0738
2025-02-13 01:01:39.647320: Training Step 10/59: batchLoss = 2.9269, diffLoss = 14.3094, kgLoss = 0.0812
2025-02-13 01:01:40.570227: Training Step 11/59: batchLoss = 3.2485, diffLoss = 15.8413, kgLoss = 0.1003
2025-02-13 01:01:41.496959: Training Step 12/59: batchLoss = 2.8088, diffLoss = 13.7289, kgLoss = 0.0787
2025-02-13 01:01:42.423726: Training Step 13/59: batchLoss = 2.8093, diffLoss = 13.7207, kgLoss = 0.0815
2025-02-13 01:01:43.358295: Training Step 14/59: batchLoss = 2.9217, diffLoss = 14.2922, kgLoss = 0.0791
2025-02-13 01:01:44.278414: Training Step 15/59: batchLoss = 2.9118, diffLoss = 14.1903, kgLoss = 0.0921
2025-02-13 01:01:45.201538: Training Step 16/59: batchLoss = 2.7697, diffLoss = 13.4980, kgLoss = 0.0876
2025-02-13 01:01:46.121090: Training Step 17/59: batchLoss = 2.9017, diffLoss = 14.1561, kgLoss = 0.0881
2025-02-13 01:01:47.053515: Training Step 18/59: batchLoss = 3.0347, diffLoss = 14.8237, kgLoss = 0.0875
2025-02-13 01:01:47.992024: Training Step 19/59: batchLoss = 3.2002, diffLoss = 15.6571, kgLoss = 0.0860
2025-02-13 01:01:48.931815: Training Step 20/59: batchLoss = 2.9844, diffLoss = 14.5486, kgLoss = 0.0934
2025-02-13 01:01:49.867106: Training Step 21/59: batchLoss = 2.7218, diffLoss = 13.2841, kgLoss = 0.0812
2025-02-13 01:01:50.797124: Training Step 22/59: batchLoss = 2.9277, diffLoss = 14.3122, kgLoss = 0.0816
2025-02-13 01:01:51.731695: Training Step 23/59: batchLoss = 2.6821, diffLoss = 13.1131, kgLoss = 0.0743
2025-02-13 01:01:52.667066: Training Step 24/59: batchLoss = 2.8311, diffLoss = 13.8425, kgLoss = 0.0783
2025-02-13 01:01:53.612743: Training Step 25/59: batchLoss = 2.9177, diffLoss = 14.2731, kgLoss = 0.0789
2025-02-13 01:01:54.546775: Training Step 26/59: batchLoss = 2.8033, diffLoss = 13.6856, kgLoss = 0.0827
2025-02-13 01:01:55.483590: Training Step 27/59: batchLoss = 3.3055, diffLoss = 16.1600, kgLoss = 0.0919
2025-02-13 01:01:56.429980: Training Step 28/59: batchLoss = 2.9634, diffLoss = 14.4785, kgLoss = 0.0846
2025-02-13 01:01:57.355065: Training Step 29/59: batchLoss = 3.0090, diffLoss = 14.6877, kgLoss = 0.0893
2025-02-13 01:01:58.282012: Training Step 30/59: batchLoss = 3.0763, diffLoss = 15.0396, kgLoss = 0.0855
2025-02-13 01:01:59.205766: Training Step 31/59: batchLoss = 2.9357, diffLoss = 14.3373, kgLoss = 0.0853
2025-02-13 01:02:00.125564: Training Step 32/59: batchLoss = 3.1107, diffLoss = 15.1833, kgLoss = 0.0925
2025-02-13 01:02:01.054122: Training Step 33/59: batchLoss = 2.8954, diffLoss = 14.1329, kgLoss = 0.0860
2025-02-13 01:02:01.980188: Training Step 34/59: batchLoss = 2.7482, diffLoss = 13.4239, kgLoss = 0.0793
2025-02-13 01:02:02.905537: Training Step 35/59: batchLoss = 3.0965, diffLoss = 15.1174, kgLoss = 0.0913
2025-02-13 01:02:03.837387: Training Step 36/59: batchLoss = 2.9341, diffLoss = 14.3356, kgLoss = 0.0837
2025-02-13 01:02:04.761142: Training Step 37/59: batchLoss = 2.9281, diffLoss = 14.2543, kgLoss = 0.0965
2025-02-13 01:02:05.700749: Training Step 38/59: batchLoss = 2.9755, diffLoss = 14.5436, kgLoss = 0.0835
2025-02-13 01:02:06.630926: Training Step 39/59: batchLoss = 2.8483, diffLoss = 13.8724, kgLoss = 0.0923
2025-02-13 01:02:07.559899: Training Step 40/59: batchLoss = 2.9046, diffLoss = 14.1947, kgLoss = 0.0821
2025-02-13 01:02:08.489106: Training Step 41/59: batchLoss = 2.9188, diffLoss = 14.2835, kgLoss = 0.0776
2025-02-13 01:02:09.424475: Training Step 42/59: batchLoss = 3.3523, diffLoss = 16.3788, kgLoss = 0.0957
2025-02-13 01:02:10.368694: Training Step 43/59: batchLoss = 3.0115, diffLoss = 14.7075, kgLoss = 0.0875
2025-02-13 01:02:11.300967: Training Step 44/59: batchLoss = 3.0890, diffLoss = 15.0997, kgLoss = 0.0863
2025-02-13 01:02:12.234763: Training Step 45/59: batchLoss = 3.5574, diffLoss = 17.4102, kgLoss = 0.0942
2025-02-13 01:02:13.176163: Training Step 46/59: batchLoss = 3.1876, diffLoss = 15.5790, kgLoss = 0.0898
2025-02-13 01:02:14.106709: Training Step 47/59: batchLoss = 3.1345, diffLoss = 15.2941, kgLoss = 0.0946
2025-02-13 01:02:15.033800: Training Step 48/59: batchLoss = 3.1605, diffLoss = 15.4532, kgLoss = 0.0873
2025-02-13 01:02:15.973688: Training Step 49/59: batchLoss = 2.8194, diffLoss = 13.7655, kgLoss = 0.0828
2025-02-13 01:02:16.911725: Training Step 50/59: batchLoss = 3.0416, diffLoss = 14.8629, kgLoss = 0.0863
2025-02-13 01:02:17.838674: Training Step 51/59: batchLoss = 2.8664, diffLoss = 13.9831, kgLoss = 0.0872
2025-02-13 01:02:18.785820: Training Step 52/59: batchLoss = 2.8389, diffLoss = 13.8417, kgLoss = 0.0883
2025-02-13 01:02:19.709904: Training Step 53/59: batchLoss = 3.1407, diffLoss = 15.3644, kgLoss = 0.0847
2025-02-13 01:02:20.641867: Training Step 54/59: batchLoss = 3.2620, diffLoss = 15.9275, kgLoss = 0.0956
2025-02-13 01:02:21.569644: Training Step 55/59: batchLoss = 2.8236, diffLoss = 13.8216, kgLoss = 0.0742
2025-02-13 01:02:22.500908: Training Step 56/59: batchLoss = 2.8888, diffLoss = 14.1265, kgLoss = 0.0794
2025-02-13 01:02:23.343287: Training Step 57/59: batchLoss = 2.6617, diffLoss = 12.9915, kgLoss = 0.0793
2025-02-13 01:02:24.192532: Training Step 58/59: batchLoss = 2.9003, diffLoss = 14.2116, kgLoss = 0.0724
2025-02-13 01:02:24.285561: 
2025-02-13 01:02:24.286305: Epoch 96/1000, Train: epLoss = 0.4346, epDfLoss = 2.1226, epKgLoss = 0.0126  
2025-02-13 01:02:25.763785: Steps 0/47: batch_recall = 36.84, batch_ndcg = 47.23 
2025-02-13 01:02:27.047419: Steps 1/47: batch_recall = 37.66, batch_ndcg = 43.01 
2025-02-13 01:02:28.294787: Steps 2/47: batch_recall = 40.35, batch_ndcg = 46.32 
2025-02-13 01:02:29.549870: Steps 3/47: batch_recall = 44.30, batch_ndcg = 46.78 
2025-02-13 01:02:30.764308: Steps 4/47: batch_recall = 40.21, batch_ndcg = 46.74 
2025-02-13 01:02:31.981931: Steps 5/47: batch_recall = 33.95, batch_ndcg = 40.38 
2025-02-13 01:02:33.186286: Steps 6/47: batch_recall = 39.02, batch_ndcg = 42.72 
2025-02-13 01:02:34.370082: Steps 7/47: batch_recall = 44.68, batch_ndcg = 46.63 
2025-02-13 01:02:35.559419: Steps 8/47: batch_recall = 48.29, batch_ndcg = 52.44 
2025-02-13 01:02:36.694059: Steps 9/47: batch_recall = 46.71, batch_ndcg = 46.16 
2025-02-13 01:02:37.858335: Steps 10/47: batch_recall = 43.55, batch_ndcg = 43.48 
2025-02-13 01:02:38.991761: Steps 11/47: batch_recall = 55.70, batch_ndcg = 52.86 
2025-02-13 01:02:40.137630: Steps 12/47: batch_recall = 49.79, batch_ndcg = 48.45 
2025-02-13 01:02:41.268909: Steps 13/47: batch_recall = 48.44, batch_ndcg = 45.50 
2025-02-13 01:02:42.354320: Steps 14/47: batch_recall = 40.99, batch_ndcg = 41.91 
2025-02-13 01:02:43.433986: Steps 15/47: batch_recall = 56.71, batch_ndcg = 53.46 
2025-02-13 01:02:44.510817: Steps 16/47: batch_recall = 52.40, batch_ndcg = 48.15 
2025-02-13 01:02:45.555626: Steps 17/47: batch_recall = 57.25, batch_ndcg = 49.03 
2025-02-13 01:02:46.626999: Steps 18/47: batch_recall = 53.36, batch_ndcg = 49.95 
2025-02-13 01:02:47.696676: Steps 19/47: batch_recall = 60.69, batch_ndcg = 54.56 
2025-02-13 01:02:48.724879: Steps 20/47: batch_recall = 67.15, batch_ndcg = 61.49 
2025-02-13 01:02:49.778914: Steps 21/47: batch_recall = 65.69, batch_ndcg = 55.44 
2025-02-13 01:02:50.832790: Steps 22/47: batch_recall = 57.00, batch_ndcg = 51.69 
2025-02-13 01:02:51.887850: Steps 23/47: batch_recall = 64.67, batch_ndcg = 56.68 
2025-02-13 01:02:52.973223: Steps 24/47: batch_recall = 65.65, batch_ndcg = 58.94 
2025-02-13 01:02:54.024372: Steps 25/47: batch_recall = 65.62, batch_ndcg = 57.75 
2025-02-13 01:02:55.055725: Steps 26/47: batch_recall = 60.07, batch_ndcg = 53.03 
2025-02-13 01:02:56.101727: Steps 27/47: batch_recall = 62.02, batch_ndcg = 52.76 
2025-02-13 01:02:57.135491: Steps 28/47: batch_recall = 70.45, batch_ndcg = 59.82 
2025-02-13 01:02:58.166912: Steps 29/47: batch_recall = 70.21, batch_ndcg = 59.18 
2025-02-13 01:02:59.186012: Steps 30/47: batch_recall = 73.76, batch_ndcg = 64.90 
2025-02-13 01:03:00.212634: Steps 31/47: batch_recall = 67.00, batch_ndcg = 56.00 
2025-02-13 01:03:01.211559: Steps 32/47: batch_recall = 71.11, batch_ndcg = 65.70 
2025-02-13 01:03:02.219252: Steps 33/47: batch_recall = 79.66, batch_ndcg = 67.15 
2025-02-13 01:03:03.228522: Steps 34/47: batch_recall = 68.84, batch_ndcg = 57.55 
2025-02-13 01:03:04.197744: Steps 35/47: batch_recall = 78.11, batch_ndcg = 65.61 
2025-02-13 01:03:05.186275: Steps 36/47: batch_recall = 78.61, batch_ndcg = 64.53 
2025-02-13 01:03:06.151722: Steps 37/47: batch_recall = 83.83, batch_ndcg = 72.58 
2025-02-13 01:03:07.129576: Steps 38/47: batch_recall = 89.37, batch_ndcg = 73.36 
2025-02-13 01:03:08.093753: Steps 39/47: batch_recall = 88.69, batch_ndcg = 69.79 
2025-02-13 01:03:09.068342: Steps 40/47: batch_recall = 72.31, batch_ndcg = 63.45 
2025-02-13 01:03:10.027169: Steps 41/47: batch_recall = 86.40, batch_ndcg = 72.65 
2025-02-13 01:03:10.983923: Steps 42/47: batch_recall = 83.70, batch_ndcg = 66.46 
2025-02-13 01:03:11.955144: Steps 43/47: batch_recall = 90.60, batch_ndcg = 72.89 
2025-02-13 01:03:12.928650: Steps 44/47: batch_recall = 88.47, batch_ndcg = 70.33 
2025-02-13 01:03:13.875965: Steps 45/47: batch_recall = 94.04, batch_ndcg = 74.30 
2025-02-13 01:03:13.981345: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.68 
2025-02-13 01:03:13.981475: Epoch 96/1000, Test: Recall = 0.1220, NDCG = 0.1100  

2025-02-13 01:03:15.186886: Training Step 0/59: batchLoss = 2.5814, diffLoss = 12.5845, kgLoss = 0.0806
2025-02-13 01:03:16.126228: Training Step 1/59: batchLoss = 2.8334, diffLoss = 13.8167, kgLoss = 0.0876
2025-02-13 01:03:17.063140: Training Step 2/59: batchLoss = 2.8971, diffLoss = 14.1346, kgLoss = 0.0878
2025-02-13 01:03:18.004920: Training Step 3/59: batchLoss = 2.8595, diffLoss = 13.9563, kgLoss = 0.0854
2025-02-13 01:03:18.941841: Training Step 4/59: batchLoss = 2.7849, diffLoss = 13.5990, kgLoss = 0.0814
2025-02-13 01:03:19.875268: Training Step 5/59: batchLoss = 2.6895, diffLoss = 13.1325, kgLoss = 0.0788
2025-02-13 01:03:20.802885: Training Step 6/59: batchLoss = 2.7647, diffLoss = 13.4709, kgLoss = 0.0881
2025-02-13 01:03:21.733128: Training Step 7/59: batchLoss = 2.8336, diffLoss = 13.8185, kgLoss = 0.0874
2025-02-13 01:03:22.663155: Training Step 8/59: batchLoss = 2.6069, diffLoss = 12.7032, kgLoss = 0.0828
2025-02-13 01:03:23.590836: Training Step 9/59: batchLoss = 2.9122, diffLoss = 14.2046, kgLoss = 0.0891
2025-02-13 01:03:24.521274: Training Step 10/59: batchLoss = 2.9802, diffLoss = 14.5443, kgLoss = 0.0891
2025-02-13 01:03:25.452813: Training Step 11/59: batchLoss = 2.8018, diffLoss = 13.6594, kgLoss = 0.0874
2025-02-13 01:03:26.388402: Training Step 12/59: batchLoss = 3.2930, diffLoss = 16.0861, kgLoss = 0.0948
2025-02-13 01:03:27.319279: Training Step 13/59: batchLoss = 2.7430, diffLoss = 13.3830, kgLoss = 0.0830
2025-02-13 01:03:28.249908: Training Step 14/59: batchLoss = 2.7780, diffLoss = 13.5302, kgLoss = 0.0900
2025-02-13 01:03:29.177401: Training Step 15/59: batchLoss = 2.9690, diffLoss = 14.4729, kgLoss = 0.0930
2025-02-13 01:03:30.108799: Training Step 16/59: batchLoss = 2.9628, diffLoss = 14.4224, kgLoss = 0.0978
2025-02-13 01:03:31.037754: Training Step 17/59: batchLoss = 2.9668, diffLoss = 14.4537, kgLoss = 0.0951
2025-02-13 01:03:31.969326: Training Step 18/59: batchLoss = 2.8732, diffLoss = 14.0350, kgLoss = 0.0828
2025-02-13 01:03:32.898196: Training Step 19/59: batchLoss = 2.8523, diffLoss = 13.8885, kgLoss = 0.0932
2025-02-13 01:03:33.838191: Training Step 20/59: batchLoss = 2.6964, diffLoss = 13.1557, kgLoss = 0.0816
2025-02-13 01:03:34.778858: Training Step 21/59: batchLoss = 2.8386, diffLoss = 13.8414, kgLoss = 0.0879
2025-02-13 01:03:35.714918: Training Step 22/59: batchLoss = 2.7263, diffLoss = 13.3262, kgLoss = 0.0763
2025-02-13 01:03:36.656131: Training Step 23/59: batchLoss = 2.9146, diffLoss = 14.2505, kgLoss = 0.0807
2025-02-13 01:03:37.578157: Training Step 24/59: batchLoss = 2.9026, diffLoss = 14.1965, kgLoss = 0.0792
2025-02-13 01:03:38.518429: Training Step 25/59: batchLoss = 3.2913, diffLoss = 16.0692, kgLoss = 0.0968
2025-02-13 01:03:39.450351: Training Step 26/59: batchLoss = 3.0853, diffLoss = 15.0763, kgLoss = 0.0876
2025-02-13 01:03:40.383138: Training Step 27/59: batchLoss = 3.0943, diffLoss = 15.1358, kgLoss = 0.0839
2025-02-13 01:03:41.316031: Training Step 28/59: batchLoss = 2.6685, diffLoss = 13.0058, kgLoss = 0.0842
2025-02-13 01:03:42.241162: Training Step 29/59: batchLoss = 2.9910, diffLoss = 14.6071, kgLoss = 0.0870
2025-02-13 01:03:43.153932: Training Step 30/59: batchLoss = 2.8218, diffLoss = 13.7761, kgLoss = 0.0832
2025-02-13 01:03:44.083267: Training Step 31/59: batchLoss = 2.7508, diffLoss = 13.4508, kgLoss = 0.0758
2025-02-13 01:03:45.010080: Training Step 32/59: batchLoss = 2.6619, diffLoss = 12.9990, kgLoss = 0.0777
2025-02-13 01:03:45.935523: Training Step 33/59: batchLoss = 3.4086, diffLoss = 16.6749, kgLoss = 0.0920
2025-02-13 01:03:46.856406: Training Step 34/59: batchLoss = 2.9171, diffLoss = 14.2569, kgLoss = 0.0822
2025-02-13 01:03:47.782961: Training Step 35/59: batchLoss = 2.8996, diffLoss = 14.1572, kgLoss = 0.0852
2025-02-13 01:03:48.701872: Training Step 36/59: batchLoss = 2.8146, diffLoss = 13.7561, kgLoss = 0.0792
2025-02-13 01:03:49.638608: Training Step 37/59: batchLoss = 2.8306, diffLoss = 13.8400, kgLoss = 0.0782
2025-02-13 01:03:50.563174: Training Step 38/59: batchLoss = 3.0246, diffLoss = 14.7525, kgLoss = 0.0926
2025-02-13 01:03:51.500569: Training Step 39/59: batchLoss = 2.8210, diffLoss = 13.7763, kgLoss = 0.0821
2025-02-13 01:03:52.434336: Training Step 40/59: batchLoss = 2.8869, diffLoss = 14.1045, kgLoss = 0.0824
2025-02-13 01:03:53.356663: Training Step 41/59: batchLoss = 3.3545, diffLoss = 16.3898, kgLoss = 0.0957
2025-02-13 01:03:54.292635: Training Step 42/59: batchLoss = 3.1391, diffLoss = 15.3430, kgLoss = 0.0881
2025-02-13 01:03:55.230379: Training Step 43/59: batchLoss = 2.8952, diffLoss = 14.1494, kgLoss = 0.0817
2025-02-13 01:03:56.164175: Training Step 44/59: batchLoss = 2.8975, diffLoss = 14.1438, kgLoss = 0.0859
2025-02-13 01:03:57.102106: Training Step 45/59: batchLoss = 2.9610, diffLoss = 14.4544, kgLoss = 0.0877
2025-02-13 01:03:58.044469: Training Step 46/59: batchLoss = 2.5005, diffLoss = 12.2174, kgLoss = 0.0713
2025-02-13 01:03:58.989593: Training Step 47/59: batchLoss = 3.2757, diffLoss = 16.0142, kgLoss = 0.0911
2025-02-13 01:03:59.922457: Training Step 48/59: batchLoss = 2.9690, diffLoss = 14.4996, kgLoss = 0.0864
2025-02-13 01:04:00.863803: Training Step 49/59: batchLoss = 2.9383, diffLoss = 14.3623, kgLoss = 0.0823
2025-02-13 01:04:01.804595: Training Step 50/59: batchLoss = 3.1401, diffLoss = 15.3587, kgLoss = 0.0855
2025-02-13 01:04:02.746643: Training Step 51/59: batchLoss = 3.3011, diffLoss = 16.1428, kgLoss = 0.0907
2025-02-13 01:04:03.676176: Training Step 52/59: batchLoss = 2.8151, diffLoss = 13.7539, kgLoss = 0.0805
2025-02-13 01:04:04.600181: Training Step 53/59: batchLoss = 3.3033, diffLoss = 16.1482, kgLoss = 0.0921
2025-02-13 01:04:05.538865: Training Step 54/59: batchLoss = 2.7534, diffLoss = 13.4539, kgLoss = 0.0783
2025-02-13 01:04:06.466684: Training Step 55/59: batchLoss = 3.0640, diffLoss = 14.9916, kgLoss = 0.0821
2025-02-13 01:04:07.403462: Training Step 56/59: batchLoss = 2.8644, diffLoss = 13.9792, kgLoss = 0.0857
2025-02-13 01:04:08.245825: Training Step 57/59: batchLoss = 2.9897, diffLoss = 14.6281, kgLoss = 0.0801
2025-02-13 01:04:09.100636: Training Step 58/59: batchLoss = 3.2321, diffLoss = 15.8018, kgLoss = 0.0896
2025-02-13 01:04:09.199172: 
2025-02-13 01:04:09.199728: Epoch 97/1000, Train: epLoss = 0.4311, epDfLoss = 2.1048, epKgLoss = 0.0126  
2025-02-13 01:04:10.660418: Steps 0/47: batch_recall = 36.54, batch_ndcg = 47.06 
2025-02-13 01:04:11.951014: Steps 1/47: batch_recall = 37.59, batch_ndcg = 42.43 
2025-02-13 01:04:13.201884: Steps 2/47: batch_recall = 40.80, batch_ndcg = 46.72 
2025-02-13 01:04:14.458569: Steps 3/47: batch_recall = 44.58, batch_ndcg = 46.42 
2025-02-13 01:04:15.654592: Steps 4/47: batch_recall = 39.32, batch_ndcg = 45.16 
2025-02-13 01:04:16.872640: Steps 5/47: batch_recall = 33.69, batch_ndcg = 40.42 
2025-02-13 01:04:18.073178: Steps 6/47: batch_recall = 38.91, batch_ndcg = 41.98 
2025-02-13 01:04:19.247944: Steps 7/47: batch_recall = 42.57, batch_ndcg = 45.82 
2025-02-13 01:04:20.437508: Steps 8/47: batch_recall = 47.92, batch_ndcg = 53.01 
2025-02-13 01:04:21.587361: Steps 9/47: batch_recall = 45.34, batch_ndcg = 45.78 
2025-02-13 01:04:22.755715: Steps 10/47: batch_recall = 44.39, batch_ndcg = 43.58 
2025-02-13 01:04:23.905863: Steps 11/47: batch_recall = 56.00, batch_ndcg = 52.82 
2025-02-13 01:04:25.051355: Steps 12/47: batch_recall = 50.06, batch_ndcg = 49.00 
2025-02-13 01:04:26.181241: Steps 13/47: batch_recall = 48.64, batch_ndcg = 45.04 
2025-02-13 01:04:27.283075: Steps 14/47: batch_recall = 40.86, batch_ndcg = 41.21 
2025-02-13 01:04:28.383634: Steps 15/47: batch_recall = 57.18, batch_ndcg = 53.32 
2025-02-13 01:04:29.465792: Steps 16/47: batch_recall = 51.69, batch_ndcg = 46.94 
2025-02-13 01:04:30.520444: Steps 17/47: batch_recall = 57.34, batch_ndcg = 49.22 
2025-02-13 01:04:31.600745: Steps 18/47: batch_recall = 52.73, batch_ndcg = 49.36 
2025-02-13 01:04:32.658675: Steps 19/47: batch_recall = 58.45, batch_ndcg = 54.32 
2025-02-13 01:04:33.712077: Steps 20/47: batch_recall = 68.26, batch_ndcg = 62.20 
2025-02-13 01:04:34.755567: Steps 21/47: batch_recall = 66.03, batch_ndcg = 55.93 
2025-02-13 01:04:35.819945: Steps 22/47: batch_recall = 55.52, batch_ndcg = 50.87 
2025-02-13 01:04:36.876164: Steps 23/47: batch_recall = 63.72, batch_ndcg = 54.68 
2025-02-13 01:04:37.945038: Steps 24/47: batch_recall = 64.94, batch_ndcg = 59.37 
2025-02-13 01:04:38.986342: Steps 25/47: batch_recall = 65.69, batch_ndcg = 57.33 
2025-02-13 01:04:40.008799: Steps 26/47: batch_recall = 60.55, batch_ndcg = 53.18 
2025-02-13 01:04:41.052949: Steps 27/47: batch_recall = 63.39, batch_ndcg = 53.17 
2025-02-13 01:04:42.079860: Steps 28/47: batch_recall = 69.95, batch_ndcg = 59.04 
2025-02-13 01:04:43.103411: Steps 29/47: batch_recall = 70.36, batch_ndcg = 58.44 
2025-02-13 01:04:44.128403: Steps 30/47: batch_recall = 72.97, batch_ndcg = 63.96 
2025-02-13 01:04:45.161209: Steps 31/47: batch_recall = 66.55, batch_ndcg = 56.26 
2025-02-13 01:04:46.176815: Steps 32/47: batch_recall = 72.38, batch_ndcg = 65.64 
2025-02-13 01:04:47.183815: Steps 33/47: batch_recall = 79.83, batch_ndcg = 68.09 
2025-02-13 01:04:48.199357: Steps 34/47: batch_recall = 68.69, batch_ndcg = 56.16 
2025-02-13 01:04:49.176643: Steps 35/47: batch_recall = 76.75, batch_ndcg = 65.40 
2025-02-13 01:04:50.157720: Steps 36/47: batch_recall = 78.08, batch_ndcg = 64.56 
2025-02-13 01:04:51.126666: Steps 37/47: batch_recall = 84.16, batch_ndcg = 71.69 
2025-02-13 01:04:52.098309: Steps 38/47: batch_recall = 90.56, batch_ndcg = 73.50 
2025-02-13 01:04:53.068124: Steps 39/47: batch_recall = 86.82, batch_ndcg = 68.51 
2025-02-13 01:04:54.032224: Steps 40/47: batch_recall = 73.45, batch_ndcg = 62.16 
2025-02-13 01:04:54.987294: Steps 41/47: batch_recall = 87.54, batch_ndcg = 73.23 
2025-02-13 01:04:55.945321: Steps 42/47: batch_recall = 83.09, batch_ndcg = 65.73 
2025-02-13 01:04:56.915301: Steps 43/47: batch_recall = 92.04, batch_ndcg = 72.96 
2025-02-13 01:04:57.893962: Steps 44/47: batch_recall = 89.60, batch_ndcg = 70.59 
2025-02-13 01:04:58.840346: Steps 45/47: batch_recall = 93.23, batch_ndcg = 74.74 
2025-02-13 01:04:58.944731: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.83 
2025-02-13 01:04:58.944862: Epoch 97/1000, Test: Recall = 0.1218, NDCG = 0.1094  

2025-02-13 01:05:00.151740: Training Step 0/59: batchLoss = 3.2450, diffLoss = 15.8252, kgLoss = 0.0999
2025-02-13 01:05:01.091975: Training Step 1/59: batchLoss = 3.0963, diffLoss = 15.1122, kgLoss = 0.0923
2025-02-13 01:05:02.028316: Training Step 2/59: batchLoss = 2.7887, diffLoss = 13.6223, kgLoss = 0.0803
2025-02-13 01:05:02.959855: Training Step 3/59: batchLoss = 2.8952, diffLoss = 14.1317, kgLoss = 0.0861
2025-02-13 01:05:03.894210: Training Step 4/59: batchLoss = 2.7988, diffLoss = 13.6493, kgLoss = 0.0862
2025-02-13 01:05:04.832171: Training Step 5/59: batchLoss = 2.6793, diffLoss = 13.0999, kgLoss = 0.0742
2025-02-13 01:05:05.770958: Training Step 6/59: batchLoss = 2.8377, diffLoss = 13.8497, kgLoss = 0.0847
2025-02-13 01:05:06.701397: Training Step 7/59: batchLoss = 2.9110, diffLoss = 14.1977, kgLoss = 0.0893
2025-02-13 01:05:07.637339: Training Step 8/59: batchLoss = 2.8975, diffLoss = 14.1506, kgLoss = 0.0842
2025-02-13 01:05:08.563327: Training Step 9/59: batchLoss = 2.8716, diffLoss = 13.9800, kgLoss = 0.0945
2025-02-13 01:05:09.489569: Training Step 10/59: batchLoss = 2.8683, diffLoss = 13.9962, kgLoss = 0.0864
2025-02-13 01:05:10.423203: Training Step 11/59: batchLoss = 3.2481, diffLoss = 15.8973, kgLoss = 0.0858
2025-02-13 01:05:11.355388: Training Step 12/59: batchLoss = 2.9789, diffLoss = 14.5588, kgLoss = 0.0839
2025-02-13 01:05:12.286154: Training Step 13/59: batchLoss = 3.0167, diffLoss = 14.7444, kgLoss = 0.0847
2025-02-13 01:05:13.205840: Training Step 14/59: batchLoss = 3.0201, diffLoss = 14.7390, kgLoss = 0.0904
2025-02-13 01:05:14.132575: Training Step 15/59: batchLoss = 2.8825, diffLoss = 14.0650, kgLoss = 0.0869
2025-02-13 01:05:15.057835: Training Step 16/59: batchLoss = 3.0363, diffLoss = 14.8419, kgLoss = 0.0849
2025-02-13 01:05:15.980391: Training Step 17/59: batchLoss = 2.7673, diffLoss = 13.5191, kgLoss = 0.0793
2025-02-13 01:05:16.906098: Training Step 18/59: batchLoss = 2.6254, diffLoss = 12.8186, kgLoss = 0.0770
2025-02-13 01:05:17.837503: Training Step 19/59: batchLoss = 2.7241, diffLoss = 13.2653, kgLoss = 0.0889
2025-02-13 01:05:18.777902: Training Step 20/59: batchLoss = 2.9891, diffLoss = 14.5772, kgLoss = 0.0921
2025-02-13 01:05:19.720764: Training Step 21/59: batchLoss = 3.1344, diffLoss = 15.3178, kgLoss = 0.0885
2025-02-13 01:05:20.669676: Training Step 22/59: batchLoss = 3.0108, diffLoss = 14.7001, kgLoss = 0.0885
2025-02-13 01:05:21.608471: Training Step 23/59: batchLoss = 3.1620, diffLoss = 15.3990, kgLoss = 0.1028
2025-02-13 01:05:22.540781: Training Step 24/59: batchLoss = 2.9999, diffLoss = 14.6645, kgLoss = 0.0838
2025-02-13 01:05:23.475396: Training Step 25/59: batchLoss = 2.8555, diffLoss = 13.9330, kgLoss = 0.0861
2025-02-13 01:05:24.410822: Training Step 26/59: batchLoss = 3.0378, diffLoss = 14.8589, kgLoss = 0.0825
2025-02-13 01:05:25.344908: Training Step 27/59: batchLoss = 2.7711, diffLoss = 13.5538, kgLoss = 0.0755
2025-02-13 01:05:26.284134: Training Step 28/59: batchLoss = 2.8553, diffLoss = 13.9224, kgLoss = 0.0885
2025-02-13 01:05:27.217116: Training Step 29/59: batchLoss = 2.8215, diffLoss = 13.7899, kgLoss = 0.0794
2025-02-13 01:05:28.146144: Training Step 30/59: batchLoss = 2.9366, diffLoss = 14.3364, kgLoss = 0.0866
2025-02-13 01:05:29.066797: Training Step 31/59: batchLoss = 2.7150, diffLoss = 13.2577, kgLoss = 0.0793
2025-02-13 01:05:30.001934: Training Step 32/59: batchLoss = 2.9176, diffLoss = 14.2392, kgLoss = 0.0872
2025-02-13 01:05:30.930858: Training Step 33/59: batchLoss = 3.1209, diffLoss = 15.2481, kgLoss = 0.0891
2025-02-13 01:05:31.873849: Training Step 34/59: batchLoss = 2.7750, diffLoss = 13.5828, kgLoss = 0.0731
2025-02-13 01:05:32.805115: Training Step 35/59: batchLoss = 3.0736, diffLoss = 15.0291, kgLoss = 0.0847
2025-02-13 01:05:33.729960: Training Step 36/59: batchLoss = 2.5763, diffLoss = 12.6012, kgLoss = 0.0700
2025-02-13 01:05:34.660801: Training Step 37/59: batchLoss = 3.2738, diffLoss = 16.0008, kgLoss = 0.0920
2025-02-13 01:05:35.580911: Training Step 38/59: batchLoss = 2.8602, diffLoss = 13.9337, kgLoss = 0.0918
2025-02-13 01:05:36.512000: Training Step 39/59: batchLoss = 2.8678, diffLoss = 14.0126, kgLoss = 0.0816
2025-02-13 01:05:37.440935: Training Step 40/59: batchLoss = 3.1748, diffLoss = 15.4923, kgLoss = 0.0955
2025-02-13 01:05:38.377809: Training Step 41/59: batchLoss = 3.1524, diffLoss = 15.3860, kgLoss = 0.0941
2025-02-13 01:05:39.312058: Training Step 42/59: batchLoss = 2.7818, diffLoss = 13.5809, kgLoss = 0.0821
2025-02-13 01:05:40.273233: Training Step 43/59: batchLoss = 2.8673, diffLoss = 14.0214, kgLoss = 0.0788
2025-02-13 01:05:41.210431: Training Step 44/59: batchLoss = 3.0951, diffLoss = 15.1364, kgLoss = 0.0848
2025-02-13 01:05:42.144894: Training Step 45/59: batchLoss = 2.8261, diffLoss = 13.8160, kgLoss = 0.0787
2025-02-13 01:05:43.079211: Training Step 46/59: batchLoss = 3.1410, diffLoss = 15.3779, kgLoss = 0.0818
2025-02-13 01:05:44.016052: Training Step 47/59: batchLoss = 2.8028, diffLoss = 13.7064, kgLoss = 0.0769
2025-02-13 01:05:44.952676: Training Step 48/59: batchLoss = 3.1767, diffLoss = 15.5470, kgLoss = 0.0842
2025-02-13 01:05:45.885689: Training Step 49/59: batchLoss = 3.1099, diffLoss = 15.2037, kgLoss = 0.0865
2025-02-13 01:05:46.821912: Training Step 50/59: batchLoss = 3.1700, diffLoss = 15.4832, kgLoss = 0.0917
2025-02-13 01:05:47.757599: Training Step 51/59: batchLoss = 2.9313, diffLoss = 14.3116, kgLoss = 0.0862
2025-02-13 01:05:48.692033: Training Step 52/59: batchLoss = 2.7765, diffLoss = 13.5773, kgLoss = 0.0763
2025-02-13 01:05:49.622193: Training Step 53/59: batchLoss = 3.0123, diffLoss = 14.7108, kgLoss = 0.0877
2025-02-13 01:05:50.553766: Training Step 54/59: batchLoss = 2.8862, diffLoss = 14.0906, kgLoss = 0.0851
2025-02-13 01:05:51.470205: Training Step 55/59: batchLoss = 3.0457, diffLoss = 14.9004, kgLoss = 0.0821
2025-02-13 01:05:52.404636: Training Step 56/59: batchLoss = 3.3872, diffLoss = 16.5405, kgLoss = 0.0989
2025-02-13 01:05:53.243172: Training Step 57/59: batchLoss = 3.2324, diffLoss = 15.8094, kgLoss = 0.0881
2025-02-13 01:05:54.097207: Training Step 58/59: batchLoss = 3.1503, diffLoss = 15.4234, kgLoss = 0.0821
2025-02-13 01:05:54.194928: 
2025-02-13 01:05:54.195463: Epoch 98/1000, Train: epLoss = 0.4367, epDfLoss = 2.1328, epKgLoss = 0.0126  
2025-02-13 01:05:55.669599: Steps 0/47: batch_recall = 36.78, batch_ndcg = 46.96 
2025-02-13 01:05:56.973354: Steps 1/47: batch_recall = 36.93, batch_ndcg = 42.47 
2025-02-13 01:05:58.219644: Steps 2/47: batch_recall = 40.86, batch_ndcg = 47.18 
2025-02-13 01:05:59.491589: Steps 3/47: batch_recall = 44.65, batch_ndcg = 47.20 
2025-02-13 01:06:00.696173: Steps 4/47: batch_recall = 39.88, batch_ndcg = 46.34 
2025-02-13 01:06:01.917009: Steps 5/47: batch_recall = 33.57, batch_ndcg = 40.21 
2025-02-13 01:06:03.134774: Steps 6/47: batch_recall = 40.09, batch_ndcg = 42.45 
2025-02-13 01:06:04.325323: Steps 7/47: batch_recall = 44.39, batch_ndcg = 46.46 
2025-02-13 01:06:05.514692: Steps 8/47: batch_recall = 48.34, batch_ndcg = 53.25 
2025-02-13 01:06:06.660243: Steps 9/47: batch_recall = 47.22, batch_ndcg = 46.68 
2025-02-13 01:06:07.834824: Steps 10/47: batch_recall = 44.15, batch_ndcg = 43.92 
2025-02-13 01:06:08.978546: Steps 11/47: batch_recall = 56.18, batch_ndcg = 53.20 
2025-02-13 01:06:10.131045: Steps 12/47: batch_recall = 50.95, batch_ndcg = 49.36 
2025-02-13 01:06:11.260575: Steps 13/47: batch_recall = 48.69, batch_ndcg = 45.05 
2025-02-13 01:06:12.358137: Steps 14/47: batch_recall = 40.19, batch_ndcg = 40.66 
2025-02-13 01:06:13.447395: Steps 15/47: batch_recall = 56.83, batch_ndcg = 53.39 
2025-02-13 01:06:14.525658: Steps 16/47: batch_recall = 50.91, batch_ndcg = 47.54 
2025-02-13 01:06:15.565479: Steps 17/47: batch_recall = 57.43, batch_ndcg = 49.50 
2025-02-13 01:06:16.647355: Steps 18/47: batch_recall = 53.39, batch_ndcg = 49.81 
2025-02-13 01:06:17.723574: Steps 19/47: batch_recall = 59.95, batch_ndcg = 55.58 
2025-02-13 01:06:18.788019: Steps 20/47: batch_recall = 69.29, batch_ndcg = 61.93 
2025-02-13 01:06:19.839300: Steps 21/47: batch_recall = 65.66, batch_ndcg = 56.22 
2025-02-13 01:06:20.898262: Steps 22/47: batch_recall = 55.82, batch_ndcg = 51.50 
2025-02-13 01:06:21.959716: Steps 23/47: batch_recall = 62.87, batch_ndcg = 54.98 
2025-02-13 01:06:23.031825: Steps 24/47: batch_recall = 65.35, batch_ndcg = 58.10 
2025-02-13 01:06:24.083588: Steps 25/47: batch_recall = 65.03, batch_ndcg = 57.72 
2025-02-13 01:06:25.127346: Steps 26/47: batch_recall = 62.24, batch_ndcg = 54.07 
2025-02-13 01:06:26.178041: Steps 27/47: batch_recall = 62.17, batch_ndcg = 53.22 
2025-02-13 01:06:27.213081: Steps 28/47: batch_recall = 70.53, batch_ndcg = 58.99 
2025-02-13 01:06:28.248885: Steps 29/47: batch_recall = 70.75, batch_ndcg = 58.91 
2025-02-13 01:06:29.278142: Steps 30/47: batch_recall = 73.69, batch_ndcg = 64.29 
2025-02-13 01:06:30.312198: Steps 31/47: batch_recall = 67.46, batch_ndcg = 56.60 
2025-02-13 01:06:31.325312: Steps 32/47: batch_recall = 72.04, batch_ndcg = 66.39 
2025-02-13 01:06:32.342062: Steps 33/47: batch_recall = 78.05, batch_ndcg = 66.84 
2025-02-13 01:06:33.347072: Steps 34/47: batch_recall = 68.31, batch_ndcg = 56.50 
2025-02-13 01:06:34.330080: Steps 35/47: batch_recall = 77.86, batch_ndcg = 64.69 
2025-02-13 01:06:35.309369: Steps 36/47: batch_recall = 77.48, batch_ndcg = 63.89 
2025-02-13 01:06:36.282757: Steps 37/47: batch_recall = 83.61, batch_ndcg = 72.33 
2025-02-13 01:06:37.258727: Steps 38/47: batch_recall = 89.21, batch_ndcg = 72.86 
2025-02-13 01:06:38.245840: Steps 39/47: batch_recall = 88.15, batch_ndcg = 69.83 
2025-02-13 01:06:39.210580: Steps 40/47: batch_recall = 74.74, batch_ndcg = 64.05 
2025-02-13 01:06:40.174502: Steps 41/47: batch_recall = 89.08, batch_ndcg = 73.00 
2025-02-13 01:06:41.142359: Steps 42/47: batch_recall = 82.10, batch_ndcg = 65.45 
2025-02-13 01:06:42.106589: Steps 43/47: batch_recall = 89.93, batch_ndcg = 72.32 
2025-02-13 01:06:43.081819: Steps 44/47: batch_recall = 89.80, batch_ndcg = 70.69 
2025-02-13 01:06:44.032004: Steps 45/47: batch_recall = 94.55, batch_ndcg = 75.42 
2025-02-13 01:06:44.140025: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.85 
2025-02-13 01:06:44.140142: Epoch 98/1000, Test: Recall = 0.1222, NDCG = 0.1099  

2025-02-13 01:06:45.340945: Training Step 0/59: batchLoss = 3.0646, diffLoss = 14.9538, kgLoss = 0.0923
2025-02-13 01:06:46.275402: Training Step 1/59: batchLoss = 3.0581, diffLoss = 14.9106, kgLoss = 0.0950
2025-02-13 01:06:47.211562: Training Step 2/59: batchLoss = 2.8629, diffLoss = 13.9901, kgLoss = 0.0811
2025-02-13 01:06:48.147544: Training Step 3/59: batchLoss = 2.7710, diffLoss = 13.5382, kgLoss = 0.0792
2025-02-13 01:06:49.073588: Training Step 4/59: batchLoss = 2.8725, diffLoss = 14.0429, kgLoss = 0.0800
2025-02-13 01:06:50.019786: Training Step 5/59: batchLoss = 2.9360, diffLoss = 14.3224, kgLoss = 0.0893
2025-02-13 01:06:50.961552: Training Step 6/59: batchLoss = 3.0735, diffLoss = 14.9867, kgLoss = 0.0952
2025-02-13 01:06:51.893922: Training Step 7/59: batchLoss = 2.7284, diffLoss = 13.3208, kgLoss = 0.0804
2025-02-13 01:06:52.832997: Training Step 8/59: batchLoss = 3.0365, diffLoss = 14.8210, kgLoss = 0.0904
2025-02-13 01:06:53.760304: Training Step 9/59: batchLoss = 2.7802, diffLoss = 13.5773, kgLoss = 0.0809
2025-02-13 01:06:54.689286: Training Step 10/59: batchLoss = 2.6535, diffLoss = 12.9458, kgLoss = 0.0805
2025-02-13 01:06:55.605980: Training Step 11/59: batchLoss = 3.0889, diffLoss = 15.0801, kgLoss = 0.0911
2025-02-13 01:06:56.531050: Training Step 12/59: batchLoss = 3.1493, diffLoss = 15.3836, kgLoss = 0.0907
2025-02-13 01:06:57.460231: Training Step 13/59: batchLoss = 2.5903, diffLoss = 12.6348, kgLoss = 0.0791
2025-02-13 01:06:58.382646: Training Step 14/59: batchLoss = 2.7037, diffLoss = 13.1949, kgLoss = 0.0810
2025-02-13 01:06:59.309299: Training Step 15/59: batchLoss = 2.7431, diffLoss = 13.3820, kgLoss = 0.0834
2025-02-13 01:07:00.227899: Training Step 16/59: batchLoss = 2.7877, diffLoss = 13.6056, kgLoss = 0.0833
2025-02-13 01:07:01.156389: Training Step 17/59: batchLoss = 3.1795, diffLoss = 15.5110, kgLoss = 0.0966
2025-02-13 01:07:02.080432: Training Step 18/59: batchLoss = 2.9389, diffLoss = 14.3605, kgLoss = 0.0835
2025-02-13 01:07:03.010295: Training Step 19/59: batchLoss = 2.8333, diffLoss = 13.8205, kgLoss = 0.0865
2025-02-13 01:07:03.938112: Training Step 20/59: batchLoss = 3.0506, diffLoss = 14.9135, kgLoss = 0.0848
2025-02-13 01:07:04.876518: Training Step 21/59: batchLoss = 3.1537, diffLoss = 15.3939, kgLoss = 0.0936
2025-02-13 01:07:05.814650: Training Step 22/59: batchLoss = 2.7375, diffLoss = 13.3440, kgLoss = 0.0859
2025-02-13 01:07:06.753179: Training Step 23/59: batchLoss = 3.0757, diffLoss = 15.0236, kgLoss = 0.0887
2025-02-13 01:07:07.695079: Training Step 24/59: batchLoss = 3.0016, diffLoss = 14.6434, kgLoss = 0.0912
2025-02-13 01:07:08.637643: Training Step 25/59: batchLoss = 2.8531, diffLoss = 13.9437, kgLoss = 0.0804
2025-02-13 01:07:09.567102: Training Step 26/59: batchLoss = 2.8871, diffLoss = 14.0895, kgLoss = 0.0865
2025-02-13 01:07:10.504652: Training Step 27/59: batchLoss = 3.1592, diffLoss = 15.4087, kgLoss = 0.0968
2025-02-13 01:07:11.444186: Training Step 28/59: batchLoss = 3.1641, diffLoss = 15.4221, kgLoss = 0.0996
2025-02-13 01:07:12.379409: Training Step 29/59: batchLoss = 2.9210, diffLoss = 14.2551, kgLoss = 0.0875
2025-02-13 01:07:13.304782: Training Step 30/59: batchLoss = 2.7622, diffLoss = 13.5153, kgLoss = 0.0739
2025-02-13 01:07:14.229198: Training Step 31/59: batchLoss = 2.9558, diffLoss = 14.4515, kgLoss = 0.0819
2025-02-13 01:07:15.158001: Training Step 32/59: batchLoss = 3.0938, diffLoss = 15.1326, kgLoss = 0.0842
2025-02-13 01:07:16.083548: Training Step 33/59: batchLoss = 2.8070, diffLoss = 13.7179, kgLoss = 0.0792
2025-02-13 01:07:17.019053: Training Step 34/59: batchLoss = 2.9591, diffLoss = 14.4885, kgLoss = 0.0768
2025-02-13 01:07:17.948099: Training Step 35/59: batchLoss = 2.8473, diffLoss = 13.9009, kgLoss = 0.0839
2025-02-13 01:07:18.875306: Training Step 36/59: batchLoss = 2.6440, diffLoss = 12.9229, kgLoss = 0.0742
2025-02-13 01:07:19.800337: Training Step 37/59: batchLoss = 2.8607, diffLoss = 13.9897, kgLoss = 0.0784
2025-02-13 01:07:20.724804: Training Step 38/59: batchLoss = 2.9007, diffLoss = 14.1768, kgLoss = 0.0817
2025-02-13 01:07:21.652215: Training Step 39/59: batchLoss = 3.0934, diffLoss = 15.0591, kgLoss = 0.1019
2025-02-13 01:07:22.575639: Training Step 40/59: batchLoss = 2.8755, diffLoss = 14.0415, kgLoss = 0.0839
2025-02-13 01:07:23.506424: Training Step 41/59: batchLoss = 2.8226, diffLoss = 13.7970, kgLoss = 0.0790
2025-02-13 01:07:24.443264: Training Step 42/59: batchLoss = 3.1034, diffLoss = 15.1600, kgLoss = 0.0893
2025-02-13 01:07:25.386976: Training Step 43/59: batchLoss = 3.0528, diffLoss = 14.9187, kgLoss = 0.0863
2025-02-13 01:07:26.321368: Training Step 44/59: batchLoss = 3.3795, diffLoss = 16.4996, kgLoss = 0.0995
2025-02-13 01:07:27.260947: Training Step 45/59: batchLoss = 2.9618, diffLoss = 14.4815, kgLoss = 0.0819
2025-02-13 01:07:28.202705: Training Step 46/59: batchLoss = 3.1921, diffLoss = 15.6055, kgLoss = 0.0887
2025-02-13 01:07:29.143491: Training Step 47/59: batchLoss = 2.8966, diffLoss = 14.1739, kgLoss = 0.0773
2025-02-13 01:07:30.081020: Training Step 48/59: batchLoss = 3.2369, diffLoss = 15.8146, kgLoss = 0.0925
2025-02-13 01:07:31.016141: Training Step 49/59: batchLoss = 3.1820, diffLoss = 15.5082, kgLoss = 0.1004
2025-02-13 01:07:31.951423: Training Step 50/59: batchLoss = 2.8663, diffLoss = 14.0194, kgLoss = 0.0780
2025-02-13 01:07:32.892598: Training Step 51/59: batchLoss = 2.9293, diffLoss = 14.3139, kgLoss = 0.0832
2025-02-13 01:07:33.822015: Training Step 52/59: batchLoss = 3.1307, diffLoss = 15.3004, kgLoss = 0.0883
2025-02-13 01:07:34.766732: Training Step 53/59: batchLoss = 2.6237, diffLoss = 12.8139, kgLoss = 0.0761
2025-02-13 01:07:35.686064: Training Step 54/59: batchLoss = 3.0559, diffLoss = 14.9370, kgLoss = 0.0856
2025-02-13 01:07:36.617515: Training Step 55/59: batchLoss = 2.8228, diffLoss = 13.8123, kgLoss = 0.0754
2025-02-13 01:07:37.531384: Training Step 56/59: batchLoss = 2.9091, diffLoss = 14.2281, kgLoss = 0.0794
2025-02-13 01:07:38.371258: Training Step 57/59: batchLoss = 2.7718, diffLoss = 13.5473, kgLoss = 0.0779
2025-02-13 01:07:39.227177: Training Step 58/59: batchLoss = 3.0933, diffLoss = 15.1094, kgLoss = 0.0892
2025-02-13 01:07:39.327207: 
2025-02-13 01:07:39.327725: Epoch 99/1000, Train: epLoss = 0.4342, epDfLoss = 2.1206, epKgLoss = 0.0126  
2025-02-13 01:07:40.795357: Steps 0/47: batch_recall = 37.24, batch_ndcg = 48.76 
2025-02-13 01:07:42.087034: Steps 1/47: batch_recall = 36.92, batch_ndcg = 41.65 
2025-02-13 01:07:43.388816: Steps 2/47: batch_recall = 41.88, batch_ndcg = 47.18 
2025-02-13 01:07:44.640180: Steps 3/47: batch_recall = 44.61, batch_ndcg = 46.35 
2025-02-13 01:07:45.843746: Steps 4/47: batch_recall = 39.57, batch_ndcg = 45.45 
2025-02-13 01:07:47.060935: Steps 5/47: batch_recall = 33.62, batch_ndcg = 40.45 
2025-02-13 01:07:48.271735: Steps 6/47: batch_recall = 38.57, batch_ndcg = 42.05 
2025-02-13 01:07:49.451028: Steps 7/47: batch_recall = 44.48, batch_ndcg = 46.28 
2025-02-13 01:07:50.632900: Steps 8/47: batch_recall = 48.84, batch_ndcg = 53.02 
2025-02-13 01:07:51.773962: Steps 9/47: batch_recall = 47.16, batch_ndcg = 46.92 
2025-02-13 01:07:52.944207: Steps 10/47: batch_recall = 43.81, batch_ndcg = 44.39 
2025-02-13 01:07:54.081287: Steps 11/47: batch_recall = 55.05, batch_ndcg = 52.67 
2025-02-13 01:07:55.236612: Steps 12/47: batch_recall = 50.24, batch_ndcg = 48.75 
2025-02-13 01:07:56.361158: Steps 13/47: batch_recall = 48.75, batch_ndcg = 45.38 
2025-02-13 01:07:57.451826: Steps 14/47: batch_recall = 41.48, batch_ndcg = 40.76 
2025-02-13 01:07:58.538423: Steps 15/47: batch_recall = 57.26, batch_ndcg = 53.52 
2025-02-13 01:07:59.615597: Steps 16/47: batch_recall = 51.43, batch_ndcg = 47.73 
2025-02-13 01:08:00.660588: Steps 17/47: batch_recall = 57.81, batch_ndcg = 49.88 
2025-02-13 01:08:01.746566: Steps 18/47: batch_recall = 53.49, batch_ndcg = 50.18 
2025-02-13 01:08:02.809725: Steps 19/47: batch_recall = 60.26, batch_ndcg = 54.69 
2025-02-13 01:08:03.860757: Steps 20/47: batch_recall = 69.26, batch_ndcg = 62.61 
2025-02-13 01:08:04.912129: Steps 21/47: batch_recall = 64.85, batch_ndcg = 55.80 
2025-02-13 01:08:05.967600: Steps 22/47: batch_recall = 55.69, batch_ndcg = 51.19 
2025-02-13 01:08:07.030483: Steps 23/47: batch_recall = 63.19, batch_ndcg = 54.78 
2025-02-13 01:08:08.097173: Steps 24/47: batch_recall = 65.28, batch_ndcg = 58.62 
2025-02-13 01:08:09.140284: Steps 25/47: batch_recall = 64.14, batch_ndcg = 57.24 
2025-02-13 01:08:10.168476: Steps 26/47: batch_recall = 62.22, batch_ndcg = 54.26 
2025-02-13 01:08:11.217006: Steps 27/47: batch_recall = 60.33, batch_ndcg = 52.64 
2025-02-13 01:08:12.250163: Steps 28/47: batch_recall = 70.93, batch_ndcg = 59.84 
2025-02-13 01:08:13.291116: Steps 29/47: batch_recall = 69.55, batch_ndcg = 58.77 
2025-02-13 01:08:14.319793: Steps 30/47: batch_recall = 73.21, batch_ndcg = 63.93 
2025-02-13 01:08:15.346556: Steps 31/47: batch_recall = 68.04, batch_ndcg = 56.12 
2025-02-13 01:08:16.357159: Steps 32/47: batch_recall = 71.03, batch_ndcg = 64.55 
2025-02-13 01:08:17.363135: Steps 33/47: batch_recall = 80.58, batch_ndcg = 68.66 
2025-02-13 01:08:18.366534: Steps 34/47: batch_recall = 67.36, batch_ndcg = 56.33 
2025-02-13 01:08:19.348137: Steps 35/47: batch_recall = 77.16, batch_ndcg = 64.91 
2025-02-13 01:08:20.326994: Steps 36/47: batch_recall = 77.14, batch_ndcg = 63.81 
2025-02-13 01:08:21.303785: Steps 37/47: batch_recall = 83.89, batch_ndcg = 72.06 
2025-02-13 01:08:22.269503: Steps 38/47: batch_recall = 92.21, batch_ndcg = 74.73 
2025-02-13 01:08:23.242638: Steps 39/47: batch_recall = 86.63, batch_ndcg = 68.72 
2025-02-13 01:08:24.206695: Steps 40/47: batch_recall = 73.42, batch_ndcg = 63.57 
2025-02-13 01:08:25.156509: Steps 41/47: batch_recall = 87.11, batch_ndcg = 71.96 
2025-02-13 01:08:26.127434: Steps 42/47: batch_recall = 82.16, batch_ndcg = 64.64 
2025-02-13 01:08:27.094770: Steps 43/47: batch_recall = 90.65, batch_ndcg = 72.78 
2025-02-13 01:08:28.077021: Steps 44/47: batch_recall = 88.98, batch_ndcg = 70.52 
2025-02-13 01:08:29.032520: Steps 45/47: batch_recall = 92.88, batch_ndcg = 75.01 
2025-02-13 01:08:29.138130: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.83 
2025-02-13 01:08:29.138259: Epoch 99/1000, Test: Recall = 0.1219, NDCG = 0.1097  

2025-02-13 01:08:30.355115: Training Step 0/59: batchLoss = 2.7283, diffLoss = 13.2990, kgLoss = 0.0857
2025-02-13 01:08:31.291148: Training Step 1/59: batchLoss = 2.4165, diffLoss = 11.7955, kgLoss = 0.0718
2025-02-13 01:08:32.232327: Training Step 2/59: batchLoss = 2.8145, diffLoss = 13.7525, kgLoss = 0.0800
2025-02-13 01:08:33.165475: Training Step 3/59: batchLoss = 2.7527, diffLoss = 13.4363, kgLoss = 0.0818
2025-02-13 01:08:34.103643: Training Step 4/59: batchLoss = 3.0290, diffLoss = 14.7455, kgLoss = 0.0998
2025-02-13 01:08:35.038519: Training Step 5/59: batchLoss = 2.6472, diffLoss = 12.9171, kgLoss = 0.0797
2025-02-13 01:08:35.978246: Training Step 6/59: batchLoss = 2.9978, diffLoss = 14.6104, kgLoss = 0.0946
2025-02-13 01:08:36.908688: Training Step 7/59: batchLoss = 2.9072, diffLoss = 14.1830, kgLoss = 0.0883
2025-02-13 01:08:37.837800: Training Step 8/59: batchLoss = 2.6742, diffLoss = 13.0538, kgLoss = 0.0793
2025-02-13 01:08:38.772223: Training Step 9/59: batchLoss = 2.7446, diffLoss = 13.3589, kgLoss = 0.0910
2025-02-13 01:08:39.700545: Training Step 10/59: batchLoss = 2.6647, diffLoss = 12.9997, kgLoss = 0.0810
2025-02-13 01:08:40.632988: Training Step 11/59: batchLoss = 2.9093, diffLoss = 14.2064, kgLoss = 0.0850
2025-02-13 01:08:41.558862: Training Step 12/59: batchLoss = 2.9226, diffLoss = 14.2083, kgLoss = 0.1012
2025-02-13 01:08:42.484985: Training Step 13/59: batchLoss = 2.7415, diffLoss = 13.3784, kgLoss = 0.0823
2025-02-13 01:08:43.407956: Training Step 14/59: batchLoss = 3.0076, diffLoss = 14.6899, kgLoss = 0.0871
2025-02-13 01:08:44.343007: Training Step 15/59: batchLoss = 2.9777, diffLoss = 14.5591, kgLoss = 0.0823
2025-02-13 01:08:45.268729: Training Step 16/59: batchLoss = 2.9241, diffLoss = 14.2569, kgLoss = 0.0909
2025-02-13 01:08:46.196211: Training Step 17/59: batchLoss = 2.9347, diffLoss = 14.3213, kgLoss = 0.0880
2025-02-13 01:08:47.123873: Training Step 18/59: batchLoss = 2.9122, diffLoss = 14.2020, kgLoss = 0.0898
2025-02-13 01:08:48.053127: Training Step 19/59: batchLoss = 2.8878, diffLoss = 14.1161, kgLoss = 0.0808
2025-02-13 01:08:48.989029: Training Step 20/59: batchLoss = 2.7311, diffLoss = 13.3402, kgLoss = 0.0788
2025-02-13 01:08:49.926024: Training Step 21/59: batchLoss = 2.6669, diffLoss = 13.0169, kgLoss = 0.0795
2025-02-13 01:08:50.863208: Training Step 22/59: batchLoss = 2.8058, diffLoss = 13.7054, kgLoss = 0.0809
2025-02-13 01:08:51.804893: Training Step 23/59: batchLoss = 2.8663, diffLoss = 14.0143, kgLoss = 0.0793
2025-02-13 01:08:52.744643: Training Step 24/59: batchLoss = 2.7354, diffLoss = 13.3538, kgLoss = 0.0809
2025-02-13 01:08:53.687508: Training Step 25/59: batchLoss = 3.1298, diffLoss = 15.3214, kgLoss = 0.0819
2025-02-13 01:08:54.627572: Training Step 26/59: batchLoss = 3.2531, diffLoss = 15.8895, kgLoss = 0.0941
2025-02-13 01:08:55.572950: Training Step 27/59: batchLoss = 2.8937, diffLoss = 14.1584, kgLoss = 0.0775
2025-02-13 01:08:56.504340: Training Step 28/59: batchLoss = 3.0492, diffLoss = 14.9031, kgLoss = 0.0857
2025-02-13 01:08:57.437643: Training Step 29/59: batchLoss = 3.2751, diffLoss = 16.0122, kgLoss = 0.0908
2025-02-13 01:08:58.380226: Training Step 30/59: batchLoss = 2.8611, diffLoss = 13.9873, kgLoss = 0.0796
2025-02-13 01:08:59.309025: Training Step 31/59: batchLoss = 3.1019, diffLoss = 15.1639, kgLoss = 0.0864
2025-02-13 01:09:00.246816: Training Step 32/59: batchLoss = 2.9449, diffLoss = 14.3881, kgLoss = 0.0841
2025-02-13 01:09:01.175869: Training Step 33/59: batchLoss = 3.0654, diffLoss = 14.9511, kgLoss = 0.0939
2025-02-13 01:09:02.098703: Training Step 34/59: batchLoss = 2.9501, diffLoss = 14.4115, kgLoss = 0.0848
2025-02-13 01:09:03.030097: Training Step 35/59: batchLoss = 3.1387, diffLoss = 15.3587, kgLoss = 0.0837
2025-02-13 01:09:03.966363: Training Step 36/59: batchLoss = 2.8238, diffLoss = 13.8067, kgLoss = 0.0781
2025-02-13 01:09:04.899340: Training Step 37/59: batchLoss = 3.0402, diffLoss = 14.8316, kgLoss = 0.0924
2025-02-13 01:09:05.834376: Training Step 38/59: batchLoss = 2.9181, diffLoss = 14.2591, kgLoss = 0.0829
2025-02-13 01:09:06.757604: Training Step 39/59: batchLoss = 2.5693, diffLoss = 12.5564, kgLoss = 0.0725
2025-02-13 01:09:07.684540: Training Step 40/59: batchLoss = 2.9038, diffLoss = 14.1880, kgLoss = 0.0828
2025-02-13 01:09:08.614330: Training Step 41/59: batchLoss = 2.8128, diffLoss = 13.7561, kgLoss = 0.0770
2025-02-13 01:09:09.547867: Training Step 42/59: batchLoss = 2.9963, diffLoss = 14.6297, kgLoss = 0.0880
2025-02-13 01:09:10.483444: Training Step 43/59: batchLoss = 3.2366, diffLoss = 15.8353, kgLoss = 0.0870
2025-02-13 01:09:11.422270: Training Step 44/59: batchLoss = 2.8276, diffLoss = 13.7934, kgLoss = 0.0862
2025-02-13 01:09:12.352103: Training Step 45/59: batchLoss = 3.2869, diffLoss = 16.0420, kgLoss = 0.0982
2025-02-13 01:09:13.288936: Training Step 46/59: batchLoss = 2.9736, diffLoss = 14.5168, kgLoss = 0.0879
2025-02-13 01:09:14.226228: Training Step 47/59: batchLoss = 3.1912, diffLoss = 15.6010, kgLoss = 0.0888
2025-02-13 01:09:15.159103: Training Step 48/59: batchLoss = 3.0769, diffLoss = 15.0064, kgLoss = 0.0945
2025-02-13 01:09:16.100285: Training Step 49/59: batchLoss = 2.8911, diffLoss = 14.1425, kgLoss = 0.0782
2025-02-13 01:09:17.036891: Training Step 50/59: batchLoss = 3.4394, diffLoss = 16.7995, kgLoss = 0.0994
2025-02-13 01:09:17.971807: Training Step 51/59: batchLoss = 2.8933, diffLoss = 14.1242, kgLoss = 0.0856
2025-02-13 01:09:18.905792: Training Step 52/59: batchLoss = 3.1536, diffLoss = 15.4067, kgLoss = 0.0903
2025-02-13 01:09:19.835148: Training Step 53/59: batchLoss = 3.1036, diffLoss = 15.1731, kgLoss = 0.0863
2025-02-13 01:09:20.764215: Training Step 54/59: batchLoss = 3.2555, diffLoss = 15.8930, kgLoss = 0.0961
2025-02-13 01:09:21.695354: Training Step 55/59: batchLoss = 2.9092, diffLoss = 14.2258, kgLoss = 0.0800
2025-02-13 01:09:22.631780: Training Step 56/59: batchLoss = 2.9882, diffLoss = 14.6283, kgLoss = 0.0782
2025-02-13 01:09:23.474161: Training Step 57/59: batchLoss = 2.9876, diffLoss = 14.5679, kgLoss = 0.0925
2025-02-13 01:09:24.328022: Training Step 58/59: batchLoss = 3.0091, diffLoss = 14.7291, kgLoss = 0.0792
2025-02-13 01:09:24.426538: 
2025-02-13 01:09:24.426911: Epoch 100/1000, Train: epLoss = 0.4334, epDfLoss = 2.1164, epKgLoss = 0.0126  
2025-02-13 01:09:25.897837: Steps 0/47: batch_recall = 36.81, batch_ndcg = 47.83 
2025-02-13 01:09:27.191432: Steps 1/47: batch_recall = 37.30, batch_ndcg = 42.58 
2025-02-13 01:09:28.437132: Steps 2/47: batch_recall = 40.30, batch_ndcg = 46.90 
2025-02-13 01:09:29.697875: Steps 3/47: batch_recall = 44.52, batch_ndcg = 46.61 
2025-02-13 01:09:30.903755: Steps 4/47: batch_recall = 39.52, batch_ndcg = 45.73 
2025-02-13 01:09:32.127519: Steps 5/47: batch_recall = 35.08, batch_ndcg = 41.68 
2025-02-13 01:09:33.330991: Steps 6/47: batch_recall = 38.68, batch_ndcg = 42.07 
2025-02-13 01:09:34.509131: Steps 7/47: batch_recall = 44.18, batch_ndcg = 46.74 
2025-02-13 01:09:35.695426: Steps 8/47: batch_recall = 48.27, batch_ndcg = 52.43 
2025-02-13 01:09:36.842523: Steps 9/47: batch_recall = 46.87, batch_ndcg = 46.50 
2025-02-13 01:09:38.017587: Steps 10/47: batch_recall = 44.14, batch_ndcg = 44.30 
2025-02-13 01:09:39.166133: Steps 11/47: batch_recall = 53.21, batch_ndcg = 52.39 
2025-02-13 01:09:40.331287: Steps 12/47: batch_recall = 49.68, batch_ndcg = 48.32 
2025-02-13 01:09:41.450991: Steps 13/47: batch_recall = 48.59, batch_ndcg = 45.28 
2025-02-13 01:09:42.536564: Steps 14/47: batch_recall = 40.04, batch_ndcg = 41.08 
2025-02-13 01:09:43.623646: Steps 15/47: batch_recall = 56.77, batch_ndcg = 53.72 
2025-02-13 01:09:44.691834: Steps 16/47: batch_recall = 50.78, batch_ndcg = 47.47 
2025-02-13 01:09:45.730223: Steps 17/47: batch_recall = 58.10, batch_ndcg = 49.52 
2025-02-13 01:09:46.808143: Steps 18/47: batch_recall = 54.30, batch_ndcg = 50.44 
2025-02-13 01:09:47.863844: Steps 19/47: batch_recall = 59.95, batch_ndcg = 54.69 
2025-02-13 01:09:48.896259: Steps 20/47: batch_recall = 68.55, batch_ndcg = 61.89 
2025-02-13 01:09:49.940838: Steps 21/47: batch_recall = 65.05, batch_ndcg = 55.90 
2025-02-13 01:09:51.000507: Steps 22/47: batch_recall = 56.27, batch_ndcg = 51.47 
2025-02-13 01:09:52.047187: Steps 23/47: batch_recall = 63.42, batch_ndcg = 55.38 
2025-02-13 01:09:53.116926: Steps 24/47: batch_recall = 65.68, batch_ndcg = 58.65 
2025-02-13 01:09:54.151454: Steps 25/47: batch_recall = 64.84, batch_ndcg = 57.21 
2025-02-13 01:09:55.182847: Steps 26/47: batch_recall = 60.53, batch_ndcg = 53.70 
2025-02-13 01:09:56.242632: Steps 27/47: batch_recall = 61.10, batch_ndcg = 52.74 
2025-02-13 01:09:57.273467: Steps 28/47: batch_recall = 69.25, batch_ndcg = 58.86 
2025-02-13 01:09:58.296092: Steps 29/47: batch_recall = 71.61, batch_ndcg = 59.58 
2025-02-13 01:09:59.323374: Steps 30/47: batch_recall = 73.21, batch_ndcg = 63.77 
2025-02-13 01:10:00.344039: Steps 31/47: batch_recall = 68.13, batch_ndcg = 56.42 
2025-02-13 01:10:01.353360: Steps 32/47: batch_recall = 71.20, batch_ndcg = 64.74 
2025-02-13 01:10:02.363943: Steps 33/47: batch_recall = 80.10, batch_ndcg = 68.50 
2025-02-13 01:10:03.363178: Steps 34/47: batch_recall = 68.80, batch_ndcg = 56.88 
2025-02-13 01:10:04.348356: Steps 35/47: batch_recall = 77.48, batch_ndcg = 65.28 
2025-02-13 01:10:05.330323: Steps 36/47: batch_recall = 78.70, batch_ndcg = 65.12 
2025-02-13 01:10:06.291133: Steps 37/47: batch_recall = 84.02, batch_ndcg = 72.45 
2025-02-13 01:10:07.290664: Steps 38/47: batch_recall = 90.73, batch_ndcg = 72.33 
2025-02-13 01:10:08.246455: Steps 39/47: batch_recall = 87.99, batch_ndcg = 69.18 
2025-02-13 01:10:09.202614: Steps 40/47: batch_recall = 73.66, batch_ndcg = 63.31 
2025-02-13 01:10:10.160804: Steps 41/47: batch_recall = 88.20, batch_ndcg = 72.95 
2025-02-13 01:10:11.136438: Steps 42/47: batch_recall = 82.46, batch_ndcg = 65.13 
2025-02-13 01:10:12.088444: Steps 43/47: batch_recall = 89.40, batch_ndcg = 72.66 
2025-02-13 01:10:13.053249: Steps 44/47: batch_recall = 87.10, batch_ndcg = 69.72 
2025-02-13 01:10:13.998606: Steps 45/47: batch_recall = 95.83, batch_ndcg = 75.66 
2025-02-13 01:10:14.103072: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.64 
2025-02-13 01:10:14.103210: Epoch 100/1000, Test: Recall = 0.1219, NDCG = 0.1098  

2025-02-13 01:10:15.293424: Training Step 0/59: batchLoss = 2.9715, diffLoss = 14.5176, kgLoss = 0.0850
2025-02-13 01:10:16.212962: Training Step 1/59: batchLoss = 2.7552, diffLoss = 13.4256, kgLoss = 0.0876
2025-02-13 01:10:17.136163: Training Step 2/59: batchLoss = 2.8267, diffLoss = 13.7618, kgLoss = 0.0930
2025-02-13 01:10:18.060443: Training Step 3/59: batchLoss = 2.8901, diffLoss = 14.1290, kgLoss = 0.0804
2025-02-13 01:10:18.982202: Training Step 4/59: batchLoss = 2.7570, diffLoss = 13.4452, kgLoss = 0.0850
2025-02-13 01:10:19.902698: Training Step 5/59: batchLoss = 2.8313, diffLoss = 13.8481, kgLoss = 0.0771
2025-02-13 01:10:20.816795: Training Step 6/59: batchLoss = 2.7535, diffLoss = 13.4400, kgLoss = 0.0819
2025-02-13 01:10:21.740560: Training Step 7/59: batchLoss = 2.7025, diffLoss = 13.2304, kgLoss = 0.0706
2025-02-13 01:10:22.655724: Training Step 8/59: batchLoss = 2.7644, diffLoss = 13.4875, kgLoss = 0.0836
2025-02-13 01:10:23.575330: Training Step 9/59: batchLoss = 2.9687, diffLoss = 14.4863, kgLoss = 0.0893
2025-02-13 01:10:24.491386: Training Step 10/59: batchLoss = 2.6098, diffLoss = 12.7303, kgLoss = 0.0797
2025-02-13 01:10:25.411353: Training Step 11/59: batchLoss = 2.4793, diffLoss = 12.0497, kgLoss = 0.0867
2025-02-13 01:10:26.328915: Training Step 12/59: batchLoss = 2.9932, diffLoss = 14.6259, kgLoss = 0.0850
2025-02-13 01:10:27.251437: Training Step 13/59: batchLoss = 2.7058, diffLoss = 13.2087, kgLoss = 0.0800
2025-02-13 01:10:28.169780: Training Step 14/59: batchLoss = 2.8322, diffLoss = 13.8433, kgLoss = 0.0794
2025-02-13 01:10:29.086080: Training Step 15/59: batchLoss = 3.0117, diffLoss = 14.7269, kgLoss = 0.0829
2025-02-13 01:10:30.013630: Training Step 16/59: batchLoss = 2.8777, diffLoss = 14.0172, kgLoss = 0.0929
2025-02-13 01:10:30.933293: Training Step 17/59: batchLoss = 2.8799, diffLoss = 14.0591, kgLoss = 0.0851
2025-02-13 01:10:31.854668: Training Step 18/59: batchLoss = 2.7776, diffLoss = 13.5519, kgLoss = 0.0840
2025-02-13 01:10:32.786214: Training Step 19/59: batchLoss = 3.1075, diffLoss = 15.1730, kgLoss = 0.0912
2025-02-13 01:10:33.723313: Training Step 20/59: batchLoss = 2.7365, diffLoss = 13.3437, kgLoss = 0.0847
2025-02-13 01:10:34.653549: Training Step 21/59: batchLoss = 2.6433, diffLoss = 12.8770, kgLoss = 0.0848
2025-02-13 01:10:35.590124: Training Step 22/59: batchLoss = 3.2121, diffLoss = 15.6807, kgLoss = 0.0950
2025-02-13 01:10:36.518194: Training Step 23/59: batchLoss = 3.0566, diffLoss = 14.9223, kgLoss = 0.0901
2025-02-13 01:10:37.446866: Training Step 24/59: batchLoss = 2.9188, diffLoss = 14.2367, kgLoss = 0.0893
2025-02-13 01:10:38.380499: Training Step 25/59: batchLoss = 2.7174, diffLoss = 13.2881, kgLoss = 0.0747
2025-02-13 01:10:39.314733: Training Step 26/59: batchLoss = 2.6650, diffLoss = 13.0091, kgLoss = 0.0790
2025-02-13 01:10:40.244535: Training Step 27/59: batchLoss = 2.8183, diffLoss = 13.7635, kgLoss = 0.0820
2025-02-13 01:10:41.181129: Training Step 28/59: batchLoss = 2.8008, diffLoss = 13.6712, kgLoss = 0.0832
2025-02-13 01:10:42.108670: Training Step 29/59: batchLoss = 2.6380, diffLoss = 12.8381, kgLoss = 0.0880
2025-02-13 01:10:43.039973: Training Step 30/59: batchLoss = 3.1947, diffLoss = 15.6181, kgLoss = 0.0889
2025-02-13 01:10:43.963844: Training Step 31/59: batchLoss = 3.2542, diffLoss = 15.8899, kgLoss = 0.0953
2025-02-13 01:10:44.894783: Training Step 32/59: batchLoss = 3.2210, diffLoss = 15.7538, kgLoss = 0.0878
2025-02-13 01:10:45.821536: Training Step 33/59: batchLoss = 3.0635, diffLoss = 14.9565, kgLoss = 0.0902
2025-02-13 01:10:46.743528: Training Step 34/59: batchLoss = 2.9604, diffLoss = 14.4978, kgLoss = 0.0760
2025-02-13 01:10:47.668462: Training Step 35/59: batchLoss = 2.9022, diffLoss = 14.1578, kgLoss = 0.0883
2025-02-13 01:10:48.596420: Training Step 36/59: batchLoss = 2.9466, diffLoss = 14.3979, kgLoss = 0.0838
2025-02-13 01:10:49.528452: Training Step 37/59: batchLoss = 3.1296, diffLoss = 15.2962, kgLoss = 0.0879
2025-02-13 01:10:50.448811: Training Step 38/59: batchLoss = 2.9821, diffLoss = 14.5651, kgLoss = 0.0864
2025-02-13 01:10:51.377842: Training Step 39/59: batchLoss = 3.1157, diffLoss = 15.2499, kgLoss = 0.0822
2025-02-13 01:10:52.307528: Training Step 40/59: batchLoss = 3.0816, diffLoss = 15.0050, kgLoss = 0.1007
2025-02-13 01:10:53.229899: Training Step 41/59: batchLoss = 2.8896, diffLoss = 14.1212, kgLoss = 0.0817
2025-02-13 01:10:54.163233: Training Step 42/59: batchLoss = 3.2172, diffLoss = 15.7228, kgLoss = 0.0908
2025-02-13 01:10:55.089406: Training Step 43/59: batchLoss = 3.2746, diffLoss = 15.9926, kgLoss = 0.0951
2025-02-13 01:10:56.043864: Training Step 44/59: batchLoss = 2.8688, diffLoss = 14.0455, kgLoss = 0.0747
2025-02-13 01:10:56.982907: Training Step 45/59: batchLoss = 3.1672, diffLoss = 15.4774, kgLoss = 0.0896
2025-02-13 01:10:57.912123: Training Step 46/59: batchLoss = 3.1860, diffLoss = 15.5677, kgLoss = 0.0906
2025-02-13 01:10:58.851812: Training Step 47/59: batchLoss = 3.0513, diffLoss = 14.9048, kgLoss = 0.0879
2025-02-13 01:10:59.787010: Training Step 48/59: batchLoss = 3.0602, diffLoss = 14.9593, kgLoss = 0.0855
2025-02-13 01:11:00.717631: Training Step 49/59: batchLoss = 3.3331, diffLoss = 16.2945, kgLoss = 0.0927
2025-02-13 01:11:01.649594: Training Step 50/59: batchLoss = 2.8957, diffLoss = 14.1456, kgLoss = 0.0833
2025-02-13 01:11:02.579570: Training Step 51/59: batchLoss = 3.0730, diffLoss = 15.0115, kgLoss = 0.0884
2025-02-13 01:11:03.505498: Training Step 52/59: batchLoss = 3.0497, diffLoss = 14.9213, kgLoss = 0.0819
2025-02-13 01:11:04.422538: Training Step 53/59: batchLoss = 2.8625, diffLoss = 14.0067, kgLoss = 0.0764
2025-02-13 01:11:05.372425: Training Step 54/59: batchLoss = 2.9755, diffLoss = 14.5547, kgLoss = 0.0807
2025-02-13 01:11:06.294512: Training Step 55/59: batchLoss = 2.8949, diffLoss = 14.1314, kgLoss = 0.0858
2025-02-13 01:11:07.220244: Training Step 56/59: batchLoss = 2.7839, diffLoss = 13.5714, kgLoss = 0.0871
2025-02-13 01:11:08.058225: Training Step 57/59: batchLoss = 3.0226, diffLoss = 14.7670, kgLoss = 0.0865
2025-02-13 01:11:08.906068: Training Step 58/59: batchLoss = 3.0984, diffLoss = 15.1487, kgLoss = 0.0858
2025-02-13 01:11:08.999933: 
2025-02-13 01:11:09.000506: Epoch 101/1000, Train: epLoss = 0.4331, epDfLoss = 2.1153, epKgLoss = 0.0126  
2025-02-13 01:11:10.476251: Steps 0/47: batch_recall = 36.39, batch_ndcg = 47.09 
2025-02-13 01:11:11.768262: Steps 1/47: batch_recall = 37.41, batch_ndcg = 42.69 
2025-02-13 01:11:13.016318: Steps 2/47: batch_recall = 40.99, batch_ndcg = 47.11 
2025-02-13 01:11:14.270362: Steps 3/47: batch_recall = 44.15, batch_ndcg = 46.41 
2025-02-13 01:11:15.477096: Steps 4/47: batch_recall = 39.89, batch_ndcg = 45.86 
2025-02-13 01:11:16.710259: Steps 5/47: batch_recall = 34.87, batch_ndcg = 40.47 
2025-02-13 01:11:17.917889: Steps 6/47: batch_recall = 37.72, batch_ndcg = 40.97 
2025-02-13 01:11:19.095836: Steps 7/47: batch_recall = 44.56, batch_ndcg = 46.42 
2025-02-13 01:11:20.281857: Steps 8/47: batch_recall = 48.45, batch_ndcg = 52.67 
2025-02-13 01:11:21.428408: Steps 9/47: batch_recall = 47.17, batch_ndcg = 46.89 
2025-02-13 01:11:22.613426: Steps 10/47: batch_recall = 44.49, batch_ndcg = 44.10 
2025-02-13 01:11:23.767818: Steps 11/47: batch_recall = 54.24, batch_ndcg = 51.64 
2025-02-13 01:11:24.907624: Steps 12/47: batch_recall = 48.59, batch_ndcg = 48.73 
2025-02-13 01:11:26.027936: Steps 13/47: batch_recall = 49.75, batch_ndcg = 45.80 
2025-02-13 01:11:27.117431: Steps 14/47: batch_recall = 40.10, batch_ndcg = 40.27 
2025-02-13 01:11:28.209071: Steps 15/47: batch_recall = 57.88, batch_ndcg = 54.24 
2025-02-13 01:11:29.277322: Steps 16/47: batch_recall = 51.71, batch_ndcg = 48.69 
2025-02-13 01:11:30.324939: Steps 17/47: batch_recall = 57.52, batch_ndcg = 49.73 
2025-02-13 01:11:31.401212: Steps 18/47: batch_recall = 55.30, batch_ndcg = 51.06 
2025-02-13 01:11:32.467019: Steps 19/47: batch_recall = 59.77, batch_ndcg = 55.13 
2025-02-13 01:11:33.506880: Steps 20/47: batch_recall = 68.47, batch_ndcg = 61.04 
2025-02-13 01:11:34.556895: Steps 21/47: batch_recall = 65.90, batch_ndcg = 56.99 
2025-02-13 01:11:35.614314: Steps 22/47: batch_recall = 55.92, batch_ndcg = 51.14 
2025-02-13 01:11:36.666444: Steps 23/47: batch_recall = 63.85, batch_ndcg = 55.08 
2025-02-13 01:11:37.736270: Steps 24/47: batch_recall = 66.72, batch_ndcg = 59.13 
2025-02-13 01:11:38.787363: Steps 25/47: batch_recall = 65.93, batch_ndcg = 57.56 
2025-02-13 01:11:39.806770: Steps 26/47: batch_recall = 60.81, batch_ndcg = 53.86 
2025-02-13 01:11:40.849840: Steps 27/47: batch_recall = 61.51, batch_ndcg = 53.43 
2025-02-13 01:11:41.882893: Steps 28/47: batch_recall = 70.77, batch_ndcg = 59.14 
2025-02-13 01:11:42.909406: Steps 29/47: batch_recall = 69.88, batch_ndcg = 59.18 
2025-02-13 01:11:43.921140: Steps 30/47: batch_recall = 73.68, batch_ndcg = 65.00 
2025-02-13 01:11:44.949546: Steps 31/47: batch_recall = 68.29, batch_ndcg = 56.80 
2025-02-13 01:11:45.961454: Steps 32/47: batch_recall = 72.13, batch_ndcg = 65.67 
2025-02-13 01:11:46.972514: Steps 33/47: batch_recall = 80.98, batch_ndcg = 69.13 
2025-02-13 01:11:47.982488: Steps 34/47: batch_recall = 66.13, batch_ndcg = 55.41 
2025-02-13 01:11:48.971113: Steps 35/47: batch_recall = 77.94, batch_ndcg = 65.44 
2025-02-13 01:11:49.956101: Steps 36/47: batch_recall = 78.36, batch_ndcg = 64.59 
2025-02-13 01:11:50.917787: Steps 37/47: batch_recall = 82.94, batch_ndcg = 71.93 
2025-02-13 01:11:51.895575: Steps 38/47: batch_recall = 92.48, batch_ndcg = 73.93 
2025-02-13 01:11:52.858279: Steps 39/47: batch_recall = 88.33, batch_ndcg = 70.28 
2025-02-13 01:11:53.825337: Steps 40/47: batch_recall = 73.04, batch_ndcg = 62.74 
2025-02-13 01:11:54.787563: Steps 41/47: batch_recall = 86.37, batch_ndcg = 71.96 
2025-02-13 01:11:55.757520: Steps 42/47: batch_recall = 83.07, batch_ndcg = 65.61 
2025-02-13 01:11:56.731222: Steps 43/47: batch_recall = 91.43, batch_ndcg = 72.95 
2025-02-13 01:11:57.712281: Steps 44/47: batch_recall = 88.77, batch_ndcg = 70.51 
2025-02-13 01:11:58.663522: Steps 45/47: batch_recall = 95.01, batch_ndcg = 75.59 
2025-02-13 01:11:58.773032: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.53 
2025-02-13 01:11:58.773182: Epoch 101/1000, Test: Recall = 0.1223, NDCG = 0.1100  

2025-02-13 01:11:59.978206: Training Step 0/59: batchLoss = 2.9335, diffLoss = 14.2968, kgLoss = 0.0926
2025-02-13 01:12:00.914770: Training Step 1/59: batchLoss = 2.7519, diffLoss = 13.4247, kgLoss = 0.0838
2025-02-13 01:12:01.853100: Training Step 2/59: batchLoss = 2.9516, diffLoss = 14.3915, kgLoss = 0.0916
2025-02-13 01:12:02.790431: Training Step 3/59: batchLoss = 2.6058, diffLoss = 12.7250, kgLoss = 0.0760
2025-02-13 01:12:03.726478: Training Step 4/59: batchLoss = 3.2453, diffLoss = 15.8200, kgLoss = 0.1016
2025-02-13 01:12:04.666095: Training Step 5/59: batchLoss = 2.8826, diffLoss = 14.0697, kgLoss = 0.0859
2025-02-13 01:12:05.599628: Training Step 6/59: batchLoss = 2.8794, diffLoss = 14.0379, kgLoss = 0.0898
2025-02-13 01:12:06.531642: Training Step 7/59: batchLoss = 2.9437, diffLoss = 14.3648, kgLoss = 0.0884
2025-02-13 01:12:07.455941: Training Step 8/59: batchLoss = 2.6062, diffLoss = 12.7389, kgLoss = 0.0730
2025-02-13 01:12:08.386094: Training Step 9/59: batchLoss = 2.5203, diffLoss = 12.3206, kgLoss = 0.0703
2025-02-13 01:12:09.314529: Training Step 10/59: batchLoss = 2.6767, diffLoss = 13.0552, kgLoss = 0.0821
2025-02-13 01:12:10.238668: Training Step 11/59: batchLoss = 3.1226, diffLoss = 15.2578, kgLoss = 0.0888
2025-02-13 01:12:11.171426: Training Step 12/59: batchLoss = 2.9172, diffLoss = 14.2358, kgLoss = 0.0875
2025-02-13 01:12:12.096753: Training Step 13/59: batchLoss = 2.9988, diffLoss = 14.6039, kgLoss = 0.0976
2025-02-13 01:12:13.025334: Training Step 14/59: batchLoss = 2.9607, diffLoss = 14.4663, kgLoss = 0.0843
2025-02-13 01:12:13.949515: Training Step 15/59: batchLoss = 2.9807, diffLoss = 14.5102, kgLoss = 0.0984
2025-02-13 01:12:14.881762: Training Step 16/59: batchLoss = 2.7891, diffLoss = 13.5838, kgLoss = 0.0905
2025-02-13 01:12:15.805501: Training Step 17/59: batchLoss = 2.6986, diffLoss = 13.1706, kgLoss = 0.0806
2025-02-13 01:12:16.738533: Training Step 18/59: batchLoss = 2.8599, diffLoss = 13.9831, kgLoss = 0.0792
2025-02-13 01:12:17.668816: Training Step 19/59: batchLoss = 2.8231, diffLoss = 13.7892, kgLoss = 0.0815
2025-02-13 01:12:18.614415: Training Step 20/59: batchLoss = 2.7574, diffLoss = 13.4759, kgLoss = 0.0778
2025-02-13 01:12:19.544942: Training Step 21/59: batchLoss = 2.9618, diffLoss = 14.4647, kgLoss = 0.0860
2025-02-13 01:12:20.481437: Training Step 22/59: batchLoss = 3.1719, diffLoss = 15.4966, kgLoss = 0.0907
2025-02-13 01:12:21.420340: Training Step 23/59: batchLoss = 3.1507, diffLoss = 15.3796, kgLoss = 0.0935
2025-02-13 01:12:22.363017: Training Step 24/59: batchLoss = 3.2444, diffLoss = 15.8505, kgLoss = 0.0928
2025-02-13 01:12:23.298026: Training Step 25/59: batchLoss = 2.7249, diffLoss = 13.2853, kgLoss = 0.0848
2025-02-13 01:12:24.228255: Training Step 26/59: batchLoss = 2.9183, diffLoss = 14.2703, kgLoss = 0.0804
2025-02-13 01:12:25.177734: Training Step 27/59: batchLoss = 3.2550, diffLoss = 15.9083, kgLoss = 0.0917
2025-02-13 01:12:26.118999: Training Step 28/59: batchLoss = 3.0132, diffLoss = 14.7117, kgLoss = 0.0886
2025-02-13 01:12:27.047119: Training Step 29/59: batchLoss = 2.5561, diffLoss = 12.4667, kgLoss = 0.0785
2025-02-13 01:12:27.983568: Training Step 30/59: batchLoss = 2.7910, diffLoss = 13.6404, kgLoss = 0.0786
2025-02-13 01:12:28.913517: Training Step 31/59: batchLoss = 2.8707, diffLoss = 14.0132, kgLoss = 0.0850
2025-02-13 01:12:29.836092: Training Step 32/59: batchLoss = 3.0473, diffLoss = 14.8769, kgLoss = 0.0899
2025-02-13 01:12:30.760546: Training Step 33/59: batchLoss = 2.9748, diffLoss = 14.5271, kgLoss = 0.0868
2025-02-13 01:12:31.682424: Training Step 34/59: batchLoss = 3.1734, diffLoss = 15.5161, kgLoss = 0.0878
2025-02-13 01:12:32.607280: Training Step 35/59: batchLoss = 2.5233, diffLoss = 12.3227, kgLoss = 0.0734
2025-02-13 01:12:33.543022: Training Step 36/59: batchLoss = 2.7675, diffLoss = 13.5288, kgLoss = 0.0771
2025-02-13 01:12:34.464529: Training Step 37/59: batchLoss = 2.9369, diffLoss = 14.3659, kgLoss = 0.0797
2025-02-13 01:12:35.395790: Training Step 38/59: batchLoss = 3.0212, diffLoss = 14.7457, kgLoss = 0.0901
2025-02-13 01:12:36.319656: Training Step 39/59: batchLoss = 2.7022, diffLoss = 13.2131, kgLoss = 0.0745
2025-02-13 01:12:37.248517: Training Step 40/59: batchLoss = 3.0658, diffLoss = 14.9939, kgLoss = 0.0837
2025-02-13 01:12:38.185141: Training Step 41/59: batchLoss = 2.7892, diffLoss = 13.6295, kgLoss = 0.0791
2025-02-13 01:12:39.115816: Training Step 42/59: batchLoss = 3.1031, diffLoss = 15.1491, kgLoss = 0.0916
2025-02-13 01:12:40.043006: Training Step 43/59: batchLoss = 3.3200, diffLoss = 16.2147, kgLoss = 0.0964
2025-02-13 01:12:40.976354: Training Step 44/59: batchLoss = 3.0625, diffLoss = 14.9675, kgLoss = 0.0862
2025-02-13 01:12:41.914991: Training Step 45/59: batchLoss = 2.6843, diffLoss = 13.1451, kgLoss = 0.0691
2025-02-13 01:12:42.845866: Training Step 46/59: batchLoss = 2.8859, diffLoss = 14.1036, kgLoss = 0.0814
2025-02-13 01:12:43.781535: Training Step 47/59: batchLoss = 2.9964, diffLoss = 14.6302, kgLoss = 0.0879
2025-02-13 01:12:44.716307: Training Step 48/59: batchLoss = 3.0425, diffLoss = 14.8899, kgLoss = 0.0807
2025-02-13 01:12:45.652276: Training Step 49/59: batchLoss = 2.9177, diffLoss = 14.2436, kgLoss = 0.0863
2025-02-13 01:12:46.595728: Training Step 50/59: batchLoss = 3.0204, diffLoss = 14.7605, kgLoss = 0.0854
2025-02-13 01:12:47.529557: Training Step 51/59: batchLoss = 3.0394, diffLoss = 14.8551, kgLoss = 0.0854
2025-02-13 01:12:48.465876: Training Step 52/59: batchLoss = 3.1179, diffLoss = 15.2155, kgLoss = 0.0935
2025-02-13 01:12:49.396926: Training Step 53/59: batchLoss = 3.0141, diffLoss = 14.7222, kgLoss = 0.0871
2025-02-13 01:12:50.330163: Training Step 54/59: batchLoss = 3.0463, diffLoss = 14.8946, kgLoss = 0.0842
2025-02-13 01:12:51.258925: Training Step 55/59: batchLoss = 3.0108, diffLoss = 14.7177, kgLoss = 0.0841
2025-02-13 01:12:52.177555: Training Step 56/59: batchLoss = 3.6056, diffLoss = 17.6290, kgLoss = 0.0997
2025-02-13 01:12:53.025900: Training Step 57/59: batchLoss = 2.9544, diffLoss = 14.4146, kgLoss = 0.0894
2025-02-13 01:12:53.874081: Training Step 58/59: batchLoss = 2.7821, diffLoss = 13.5875, kgLoss = 0.0808
2025-02-13 01:12:53.975659: 
2025-02-13 01:12:53.976022: Epoch 102/1000, Train: epLoss = 0.4329, epDfLoss = 2.1142, epKgLoss = 0.0126  
2025-02-13 01:12:55.449926: Steps 0/47: batch_recall = 36.85, batch_ndcg = 47.08 
2025-02-13 01:12:56.752446: Steps 1/47: batch_recall = 36.94, batch_ndcg = 41.97 
2025-02-13 01:12:58.003566: Steps 2/47: batch_recall = 40.79, batch_ndcg = 47.02 
2025-02-13 01:12:59.264916: Steps 3/47: batch_recall = 44.23, batch_ndcg = 46.31 
2025-02-13 01:13:00.478888: Steps 4/47: batch_recall = 39.84, batch_ndcg = 45.69 
2025-02-13 01:13:01.699931: Steps 5/47: batch_recall = 34.34, batch_ndcg = 40.69 
2025-02-13 01:13:02.911520: Steps 6/47: batch_recall = 38.64, batch_ndcg = 41.89 
2025-02-13 01:13:04.095509: Steps 7/47: batch_recall = 44.54, batch_ndcg = 46.91 
2025-02-13 01:13:05.284420: Steps 8/47: batch_recall = 48.19, batch_ndcg = 52.06 
2025-02-13 01:13:06.439015: Steps 9/47: batch_recall = 47.46, batch_ndcg = 46.40 
2025-02-13 01:13:07.613836: Steps 10/47: batch_recall = 44.97, batch_ndcg = 44.68 
2025-02-13 01:13:08.757358: Steps 11/47: batch_recall = 54.98, batch_ndcg = 52.61 
2025-02-13 01:13:09.901104: Steps 12/47: batch_recall = 49.92, batch_ndcg = 49.29 
2025-02-13 01:13:11.030939: Steps 13/47: batch_recall = 48.60, batch_ndcg = 45.84 
2025-02-13 01:13:12.156806: Steps 14/47: batch_recall = 40.12, batch_ndcg = 40.74 
2025-02-13 01:13:13.235434: Steps 15/47: batch_recall = 56.68, batch_ndcg = 54.00 
2025-02-13 01:13:14.321386: Steps 16/47: batch_recall = 49.25, batch_ndcg = 47.21 
2025-02-13 01:13:15.371462: Steps 17/47: batch_recall = 58.20, batch_ndcg = 49.57 
2025-02-13 01:13:16.454858: Steps 18/47: batch_recall = 53.42, batch_ndcg = 50.20 
2025-02-13 01:13:17.519109: Steps 19/47: batch_recall = 60.35, batch_ndcg = 55.64 
2025-02-13 01:13:18.557827: Steps 20/47: batch_recall = 68.73, batch_ndcg = 61.47 
2025-02-13 01:13:19.618529: Steps 21/47: batch_recall = 65.79, batch_ndcg = 56.01 
2025-02-13 01:13:20.687043: Steps 22/47: batch_recall = 56.42, batch_ndcg = 50.88 
2025-02-13 01:13:21.755039: Steps 23/47: batch_recall = 66.28, batch_ndcg = 56.18 
2025-02-13 01:13:22.826736: Steps 24/47: batch_recall = 66.84, batch_ndcg = 59.81 
2025-02-13 01:13:23.882730: Steps 25/47: batch_recall = 66.14, batch_ndcg = 57.03 
2025-02-13 01:13:24.911574: Steps 26/47: batch_recall = 60.77, batch_ndcg = 53.57 
2025-02-13 01:13:25.962500: Steps 27/47: batch_recall = 60.95, batch_ndcg = 52.48 
2025-02-13 01:13:26.995911: Steps 28/47: batch_recall = 69.89, batch_ndcg = 59.26 
2025-02-13 01:13:28.020332: Steps 29/47: batch_recall = 70.82, batch_ndcg = 59.27 
2025-02-13 01:13:29.034509: Steps 30/47: batch_recall = 72.33, batch_ndcg = 63.08 
2025-02-13 01:13:30.063193: Steps 31/47: batch_recall = 67.02, batch_ndcg = 55.44 
2025-02-13 01:13:31.069011: Steps 32/47: batch_recall = 71.61, batch_ndcg = 64.57 
2025-02-13 01:13:32.082561: Steps 33/47: batch_recall = 79.97, batch_ndcg = 68.25 
2025-02-13 01:13:33.088338: Steps 34/47: batch_recall = 68.52, batch_ndcg = 55.96 
2025-02-13 01:13:34.065401: Steps 35/47: batch_recall = 79.38, batch_ndcg = 66.94 
2025-02-13 01:13:35.051975: Steps 36/47: batch_recall = 76.96, batch_ndcg = 63.25 
2025-02-13 01:13:36.020689: Steps 37/47: batch_recall = 84.70, batch_ndcg = 72.90 
2025-02-13 01:13:36.997279: Steps 38/47: batch_recall = 90.25, batch_ndcg = 73.37 
2025-02-13 01:13:37.970437: Steps 39/47: batch_recall = 88.90, batch_ndcg = 69.80 
2025-02-13 01:13:38.930301: Steps 40/47: batch_recall = 73.75, batch_ndcg = 63.24 
2025-02-13 01:13:39.881759: Steps 41/47: batch_recall = 86.94, batch_ndcg = 72.68 
2025-02-13 01:13:40.855843: Steps 42/47: batch_recall = 83.95, batch_ndcg = 65.10 
2025-02-13 01:13:41.823703: Steps 43/47: batch_recall = 89.07, batch_ndcg = 72.66 
2025-02-13 01:13:42.797035: Steps 44/47: batch_recall = 88.22, batch_ndcg = 70.11 
2025-02-13 01:13:43.746357: Steps 45/47: batch_recall = 94.58, batch_ndcg = 75.09 
2025-02-13 01:13:43.853030: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 01:13:43.853171: Epoch 102/1000, Test: Recall = 0.1222, NDCG = 0.1097  

2025-02-13 01:13:45.058497: Training Step 0/59: batchLoss = 2.5742, diffLoss = 12.5678, kgLoss = 0.0758
2025-02-13 01:13:45.987958: Training Step 1/59: batchLoss = 2.6841, diffLoss = 13.0825, kgLoss = 0.0844
2025-02-13 01:13:46.926849: Training Step 2/59: batchLoss = 3.0118, diffLoss = 14.6827, kgLoss = 0.0941
2025-02-13 01:13:47.867434: Training Step 3/59: batchLoss = 2.7454, diffLoss = 13.3858, kgLoss = 0.0853
2025-02-13 01:13:48.804119: Training Step 4/59: batchLoss = 2.7081, diffLoss = 13.2096, kgLoss = 0.0827
2025-02-13 01:13:49.730995: Training Step 5/59: batchLoss = 2.8340, diffLoss = 13.8283, kgLoss = 0.0854
2025-02-13 01:13:50.659122: Training Step 6/59: batchLoss = 2.6304, diffLoss = 12.8274, kgLoss = 0.0811
2025-02-13 01:13:51.586320: Training Step 7/59: batchLoss = 3.1218, diffLoss = 15.2493, kgLoss = 0.0899
2025-02-13 01:13:52.530205: Training Step 8/59: batchLoss = 2.6224, diffLoss = 12.8134, kgLoss = 0.0746
2025-02-13 01:13:53.456338: Training Step 9/59: batchLoss = 3.0494, diffLoss = 14.9075, kgLoss = 0.0849
2025-02-13 01:13:54.381123: Training Step 10/59: batchLoss = 2.7116, diffLoss = 13.2477, kgLoss = 0.0775
2025-02-13 01:13:55.303188: Training Step 11/59: batchLoss = 2.5914, diffLoss = 12.6195, kgLoss = 0.0844
2025-02-13 01:13:56.226562: Training Step 12/59: batchLoss = 2.5816, diffLoss = 12.6063, kgLoss = 0.0754
2025-02-13 01:13:57.152449: Training Step 13/59: batchLoss = 2.7954, diffLoss = 13.6164, kgLoss = 0.0901
2025-02-13 01:13:58.091571: Training Step 14/59: batchLoss = 3.2136, diffLoss = 15.6844, kgLoss = 0.0960
2025-02-13 01:13:59.015756: Training Step 15/59: batchLoss = 3.0635, diffLoss = 14.9621, kgLoss = 0.0888
2025-02-13 01:13:59.940550: Training Step 16/59: batchLoss = 2.9581, diffLoss = 14.4566, kgLoss = 0.0835
2025-02-13 01:14:00.858540: Training Step 17/59: batchLoss = 2.9943, diffLoss = 14.6491, kgLoss = 0.0806
2025-02-13 01:14:01.793710: Training Step 18/59: batchLoss = 2.8553, diffLoss = 13.9739, kgLoss = 0.0756
2025-02-13 01:14:02.729020: Training Step 19/59: batchLoss = 2.9677, diffLoss = 14.5139, kgLoss = 0.0812
2025-02-13 01:14:03.664042: Training Step 20/59: batchLoss = 2.9506, diffLoss = 14.4060, kgLoss = 0.0867
2025-02-13 01:14:04.603931: Training Step 21/59: batchLoss = 2.6618, diffLoss = 12.9839, kgLoss = 0.0813
2025-02-13 01:14:05.543885: Training Step 22/59: batchLoss = 3.2717, diffLoss = 15.9954, kgLoss = 0.0908
2025-02-13 01:14:06.481632: Training Step 23/59: batchLoss = 2.8016, diffLoss = 13.6650, kgLoss = 0.0857
2025-02-13 01:14:07.419759: Training Step 24/59: batchLoss = 2.8364, diffLoss = 13.8558, kgLoss = 0.0815
2025-02-13 01:14:08.356076: Training Step 25/59: batchLoss = 2.9770, diffLoss = 14.5356, kgLoss = 0.0874
2025-02-13 01:14:09.287963: Training Step 26/59: batchLoss = 2.7781, diffLoss = 13.5777, kgLoss = 0.0782
2025-02-13 01:14:10.220047: Training Step 27/59: batchLoss = 2.9366, diffLoss = 14.3180, kgLoss = 0.0913
2025-02-13 01:14:11.153352: Training Step 28/59: batchLoss = 3.0433, diffLoss = 14.8454, kgLoss = 0.0928
2025-02-13 01:14:12.088131: Training Step 29/59: batchLoss = 2.9069, diffLoss = 14.2032, kgLoss = 0.0829
2025-02-13 01:14:13.016473: Training Step 30/59: batchLoss = 3.0130, diffLoss = 14.7457, kgLoss = 0.0798
2025-02-13 01:14:13.947639: Training Step 31/59: batchLoss = 3.0233, diffLoss = 14.7498, kgLoss = 0.0917
2025-02-13 01:14:14.870129: Training Step 32/59: batchLoss = 3.2280, diffLoss = 15.7411, kgLoss = 0.0997
2025-02-13 01:14:15.801442: Training Step 33/59: batchLoss = 2.9056, diffLoss = 14.1814, kgLoss = 0.0866
2025-02-13 01:14:16.729912: Training Step 34/59: batchLoss = 3.2695, diffLoss = 15.9598, kgLoss = 0.0969
2025-02-13 01:14:17.662756: Training Step 35/59: batchLoss = 2.8977, diffLoss = 14.1757, kgLoss = 0.0782
2025-02-13 01:14:18.590802: Training Step 36/59: batchLoss = 3.1328, diffLoss = 15.3025, kgLoss = 0.0903
2025-02-13 01:14:19.518221: Training Step 37/59: batchLoss = 2.9291, diffLoss = 14.2956, kgLoss = 0.0875
2025-02-13 01:14:20.461651: Training Step 38/59: batchLoss = 2.8168, diffLoss = 13.7617, kgLoss = 0.0806
2025-02-13 01:14:21.384785: Training Step 39/59: batchLoss = 3.2612, diffLoss = 15.9522, kgLoss = 0.0884
2025-02-13 01:14:22.314114: Training Step 40/59: batchLoss = 3.3679, diffLoss = 16.4612, kgLoss = 0.0946
2025-02-13 01:14:23.247099: Training Step 41/59: batchLoss = 2.7805, diffLoss = 13.5838, kgLoss = 0.0797
2025-02-13 01:14:24.177625: Training Step 42/59: batchLoss = 2.7598, diffLoss = 13.4987, kgLoss = 0.0750
2025-02-13 01:14:25.110950: Training Step 43/59: batchLoss = 3.1042, diffLoss = 15.1556, kgLoss = 0.0914
2025-02-13 01:14:26.042723: Training Step 44/59: batchLoss = 2.8464, diffLoss = 13.9201, kgLoss = 0.0780
2025-02-13 01:14:26.976934: Training Step 45/59: batchLoss = 3.0780, diffLoss = 15.0339, kgLoss = 0.0890
2025-02-13 01:14:27.910365: Training Step 46/59: batchLoss = 3.3661, diffLoss = 16.4592, kgLoss = 0.0928
2025-02-13 01:14:28.847040: Training Step 47/59: batchLoss = 2.8607, diffLoss = 14.0029, kgLoss = 0.0751
2025-02-13 01:14:29.794148: Training Step 48/59: batchLoss = 3.7093, diffLoss = 18.1377, kgLoss = 0.1022
2025-02-13 01:14:30.726785: Training Step 49/59: batchLoss = 2.9653, diffLoss = 14.5160, kgLoss = 0.0776
2025-02-13 01:14:31.660195: Training Step 50/59: batchLoss = 2.7549, diffLoss = 13.4550, kgLoss = 0.0799
2025-02-13 01:14:32.596672: Training Step 51/59: batchLoss = 3.1951, diffLoss = 15.6133, kgLoss = 0.0905
2025-02-13 01:14:33.528728: Training Step 52/59: batchLoss = 3.0941, diffLoss = 15.1244, kgLoss = 0.0866
2025-02-13 01:14:34.460098: Training Step 53/59: batchLoss = 3.1011, diffLoss = 15.1400, kgLoss = 0.0914
2025-02-13 01:14:35.383532: Training Step 54/59: batchLoss = 2.9011, diffLoss = 14.1561, kgLoss = 0.0874
2025-02-13 01:14:36.310645: Training Step 55/59: batchLoss = 3.1799, diffLoss = 15.5324, kgLoss = 0.0918
2025-02-13 01:14:37.230612: Training Step 56/59: batchLoss = 2.7458, diffLoss = 13.4099, kgLoss = 0.0798
2025-02-13 01:14:38.077726: Training Step 57/59: batchLoss = 3.0494, diffLoss = 14.8923, kgLoss = 0.0887
2025-02-13 01:14:38.925789: Training Step 58/59: batchLoss = 3.0637, diffLoss = 14.9906, kgLoss = 0.0820
2025-02-13 01:14:39.023731: 
2025-02-13 01:14:39.024261: Epoch 103/1000, Train: epLoss = 0.4357, epDfLoss = 2.1280, epKgLoss = 0.0126  
2025-02-13 01:14:40.498377: Steps 0/47: batch_recall = 36.40, batch_ndcg = 48.24 
2025-02-13 01:14:41.801161: Steps 1/47: batch_recall = 36.35, batch_ndcg = 41.62 
2025-02-13 01:14:43.055311: Steps 2/47: batch_recall = 40.78, batch_ndcg = 47.40 
2025-02-13 01:14:44.319830: Steps 3/47: batch_recall = 43.31, batch_ndcg = 45.49 
2025-02-13 01:14:45.519734: Steps 4/47: batch_recall = 39.35, batch_ndcg = 45.35 
2025-02-13 01:14:46.743417: Steps 5/47: batch_recall = 34.03, batch_ndcg = 40.72 
2025-02-13 01:14:47.947038: Steps 6/47: batch_recall = 38.82, batch_ndcg = 41.95 
2025-02-13 01:14:49.107491: Steps 7/47: batch_recall = 44.65, batch_ndcg = 46.74 
2025-02-13 01:14:50.283081: Steps 8/47: batch_recall = 48.47, batch_ndcg = 52.85 
2025-02-13 01:14:51.413960: Steps 9/47: batch_recall = 47.12, batch_ndcg = 47.06 
2025-02-13 01:14:52.578762: Steps 10/47: batch_recall = 43.06, batch_ndcg = 43.91 
2025-02-13 01:14:53.703003: Steps 11/47: batch_recall = 54.16, batch_ndcg = 52.11 
2025-02-13 01:14:54.848769: Steps 12/47: batch_recall = 50.70, batch_ndcg = 49.51 
2025-02-13 01:14:55.979695: Steps 13/47: batch_recall = 48.30, batch_ndcg = 44.55 
2025-02-13 01:14:57.083668: Steps 14/47: batch_recall = 39.20, batch_ndcg = 40.12 
2025-02-13 01:14:58.180533: Steps 15/47: batch_recall = 57.45, batch_ndcg = 53.61 
2025-02-13 01:14:59.275703: Steps 16/47: batch_recall = 50.88, batch_ndcg = 47.44 
2025-02-13 01:15:00.336388: Steps 17/47: batch_recall = 58.41, batch_ndcg = 49.58 
2025-02-13 01:15:01.421135: Steps 18/47: batch_recall = 53.58, batch_ndcg = 49.75 
2025-02-13 01:15:02.499173: Steps 19/47: batch_recall = 61.05, batch_ndcg = 55.43 
2025-02-13 01:15:03.549878: Steps 20/47: batch_recall = 66.92, batch_ndcg = 61.27 
2025-02-13 01:15:04.599383: Steps 21/47: batch_recall = 67.09, batch_ndcg = 56.56 
2025-02-13 01:15:05.658052: Steps 22/47: batch_recall = 54.65, batch_ndcg = 50.68 
2025-02-13 01:15:06.697870: Steps 23/47: batch_recall = 65.22, batch_ndcg = 56.07 
2025-02-13 01:15:07.752942: Steps 24/47: batch_recall = 65.52, batch_ndcg = 59.13 
2025-02-13 01:15:08.796859: Steps 25/47: batch_recall = 67.36, batch_ndcg = 58.21 
2025-02-13 01:15:09.812822: Steps 26/47: batch_recall = 60.30, batch_ndcg = 52.97 
2025-02-13 01:15:10.843190: Steps 27/47: batch_recall = 62.05, batch_ndcg = 52.30 
2025-02-13 01:15:11.857791: Steps 28/47: batch_recall = 70.25, batch_ndcg = 59.56 
2025-02-13 01:15:12.865450: Steps 29/47: batch_recall = 70.97, batch_ndcg = 59.47 
2025-02-13 01:15:13.875116: Steps 30/47: batch_recall = 72.03, batch_ndcg = 63.38 
2025-02-13 01:15:14.912027: Steps 31/47: batch_recall = 67.45, batch_ndcg = 55.73 
2025-02-13 01:15:15.924706: Steps 32/47: batch_recall = 72.18, batch_ndcg = 64.92 
2025-02-13 01:15:16.945095: Steps 33/47: batch_recall = 79.68, batch_ndcg = 67.17 
2025-02-13 01:15:17.958511: Steps 34/47: batch_recall = 65.99, batch_ndcg = 55.69 
2025-02-13 01:15:18.955978: Steps 35/47: batch_recall = 77.03, batch_ndcg = 65.91 
2025-02-13 01:15:19.954259: Steps 36/47: batch_recall = 77.91, batch_ndcg = 64.28 
2025-02-13 01:15:20.940489: Steps 37/47: batch_recall = 84.19, batch_ndcg = 72.74 
2025-02-13 01:15:21.926724: Steps 38/47: batch_recall = 90.16, batch_ndcg = 72.69 
2025-02-13 01:15:22.909931: Steps 39/47: batch_recall = 88.28, batch_ndcg = 69.51 
2025-02-13 01:15:23.892900: Steps 40/47: batch_recall = 74.84, batch_ndcg = 63.33 
2025-02-13 01:15:24.860337: Steps 41/47: batch_recall = 87.99, batch_ndcg = 73.77 
2025-02-13 01:15:25.813775: Steps 42/47: batch_recall = 82.26, batch_ndcg = 64.43 
2025-02-13 01:15:26.770284: Steps 43/47: batch_recall = 89.38, batch_ndcg = 71.70 
2025-02-13 01:15:27.731377: Steps 44/47: batch_recall = 88.32, batch_ndcg = 70.60 
2025-02-13 01:15:28.681821: Steps 45/47: batch_recall = 92.57, batch_ndcg = 74.09 
2025-02-13 01:15:28.788919: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.66 
2025-02-13 01:15:28.789053: Epoch 103/1000, Test: Recall = 0.1217, NDCG = 0.1095  

2025-02-13 01:15:29.976862: Training Step 0/59: batchLoss = 2.6685, diffLoss = 13.0350, kgLoss = 0.0768
2025-02-13 01:15:30.903567: Training Step 1/59: batchLoss = 2.7757, diffLoss = 13.5406, kgLoss = 0.0845
2025-02-13 01:15:31.830643: Training Step 2/59: batchLoss = 2.9786, diffLoss = 14.5140, kgLoss = 0.0948
2025-02-13 01:15:32.749053: Training Step 3/59: batchLoss = 2.8762, diffLoss = 14.0508, kgLoss = 0.0826
2025-02-13 01:15:33.666217: Training Step 4/59: batchLoss = 2.8753, diffLoss = 14.0392, kgLoss = 0.0843
2025-02-13 01:15:34.593130: Training Step 5/59: batchLoss = 2.8832, diffLoss = 14.0852, kgLoss = 0.0827
2025-02-13 01:15:35.506167: Training Step 6/59: batchLoss = 2.7542, diffLoss = 13.4553, kgLoss = 0.0789
2025-02-13 01:15:36.427682: Training Step 7/59: batchLoss = 2.7673, diffLoss = 13.4956, kgLoss = 0.0852
2025-02-13 01:15:37.362671: Training Step 8/59: batchLoss = 2.7829, diffLoss = 13.5984, kgLoss = 0.0791
2025-02-13 01:15:38.299781: Training Step 9/59: batchLoss = 2.9203, diffLoss = 14.2591, kgLoss = 0.0856
2025-02-13 01:15:39.225835: Training Step 10/59: batchLoss = 3.0161, diffLoss = 14.7277, kgLoss = 0.0882
2025-02-13 01:15:40.163080: Training Step 11/59: batchLoss = 2.8790, diffLoss = 14.0729, kgLoss = 0.0806
2025-02-13 01:15:41.094663: Training Step 12/59: batchLoss = 2.4518, diffLoss = 11.9606, kgLoss = 0.0746
2025-02-13 01:15:42.025123: Training Step 13/59: batchLoss = 2.6770, diffLoss = 13.0892, kgLoss = 0.0740
2025-02-13 01:15:42.959379: Training Step 14/59: batchLoss = 3.2064, diffLoss = 15.6285, kgLoss = 0.1008
2025-02-13 01:15:43.896656: Training Step 15/59: batchLoss = 2.8634, diffLoss = 13.9709, kgLoss = 0.0865
2025-02-13 01:15:44.830652: Training Step 16/59: batchLoss = 2.8184, diffLoss = 13.7672, kgLoss = 0.0812
2025-02-13 01:15:45.761530: Training Step 17/59: batchLoss = 2.7102, diffLoss = 13.2103, kgLoss = 0.0852
2025-02-13 01:15:46.679819: Training Step 18/59: batchLoss = 2.9608, diffLoss = 14.4575, kgLoss = 0.0867
2025-02-13 01:15:47.612204: Training Step 19/59: batchLoss = 2.8915, diffLoss = 14.1112, kgLoss = 0.0866
2025-02-13 01:15:48.538588: Training Step 20/59: batchLoss = 3.1356, diffLoss = 15.3068, kgLoss = 0.0928
2025-02-13 01:15:49.470722: Training Step 21/59: batchLoss = 2.9317, diffLoss = 14.3169, kgLoss = 0.0854
2025-02-13 01:15:50.390226: Training Step 22/59: batchLoss = 2.8660, diffLoss = 13.9849, kgLoss = 0.0863
2025-02-13 01:15:51.331697: Training Step 23/59: batchLoss = 2.8627, diffLoss = 13.9839, kgLoss = 0.0824
2025-02-13 01:15:52.263403: Training Step 24/59: batchLoss = 3.0975, diffLoss = 15.1467, kgLoss = 0.0852
2025-02-13 01:15:53.192698: Training Step 25/59: batchLoss = 2.6483, diffLoss = 12.9044, kgLoss = 0.0843
2025-02-13 01:15:54.128332: Training Step 26/59: batchLoss = 3.0219, diffLoss = 14.7647, kgLoss = 0.0863
2025-02-13 01:15:55.051652: Training Step 27/59: batchLoss = 2.6494, diffLoss = 12.9308, kgLoss = 0.0790
2025-02-13 01:15:55.976372: Training Step 28/59: batchLoss = 2.8047, diffLoss = 13.6887, kgLoss = 0.0838
2025-02-13 01:15:56.893291: Training Step 29/59: batchLoss = 2.9995, diffLoss = 14.6300, kgLoss = 0.0918
2025-02-13 01:15:57.818608: Training Step 30/59: batchLoss = 2.8520, diffLoss = 13.9079, kgLoss = 0.0880
2025-02-13 01:15:58.762640: Training Step 31/59: batchLoss = 3.1869, diffLoss = 15.5682, kgLoss = 0.0916
2025-02-13 01:15:59.701450: Training Step 32/59: batchLoss = 2.8723, diffLoss = 14.0283, kgLoss = 0.0833
2025-02-13 01:16:00.632381: Training Step 33/59: batchLoss = 2.7947, diffLoss = 13.6486, kgLoss = 0.0812
2025-02-13 01:16:01.568131: Training Step 34/59: batchLoss = 3.2193, diffLoss = 15.7173, kgLoss = 0.0948
2025-02-13 01:16:02.504121: Training Step 35/59: batchLoss = 3.1180, diffLoss = 15.2133, kgLoss = 0.0941
2025-02-13 01:16:03.438105: Training Step 36/59: batchLoss = 3.1074, diffLoss = 15.2014, kgLoss = 0.0838
2025-02-13 01:16:04.371852: Training Step 37/59: batchLoss = 2.9147, diffLoss = 14.2545, kgLoss = 0.0798
2025-02-13 01:16:05.311297: Training Step 38/59: batchLoss = 3.1251, diffLoss = 15.2681, kgLoss = 0.0894
2025-02-13 01:16:06.252919: Training Step 39/59: batchLoss = 2.9694, diffLoss = 14.5245, kgLoss = 0.0806
2025-02-13 01:16:07.194686: Training Step 40/59: batchLoss = 2.7988, diffLoss = 13.6742, kgLoss = 0.0800
2025-02-13 01:16:08.123156: Training Step 41/59: batchLoss = 3.0619, diffLoss = 14.9642, kgLoss = 0.0864
2025-02-13 01:16:09.049520: Training Step 42/59: batchLoss = 3.2146, diffLoss = 15.6898, kgLoss = 0.0958
2025-02-13 01:16:09.977130: Training Step 43/59: batchLoss = 3.1566, diffLoss = 15.3928, kgLoss = 0.0975
2025-02-13 01:16:10.904882: Training Step 44/59: batchLoss = 3.1743, diffLoss = 15.5172, kgLoss = 0.0886
2025-02-13 01:16:11.833633: Training Step 45/59: batchLoss = 3.3694, diffLoss = 16.4315, kgLoss = 0.1039
2025-02-13 01:16:12.763769: Training Step 46/59: batchLoss = 2.9412, diffLoss = 14.3795, kgLoss = 0.0816
2025-02-13 01:16:13.693482: Training Step 47/59: batchLoss = 2.9622, diffLoss = 14.4892, kgLoss = 0.0804
2025-02-13 01:16:14.625799: Training Step 48/59: batchLoss = 3.2798, diffLoss = 16.0323, kgLoss = 0.0917
2025-02-13 01:16:15.555858: Training Step 49/59: batchLoss = 2.8921, diffLoss = 14.1388, kgLoss = 0.0804
2025-02-13 01:16:16.482358: Training Step 50/59: batchLoss = 3.0637, diffLoss = 14.9603, kgLoss = 0.0896
2025-02-13 01:16:17.404048: Training Step 51/59: batchLoss = 3.1796, diffLoss = 15.5537, kgLoss = 0.0861
2025-02-13 01:16:18.330043: Training Step 52/59: batchLoss = 2.7498, diffLoss = 13.4312, kgLoss = 0.0794
2025-02-13 01:16:19.262416: Training Step 53/59: batchLoss = 2.9036, diffLoss = 14.1895, kgLoss = 0.0822
2025-02-13 01:16:20.198684: Training Step 54/59: batchLoss = 2.7846, diffLoss = 13.6048, kgLoss = 0.0795
2025-02-13 01:16:21.126412: Training Step 55/59: batchLoss = 3.0832, diffLoss = 15.0942, kgLoss = 0.0804
2025-02-13 01:16:22.062495: Training Step 56/59: batchLoss = 2.8212, diffLoss = 13.7667, kgLoss = 0.0848
2025-02-13 01:16:22.910765: Training Step 57/59: batchLoss = 2.8370, diffLoss = 13.8790, kgLoss = 0.0765
2025-02-13 01:16:23.768500: Training Step 58/59: batchLoss = 3.4015, diffLoss = 16.6133, kgLoss = 0.0986
2025-02-13 01:16:23.868052: 
2025-02-13 01:16:23.868624: Epoch 104/1000, Train: epLoss = 0.4340, epDfLoss = 2.1197, epKgLoss = 0.0126  
2025-02-13 01:16:25.357731: Steps 0/47: batch_recall = 37.76, batch_ndcg = 48.47 
2025-02-13 01:16:26.672604: Steps 1/47: batch_recall = 36.89, batch_ndcg = 42.33 
2025-02-13 01:16:27.932424: Steps 2/47: batch_recall = 40.50, batch_ndcg = 46.96 
2025-02-13 01:16:29.186552: Steps 3/47: batch_recall = 44.03, batch_ndcg = 46.77 
2025-02-13 01:16:30.372125: Steps 4/47: batch_recall = 39.13, batch_ndcg = 45.27 
2025-02-13 01:16:31.582923: Steps 5/47: batch_recall = 33.62, batch_ndcg = 40.88 
2025-02-13 01:16:32.774536: Steps 6/47: batch_recall = 38.05, batch_ndcg = 41.44 
2025-02-13 01:16:33.937282: Steps 7/47: batch_recall = 44.33, batch_ndcg = 46.13 
2025-02-13 01:16:35.104353: Steps 8/47: batch_recall = 47.80, batch_ndcg = 53.03 
2025-02-13 01:16:36.242641: Steps 9/47: batch_recall = 48.07, batch_ndcg = 46.91 
2025-02-13 01:16:37.402442: Steps 10/47: batch_recall = 43.62, batch_ndcg = 43.79 
2025-02-13 01:16:38.527025: Steps 11/47: batch_recall = 55.28, batch_ndcg = 53.19 
2025-02-13 01:16:39.662538: Steps 12/47: batch_recall = 50.37, batch_ndcg = 49.36 
2025-02-13 01:16:40.802813: Steps 13/47: batch_recall = 48.36, batch_ndcg = 45.55 
2025-02-13 01:16:41.909854: Steps 14/47: batch_recall = 40.98, batch_ndcg = 40.71 
2025-02-13 01:16:42.999524: Steps 15/47: batch_recall = 57.70, batch_ndcg = 53.68 
2025-02-13 01:16:44.095386: Steps 16/47: batch_recall = 50.91, batch_ndcg = 47.94 
2025-02-13 01:16:45.131252: Steps 17/47: batch_recall = 57.52, batch_ndcg = 49.30 
2025-02-13 01:16:46.207869: Steps 18/47: batch_recall = 53.34, batch_ndcg = 49.68 
2025-02-13 01:16:47.272193: Steps 19/47: batch_recall = 60.12, batch_ndcg = 55.04 
2025-02-13 01:16:48.318256: Steps 20/47: batch_recall = 66.46, batch_ndcg = 61.02 
2025-02-13 01:16:49.360665: Steps 21/47: batch_recall = 65.41, batch_ndcg = 56.86 
2025-02-13 01:16:50.410669: Steps 22/47: batch_recall = 58.01, batch_ndcg = 51.81 
2025-02-13 01:16:51.443939: Steps 23/47: batch_recall = 64.79, batch_ndcg = 56.49 
2025-02-13 01:16:52.500027: Steps 24/47: batch_recall = 66.05, batch_ndcg = 59.30 
2025-02-13 01:16:53.535070: Steps 25/47: batch_recall = 67.21, batch_ndcg = 57.84 
2025-02-13 01:16:54.531279: Steps 26/47: batch_recall = 60.74, batch_ndcg = 53.93 
2025-02-13 01:16:55.564982: Steps 27/47: batch_recall = 61.53, batch_ndcg = 51.97 
2025-02-13 01:16:56.573922: Steps 28/47: batch_recall = 70.67, batch_ndcg = 59.72 
2025-02-13 01:16:57.599451: Steps 29/47: batch_recall = 70.69, batch_ndcg = 59.13 
2025-02-13 01:16:58.604918: Steps 30/47: batch_recall = 71.92, batch_ndcg = 63.36 
2025-02-13 01:16:59.638253: Steps 31/47: batch_recall = 67.63, batch_ndcg = 56.16 
2025-02-13 01:17:00.647641: Steps 32/47: batch_recall = 72.94, batch_ndcg = 66.08 
2025-02-13 01:17:01.659043: Steps 33/47: batch_recall = 77.44, batch_ndcg = 66.30 
2025-02-13 01:17:02.677963: Steps 34/47: batch_recall = 69.27, batch_ndcg = 57.83 
2025-02-13 01:17:03.673376: Steps 35/47: batch_recall = 77.66, batch_ndcg = 65.62 
2025-02-13 01:17:04.665641: Steps 36/47: batch_recall = 78.29, batch_ndcg = 64.82 
2025-02-13 01:17:05.653360: Steps 37/47: batch_recall = 84.61, batch_ndcg = 72.52 
2025-02-13 01:17:06.640098: Steps 38/47: batch_recall = 90.36, batch_ndcg = 72.57 
2025-02-13 01:17:07.624907: Steps 39/47: batch_recall = 87.71, batch_ndcg = 69.09 
2025-02-13 01:17:08.605520: Steps 40/47: batch_recall = 74.32, batch_ndcg = 63.23 
2025-02-13 01:17:09.576491: Steps 41/47: batch_recall = 90.31, batch_ndcg = 73.92 
2025-02-13 01:17:10.546739: Steps 42/47: batch_recall = 83.29, batch_ndcg = 64.79 
2025-02-13 01:17:11.505570: Steps 43/47: batch_recall = 90.89, batch_ndcg = 73.68 
2025-02-13 01:17:12.470935: Steps 44/47: batch_recall = 89.47, batch_ndcg = 71.60 
2025-02-13 01:17:13.407002: Steps 45/47: batch_recall = 92.16, batch_ndcg = 74.49 
2025-02-13 01:17:13.508243: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.86 
2025-02-13 01:17:13.508373: Epoch 104/1000, Test: Recall = 0.1222, NDCG = 0.1100  

2025-02-13 01:17:14.692054: Training Step 0/59: batchLoss = 3.1439, diffLoss = 15.3421, kgLoss = 0.0944
2025-02-13 01:17:15.619064: Training Step 1/59: batchLoss = 2.7866, diffLoss = 13.5687, kgLoss = 0.0911
2025-02-13 01:17:16.541305: Training Step 2/59: batchLoss = 2.9961, diffLoss = 14.6406, kgLoss = 0.0850
2025-02-13 01:17:17.468164: Training Step 3/59: batchLoss = 2.9600, diffLoss = 14.4510, kgLoss = 0.0872
2025-02-13 01:17:18.392845: Training Step 4/59: batchLoss = 2.8002, diffLoss = 13.6571, kgLoss = 0.0860
2025-02-13 01:17:19.318036: Training Step 5/59: batchLoss = 2.5553, diffLoss = 12.4728, kgLoss = 0.0759
2025-02-13 01:17:20.249281: Training Step 6/59: batchLoss = 2.5511, diffLoss = 12.4392, kgLoss = 0.0791
2025-02-13 01:17:21.178904: Training Step 7/59: batchLoss = 2.9430, diffLoss = 14.3552, kgLoss = 0.0900
2025-02-13 01:17:22.105806: Training Step 8/59: batchLoss = 2.7255, diffLoss = 13.2850, kgLoss = 0.0856
2025-02-13 01:17:23.040220: Training Step 9/59: batchLoss = 2.9716, diffLoss = 14.4966, kgLoss = 0.0903
2025-02-13 01:17:23.976643: Training Step 10/59: batchLoss = 2.7312, diffLoss = 13.3274, kgLoss = 0.0821
2025-02-13 01:17:24.912099: Training Step 11/59: batchLoss = 2.6194, diffLoss = 12.7734, kgLoss = 0.0809
2025-02-13 01:17:25.842438: Training Step 12/59: batchLoss = 2.7752, diffLoss = 13.5303, kgLoss = 0.0864
2025-02-13 01:17:26.776544: Training Step 13/59: batchLoss = 2.7244, diffLoss = 13.2890, kgLoss = 0.0833
2025-02-13 01:17:27.709648: Training Step 14/59: batchLoss = 2.6236, diffLoss = 12.7988, kgLoss = 0.0799
2025-02-13 01:17:28.642626: Training Step 15/59: batchLoss = 2.8915, diffLoss = 14.1028, kgLoss = 0.0887
2025-02-13 01:17:29.579823: Training Step 16/59: batchLoss = 3.2668, diffLoss = 15.9761, kgLoss = 0.0895
2025-02-13 01:17:30.517994: Training Step 17/59: batchLoss = 3.0384, diffLoss = 14.8100, kgLoss = 0.0955
2025-02-13 01:17:31.453557: Training Step 18/59: batchLoss = 2.8976, diffLoss = 14.1481, kgLoss = 0.0850
2025-02-13 01:17:32.378565: Training Step 19/59: batchLoss = 2.8738, diffLoss = 14.0286, kgLoss = 0.0851
2025-02-13 01:17:33.308467: Training Step 20/59: batchLoss = 2.9784, diffLoss = 14.5593, kgLoss = 0.0832
2025-02-13 01:17:34.232914: Training Step 21/59: batchLoss = 2.9205, diffLoss = 14.2698, kgLoss = 0.0831
2025-02-13 01:17:35.155648: Training Step 22/59: batchLoss = 2.9260, diffLoss = 14.3115, kgLoss = 0.0796
2025-02-13 01:17:36.081995: Training Step 23/59: batchLoss = 2.6700, diffLoss = 13.0622, kgLoss = 0.0719
2025-02-13 01:17:37.006167: Training Step 24/59: batchLoss = 3.1124, diffLoss = 15.2035, kgLoss = 0.0896
2025-02-13 01:17:37.938119: Training Step 25/59: batchLoss = 3.2607, diffLoss = 15.9097, kgLoss = 0.0984
2025-02-13 01:17:38.862228: Training Step 26/59: batchLoss = 3.1990, diffLoss = 15.6100, kgLoss = 0.0963
2025-02-13 01:17:39.783746: Training Step 27/59: batchLoss = 3.4713, diffLoss = 16.8959, kgLoss = 0.1151
2025-02-13 01:17:40.704670: Training Step 28/59: batchLoss = 2.7539, diffLoss = 13.4249, kgLoss = 0.0862
2025-02-13 01:17:41.624030: Training Step 29/59: batchLoss = 2.8552, diffLoss = 13.9435, kgLoss = 0.0832
2025-02-13 01:17:42.544642: Training Step 30/59: batchLoss = 2.9536, diffLoss = 14.4303, kgLoss = 0.0844
2025-02-13 01:17:43.476679: Training Step 31/59: batchLoss = 2.8731, diffLoss = 14.0206, kgLoss = 0.0862
2025-02-13 01:17:44.412768: Training Step 32/59: batchLoss = 3.1812, diffLoss = 15.5503, kgLoss = 0.0889
2025-02-13 01:17:45.341961: Training Step 33/59: batchLoss = 2.8987, diffLoss = 14.1603, kgLoss = 0.0833
2025-02-13 01:17:46.272182: Training Step 34/59: batchLoss = 2.9712, diffLoss = 14.5211, kgLoss = 0.0838
2025-02-13 01:17:47.212426: Training Step 35/59: batchLoss = 3.2855, diffLoss = 16.0696, kgLoss = 0.0895
2025-02-13 01:17:48.145810: Training Step 36/59: batchLoss = 3.2966, diffLoss = 16.1147, kgLoss = 0.0921
2025-02-13 01:17:49.076890: Training Step 37/59: batchLoss = 3.1172, diffLoss = 15.2155, kgLoss = 0.0926
2025-02-13 01:17:50.014412: Training Step 38/59: batchLoss = 2.9889, diffLoss = 14.6169, kgLoss = 0.0819
2025-02-13 01:17:50.966689: Training Step 39/59: batchLoss = 2.7145, diffLoss = 13.2834, kgLoss = 0.0723
2025-02-13 01:17:51.903645: Training Step 40/59: batchLoss = 3.0817, diffLoss = 15.0502, kgLoss = 0.0896
2025-02-13 01:17:52.831702: Training Step 41/59: batchLoss = 3.0819, diffLoss = 15.0647, kgLoss = 0.0862
2025-02-13 01:17:53.762549: Training Step 42/59: batchLoss = 2.4988, diffLoss = 12.1912, kgLoss = 0.0757
2025-02-13 01:17:54.688880: Training Step 43/59: batchLoss = 2.6370, diffLoss = 12.8991, kgLoss = 0.0715
2025-02-13 01:17:55.618204: Training Step 44/59: batchLoss = 2.8749, diffLoss = 14.0391, kgLoss = 0.0839
2025-02-13 01:17:56.548314: Training Step 45/59: batchLoss = 2.9064, diffLoss = 14.2123, kgLoss = 0.0800
2025-02-13 01:17:57.471002: Training Step 46/59: batchLoss = 2.9842, diffLoss = 14.5954, kgLoss = 0.0814
2025-02-13 01:17:58.401676: Training Step 47/59: batchLoss = 2.9136, diffLoss = 14.2310, kgLoss = 0.0842
2025-02-13 01:17:59.339119: Training Step 48/59: batchLoss = 3.0539, diffLoss = 14.9554, kgLoss = 0.0786
2025-02-13 01:18:00.285207: Training Step 49/59: batchLoss = 3.2158, diffLoss = 15.7266, kgLoss = 0.0882
2025-02-13 01:18:01.210728: Training Step 50/59: batchLoss = 3.1337, diffLoss = 15.3375, kgLoss = 0.0827
2025-02-13 01:18:02.148811: Training Step 51/59: batchLoss = 3.1265, diffLoss = 15.2519, kgLoss = 0.0951
2025-02-13 01:18:03.076010: Training Step 52/59: batchLoss = 2.6669, diffLoss = 13.0488, kgLoss = 0.0715
2025-02-13 01:18:04.005070: Training Step 53/59: batchLoss = 3.0818, diffLoss = 15.0767, kgLoss = 0.0830
2025-02-13 01:18:04.931297: Training Step 54/59: batchLoss = 2.9125, diffLoss = 14.2500, kgLoss = 0.0781
2025-02-13 01:18:05.867682: Training Step 55/59: batchLoss = 3.0370, diffLoss = 14.8537, kgLoss = 0.0829
2025-02-13 01:18:06.794915: Training Step 56/59: batchLoss = 3.0580, diffLoss = 14.9068, kgLoss = 0.0957
2025-02-13 01:18:07.644214: Training Step 57/59: batchLoss = 3.0113, diffLoss = 14.7283, kgLoss = 0.0821
2025-02-13 01:18:08.503430: Training Step 58/59: batchLoss = 3.1089, diffLoss = 15.1717, kgLoss = 0.0932
2025-02-13 01:18:08.608005: 
2025-02-13 01:18:08.608581: Epoch 105/1000, Train: epLoss = 0.4342, epDfLoss = 2.1206, epKgLoss = 0.0126  
2025-02-13 01:18:10.087003: Steps 0/47: batch_recall = 36.89, batch_ndcg = 47.58 
2025-02-13 01:18:11.392652: Steps 1/47: batch_recall = 36.80, batch_ndcg = 42.12 
2025-02-13 01:18:12.659042: Steps 2/47: batch_recall = 41.25, batch_ndcg = 46.77 
2025-02-13 01:18:13.920717: Steps 3/47: batch_recall = 43.47, batch_ndcg = 45.62 
2025-02-13 01:18:15.113479: Steps 4/47: batch_recall = 39.22, batch_ndcg = 45.36 
2025-02-13 01:18:16.317824: Steps 5/47: batch_recall = 34.26, batch_ndcg = 41.06 
2025-02-13 01:18:17.504965: Steps 6/47: batch_recall = 38.65, batch_ndcg = 41.71 
2025-02-13 01:18:18.674792: Steps 7/47: batch_recall = 44.11, batch_ndcg = 46.05 
2025-02-13 01:18:19.843234: Steps 8/47: batch_recall = 48.30, batch_ndcg = 51.96 
2025-02-13 01:18:20.973474: Steps 9/47: batch_recall = 47.66, batch_ndcg = 46.83 
2025-02-13 01:18:22.134186: Steps 10/47: batch_recall = 42.88, batch_ndcg = 43.19 
2025-02-13 01:18:23.261435: Steps 11/47: batch_recall = 55.06, batch_ndcg = 52.53 
2025-02-13 01:18:24.392941: Steps 12/47: batch_recall = 50.22, batch_ndcg = 49.18 
2025-02-13 01:18:25.524128: Steps 13/47: batch_recall = 48.14, batch_ndcg = 45.80 
2025-02-13 01:18:26.626541: Steps 14/47: batch_recall = 40.88, batch_ndcg = 40.98 
2025-02-13 01:18:27.726374: Steps 15/47: batch_recall = 56.53, batch_ndcg = 53.60 
2025-02-13 01:18:28.812246: Steps 16/47: batch_recall = 50.86, batch_ndcg = 47.22 
2025-02-13 01:18:29.873662: Steps 17/47: batch_recall = 58.80, batch_ndcg = 49.78 
2025-02-13 01:18:30.962720: Steps 18/47: batch_recall = 52.36, batch_ndcg = 49.39 
2025-02-13 01:18:32.032029: Steps 19/47: batch_recall = 60.58, batch_ndcg = 55.06 
2025-02-13 01:18:33.092619: Steps 20/47: batch_recall = 65.56, batch_ndcg = 60.50 
2025-02-13 01:18:34.147651: Steps 21/47: batch_recall = 65.90, batch_ndcg = 55.91 
2025-02-13 01:18:35.205346: Steps 22/47: batch_recall = 56.87, batch_ndcg = 52.26 
2025-02-13 01:18:36.253870: Steps 23/47: batch_recall = 62.83, batch_ndcg = 55.29 
2025-02-13 01:18:37.319855: Steps 24/47: batch_recall = 64.44, batch_ndcg = 58.45 
2025-02-13 01:18:38.356140: Steps 25/47: batch_recall = 68.26, batch_ndcg = 58.07 
2025-02-13 01:18:39.362314: Steps 26/47: batch_recall = 61.26, batch_ndcg = 54.48 
2025-02-13 01:18:40.394409: Steps 27/47: batch_recall = 63.02, batch_ndcg = 52.76 
2025-02-13 01:18:41.404790: Steps 28/47: batch_recall = 70.21, batch_ndcg = 60.47 
2025-02-13 01:18:42.408201: Steps 29/47: batch_recall = 70.41, batch_ndcg = 59.81 
2025-02-13 01:18:43.407435: Steps 30/47: batch_recall = 71.61, batch_ndcg = 63.40 
2025-02-13 01:18:44.427530: Steps 31/47: batch_recall = 66.55, batch_ndcg = 55.33 
2025-02-13 01:18:45.434910: Steps 32/47: batch_recall = 73.21, batch_ndcg = 66.32 
2025-02-13 01:18:46.458340: Steps 33/47: batch_recall = 79.87, batch_ndcg = 67.48 
2025-02-13 01:18:47.481214: Steps 34/47: batch_recall = 70.35, batch_ndcg = 57.05 
2025-02-13 01:18:48.471732: Steps 35/47: batch_recall = 77.63, batch_ndcg = 66.01 
2025-02-13 01:18:49.469213: Steps 36/47: batch_recall = 78.59, batch_ndcg = 64.62 
2025-02-13 01:18:50.455596: Steps 37/47: batch_recall = 84.30, batch_ndcg = 72.66 
2025-02-13 01:18:51.434452: Steps 38/47: batch_recall = 89.45, batch_ndcg = 72.65 
2025-02-13 01:18:52.405866: Steps 39/47: batch_recall = 88.82, batch_ndcg = 69.81 
2025-02-13 01:18:53.381304: Steps 40/47: batch_recall = 73.62, batch_ndcg = 63.00 
2025-02-13 01:18:54.346024: Steps 41/47: batch_recall = 89.92, batch_ndcg = 73.76 
2025-02-13 01:18:55.317829: Steps 42/47: batch_recall = 83.51, batch_ndcg = 65.92 
2025-02-13 01:18:56.281945: Steps 43/47: batch_recall = 90.20, batch_ndcg = 72.36 
2025-02-13 01:18:57.243155: Steps 44/47: batch_recall = 88.40, batch_ndcg = 71.22 
2025-02-13 01:18:58.177191: Steps 45/47: batch_recall = 93.60, batch_ndcg = 74.63 
2025-02-13 01:18:58.284012: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.86 
2025-02-13 01:18:58.284151: Epoch 105/1000, Test: Recall = 0.1221, NDCG = 0.1098  

2025-02-13 01:18:59.524564: Training Step 0/59: batchLoss = 2.8006, diffLoss = 13.6484, kgLoss = 0.0887
2025-02-13 01:19:00.447642: Training Step 1/59: batchLoss = 2.7004, diffLoss = 13.1937, kgLoss = 0.0770
2025-02-13 01:19:01.371643: Training Step 2/59: batchLoss = 2.9502, diffLoss = 14.4029, kgLoss = 0.0871
2025-02-13 01:19:02.295557: Training Step 3/59: batchLoss = 2.7840, diffLoss = 13.6000, kgLoss = 0.0800
2025-02-13 01:19:03.220747: Training Step 4/59: batchLoss = 2.8149, diffLoss = 13.7585, kgLoss = 0.0790
2025-02-13 01:19:04.145407: Training Step 5/59: batchLoss = 2.8770, diffLoss = 14.0017, kgLoss = 0.0959
2025-02-13 01:19:05.068565: Training Step 6/59: batchLoss = 3.1988, diffLoss = 15.6122, kgLoss = 0.0954
2025-02-13 01:19:05.992873: Training Step 7/59: batchLoss = 2.6409, diffLoss = 12.8960, kgLoss = 0.0771
2025-02-13 01:19:06.919175: Training Step 8/59: batchLoss = 2.9302, diffLoss = 14.2950, kgLoss = 0.0889
2025-02-13 01:19:07.845931: Training Step 9/59: batchLoss = 3.1540, diffLoss = 15.3943, kgLoss = 0.0939
2025-02-13 01:19:08.773680: Training Step 10/59: batchLoss = 2.7128, diffLoss = 13.2251, kgLoss = 0.0847
2025-02-13 01:19:09.697339: Training Step 11/59: batchLoss = 2.6693, diffLoss = 13.0334, kgLoss = 0.0783
2025-02-13 01:19:10.622167: Training Step 12/59: batchLoss = 2.7746, diffLoss = 13.5265, kgLoss = 0.0866
2025-02-13 01:19:11.542462: Training Step 13/59: batchLoss = 2.9280, diffLoss = 14.2969, kgLoss = 0.0857
2025-02-13 01:19:12.467427: Training Step 14/59: batchLoss = 2.7613, diffLoss = 13.4704, kgLoss = 0.0840
2025-02-13 01:19:13.390796: Training Step 15/59: batchLoss = 3.0808, diffLoss = 15.0482, kgLoss = 0.0889
2025-02-13 01:19:14.318406: Training Step 16/59: batchLoss = 2.7783, diffLoss = 13.6023, kgLoss = 0.0723
2025-02-13 01:19:15.244565: Training Step 17/59: batchLoss = 2.9318, diffLoss = 14.2923, kgLoss = 0.0916
2025-02-13 01:19:16.170081: Training Step 18/59: batchLoss = 2.9220, diffLoss = 14.2680, kgLoss = 0.0855
2025-02-13 01:19:17.096341: Training Step 19/59: batchLoss = 2.8722, diffLoss = 14.0425, kgLoss = 0.0796
2025-02-13 01:19:18.033817: Training Step 20/59: batchLoss = 2.8079, diffLoss = 13.7232, kgLoss = 0.0791
2025-02-13 01:19:18.971574: Training Step 21/59: batchLoss = 2.9122, diffLoss = 14.2077, kgLoss = 0.0883
2025-02-13 01:19:19.905531: Training Step 22/59: batchLoss = 2.9781, diffLoss = 14.5556, kgLoss = 0.0837
2025-02-13 01:19:20.843367: Training Step 23/59: batchLoss = 3.1402, diffLoss = 15.3303, kgLoss = 0.0927
2025-02-13 01:19:21.787329: Training Step 24/59: batchLoss = 3.0539, diffLoss = 14.9405, kgLoss = 0.0823
2025-02-13 01:19:22.719138: Training Step 25/59: batchLoss = 3.1390, diffLoss = 15.3308, kgLoss = 0.0911
2025-02-13 01:19:23.658147: Training Step 26/59: batchLoss = 2.8799, diffLoss = 14.0601, kgLoss = 0.0848
2025-02-13 01:19:24.593339: Training Step 27/59: batchLoss = 2.9499, diffLoss = 14.3623, kgLoss = 0.0969
2025-02-13 01:19:25.534996: Training Step 28/59: batchLoss = 2.9984, diffLoss = 14.6819, kgLoss = 0.0775
2025-02-13 01:19:26.464604: Training Step 29/59: batchLoss = 3.0523, diffLoss = 14.9141, kgLoss = 0.0869
2025-02-13 01:19:27.384231: Training Step 30/59: batchLoss = 2.9351, diffLoss = 14.3115, kgLoss = 0.0910
2025-02-13 01:19:28.316560: Training Step 31/59: batchLoss = 2.8488, diffLoss = 13.9292, kgLoss = 0.0788
2025-02-13 01:19:29.248457: Training Step 32/59: batchLoss = 2.8338, diffLoss = 13.8222, kgLoss = 0.0867
2025-02-13 01:19:30.181500: Training Step 33/59: batchLoss = 2.6131, diffLoss = 12.7760, kgLoss = 0.0724
2025-02-13 01:19:31.104967: Training Step 34/59: batchLoss = 2.9745, diffLoss = 14.5210, kgLoss = 0.0879
2025-02-13 01:19:32.041159: Training Step 35/59: batchLoss = 3.0213, diffLoss = 14.7536, kgLoss = 0.0883
2025-02-13 01:19:32.966866: Training Step 36/59: batchLoss = 2.8213, diffLoss = 13.7756, kgLoss = 0.0828
2025-02-13 01:19:33.898128: Training Step 37/59: batchLoss = 3.4195, diffLoss = 16.7232, kgLoss = 0.0936
2025-02-13 01:19:34.827353: Training Step 38/59: batchLoss = 3.1129, diffLoss = 15.2024, kgLoss = 0.0905
2025-02-13 01:19:35.757305: Training Step 39/59: batchLoss = 2.6959, diffLoss = 13.1918, kgLoss = 0.0719
2025-02-13 01:19:36.687806: Training Step 40/59: batchLoss = 2.8244, diffLoss = 13.7836, kgLoss = 0.0846
2025-02-13 01:19:37.620594: Training Step 41/59: batchLoss = 2.9305, diffLoss = 14.3207, kgLoss = 0.0829
2025-02-13 01:19:38.553241: Training Step 42/59: batchLoss = 3.0937, diffLoss = 15.1396, kgLoss = 0.0822
2025-02-13 01:19:39.487463: Training Step 43/59: batchLoss = 2.9404, diffLoss = 14.3749, kgLoss = 0.0818
2025-02-13 01:19:40.420838: Training Step 44/59: batchLoss = 3.0022, diffLoss = 14.6932, kgLoss = 0.0795
2025-02-13 01:19:41.356561: Training Step 45/59: batchLoss = 2.7623, diffLoss = 13.4881, kgLoss = 0.0808
2025-02-13 01:19:42.287559: Training Step 46/59: batchLoss = 3.2626, diffLoss = 15.9380, kgLoss = 0.0937
2025-02-13 01:19:43.221208: Training Step 47/59: batchLoss = 3.3825, diffLoss = 16.5091, kgLoss = 0.1008
2025-02-13 01:19:44.154756: Training Step 48/59: batchLoss = 3.0323, diffLoss = 14.8286, kgLoss = 0.0832
2025-02-13 01:19:45.091482: Training Step 49/59: batchLoss = 3.0950, diffLoss = 15.1443, kgLoss = 0.0827
2025-02-13 01:19:46.012904: Training Step 50/59: batchLoss = 2.9892, diffLoss = 14.6149, kgLoss = 0.0827
2025-02-13 01:19:46.940125: Training Step 51/59: batchLoss = 3.2322, diffLoss = 15.7804, kgLoss = 0.0952
2025-02-13 01:19:47.865831: Training Step 52/59: batchLoss = 3.0232, diffLoss = 14.7727, kgLoss = 0.0859
2025-02-13 01:19:48.793971: Training Step 53/59: batchLoss = 3.2666, diffLoss = 15.9796, kgLoss = 0.0884
2025-02-13 01:19:49.721775: Training Step 54/59: batchLoss = 3.0825, diffLoss = 15.0591, kgLoss = 0.0884
2025-02-13 01:19:50.646996: Training Step 55/59: batchLoss = 3.2669, diffLoss = 15.9756, kgLoss = 0.0897
2025-02-13 01:19:51.565690: Training Step 56/59: batchLoss = 2.9481, diffLoss = 14.4045, kgLoss = 0.0840
2025-02-13 01:19:52.408767: Training Step 57/59: batchLoss = 2.7868, diffLoss = 13.5988, kgLoss = 0.0838
2025-02-13 01:19:53.257862: Training Step 58/59: batchLoss = 3.0984, diffLoss = 15.1398, kgLoss = 0.0880
2025-02-13 01:19:53.355079: 
2025-02-13 01:19:53.355410: Epoch 106/1000, Train: epLoss = 0.4362, epDfLoss = 2.1307, epKgLoss = 0.0126  
2025-02-13 01:19:54.831272: Steps 0/47: batch_recall = 36.71, batch_ndcg = 47.53 
2025-02-13 01:19:56.130859: Steps 1/47: batch_recall = 36.88, batch_ndcg = 42.08 
2025-02-13 01:19:57.399600: Steps 2/47: batch_recall = 40.18, batch_ndcg = 47.64 
2025-02-13 01:19:58.666532: Steps 3/47: batch_recall = 44.23, batch_ndcg = 46.69 
2025-02-13 01:19:59.861065: Steps 4/47: batch_recall = 38.98, batch_ndcg = 44.90 
2025-02-13 01:20:01.076855: Steps 5/47: batch_recall = 33.64, batch_ndcg = 39.95 
2025-02-13 01:20:02.285905: Steps 6/47: batch_recall = 38.81, batch_ndcg = 42.20 
2025-02-13 01:20:03.467509: Steps 7/47: batch_recall = 44.66, batch_ndcg = 46.39 
2025-02-13 01:20:04.642597: Steps 8/47: batch_recall = 48.09, batch_ndcg = 52.15 
2025-02-13 01:20:05.773376: Steps 9/47: batch_recall = 47.65, batch_ndcg = 47.23 
2025-02-13 01:20:06.939368: Steps 10/47: batch_recall = 43.06, batch_ndcg = 43.82 
2025-02-13 01:20:08.073124: Steps 11/47: batch_recall = 55.15, batch_ndcg = 52.80 
2025-02-13 01:20:09.212159: Steps 12/47: batch_recall = 50.06, batch_ndcg = 48.79 
2025-02-13 01:20:10.332226: Steps 13/47: batch_recall = 48.41, batch_ndcg = 45.41 
2025-02-13 01:20:11.428441: Steps 14/47: batch_recall = 41.34, batch_ndcg = 40.95 
2025-02-13 01:20:12.522489: Steps 15/47: batch_recall = 57.47, batch_ndcg = 54.39 
2025-02-13 01:20:13.604015: Steps 16/47: batch_recall = 50.77, batch_ndcg = 46.95 
2025-02-13 01:20:14.664820: Steps 17/47: batch_recall = 58.26, batch_ndcg = 49.52 
2025-02-13 01:20:15.754080: Steps 18/47: batch_recall = 53.62, batch_ndcg = 50.30 
2025-02-13 01:20:16.830810: Steps 19/47: batch_recall = 60.96, batch_ndcg = 55.91 
2025-02-13 01:20:17.883856: Steps 20/47: batch_recall = 67.87, batch_ndcg = 61.36 
2025-02-13 01:20:18.938744: Steps 21/47: batch_recall = 64.97, batch_ndcg = 56.30 
2025-02-13 01:20:19.989838: Steps 22/47: batch_recall = 56.46, batch_ndcg = 51.68 
2025-02-13 01:20:21.039725: Steps 23/47: batch_recall = 62.34, batch_ndcg = 55.10 
2025-02-13 01:20:22.112655: Steps 24/47: batch_recall = 64.33, batch_ndcg = 58.20 
2025-02-13 01:20:23.157865: Steps 25/47: batch_recall = 66.72, batch_ndcg = 58.19 
2025-02-13 01:20:24.176741: Steps 26/47: batch_recall = 59.79, batch_ndcg = 53.22 
2025-02-13 01:20:25.211724: Steps 27/47: batch_recall = 62.78, batch_ndcg = 53.46 
2025-02-13 01:20:26.227067: Steps 28/47: batch_recall = 70.49, batch_ndcg = 59.81 
2025-02-13 01:20:27.234699: Steps 29/47: batch_recall = 68.77, batch_ndcg = 58.41 
2025-02-13 01:20:28.239415: Steps 30/47: batch_recall = 72.84, batch_ndcg = 63.57 
2025-02-13 01:20:29.263080: Steps 31/47: batch_recall = 68.56, batch_ndcg = 55.86 
2025-02-13 01:20:30.276544: Steps 32/47: batch_recall = 71.20, batch_ndcg = 65.09 
2025-02-13 01:20:31.281227: Steps 33/47: batch_recall = 79.35, batch_ndcg = 67.90 
2025-02-13 01:20:32.299847: Steps 34/47: batch_recall = 69.82, batch_ndcg = 57.39 
2025-02-13 01:20:33.288037: Steps 35/47: batch_recall = 77.16, batch_ndcg = 65.16 
2025-02-13 01:20:34.282029: Steps 36/47: batch_recall = 79.52, batch_ndcg = 64.88 
2025-02-13 01:20:35.257225: Steps 37/47: batch_recall = 84.16, batch_ndcg = 73.05 
2025-02-13 01:20:36.233063: Steps 38/47: batch_recall = 90.28, batch_ndcg = 72.52 
2025-02-13 01:20:37.208803: Steps 39/47: batch_recall = 88.23, batch_ndcg = 69.76 
2025-02-13 01:20:38.182236: Steps 40/47: batch_recall = 73.58, batch_ndcg = 62.61 
2025-02-13 01:20:39.154126: Steps 41/47: batch_recall = 88.88, batch_ndcg = 73.71 
2025-02-13 01:20:40.116437: Steps 42/47: batch_recall = 82.89, batch_ndcg = 65.02 
2025-02-13 01:20:41.080837: Steps 43/47: batch_recall = 90.42, batch_ndcg = 72.98 
2025-02-13 01:20:42.053032: Steps 44/47: batch_recall = 89.26, batch_ndcg = 70.06 
2025-02-13 01:20:42.986970: Steps 45/47: batch_recall = 93.79, batch_ndcg = 75.04 
2025-02-13 01:20:43.093245: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.83 
2025-02-13 01:20:43.093385: Epoch 106/1000, Test: Recall = 0.1220, NDCG = 0.1098  

2025-02-13 01:20:44.282493: Training Step 0/59: batchLoss = 2.9436, diffLoss = 14.3743, kgLoss = 0.0859
2025-02-13 01:20:45.211346: Training Step 1/59: batchLoss = 2.9697, diffLoss = 14.4772, kgLoss = 0.0929
2025-02-13 01:20:46.138988: Training Step 2/59: batchLoss = 3.0724, diffLoss = 15.0056, kgLoss = 0.0891
2025-02-13 01:20:47.062172: Training Step 3/59: batchLoss = 2.8190, diffLoss = 13.7586, kgLoss = 0.0840
2025-02-13 01:20:47.989468: Training Step 4/59: batchLoss = 2.9074, diffLoss = 14.1950, kgLoss = 0.0855
2025-02-13 01:20:48.910446: Training Step 5/59: batchLoss = 2.8392, diffLoss = 13.8608, kgLoss = 0.0838
2025-02-13 01:20:49.831171: Training Step 6/59: batchLoss = 2.7619, diffLoss = 13.4985, kgLoss = 0.0778
2025-02-13 01:20:50.755079: Training Step 7/59: batchLoss = 2.6221, diffLoss = 12.7994, kgLoss = 0.0778
2025-02-13 01:20:51.686400: Training Step 8/59: batchLoss = 2.7109, diffLoss = 13.2226, kgLoss = 0.0830
2025-02-13 01:20:52.615378: Training Step 9/59: batchLoss = 2.8029, diffLoss = 13.6666, kgLoss = 0.0869
2025-02-13 01:20:53.543082: Training Step 10/59: batchLoss = 2.8793, diffLoss = 14.0442, kgLoss = 0.0880
2025-02-13 01:20:54.472560: Training Step 11/59: batchLoss = 2.7997, diffLoss = 13.6548, kgLoss = 0.0859
2025-02-13 01:20:55.406411: Training Step 12/59: batchLoss = 2.8159, diffLoss = 13.7271, kgLoss = 0.0881
2025-02-13 01:20:56.338928: Training Step 13/59: batchLoss = 3.0641, diffLoss = 14.9658, kgLoss = 0.0886
2025-02-13 01:20:57.273560: Training Step 14/59: batchLoss = 2.5642, diffLoss = 12.5142, kgLoss = 0.0767
2025-02-13 01:20:58.205280: Training Step 15/59: batchLoss = 3.1189, diffLoss = 15.2021, kgLoss = 0.0981
2025-02-13 01:20:59.142129: Training Step 16/59: batchLoss = 2.8540, diffLoss = 13.9230, kgLoss = 0.0868
2025-02-13 01:21:00.072176: Training Step 17/59: batchLoss = 2.8444, diffLoss = 13.8782, kgLoss = 0.0860
2025-02-13 01:21:00.998887: Training Step 18/59: batchLoss = 2.7945, diffLoss = 13.6294, kgLoss = 0.0858
2025-02-13 01:21:01.928518: Training Step 19/59: batchLoss = 2.9749, diffLoss = 14.5073, kgLoss = 0.0918
2025-02-13 01:21:02.850267: Training Step 20/59: batchLoss = 3.0886, diffLoss = 15.0764, kgLoss = 0.0917
2025-02-13 01:21:03.777656: Training Step 21/59: batchLoss = 2.6731, diffLoss = 13.0483, kgLoss = 0.0793
2025-02-13 01:21:04.694389: Training Step 22/59: batchLoss = 2.8735, diffLoss = 14.0474, kgLoss = 0.0801
2025-02-13 01:21:05.614231: Training Step 23/59: batchLoss = 2.9915, diffLoss = 14.6145, kgLoss = 0.0858
2025-02-13 01:21:06.538180: Training Step 24/59: batchLoss = 2.7013, diffLoss = 13.1861, kgLoss = 0.0801
2025-02-13 01:21:07.461488: Training Step 25/59: batchLoss = 2.8993, diffLoss = 14.1581, kgLoss = 0.0846
2025-02-13 01:21:08.388936: Training Step 26/59: batchLoss = 2.8789, diffLoss = 14.0365, kgLoss = 0.0895
2025-02-13 01:21:09.314078: Training Step 27/59: batchLoss = 2.6201, diffLoss = 12.8023, kgLoss = 0.0745
2025-02-13 01:21:10.245704: Training Step 28/59: batchLoss = 2.8239, diffLoss = 13.7951, kgLoss = 0.0811
2025-02-13 01:21:11.173879: Training Step 29/59: batchLoss = 3.1824, diffLoss = 15.5374, kgLoss = 0.0936
2025-02-13 01:21:12.103592: Training Step 30/59: batchLoss = 2.9646, diffLoss = 14.4700, kgLoss = 0.0882
2025-02-13 01:21:13.041352: Training Step 31/59: batchLoss = 2.8727, diffLoss = 14.0009, kgLoss = 0.0906
2025-02-13 01:21:13.971930: Training Step 32/59: batchLoss = 3.2396, diffLoss = 15.8014, kgLoss = 0.0991
2025-02-13 01:21:14.908551: Training Step 33/59: batchLoss = 2.8124, diffLoss = 13.7245, kgLoss = 0.0843
2025-02-13 01:21:15.848568: Training Step 34/59: batchLoss = 2.9973, diffLoss = 14.6627, kgLoss = 0.0810
2025-02-13 01:21:16.786009: Training Step 35/59: batchLoss = 3.1062, diffLoss = 15.1754, kgLoss = 0.0890
2025-02-13 01:21:17.734878: Training Step 36/59: batchLoss = 3.0238, diffLoss = 14.8056, kgLoss = 0.0784
2025-02-13 01:21:18.662783: Training Step 37/59: batchLoss = 3.6412, diffLoss = 17.8045, kgLoss = 0.1003
2025-02-13 01:21:19.589053: Training Step 38/59: batchLoss = 2.9991, diffLoss = 14.6281, kgLoss = 0.0918
2025-02-13 01:21:20.519326: Training Step 39/59: batchLoss = 2.8651, diffLoss = 13.9792, kgLoss = 0.0866
2025-02-13 01:21:21.444604: Training Step 40/59: batchLoss = 2.7952, diffLoss = 13.6507, kgLoss = 0.0813
2025-02-13 01:21:22.370951: Training Step 41/59: batchLoss = 3.0685, diffLoss = 14.9781, kgLoss = 0.0911
2025-02-13 01:21:23.287885: Training Step 42/59: batchLoss = 3.0875, diffLoss = 15.0792, kgLoss = 0.0896
2025-02-13 01:21:24.223499: Training Step 43/59: batchLoss = 2.8306, diffLoss = 13.8197, kgLoss = 0.0833
2025-02-13 01:21:25.148515: Training Step 44/59: batchLoss = 2.6909, diffLoss = 13.1698, kgLoss = 0.0712
2025-02-13 01:21:26.076451: Training Step 45/59: batchLoss = 2.7406, diffLoss = 13.3987, kgLoss = 0.0760
2025-02-13 01:21:27.001173: Training Step 46/59: batchLoss = 3.0629, diffLoss = 14.9905, kgLoss = 0.0810
2025-02-13 01:21:27.926316: Training Step 47/59: batchLoss = 3.2059, diffLoss = 15.6461, kgLoss = 0.0958
2025-02-13 01:21:28.853160: Training Step 48/59: batchLoss = 2.9266, diffLoss = 14.2930, kgLoss = 0.0850
2025-02-13 01:21:29.782815: Training Step 49/59: batchLoss = 3.0575, diffLoss = 14.9169, kgLoss = 0.0926
2025-02-13 01:21:30.708262: Training Step 50/59: batchLoss = 3.4349, diffLoss = 16.8103, kgLoss = 0.0910
2025-02-13 01:21:31.642720: Training Step 51/59: batchLoss = 2.7751, diffLoss = 13.5536, kgLoss = 0.0805
2025-02-13 01:21:32.577723: Training Step 52/59: batchLoss = 2.9075, diffLoss = 14.2145, kgLoss = 0.0808
2025-02-13 01:21:33.513353: Training Step 53/59: batchLoss = 2.8806, diffLoss = 14.0876, kgLoss = 0.0789
2025-02-13 01:21:34.456717: Training Step 54/59: batchLoss = 2.8498, diffLoss = 13.9078, kgLoss = 0.0853
2025-02-13 01:21:35.400357: Training Step 55/59: batchLoss = 2.6282, diffLoss = 12.8352, kgLoss = 0.0765
2025-02-13 01:21:36.335101: Training Step 56/59: batchLoss = 3.1712, diffLoss = 15.5183, kgLoss = 0.0844
2025-02-13 01:21:37.190295: Training Step 57/59: batchLoss = 3.0350, diffLoss = 14.8295, kgLoss = 0.0864
2025-02-13 01:21:38.043761: Training Step 58/59: batchLoss = 2.9053, diffLoss = 14.2078, kgLoss = 0.0797
2025-02-13 01:21:38.139560: 
2025-02-13 01:21:38.139894: Epoch 107/1000, Train: epLoss = 0.4312, epDfLoss = 2.1054, epKgLoss = 0.0126  
2025-02-13 01:21:39.607564: Steps 0/47: batch_recall = 37.48, batch_ndcg = 48.01 
2025-02-13 01:21:40.905547: Steps 1/47: batch_recall = 37.17, batch_ndcg = 42.70 
2025-02-13 01:21:42.155474: Steps 2/47: batch_recall = 41.31, batch_ndcg = 47.44 
2025-02-13 01:21:43.408616: Steps 3/47: batch_recall = 43.65, batch_ndcg = 46.37 
2025-02-13 01:21:44.601333: Steps 4/47: batch_recall = 38.81, batch_ndcg = 45.12 
2025-02-13 01:21:45.809765: Steps 5/47: batch_recall = 33.26, batch_ndcg = 40.04 
2025-02-13 01:21:47.001325: Steps 6/47: batch_recall = 39.03, batch_ndcg = 42.00 
2025-02-13 01:21:48.172723: Steps 7/47: batch_recall = 44.02, batch_ndcg = 46.48 
2025-02-13 01:21:49.365391: Steps 8/47: batch_recall = 48.96, batch_ndcg = 52.90 
2025-02-13 01:21:50.518885: Steps 9/47: batch_recall = 47.44, batch_ndcg = 47.11 
2025-02-13 01:21:51.696956: Steps 10/47: batch_recall = 44.28, batch_ndcg = 43.57 
2025-02-13 01:21:52.846401: Steps 11/47: batch_recall = 55.34, batch_ndcg = 52.89 
2025-02-13 01:21:54.002264: Steps 12/47: batch_recall = 49.72, batch_ndcg = 49.43 
2025-02-13 01:21:55.143046: Steps 13/47: batch_recall = 48.63, batch_ndcg = 45.32 
2025-02-13 01:21:56.253727: Steps 14/47: batch_recall = 40.08, batch_ndcg = 40.46 
2025-02-13 01:21:57.349783: Steps 15/47: batch_recall = 56.64, batch_ndcg = 54.01 
2025-02-13 01:21:58.433375: Steps 16/47: batch_recall = 50.05, batch_ndcg = 46.87 
2025-02-13 01:21:59.481483: Steps 17/47: batch_recall = 58.30, batch_ndcg = 49.74 
2025-02-13 01:22:00.557513: Steps 18/47: batch_recall = 53.13, batch_ndcg = 49.48 
2025-02-13 01:22:01.621134: Steps 19/47: batch_recall = 58.92, batch_ndcg = 55.08 
2025-02-13 01:22:02.663699: Steps 20/47: batch_recall = 66.12, batch_ndcg = 60.35 
2025-02-13 01:22:03.720550: Steps 21/47: batch_recall = 64.52, batch_ndcg = 56.15 
2025-02-13 01:22:04.770320: Steps 22/47: batch_recall = 55.86, batch_ndcg = 51.59 
2025-02-13 01:22:05.810854: Steps 23/47: batch_recall = 64.78, batch_ndcg = 56.14 
2025-02-13 01:22:06.872460: Steps 24/47: batch_recall = 66.52, batch_ndcg = 59.68 
2025-02-13 01:22:07.923721: Steps 25/47: batch_recall = 67.47, batch_ndcg = 58.71 
2025-02-13 01:22:08.948535: Steps 26/47: batch_recall = 59.62, batch_ndcg = 52.91 
2025-02-13 01:22:09.997326: Steps 27/47: batch_recall = 62.48, batch_ndcg = 52.71 
2025-02-13 01:22:11.027305: Steps 28/47: batch_recall = 70.58, batch_ndcg = 59.72 
2025-02-13 01:22:12.051510: Steps 29/47: batch_recall = 69.20, batch_ndcg = 58.56 
2025-02-13 01:22:13.075887: Steps 30/47: batch_recall = 72.97, batch_ndcg = 63.96 
2025-02-13 01:22:14.111611: Steps 31/47: batch_recall = 67.39, batch_ndcg = 55.81 
2025-02-13 01:22:15.125682: Steps 32/47: batch_recall = 71.27, batch_ndcg = 64.85 
2025-02-13 01:22:16.135857: Steps 33/47: batch_recall = 79.80, batch_ndcg = 68.12 
2025-02-13 01:22:17.134407: Steps 34/47: batch_recall = 68.69, batch_ndcg = 56.38 
2025-02-13 01:22:18.111331: Steps 35/47: batch_recall = 77.26, batch_ndcg = 65.70 
2025-02-13 01:22:19.095739: Steps 36/47: batch_recall = 79.96, batch_ndcg = 64.41 
2025-02-13 01:22:20.057991: Steps 37/47: batch_recall = 84.42, batch_ndcg = 73.48 
2025-02-13 01:22:21.025035: Steps 38/47: batch_recall = 90.32, batch_ndcg = 72.86 
2025-02-13 01:22:21.999058: Steps 39/47: batch_recall = 88.07, batch_ndcg = 69.43 
2025-02-13 01:22:22.952539: Steps 40/47: batch_recall = 74.07, batch_ndcg = 63.25 
2025-02-13 01:22:23.912210: Steps 41/47: batch_recall = 87.46, batch_ndcg = 72.36 
2025-02-13 01:22:24.860768: Steps 42/47: batch_recall = 82.65, batch_ndcg = 64.99 
2025-02-13 01:22:25.828751: Steps 43/47: batch_recall = 90.05, batch_ndcg = 72.48 
2025-02-13 01:22:26.802451: Steps 44/47: batch_recall = 88.58, batch_ndcg = 70.61 
2025-02-13 01:22:27.749591: Steps 45/47: batch_recall = 92.73, batch_ndcg = 74.97 
2025-02-13 01:22:27.853163: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.67 
2025-02-13 01:22:27.853263: Epoch 107/1000, Test: Recall = 0.1218, NDCG = 0.1098  

2025-02-13 01:22:29.058655: Training Step 0/59: batchLoss = 2.7902, diffLoss = 13.6264, kgLoss = 0.0811
2025-02-13 01:22:29.988616: Training Step 1/59: batchLoss = 2.6024, diffLoss = 12.7121, kgLoss = 0.0750
2025-02-13 01:22:30.916294: Training Step 2/59: batchLoss = 2.6536, diffLoss = 12.9726, kgLoss = 0.0739
2025-02-13 01:22:31.849646: Training Step 3/59: batchLoss = 2.7136, diffLoss = 13.2172, kgLoss = 0.0877
2025-02-13 01:22:32.795432: Training Step 4/59: batchLoss = 2.6397, diffLoss = 12.8867, kgLoss = 0.0780
2025-02-13 01:22:33.727590: Training Step 5/59: batchLoss = 3.0773, diffLoss = 15.0364, kgLoss = 0.0876
2025-02-13 01:22:34.666260: Training Step 6/59: batchLoss = 2.8525, diffLoss = 13.9098, kgLoss = 0.0882
2025-02-13 01:22:35.604976: Training Step 7/59: batchLoss = 2.5319, diffLoss = 12.3427, kgLoss = 0.0792
2025-02-13 01:22:36.530757: Training Step 8/59: batchLoss = 2.9758, diffLoss = 14.5432, kgLoss = 0.0840
2025-02-13 01:22:37.455448: Training Step 9/59: batchLoss = 2.6608, diffLoss = 12.9625, kgLoss = 0.0854
2025-02-13 01:22:38.383143: Training Step 10/59: batchLoss = 2.9400, diffLoss = 14.3188, kgLoss = 0.0953
2025-02-13 01:22:39.310929: Training Step 11/59: batchLoss = 2.8161, diffLoss = 13.7156, kgLoss = 0.0912
2025-02-13 01:22:40.239825: Training Step 12/59: batchLoss = 2.8435, diffLoss = 13.9044, kgLoss = 0.0783
2025-02-13 01:22:41.168924: Training Step 13/59: batchLoss = 2.8275, diffLoss = 13.8050, kgLoss = 0.0831
2025-02-13 01:22:42.095018: Training Step 14/59: batchLoss = 2.8195, diffLoss = 13.7410, kgLoss = 0.0891
2025-02-13 01:22:43.025495: Training Step 15/59: batchLoss = 2.8152, diffLoss = 13.7360, kgLoss = 0.0850
2025-02-13 01:22:43.943590: Training Step 16/59: batchLoss = 2.6809, diffLoss = 13.0881, kgLoss = 0.0791
2025-02-13 01:22:44.860052: Training Step 17/59: batchLoss = 2.9027, diffLoss = 14.1970, kgLoss = 0.0791
2025-02-13 01:22:45.781165: Training Step 18/59: batchLoss = 3.0928, diffLoss = 15.1043, kgLoss = 0.0899
2025-02-13 01:22:46.710576: Training Step 19/59: batchLoss = 2.5711, diffLoss = 12.5626, kgLoss = 0.0732
2025-02-13 01:22:47.698310: Training Step 20/59: batchLoss = 2.9999, diffLoss = 14.6168, kgLoss = 0.0956
2025-02-13 01:22:48.644398: Training Step 21/59: batchLoss = 2.8267, diffLoss = 13.8067, kgLoss = 0.0818
2025-02-13 01:22:49.591492: Training Step 22/59: batchLoss = 2.9149, diffLoss = 14.2314, kgLoss = 0.0858
2025-02-13 01:22:50.526955: Training Step 23/59: batchLoss = 2.8720, diffLoss = 14.0074, kgLoss = 0.0881
2025-02-13 01:22:51.471071: Training Step 24/59: batchLoss = 3.0087, diffLoss = 14.6988, kgLoss = 0.0861
2025-02-13 01:22:52.413530: Training Step 25/59: batchLoss = 3.1017, diffLoss = 15.1334, kgLoss = 0.0938
2025-02-13 01:22:53.364992: Training Step 26/59: batchLoss = 2.7731, diffLoss = 13.5494, kgLoss = 0.0790
2025-02-13 01:22:54.300976: Training Step 27/59: batchLoss = 3.1899, diffLoss = 15.5884, kgLoss = 0.0903
2025-02-13 01:22:55.226659: Training Step 28/59: batchLoss = 3.0971, diffLoss = 15.1323, kgLoss = 0.0883
2025-02-13 01:22:56.155196: Training Step 29/59: batchLoss = 3.1894, diffLoss = 15.5721, kgLoss = 0.0937
2025-02-13 01:22:57.092430: Training Step 30/59: batchLoss = 2.7090, diffLoss = 13.2241, kgLoss = 0.0802
2025-02-13 01:22:58.018585: Training Step 31/59: batchLoss = 3.1423, diffLoss = 15.3706, kgLoss = 0.0853
2025-02-13 01:22:58.941968: Training Step 32/59: batchLoss = 3.0933, diffLoss = 15.1305, kgLoss = 0.0841
2025-02-13 01:22:59.868169: Training Step 33/59: batchLoss = 3.0243, diffLoss = 14.7837, kgLoss = 0.0845
2025-02-13 01:23:00.790898: Training Step 34/59: batchLoss = 3.2048, diffLoss = 15.6302, kgLoss = 0.0984
2025-02-13 01:23:01.717844: Training Step 35/59: batchLoss = 3.0405, diffLoss = 14.8740, kgLoss = 0.0821
2025-02-13 01:23:02.643778: Training Step 36/59: batchLoss = 3.0852, diffLoss = 15.0694, kgLoss = 0.0892
2025-02-13 01:23:03.578113: Training Step 37/59: batchLoss = 3.1355, diffLoss = 15.3195, kgLoss = 0.0895
2025-02-13 01:23:04.514066: Training Step 38/59: batchLoss = 3.1281, diffLoss = 15.2623, kgLoss = 0.0945
2025-02-13 01:23:05.449504: Training Step 39/59: batchLoss = 2.8051, diffLoss = 13.6777, kgLoss = 0.0869
2025-02-13 01:23:06.381377: Training Step 40/59: batchLoss = 2.9453, diffLoss = 14.3731, kgLoss = 0.0884
2025-02-13 01:23:07.325654: Training Step 41/59: batchLoss = 3.5835, diffLoss = 17.5258, kgLoss = 0.0979
2025-02-13 01:23:08.261876: Training Step 42/59: batchLoss = 3.0332, diffLoss = 14.8310, kgLoss = 0.0837
2025-02-13 01:23:09.192749: Training Step 43/59: batchLoss = 3.2283, diffLoss = 15.7743, kgLoss = 0.0918
2025-02-13 01:23:10.128149: Training Step 44/59: batchLoss = 3.4349, diffLoss = 16.7937, kgLoss = 0.0952
2025-02-13 01:23:11.050810: Training Step 45/59: batchLoss = 2.8804, diffLoss = 14.0924, kgLoss = 0.0774
2025-02-13 01:23:11.978885: Training Step 46/59: batchLoss = 2.9580, diffLoss = 14.4278, kgLoss = 0.0906
2025-02-13 01:23:12.909579: Training Step 47/59: batchLoss = 2.9565, diffLoss = 14.4376, kgLoss = 0.0862
2025-02-13 01:23:13.834573: Training Step 48/59: batchLoss = 2.8679, diffLoss = 14.0061, kgLoss = 0.0834
2025-02-13 01:23:14.764101: Training Step 49/59: batchLoss = 2.7310, diffLoss = 13.3331, kgLoss = 0.0805
2025-02-13 01:23:15.681217: Training Step 50/59: batchLoss = 2.7425, diffLoss = 13.3920, kgLoss = 0.0801
2025-02-13 01:23:16.605002: Training Step 51/59: batchLoss = 2.7225, diffLoss = 13.3119, kgLoss = 0.0752
2025-02-13 01:23:17.529590: Training Step 52/59: batchLoss = 2.7395, diffLoss = 13.3840, kgLoss = 0.0784
2025-02-13 01:23:18.464031: Training Step 53/59: batchLoss = 2.9477, diffLoss = 14.4178, kgLoss = 0.0802
2025-02-13 01:23:19.393441: Training Step 54/59: batchLoss = 3.1295, diffLoss = 15.2672, kgLoss = 0.0950
2025-02-13 01:23:20.316836: Training Step 55/59: batchLoss = 3.1662, diffLoss = 15.5012, kgLoss = 0.0824
2025-02-13 01:23:21.239911: Training Step 56/59: batchLoss = 2.9649, diffLoss = 14.4823, kgLoss = 0.0856
2025-02-13 01:23:22.072936: Training Step 57/59: batchLoss = 3.3246, diffLoss = 16.2491, kgLoss = 0.0935
2025-02-13 01:23:22.918428: Training Step 58/59: batchLoss = 2.7872, diffLoss = 13.6278, kgLoss = 0.0771
2025-02-13 01:23:23.014184: 
2025-02-13 01:23:23.014706: Epoch 108/1000, Train: epLoss = 0.4332, epDfLoss = 2.1157, epKgLoss = 0.0126  
2025-02-13 01:23:24.487584: Steps 0/47: batch_recall = 37.34, batch_ndcg = 47.35 
2025-02-13 01:23:25.798096: Steps 1/47: batch_recall = 36.90, batch_ndcg = 42.00 
2025-02-13 01:23:27.060418: Steps 2/47: batch_recall = 41.50, batch_ndcg = 47.26 
2025-02-13 01:23:28.334493: Steps 3/47: batch_recall = 43.84, batch_ndcg = 46.97 
2025-02-13 01:23:29.542024: Steps 4/47: batch_recall = 39.52, batch_ndcg = 45.07 
2025-02-13 01:23:30.771872: Steps 5/47: batch_recall = 33.89, batch_ndcg = 39.96 
2025-02-13 01:23:31.975540: Steps 6/47: batch_recall = 38.40, batch_ndcg = 41.02 
2025-02-13 01:23:33.156680: Steps 7/47: batch_recall = 43.56, batch_ndcg = 46.01 
2025-02-13 01:23:34.335859: Steps 8/47: batch_recall = 48.55, batch_ndcg = 52.82 
2025-02-13 01:23:35.473706: Steps 9/47: batch_recall = 46.50, batch_ndcg = 46.89 
2025-02-13 01:23:36.645031: Steps 10/47: batch_recall = 44.34, batch_ndcg = 44.32 
2025-02-13 01:23:37.781119: Steps 11/47: batch_recall = 55.89, batch_ndcg = 53.57 
2025-02-13 01:23:38.930045: Steps 12/47: batch_recall = 49.62, batch_ndcg = 48.56 
2025-02-13 01:23:40.062198: Steps 13/47: batch_recall = 48.08, batch_ndcg = 45.21 
2025-02-13 01:23:41.146755: Steps 14/47: batch_recall = 40.74, batch_ndcg = 41.10 
2025-02-13 01:23:42.235682: Steps 15/47: batch_recall = 56.37, batch_ndcg = 54.22 
2025-02-13 01:23:43.330062: Steps 16/47: batch_recall = 50.67, batch_ndcg = 47.87 
2025-02-13 01:23:44.390805: Steps 17/47: batch_recall = 58.97, batch_ndcg = 50.20 
2025-02-13 01:23:45.489418: Steps 18/47: batch_recall = 53.28, batch_ndcg = 50.19 
2025-02-13 01:23:46.567808: Steps 19/47: batch_recall = 61.20, batch_ndcg = 56.30 
2025-02-13 01:23:47.623995: Steps 20/47: batch_recall = 68.42, batch_ndcg = 62.97 
2025-02-13 01:23:48.680887: Steps 21/47: batch_recall = 63.99, batch_ndcg = 55.67 
2025-02-13 01:23:49.743657: Steps 22/47: batch_recall = 56.32, batch_ndcg = 51.19 
2025-02-13 01:23:50.804308: Steps 23/47: batch_recall = 65.63, batch_ndcg = 56.87 
2025-02-13 01:23:51.868445: Steps 24/47: batch_recall = 65.81, batch_ndcg = 59.49 
2025-02-13 01:23:52.905538: Steps 25/47: batch_recall = 67.56, batch_ndcg = 58.12 
2025-02-13 01:23:53.920128: Steps 26/47: batch_recall = 60.16, batch_ndcg = 52.92 
2025-02-13 01:23:54.947699: Steps 27/47: batch_recall = 62.15, batch_ndcg = 52.19 
2025-02-13 01:23:55.963467: Steps 28/47: batch_recall = 70.35, batch_ndcg = 59.30 
2025-02-13 01:23:56.966937: Steps 29/47: batch_recall = 71.53, batch_ndcg = 59.62 
2025-02-13 01:23:57.968333: Steps 30/47: batch_recall = 72.87, batch_ndcg = 64.10 
2025-02-13 01:23:58.995756: Steps 31/47: batch_recall = 67.31, batch_ndcg = 56.17 
2025-02-13 01:24:00.001788: Steps 32/47: batch_recall = 71.16, batch_ndcg = 65.49 
2025-02-13 01:24:01.021164: Steps 33/47: batch_recall = 79.08, batch_ndcg = 68.26 
2025-02-13 01:24:02.036759: Steps 34/47: batch_recall = 68.97, batch_ndcg = 57.37 
2025-02-13 01:24:03.026356: Steps 35/47: batch_recall = 76.56, batch_ndcg = 65.84 
2025-02-13 01:24:04.026508: Steps 36/47: batch_recall = 77.81, batch_ndcg = 63.97 
2025-02-13 01:24:05.015673: Steps 37/47: batch_recall = 84.61, batch_ndcg = 73.70 
2025-02-13 01:24:06.001983: Steps 38/47: batch_recall = 91.77, batch_ndcg = 73.77 
2025-02-13 01:24:06.981629: Steps 39/47: batch_recall = 88.57, batch_ndcg = 69.67 
2025-02-13 01:24:07.953272: Steps 40/47: batch_recall = 73.93, batch_ndcg = 63.13 
2025-02-13 01:24:08.914054: Steps 41/47: batch_recall = 87.82, batch_ndcg = 72.97 
2025-02-13 01:24:09.881576: Steps 42/47: batch_recall = 82.96, batch_ndcg = 65.36 
2025-02-13 01:24:10.837350: Steps 43/47: batch_recall = 91.64, batch_ndcg = 72.25 
2025-02-13 01:24:11.800137: Steps 44/47: batch_recall = 88.40, batch_ndcg = 70.93 
2025-02-13 01:24:12.739033: Steps 45/47: batch_recall = 93.87, batch_ndcg = 75.63 
2025-02-13 01:24:12.845489: Steps 46/47: batch_recall = 2.00, batch_ndcg = 1.85 
2025-02-13 01:24:12.845621: Epoch 108/1000, Test: Recall = 0.1222, NDCG = 0.1101  

------------------
Exiting from training early
Best epoch :  89  , Recall :  0.12262161166724789  , NDCG :  0.11041045286895207

------------------------------------------------------------
Sender: LSF System <XXX@XXX-dl01-w079>
Subject: Job 7824385: <RecommenderSys> in cluster <XXX-dl01> Done

Job <RecommenderSys> was submitted from host <XXX-dl01-login1> by user <XXX> in cluster <XXX-dl01> at Wed Feb 12 21:56:43 2025
Job was executed on host(s) <XXX-dl01-w079>, in queue <batch_a100>, as user <XXX> in cluster <XXX-dl01> at Wed Feb 12 22:13:25 2025
</home/XXX> was used as the home directory.
</home/XXX/recommenderSys/ DiKGRec> was used as the working directory.
Started at Wed Feb 12 22:13:25 2025
Terminated at Thu Feb 13 01:24:23 2025
Results reported at Thu Feb 13 01:24:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python Main.py --data lastfm --lr2 5e-4 --kg_loss_ratio 0.8 --updateW 1 --oriW 1
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   13728.00 sec.
    Max Memory :                                 4567 MB
    Average Memory :                             4214.00 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               5433.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   11453 sec.
    Turnaround time :                            12460 sec.

The output (if any) is above this job summary.



PS:

Read file <recommander.7824385.stderr> for stderr output of this job.


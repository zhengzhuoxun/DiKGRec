2025-04-09 09:14:08.747825: Start
Begin to load knowledge graph triples ...
In KG: 84 relations, 45538 entities, 1739206 triples
2025-04-09 09:14:26.790452: Load Data
2025-04-09 09:14:26.790480: Namespace(batch=400, cold_start_num=0, d_emb_size=10, data='yelp2018', diff_type=0, dims='[1000]', entity=45538, epoch=1000, gpu='0', head=2, head_diff_ratio=0, item=45538, kg_loss_ratio=0.4, kg_norm=2, latdim=64, layer=2, load_model=None, lr=5e-05, lr2=0.001, noise_max=0.005, noise_min=0.0005, noise_ratio=0.2, noise_scale=0.0005, norm=True, oriW=0.0, relation=84, res_lambda=0.5, sampling_N=20, sampling_steps=0, save_path='tem', seed=421, steps=5, topk=20, trans_ratio=0, tstBat=512, tstEpoch=1, updateW=2.0, user=45919)
USER 45919 ITEM 45538
NUM OF INTERACTIONS 930032
Number of all parameters: 91132816
2025-04-09 09:14:27.589736: Model Prepared
2025-04-09 09:14:27.589755: Model Initialized
2025-04-09 09:14:32.572411: Training Step 0/115: batchLoss = 4.4893, diffLoss = 7.3620, kgLoss = 0.1803
2025-04-09 09:14:33.305390: Training Step 1/115: batchLoss = 4.8473, diffLoss = 7.9595, kgLoss = 0.1792
2025-04-09 09:14:34.040761: Training Step 2/115: batchLoss = 5.1842, diffLoss = 8.5142, kgLoss = 0.1893
2025-04-09 09:14:34.772983: Training Step 3/115: batchLoss = 4.6874, diffLoss = 7.7026, kgLoss = 0.1647
2025-04-09 09:14:35.506368: Training Step 4/115: batchLoss = 5.0126, diffLoss = 8.2381, kgLoss = 0.1744
2025-04-09 09:14:36.238752: Training Step 5/115: batchLoss = 5.2484, diffLoss = 8.6175, kgLoss = 0.1949
2025-04-09 09:14:36.985922: Training Step 6/115: batchLoss = 4.6340, diffLoss = 7.6071, kgLoss = 0.1743
2025-04-09 09:14:37.718877: Training Step 7/115: batchLoss = 5.1448, diffLoss = 8.4490, kgLoss = 0.1885
2025-04-09 09:14:38.454850: Training Step 8/115: batchLoss = 4.8850, diffLoss = 8.0215, kgLoss = 0.1804
2025-04-09 09:14:39.190740: Training Step 9/115: batchLoss = 4.4146, diffLoss = 7.2469, kgLoss = 0.1660
2025-04-09 09:14:39.927359: Training Step 10/115: batchLoss = 5.2104, diffLoss = 8.5598, kgLoss = 0.1863
2025-04-09 09:14:40.662278: Training Step 11/115: batchLoss = 5.0597, diffLoss = 8.3138, kgLoss = 0.1785
2025-04-09 09:14:41.395822: Training Step 12/115: batchLoss = 5.1394, diffLoss = 8.4436, kgLoss = 0.1831
2025-04-09 09:14:42.115673: Training Step 13/115: batchLoss = 4.3693, diffLoss = 7.1763, kgLoss = 0.1589
2025-04-09 09:14:42.838284: Training Step 14/115: batchLoss = 4.9401, diffLoss = 8.1180, kgLoss = 0.1733
2025-04-09 09:14:43.562265: Training Step 15/115: batchLoss = 4.7395, diffLoss = 7.7842, kgLoss = 0.1724
2025-04-09 09:14:44.289205: Training Step 16/115: batchLoss = 4.4272, diffLoss = 7.2690, kgLoss = 0.1644
2025-04-09 09:14:45.027426: Training Step 17/115: batchLoss = 4.5396, diffLoss = 7.4558, kgLoss = 0.1653
2025-04-09 09:14:45.766957: Training Step 18/115: batchLoss = 4.2010, diffLoss = 6.8877, kgLoss = 0.1710
2025-04-09 09:14:46.491742: Training Step 19/115: batchLoss = 4.5068, diffLoss = 7.3965, kgLoss = 0.1723
2025-04-09 09:14:47.226760: Training Step 20/115: batchLoss = 5.0653, diffLoss = 8.3188, kgLoss = 0.1850
2025-04-09 09:14:47.967818: Training Step 21/115: batchLoss = 5.3870, diffLoss = 8.8443, kgLoss = 0.2010
2025-04-09 09:14:48.706929: Training Step 22/115: batchLoss = 4.6695, diffLoss = 7.6627, kgLoss = 0.1798
2025-04-09 09:14:49.439338: Training Step 23/115: batchLoss = 5.3259, diffLoss = 8.7533, kgLoss = 0.1848
2025-04-09 09:14:50.178556: Training Step 24/115: batchLoss = 4.2464, diffLoss = 6.9717, kgLoss = 0.1585
2025-04-09 09:14:50.922788: Training Step 25/115: batchLoss = 4.7303, diffLoss = 7.7593, kgLoss = 0.1869
2025-04-09 09:14:51.657595: Training Step 26/115: batchLoss = 4.3676, diffLoss = 7.1659, kgLoss = 0.1703
2025-04-09 09:14:52.419596: Training Step 27/115: batchLoss = 4.5555, diffLoss = 7.4784, kgLoss = 0.1713
2025-04-09 09:14:53.156437: Training Step 28/115: batchLoss = 5.5995, diffLoss = 9.2060, kgLoss = 0.1899
2025-04-09 09:14:53.878898: Training Step 29/115: batchLoss = 5.4451, diffLoss = 8.9460, kgLoss = 0.1937
2025-04-09 09:14:54.610832: Training Step 30/115: batchLoss = 5.3954, diffLoss = 8.8650, kgLoss = 0.1911
2025-04-09 09:14:55.342263: Training Step 31/115: batchLoss = 4.9414, diffLoss = 8.1168, kgLoss = 0.1783
2025-04-09 09:14:56.055837: Training Step 32/115: batchLoss = 4.5922, diffLoss = 7.5428, kgLoss = 0.1662
2025-04-09 09:14:56.769540: Training Step 33/115: batchLoss = 4.6685, diffLoss = 7.6644, kgLoss = 0.1747
2025-04-09 09:14:57.488280: Training Step 34/115: batchLoss = 5.0687, diffLoss = 8.3295, kgLoss = 0.1776
2025-04-09 09:14:58.222111: Training Step 35/115: batchLoss = 4.8356, diffLoss = 7.9378, kgLoss = 0.1824
2025-04-09 09:14:58.954195: Training Step 36/115: batchLoss = 4.4447, diffLoss = 7.2994, kgLoss = 0.1627
2025-04-09 09:14:59.679592: Training Step 37/115: batchLoss = 5.0788, diffLoss = 8.3429, kgLoss = 0.1827
2025-04-09 09:15:00.401775: Training Step 38/115: batchLoss = 5.9213, diffLoss = 9.7304, kgLoss = 0.2076
2025-04-09 09:15:01.125344: Training Step 39/115: batchLoss = 4.4403, diffLoss = 7.2933, kgLoss = 0.1607
2025-04-09 09:15:01.848140: Training Step 40/115: batchLoss = 5.7503, diffLoss = 9.4461, kgLoss = 0.2067
2025-04-09 09:15:02.563740: Training Step 41/115: batchLoss = 4.6773, diffLoss = 7.6830, kgLoss = 0.1687
2025-04-09 09:15:03.280710: Training Step 42/115: batchLoss = 4.8254, diffLoss = 7.9271, kgLoss = 0.1729
2025-04-09 09:15:04.015558: Training Step 43/115: batchLoss = 4.6533, diffLoss = 7.6353, kgLoss = 0.1804
2025-04-09 09:15:04.733490: Training Step 44/115: batchLoss = 4.8782, diffLoss = 8.0158, kgLoss = 0.1718
2025-04-09 09:15:05.463968: Training Step 45/115: batchLoss = 4.1922, diffLoss = 6.8814, kgLoss = 0.1584
2025-04-09 09:15:06.181405: Training Step 46/115: batchLoss = 4.1858, diffLoss = 6.8680, kgLoss = 0.1624
2025-04-09 09:15:06.902809: Training Step 47/115: batchLoss = 4.5673, diffLoss = 7.4929, kgLoss = 0.1788
2025-04-09 09:15:07.629476: Training Step 48/115: batchLoss = 4.8319, diffLoss = 7.9285, kgLoss = 0.1871
2025-04-09 09:15:08.355084: Training Step 49/115: batchLoss = 4.6941, diffLoss = 7.7111, kgLoss = 0.1686
2025-04-09 09:15:09.083283: Training Step 50/115: batchLoss = 4.4699, diffLoss = 7.3350, kgLoss = 0.1724
2025-04-09 09:15:09.805043: Training Step 51/115: batchLoss = 4.2576, diffLoss = 6.9807, kgLoss = 0.1730
2025-04-09 09:15:10.528724: Training Step 52/115: batchLoss = 4.2619, diffLoss = 6.9914, kgLoss = 0.1675
2025-04-09 09:15:11.257544: Training Step 53/115: batchLoss = 4.8987, diffLoss = 8.0498, kgLoss = 0.1721
2025-04-09 09:15:11.977730: Training Step 54/115: batchLoss = 5.3190, diffLoss = 8.7378, kgLoss = 0.1909
2025-04-09 09:15:12.690842: Training Step 55/115: batchLoss = 5.0511, diffLoss = 8.2999, kgLoss = 0.1779
2025-04-09 09:15:13.438067: Training Step 56/115: batchLoss = 5.3253, diffLoss = 8.7496, kgLoss = 0.1890
2025-04-09 09:15:14.186547: Training Step 57/115: batchLoss = 5.1727, diffLoss = 8.4950, kgLoss = 0.1893
2025-04-09 09:15:14.912752: Training Step 58/115: batchLoss = 4.7073, diffLoss = 7.7304, kgLoss = 0.1727
2025-04-09 09:15:15.647597: Training Step 59/115: batchLoss = 5.0957, diffLoss = 8.3729, kgLoss = 0.1799
2025-04-09 09:15:16.373711: Training Step 60/115: batchLoss = 5.3396, diffLoss = 8.7730, kgLoss = 0.1896
2025-04-09 09:15:17.096790: Training Step 61/115: batchLoss = 4.8767, diffLoss = 8.0036, kgLoss = 0.1864
2025-04-09 09:15:17.820518: Training Step 62/115: batchLoss = 5.3780, diffLoss = 8.8370, kgLoss = 0.1896
2025-04-09 09:15:18.541421: Training Step 63/115: batchLoss = 4.5024, diffLoss = 7.3951, kgLoss = 0.1635
2025-04-09 09:15:19.263813: Training Step 64/115: batchLoss = 4.7476, diffLoss = 7.7940, kgLoss = 0.1780
2025-04-09 09:15:19.981739: Training Step 65/115: batchLoss = 4.8388, diffLoss = 7.9434, kgLoss = 0.1819
2025-04-09 09:15:20.697781: Training Step 66/115: batchLoss = 4.4385, diffLoss = 7.2868, kgLoss = 0.1659
2025-04-09 09:15:21.415938: Training Step 67/115: batchLoss = 4.9138, diffLoss = 8.0711, kgLoss = 0.1779
2025-04-09 09:15:22.149279: Training Step 68/115: batchLoss = 4.8013, diffLoss = 7.8881, kgLoss = 0.1710
2025-04-09 09:15:22.863654: Training Step 69/115: batchLoss = 4.5294, diffLoss = 7.4325, kgLoss = 0.1747
2025-04-09 09:15:23.582427: Training Step 70/115: batchLoss = 5.0206, diffLoss = 8.2455, kgLoss = 0.1833
2025-04-09 09:15:24.310464: Training Step 71/115: batchLoss = 4.4349, diffLoss = 7.2842, kgLoss = 0.1611
2025-04-09 09:15:25.023743: Training Step 72/115: batchLoss = 4.1819, diffLoss = 6.8625, kgLoss = 0.1610
2025-04-09 09:15:25.742561: Training Step 73/115: batchLoss = 4.7115, diffLoss = 7.7383, kgLoss = 0.1713
2025-04-09 09:15:26.456679: Training Step 74/115: batchLoss = 4.1023, diffLoss = 6.7309, kgLoss = 0.1593
2025-04-09 09:15:27.177625: Training Step 75/115: batchLoss = 4.9649, diffLoss = 8.1584, kgLoss = 0.1747
2025-04-09 09:15:27.898606: Training Step 76/115: batchLoss = 4.7316, diffLoss = 7.7719, kgLoss = 0.1711
2025-04-09 09:15:28.623473: Training Step 77/115: batchLoss = 5.1155, diffLoss = 8.4004, kgLoss = 0.1882
2025-04-09 09:15:29.338776: Training Step 78/115: batchLoss = 4.6669, diffLoss = 7.6679, kgLoss = 0.1655
2025-04-09 09:15:30.054527: Training Step 79/115: batchLoss = 4.4030, diffLoss = 7.2216, kgLoss = 0.1752
2025-04-09 09:15:30.771364: Training Step 80/115: batchLoss = 4.7294, diffLoss = 7.7524, kgLoss = 0.1948
2025-04-09 09:15:31.479907: Training Step 81/115: batchLoss = 4.2167, diffLoss = 6.9190, kgLoss = 0.1633
2025-04-09 09:15:32.191246: Training Step 82/115: batchLoss = 5.0926, diffLoss = 8.3621, kgLoss = 0.1883
2025-04-09 09:15:32.919733: Training Step 83/115: batchLoss = 4.9882, diffLoss = 8.1951, kgLoss = 0.1779
2025-04-09 09:15:33.635327: Training Step 84/115: batchLoss = 4.9918, diffLoss = 8.1990, kgLoss = 0.1808
2025-04-09 09:15:34.343502: Training Step 85/115: batchLoss = 5.0153, diffLoss = 8.2401, kgLoss = 0.1781
2025-04-09 09:15:35.063287: Training Step 86/115: batchLoss = 4.5563, diffLoss = 7.4801, kgLoss = 0.1707
2025-04-09 09:15:35.778992: Training Step 87/115: batchLoss = 4.9929, diffLoss = 8.2069, kgLoss = 0.1719
2025-04-09 09:15:36.487721: Training Step 88/115: batchLoss = 3.8931, diffLoss = 6.3941, kgLoss = 0.1415
2025-04-09 09:15:37.203062: Training Step 89/115: batchLoss = 4.3302, diffLoss = 7.1057, kgLoss = 0.1670
2025-04-09 09:15:37.924570: Training Step 90/115: batchLoss = 4.1588, diffLoss = 6.8122, kgLoss = 0.1788
2025-04-09 09:15:38.634851: Training Step 91/115: batchLoss = 5.1140, diffLoss = 8.3948, kgLoss = 0.1929
2025-04-09 09:15:39.340918: Training Step 92/115: batchLoss = 4.9737, diffLoss = 8.1634, kgLoss = 0.1892
2025-04-09 09:15:40.046910: Training Step 93/115: batchLoss = 5.0401, diffLoss = 8.2791, kgLoss = 0.1817
2025-04-09 09:15:40.759326: Training Step 94/115: batchLoss = 4.5791, diffLoss = 7.5198, kgLoss = 0.1681
2025-04-09 09:15:41.468647: Training Step 95/115: batchLoss = 5.7255, diffLoss = 9.3964, kgLoss = 0.2191
2025-04-09 09:15:42.195625: Training Step 96/115: batchLoss = 5.0905, diffLoss = 8.3622, kgLoss = 0.1829
2025-04-09 09:15:42.924171: Training Step 97/115: batchLoss = 5.1549, diffLoss = 8.4516, kgLoss = 0.2099
2025-04-09 09:15:43.637546: Training Step 98/115: batchLoss = 5.2803, diffLoss = 8.6742, kgLoss = 0.1895
2025-04-09 09:15:44.351447: Training Step 99/115: batchLoss = 5.1088, diffLoss = 8.3899, kgLoss = 0.1870
2025-04-09 09:15:45.069509: Training Step 100/115: batchLoss = 4.9107, diffLoss = 8.0697, kgLoss = 0.1722
2025-04-09 09:15:45.808658: Training Step 101/115: batchLoss = 5.2360, diffLoss = 8.5977, kgLoss = 0.1935
2025-04-09 09:15:46.523049: Training Step 102/115: batchLoss = 4.2005, diffLoss = 6.8856, kgLoss = 0.1728
2025-04-09 09:15:47.238468: Training Step 103/115: batchLoss = 4.8689, diffLoss = 7.9995, kgLoss = 0.1730
2025-04-09 09:15:47.971288: Training Step 104/115: batchLoss = 4.8763, diffLoss = 8.0075, kgLoss = 0.1796
2025-04-09 09:15:48.692073: Training Step 105/115: batchLoss = 5.2881, diffLoss = 8.6875, kgLoss = 0.1890
2025-04-09 09:15:49.416294: Training Step 106/115: batchLoss = 5.4861, diffLoss = 9.0176, kgLoss = 0.1888
2025-04-09 09:15:50.140740: Training Step 107/115: batchLoss = 4.7993, diffLoss = 7.8785, kgLoss = 0.1806
2025-04-09 09:15:50.855833: Training Step 108/115: batchLoss = 4.5928, diffLoss = 7.5365, kgLoss = 0.1774
2025-04-09 09:15:51.587525: Training Step 109/115: batchLoss = 4.1359, diffLoss = 6.7854, kgLoss = 0.1616
2025-04-09 09:15:52.295145: Training Step 110/115: batchLoss = 4.8431, diffLoss = 7.9446, kgLoss = 0.1909
2025-04-09 09:15:53.007869: Training Step 111/115: batchLoss = 4.5568, diffLoss = 7.4859, kgLoss = 0.1632
2025-04-09 09:15:53.707968: Training Step 112/115: batchLoss = 4.4976, diffLoss = 7.3707, kgLoss = 0.1879
2025-04-09 09:15:54.346570: Training Step 113/115: batchLoss = 4.7948, diffLoss = 7.8795, kgLoss = 0.1679
2025-04-09 09:15:54.969797: Training Step 114/115: batchLoss = 4.6345, diffLoss = 7.6095, kgLoss = 0.1720
2025-04-09 09:15:55.072849: 
2025-04-09 09:15:55.073531: Epoch 0/1000, Train: epLoss = 1.3843, epDfLoss = 2.2731, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:15:55.845353: Steps 0/90: batch_recall = 29.80, batch_ndcg = 21.59 
2025-04-09 09:15:56.611008: Steps 1/90: batch_recall = 28.48, batch_ndcg = 19.17 
2025-04-09 09:15:57.381503: Steps 2/90: batch_recall = 16.91, batch_ndcg = 10.95 
2025-04-09 09:15:58.128087: Steps 3/90: batch_recall = 18.79, batch_ndcg = 11.18 
2025-04-09 09:15:58.885280: Steps 4/90: batch_recall = 22.51, batch_ndcg = 15.35 
2025-04-09 09:15:59.637324: Steps 5/90: batch_recall = 14.49, batch_ndcg = 10.58 
2025-04-09 09:16:00.380155: Steps 6/90: batch_recall = 20.37, batch_ndcg = 12.38 
2025-04-09 09:16:01.145776: Steps 7/90: batch_recall = 16.07, batch_ndcg = 10.71 
2025-04-09 09:16:01.893119: Steps 8/90: batch_recall = 18.42, batch_ndcg = 12.43 
2025-04-09 09:16:02.654015: Steps 9/90: batch_recall = 16.31, batch_ndcg = 11.37 
2025-04-09 09:16:03.394902: Steps 10/90: batch_recall = 13.50, batch_ndcg = 10.32 
2025-04-09 09:16:04.116836: Steps 11/90: batch_recall = 20.06, batch_ndcg = 13.38 
2025-04-09 09:16:04.841024: Steps 12/90: batch_recall = 15.87, batch_ndcg = 10.19 
2025-04-09 09:16:05.571337: Steps 13/90: batch_recall = 12.48, batch_ndcg = 7.96 
2025-04-09 09:16:06.309388: Steps 14/90: batch_recall = 14.13, batch_ndcg = 9.91 
2025-04-09 09:16:07.037068: Steps 15/90: batch_recall = 19.03, batch_ndcg = 12.10 
2025-04-09 09:16:07.769384: Steps 16/90: batch_recall = 12.02, batch_ndcg = 8.28 
2025-04-09 09:16:08.493815: Steps 17/90: batch_recall = 13.96, batch_ndcg = 9.34 
2025-04-09 09:16:09.225219: Steps 18/90: batch_recall = 10.97, batch_ndcg = 8.02 
2025-04-09 09:16:09.963876: Steps 19/90: batch_recall = 8.72, batch_ndcg = 5.19 
2025-04-09 09:16:10.696662: Steps 20/90: batch_recall = 12.94, batch_ndcg = 8.08 
2025-04-09 09:16:11.434400: Steps 21/90: batch_recall = 13.81, batch_ndcg = 7.80 
2025-04-09 09:16:12.163449: Steps 22/90: batch_recall = 14.30, batch_ndcg = 9.05 
2025-04-09 09:16:12.893742: Steps 23/90: batch_recall = 10.75, batch_ndcg = 7.37 
2025-04-09 09:16:13.638553: Steps 24/90: batch_recall = 14.87, batch_ndcg = 9.84 
2025-04-09 09:16:14.381977: Steps 25/90: batch_recall = 9.58, batch_ndcg = 5.99 
2025-04-09 09:16:15.106223: Steps 26/90: batch_recall = 9.44, batch_ndcg = 5.88 
2025-04-09 09:16:15.820399: Steps 27/90: batch_recall = 8.74, batch_ndcg = 5.63 
2025-04-09 09:16:16.546248: Steps 28/90: batch_recall = 6.85, batch_ndcg = 4.97 
2025-04-09 09:16:17.262882: Steps 29/90: batch_recall = 9.37, batch_ndcg = 6.53 
2025-04-09 09:16:17.987778: Steps 30/90: batch_recall = 11.44, batch_ndcg = 7.08 
2025-04-09 09:16:18.706122: Steps 31/90: batch_recall = 9.48, batch_ndcg = 6.54 
2025-04-09 09:16:19.410965: Steps 32/90: batch_recall = 11.37, batch_ndcg = 6.05 
2025-04-09 09:16:20.115810: Steps 33/90: batch_recall = 6.28, batch_ndcg = 4.22 
2025-04-09 09:16:20.834101: Steps 34/90: batch_recall = 6.55, batch_ndcg = 3.79 
2025-04-09 09:16:21.538079: Steps 35/90: batch_recall = 9.33, batch_ndcg = 6.47 
2025-04-09 09:16:22.259343: Steps 36/90: batch_recall = 7.80, batch_ndcg = 4.54 
2025-04-09 09:16:22.991569: Steps 37/90: batch_recall = 5.67, batch_ndcg = 3.65 
2025-04-09 09:16:23.728027: Steps 38/90: batch_recall = 8.76, batch_ndcg = 5.27 
2025-04-09 09:16:24.441156: Steps 39/90: batch_recall = 7.54, batch_ndcg = 3.79 
2025-04-09 09:16:25.145317: Steps 40/90: batch_recall = 8.86, batch_ndcg = 5.77 
2025-04-09 09:16:25.858360: Steps 41/90: batch_recall = 8.87, batch_ndcg = 5.69 
2025-04-09 09:16:26.562630: Steps 42/90: batch_recall = 6.71, batch_ndcg = 3.33 
2025-04-09 09:16:27.278500: Steps 43/90: batch_recall = 6.98, batch_ndcg = 4.38 
2025-04-09 09:16:27.990331: Steps 44/90: batch_recall = 6.04, batch_ndcg = 3.34 
2025-04-09 09:16:28.699767: Steps 45/90: batch_recall = 6.77, batch_ndcg = 3.74 
2025-04-09 09:16:29.400627: Steps 46/90: batch_recall = 6.24, batch_ndcg = 3.56 
2025-04-09 09:16:30.108711: Steps 47/90: batch_recall = 7.44, batch_ndcg = 4.77 
2025-04-09 09:16:30.818721: Steps 48/90: batch_recall = 8.96, batch_ndcg = 4.88 
2025-04-09 09:16:31.532812: Steps 49/90: batch_recall = 7.12, batch_ndcg = 3.93 
2025-04-09 09:16:32.240552: Steps 50/90: batch_recall = 7.51, batch_ndcg = 4.66 
2025-04-09 09:16:32.942373: Steps 51/90: batch_recall = 6.75, batch_ndcg = 3.31 
2025-04-09 09:16:33.645613: Steps 52/90: batch_recall = 5.97, batch_ndcg = 2.95 
2025-04-09 09:16:34.364227: Steps 53/90: batch_recall = 3.53, batch_ndcg = 2.14 
2025-04-09 09:16:35.073186: Steps 54/90: batch_recall = 5.42, batch_ndcg = 3.10 
2025-04-09 09:16:35.792931: Steps 55/90: batch_recall = 8.08, batch_ndcg = 4.58 
2025-04-09 09:16:36.511542: Steps 56/90: batch_recall = 6.30, batch_ndcg = 3.66 
2025-04-09 09:16:37.213023: Steps 57/90: batch_recall = 4.92, batch_ndcg = 2.72 
2025-04-09 09:16:37.914356: Steps 58/90: batch_recall = 8.89, batch_ndcg = 4.54 
2025-04-09 09:16:38.610697: Steps 59/90: batch_recall = 5.11, batch_ndcg = 3.06 
2025-04-09 09:16:39.327991: Steps 60/90: batch_recall = 5.40, batch_ndcg = 4.05 
2025-04-09 09:16:40.042249: Steps 61/90: batch_recall = 5.80, batch_ndcg = 4.03 
2025-04-09 09:16:40.766537: Steps 62/90: batch_recall = 7.25, batch_ndcg = 4.24 
2025-04-09 09:16:41.502484: Steps 63/90: batch_recall = 4.08, batch_ndcg = 2.47 
2025-04-09 09:16:42.257584: Steps 64/90: batch_recall = 3.91, batch_ndcg = 2.54 
2025-04-09 09:16:42.980483: Steps 65/90: batch_recall = 6.74, batch_ndcg = 4.06 
2025-04-09 09:16:43.698098: Steps 66/90: batch_recall = 6.41, batch_ndcg = 4.15 
2025-04-09 09:16:44.413988: Steps 67/90: batch_recall = 3.57, batch_ndcg = 2.78 
2025-04-09 09:16:45.118421: Steps 68/90: batch_recall = 4.39, batch_ndcg = 2.74 
2025-04-09 09:16:45.829726: Steps 69/90: batch_recall = 5.84, batch_ndcg = 2.83 
2025-04-09 09:16:46.551876: Steps 70/90: batch_recall = 4.87, batch_ndcg = 3.23 
2025-04-09 09:16:47.288986: Steps 71/90: batch_recall = 6.71, batch_ndcg = 4.60 
2025-04-09 09:16:48.016341: Steps 72/90: batch_recall = 3.77, batch_ndcg = 2.30 
2025-04-09 09:16:48.727654: Steps 73/90: batch_recall = 5.21, batch_ndcg = 3.11 
2025-04-09 09:16:49.465654: Steps 74/90: batch_recall = 5.10, batch_ndcg = 2.90 
2025-04-09 09:16:50.203023: Steps 75/90: batch_recall = 4.79, batch_ndcg = 2.28 
2025-04-09 09:16:50.926824: Steps 76/90: batch_recall = 5.64, batch_ndcg = 3.20 
2025-04-09 09:16:51.638461: Steps 77/90: batch_recall = 7.24, batch_ndcg = 3.67 
2025-04-09 09:16:52.346914: Steps 78/90: batch_recall = 2.74, batch_ndcg = 1.47 
2025-04-09 09:16:53.047502: Steps 79/90: batch_recall = 8.90, batch_ndcg = 5.22 
2025-04-09 09:16:53.754980: Steps 80/90: batch_recall = 5.96, batch_ndcg = 3.02 
2025-04-09 09:16:54.464497: Steps 81/90: batch_recall = 5.57, batch_ndcg = 2.79 
2025-04-09 09:16:55.189210: Steps 82/90: batch_recall = 7.04, batch_ndcg = 4.23 
2025-04-09 09:16:55.908557: Steps 83/90: batch_recall = 6.45, batch_ndcg = 3.53 
2025-04-09 09:16:56.653315: Steps 84/90: batch_recall = 4.66, batch_ndcg = 3.31 
2025-04-09 09:16:57.406360: Steps 85/90: batch_recall = 6.17, batch_ndcg = 3.17 
2025-04-09 09:16:58.161677: Steps 86/90: batch_recall = 6.30, batch_ndcg = 3.67 
2025-04-09 09:16:58.877769: Steps 87/90: batch_recall = 5.68, batch_ndcg = 3.47 
2025-04-09 09:16:59.576427: Steps 88/90: batch_recall = 5.18, batch_ndcg = 3.20 
2025-04-09 09:17:00.134686: Steps 89/90: batch_recall = 4.87, batch_ndcg = 2.57 
2025-04-09 09:17:00.135330: Epoch 0/1000, Test: Recall = 0.0183, NDCG = 0.0115  

2025-04-09 09:17:01.139192: Training Step 0/115: batchLoss = 5.5266, diffLoss = 9.0722, kgLoss = 0.2083
2025-04-09 09:17:01.866228: Training Step 1/115: batchLoss = 4.6210, diffLoss = 7.5919, kgLoss = 0.1647
2025-04-09 09:17:02.595995: Training Step 2/115: batchLoss = 4.1611, diffLoss = 6.8330, kgLoss = 0.1532
2025-04-09 09:17:03.348668: Training Step 3/115: batchLoss = 5.0746, diffLoss = 8.3359, kgLoss = 0.1828
2025-04-09 09:17:04.078879: Training Step 4/115: batchLoss = 4.4169, diffLoss = 7.2539, kgLoss = 0.1613
2025-04-09 09:17:04.806746: Training Step 5/115: batchLoss = 4.2224, diffLoss = 6.9236, kgLoss = 0.1706
2025-04-09 09:17:05.537106: Training Step 6/115: batchLoss = 4.7308, diffLoss = 7.7697, kgLoss = 0.1725
2025-04-09 09:17:06.279227: Training Step 7/115: batchLoss = 4.6703, diffLoss = 7.6695, kgLoss = 0.1714
2025-04-09 09:17:07.015537: Training Step 8/115: batchLoss = 5.4732, diffLoss = 8.9949, kgLoss = 0.1906
2025-04-09 09:17:07.767946: Training Step 9/115: batchLoss = 4.2389, diffLoss = 6.9546, kgLoss = 0.1654
2025-04-09 09:17:08.523381: Training Step 10/115: batchLoss = 5.2637, diffLoss = 8.6509, kgLoss = 0.1830
2025-04-09 09:17:09.256300: Training Step 11/115: batchLoss = 5.3139, diffLoss = 8.7291, kgLoss = 0.1909
2025-04-09 09:17:09.985317: Training Step 12/115: batchLoss = 4.1498, diffLoss = 6.8129, kgLoss = 0.1550
2025-04-09 09:17:10.718242: Training Step 13/115: batchLoss = 4.2155, diffLoss = 6.9138, kgLoss = 0.1680
2025-04-09 09:17:11.443294: Training Step 14/115: batchLoss = 4.7328, diffLoss = 7.7687, kgLoss = 0.1790
2025-04-09 09:17:12.169856: Training Step 15/115: batchLoss = 5.5927, diffLoss = 9.1806, kgLoss = 0.2109
2025-04-09 09:17:12.895731: Training Step 16/115: batchLoss = 4.6865, diffLoss = 7.6898, kgLoss = 0.1816
2025-04-09 09:17:13.618637: Training Step 17/115: batchLoss = 4.5229, diffLoss = 7.4292, kgLoss = 0.1633
2025-04-09 09:17:14.354334: Training Step 18/115: batchLoss = 5.3608, diffLoss = 8.8111, kgLoss = 0.1853
2025-04-09 09:17:15.103393: Training Step 19/115: batchLoss = 4.9135, diffLoss = 8.0692, kgLoss = 0.1800
2025-04-09 09:17:15.832888: Training Step 20/115: batchLoss = 5.1290, diffLoss = 8.4169, kgLoss = 0.1971
2025-04-09 09:17:16.554989: Training Step 21/115: batchLoss = 5.2271, diffLoss = 8.5827, kgLoss = 0.1938
2025-04-09 09:17:17.277517: Training Step 22/115: batchLoss = 5.0299, diffLoss = 8.2639, kgLoss = 0.1790
2025-04-09 09:17:18.001262: Training Step 23/115: batchLoss = 4.7281, diffLoss = 7.7658, kgLoss = 0.1717
2025-04-09 09:17:18.726817: Training Step 24/115: batchLoss = 4.8649, diffLoss = 7.9883, kgLoss = 0.1798
2025-04-09 09:17:19.438363: Training Step 25/115: batchLoss = 4.5977, diffLoss = 7.5446, kgLoss = 0.1773
2025-04-09 09:17:20.146225: Training Step 26/115: batchLoss = 4.8163, diffLoss = 7.9084, kgLoss = 0.1781
2025-04-09 09:17:20.866547: Training Step 27/115: batchLoss = 5.6015, diffLoss = 9.2047, kgLoss = 0.1968
2025-04-09 09:17:21.598711: Training Step 28/115: batchLoss = 5.1504, diffLoss = 8.4551, kgLoss = 0.1935
2025-04-09 09:17:22.317727: Training Step 29/115: batchLoss = 5.0615, diffLoss = 8.3076, kgLoss = 0.1924
2025-04-09 09:17:23.038845: Training Step 30/115: batchLoss = 4.5824, diffLoss = 7.5273, kgLoss = 0.1650
2025-04-09 09:17:23.763808: Training Step 31/115: batchLoss = 4.6432, diffLoss = 7.6270, kgLoss = 0.1675
2025-04-09 09:17:24.497758: Training Step 32/115: batchLoss = 4.1133, diffLoss = 6.7494, kgLoss = 0.1590
2025-04-09 09:17:25.231993: Training Step 33/115: batchLoss = 5.3817, diffLoss = 8.8432, kgLoss = 0.1895
2025-04-09 09:17:25.971820: Training Step 34/115: batchLoss = 5.2788, diffLoss = 8.6671, kgLoss = 0.1962
2025-04-09 09:17:26.715067: Training Step 35/115: batchLoss = 4.9557, diffLoss = 8.1425, kgLoss = 0.1755
2025-04-09 09:17:27.453474: Training Step 36/115: batchLoss = 4.9324, diffLoss = 8.1004, kgLoss = 0.1804
2025-04-09 09:17:28.183555: Training Step 37/115: batchLoss = 5.1865, diffLoss = 8.5164, kgLoss = 0.1916
2025-04-09 09:17:28.912822: Training Step 38/115: batchLoss = 4.5670, diffLoss = 7.4943, kgLoss = 0.1761
2025-04-09 09:17:29.640714: Training Step 39/115: batchLoss = 3.9333, diffLoss = 6.4501, kgLoss = 0.1581
2025-04-09 09:17:30.371783: Training Step 40/115: batchLoss = 4.7055, diffLoss = 7.7165, kgLoss = 0.1890
2025-04-09 09:17:31.106928: Training Step 41/115: batchLoss = 4.6186, diffLoss = 7.5821, kgLoss = 0.1732
2025-04-09 09:17:31.848588: Training Step 42/115: batchLoss = 5.4327, diffLoss = 8.9263, kgLoss = 0.1922
2025-04-09 09:17:32.599532: Training Step 43/115: batchLoss = 4.3356, diffLoss = 7.1122, kgLoss = 0.1707
2025-04-09 09:17:33.335546: Training Step 44/115: batchLoss = 5.2235, diffLoss = 8.5749, kgLoss = 0.1963
2025-04-09 09:17:34.060019: Training Step 45/115: batchLoss = 5.4615, diffLoss = 8.9742, kgLoss = 0.1925
2025-04-09 09:17:34.782231: Training Step 46/115: batchLoss = 5.0747, diffLoss = 8.3380, kgLoss = 0.1798
2025-04-09 09:17:35.516431: Training Step 47/115: batchLoss = 4.6228, diffLoss = 7.5861, kgLoss = 0.1779
2025-04-09 09:17:36.259073: Training Step 48/115: batchLoss = 4.6311, diffLoss = 7.6017, kgLoss = 0.1753
2025-04-09 09:17:36.989637: Training Step 49/115: batchLoss = 4.3849, diffLoss = 7.2039, kgLoss = 0.1564
2025-04-09 09:17:37.713499: Training Step 50/115: batchLoss = 4.1187, diffLoss = 6.7664, kgLoss = 0.1472
2025-04-09 09:17:38.451612: Training Step 51/115: batchLoss = 4.8481, diffLoss = 7.9510, kgLoss = 0.1939
2025-04-09 09:17:39.173360: Training Step 52/115: batchLoss = 4.8151, diffLoss = 7.9110, kgLoss = 0.1712
2025-04-09 09:17:39.903081: Training Step 53/115: batchLoss = 4.8738, diffLoss = 8.0034, kgLoss = 0.1793
2025-04-09 09:17:40.631156: Training Step 54/115: batchLoss = 5.0082, diffLoss = 8.2265, kgLoss = 0.1808
2025-04-09 09:17:41.368555: Training Step 55/115: batchLoss = 4.9905, diffLoss = 8.1911, kgLoss = 0.1896
2025-04-09 09:17:42.088840: Training Step 56/115: batchLoss = 4.4091, diffLoss = 7.2314, kgLoss = 0.1757
2025-04-09 09:17:42.810319: Training Step 57/115: batchLoss = 4.5327, diffLoss = 7.4402, kgLoss = 0.1715
2025-04-09 09:17:43.538143: Training Step 58/115: batchLoss = 4.7302, diffLoss = 7.7692, kgLoss = 0.1717
2025-04-09 09:17:44.259712: Training Step 59/115: batchLoss = 4.9243, diffLoss = 8.0820, kgLoss = 0.1877
2025-04-09 09:17:44.990402: Training Step 60/115: batchLoss = 5.3380, diffLoss = 8.7678, kgLoss = 0.1932
2025-04-09 09:17:45.717853: Training Step 61/115: batchLoss = 4.5849, diffLoss = 7.5309, kgLoss = 0.1659
2025-04-09 09:17:46.432763: Training Step 62/115: batchLoss = 4.4313, diffLoss = 7.2757, kgLoss = 0.1647
2025-04-09 09:17:47.158896: Training Step 63/115: batchLoss = 5.5417, diffLoss = 9.0966, kgLoss = 0.2093
2025-04-09 09:17:47.880497: Training Step 64/115: batchLoss = 4.7224, diffLoss = 7.7552, kgLoss = 0.1731
2025-04-09 09:17:48.602365: Training Step 65/115: batchLoss = 4.5440, diffLoss = 7.4519, kgLoss = 0.1821
2025-04-09 09:17:49.327720: Training Step 66/115: batchLoss = 5.0897, diffLoss = 8.3514, kgLoss = 0.1973
2025-04-09 09:17:50.047778: Training Step 67/115: batchLoss = 6.2580, diffLoss = 10.2756, kgLoss = 0.2317
2025-04-09 09:17:50.765838: Training Step 68/115: batchLoss = 5.0858, diffLoss = 8.3556, kgLoss = 0.1810
2025-04-09 09:17:51.476881: Training Step 69/115: batchLoss = 4.9759, diffLoss = 8.1696, kgLoss = 0.1853
2025-04-09 09:17:52.187954: Training Step 70/115: batchLoss = 4.2425, diffLoss = 6.9603, kgLoss = 0.1657
2025-04-09 09:17:52.901989: Training Step 71/115: batchLoss = 4.6578, diffLoss = 7.6496, kgLoss = 0.1701
2025-04-09 09:17:53.622188: Training Step 72/115: batchLoss = 4.3855, diffLoss = 7.1957, kgLoss = 0.1703
2025-04-09 09:17:54.339658: Training Step 73/115: batchLoss = 4.5170, diffLoss = 7.4079, kgLoss = 0.1806
2025-04-09 09:17:55.060932: Training Step 74/115: batchLoss = 4.6951, diffLoss = 7.7123, kgLoss = 0.1694
2025-04-09 09:17:55.791732: Training Step 75/115: batchLoss = 4.6149, diffLoss = 7.5819, kgLoss = 0.1644
2025-04-09 09:17:56.519969: Training Step 76/115: batchLoss = 4.3008, diffLoss = 7.0541, kgLoss = 0.1707
2025-04-09 09:17:57.248527: Training Step 77/115: batchLoss = 4.9625, diffLoss = 8.1519, kgLoss = 0.1784
2025-04-09 09:17:57.978934: Training Step 78/115: batchLoss = 5.2335, diffLoss = 8.5920, kgLoss = 0.1958
2025-04-09 09:17:58.705474: Training Step 79/115: batchLoss = 5.3963, diffLoss = 8.8606, kgLoss = 0.2000
2025-04-09 09:17:59.422915: Training Step 80/115: batchLoss = 5.3139, diffLoss = 8.7231, kgLoss = 0.2001
2025-04-09 09:18:00.147926: Training Step 81/115: batchLoss = 4.8491, diffLoss = 7.9600, kgLoss = 0.1829
2025-04-09 09:18:00.865274: Training Step 82/115: batchLoss = 5.4144, diffLoss = 8.8972, kgLoss = 0.1900
2025-04-09 09:18:01.581541: Training Step 83/115: batchLoss = 5.0110, diffLoss = 8.2253, kgLoss = 0.1895
2025-04-09 09:18:02.299757: Training Step 84/115: batchLoss = 4.0833, diffLoss = 6.6976, kgLoss = 0.1619
2025-04-09 09:18:03.017857: Training Step 85/115: batchLoss = 4.4424, diffLoss = 7.2943, kgLoss = 0.1645
2025-04-09 09:18:03.740299: Training Step 86/115: batchLoss = 4.3578, diffLoss = 7.1555, kgLoss = 0.1611
2025-04-09 09:18:04.471106: Training Step 87/115: batchLoss = 3.7621, diffLoss = 6.1695, kgLoss = 0.1511
2025-04-09 09:18:05.192982: Training Step 88/115: batchLoss = 4.5926, diffLoss = 7.5423, kgLoss = 0.1681
2025-04-09 09:18:05.916289: Training Step 89/115: batchLoss = 4.9356, diffLoss = 8.1087, kgLoss = 0.1759
2025-04-09 09:18:06.632361: Training Step 90/115: batchLoss = 4.8071, diffLoss = 7.8924, kgLoss = 0.1791
2025-04-09 09:18:07.350075: Training Step 91/115: batchLoss = 4.4068, diffLoss = 7.2318, kgLoss = 0.1693
2025-04-09 09:18:08.074499: Training Step 92/115: batchLoss = 4.5605, diffLoss = 7.4879, kgLoss = 0.1694
2025-04-09 09:18:08.809329: Training Step 93/115: batchLoss = 4.7567, diffLoss = 7.8111, kgLoss = 0.1752
2025-04-09 09:18:09.561573: Training Step 94/115: batchLoss = 5.1237, diffLoss = 8.4139, kgLoss = 0.1885
2025-04-09 09:18:10.311375: Training Step 95/115: batchLoss = 4.5218, diffLoss = 7.4268, kgLoss = 0.1643
2025-04-09 09:18:11.051451: Training Step 96/115: batchLoss = 4.6898, diffLoss = 7.7002, kgLoss = 0.1741
2025-04-09 09:18:11.781140: Training Step 97/115: batchLoss = 4.5244, diffLoss = 7.4308, kgLoss = 0.1649
2025-04-09 09:18:12.510650: Training Step 98/115: batchLoss = 4.5268, diffLoss = 7.4287, kgLoss = 0.1740
2025-04-09 09:18:13.235390: Training Step 99/115: batchLoss = 4.8587, diffLoss = 7.9751, kgLoss = 0.1839
2025-04-09 09:18:13.954347: Training Step 100/115: batchLoss = 4.8963, diffLoss = 8.0422, kgLoss = 0.1776
2025-04-09 09:18:14.689172: Training Step 101/115: batchLoss = 5.0786, diffLoss = 8.3410, kgLoss = 0.1851
2025-04-09 09:18:15.418923: Training Step 102/115: batchLoss = 4.8190, diffLoss = 7.9157, kgLoss = 0.1741
2025-04-09 09:18:16.153062: Training Step 103/115: batchLoss = 4.9417, diffLoss = 8.1080, kgLoss = 0.1923
2025-04-09 09:18:16.890779: Training Step 104/115: batchLoss = 4.6357, diffLoss = 7.6172, kgLoss = 0.1635
2025-04-09 09:18:17.627118: Training Step 105/115: batchLoss = 4.6309, diffLoss = 7.6026, kgLoss = 0.1733
2025-04-09 09:18:18.362590: Training Step 106/115: batchLoss = 4.4512, diffLoss = 7.3092, kgLoss = 0.1641
2025-04-09 09:18:19.078104: Training Step 107/115: batchLoss = 4.9431, diffLoss = 8.1175, kgLoss = 0.1815
2025-04-09 09:18:19.790247: Training Step 108/115: batchLoss = 4.6898, diffLoss = 7.6937, kgLoss = 0.1838
2025-04-09 09:18:20.500067: Training Step 109/115: batchLoss = 4.8316, diffLoss = 7.9338, kgLoss = 0.1782
2025-04-09 09:18:21.216374: Training Step 110/115: batchLoss = 4.2296, diffLoss = 6.9458, kgLoss = 0.1554
2025-04-09 09:18:21.942296: Training Step 111/115: batchLoss = 4.7481, diffLoss = 7.8002, kgLoss = 0.1699
2025-04-09 09:18:22.643855: Training Step 112/115: batchLoss = 4.3041, diffLoss = 7.0668, kgLoss = 0.1600
2025-04-09 09:18:23.295275: Training Step 113/115: batchLoss = 4.3927, diffLoss = 7.2150, kgLoss = 0.1593
2025-04-09 09:18:23.941078: Training Step 114/115: batchLoss = 4.6862, diffLoss = 7.6946, kgLoss = 0.1735
2025-04-09 09:18:24.058352: 
2025-04-09 09:18:24.058911: Epoch 1/1000, Train: epLoss = 1.3770, epDfLoss = 2.2609, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:18:24.816021: Steps 0/90: batch_recall = 34.67, batch_ndcg = 26.36 
2025-04-09 09:18:25.544472: Steps 1/90: batch_recall = 39.15, batch_ndcg = 25.42 
2025-04-09 09:18:26.289858: Steps 2/90: batch_recall = 25.43, batch_ndcg = 17.02 
2025-04-09 09:18:27.028697: Steps 3/90: batch_recall = 25.56, batch_ndcg = 16.62 
2025-04-09 09:18:27.760609: Steps 4/90: batch_recall = 29.08, batch_ndcg = 21.13 
2025-04-09 09:18:28.499917: Steps 5/90: batch_recall = 17.52, batch_ndcg = 13.87 
2025-04-09 09:18:29.227375: Steps 6/90: batch_recall = 25.78, batch_ndcg = 16.67 
2025-04-09 09:18:29.965176: Steps 7/90: batch_recall = 21.62, batch_ndcg = 15.41 
2025-04-09 09:18:30.697509: Steps 8/90: batch_recall = 24.26, batch_ndcg = 16.93 
2025-04-09 09:18:31.447842: Steps 9/90: batch_recall = 22.00, batch_ndcg = 15.60 
2025-04-09 09:18:32.199578: Steps 10/90: batch_recall = 19.69, batch_ndcg = 14.94 
2025-04-09 09:18:32.917090: Steps 11/90: batch_recall = 25.98, batch_ndcg = 17.35 
2025-04-09 09:18:33.630062: Steps 12/90: batch_recall = 20.20, batch_ndcg = 13.49 
2025-04-09 09:18:34.360609: Steps 13/90: batch_recall = 17.06, batch_ndcg = 12.03 
2025-04-09 09:18:35.087898: Steps 14/90: batch_recall = 18.03, batch_ndcg = 13.31 
2025-04-09 09:18:35.811657: Steps 15/90: batch_recall = 24.25, batch_ndcg = 16.44 
2025-04-09 09:18:36.536195: Steps 16/90: batch_recall = 15.62, batch_ndcg = 11.46 
2025-04-09 09:18:37.264315: Steps 17/90: batch_recall = 18.35, batch_ndcg = 13.85 
2025-04-09 09:18:37.988779: Steps 18/90: batch_recall = 16.55, batch_ndcg = 11.86 
2025-04-09 09:18:38.713206: Steps 19/90: batch_recall = 13.31, batch_ndcg = 8.30 
2025-04-09 09:18:39.430219: Steps 20/90: batch_recall = 18.77, batch_ndcg = 11.71 
2025-04-09 09:18:40.172719: Steps 21/90: batch_recall = 19.73, batch_ndcg = 12.85 
2025-04-09 09:18:40.892560: Steps 22/90: batch_recall = 20.22, batch_ndcg = 12.73 
2025-04-09 09:18:41.608925: Steps 23/90: batch_recall = 12.58, batch_ndcg = 9.82 
2025-04-09 09:18:42.316615: Steps 24/90: batch_recall = 19.59, batch_ndcg = 13.13 
2025-04-09 09:18:43.033273: Steps 25/90: batch_recall = 13.61, batch_ndcg = 8.96 
2025-04-09 09:18:43.747499: Steps 26/90: batch_recall = 13.91, batch_ndcg = 9.42 
2025-04-09 09:18:44.464187: Steps 27/90: batch_recall = 11.45, batch_ndcg = 8.32 
2025-04-09 09:18:45.177218: Steps 28/90: batch_recall = 11.79, batch_ndcg = 8.29 
2025-04-09 09:18:45.891137: Steps 29/90: batch_recall = 11.95, batch_ndcg = 8.61 
2025-04-09 09:18:46.609925: Steps 30/90: batch_recall = 14.94, batch_ndcg = 9.58 
2025-04-09 09:18:47.343667: Steps 31/90: batch_recall = 11.40, batch_ndcg = 8.04 
2025-04-09 09:18:48.064917: Steps 32/90: batch_recall = 15.48, batch_ndcg = 8.46 
2025-04-09 09:18:48.785661: Steps 33/90: batch_recall = 8.57, batch_ndcg = 6.09 
2025-04-09 09:18:49.512385: Steps 34/90: batch_recall = 10.92, batch_ndcg = 7.03 
2025-04-09 09:18:50.217253: Steps 35/90: batch_recall = 14.35, batch_ndcg = 8.50 
2025-04-09 09:18:50.931953: Steps 36/90: batch_recall = 13.36, batch_ndcg = 8.14 
2025-04-09 09:18:51.640713: Steps 37/90: batch_recall = 9.11, batch_ndcg = 6.38 
2025-04-09 09:18:52.352263: Steps 38/90: batch_recall = 10.64, batch_ndcg = 7.41 
2025-04-09 09:18:53.066372: Steps 39/90: batch_recall = 10.30, batch_ndcg = 5.66 
2025-04-09 09:18:53.773710: Steps 40/90: batch_recall = 12.48, batch_ndcg = 8.43 
2025-04-09 09:18:54.493241: Steps 41/90: batch_recall = 13.27, batch_ndcg = 8.92 
2025-04-09 09:18:55.213242: Steps 42/90: batch_recall = 14.33, batch_ndcg = 9.28 
2025-04-09 09:18:55.934713: Steps 43/90: batch_recall = 9.88, batch_ndcg = 6.42 
2025-04-09 09:18:56.666637: Steps 44/90: batch_recall = 8.83, batch_ndcg = 5.70 
2025-04-09 09:18:57.400697: Steps 45/90: batch_recall = 9.63, batch_ndcg = 5.48 
2025-04-09 09:18:58.123887: Steps 46/90: batch_recall = 9.14, batch_ndcg = 5.77 
2025-04-09 09:18:58.841784: Steps 47/90: batch_recall = 10.37, batch_ndcg = 6.50 
2025-04-09 09:18:59.540906: Steps 48/90: batch_recall = 11.04, batch_ndcg = 6.84 
2025-04-09 09:19:00.245278: Steps 49/90: batch_recall = 11.26, batch_ndcg = 6.74 
2025-04-09 09:19:00.953621: Steps 50/90: batch_recall = 13.32, batch_ndcg = 8.37 
2025-04-09 09:19:01.665849: Steps 51/90: batch_recall = 10.24, batch_ndcg = 6.38 
2025-04-09 09:19:02.376691: Steps 52/90: batch_recall = 10.26, batch_ndcg = 5.56 
2025-04-09 09:19:03.072762: Steps 53/90: batch_recall = 7.68, batch_ndcg = 5.06 
2025-04-09 09:19:03.778445: Steps 54/90: batch_recall = 7.95, batch_ndcg = 4.51 
2025-04-09 09:19:04.478893: Steps 55/90: batch_recall = 10.23, batch_ndcg = 6.58 
2025-04-09 09:19:05.189449: Steps 56/90: batch_recall = 8.81, batch_ndcg = 5.78 
2025-04-09 09:19:05.899851: Steps 57/90: batch_recall = 9.56, batch_ndcg = 6.62 
2025-04-09 09:19:06.614038: Steps 58/90: batch_recall = 14.33, batch_ndcg = 7.15 
2025-04-09 09:19:07.336581: Steps 59/90: batch_recall = 9.06, batch_ndcg = 5.11 
2025-04-09 09:19:08.059974: Steps 60/90: batch_recall = 8.56, batch_ndcg = 5.76 
2025-04-09 09:19:08.780727: Steps 61/90: batch_recall = 12.16, batch_ndcg = 7.06 
2025-04-09 09:19:09.492997: Steps 62/90: batch_recall = 10.93, batch_ndcg = 6.87 
2025-04-09 09:19:10.191813: Steps 63/90: batch_recall = 7.59, batch_ndcg = 4.49 
2025-04-09 09:19:10.915105: Steps 64/90: batch_recall = 6.05, batch_ndcg = 3.93 
2025-04-09 09:19:11.626475: Steps 65/90: batch_recall = 10.62, batch_ndcg = 6.69 
2025-04-09 09:19:12.351891: Steps 66/90: batch_recall = 9.41, batch_ndcg = 6.57 
2025-04-09 09:19:13.105522: Steps 67/90: batch_recall = 5.77, batch_ndcg = 4.01 
2025-04-09 09:19:13.809020: Steps 68/90: batch_recall = 9.33, batch_ndcg = 5.62 
2025-04-09 09:19:14.525794: Steps 69/90: batch_recall = 10.13, batch_ndcg = 5.13 
2025-04-09 09:19:15.253060: Steps 70/90: batch_recall = 9.63, batch_ndcg = 6.21 
2025-04-09 09:19:15.967754: Steps 71/90: batch_recall = 8.42, batch_ndcg = 6.37 
2025-04-09 09:19:16.682463: Steps 72/90: batch_recall = 5.59, batch_ndcg = 4.11 
2025-04-09 09:19:17.411421: Steps 73/90: batch_recall = 8.94, batch_ndcg = 5.57 
2025-04-09 09:19:18.130049: Steps 74/90: batch_recall = 8.84, batch_ndcg = 4.91 
2025-04-09 09:19:18.830837: Steps 75/90: batch_recall = 9.06, batch_ndcg = 5.09 
2025-04-09 09:19:19.559249: Steps 76/90: batch_recall = 8.99, batch_ndcg = 5.40 
2025-04-09 09:19:20.296603: Steps 77/90: batch_recall = 12.13, batch_ndcg = 6.55 
2025-04-09 09:19:21.029423: Steps 78/90: batch_recall = 5.63, batch_ndcg = 3.77 
2025-04-09 09:19:21.751077: Steps 79/90: batch_recall = 14.12, batch_ndcg = 8.42 
2025-04-09 09:19:22.474510: Steps 80/90: batch_recall = 9.40, batch_ndcg = 5.09 
2025-04-09 09:19:23.191733: Steps 81/90: batch_recall = 9.76, batch_ndcg = 5.00 
2025-04-09 09:19:23.897337: Steps 82/90: batch_recall = 9.23, batch_ndcg = 5.80 
2025-04-09 09:19:24.598203: Steps 83/90: batch_recall = 10.04, batch_ndcg = 5.92 
2025-04-09 09:19:25.332500: Steps 84/90: batch_recall = 6.90, batch_ndcg = 4.88 
2025-04-09 09:19:26.048198: Steps 85/90: batch_recall = 9.95, batch_ndcg = 5.96 
2025-04-09 09:19:26.763638: Steps 86/90: batch_recall = 9.99, batch_ndcg = 5.71 
2025-04-09 09:19:27.498042: Steps 87/90: batch_recall = 9.81, batch_ndcg = 5.78 
2025-04-09 09:19:28.288188: Steps 88/90: batch_recall = 8.28, batch_ndcg = 4.87 
2025-04-09 09:19:28.851034: Steps 89/90: batch_recall = 6.44, batch_ndcg = 3.40 
2025-04-09 09:19:28.851691: Epoch 1/1000, Test: Recall = 0.0264, NDCG = 0.0173  

2025-04-09 09:19:29.872918: Training Step 0/115: batchLoss = 5.6804, diffLoss = 9.3326, kgLoss = 0.2020
2025-04-09 09:19:30.610745: Training Step 1/115: batchLoss = 4.3446, diffLoss = 7.1366, kgLoss = 0.1566
2025-04-09 09:19:31.343914: Training Step 2/115: batchLoss = 4.2775, diffLoss = 7.0226, kgLoss = 0.1598
2025-04-09 09:19:32.067882: Training Step 3/115: batchLoss = 4.7847, diffLoss = 7.8576, kgLoss = 0.1754
2025-04-09 09:19:32.800660: Training Step 4/115: batchLoss = 4.7704, diffLoss = 7.8370, kgLoss = 0.1705
2025-04-09 09:19:33.533477: Training Step 5/115: batchLoss = 5.2142, diffLoss = 8.5649, kgLoss = 0.1881
2025-04-09 09:19:34.267259: Training Step 6/115: batchLoss = 5.4824, diffLoss = 9.0079, kgLoss = 0.1942
2025-04-09 09:19:34.998747: Training Step 7/115: batchLoss = 5.0963, diffLoss = 8.3716, kgLoss = 0.1835
2025-04-09 09:19:35.712346: Training Step 8/115: batchLoss = 5.0105, diffLoss = 8.2304, kgLoss = 0.1807
2025-04-09 09:19:36.430794: Training Step 9/115: batchLoss = 4.6828, diffLoss = 7.6897, kgLoss = 0.1725
2025-04-09 09:19:37.140308: Training Step 10/115: batchLoss = 4.4899, diffLoss = 7.3776, kgLoss = 0.1585
2025-04-09 09:19:37.864152: Training Step 11/115: batchLoss = 4.6240, diffLoss = 7.5883, kgLoss = 0.1776
2025-04-09 09:19:38.589053: Training Step 12/115: batchLoss = 4.9773, diffLoss = 8.1736, kgLoss = 0.1828
2025-04-09 09:19:39.330164: Training Step 13/115: batchLoss = 5.0904, diffLoss = 8.3560, kgLoss = 0.1920
2025-04-09 09:19:40.068382: Training Step 14/115: batchLoss = 4.5032, diffLoss = 7.3982, kgLoss = 0.1607
2025-04-09 09:19:40.801951: Training Step 15/115: batchLoss = 4.6746, diffLoss = 7.6755, kgLoss = 0.1732
2025-04-09 09:19:41.520135: Training Step 16/115: batchLoss = 5.2004, diffLoss = 8.5445, kgLoss = 0.1842
2025-04-09 09:19:42.242953: Training Step 17/115: batchLoss = 4.3364, diffLoss = 7.1202, kgLoss = 0.1607
2025-04-09 09:19:42.969756: Training Step 18/115: batchLoss = 3.9251, diffLoss = 6.4324, kgLoss = 0.1642
2025-04-09 09:19:43.693521: Training Step 19/115: batchLoss = 4.9108, diffLoss = 8.0662, kgLoss = 0.1778
2025-04-09 09:19:44.421102: Training Step 20/115: batchLoss = 4.2130, diffLoss = 6.9178, kgLoss = 0.1557
2025-04-09 09:19:45.147370: Training Step 21/115: batchLoss = 4.7542, diffLoss = 7.8036, kgLoss = 0.1801
2025-04-09 09:19:45.875564: Training Step 22/115: batchLoss = 4.5998, diffLoss = 7.5470, kgLoss = 0.1790
2025-04-09 09:19:46.596489: Training Step 23/115: batchLoss = 4.5381, diffLoss = 7.4511, kgLoss = 0.1685
2025-04-09 09:19:47.322612: Training Step 24/115: batchLoss = 4.7355, diffLoss = 7.7705, kgLoss = 0.1830
2025-04-09 09:19:48.044500: Training Step 25/115: batchLoss = 3.7789, diffLoss = 6.1962, kgLoss = 0.1528
2025-04-09 09:19:48.770366: Training Step 26/115: batchLoss = 4.8494, diffLoss = 7.9531, kgLoss = 0.1939
2025-04-09 09:19:49.495628: Training Step 27/115: batchLoss = 4.8437, diffLoss = 7.9488, kgLoss = 0.1861
2025-04-09 09:19:50.234015: Training Step 28/115: batchLoss = 4.1391, diffLoss = 6.7866, kgLoss = 0.1678
2025-04-09 09:19:50.987631: Training Step 29/115: batchLoss = 4.7284, diffLoss = 7.7643, kgLoss = 0.1747
2025-04-09 09:19:51.727951: Training Step 30/115: batchLoss = 4.6653, diffLoss = 7.6556, kgLoss = 0.1799
2025-04-09 09:19:52.458653: Training Step 31/115: batchLoss = 5.1093, diffLoss = 8.3881, kgLoss = 0.1910
2025-04-09 09:19:53.183594: Training Step 32/115: batchLoss = 5.2567, diffLoss = 8.6323, kgLoss = 0.1935
2025-04-09 09:19:53.910637: Training Step 33/115: batchLoss = 5.2595, diffLoss = 8.6378, kgLoss = 0.1920
2025-04-09 09:19:54.642177: Training Step 34/115: batchLoss = 5.9249, diffLoss = 9.7359, kgLoss = 0.2084
2025-04-09 09:19:55.379114: Training Step 35/115: batchLoss = 4.4761, diffLoss = 7.3509, kgLoss = 0.1639
2025-04-09 09:19:56.101308: Training Step 36/115: batchLoss = 4.3030, diffLoss = 7.0626, kgLoss = 0.1635
2025-04-09 09:19:56.846088: Training Step 37/115: batchLoss = 5.0610, diffLoss = 8.3089, kgLoss = 0.1891
2025-04-09 09:19:57.575139: Training Step 38/115: batchLoss = 4.1966, diffLoss = 6.8873, kgLoss = 0.1606
2025-04-09 09:19:58.311058: Training Step 39/115: batchLoss = 4.2357, diffLoss = 6.9545, kgLoss = 0.1577
2025-04-09 09:19:59.042161: Training Step 40/115: batchLoss = 4.7356, diffLoss = 7.7789, kgLoss = 0.1705
2025-04-09 09:19:59.769998: Training Step 41/115: batchLoss = 4.8005, diffLoss = 7.8732, kgLoss = 0.1913
2025-04-09 09:20:00.497860: Training Step 42/115: batchLoss = 5.3894, diffLoss = 8.8524, kgLoss = 0.1949
2025-04-09 09:20:01.239432: Training Step 43/115: batchLoss = 4.4127, diffLoss = 7.2428, kgLoss = 0.1676
2025-04-09 09:20:01.988359: Training Step 44/115: batchLoss = 4.1356, diffLoss = 6.7821, kgLoss = 0.1660
2025-04-09 09:20:02.708857: Training Step 45/115: batchLoss = 4.6307, diffLoss = 7.6022, kgLoss = 0.1735
2025-04-09 09:20:03.433502: Training Step 46/115: batchLoss = 4.4683, diffLoss = 7.3397, kgLoss = 0.1613
2025-04-09 09:20:04.166779: Training Step 47/115: batchLoss = 4.7820, diffLoss = 7.8506, kgLoss = 0.1791
2025-04-09 09:20:04.883159: Training Step 48/115: batchLoss = 4.9255, diffLoss = 8.0869, kgLoss = 0.1834
2025-04-09 09:20:05.615653: Training Step 49/115: batchLoss = 4.4526, diffLoss = 7.3099, kgLoss = 0.1665
2025-04-09 09:20:06.354415: Training Step 50/115: batchLoss = 4.8095, diffLoss = 7.8939, kgLoss = 0.1829
2025-04-09 09:20:07.079242: Training Step 51/115: batchLoss = 5.1761, diffLoss = 8.4963, kgLoss = 0.1959
2025-04-09 09:20:07.811936: Training Step 52/115: batchLoss = 4.9023, diffLoss = 8.0548, kgLoss = 0.1734
2025-04-09 09:20:08.543907: Training Step 53/115: batchLoss = 4.1326, diffLoss = 6.7776, kgLoss = 0.1651
2025-04-09 09:20:09.268826: Training Step 54/115: batchLoss = 4.6314, diffLoss = 7.6083, kgLoss = 0.1660
2025-04-09 09:20:09.996188: Training Step 55/115: batchLoss = 4.7176, diffLoss = 7.7509, kgLoss = 0.1677
2025-04-09 09:20:10.722750: Training Step 56/115: batchLoss = 5.1029, diffLoss = 8.3884, kgLoss = 0.1748
2025-04-09 09:20:11.446568: Training Step 57/115: batchLoss = 5.1652, diffLoss = 8.4808, kgLoss = 0.1919
2025-04-09 09:20:12.171784: Training Step 58/115: batchLoss = 4.5260, diffLoss = 7.4304, kgLoss = 0.1695
2025-04-09 09:20:12.906364: Training Step 59/115: batchLoss = 5.1073, diffLoss = 8.3757, kgLoss = 0.2046
2025-04-09 09:20:13.653023: Training Step 60/115: batchLoss = 5.0107, diffLoss = 8.2255, kgLoss = 0.1886
2025-04-09 09:20:14.384087: Training Step 61/115: batchLoss = 5.1692, diffLoss = 8.4818, kgLoss = 0.2002
2025-04-09 09:20:15.128920: Training Step 62/115: batchLoss = 4.5918, diffLoss = 7.5376, kgLoss = 0.1730
2025-04-09 09:20:15.868768: Training Step 63/115: batchLoss = 5.0490, diffLoss = 8.2878, kgLoss = 0.1909
2025-04-09 09:20:16.586555: Training Step 64/115: batchLoss = 4.9839, diffLoss = 8.1819, kgLoss = 0.1869
2025-04-09 09:20:17.298308: Training Step 65/115: batchLoss = 4.1797, diffLoss = 6.8595, kgLoss = 0.1600
2025-04-09 09:20:18.014749: Training Step 66/115: batchLoss = 4.6956, diffLoss = 7.6985, kgLoss = 0.1912
2025-04-09 09:20:18.737051: Training Step 67/115: batchLoss = 4.6008, diffLoss = 7.5597, kgLoss = 0.1624
2025-04-09 09:20:19.462943: Training Step 68/115: batchLoss = 5.0862, diffLoss = 8.3554, kgLoss = 0.1823
2025-04-09 09:20:20.179432: Training Step 69/115: batchLoss = 4.8524, diffLoss = 7.9712, kgLoss = 0.1742
2025-04-09 09:20:20.909637: Training Step 70/115: batchLoss = 4.7400, diffLoss = 7.7812, kgLoss = 0.1783
2025-04-09 09:20:21.631799: Training Step 71/115: batchLoss = 4.3721, diffLoss = 7.1754, kgLoss = 0.1671
2025-04-09 09:20:22.350188: Training Step 72/115: batchLoss = 5.2783, diffLoss = 8.6692, kgLoss = 0.1921
2025-04-09 09:20:23.084508: Training Step 73/115: batchLoss = 5.4062, diffLoss = 8.8772, kgLoss = 0.1997
2025-04-09 09:20:23.803747: Training Step 74/115: batchLoss = 5.0911, diffLoss = 8.3569, kgLoss = 0.1925
2025-04-09 09:20:24.523908: Training Step 75/115: batchLoss = 4.6523, diffLoss = 7.6277, kgLoss = 0.1891
2025-04-09 09:20:25.238244: Training Step 76/115: batchLoss = 4.5564, diffLoss = 7.4799, kgLoss = 0.1712
2025-04-09 09:20:25.951186: Training Step 77/115: batchLoss = 4.6020, diffLoss = 7.5471, kgLoss = 0.1845
2025-04-09 09:20:26.659353: Training Step 78/115: batchLoss = 5.4081, diffLoss = 8.8858, kgLoss = 0.1916
2025-04-09 09:20:27.379580: Training Step 79/115: batchLoss = 4.5638, diffLoss = 7.4904, kgLoss = 0.1739
2025-04-09 09:20:28.105590: Training Step 80/115: batchLoss = 4.7015, diffLoss = 7.7170, kgLoss = 0.1782
2025-04-09 09:20:28.824091: Training Step 81/115: batchLoss = 4.7415, diffLoss = 7.7896, kgLoss = 0.1694
2025-04-09 09:20:29.539281: Training Step 82/115: batchLoss = 4.4702, diffLoss = 7.3300, kgLoss = 0.1805
2025-04-09 09:20:30.256238: Training Step 83/115: batchLoss = 4.5696, diffLoss = 7.4950, kgLoss = 0.1815
2025-04-09 09:20:30.977219: Training Step 84/115: batchLoss = 4.3447, diffLoss = 7.1297, kgLoss = 0.1672
2025-04-09 09:20:31.694025: Training Step 85/115: batchLoss = 4.0044, diffLoss = 6.5679, kgLoss = 0.1591
2025-04-09 09:20:32.412231: Training Step 86/115: batchLoss = 4.9154, diffLoss = 8.0734, kgLoss = 0.1784
2025-04-09 09:20:33.129370: Training Step 87/115: batchLoss = 4.5002, diffLoss = 7.3870, kgLoss = 0.1700
2025-04-09 09:20:33.842657: Training Step 88/115: batchLoss = 4.2408, diffLoss = 6.9514, kgLoss = 0.1748
2025-04-09 09:20:34.560018: Training Step 89/115: batchLoss = 4.7227, diffLoss = 7.7499, kgLoss = 0.1819
2025-04-09 09:20:35.290142: Training Step 90/115: batchLoss = 4.8755, diffLoss = 8.0137, kgLoss = 0.1682
2025-04-09 09:20:36.029041: Training Step 91/115: batchLoss = 4.3843, diffLoss = 7.1950, kgLoss = 0.1682
2025-04-09 09:20:36.760682: Training Step 92/115: batchLoss = 5.0538, diffLoss = 8.2939, kgLoss = 0.1935
2025-04-09 09:20:37.482508: Training Step 93/115: batchLoss = 5.3463, diffLoss = 8.7701, kgLoss = 0.2107
2025-04-09 09:20:38.205700: Training Step 94/115: batchLoss = 4.9009, diffLoss = 8.0490, kgLoss = 0.1788
2025-04-09 09:20:38.938303: Training Step 95/115: batchLoss = 5.1413, diffLoss = 8.4439, kgLoss = 0.1873
2025-04-09 09:20:39.663854: Training Step 96/115: batchLoss = 4.5303, diffLoss = 7.4411, kgLoss = 0.1641
2025-04-09 09:20:40.394796: Training Step 97/115: batchLoss = 5.3188, diffLoss = 8.7356, kgLoss = 0.1937
2025-04-09 09:20:41.132163: Training Step 98/115: batchLoss = 4.4885, diffLoss = 7.3681, kgLoss = 0.1692
2025-04-09 09:20:41.868312: Training Step 99/115: batchLoss = 4.9801, diffLoss = 8.1762, kgLoss = 0.1860
2025-04-09 09:20:42.590183: Training Step 100/115: batchLoss = 4.4021, diffLoss = 7.2202, kgLoss = 0.1749
2025-04-09 09:20:43.311123: Training Step 101/115: batchLoss = 5.6200, diffLoss = 9.2326, kgLoss = 0.2010
2025-04-09 09:20:44.031796: Training Step 102/115: batchLoss = 4.6300, diffLoss = 7.5993, kgLoss = 0.1761
2025-04-09 09:20:44.762524: Training Step 103/115: batchLoss = 4.2661, diffLoss = 6.9896, kgLoss = 0.1807
2025-04-09 09:20:45.489877: Training Step 104/115: batchLoss = 4.6196, diffLoss = 7.5843, kgLoss = 0.1724
2025-04-09 09:20:46.212743: Training Step 105/115: batchLoss = 4.8518, diffLoss = 7.9630, kgLoss = 0.1851
2025-04-09 09:20:46.934943: Training Step 106/115: batchLoss = 4.9223, diffLoss = 8.0847, kgLoss = 0.1786
2025-04-09 09:20:47.671473: Training Step 107/115: batchLoss = 4.9369, diffLoss = 8.1080, kgLoss = 0.1804
2025-04-09 09:20:48.392556: Training Step 108/115: batchLoss = 4.2552, diffLoss = 6.9818, kgLoss = 0.1654
2025-04-09 09:20:49.109568: Training Step 109/115: batchLoss = 4.6436, diffLoss = 7.6277, kgLoss = 0.1674
2025-04-09 09:20:49.834376: Training Step 110/115: batchLoss = 4.4342, diffLoss = 7.2871, kgLoss = 0.1548
2025-04-09 09:20:50.561932: Training Step 111/115: batchLoss = 4.6786, diffLoss = 7.6798, kgLoss = 0.1767
2025-04-09 09:20:51.281935: Training Step 112/115: batchLoss = 4.4964, diffLoss = 7.3813, kgLoss = 0.1691
2025-04-09 09:20:51.936087: Training Step 113/115: batchLoss = 4.9960, diffLoss = 8.2012, kgLoss = 0.1881
2025-04-09 09:20:52.584043: Training Step 114/115: batchLoss = 4.3674, diffLoss = 7.1643, kgLoss = 0.1720
2025-04-09 09:20:52.717014: 
2025-04-09 09:20:52.717635: Epoch 2/1000, Train: epLoss = 1.3650, epDfLoss = 2.2409, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:20:53.473804: Steps 0/90: batch_recall = 42.43, batch_ndcg = 30.93 
2025-04-09 09:20:54.205299: Steps 1/90: batch_recall = 46.80, batch_ndcg = 31.29 
2025-04-09 09:20:54.955171: Steps 2/90: batch_recall = 32.95, batch_ndcg = 22.36 
2025-04-09 09:20:55.702491: Steps 3/90: batch_recall = 30.44, batch_ndcg = 21.05 
2025-04-09 09:20:56.443726: Steps 4/90: batch_recall = 38.98, batch_ndcg = 26.88 
2025-04-09 09:20:57.194654: Steps 5/90: batch_recall = 24.39, batch_ndcg = 17.64 
2025-04-09 09:20:57.941727: Steps 6/90: batch_recall = 34.52, batch_ndcg = 23.95 
2025-04-09 09:20:58.707429: Steps 7/90: batch_recall = 28.62, batch_ndcg = 20.22 
2025-04-09 09:20:59.458546: Steps 8/90: batch_recall = 33.75, batch_ndcg = 24.44 
2025-04-09 09:21:00.221758: Steps 9/90: batch_recall = 28.59, batch_ndcg = 21.66 
2025-04-09 09:21:00.965854: Steps 10/90: batch_recall = 25.31, batch_ndcg = 18.87 
2025-04-09 09:21:01.712116: Steps 11/90: batch_recall = 31.95, batch_ndcg = 22.56 
2025-04-09 09:21:02.444953: Steps 12/90: batch_recall = 26.49, batch_ndcg = 16.71 
2025-04-09 09:21:03.187633: Steps 13/90: batch_recall = 24.05, batch_ndcg = 17.29 
2025-04-09 09:21:03.928364: Steps 14/90: batch_recall = 22.26, batch_ndcg = 16.11 
2025-04-09 09:21:04.670379: Steps 15/90: batch_recall = 29.26, batch_ndcg = 19.62 
2025-04-09 09:21:05.427502: Steps 16/90: batch_recall = 21.90, batch_ndcg = 16.09 
2025-04-09 09:21:06.161625: Steps 17/90: batch_recall = 23.23, batch_ndcg = 16.45 
2025-04-09 09:21:06.891408: Steps 18/90: batch_recall = 21.90, batch_ndcg = 14.72 
2025-04-09 09:21:07.645384: Steps 19/90: batch_recall = 19.01, batch_ndcg = 11.98 
2025-04-09 09:21:08.382635: Steps 20/90: batch_recall = 24.80, batch_ndcg = 15.90 
2025-04-09 09:21:09.123061: Steps 21/90: batch_recall = 26.11, batch_ndcg = 18.35 
2025-04-09 09:21:09.864066: Steps 22/90: batch_recall = 25.74, batch_ndcg = 17.82 
2025-04-09 09:21:10.599574: Steps 23/90: batch_recall = 21.22, batch_ndcg = 14.59 
2025-04-09 09:21:11.320859: Steps 24/90: batch_recall = 25.95, batch_ndcg = 18.17 
2025-04-09 09:21:12.040611: Steps 25/90: batch_recall = 21.62, batch_ndcg = 14.53 
2025-04-09 09:21:12.760621: Steps 26/90: batch_recall = 19.08, batch_ndcg = 14.37 
2025-04-09 09:21:13.477225: Steps 27/90: batch_recall = 19.91, batch_ndcg = 12.86 
2025-04-09 09:21:14.195854: Steps 28/90: batch_recall = 19.55, batch_ndcg = 12.57 
2025-04-09 09:21:14.904762: Steps 29/90: batch_recall = 17.82, batch_ndcg = 12.33 
2025-04-09 09:21:15.614027: Steps 30/90: batch_recall = 20.45, batch_ndcg = 12.49 
2025-04-09 09:21:16.346961: Steps 31/90: batch_recall = 15.93, batch_ndcg = 11.09 
2025-04-09 09:21:17.053729: Steps 32/90: batch_recall = 21.76, batch_ndcg = 12.94 
2025-04-09 09:21:17.771353: Steps 33/90: batch_recall = 17.23, batch_ndcg = 11.18 
2025-04-09 09:21:18.488356: Steps 34/90: batch_recall = 22.22, batch_ndcg = 13.54 
2025-04-09 09:21:19.204703: Steps 35/90: batch_recall = 21.02, batch_ndcg = 14.30 
2025-04-09 09:21:19.951173: Steps 36/90: batch_recall = 17.08, batch_ndcg = 13.31 
2025-04-09 09:21:20.674977: Steps 37/90: batch_recall = 15.15, batch_ndcg = 10.63 
2025-04-09 09:21:21.384936: Steps 38/90: batch_recall = 16.10, batch_ndcg = 11.12 
2025-04-09 09:21:22.099198: Steps 39/90: batch_recall = 16.07, batch_ndcg = 9.12 
2025-04-09 09:21:22.809846: Steps 40/90: batch_recall = 16.87, batch_ndcg = 11.64 
2025-04-09 09:21:23.519325: Steps 41/90: batch_recall = 18.08, batch_ndcg = 11.71 
2025-04-09 09:21:24.242821: Steps 42/90: batch_recall = 20.61, batch_ndcg = 14.84 
2025-04-09 09:21:24.940031: Steps 43/90: batch_recall = 12.40, batch_ndcg = 9.47 
2025-04-09 09:21:25.648062: Steps 44/90: batch_recall = 15.68, batch_ndcg = 10.76 
2025-04-09 09:21:26.355607: Steps 45/90: batch_recall = 15.25, batch_ndcg = 7.91 
2025-04-09 09:21:27.062344: Steps 46/90: batch_recall = 13.51, batch_ndcg = 8.81 
2025-04-09 09:21:27.774951: Steps 47/90: batch_recall = 16.53, batch_ndcg = 10.83 
2025-04-09 09:21:28.468896: Steps 48/90: batch_recall = 17.36, batch_ndcg = 10.54 
2025-04-09 09:21:29.165589: Steps 49/90: batch_recall = 13.32, batch_ndcg = 8.93 
2025-04-09 09:21:29.866888: Steps 50/90: batch_recall = 20.15, batch_ndcg = 13.24 
2025-04-09 09:21:30.584337: Steps 51/90: batch_recall = 17.12, batch_ndcg = 11.04 
2025-04-09 09:21:31.300009: Steps 52/90: batch_recall = 16.17, batch_ndcg = 10.25 
2025-04-09 09:21:32.007622: Steps 53/90: batch_recall = 13.57, batch_ndcg = 9.53 
2025-04-09 09:21:32.723338: Steps 54/90: batch_recall = 13.93, batch_ndcg = 8.87 
2025-04-09 09:21:33.424216: Steps 55/90: batch_recall = 13.70, batch_ndcg = 8.16 
2025-04-09 09:21:34.141102: Steps 56/90: batch_recall = 13.65, batch_ndcg = 9.50 
2025-04-09 09:21:34.853345: Steps 57/90: batch_recall = 15.99, batch_ndcg = 11.26 
2025-04-09 09:21:35.559754: Steps 58/90: batch_recall = 22.19, batch_ndcg = 13.70 
2025-04-09 09:21:36.278148: Steps 59/90: batch_recall = 15.28, batch_ndcg = 9.01 
2025-04-09 09:21:36.987068: Steps 60/90: batch_recall = 15.40, batch_ndcg = 9.74 
2025-04-09 09:21:37.706493: Steps 61/90: batch_recall = 17.20, batch_ndcg = 11.27 
2025-04-09 09:21:38.422728: Steps 62/90: batch_recall = 17.64, batch_ndcg = 11.12 
2025-04-09 09:21:39.147728: Steps 63/90: batch_recall = 13.35, batch_ndcg = 8.85 
2025-04-09 09:21:39.847158: Steps 64/90: batch_recall = 14.09, batch_ndcg = 8.64 
2025-04-09 09:21:40.586612: Steps 65/90: batch_recall = 18.41, batch_ndcg = 11.67 
2025-04-09 09:21:41.311081: Steps 66/90: batch_recall = 16.49, batch_ndcg = 11.28 
2025-04-09 09:21:42.042466: Steps 67/90: batch_recall = 11.69, batch_ndcg = 7.65 
2025-04-09 09:21:42.756696: Steps 68/90: batch_recall = 15.02, batch_ndcg = 9.17 
2025-04-09 09:21:43.491503: Steps 69/90: batch_recall = 15.86, batch_ndcg = 9.56 
2025-04-09 09:21:44.238480: Steps 70/90: batch_recall = 16.09, batch_ndcg = 11.29 
2025-04-09 09:21:44.958325: Steps 71/90: batch_recall = 15.63, batch_ndcg = 10.68 
2025-04-09 09:21:45.657786: Steps 72/90: batch_recall = 9.77, batch_ndcg = 7.09 
2025-04-09 09:21:46.386925: Steps 73/90: batch_recall = 16.49, batch_ndcg = 10.33 
2025-04-09 09:21:47.112697: Steps 74/90: batch_recall = 17.77, batch_ndcg = 10.60 
2025-04-09 09:21:47.827051: Steps 75/90: batch_recall = 17.76, batch_ndcg = 10.25 
2025-04-09 09:21:48.530800: Steps 76/90: batch_recall = 16.07, batch_ndcg = 10.76 
2025-04-09 09:21:49.245642: Steps 77/90: batch_recall = 18.02, batch_ndcg = 11.45 
2025-04-09 09:21:49.958841: Steps 78/90: batch_recall = 13.17, batch_ndcg = 8.59 
2025-04-09 09:21:50.684736: Steps 79/90: batch_recall = 18.68, batch_ndcg = 13.39 
2025-04-09 09:21:51.402723: Steps 80/90: batch_recall = 18.60, batch_ndcg = 11.58 
2025-04-09 09:21:52.151045: Steps 81/90: batch_recall = 18.80, batch_ndcg = 10.58 
2025-04-09 09:21:52.915010: Steps 82/90: batch_recall = 16.16, batch_ndcg = 10.00 
2025-04-09 09:21:53.623149: Steps 83/90: batch_recall = 14.45, batch_ndcg = 8.86 
2025-04-09 09:21:54.337709: Steps 84/90: batch_recall = 13.73, batch_ndcg = 8.56 
2025-04-09 09:21:55.054256: Steps 85/90: batch_recall = 16.36, batch_ndcg = 10.99 
2025-04-09 09:21:55.768938: Steps 86/90: batch_recall = 20.19, batch_ndcg = 12.24 
2025-04-09 09:21:56.476281: Steps 87/90: batch_recall = 18.15, batch_ndcg = 11.79 
2025-04-09 09:21:57.192667: Steps 88/90: batch_recall = 20.51, batch_ndcg = 11.86 
2025-04-09 09:21:57.764819: Steps 89/90: batch_recall = 14.34, batch_ndcg = 7.38 
2025-04-09 09:21:57.765496: Epoch 2/1000, Test: Recall = 0.0392, NDCG = 0.0261  

2025-04-09 09:21:58.779357: Training Step 0/115: batchLoss = 5.4712, diffLoss = 8.9835, kgLoss = 0.2026
2025-04-09 09:21:59.498834: Training Step 1/115: batchLoss = 4.6472, diffLoss = 7.6288, kgLoss = 0.1749
2025-04-09 09:22:00.227008: Training Step 2/115: batchLoss = 4.3768, diffLoss = 7.1815, kgLoss = 0.1697
2025-04-09 09:22:00.951236: Training Step 3/115: batchLoss = 4.6028, diffLoss = 7.5518, kgLoss = 0.1794
2025-04-09 09:22:01.681669: Training Step 4/115: batchLoss = 5.1819, diffLoss = 8.5094, kgLoss = 0.1907
2025-04-09 09:22:02.402657: Training Step 5/115: batchLoss = 5.0669, diffLoss = 8.3207, kgLoss = 0.1862
2025-04-09 09:22:03.135046: Training Step 6/115: batchLoss = 4.5514, diffLoss = 7.4607, kgLoss = 0.1875
2025-04-09 09:22:03.864060: Training Step 7/115: batchLoss = 4.2873, diffLoss = 7.0354, kgLoss = 0.1652
2025-04-09 09:22:04.596628: Training Step 8/115: batchLoss = 4.7585, diffLoss = 7.8077, kgLoss = 0.1847
2025-04-09 09:22:05.329613: Training Step 9/115: batchLoss = 4.7413, diffLoss = 7.7830, kgLoss = 0.1788
2025-04-09 09:22:06.077594: Training Step 10/115: batchLoss = 4.3706, diffLoss = 7.1756, kgLoss = 0.1631
2025-04-09 09:22:06.822820: Training Step 11/115: batchLoss = 4.8002, diffLoss = 7.8865, kgLoss = 0.1707
2025-04-09 09:22:07.554739: Training Step 12/115: batchLoss = 5.2811, diffLoss = 8.6620, kgLoss = 0.2097
2025-04-09 09:22:08.289802: Training Step 13/115: batchLoss = 4.1614, diffLoss = 6.8294, kgLoss = 0.1594
2025-04-09 09:22:09.019746: Training Step 14/115: batchLoss = 4.3467, diffLoss = 7.1359, kgLoss = 0.1629
2025-04-09 09:22:09.755660: Training Step 15/115: batchLoss = 4.2852, diffLoss = 7.0330, kgLoss = 0.1636
2025-04-09 09:22:10.485664: Training Step 16/115: batchLoss = 4.4223, diffLoss = 7.2649, kgLoss = 0.1585
2025-04-09 09:22:11.223247: Training Step 17/115: batchLoss = 4.9562, diffLoss = 8.1372, kgLoss = 0.1848
2025-04-09 09:22:11.963860: Training Step 18/115: batchLoss = 4.9921, diffLoss = 8.1977, kgLoss = 0.1837
2025-04-09 09:22:12.710272: Training Step 19/115: batchLoss = 4.7098, diffLoss = 7.7316, kgLoss = 0.1772
2025-04-09 09:22:13.453061: Training Step 20/115: batchLoss = 4.8462, diffLoss = 7.9623, kgLoss = 0.1719
2025-04-09 09:22:14.176718: Training Step 21/115: batchLoss = 4.7336, diffLoss = 7.7734, kgLoss = 0.1740
2025-04-09 09:22:14.904570: Training Step 22/115: batchLoss = 4.9793, diffLoss = 8.1734, kgLoss = 0.1882
2025-04-09 09:22:15.634188: Training Step 23/115: batchLoss = 4.4099, diffLoss = 7.2395, kgLoss = 0.1655
2025-04-09 09:22:16.360583: Training Step 24/115: batchLoss = 4.8493, diffLoss = 7.9596, kgLoss = 0.1838
2025-04-09 09:22:17.069918: Training Step 25/115: batchLoss = 4.7914, diffLoss = 7.8628, kgLoss = 0.1843
2025-04-09 09:22:17.793392: Training Step 26/115: batchLoss = 5.4133, diffLoss = 8.8907, kgLoss = 0.1973
2025-04-09 09:22:18.521438: Training Step 27/115: batchLoss = 4.6206, diffLoss = 7.5868, kgLoss = 0.1712
2025-04-09 09:22:19.264170: Training Step 28/115: batchLoss = 4.9039, diffLoss = 8.0528, kgLoss = 0.1805
2025-04-09 09:22:19.987984: Training Step 29/115: batchLoss = 4.9482, diffLoss = 8.1188, kgLoss = 0.1922
2025-04-09 09:22:20.714288: Training Step 30/115: batchLoss = 4.6256, diffLoss = 7.5863, kgLoss = 0.1844
2025-04-09 09:22:21.446244: Training Step 31/115: batchLoss = 5.8814, diffLoss = 9.6614, kgLoss = 0.2113
2025-04-09 09:22:22.167445: Training Step 32/115: batchLoss = 4.5204, diffLoss = 7.4206, kgLoss = 0.1702
2025-04-09 09:22:22.887268: Training Step 33/115: batchLoss = 4.3944, diffLoss = 7.2105, kgLoss = 0.1701
2025-04-09 09:22:23.617817: Training Step 34/115: batchLoss = 5.0454, diffLoss = 8.2888, kgLoss = 0.1803
2025-04-09 09:22:24.358562: Training Step 35/115: batchLoss = 4.2231, diffLoss = 6.9360, kgLoss = 0.1539
2025-04-09 09:22:25.090388: Training Step 36/115: batchLoss = 5.3835, diffLoss = 8.8439, kgLoss = 0.1930
2025-04-09 09:22:25.822040: Training Step 37/115: batchLoss = 5.1693, diffLoss = 8.4879, kgLoss = 0.1914
2025-04-09 09:22:26.550889: Training Step 38/115: batchLoss = 5.0063, diffLoss = 8.2192, kgLoss = 0.1870
2025-04-09 09:22:27.277725: Training Step 39/115: batchLoss = 5.0027, diffLoss = 8.2116, kgLoss = 0.1895
2025-04-09 09:22:28.004471: Training Step 40/115: batchLoss = 4.4908, diffLoss = 7.3623, kgLoss = 0.1834
2025-04-09 09:22:28.732523: Training Step 41/115: batchLoss = 5.6221, diffLoss = 9.2346, kgLoss = 0.2034
2025-04-09 09:22:29.481815: Training Step 42/115: batchLoss = 4.6535, diffLoss = 7.6434, kgLoss = 0.1687
2025-04-09 09:22:30.218956: Training Step 43/115: batchLoss = 4.8546, diffLoss = 7.9671, kgLoss = 0.1858
2025-04-09 09:22:30.946446: Training Step 44/115: batchLoss = 4.8704, diffLoss = 7.9918, kgLoss = 0.1883
2025-04-09 09:22:31.672408: Training Step 45/115: batchLoss = 4.7288, diffLoss = 7.7660, kgLoss = 0.1730
2025-04-09 09:22:32.396905: Training Step 46/115: batchLoss = 4.4009, diffLoss = 7.2228, kgLoss = 0.1681
2025-04-09 09:22:33.129563: Training Step 47/115: batchLoss = 4.5598, diffLoss = 7.4876, kgLoss = 0.1681
2025-04-09 09:22:33.856821: Training Step 48/115: batchLoss = 5.9745, diffLoss = 9.8125, kgLoss = 0.2175
2025-04-09 09:22:34.590474: Training Step 49/115: batchLoss = 4.5135, diffLoss = 7.4099, kgLoss = 0.1688
2025-04-09 09:22:35.339103: Training Step 50/115: batchLoss = 4.5693, diffLoss = 7.5053, kgLoss = 0.1652
2025-04-09 09:22:36.068293: Training Step 51/115: batchLoss = 4.8465, diffLoss = 7.9584, kgLoss = 0.1785
2025-04-09 09:22:36.801946: Training Step 52/115: batchLoss = 5.0337, diffLoss = 8.2571, kgLoss = 0.1985
2025-04-09 09:22:37.564007: Training Step 53/115: batchLoss = 4.5089, diffLoss = 7.4024, kgLoss = 0.1685
2025-04-09 09:22:38.292454: Training Step 54/115: batchLoss = 4.7692, diffLoss = 7.8334, kgLoss = 0.1729
2025-04-09 09:22:39.019135: Training Step 55/115: batchLoss = 5.1076, diffLoss = 8.3896, kgLoss = 0.1846
2025-04-09 09:22:39.757071: Training Step 56/115: batchLoss = 4.7792, diffLoss = 7.8459, kgLoss = 0.1792
2025-04-09 09:22:40.482457: Training Step 57/115: batchLoss = 4.7065, diffLoss = 7.7274, kgLoss = 0.1753
2025-04-09 09:22:41.213019: Training Step 58/115: batchLoss = 4.2307, diffLoss = 6.9369, kgLoss = 0.1715
2025-04-09 09:22:41.946132: Training Step 59/115: batchLoss = 5.0819, diffLoss = 8.3478, kgLoss = 0.1831
2025-04-09 09:22:42.673249: Training Step 60/115: batchLoss = 4.4248, diffLoss = 7.2643, kgLoss = 0.1654
2025-04-09 09:22:43.391358: Training Step 61/115: batchLoss = 4.5118, diffLoss = 7.4093, kgLoss = 0.1654
2025-04-09 09:22:44.102706: Training Step 62/115: batchLoss = 4.4161, diffLoss = 7.2545, kgLoss = 0.1584
2025-04-09 09:22:44.821582: Training Step 63/115: batchLoss = 5.2301, diffLoss = 8.5867, kgLoss = 0.1952
2025-04-09 09:22:45.545973: Training Step 64/115: batchLoss = 4.7374, diffLoss = 7.7708, kgLoss = 0.1873
2025-04-09 09:22:46.270680: Training Step 65/115: batchLoss = 5.3355, diffLoss = 8.7610, kgLoss = 0.1974
2025-04-09 09:22:46.989954: Training Step 66/115: batchLoss = 4.5143, diffLoss = 7.4025, kgLoss = 0.1821
2025-04-09 09:22:47.709029: Training Step 67/115: batchLoss = 4.1242, diffLoss = 6.7698, kgLoss = 0.1557
2025-04-09 09:22:48.427059: Training Step 68/115: batchLoss = 4.9589, diffLoss = 8.1376, kgLoss = 0.1909
2025-04-09 09:22:49.144036: Training Step 69/115: batchLoss = 5.3074, diffLoss = 8.7209, kgLoss = 0.1870
2025-04-09 09:22:49.886807: Training Step 70/115: batchLoss = 4.0175, diffLoss = 6.5923, kgLoss = 0.1554
2025-04-09 09:22:50.612544: Training Step 71/115: batchLoss = 5.3336, diffLoss = 8.7616, kgLoss = 0.1916
2025-04-09 09:22:51.328710: Training Step 72/115: batchLoss = 5.0176, diffLoss = 8.2349, kgLoss = 0.1917
2025-04-09 09:22:52.046728: Training Step 73/115: batchLoss = 4.6787, diffLoss = 7.6830, kgLoss = 0.1722
2025-04-09 09:22:52.766428: Training Step 74/115: batchLoss = 5.0811, diffLoss = 8.3318, kgLoss = 0.2050
2025-04-09 09:22:53.490406: Training Step 75/115: batchLoss = 4.3436, diffLoss = 7.1261, kgLoss = 0.1698
2025-04-09 09:22:54.209135: Training Step 76/115: batchLoss = 4.3106, diffLoss = 7.0761, kgLoss = 0.1623
2025-04-09 09:22:54.932801: Training Step 77/115: batchLoss = 4.1699, diffLoss = 6.8408, kgLoss = 0.1637
2025-04-09 09:22:55.654881: Training Step 78/115: batchLoss = 4.4254, diffLoss = 7.2704, kgLoss = 0.1579
2025-04-09 09:22:56.374099: Training Step 79/115: batchLoss = 4.4646, diffLoss = 7.3298, kgLoss = 0.1668
2025-04-09 09:22:57.100801: Training Step 80/115: batchLoss = 4.8932, diffLoss = 8.0347, kgLoss = 0.1808
2025-04-09 09:22:57.818641: Training Step 81/115: batchLoss = 4.5754, diffLoss = 7.5108, kgLoss = 0.1724
2025-04-09 09:22:58.537082: Training Step 82/115: batchLoss = 4.8367, diffLoss = 7.9444, kgLoss = 0.1753
2025-04-09 09:22:59.262665: Training Step 83/115: batchLoss = 4.5928, diffLoss = 7.5395, kgLoss = 0.1727
2025-04-09 09:22:59.976775: Training Step 84/115: batchLoss = 5.0615, diffLoss = 8.3107, kgLoss = 0.1876
2025-04-09 09:23:00.694146: Training Step 85/115: batchLoss = 4.3914, diffLoss = 7.2044, kgLoss = 0.1720
2025-04-09 09:23:01.410011: Training Step 86/115: batchLoss = 4.8644, diffLoss = 7.9923, kgLoss = 0.1724
2025-04-09 09:23:02.123842: Training Step 87/115: batchLoss = 4.7781, diffLoss = 7.8489, kgLoss = 0.1719
2025-04-09 09:23:02.839341: Training Step 88/115: batchLoss = 4.4109, diffLoss = 7.2393, kgLoss = 0.1682
2025-04-09 09:23:03.570569: Training Step 89/115: batchLoss = 4.1003, diffLoss = 6.7296, kgLoss = 0.1563
2025-04-09 09:23:04.308879: Training Step 90/115: batchLoss = 4.5413, diffLoss = 7.4535, kgLoss = 0.1732
2025-04-09 09:23:05.036472: Training Step 91/115: batchLoss = 4.7955, diffLoss = 7.8701, kgLoss = 0.1836
2025-04-09 09:23:05.759120: Training Step 92/115: batchLoss = 4.0130, diffLoss = 6.5840, kgLoss = 0.1565
2025-04-09 09:23:06.487437: Training Step 93/115: batchLoss = 4.7283, diffLoss = 7.7696, kgLoss = 0.1662
2025-04-09 09:23:07.210834: Training Step 94/115: batchLoss = 4.7761, diffLoss = 7.8354, kgLoss = 0.1872
2025-04-09 09:23:07.940501: Training Step 95/115: batchLoss = 4.9827, diffLoss = 8.1826, kgLoss = 0.1830
2025-04-09 09:23:08.672088: Training Step 96/115: batchLoss = 4.1812, diffLoss = 6.8594, kgLoss = 0.1638
2025-04-09 09:23:09.406967: Training Step 97/115: batchLoss = 4.8235, diffLoss = 7.9162, kgLoss = 0.1844
2025-04-09 09:23:10.136111: Training Step 98/115: batchLoss = 5.2569, diffLoss = 8.6284, kgLoss = 0.1998
2025-04-09 09:23:10.865566: Training Step 99/115: batchLoss = 4.3916, diffLoss = 7.2074, kgLoss = 0.1680
2025-04-09 09:23:11.593738: Training Step 100/115: batchLoss = 5.1017, diffLoss = 8.3809, kgLoss = 0.1831
2025-04-09 09:23:12.310935: Training Step 101/115: batchLoss = 4.3156, diffLoss = 7.0808, kgLoss = 0.1679
2025-04-09 09:23:13.030477: Training Step 102/115: batchLoss = 4.5113, diffLoss = 7.4087, kgLoss = 0.1653
2025-04-09 09:23:13.745445: Training Step 103/115: batchLoss = 4.3505, diffLoss = 7.1413, kgLoss = 0.1643
2025-04-09 09:23:14.472345: Training Step 104/115: batchLoss = 4.6285, diffLoss = 7.6021, kgLoss = 0.1681
2025-04-09 09:23:15.204680: Training Step 105/115: batchLoss = 5.0563, diffLoss = 8.3065, kgLoss = 0.1809
2025-04-09 09:23:15.942155: Training Step 106/115: batchLoss = 4.4642, diffLoss = 7.3341, kgLoss = 0.1593
2025-04-09 09:23:16.658295: Training Step 107/115: batchLoss = 4.9941, diffLoss = 8.1947, kgLoss = 0.1930
2025-04-09 09:23:17.378034: Training Step 108/115: batchLoss = 4.1992, diffLoss = 6.8889, kgLoss = 0.1648
2025-04-09 09:23:18.113372: Training Step 109/115: batchLoss = 4.3305, diffLoss = 7.1042, kgLoss = 0.1700
2025-04-09 09:23:18.837021: Training Step 110/115: batchLoss = 5.0597, diffLoss = 8.3099, kgLoss = 0.1845
2025-04-09 09:23:19.565826: Training Step 111/115: batchLoss = 5.1322, diffLoss = 8.4258, kgLoss = 0.1917
2025-04-09 09:23:20.288126: Training Step 112/115: batchLoss = 5.0133, diffLoss = 8.2356, kgLoss = 0.1800
2025-04-09 09:23:20.951825: Training Step 113/115: batchLoss = 4.8709, diffLoss = 8.0041, kgLoss = 0.1710
2025-04-09 09:23:21.572265: Training Step 114/115: batchLoss = 5.0134, diffLoss = 8.2182, kgLoss = 0.2061
2025-04-09 09:23:21.685221: 
2025-04-09 09:23:21.686227: Epoch 3/1000, Train: epLoss = 1.3638, epDfLoss = 2.2388, epfTransLoss = 0.0000, epKgLoss = 0.0512  
2025-04-09 09:23:22.474457: Steps 0/90: batch_recall = 45.84, batch_ndcg = 33.24 
2025-04-09 09:23:23.218383: Steps 1/90: batch_recall = 50.29, batch_ndcg = 33.78 
2025-04-09 09:23:23.971625: Steps 2/90: batch_recall = 33.43, batch_ndcg = 23.17 
2025-04-09 09:23:24.724252: Steps 3/90: batch_recall = 31.09, batch_ndcg = 21.91 
2025-04-09 09:23:25.471431: Steps 4/90: batch_recall = 44.23, batch_ndcg = 29.70 
2025-04-09 09:23:26.215769: Steps 5/90: batch_recall = 26.47, batch_ndcg = 19.44 
2025-04-09 09:23:26.952274: Steps 6/90: batch_recall = 34.13, batch_ndcg = 23.95 
2025-04-09 09:23:27.703268: Steps 7/90: batch_recall = 29.42, batch_ndcg = 20.75 
2025-04-09 09:23:28.452338: Steps 8/90: batch_recall = 36.05, batch_ndcg = 24.48 
2025-04-09 09:23:29.204500: Steps 9/90: batch_recall = 30.89, batch_ndcg = 23.65 
2025-04-09 09:23:29.941085: Steps 10/90: batch_recall = 28.55, batch_ndcg = 20.78 
2025-04-09 09:23:30.682368: Steps 11/90: batch_recall = 33.87, batch_ndcg = 22.68 
2025-04-09 09:23:31.411049: Steps 12/90: batch_recall = 27.22, batch_ndcg = 17.75 
2025-04-09 09:23:32.151057: Steps 13/90: batch_recall = 26.01, batch_ndcg = 18.47 
2025-04-09 09:23:32.903798: Steps 14/90: batch_recall = 25.55, batch_ndcg = 17.29 
2025-04-09 09:23:33.644203: Steps 15/90: batch_recall = 30.54, batch_ndcg = 20.36 
2025-04-09 09:23:34.386702: Steps 16/90: batch_recall = 24.75, batch_ndcg = 17.86 
2025-04-09 09:23:35.124837: Steps 17/90: batch_recall = 24.70, batch_ndcg = 16.87 
2025-04-09 09:23:35.850896: Steps 18/90: batch_recall = 24.85, batch_ndcg = 16.94 
2025-04-09 09:23:36.594753: Steps 19/90: batch_recall = 21.47, batch_ndcg = 13.61 
2025-04-09 09:23:37.320652: Steps 20/90: batch_recall = 24.62, batch_ndcg = 15.92 
2025-04-09 09:23:38.064986: Steps 21/90: batch_recall = 27.39, batch_ndcg = 19.32 
2025-04-09 09:23:38.796789: Steps 22/90: batch_recall = 29.58, batch_ndcg = 19.34 
2025-04-09 09:23:39.552470: Steps 23/90: batch_recall = 21.86, batch_ndcg = 15.65 
2025-04-09 09:23:40.278268: Steps 24/90: batch_recall = 27.65, batch_ndcg = 18.56 
2025-04-09 09:23:40.995422: Steps 25/90: batch_recall = 25.77, batch_ndcg = 17.23 
2025-04-09 09:23:41.725953: Steps 26/90: batch_recall = 22.43, batch_ndcg = 15.99 
2025-04-09 09:23:42.458494: Steps 27/90: batch_recall = 22.85, batch_ndcg = 14.62 
2025-04-09 09:23:43.171352: Steps 28/90: batch_recall = 19.55, batch_ndcg = 13.33 
2025-04-09 09:23:43.883203: Steps 29/90: batch_recall = 19.21, batch_ndcg = 12.62 
2025-04-09 09:23:44.593709: Steps 30/90: batch_recall = 21.70, batch_ndcg = 12.31 
2025-04-09 09:23:45.323615: Steps 31/90: batch_recall = 19.84, batch_ndcg = 12.73 
2025-04-09 09:23:46.021089: Steps 32/90: batch_recall = 23.56, batch_ndcg = 14.71 
2025-04-09 09:23:46.725934: Steps 33/90: batch_recall = 20.51, batch_ndcg = 13.01 
2025-04-09 09:23:47.451441: Steps 34/90: batch_recall = 24.33, batch_ndcg = 14.74 
2025-04-09 09:23:48.171440: Steps 35/90: batch_recall = 21.87, batch_ndcg = 14.93 
2025-04-09 09:23:48.886524: Steps 36/90: batch_recall = 21.24, batch_ndcg = 15.02 
2025-04-09 09:23:49.607752: Steps 37/90: batch_recall = 17.35, batch_ndcg = 11.99 
2025-04-09 09:23:50.322005: Steps 38/90: batch_recall = 17.70, batch_ndcg = 11.96 
2025-04-09 09:23:51.031218: Steps 39/90: batch_recall = 18.26, batch_ndcg = 10.61 
2025-04-09 09:23:51.735042: Steps 40/90: batch_recall = 18.23, batch_ndcg = 12.34 
2025-04-09 09:23:52.449841: Steps 41/90: batch_recall = 19.62, batch_ndcg = 13.10 
2025-04-09 09:23:53.165342: Steps 42/90: batch_recall = 24.32, batch_ndcg = 17.47 
2025-04-09 09:23:53.894518: Steps 43/90: batch_recall = 14.00, batch_ndcg = 10.36 
2025-04-09 09:23:54.606328: Steps 44/90: batch_recall = 17.49, batch_ndcg = 11.83 
2025-04-09 09:23:55.311244: Steps 45/90: batch_recall = 18.22, batch_ndcg = 9.95 
2025-04-09 09:23:56.020321: Steps 46/90: batch_recall = 15.63, batch_ndcg = 9.68 
2025-04-09 09:23:56.731528: Steps 47/90: batch_recall = 18.30, batch_ndcg = 11.43 
2025-04-09 09:23:57.433098: Steps 48/90: batch_recall = 19.01, batch_ndcg = 11.46 
2025-04-09 09:23:58.151376: Steps 49/90: batch_recall = 14.17, batch_ndcg = 9.45 
2025-04-09 09:23:58.846487: Steps 50/90: batch_recall = 21.19, batch_ndcg = 14.18 
2025-04-09 09:23:59.562725: Steps 51/90: batch_recall = 18.77, batch_ndcg = 12.42 
2025-04-09 09:24:00.295323: Steps 52/90: batch_recall = 17.41, batch_ndcg = 11.31 
2025-04-09 09:24:00.999139: Steps 53/90: batch_recall = 14.88, batch_ndcg = 10.52 
2025-04-09 09:24:01.703604: Steps 54/90: batch_recall = 14.45, batch_ndcg = 9.73 
2025-04-09 09:24:02.404709: Steps 55/90: batch_recall = 15.04, batch_ndcg = 9.07 
2025-04-09 09:24:03.115626: Steps 56/90: batch_recall = 15.02, batch_ndcg = 10.21 
2025-04-09 09:24:03.836057: Steps 57/90: batch_recall = 18.18, batch_ndcg = 12.55 
2025-04-09 09:24:04.533867: Steps 58/90: batch_recall = 23.79, batch_ndcg = 15.30 
2025-04-09 09:24:05.259016: Steps 59/90: batch_recall = 16.79, batch_ndcg = 9.61 
2025-04-09 09:24:05.958483: Steps 60/90: batch_recall = 17.04, batch_ndcg = 11.09 
2025-04-09 09:24:06.663707: Steps 61/90: batch_recall = 18.90, batch_ndcg = 12.61 
2025-04-09 09:24:07.366642: Steps 62/90: batch_recall = 20.28, batch_ndcg = 12.25 
2025-04-09 09:24:08.065750: Steps 63/90: batch_recall = 16.90, batch_ndcg = 10.61 
2025-04-09 09:24:08.773836: Steps 64/90: batch_recall = 18.21, batch_ndcg = 10.25 
2025-04-09 09:24:09.482452: Steps 65/90: batch_recall = 20.19, batch_ndcg = 12.66 
2025-04-09 09:24:10.191249: Steps 66/90: batch_recall = 18.75, batch_ndcg = 12.09 
2025-04-09 09:24:10.908864: Steps 67/90: batch_recall = 12.83, batch_ndcg = 8.11 
2025-04-09 09:24:11.621956: Steps 68/90: batch_recall = 17.56, batch_ndcg = 10.94 
2025-04-09 09:24:12.332741: Steps 69/90: batch_recall = 16.79, batch_ndcg = 10.14 
2025-04-09 09:24:13.039820: Steps 70/90: batch_recall = 18.18, batch_ndcg = 12.19 
2025-04-09 09:24:13.756377: Steps 71/90: batch_recall = 20.97, batch_ndcg = 12.88 
2025-04-09 09:24:14.474662: Steps 72/90: batch_recall = 12.86, batch_ndcg = 8.92 
2025-04-09 09:24:15.222932: Steps 73/90: batch_recall = 19.85, batch_ndcg = 12.26 
2025-04-09 09:24:15.959676: Steps 74/90: batch_recall = 19.60, batch_ndcg = 11.99 
2025-04-09 09:24:16.670673: Steps 75/90: batch_recall = 19.79, batch_ndcg = 12.30 
2025-04-09 09:24:17.371846: Steps 76/90: batch_recall = 19.69, batch_ndcg = 11.25 
2025-04-09 09:24:18.072664: Steps 77/90: batch_recall = 20.29, batch_ndcg = 12.60 
2025-04-09 09:24:18.780447: Steps 78/90: batch_recall = 15.67, batch_ndcg = 10.11 
2025-04-09 09:24:19.498617: Steps 79/90: batch_recall = 20.76, batch_ndcg = 14.76 
2025-04-09 09:24:20.227891: Steps 80/90: batch_recall = 21.06, batch_ndcg = 13.62 
2025-04-09 09:24:20.951626: Steps 81/90: batch_recall = 19.74, batch_ndcg = 11.66 
2025-04-09 09:24:21.651902: Steps 82/90: batch_recall = 19.22, batch_ndcg = 11.50 
2025-04-09 09:24:22.379579: Steps 83/90: batch_recall = 15.88, batch_ndcg = 9.96 
2025-04-09 09:24:23.128719: Steps 84/90: batch_recall = 16.58, batch_ndcg = 10.14 
2025-04-09 09:24:23.858621: Steps 85/90: batch_recall = 20.06, batch_ndcg = 12.77 
2025-04-09 09:24:24.583867: Steps 86/90: batch_recall = 21.86, batch_ndcg = 13.11 
2025-04-09 09:24:25.301865: Steps 87/90: batch_recall = 23.75, batch_ndcg = 14.26 
2025-04-09 09:24:26.017108: Steps 88/90: batch_recall = 24.01, batch_ndcg = 13.94 
2025-04-09 09:24:26.581497: Steps 89/90: batch_recall = 16.86, batch_ndcg = 9.15 
2025-04-09 09:24:26.582285: Epoch 3/1000, Test: Recall = 0.0435, NDCG = 0.0286  

2025-04-09 09:24:27.608085: Training Step 0/115: batchLoss = 4.5659, diffLoss = 7.4958, kgLoss = 0.1710
2025-04-09 09:24:28.329493: Training Step 1/115: batchLoss = 4.0738, diffLoss = 6.6771, kgLoss = 0.1689
2025-04-09 09:24:29.048982: Training Step 2/115: batchLoss = 4.2584, diffLoss = 6.9903, kgLoss = 0.1606
2025-04-09 09:24:29.774083: Training Step 3/115: batchLoss = 4.4845, diffLoss = 7.3621, kgLoss = 0.1681
2025-04-09 09:24:30.495843: Training Step 4/115: batchLoss = 4.5299, diffLoss = 7.4328, kgLoss = 0.1756
2025-04-09 09:24:31.218372: Training Step 5/115: batchLoss = 4.6898, diffLoss = 7.6989, kgLoss = 0.1761
2025-04-09 09:24:31.957160: Training Step 6/115: batchLoss = 4.9437, diffLoss = 8.1078, kgLoss = 0.1974
2025-04-09 09:24:32.678957: Training Step 7/115: batchLoss = 4.7191, diffLoss = 7.7495, kgLoss = 0.1734
2025-04-09 09:24:33.397151: Training Step 8/115: batchLoss = 4.7123, diffLoss = 7.7315, kgLoss = 0.1836
2025-04-09 09:24:34.123000: Training Step 9/115: batchLoss = 4.3199, diffLoss = 7.0684, kgLoss = 0.1973
2025-04-09 09:24:34.852695: Training Step 10/115: batchLoss = 4.5622, diffLoss = 7.4840, kgLoss = 0.1794
2025-04-09 09:24:35.581105: Training Step 11/115: batchLoss = 5.4142, diffLoss = 8.8870, kgLoss = 0.2049
2025-04-09 09:24:36.305992: Training Step 12/115: batchLoss = 4.1719, diffLoss = 6.8421, kgLoss = 0.1667
2025-04-09 09:24:37.028385: Training Step 13/115: batchLoss = 4.8620, diffLoss = 7.9804, kgLoss = 0.1844
2025-04-09 09:24:37.744558: Training Step 14/115: batchLoss = 4.9635, diffLoss = 8.1520, kgLoss = 0.1809
2025-04-09 09:24:38.475078: Training Step 15/115: batchLoss = 4.7581, diffLoss = 7.8072, kgLoss = 0.1843
2025-04-09 09:24:39.209531: Training Step 16/115: batchLoss = 4.6683, diffLoss = 7.6572, kgLoss = 0.1849
2025-04-09 09:24:39.950458: Training Step 17/115: batchLoss = 4.7064, diffLoss = 7.7243, kgLoss = 0.1796
2025-04-09 09:24:40.679666: Training Step 18/115: batchLoss = 5.0818, diffLoss = 8.3498, kgLoss = 0.1798
2025-04-09 09:24:41.418103: Training Step 19/115: batchLoss = 4.7919, diffLoss = 7.8704, kgLoss = 0.1741
2025-04-09 09:24:42.155408: Training Step 20/115: batchLoss = 4.5213, diffLoss = 7.4256, kgLoss = 0.1650
2025-04-09 09:24:42.898892: Training Step 21/115: batchLoss = 4.5995, diffLoss = 7.5455, kgLoss = 0.1806
2025-04-09 09:24:43.635660: Training Step 22/115: batchLoss = 4.8796, diffLoss = 8.0117, kgLoss = 0.1814
2025-04-09 09:24:44.370607: Training Step 23/115: batchLoss = 5.0398, diffLoss = 8.2807, kgLoss = 0.1784
2025-04-09 09:24:45.093570: Training Step 24/115: batchLoss = 4.2392, diffLoss = 6.9591, kgLoss = 0.1593
2025-04-09 09:24:45.825685: Training Step 25/115: batchLoss = 4.5730, diffLoss = 7.5041, kgLoss = 0.1764
2025-04-09 09:24:46.575194: Training Step 26/115: batchLoss = 4.7869, diffLoss = 7.8551, kgLoss = 0.1847
2025-04-09 09:24:47.302280: Training Step 27/115: batchLoss = 5.3185, diffLoss = 8.7275, kgLoss = 0.2049
2025-04-09 09:24:48.026028: Training Step 28/115: batchLoss = 5.4717, diffLoss = 8.9885, kgLoss = 0.1966
2025-04-09 09:24:48.758802: Training Step 29/115: batchLoss = 4.8152, diffLoss = 7.9093, kgLoss = 0.1741
2025-04-09 09:24:49.501333: Training Step 30/115: batchLoss = 4.3587, diffLoss = 7.1464, kgLoss = 0.1772
2025-04-09 09:24:50.234173: Training Step 31/115: batchLoss = 4.4544, diffLoss = 7.3133, kgLoss = 0.1661
2025-04-09 09:24:50.970176: Training Step 32/115: batchLoss = 4.1372, diffLoss = 6.7914, kgLoss = 0.1559
2025-04-09 09:24:51.705974: Training Step 33/115: batchLoss = 4.7229, diffLoss = 7.7559, kgLoss = 0.1734
2025-04-09 09:24:52.457191: Training Step 34/115: batchLoss = 5.1902, diffLoss = 8.5238, kgLoss = 0.1897
2025-04-09 09:24:53.223196: Training Step 35/115: batchLoss = 4.0594, diffLoss = 6.6550, kgLoss = 0.1660
2025-04-09 09:24:53.956012: Training Step 36/115: batchLoss = 5.2190, diffLoss = 8.5696, kgLoss = 0.1932
2025-04-09 09:24:54.691735: Training Step 37/115: batchLoss = 4.8244, diffLoss = 7.9183, kgLoss = 0.1834
2025-04-09 09:24:55.430182: Training Step 38/115: batchLoss = 5.2731, diffLoss = 8.6620, kgLoss = 0.1897
2025-04-09 09:24:56.153754: Training Step 39/115: batchLoss = 4.9775, diffLoss = 8.1732, kgLoss = 0.1841
2025-04-09 09:24:56.884257: Training Step 40/115: batchLoss = 4.6534, diffLoss = 7.6340, kgLoss = 0.1826
2025-04-09 09:24:57.619674: Training Step 41/115: batchLoss = 4.1036, diffLoss = 6.7345, kgLoss = 0.1573
2025-04-09 09:24:58.343046: Training Step 42/115: batchLoss = 4.6585, diffLoss = 7.6519, kgLoss = 0.1683
2025-04-09 09:24:59.064721: Training Step 43/115: batchLoss = 4.5483, diffLoss = 7.4683, kgLoss = 0.1682
2025-04-09 09:24:59.792694: Training Step 44/115: batchLoss = 4.5278, diffLoss = 7.4348, kgLoss = 0.1672
2025-04-09 09:25:00.517483: Training Step 45/115: batchLoss = 4.5844, diffLoss = 7.5281, kgLoss = 0.1687
2025-04-09 09:25:01.243489: Training Step 46/115: batchLoss = 4.5720, diffLoss = 7.4968, kgLoss = 0.1848
2025-04-09 09:25:01.974585: Training Step 47/115: batchLoss = 4.3173, diffLoss = 7.0839, kgLoss = 0.1675
2025-04-09 09:25:02.717791: Training Step 48/115: batchLoss = 4.7197, diffLoss = 7.7435, kgLoss = 0.1842
2025-04-09 09:25:03.452189: Training Step 49/115: batchLoss = 4.9454, diffLoss = 8.1206, kgLoss = 0.1826
2025-04-09 09:25:04.203438: Training Step 50/115: batchLoss = 5.2577, diffLoss = 8.6369, kgLoss = 0.1890
2025-04-09 09:25:04.942523: Training Step 51/115: batchLoss = 4.2596, diffLoss = 6.9856, kgLoss = 0.1706
2025-04-09 09:25:05.678796: Training Step 52/115: batchLoss = 4.5784, diffLoss = 7.5150, kgLoss = 0.1734
2025-04-09 09:25:06.418002: Training Step 53/115: batchLoss = 4.7216, diffLoss = 7.7470, kgLoss = 0.1836
2025-04-09 09:25:07.155316: Training Step 54/115: batchLoss = 4.7557, diffLoss = 7.8071, kgLoss = 0.1786
2025-04-09 09:25:07.881502: Training Step 55/115: batchLoss = 4.6274, diffLoss = 7.5862, kgLoss = 0.1893
2025-04-09 09:25:08.607521: Training Step 56/115: batchLoss = 4.7658, diffLoss = 7.8196, kgLoss = 0.1852
2025-04-09 09:25:09.344150: Training Step 57/115: batchLoss = 4.5007, diffLoss = 7.3826, kgLoss = 0.1779
2025-04-09 09:25:10.085067: Training Step 58/115: batchLoss = 4.3418, diffLoss = 7.1172, kgLoss = 0.1788
2025-04-09 09:25:10.812978: Training Step 59/115: batchLoss = 4.1507, diffLoss = 6.8117, kgLoss = 0.1592
2025-04-09 09:25:11.539878: Training Step 60/115: batchLoss = 4.4734, diffLoss = 7.3365, kgLoss = 0.1787
2025-04-09 09:25:12.295164: Training Step 61/115: batchLoss = 4.9638, diffLoss = 8.1449, kgLoss = 0.1923
2025-04-09 09:25:13.013965: Training Step 62/115: batchLoss = 5.0267, diffLoss = 8.2562, kgLoss = 0.1824
2025-04-09 09:25:13.735280: Training Step 63/115: batchLoss = 4.9509, diffLoss = 8.1207, kgLoss = 0.1961
2025-04-09 09:25:14.462142: Training Step 64/115: batchLoss = 5.2674, diffLoss = 8.6418, kgLoss = 0.2058
2025-04-09 09:25:15.190027: Training Step 65/115: batchLoss = 5.6394, diffLoss = 9.2648, kgLoss = 0.2012
2025-04-09 09:25:15.921675: Training Step 66/115: batchLoss = 5.0075, diffLoss = 8.2161, kgLoss = 0.1948
2025-04-09 09:25:16.650048: Training Step 67/115: batchLoss = 4.6325, diffLoss = 7.6120, kgLoss = 0.1633
2025-04-09 09:25:17.383358: Training Step 68/115: batchLoss = 4.8872, diffLoss = 8.0250, kgLoss = 0.1804
2025-04-09 09:25:18.122583: Training Step 69/115: batchLoss = 5.1115, diffLoss = 8.3953, kgLoss = 0.1859
2025-04-09 09:25:18.849187: Training Step 70/115: batchLoss = 4.0458, diffLoss = 6.6402, kgLoss = 0.1542
2025-04-09 09:25:19.589190: Training Step 71/115: batchLoss = 4.6808, diffLoss = 7.6859, kgLoss = 0.1733
2025-04-09 09:25:20.324009: Training Step 72/115: batchLoss = 4.7448, diffLoss = 7.7970, kgLoss = 0.1665
2025-04-09 09:25:21.055404: Training Step 73/115: batchLoss = 4.6162, diffLoss = 7.5760, kgLoss = 0.1766
2025-04-09 09:25:21.790992: Training Step 74/115: batchLoss = 4.5587, diffLoss = 7.4832, kgLoss = 0.1718
2025-04-09 09:25:22.526605: Training Step 75/115: batchLoss = 4.6523, diffLoss = 7.6322, kgLoss = 0.1825
2025-04-09 09:25:23.256875: Training Step 76/115: batchLoss = 4.8784, diffLoss = 8.0099, kgLoss = 0.1812
2025-04-09 09:25:23.989835: Training Step 77/115: batchLoss = 4.5004, diffLoss = 7.3897, kgLoss = 0.1665
2025-04-09 09:25:24.720127: Training Step 78/115: batchLoss = 4.7781, diffLoss = 7.8419, kgLoss = 0.1824
2025-04-09 09:25:25.454380: Training Step 79/115: batchLoss = 4.5069, diffLoss = 7.4009, kgLoss = 0.1659
2025-04-09 09:25:26.184113: Training Step 80/115: batchLoss = 4.3429, diffLoss = 7.1284, kgLoss = 0.1647
2025-04-09 09:25:26.924277: Training Step 81/115: batchLoss = 4.3224, diffLoss = 7.0892, kgLoss = 0.1720
2025-04-09 09:25:27.664981: Training Step 82/115: batchLoss = 4.3628, diffLoss = 7.1575, kgLoss = 0.1708
2025-04-09 09:25:28.395769: Training Step 83/115: batchLoss = 3.6131, diffLoss = 5.9235, kgLoss = 0.1474
2025-04-09 09:25:29.130738: Training Step 84/115: batchLoss = 4.6773, diffLoss = 7.6677, kgLoss = 0.1917
2025-04-09 09:25:29.875285: Training Step 85/115: batchLoss = 4.1637, diffLoss = 6.8228, kgLoss = 0.1752
2025-04-09 09:25:30.611533: Training Step 86/115: batchLoss = 4.9762, diffLoss = 8.1733, kgLoss = 0.1806
2025-04-09 09:25:31.346553: Training Step 87/115: batchLoss = 5.4186, diffLoss = 8.9000, kgLoss = 0.1965
2025-04-09 09:25:32.089835: Training Step 88/115: batchLoss = 4.6722, diffLoss = 7.6642, kgLoss = 0.1841
2025-04-09 09:25:32.832800: Training Step 89/115: batchLoss = 4.3300, diffLoss = 7.0995, kgLoss = 0.1757
2025-04-09 09:25:33.564885: Training Step 90/115: batchLoss = 4.4875, diffLoss = 7.3606, kgLoss = 0.1779
2025-04-09 09:25:34.307055: Training Step 91/115: batchLoss = 4.5619, diffLoss = 7.4816, kgLoss = 0.1823
2025-04-09 09:25:35.040294: Training Step 92/115: batchLoss = 4.4538, diffLoss = 7.2989, kgLoss = 0.1862
2025-04-09 09:25:35.790034: Training Step 93/115: batchLoss = 5.0879, diffLoss = 8.3580, kgLoss = 0.1828
2025-04-09 09:25:36.522646: Training Step 94/115: batchLoss = 4.9948, diffLoss = 8.2029, kgLoss = 0.1827
2025-04-09 09:25:37.243561: Training Step 95/115: batchLoss = 4.4166, diffLoss = 7.2402, kgLoss = 0.1811
2025-04-09 09:25:37.963100: Training Step 96/115: batchLoss = 4.4341, diffLoss = 7.2711, kgLoss = 0.1786
2025-04-09 09:25:38.702066: Training Step 97/115: batchLoss = 4.7189, diffLoss = 7.7482, kgLoss = 0.1749
2025-04-09 09:25:39.427525: Training Step 98/115: batchLoss = 5.2463, diffLoss = 8.6143, kgLoss = 0.1942
2025-04-09 09:25:40.151086: Training Step 99/115: batchLoss = 4.3527, diffLoss = 7.1430, kgLoss = 0.1673
2025-04-09 09:25:40.879181: Training Step 100/115: batchLoss = 5.2490, diffLoss = 8.6141, kgLoss = 0.2012
2025-04-09 09:25:41.605767: Training Step 101/115: batchLoss = 4.3915, diffLoss = 7.2022, kgLoss = 0.1755
2025-04-09 09:25:42.329190: Training Step 102/115: batchLoss = 4.7319, diffLoss = 7.7677, kgLoss = 0.1783
2025-04-09 09:25:43.062890: Training Step 103/115: batchLoss = 4.4916, diffLoss = 7.3763, kgLoss = 0.1644
2025-04-09 09:25:43.789744: Training Step 104/115: batchLoss = 4.6050, diffLoss = 7.5553, kgLoss = 0.1794
2025-04-09 09:25:44.514429: Training Step 105/115: batchLoss = 4.3776, diffLoss = 7.1920, kgLoss = 0.1560
2025-04-09 09:25:45.237854: Training Step 106/115: batchLoss = 4.9411, diffLoss = 8.1190, kgLoss = 0.1741
2025-04-09 09:25:45.967664: Training Step 107/115: batchLoss = 4.2349, diffLoss = 6.9562, kgLoss = 0.1528
2025-04-09 09:25:46.698718: Training Step 108/115: batchLoss = 4.6943, diffLoss = 7.7079, kgLoss = 0.1739
2025-04-09 09:25:47.438868: Training Step 109/115: batchLoss = 4.8439, diffLoss = 7.9527, kgLoss = 0.1808
2025-04-09 09:25:48.181239: Training Step 110/115: batchLoss = 4.2591, diffLoss = 6.9885, kgLoss = 0.1650
2025-04-09 09:25:48.905131: Training Step 111/115: batchLoss = 3.9573, diffLoss = 6.4888, kgLoss = 0.1602
2025-04-09 09:25:49.626965: Training Step 112/115: batchLoss = 4.6600, diffLoss = 7.6496, kgLoss = 0.1757
2025-04-09 09:25:50.273398: Training Step 113/115: batchLoss = 4.4592, diffLoss = 7.3136, kgLoss = 0.1776
2025-04-09 09:25:50.907730: Training Step 114/115: batchLoss = 4.6318, diffLoss = 7.5952, kgLoss = 0.1867
2025-04-09 09:25:51.021347: 
2025-04-09 09:25:51.022294: Epoch 4/1000, Train: epLoss = 1.3410, epDfLoss = 2.2010, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:25:51.772512: Steps 0/90: batch_recall = 46.29, batch_ndcg = 33.87 
2025-04-09 09:25:52.512863: Steps 1/90: batch_recall = 51.33, batch_ndcg = 35.18 
2025-04-09 09:25:53.259582: Steps 2/90: batch_recall = 35.92, batch_ndcg = 24.42 
2025-04-09 09:25:53.994389: Steps 3/90: batch_recall = 33.22, batch_ndcg = 23.12 
2025-04-09 09:25:54.728605: Steps 4/90: batch_recall = 44.79, batch_ndcg = 31.31 
2025-04-09 09:25:55.457840: Steps 5/90: batch_recall = 28.12, batch_ndcg = 21.01 
2025-04-09 09:25:56.195904: Steps 6/90: batch_recall = 35.06, batch_ndcg = 25.19 
2025-04-09 09:25:56.955384: Steps 7/90: batch_recall = 32.68, batch_ndcg = 22.54 
2025-04-09 09:25:57.693758: Steps 8/90: batch_recall = 39.26, batch_ndcg = 26.45 
2025-04-09 09:25:58.437787: Steps 9/90: batch_recall = 33.26, batch_ndcg = 24.55 
2025-04-09 09:25:59.174786: Steps 10/90: batch_recall = 30.78, batch_ndcg = 21.91 
2025-04-09 09:25:59.900675: Steps 11/90: batch_recall = 34.83, batch_ndcg = 23.19 
2025-04-09 09:26:00.624268: Steps 12/90: batch_recall = 27.34, batch_ndcg = 18.40 
2025-04-09 09:26:01.346084: Steps 13/90: batch_recall = 27.05, batch_ndcg = 18.89 
2025-04-09 09:26:02.082941: Steps 14/90: batch_recall = 26.88, batch_ndcg = 17.78 
2025-04-09 09:26:02.800813: Steps 15/90: batch_recall = 30.88, batch_ndcg = 20.86 
2025-04-09 09:26:03.530503: Steps 16/90: batch_recall = 25.95, batch_ndcg = 18.72 
2025-04-09 09:26:04.275046: Steps 17/90: batch_recall = 25.48, batch_ndcg = 17.72 
2025-04-09 09:26:04.994502: Steps 18/90: batch_recall = 27.08, batch_ndcg = 17.79 
2025-04-09 09:26:05.736605: Steps 19/90: batch_recall = 23.95, batch_ndcg = 15.66 
2025-04-09 09:26:06.470205: Steps 20/90: batch_recall = 25.44, batch_ndcg = 16.29 
2025-04-09 09:26:07.196958: Steps 21/90: batch_recall = 28.96, batch_ndcg = 20.04 
2025-04-09 09:26:07.909778: Steps 22/90: batch_recall = 31.92, batch_ndcg = 21.46 
2025-04-09 09:26:08.628582: Steps 23/90: batch_recall = 24.76, batch_ndcg = 17.12 
2025-04-09 09:26:09.350267: Steps 24/90: batch_recall = 27.60, batch_ndcg = 18.89 
2025-04-09 09:26:10.062346: Steps 25/90: batch_recall = 26.49, batch_ndcg = 17.98 
2025-04-09 09:26:10.768253: Steps 26/90: batch_recall = 23.79, batch_ndcg = 16.49 
2025-04-09 09:26:11.489336: Steps 27/90: batch_recall = 24.40, batch_ndcg = 15.68 
2025-04-09 09:26:12.206426: Steps 28/90: batch_recall = 22.60, batch_ndcg = 15.33 
2025-04-09 09:26:12.916735: Steps 29/90: batch_recall = 23.37, batch_ndcg = 14.19 
2025-04-09 09:26:13.617415: Steps 30/90: batch_recall = 22.76, batch_ndcg = 13.32 
2025-04-09 09:26:14.342768: Steps 31/90: batch_recall = 22.00, batch_ndcg = 14.09 
2025-04-09 09:26:15.060295: Steps 32/90: batch_recall = 24.98, batch_ndcg = 15.77 
2025-04-09 09:26:15.796974: Steps 33/90: batch_recall = 21.90, batch_ndcg = 13.72 
2025-04-09 09:26:16.528777: Steps 34/90: batch_recall = 25.16, batch_ndcg = 16.18 
2025-04-09 09:26:17.258136: Steps 35/90: batch_recall = 23.31, batch_ndcg = 16.67 
2025-04-09 09:26:17.984507: Steps 36/90: batch_recall = 22.42, batch_ndcg = 15.97 
2025-04-09 09:26:18.704884: Steps 37/90: batch_recall = 20.66, batch_ndcg = 13.39 
2025-04-09 09:26:19.431019: Steps 38/90: batch_recall = 19.75, batch_ndcg = 12.80 
2025-04-09 09:26:20.149735: Steps 39/90: batch_recall = 20.75, batch_ndcg = 11.48 
2025-04-09 09:26:20.867292: Steps 40/90: batch_recall = 19.98, batch_ndcg = 13.53 
2025-04-09 09:26:21.576730: Steps 41/90: batch_recall = 21.67, batch_ndcg = 14.40 
2025-04-09 09:26:22.296033: Steps 42/90: batch_recall = 25.39, batch_ndcg = 18.53 
2025-04-09 09:26:23.016091: Steps 43/90: batch_recall = 16.68, batch_ndcg = 11.36 
2025-04-09 09:26:23.730226: Steps 44/90: batch_recall = 20.23, batch_ndcg = 13.34 
2025-04-09 09:26:24.438060: Steps 45/90: batch_recall = 19.94, batch_ndcg = 11.12 
2025-04-09 09:26:25.165703: Steps 46/90: batch_recall = 17.74, batch_ndcg = 10.74 
2025-04-09 09:26:25.866778: Steps 47/90: batch_recall = 18.02, batch_ndcg = 11.74 
2025-04-09 09:26:26.569782: Steps 48/90: batch_recall = 20.95, batch_ndcg = 12.44 
2025-04-09 09:26:27.261015: Steps 49/90: batch_recall = 14.38, batch_ndcg = 9.87 
2025-04-09 09:26:27.965479: Steps 50/90: batch_recall = 24.84, batch_ndcg = 15.71 
2025-04-09 09:26:28.682884: Steps 51/90: batch_recall = 21.94, batch_ndcg = 13.94 
2025-04-09 09:26:29.388047: Steps 52/90: batch_recall = 20.87, batch_ndcg = 13.44 
2025-04-09 09:26:30.092999: Steps 53/90: batch_recall = 18.49, batch_ndcg = 12.34 
2025-04-09 09:26:30.796970: Steps 54/90: batch_recall = 16.04, batch_ndcg = 10.92 
2025-04-09 09:26:31.502005: Steps 55/90: batch_recall = 18.29, batch_ndcg = 11.28 
2025-04-09 09:26:32.209065: Steps 56/90: batch_recall = 16.34, batch_ndcg = 10.72 
2025-04-09 09:26:32.939270: Steps 57/90: batch_recall = 20.91, batch_ndcg = 13.83 
2025-04-09 09:26:33.654136: Steps 58/90: batch_recall = 26.87, batch_ndcg = 16.64 
2025-04-09 09:26:34.355449: Steps 59/90: batch_recall = 16.74, batch_ndcg = 10.13 
2025-04-09 09:26:35.060935: Steps 60/90: batch_recall = 19.04, batch_ndcg = 12.58 
2025-04-09 09:26:35.766669: Steps 61/90: batch_recall = 23.70, batch_ndcg = 14.97 
2025-04-09 09:26:36.467482: Steps 62/90: batch_recall = 22.03, batch_ndcg = 13.90 
2025-04-09 09:26:37.184003: Steps 63/90: batch_recall = 18.10, batch_ndcg = 11.62 
2025-04-09 09:26:37.896882: Steps 64/90: batch_recall = 20.55, batch_ndcg = 11.67 
2025-04-09 09:26:38.619795: Steps 65/90: batch_recall = 21.39, batch_ndcg = 13.71 
2025-04-09 09:26:39.336207: Steps 66/90: batch_recall = 20.86, batch_ndcg = 13.20 
2025-04-09 09:26:40.028395: Steps 67/90: batch_recall = 15.79, batch_ndcg = 9.51 
2025-04-09 09:26:40.730178: Steps 68/90: batch_recall = 18.14, batch_ndcg = 11.88 
2025-04-09 09:26:41.432979: Steps 69/90: batch_recall = 21.80, batch_ndcg = 12.16 
2025-04-09 09:26:42.144878: Steps 70/90: batch_recall = 19.27, batch_ndcg = 13.03 
2025-04-09 09:26:42.858056: Steps 71/90: batch_recall = 23.58, batch_ndcg = 14.39 
2025-04-09 09:26:43.581126: Steps 72/90: batch_recall = 15.20, batch_ndcg = 10.10 
2025-04-09 09:26:44.320297: Steps 73/90: batch_recall = 21.52, batch_ndcg = 13.51 
2025-04-09 09:26:45.036885: Steps 74/90: batch_recall = 21.59, batch_ndcg = 13.35 
2025-04-09 09:26:45.749197: Steps 75/90: batch_recall = 21.54, batch_ndcg = 13.62 
2025-04-09 09:26:46.491100: Steps 76/90: batch_recall = 21.47, batch_ndcg = 12.09 
2025-04-09 09:26:47.221225: Steps 77/90: batch_recall = 20.58, batch_ndcg = 12.76 
2025-04-09 09:26:47.937178: Steps 78/90: batch_recall = 16.83, batch_ndcg = 11.26 
2025-04-09 09:26:48.647219: Steps 79/90: batch_recall = 21.64, batch_ndcg = 15.72 
2025-04-09 09:26:49.342011: Steps 80/90: batch_recall = 26.19, batch_ndcg = 16.07 
2025-04-09 09:26:50.051784: Steps 81/90: batch_recall = 24.07, batch_ndcg = 14.46 
2025-04-09 09:26:50.766628: Steps 82/90: batch_recall = 21.81, batch_ndcg = 13.38 
2025-04-09 09:26:51.489135: Steps 83/90: batch_recall = 16.61, batch_ndcg = 11.13 
2025-04-09 09:26:52.240265: Steps 84/90: batch_recall = 16.94, batch_ndcg = 10.05 
2025-04-09 09:26:52.977430: Steps 85/90: batch_recall = 20.59, batch_ndcg = 13.17 
2025-04-09 09:26:53.675312: Steps 86/90: batch_recall = 23.58, batch_ndcg = 14.05 
2025-04-09 09:26:54.381533: Steps 87/90: batch_recall = 25.93, batch_ndcg = 15.78 
2025-04-09 09:26:55.090371: Steps 88/90: batch_recall = 26.91, batch_ndcg = 15.75 
2025-04-09 09:26:55.633987: Steps 89/90: batch_recall = 18.57, batch_ndcg = 10.12 
2025-04-09 09:26:55.634725: Epoch 4/1000, Test: Recall = 0.0472, NDCG = 0.0309  

2025-04-09 09:26:56.637651: Training Step 0/115: batchLoss = 4.7460, diffLoss = 7.7928, kgLoss = 0.1758
2025-04-09 09:26:57.371170: Training Step 1/115: batchLoss = 5.0493, diffLoss = 8.2918, kgLoss = 0.1857
2025-04-09 09:26:58.102106: Training Step 2/115: batchLoss = 4.4706, diffLoss = 7.3337, kgLoss = 0.1761
2025-04-09 09:26:58.844689: Training Step 3/115: batchLoss = 4.5494, diffLoss = 7.4703, kgLoss = 0.1680
2025-04-09 09:26:59.585871: Training Step 4/115: batchLoss = 5.1889, diffLoss = 8.5187, kgLoss = 0.1941
2025-04-09 09:27:00.317871: Training Step 5/115: batchLoss = 5.1787, diffLoss = 8.4988, kgLoss = 0.1986
2025-04-09 09:27:01.045560: Training Step 6/115: batchLoss = 4.7962, diffLoss = 7.8715, kgLoss = 0.1833
2025-04-09 09:27:01.776511: Training Step 7/115: batchLoss = 4.2875, diffLoss = 7.0259, kgLoss = 0.1799
2025-04-09 09:27:02.515705: Training Step 8/115: batchLoss = 4.1350, diffLoss = 6.7825, kgLoss = 0.1638
2025-04-09 09:27:03.247049: Training Step 9/115: batchLoss = 5.1809, diffLoss = 8.5054, kgLoss = 0.1942
2025-04-09 09:27:03.994718: Training Step 10/115: batchLoss = 4.9541, diffLoss = 8.1375, kgLoss = 0.1790
2025-04-09 09:27:04.740320: Training Step 11/115: batchLoss = 4.6568, diffLoss = 7.6409, kgLoss = 0.1805
2025-04-09 09:27:05.490117: Training Step 12/115: batchLoss = 4.9138, diffLoss = 8.0688, kgLoss = 0.1813
2025-04-09 09:27:06.230552: Training Step 13/115: batchLoss = 5.2223, diffLoss = 8.5741, kgLoss = 0.1946
2025-04-09 09:27:06.962314: Training Step 14/115: batchLoss = 4.1112, diffLoss = 6.7398, kgLoss = 0.1682
2025-04-09 09:27:07.686622: Training Step 15/115: batchLoss = 4.7201, diffLoss = 7.7493, kgLoss = 0.1764
2025-04-09 09:27:08.417614: Training Step 16/115: batchLoss = 4.1435, diffLoss = 6.7944, kgLoss = 0.1672
2025-04-09 09:27:09.149189: Training Step 17/115: batchLoss = 4.6685, diffLoss = 7.6623, kgLoss = 0.1778
2025-04-09 09:27:09.867509: Training Step 18/115: batchLoss = 4.7011, diffLoss = 7.7142, kgLoss = 0.1816
2025-04-09 09:27:10.588385: Training Step 19/115: batchLoss = 4.8835, diffLoss = 8.0120, kgLoss = 0.1909
2025-04-09 09:27:11.311160: Training Step 20/115: batchLoss = 4.6779, diffLoss = 7.6828, kgLoss = 0.1706
2025-04-09 09:27:12.050236: Training Step 21/115: batchLoss = 4.7730, diffLoss = 7.8436, kgLoss = 0.1671
2025-04-09 09:27:12.789422: Training Step 22/115: batchLoss = 4.8771, diffLoss = 8.0077, kgLoss = 0.1813
2025-04-09 09:27:13.506431: Training Step 23/115: batchLoss = 4.3086, diffLoss = 7.0648, kgLoss = 0.1743
2025-04-09 09:27:14.235628: Training Step 24/115: batchLoss = 4.4290, diffLoss = 7.2711, kgLoss = 0.1658
2025-04-09 09:27:14.968173: Training Step 25/115: batchLoss = 4.7942, diffLoss = 7.8747, kgLoss = 0.1734
2025-04-09 09:27:15.692844: Training Step 26/115: batchLoss = 4.4202, diffLoss = 7.2514, kgLoss = 0.1735
2025-04-09 09:27:16.416674: Training Step 27/115: batchLoss = 5.9529, diffLoss = 9.7725, kgLoss = 0.2235
2025-04-09 09:27:17.147181: Training Step 28/115: batchLoss = 4.7924, diffLoss = 7.8636, kgLoss = 0.1856
2025-04-09 09:27:17.877040: Training Step 29/115: batchLoss = 4.6185, diffLoss = 7.5763, kgLoss = 0.1818
2025-04-09 09:27:18.601273: Training Step 30/115: batchLoss = 5.0794, diffLoss = 8.3338, kgLoss = 0.1978
2025-04-09 09:27:19.322184: Training Step 31/115: batchLoss = 5.2742, diffLoss = 8.6658, kgLoss = 0.1869
2025-04-09 09:27:20.060037: Training Step 32/115: batchLoss = 4.8972, diffLoss = 8.0427, kgLoss = 0.1789
2025-04-09 09:27:20.799309: Training Step 33/115: batchLoss = 4.2357, diffLoss = 6.9447, kgLoss = 0.1722
2025-04-09 09:27:21.543332: Training Step 34/115: batchLoss = 4.5891, diffLoss = 7.5255, kgLoss = 0.1844
2025-04-09 09:27:22.277824: Training Step 35/115: batchLoss = 4.7952, diffLoss = 7.8727, kgLoss = 0.1791
2025-04-09 09:27:23.010148: Training Step 36/115: batchLoss = 5.2081, diffLoss = 8.5559, kgLoss = 0.1864
2025-04-09 09:27:23.748215: Training Step 37/115: batchLoss = 4.5531, diffLoss = 7.4746, kgLoss = 0.1708
2025-04-09 09:27:24.471522: Training Step 38/115: batchLoss = 4.7513, diffLoss = 7.7926, kgLoss = 0.1895
2025-04-09 09:27:25.193316: Training Step 39/115: batchLoss = 4.8334, diffLoss = 7.9393, kgLoss = 0.1746
2025-04-09 09:27:25.926740: Training Step 40/115: batchLoss = 5.2280, diffLoss = 8.5698, kgLoss = 0.2153
2025-04-09 09:27:26.653917: Training Step 41/115: batchLoss = 5.3792, diffLoss = 8.8351, kgLoss = 0.1954
2025-04-09 09:27:27.385613: Training Step 42/115: batchLoss = 4.9510, diffLoss = 8.1251, kgLoss = 0.1898
2025-04-09 09:27:28.121235: Training Step 43/115: batchLoss = 4.6893, diffLoss = 7.6977, kgLoss = 0.1768
2025-04-09 09:27:28.852377: Training Step 44/115: batchLoss = 4.9353, diffLoss = 8.1009, kgLoss = 0.1870
2025-04-09 09:27:29.589023: Training Step 45/115: batchLoss = 4.3944, diffLoss = 7.2095, kgLoss = 0.1717
2025-04-09 09:27:30.327913: Training Step 46/115: batchLoss = 4.7469, diffLoss = 7.7959, kgLoss = 0.1735
2025-04-09 09:27:31.073564: Training Step 47/115: batchLoss = 4.8630, diffLoss = 7.9775, kgLoss = 0.1912
2025-04-09 09:27:31.809692: Training Step 48/115: batchLoss = 4.4696, diffLoss = 7.3364, kgLoss = 0.1694
2025-04-09 09:27:32.551106: Training Step 49/115: batchLoss = 5.1390, diffLoss = 8.4379, kgLoss = 0.1906
2025-04-09 09:27:33.282936: Training Step 50/115: batchLoss = 4.4445, diffLoss = 7.2945, kgLoss = 0.1695
2025-04-09 09:27:34.001368: Training Step 51/115: batchLoss = 4.6138, diffLoss = 7.5765, kgLoss = 0.1699
2025-04-09 09:27:34.713716: Training Step 52/115: batchLoss = 4.2784, diffLoss = 7.0261, kgLoss = 0.1569
2025-04-09 09:27:35.448319: Training Step 53/115: batchLoss = 4.4236, diffLoss = 7.2571, kgLoss = 0.1734
2025-04-09 09:27:36.169015: Training Step 54/115: batchLoss = 4.4663, diffLoss = 7.3255, kgLoss = 0.1775
2025-04-09 09:27:36.888647: Training Step 55/115: batchLoss = 4.1083, diffLoss = 6.7393, kgLoss = 0.1619
2025-04-09 09:27:37.607118: Training Step 56/115: batchLoss = 4.5890, diffLoss = 7.5339, kgLoss = 0.1715
2025-04-09 09:27:38.334838: Training Step 57/115: batchLoss = 4.1674, diffLoss = 6.8398, kgLoss = 0.1588
2025-04-09 09:27:39.054646: Training Step 58/115: batchLoss = 5.5325, diffLoss = 9.0843, kgLoss = 0.2048
2025-04-09 09:27:39.781257: Training Step 59/115: batchLoss = 4.2413, diffLoss = 6.9553, kgLoss = 0.1703
2025-04-09 09:27:40.498371: Training Step 60/115: batchLoss = 4.0596, diffLoss = 6.6561, kgLoss = 0.1649
2025-04-09 09:27:41.222020: Training Step 61/115: batchLoss = 4.4221, diffLoss = 7.2628, kgLoss = 0.1612
2025-04-09 09:27:41.960616: Training Step 62/115: batchLoss = 4.8090, diffLoss = 7.8989, kgLoss = 0.1743
2025-04-09 09:27:42.708822: Training Step 63/115: batchLoss = 4.5398, diffLoss = 7.4446, kgLoss = 0.1825
2025-04-09 09:27:43.440527: Training Step 64/115: batchLoss = 4.0032, diffLoss = 6.5675, kgLoss = 0.1568
2025-04-09 09:27:44.169357: Training Step 65/115: batchLoss = 4.9601, diffLoss = 8.1310, kgLoss = 0.2038
2025-04-09 09:27:44.890449: Training Step 66/115: batchLoss = 4.1591, diffLoss = 6.8269, kgLoss = 0.1575
2025-04-09 09:27:45.625281: Training Step 67/115: batchLoss = 4.1930, diffLoss = 6.8499, kgLoss = 0.2076
2025-04-09 09:27:46.353619: Training Step 68/115: batchLoss = 4.3561, diffLoss = 7.1534, kgLoss = 0.1602
2025-04-09 09:27:47.081166: Training Step 69/115: batchLoss = 3.9017, diffLoss = 6.4045, kgLoss = 0.1476
2025-04-09 09:27:47.812622: Training Step 70/115: batchLoss = 4.6249, diffLoss = 7.5890, kgLoss = 0.1787
2025-04-09 09:27:48.532458: Training Step 71/115: batchLoss = 5.0402, diffLoss = 8.2750, kgLoss = 0.1880
2025-04-09 09:27:49.258545: Training Step 72/115: batchLoss = 4.6348, diffLoss = 7.6047, kgLoss = 0.1799
2025-04-09 09:27:49.980012: Training Step 73/115: batchLoss = 4.5069, diffLoss = 7.4010, kgLoss = 0.1658
2025-04-09 09:27:50.702862: Training Step 74/115: batchLoss = 5.0617, diffLoss = 8.3071, kgLoss = 0.1934
2025-04-09 09:27:51.419588: Training Step 75/115: batchLoss = 4.4133, diffLoss = 7.2443, kgLoss = 0.1669
2025-04-09 09:27:52.138501: Training Step 76/115: batchLoss = 5.2325, diffLoss = 8.5928, kgLoss = 0.1921
2025-04-09 09:27:52.857487: Training Step 77/115: batchLoss = 4.4151, diffLoss = 7.2454, kgLoss = 0.1696
2025-04-09 09:27:53.572505: Training Step 78/115: batchLoss = 4.1812, diffLoss = 6.8582, kgLoss = 0.1658
2025-04-09 09:27:54.286882: Training Step 79/115: batchLoss = 5.1687, diffLoss = 8.4719, kgLoss = 0.2138
2025-04-09 09:27:55.000808: Training Step 80/115: batchLoss = 4.2984, diffLoss = 7.0513, kgLoss = 0.1690
2025-04-09 09:27:55.711282: Training Step 81/115: batchLoss = 4.2455, diffLoss = 6.9617, kgLoss = 0.1712
2025-04-09 09:27:56.421417: Training Step 82/115: batchLoss = 4.0251, diffLoss = 6.5998, kgLoss = 0.1632
2025-04-09 09:27:57.143930: Training Step 83/115: batchLoss = 4.1428, diffLoss = 6.7978, kgLoss = 0.1603
2025-04-09 09:27:57.861020: Training Step 84/115: batchLoss = 4.7142, diffLoss = 7.7346, kgLoss = 0.1836
2025-04-09 09:27:58.573469: Training Step 85/115: batchLoss = 4.3857, diffLoss = 7.1987, kgLoss = 0.1663
2025-04-09 09:27:59.290054: Training Step 86/115: batchLoss = 4.6106, diffLoss = 7.5637, kgLoss = 0.1809
2025-04-09 09:28:00.005290: Training Step 87/115: batchLoss = 4.7350, diffLoss = 7.7715, kgLoss = 0.1802
2025-04-09 09:28:00.730564: Training Step 88/115: batchLoss = 4.9079, diffLoss = 8.0626, kgLoss = 0.1759
2025-04-09 09:28:01.456710: Training Step 89/115: batchLoss = 4.1412, diffLoss = 6.7932, kgLoss = 0.1632
2025-04-09 09:28:02.174799: Training Step 90/115: batchLoss = 5.1198, diffLoss = 8.4029, kgLoss = 0.1951
2025-04-09 09:28:02.891894: Training Step 91/115: batchLoss = 4.0790, diffLoss = 6.6951, kgLoss = 0.1547
2025-04-09 09:28:03.607538: Training Step 92/115: batchLoss = 4.6056, diffLoss = 7.5653, kgLoss = 0.1661
2025-04-09 09:28:04.325728: Training Step 93/115: batchLoss = 4.4207, diffLoss = 7.2607, kgLoss = 0.1607
2025-04-09 09:28:05.051008: Training Step 94/115: batchLoss = 4.9657, diffLoss = 8.1554, kgLoss = 0.1811
2025-04-09 09:28:05.788118: Training Step 95/115: batchLoss = 4.9047, diffLoss = 8.0503, kgLoss = 0.1865
2025-04-09 09:28:06.504295: Training Step 96/115: batchLoss = 4.7064, diffLoss = 7.7244, kgLoss = 0.1793
2025-04-09 09:28:07.217056: Training Step 97/115: batchLoss = 5.0832, diffLoss = 8.3522, kgLoss = 0.1797
2025-04-09 09:28:07.934038: Training Step 98/115: batchLoss = 4.8390, diffLoss = 7.9419, kgLoss = 0.1845
2025-04-09 09:28:08.647373: Training Step 99/115: batchLoss = 4.3813, diffLoss = 7.1947, kgLoss = 0.1613
2025-04-09 09:28:09.365579: Training Step 100/115: batchLoss = 4.5240, diffLoss = 7.4223, kgLoss = 0.1766
2025-04-09 09:28:10.097082: Training Step 101/115: batchLoss = 4.8169, diffLoss = 7.9076, kgLoss = 0.1809
2025-04-09 09:28:10.817928: Training Step 102/115: batchLoss = 4.2190, diffLoss = 6.9208, kgLoss = 0.1662
2025-04-09 09:28:11.546877: Training Step 103/115: batchLoss = 4.2163, diffLoss = 6.9139, kgLoss = 0.1699
2025-04-09 09:28:12.269306: Training Step 104/115: batchLoss = 4.7251, diffLoss = 7.7544, kgLoss = 0.1813
2025-04-09 09:28:13.010903: Training Step 105/115: batchLoss = 4.9523, diffLoss = 8.1276, kgLoss = 0.1893
2025-04-09 09:28:13.729999: Training Step 106/115: batchLoss = 5.0654, diffLoss = 8.3154, kgLoss = 0.1903
2025-04-09 09:28:14.457830: Training Step 107/115: batchLoss = 4.3835, diffLoss = 7.1952, kgLoss = 0.1660
2025-04-09 09:28:15.207181: Training Step 108/115: batchLoss = 4.5580, diffLoss = 7.4830, kgLoss = 0.1704
2025-04-09 09:28:15.956637: Training Step 109/115: batchLoss = 4.7006, diffLoss = 7.7105, kgLoss = 0.1857
2025-04-09 09:28:16.701596: Training Step 110/115: batchLoss = 4.5766, diffLoss = 7.5173, kgLoss = 0.1655
2025-04-09 09:28:17.423997: Training Step 111/115: batchLoss = 4.6260, diffLoss = 7.5943, kgLoss = 0.1736
2025-04-09 09:28:18.143951: Training Step 112/115: batchLoss = 4.7619, diffLoss = 7.8203, kgLoss = 0.1744
2025-04-09 09:28:18.796594: Training Step 113/115: batchLoss = 4.3288, diffLoss = 7.1059, kgLoss = 0.1631
2025-04-09 09:28:19.439219: Training Step 114/115: batchLoss = 5.9605, diffLoss = 9.7970, kgLoss = 0.2058
2025-04-09 09:28:19.557497: 
2025-04-09 09:28:19.558460: Epoch 5/1000, Train: epLoss = 1.3412, epDfLoss = 2.2013, epfTransLoss = 0.0000, epKgLoss = 0.0512  
2025-04-09 09:28:20.329566: Steps 0/90: batch_recall = 46.72, batch_ndcg = 35.20 
2025-04-09 09:28:21.088469: Steps 1/90: batch_recall = 50.55, batch_ndcg = 35.76 
2025-04-09 09:28:21.848176: Steps 2/90: batch_recall = 36.52, batch_ndcg = 24.96 
2025-04-09 09:28:22.614101: Steps 3/90: batch_recall = 34.33, batch_ndcg = 24.08 
2025-04-09 09:28:23.367028: Steps 4/90: batch_recall = 45.33, batch_ndcg = 32.00 
2025-04-09 09:28:24.114733: Steps 5/90: batch_recall = 29.34, batch_ndcg = 21.74 
2025-04-09 09:28:24.848357: Steps 6/90: batch_recall = 36.92, batch_ndcg = 26.99 
2025-04-09 09:28:25.603539: Steps 7/90: batch_recall = 33.27, batch_ndcg = 23.49 
2025-04-09 09:28:26.351910: Steps 8/90: batch_recall = 39.37, batch_ndcg = 27.02 
2025-04-09 09:28:27.099166: Steps 9/90: batch_recall = 33.22, batch_ndcg = 25.03 
2025-04-09 09:28:27.848213: Steps 10/90: batch_recall = 32.13, batch_ndcg = 22.38 
2025-04-09 09:28:28.605739: Steps 11/90: batch_recall = 36.34, batch_ndcg = 23.85 
2025-04-09 09:28:29.345509: Steps 12/90: batch_recall = 26.96, batch_ndcg = 18.86 
2025-04-09 09:28:30.088711: Steps 13/90: batch_recall = 28.62, batch_ndcg = 19.55 
2025-04-09 09:28:30.828760: Steps 14/90: batch_recall = 28.79, batch_ndcg = 19.40 
2025-04-09 09:28:31.561907: Steps 15/90: batch_recall = 33.42, batch_ndcg = 22.22 
2025-04-09 09:28:32.290021: Steps 16/90: batch_recall = 26.47, batch_ndcg = 19.20 
2025-04-09 09:28:33.025606: Steps 17/90: batch_recall = 27.26, batch_ndcg = 18.47 
2025-04-09 09:28:33.742852: Steps 18/90: batch_recall = 28.57, batch_ndcg = 19.33 
2025-04-09 09:28:34.469789: Steps 19/90: batch_recall = 25.99, batch_ndcg = 17.21 
2025-04-09 09:28:35.184026: Steps 20/90: batch_recall = 26.88, batch_ndcg = 17.40 
2025-04-09 09:28:35.906013: Steps 21/90: batch_recall = 29.63, batch_ndcg = 20.58 
2025-04-09 09:28:36.620306: Steps 22/90: batch_recall = 33.16, batch_ndcg = 22.84 
2025-04-09 09:28:37.333137: Steps 23/90: batch_recall = 24.91, batch_ndcg = 17.95 
2025-04-09 09:28:38.058329: Steps 24/90: batch_recall = 27.79, batch_ndcg = 18.95 
2025-04-09 09:28:38.771624: Steps 25/90: batch_recall = 27.20, batch_ndcg = 18.30 
2025-04-09 09:28:39.480608: Steps 26/90: batch_recall = 25.89, batch_ndcg = 17.40 
2025-04-09 09:28:40.192184: Steps 27/90: batch_recall = 26.05, batch_ndcg = 16.43 
2025-04-09 09:28:40.912164: Steps 28/90: batch_recall = 24.15, batch_ndcg = 16.94 
2025-04-09 09:28:41.629459: Steps 29/90: batch_recall = 25.71, batch_ndcg = 15.75 
2025-04-09 09:28:42.336410: Steps 30/90: batch_recall = 23.66, batch_ndcg = 13.88 
2025-04-09 09:28:43.061742: Steps 31/90: batch_recall = 25.64, batch_ndcg = 15.70 
2025-04-09 09:28:43.780275: Steps 32/90: batch_recall = 28.00, batch_ndcg = 17.34 
2025-04-09 09:28:44.494666: Steps 33/90: batch_recall = 22.39, batch_ndcg = 14.14 
2025-04-09 09:28:45.195471: Steps 34/90: batch_recall = 25.85, batch_ndcg = 17.16 
2025-04-09 09:28:45.899721: Steps 35/90: batch_recall = 25.53, batch_ndcg = 17.67 
2025-04-09 09:28:46.621211: Steps 36/90: batch_recall = 24.76, batch_ndcg = 16.80 
2025-04-09 09:28:47.350430: Steps 37/90: batch_recall = 23.26, batch_ndcg = 14.72 
2025-04-09 09:28:48.083382: Steps 38/90: batch_recall = 22.02, batch_ndcg = 14.49 
2025-04-09 09:28:48.809797: Steps 39/90: batch_recall = 22.89, batch_ndcg = 12.90 
2025-04-09 09:28:49.544579: Steps 40/90: batch_recall = 22.56, batch_ndcg = 15.28 
2025-04-09 09:28:50.259789: Steps 41/90: batch_recall = 21.86, batch_ndcg = 14.53 
2025-04-09 09:28:50.981033: Steps 42/90: batch_recall = 28.00, batch_ndcg = 19.93 
2025-04-09 09:28:51.680286: Steps 43/90: batch_recall = 17.53, batch_ndcg = 12.00 
2025-04-09 09:28:52.396030: Steps 44/90: batch_recall = 21.81, batch_ndcg = 14.32 
2025-04-09 09:28:53.085940: Steps 45/90: batch_recall = 20.83, batch_ndcg = 12.19 
2025-04-09 09:28:53.787122: Steps 46/90: batch_recall = 20.70, batch_ndcg = 12.14 
2025-04-09 09:28:54.499101: Steps 47/90: batch_recall = 19.02, batch_ndcg = 12.34 
2025-04-09 09:28:55.211375: Steps 48/90: batch_recall = 22.84, batch_ndcg = 13.85 
2025-04-09 09:28:55.930721: Steps 49/90: batch_recall = 15.46, batch_ndcg = 11.08 
2025-04-09 09:28:56.661106: Steps 50/90: batch_recall = 26.10, batch_ndcg = 16.35 
2025-04-09 09:28:57.364864: Steps 51/90: batch_recall = 25.55, batch_ndcg = 15.90 
2025-04-09 09:28:58.063039: Steps 52/90: batch_recall = 21.75, batch_ndcg = 13.46 
2025-04-09 09:28:58.762881: Steps 53/90: batch_recall = 21.83, batch_ndcg = 13.68 
2025-04-09 09:28:59.465536: Steps 54/90: batch_recall = 19.81, batch_ndcg = 13.18 
2025-04-09 09:29:00.180417: Steps 55/90: batch_recall = 20.47, batch_ndcg = 12.02 
2025-04-09 09:29:00.881292: Steps 56/90: batch_recall = 16.88, batch_ndcg = 11.08 
2025-04-09 09:29:01.594258: Steps 57/90: batch_recall = 22.30, batch_ndcg = 14.52 
2025-04-09 09:29:02.294753: Steps 58/90: batch_recall = 28.35, batch_ndcg = 17.52 
2025-04-09 09:29:02.997261: Steps 59/90: batch_recall = 18.79, batch_ndcg = 11.66 
2025-04-09 09:29:03.704501: Steps 60/90: batch_recall = 20.50, batch_ndcg = 12.93 
2025-04-09 09:29:04.415720: Steps 61/90: batch_recall = 25.88, batch_ndcg = 15.97 
2025-04-09 09:29:05.105950: Steps 62/90: batch_recall = 22.94, batch_ndcg = 15.27 
2025-04-09 09:29:05.816197: Steps 63/90: batch_recall = 20.94, batch_ndcg = 13.13 
2025-04-09 09:29:06.524025: Steps 64/90: batch_recall = 23.03, batch_ndcg = 12.77 
2025-04-09 09:29:07.235907: Steps 65/90: batch_recall = 23.93, batch_ndcg = 15.38 
2025-04-09 09:29:07.951232: Steps 66/90: batch_recall = 24.58, batch_ndcg = 14.98 
2025-04-09 09:29:08.648108: Steps 67/90: batch_recall = 17.86, batch_ndcg = 10.62 
2025-04-09 09:29:09.365113: Steps 68/90: batch_recall = 20.04, batch_ndcg = 12.58 
2025-04-09 09:29:10.062437: Steps 69/90: batch_recall = 21.54, batch_ndcg = 13.15 
2025-04-09 09:29:10.778714: Steps 70/90: batch_recall = 21.73, batch_ndcg = 13.94 
2025-04-09 09:29:11.504143: Steps 71/90: batch_recall = 24.64, batch_ndcg = 15.29 
2025-04-09 09:29:12.250501: Steps 72/90: batch_recall = 17.27, batch_ndcg = 11.09 
2025-04-09 09:29:13.014545: Steps 73/90: batch_recall = 22.98, batch_ndcg = 14.58 
2025-04-09 09:29:13.715507: Steps 74/90: batch_recall = 24.16, batch_ndcg = 15.22 
2025-04-09 09:29:14.423698: Steps 75/90: batch_recall = 25.31, batch_ndcg = 14.99 
2025-04-09 09:29:15.127080: Steps 76/90: batch_recall = 25.02, batch_ndcg = 13.99 
2025-04-09 09:29:15.841377: Steps 77/90: batch_recall = 20.65, batch_ndcg = 13.20 
2025-04-09 09:29:16.557860: Steps 78/90: batch_recall = 18.62, batch_ndcg = 12.21 
2025-04-09 09:29:17.286330: Steps 79/90: batch_recall = 23.51, batch_ndcg = 17.02 
2025-04-09 09:29:18.007902: Steps 80/90: batch_recall = 28.31, batch_ndcg = 17.66 
2025-04-09 09:29:18.715824: Steps 81/90: batch_recall = 25.15, batch_ndcg = 15.53 
2025-04-09 09:29:19.436591: Steps 82/90: batch_recall = 23.54, batch_ndcg = 14.77 
2025-04-09 09:29:20.176467: Steps 83/90: batch_recall = 18.26, batch_ndcg = 11.79 
2025-04-09 09:29:20.908961: Steps 84/90: batch_recall = 21.13, batch_ndcg = 12.19 
2025-04-09 09:29:21.621519: Steps 85/90: batch_recall = 22.68, batch_ndcg = 13.97 
2025-04-09 09:29:22.335690: Steps 86/90: batch_recall = 24.98, batch_ndcg = 14.91 
2025-04-09 09:29:23.049157: Steps 87/90: batch_recall = 27.38, batch_ndcg = 16.95 
2025-04-09 09:29:23.743826: Steps 88/90: batch_recall = 29.39, batch_ndcg = 16.83 
2025-04-09 09:29:24.316063: Steps 89/90: batch_recall = 20.75, batch_ndcg = 11.26 
2025-04-09 09:29:24.316727: Epoch 5/1000, Test: Recall = 0.0504, NDCG = 0.0330  

2025-04-09 09:29:25.340731: Training Step 0/115: batchLoss = 5.4398, diffLoss = 8.9358, kgLoss = 0.1959
2025-04-09 09:29:26.066926: Training Step 1/115: batchLoss = 5.1425, diffLoss = 8.4441, kgLoss = 0.1902
2025-04-09 09:29:26.811395: Training Step 2/115: batchLoss = 5.3505, diffLoss = 8.7814, kgLoss = 0.2041
2025-04-09 09:29:27.537726: Training Step 3/115: batchLoss = 4.6202, diffLoss = 7.5808, kgLoss = 0.1794
2025-04-09 09:29:28.268100: Training Step 4/115: batchLoss = 4.6036, diffLoss = 7.5554, kgLoss = 0.1759
2025-04-09 09:29:29.002831: Training Step 5/115: batchLoss = 4.5884, diffLoss = 7.5285, kgLoss = 0.1783
2025-04-09 09:29:29.744479: Training Step 6/115: batchLoss = 4.6345, diffLoss = 7.6036, kgLoss = 0.1809
2025-04-09 09:29:30.482666: Training Step 7/115: batchLoss = 4.6291, diffLoss = 7.5948, kgLoss = 0.1805
2025-04-09 09:29:31.202954: Training Step 8/115: batchLoss = 4.3918, diffLoss = 7.2063, kgLoss = 0.1700
2025-04-09 09:29:31.927873: Training Step 9/115: batchLoss = 4.9728, diffLoss = 8.1697, kgLoss = 0.1775
2025-04-09 09:29:32.657402: Training Step 10/115: batchLoss = 4.3035, diffLoss = 7.0509, kgLoss = 0.1823
2025-04-09 09:29:33.376355: Training Step 11/115: batchLoss = 4.4923, diffLoss = 7.3692, kgLoss = 0.1769
2025-04-09 09:29:34.101995: Training Step 12/115: batchLoss = 4.6852, diffLoss = 7.6938, kgLoss = 0.1721
2025-04-09 09:29:34.821520: Training Step 13/115: batchLoss = 4.6181, diffLoss = 7.5828, kgLoss = 0.1709
2025-04-09 09:29:35.542329: Training Step 14/115: batchLoss = 4.9784, diffLoss = 8.1785, kgLoss = 0.1782
2025-04-09 09:29:36.268548: Training Step 15/115: batchLoss = 4.2333, diffLoss = 6.9413, kgLoss = 0.1713
2025-04-09 09:29:36.994357: Training Step 16/115: batchLoss = 4.3627, diffLoss = 7.1562, kgLoss = 0.1724
2025-04-09 09:29:37.715789: Training Step 17/115: batchLoss = 4.4507, diffLoss = 7.3009, kgLoss = 0.1754
2025-04-09 09:29:38.433685: Training Step 18/115: batchLoss = 5.1815, diffLoss = 8.5028, kgLoss = 0.1997
2025-04-09 09:29:39.152919: Training Step 19/115: batchLoss = 4.0365, diffLoss = 6.6204, kgLoss = 0.1607
2025-04-09 09:29:39.884354: Training Step 20/115: batchLoss = 4.3191, diffLoss = 7.0867, kgLoss = 0.1679
2025-04-09 09:29:40.615661: Training Step 21/115: batchLoss = 4.9824, diffLoss = 8.1863, kgLoss = 0.1766
2025-04-09 09:29:41.345501: Training Step 22/115: batchLoss = 4.6453, diffLoss = 7.6247, kgLoss = 0.1763
2025-04-09 09:29:42.076588: Training Step 23/115: batchLoss = 4.5134, diffLoss = 7.4079, kgLoss = 0.1715
2025-04-09 09:29:42.805798: Training Step 24/115: batchLoss = 4.7254, diffLoss = 7.7518, kgLoss = 0.1857
2025-04-09 09:29:43.541524: Training Step 25/115: batchLoss = 4.3075, diffLoss = 7.0708, kgLoss = 0.1625
2025-04-09 09:29:44.286520: Training Step 26/115: batchLoss = 4.8335, diffLoss = 7.9326, kgLoss = 0.1847
2025-04-09 09:29:45.049531: Training Step 27/115: batchLoss = 4.6464, diffLoss = 7.6275, kgLoss = 0.1748
2025-04-09 09:29:45.794014: Training Step 28/115: batchLoss = 4.3554, diffLoss = 7.1461, kgLoss = 0.1694
2025-04-09 09:29:46.524722: Training Step 29/115: batchLoss = 4.4823, diffLoss = 7.3578, kgLoss = 0.1690
2025-04-09 09:29:47.263017: Training Step 30/115: batchLoss = 4.7874, diffLoss = 7.8523, kgLoss = 0.1900
2025-04-09 09:29:47.997244: Training Step 31/115: batchLoss = 4.3961, diffLoss = 7.2147, kgLoss = 0.1681
2025-04-09 09:29:48.730923: Training Step 32/115: batchLoss = 4.7579, diffLoss = 7.8076, kgLoss = 0.1833
2025-04-09 09:29:49.461455: Training Step 33/115: batchLoss = 5.0598, diffLoss = 8.3086, kgLoss = 0.1867
2025-04-09 09:29:50.184817: Training Step 34/115: batchLoss = 3.7904, diffLoss = 6.2165, kgLoss = 0.1511
2025-04-09 09:29:50.910018: Training Step 35/115: batchLoss = 4.7646, diffLoss = 7.8222, kgLoss = 0.1782
2025-04-09 09:29:51.640777: Training Step 36/115: batchLoss = 4.9098, diffLoss = 8.0590, kgLoss = 0.1860
2025-04-09 09:29:52.375710: Training Step 37/115: batchLoss = 4.5547, diffLoss = 7.4703, kgLoss = 0.1812
2025-04-09 09:29:53.099062: Training Step 38/115: batchLoss = 4.5970, diffLoss = 7.5534, kgLoss = 0.1623
2025-04-09 09:29:53.831735: Training Step 39/115: batchLoss = 4.9858, diffLoss = 8.1862, kgLoss = 0.1852
2025-04-09 09:29:54.581932: Training Step 40/115: batchLoss = 4.6746, diffLoss = 7.6694, kgLoss = 0.1825
2025-04-09 09:29:55.331846: Training Step 41/115: batchLoss = 4.5770, diffLoss = 7.5067, kgLoss = 0.1825
2025-04-09 09:29:56.071328: Training Step 42/115: batchLoss = 5.0905, diffLoss = 8.3502, kgLoss = 0.2010
2025-04-09 09:29:56.795908: Training Step 43/115: batchLoss = 4.3962, diffLoss = 7.2136, kgLoss = 0.1703
2025-04-09 09:29:57.537053: Training Step 44/115: batchLoss = 4.1052, diffLoss = 6.7349, kgLoss = 0.1606
2025-04-09 09:29:58.260583: Training Step 45/115: batchLoss = 4.9057, diffLoss = 8.0556, kgLoss = 0.1808
2025-04-09 09:29:58.986899: Training Step 46/115: batchLoss = 4.3451, diffLoss = 7.1324, kgLoss = 0.1643
2025-04-09 09:29:59.715530: Training Step 47/115: batchLoss = 4.7548, diffLoss = 7.8034, kgLoss = 0.1819
2025-04-09 09:30:00.437076: Training Step 48/115: batchLoss = 4.5540, diffLoss = 7.4751, kgLoss = 0.1722
2025-04-09 09:30:01.148169: Training Step 49/115: batchLoss = 4.3478, diffLoss = 7.1351, kgLoss = 0.1668
2025-04-09 09:30:01.867696: Training Step 50/115: batchLoss = 4.6522, diffLoss = 7.6405, kgLoss = 0.1698
2025-04-09 09:30:02.590062: Training Step 51/115: batchLoss = 4.4606, diffLoss = 7.3260, kgLoss = 0.1626
2025-04-09 09:30:03.312597: Training Step 52/115: batchLoss = 4.0658, diffLoss = 6.6639, kgLoss = 0.1686
2025-04-09 09:30:04.035176: Training Step 53/115: batchLoss = 4.4503, diffLoss = 7.3042, kgLoss = 0.1694
2025-04-09 09:30:04.759030: Training Step 54/115: batchLoss = 4.3984, diffLoss = 7.2232, kgLoss = 0.1612
2025-04-09 09:30:05.486494: Training Step 55/115: batchLoss = 4.5343, diffLoss = 7.4361, kgLoss = 0.1816
2025-04-09 09:30:06.216078: Training Step 56/115: batchLoss = 4.2457, diffLoss = 6.9645, kgLoss = 0.1676
2025-04-09 09:30:06.945958: Training Step 57/115: batchLoss = 4.9717, diffLoss = 8.1684, kgLoss = 0.1768
2025-04-09 09:30:07.683099: Training Step 58/115: batchLoss = 4.9867, diffLoss = 8.1900, kgLoss = 0.1816
2025-04-09 09:30:08.427081: Training Step 59/115: batchLoss = 4.4328, diffLoss = 7.2683, kgLoss = 0.1797
2025-04-09 09:30:09.166151: Training Step 60/115: batchLoss = 4.5565, diffLoss = 7.4740, kgLoss = 0.1802
2025-04-09 09:30:09.897160: Training Step 61/115: batchLoss = 4.8722, diffLoss = 7.9981, kgLoss = 0.1834
2025-04-09 09:30:10.637478: Training Step 62/115: batchLoss = 4.2401, diffLoss = 6.9622, kgLoss = 0.1571
2025-04-09 09:30:11.363644: Training Step 63/115: batchLoss = 4.6182, diffLoss = 7.5788, kgLoss = 0.1772
2025-04-09 09:30:12.091257: Training Step 64/115: batchLoss = 4.1750, diffLoss = 6.8429, kgLoss = 0.1732
2025-04-09 09:30:12.833716: Training Step 65/115: batchLoss = 3.8730, diffLoss = 6.3520, kgLoss = 0.1545
2025-04-09 09:30:13.567204: Training Step 66/115: batchLoss = 4.7701, diffLoss = 7.8329, kgLoss = 0.1759
2025-04-09 09:30:14.295270: Training Step 67/115: batchLoss = 4.4498, diffLoss = 7.3042, kgLoss = 0.1683
2025-04-09 09:30:15.029528: Training Step 68/115: batchLoss = 4.5116, diffLoss = 7.3984, kgLoss = 0.1814
2025-04-09 09:30:17.108309: Training Step 69/115: batchLoss = 4.5542, diffLoss = 7.4709, kgLoss = 0.1791
2025-04-09 09:30:17.859280: Training Step 70/115: batchLoss = 4.8503, diffLoss = 7.9618, kgLoss = 0.1829
2025-04-09 09:30:18.892168: Training Step 71/115: batchLoss = 4.3973, diffLoss = 7.2189, kgLoss = 0.1649
2025-04-09 09:30:19.619462: Training Step 72/115: batchLoss = 4.9737, diffLoss = 8.1700, kgLoss = 0.1791
2025-04-09 09:30:20.352585: Training Step 73/115: batchLoss = 5.1483, diffLoss = 8.4588, kgLoss = 0.1825
2025-04-09 09:30:21.084372: Training Step 74/115: batchLoss = 5.1699, diffLoss = 8.4821, kgLoss = 0.2015
2025-04-09 09:30:21.820339: Training Step 75/115: batchLoss = 4.7887, diffLoss = 7.8490, kgLoss = 0.1983
2025-04-09 09:30:22.550510: Training Step 76/115: batchLoss = 4.4285, diffLoss = 7.2697, kgLoss = 0.1667
2025-04-09 09:30:23.281262: Training Step 77/115: batchLoss = 5.5107, diffLoss = 9.0485, kgLoss = 0.2040
2025-04-09 09:30:24.014548: Training Step 78/115: batchLoss = 4.8132, diffLoss = 7.8998, kgLoss = 0.1835
2025-04-09 09:30:24.747989: Training Step 79/115: batchLoss = 4.5652, diffLoss = 7.4882, kgLoss = 0.1807
2025-04-09 09:30:25.471362: Training Step 80/115: batchLoss = 5.2357, diffLoss = 8.5913, kgLoss = 0.2023
2025-04-09 09:30:26.204345: Training Step 81/115: batchLoss = 4.7799, diffLoss = 7.8444, kgLoss = 0.1830
2025-04-09 09:30:26.925658: Training Step 82/115: batchLoss = 4.6974, diffLoss = 7.6993, kgLoss = 0.1946
2025-04-09 09:30:27.641512: Training Step 83/115: batchLoss = 4.9161, diffLoss = 8.0652, kgLoss = 0.1925
2025-04-09 09:30:28.361473: Training Step 84/115: batchLoss = 4.5276, diffLoss = 7.4377, kgLoss = 0.1625
2025-04-09 09:30:29.101240: Training Step 85/115: batchLoss = 4.3266, diffLoss = 7.0975, kgLoss = 0.1702
2025-04-09 09:30:29.823140: Training Step 86/115: batchLoss = 4.5938, diffLoss = 7.5403, kgLoss = 0.1740
2025-04-09 09:30:30.553579: Training Step 87/115: batchLoss = 5.5429, diffLoss = 9.1031, kgLoss = 0.2026
2025-04-09 09:30:31.280669: Training Step 88/115: batchLoss = 4.4369, diffLoss = 7.2767, kgLoss = 0.1773
2025-04-09 09:30:32.017810: Training Step 89/115: batchLoss = 4.4804, diffLoss = 7.3585, kgLoss = 0.1632
2025-04-09 09:30:32.738285: Training Step 90/115: batchLoss = 4.2321, diffLoss = 6.9426, kgLoss = 0.1665
2025-04-09 09:30:33.454765: Training Step 91/115: batchLoss = 4.2855, diffLoss = 7.0324, kgLoss = 0.1652
2025-04-09 09:30:34.175482: Training Step 92/115: batchLoss = 4.2638, diffLoss = 6.9958, kgLoss = 0.1659
2025-04-09 09:30:34.893983: Training Step 93/115: batchLoss = 4.5832, diffLoss = 7.5232, kgLoss = 0.1730
2025-04-09 09:30:35.616590: Training Step 94/115: batchLoss = 4.0936, diffLoss = 6.7034, kgLoss = 0.1789
2025-04-09 09:30:36.340742: Training Step 95/115: batchLoss = 4.7217, diffLoss = 7.7516, kgLoss = 0.1770
2025-04-09 09:30:37.055752: Training Step 96/115: batchLoss = 4.3134, diffLoss = 7.0719, kgLoss = 0.1756
2025-04-09 09:30:37.772764: Training Step 97/115: batchLoss = 4.9699, diffLoss = 8.1573, kgLoss = 0.1889
2025-04-09 09:30:38.488934: Training Step 98/115: batchLoss = 4.8517, diffLoss = 7.9693, kgLoss = 0.1754
2025-04-09 09:30:39.209017: Training Step 99/115: batchLoss = 5.1245, diffLoss = 8.4161, kgLoss = 0.1871
2025-04-09 09:30:39.933946: Training Step 100/115: batchLoss = 4.1993, diffLoss = 6.8885, kgLoss = 0.1655
2025-04-09 09:30:40.658636: Training Step 101/115: batchLoss = 4.0836, diffLoss = 6.6945, kgLoss = 0.1673
2025-04-09 09:30:41.379325: Training Step 102/115: batchLoss = 4.5745, diffLoss = 7.5033, kgLoss = 0.1814
2025-04-09 09:30:42.097135: Training Step 103/115: batchLoss = 4.5562, diffLoss = 7.4780, kgLoss = 0.1734
2025-04-09 09:30:42.819514: Training Step 104/115: batchLoss = 4.3505, diffLoss = 7.1397, kgLoss = 0.1666
2025-04-09 09:30:43.545689: Training Step 105/115: batchLoss = 4.3484, diffLoss = 7.1246, kgLoss = 0.1841
2025-04-09 09:30:44.261224: Training Step 106/115: batchLoss = 4.1525, diffLoss = 6.8171, kgLoss = 0.1555
2025-04-09 09:30:44.981105: Training Step 107/115: batchLoss = 4.3637, diffLoss = 7.1631, kgLoss = 0.1647
2025-04-09 09:30:45.706620: Training Step 108/115: batchLoss = 5.2742, diffLoss = 8.6626, kgLoss = 0.1917
2025-04-09 09:30:46.433293: Training Step 109/115: batchLoss = 5.5372, diffLoss = 9.0900, kgLoss = 0.2080
2025-04-09 09:30:47.152641: Training Step 110/115: batchLoss = 4.9715, diffLoss = 8.1524, kgLoss = 0.2002
2025-04-09 09:30:47.869575: Training Step 111/115: batchLoss = 4.5834, diffLoss = 7.5213, kgLoss = 0.1765
2025-04-09 09:30:48.576011: Training Step 112/115: batchLoss = 5.0614, diffLoss = 8.3047, kgLoss = 0.1965
2025-04-09 09:30:49.217227: Training Step 113/115: batchLoss = 5.1577, diffLoss = 8.4606, kgLoss = 0.2032
2025-04-09 09:30:49.829957: Training Step 114/115: batchLoss = 4.7628, diffLoss = 7.8126, kgLoss = 0.1882
2025-04-09 09:30:49.952753: 
2025-04-09 09:30:49.953652: Epoch 6/1000, Train: epLoss = 1.3332, epDfLoss = 2.1880, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:30:50.699114: Steps 0/90: batch_recall = 48.11, batch_ndcg = 35.95 
2025-04-09 09:30:51.424392: Steps 1/90: batch_recall = 51.39, batch_ndcg = 35.84 
2025-04-09 09:30:52.155987: Steps 2/90: batch_recall = 38.16, batch_ndcg = 25.56 
2025-04-09 09:30:52.883600: Steps 3/90: batch_recall = 33.99, batch_ndcg = 24.70 
2025-04-09 09:30:53.613837: Steps 4/90: batch_recall = 46.00, batch_ndcg = 32.70 
2025-04-09 09:30:54.353145: Steps 5/90: batch_recall = 29.66, batch_ndcg = 21.78 
2025-04-09 09:30:55.088793: Steps 6/90: batch_recall = 37.69, batch_ndcg = 27.48 
2025-04-09 09:30:55.828638: Steps 7/90: batch_recall = 35.17, batch_ndcg = 24.82 
2025-04-09 09:30:56.562325: Steps 8/90: batch_recall = 40.38, batch_ndcg = 27.90 
2025-04-09 09:30:57.367469: Steps 9/90: batch_recall = 34.59, batch_ndcg = 25.83 
2025-04-09 09:30:58.116118: Steps 10/90: batch_recall = 33.76, batch_ndcg = 23.20 
2025-04-09 09:30:58.862481: Steps 11/90: batch_recall = 37.81, batch_ndcg = 23.90 
2025-04-09 09:30:59.589078: Steps 12/90: batch_recall = 28.61, batch_ndcg = 20.08 
2025-04-09 09:31:00.329554: Steps 13/90: batch_recall = 29.17, batch_ndcg = 19.95 
2025-04-09 09:31:01.081720: Steps 14/90: batch_recall = 30.57, batch_ndcg = 20.35 
2025-04-09 09:31:01.824394: Steps 15/90: batch_recall = 35.57, batch_ndcg = 24.51 
2025-04-09 09:31:02.562886: Steps 16/90: batch_recall = 28.40, batch_ndcg = 20.19 
2025-04-09 09:31:03.300751: Steps 17/90: batch_recall = 29.77, batch_ndcg = 19.56 
2025-04-09 09:31:04.041656: Steps 18/90: batch_recall = 30.68, batch_ndcg = 20.30 
2025-04-09 09:31:04.790420: Steps 19/90: batch_recall = 26.63, batch_ndcg = 17.83 
2025-04-09 09:31:05.521433: Steps 20/90: batch_recall = 29.29, batch_ndcg = 18.67 
2025-04-09 09:31:06.270350: Steps 21/90: batch_recall = 30.71, batch_ndcg = 21.55 
2025-04-09 09:31:06.990702: Steps 22/90: batch_recall = 35.15, batch_ndcg = 24.86 
2025-04-09 09:31:07.731680: Steps 23/90: batch_recall = 26.34, batch_ndcg = 19.32 
2025-04-09 09:31:08.457352: Steps 24/90: batch_recall = 29.35, batch_ndcg = 19.97 
2025-04-09 09:31:09.195567: Steps 25/90: batch_recall = 27.76, batch_ndcg = 18.18 
2025-04-09 09:31:09.928258: Steps 26/90: batch_recall = 27.12, batch_ndcg = 18.40 
2025-04-09 09:31:10.659215: Steps 27/90: batch_recall = 26.76, batch_ndcg = 17.30 
2025-04-09 09:31:11.399807: Steps 28/90: batch_recall = 24.48, batch_ndcg = 17.90 
2025-04-09 09:31:12.139822: Steps 29/90: batch_recall = 25.41, batch_ndcg = 15.92 
2025-04-09 09:31:12.890002: Steps 30/90: batch_recall = 24.40, batch_ndcg = 14.71 
2025-04-09 09:31:13.634525: Steps 31/90: batch_recall = 25.20, batch_ndcg = 15.98 
2025-04-09 09:31:14.372261: Steps 32/90: batch_recall = 29.65, batch_ndcg = 18.52 
2025-04-09 09:31:15.093916: Steps 33/90: batch_recall = 23.86, batch_ndcg = 14.98 
2025-04-09 09:31:15.822591: Steps 34/90: batch_recall = 26.64, batch_ndcg = 17.97 
2025-04-09 09:31:16.563218: Steps 35/90: batch_recall = 26.72, batch_ndcg = 18.87 
2025-04-09 09:31:17.305644: Steps 36/90: batch_recall = 27.09, batch_ndcg = 17.69 
2025-04-09 09:31:18.044237: Steps 37/90: batch_recall = 25.36, batch_ndcg = 16.05 
2025-04-09 09:31:18.786630: Steps 38/90: batch_recall = 24.66, batch_ndcg = 16.01 
2025-04-09 09:31:19.534112: Steps 39/90: batch_recall = 23.87, batch_ndcg = 13.65 
2025-04-09 09:31:20.271426: Steps 40/90: batch_recall = 23.20, batch_ndcg = 16.46 
2025-04-09 09:31:20.978031: Steps 41/90: batch_recall = 23.70, batch_ndcg = 15.61 
2025-04-09 09:31:21.696331: Steps 42/90: batch_recall = 28.69, batch_ndcg = 20.60 
2025-04-09 09:31:22.407816: Steps 43/90: batch_recall = 18.78, batch_ndcg = 13.24 
2025-04-09 09:31:23.124749: Steps 44/90: batch_recall = 23.62, batch_ndcg = 15.85 
2025-04-09 09:31:23.823853: Steps 45/90: batch_recall = 23.38, batch_ndcg = 13.56 
2025-04-09 09:31:24.526522: Steps 46/90: batch_recall = 23.49, batch_ndcg = 13.19 
2025-04-09 09:31:25.236071: Steps 47/90: batch_recall = 20.98, batch_ndcg = 13.38 
2025-04-09 09:31:25.946399: Steps 48/90: batch_recall = 22.96, batch_ndcg = 14.05 
2025-04-09 09:31:26.670933: Steps 49/90: batch_recall = 16.68, batch_ndcg = 11.91 
2025-04-09 09:31:27.350546: Steps 50/90: batch_recall = 26.50, batch_ndcg = 17.08 
2025-04-09 09:31:28.062778: Steps 51/90: batch_recall = 27.71, batch_ndcg = 17.61 
2025-04-09 09:31:28.806292: Steps 52/90: batch_recall = 23.49, batch_ndcg = 14.79 
2025-04-09 09:31:29.550599: Steps 53/90: batch_recall = 26.86, batch_ndcg = 15.67 
2025-04-09 09:31:30.284520: Steps 54/90: batch_recall = 20.21, batch_ndcg = 13.41 
2025-04-09 09:31:31.021357: Steps 55/90: batch_recall = 22.84, batch_ndcg = 13.31 
2025-04-09 09:31:31.761309: Steps 56/90: batch_recall = 18.34, batch_ndcg = 11.59 
2025-04-09 09:31:32.499261: Steps 57/90: batch_recall = 24.09, batch_ndcg = 15.19 
2025-04-09 09:31:33.223605: Steps 58/90: batch_recall = 28.94, batch_ndcg = 18.13 
2025-04-09 09:31:33.948172: Steps 59/90: batch_recall = 20.40, batch_ndcg = 12.59 
2025-04-09 09:31:34.673063: Steps 60/90: batch_recall = 19.94, batch_ndcg = 12.89 
2025-04-09 09:31:35.404742: Steps 61/90: batch_recall = 28.68, batch_ndcg = 17.82 
2025-04-09 09:31:36.127004: Steps 62/90: batch_recall = 25.91, batch_ndcg = 17.08 
2025-04-09 09:31:36.832751: Steps 63/90: batch_recall = 22.86, batch_ndcg = 14.11 
2025-04-09 09:31:37.573993: Steps 64/90: batch_recall = 24.67, batch_ndcg = 13.93 
2025-04-09 09:31:38.290859: Steps 65/90: batch_recall = 26.67, batch_ndcg = 16.35 
2025-04-09 09:31:38.994868: Steps 66/90: batch_recall = 25.44, batch_ndcg = 15.99 
2025-04-09 09:31:39.699813: Steps 67/90: batch_recall = 18.26, batch_ndcg = 11.44 
2025-04-09 09:31:40.402324: Steps 68/90: batch_recall = 20.31, batch_ndcg = 14.05 
2025-04-09 09:31:41.118645: Steps 69/90: batch_recall = 23.74, batch_ndcg = 14.44 
2025-04-09 09:31:41.802805: Steps 70/90: batch_recall = 22.72, batch_ndcg = 14.53 
2025-04-09 09:31:42.520327: Steps 71/90: batch_recall = 28.12, batch_ndcg = 17.05 
2025-04-09 09:31:43.254684: Steps 72/90: batch_recall = 19.77, batch_ndcg = 12.18 
2025-04-09 09:31:43.987508: Steps 73/90: batch_recall = 23.32, batch_ndcg = 15.37 
2025-04-09 09:31:44.745564: Steps 74/90: batch_recall = 27.10, batch_ndcg = 17.15 
2025-04-09 09:31:45.497247: Steps 75/90: batch_recall = 26.73, batch_ndcg = 15.87 
2025-04-09 09:31:46.231178: Steps 76/90: batch_recall = 26.58, batch_ndcg = 14.77 
2025-04-09 09:31:46.957539: Steps 77/90: batch_recall = 23.54, batch_ndcg = 14.50 
2025-04-09 09:31:47.680608: Steps 78/90: batch_recall = 20.30, batch_ndcg = 13.15 
2025-04-09 09:31:48.414813: Steps 79/90: batch_recall = 26.03, batch_ndcg = 18.46 
2025-04-09 09:31:49.146614: Steps 80/90: batch_recall = 30.37, batch_ndcg = 18.72 
2025-04-09 09:31:49.902246: Steps 81/90: batch_recall = 28.04, batch_ndcg = 16.86 
2025-04-09 09:31:50.652765: Steps 82/90: batch_recall = 24.05, batch_ndcg = 15.93 
2025-04-09 09:31:51.382643: Steps 83/90: batch_recall = 22.17, batch_ndcg = 13.30 
2025-04-09 09:31:52.115298: Steps 84/90: batch_recall = 22.38, batch_ndcg = 13.34 
2025-04-09 09:31:52.823335: Steps 85/90: batch_recall = 23.90, batch_ndcg = 14.34 
2025-04-09 09:31:53.529238: Steps 86/90: batch_recall = 29.52, batch_ndcg = 17.52 
2025-04-09 09:31:54.241651: Steps 87/90: batch_recall = 28.66, batch_ndcg = 18.70 
2025-04-09 09:31:54.981185: Steps 88/90: batch_recall = 31.97, batch_ndcg = 18.99 
2025-04-09 09:31:55.541457: Steps 89/90: batch_recall = 21.80, batch_ndcg = 12.47 
2025-04-09 09:31:55.542418: Epoch 6/1000, Test: Recall = 0.0535, NDCG = 0.0350  

2025-04-09 09:31:56.570218: Training Step 0/115: batchLoss = 4.8963, diffLoss = 8.0360, kgLoss = 0.1868
2025-04-09 09:31:57.312981: Training Step 1/115: batchLoss = 4.3636, diffLoss = 7.1592, kgLoss = 0.1702
2025-04-09 09:31:58.050104: Training Step 2/115: batchLoss = 4.8852, diffLoss = 8.0246, kgLoss = 0.1763
2025-04-09 09:31:58.796007: Training Step 3/115: batchLoss = 4.3744, diffLoss = 7.1833, kgLoss = 0.1610
2025-04-09 09:31:59.541966: Training Step 4/115: batchLoss = 4.4915, diffLoss = 7.3742, kgLoss = 0.1674
2025-04-09 09:32:00.295374: Training Step 5/115: batchLoss = 4.6943, diffLoss = 7.6819, kgLoss = 0.2129
2025-04-09 09:32:01.057344: Training Step 6/115: batchLoss = 4.7553, diffLoss = 7.8024, kgLoss = 0.1847
2025-04-09 09:32:01.821372: Training Step 7/115: batchLoss = 4.6816, diffLoss = 7.6858, kgLoss = 0.1752
2025-04-09 09:32:02.577988: Training Step 8/115: batchLoss = 4.5459, diffLoss = 7.4642, kgLoss = 0.1683
2025-04-09 09:32:03.331406: Training Step 9/115: batchLoss = 4.9780, diffLoss = 8.1644, kgLoss = 0.1985
2025-04-09 09:32:04.089074: Training Step 10/115: batchLoss = 4.4303, diffLoss = 7.2655, kgLoss = 0.1776
2025-04-09 09:32:04.841708: Training Step 11/115: batchLoss = 4.6636, diffLoss = 7.6481, kgLoss = 0.1867
2025-04-09 09:32:05.599216: Training Step 12/115: batchLoss = 4.9066, diffLoss = 8.0582, kgLoss = 0.1792
2025-04-09 09:32:06.355070: Training Step 13/115: batchLoss = 4.7767, diffLoss = 7.8362, kgLoss = 0.1873
2025-04-09 09:32:07.107446: Training Step 14/115: batchLoss = 5.4174, diffLoss = 8.8939, kgLoss = 0.2026
2025-04-09 09:32:07.868218: Training Step 15/115: batchLoss = 5.4168, diffLoss = 8.8965, kgLoss = 0.1973
2025-04-09 09:32:08.622833: Training Step 16/115: batchLoss = 4.4111, diffLoss = 7.2314, kgLoss = 0.1807
2025-04-09 09:32:09.356938: Training Step 17/115: batchLoss = 5.2586, diffLoss = 8.6349, kgLoss = 0.1942
2025-04-09 09:32:10.093727: Training Step 18/115: batchLoss = 4.6331, diffLoss = 7.6038, kgLoss = 0.1770
2025-04-09 09:32:10.816501: Training Step 19/115: batchLoss = 5.2904, diffLoss = 8.6907, kgLoss = 0.1900
2025-04-09 09:32:11.550028: Training Step 20/115: batchLoss = 4.3529, diffLoss = 7.1423, kgLoss = 0.1688
2025-04-09 09:32:12.285680: Training Step 21/115: batchLoss = 4.8426, diffLoss = 7.9523, kgLoss = 0.1782
2025-04-09 09:32:13.007468: Training Step 22/115: batchLoss = 4.7488, diffLoss = 7.7970, kgLoss = 0.1764
2025-04-09 09:32:13.729301: Training Step 23/115: batchLoss = 4.5997, diffLoss = 7.5468, kgLoss = 0.1791
2025-04-09 09:32:14.443738: Training Step 24/115: batchLoss = 4.9424, diffLoss = 8.1188, kgLoss = 0.1778
2025-04-09 09:32:15.175854: Training Step 25/115: batchLoss = 4.4288, diffLoss = 7.2647, kgLoss = 0.1749
2025-04-09 09:32:15.914442: Training Step 26/115: batchLoss = 4.5175, diffLoss = 7.4176, kgLoss = 0.1673
2025-04-09 09:32:16.665282: Training Step 27/115: batchLoss = 4.9027, diffLoss = 8.0422, kgLoss = 0.1936
2025-04-09 09:32:17.419318: Training Step 28/115: batchLoss = 4.2245, diffLoss = 6.9235, kgLoss = 0.1759
2025-04-09 09:32:18.167676: Training Step 29/115: batchLoss = 4.5974, diffLoss = 7.5469, kgLoss = 0.1733
2025-04-09 09:32:18.909428: Training Step 30/115: batchLoss = 4.9905, diffLoss = 8.1978, kgLoss = 0.1796
2025-04-09 09:32:19.660453: Training Step 31/115: batchLoss = 4.5132, diffLoss = 7.4082, kgLoss = 0.1707
2025-04-09 09:32:20.427254: Training Step 32/115: batchLoss = 4.0458, diffLoss = 6.6416, kgLoss = 0.1523
2025-04-09 09:32:21.194723: Training Step 33/115: batchLoss = 4.3086, diffLoss = 7.0742, kgLoss = 0.1601
2025-04-09 09:32:21.945538: Training Step 34/115: batchLoss = 4.5341, diffLoss = 7.4353, kgLoss = 0.1824
2025-04-09 09:32:22.695336: Training Step 35/115: batchLoss = 5.1137, diffLoss = 8.3993, kgLoss = 0.1854
2025-04-09 09:32:23.443269: Training Step 36/115: batchLoss = 4.6909, diffLoss = 7.7014, kgLoss = 0.1752
2025-04-09 09:32:24.196017: Training Step 37/115: batchLoss = 4.6146, diffLoss = 7.5738, kgLoss = 0.1757
2025-04-09 09:32:24.936580: Training Step 38/115: batchLoss = 5.2333, diffLoss = 8.5956, kgLoss = 0.1897
2025-04-09 09:32:25.651129: Training Step 39/115: batchLoss = 4.8722, diffLoss = 7.9956, kgLoss = 0.1870
2025-04-09 09:32:26.389536: Training Step 40/115: batchLoss = 4.3081, diffLoss = 7.0676, kgLoss = 0.1687
2025-04-09 09:32:27.123559: Training Step 41/115: batchLoss = 4.3173, diffLoss = 7.0855, kgLoss = 0.1650
2025-04-09 09:32:27.853937: Training Step 42/115: batchLoss = 4.5338, diffLoss = 7.4461, kgLoss = 0.1653
2025-04-09 09:32:28.577484: Training Step 43/115: batchLoss = 5.3402, diffLoss = 8.7695, kgLoss = 0.1962
2025-04-09 09:32:29.302475: Training Step 44/115: batchLoss = 4.6352, diffLoss = 7.6129, kgLoss = 0.1686
2025-04-09 09:32:30.035759: Training Step 45/115: batchLoss = 4.3219, diffLoss = 7.0857, kgLoss = 0.1760
2025-04-09 09:32:30.767115: Training Step 46/115: batchLoss = 4.5657, diffLoss = 7.4935, kgLoss = 0.1740
2025-04-09 09:32:31.500255: Training Step 47/115: batchLoss = 4.6480, diffLoss = 7.6264, kgLoss = 0.1804
2025-04-09 09:32:32.240074: Training Step 48/115: batchLoss = 4.7972, diffLoss = 7.8737, kgLoss = 0.1826
2025-04-09 09:32:32.984400: Training Step 49/115: batchLoss = 4.3808, diffLoss = 7.1883, kgLoss = 0.1695
2025-04-09 09:32:33.722046: Training Step 50/115: batchLoss = 4.7299, diffLoss = 7.7690, kgLoss = 0.1712
2025-04-09 09:32:34.458463: Training Step 51/115: batchLoss = 4.8298, diffLoss = 7.9276, kgLoss = 0.1832
2025-04-09 09:32:35.203079: Training Step 52/115: batchLoss = 4.9084, diffLoss = 8.0579, kgLoss = 0.1840
2025-04-09 09:32:35.951656: Training Step 53/115: batchLoss = 4.4033, diffLoss = 7.2225, kgLoss = 0.1746
2025-04-09 09:32:36.710699: Training Step 54/115: batchLoss = 4.5277, diffLoss = 7.4303, kgLoss = 0.1737
2025-04-09 09:32:37.456617: Training Step 55/115: batchLoss = 4.4436, diffLoss = 7.2909, kgLoss = 0.1726
2025-04-09 09:32:38.203979: Training Step 56/115: batchLoss = 4.5696, diffLoss = 7.5033, kgLoss = 0.1691
2025-04-09 09:32:38.957948: Training Step 57/115: batchLoss = 4.3788, diffLoss = 7.1872, kgLoss = 0.1663
2025-04-09 09:32:39.701103: Training Step 58/115: batchLoss = 4.4705, diffLoss = 7.3373, kgLoss = 0.1704
2025-04-09 09:32:40.446321: Training Step 59/115: batchLoss = 4.4962, diffLoss = 7.3768, kgLoss = 0.1753
2025-04-09 09:32:41.188836: Training Step 60/115: batchLoss = 4.8472, diffLoss = 7.9562, kgLoss = 0.1838
2025-04-09 09:32:41.932258: Training Step 61/115: batchLoss = 4.5711, diffLoss = 7.5042, kgLoss = 0.1715
2025-04-09 09:32:42.676189: Training Step 62/115: batchLoss = 4.4709, diffLoss = 7.3308, kgLoss = 0.1809
2025-04-09 09:32:43.402330: Training Step 63/115: batchLoss = 4.6867, diffLoss = 7.6924, kgLoss = 0.1782
2025-04-09 09:32:44.137535: Training Step 64/115: batchLoss = 4.2866, diffLoss = 7.0337, kgLoss = 0.1659
2025-04-09 09:32:44.879796: Training Step 65/115: batchLoss = 5.2720, diffLoss = 8.6611, kgLoss = 0.1882
2025-04-09 09:32:45.619206: Training Step 66/115: batchLoss = 4.9805, diffLoss = 8.1781, kgLoss = 0.1842
2025-04-09 09:32:46.381415: Training Step 67/115: batchLoss = 5.0215, diffLoss = 8.2492, kgLoss = 0.1798
2025-04-09 09:32:47.102559: Training Step 68/115: batchLoss = 4.7215, diffLoss = 7.7453, kgLoss = 0.1858
2025-04-09 09:32:47.832349: Training Step 69/115: batchLoss = 4.0384, diffLoss = 6.6061, kgLoss = 0.1869
2025-04-09 09:32:48.568500: Training Step 70/115: batchLoss = 4.0881, diffLoss = 6.7055, kgLoss = 0.1619
2025-04-09 09:32:49.305906: Training Step 71/115: batchLoss = 4.4335, diffLoss = 7.2800, kgLoss = 0.1636
2025-04-09 09:32:50.051038: Training Step 72/115: batchLoss = 4.0382, diffLoss = 6.6262, kgLoss = 0.1563
2025-04-09 09:32:50.809517: Training Step 73/115: batchLoss = 4.8522, diffLoss = 7.9674, kgLoss = 0.1794
2025-04-09 09:32:51.560335: Training Step 74/115: batchLoss = 4.1944, diffLoss = 6.8830, kgLoss = 0.1614
2025-04-09 09:32:52.313453: Training Step 75/115: batchLoss = 4.4581, diffLoss = 7.3170, kgLoss = 0.1698
2025-04-09 09:32:53.041582: Training Step 76/115: batchLoss = 4.6649, diffLoss = 7.6578, kgLoss = 0.1755
2025-04-09 09:32:53.782328: Training Step 77/115: batchLoss = 4.6828, diffLoss = 7.6872, kgLoss = 0.1763
2025-04-09 09:32:54.514489: Training Step 78/115: batchLoss = 4.7774, diffLoss = 7.8374, kgLoss = 0.1875
2025-04-09 09:32:55.263459: Training Step 79/115: batchLoss = 4.6784, diffLoss = 7.6727, kgLoss = 0.1868
2025-04-09 09:32:55.995829: Training Step 80/115: batchLoss = 4.1360, diffLoss = 6.7875, kgLoss = 0.1587
2025-04-09 09:32:56.735694: Training Step 81/115: batchLoss = 4.5666, diffLoss = 7.4941, kgLoss = 0.1752
2025-04-09 09:32:57.487473: Training Step 82/115: batchLoss = 4.8844, diffLoss = 8.0188, kgLoss = 0.1828
2025-04-09 09:32:58.222968: Training Step 83/115: batchLoss = 5.9069, diffLoss = 9.6936, kgLoss = 0.2268
2025-04-09 09:32:58.961345: Training Step 84/115: batchLoss = 4.7364, diffLoss = 7.7718, kgLoss = 0.1833
2025-04-09 09:32:59.708550: Training Step 85/115: batchLoss = 4.6389, diffLoss = 7.6100, kgLoss = 0.1823
2025-04-09 09:33:00.456013: Training Step 86/115: batchLoss = 4.4303, diffLoss = 7.2604, kgLoss = 0.1851
2025-04-09 09:33:01.195072: Training Step 87/115: batchLoss = 5.7612, diffLoss = 9.4577, kgLoss = 0.2164
2025-04-09 09:33:01.944241: Training Step 88/115: batchLoss = 4.7819, diffLoss = 7.8556, kgLoss = 0.1713
2025-04-09 09:33:02.684867: Training Step 89/115: batchLoss = 4.7939, diffLoss = 7.8705, kgLoss = 0.1790
2025-04-09 09:33:03.418048: Training Step 90/115: batchLoss = 4.0532, diffLoss = 6.6529, kgLoss = 0.1538
2025-04-09 09:33:04.151723: Training Step 91/115: batchLoss = 4.9205, diffLoss = 8.0715, kgLoss = 0.1940
2025-04-09 09:33:04.893342: Training Step 92/115: batchLoss = 5.2824, diffLoss = 8.6687, kgLoss = 0.2031
2025-04-09 09:33:05.642547: Training Step 93/115: batchLoss = 4.5814, diffLoss = 7.5203, kgLoss = 0.1731
2025-04-09 09:33:06.389791: Training Step 94/115: batchLoss = 4.6672, diffLoss = 7.6542, kgLoss = 0.1867
2025-04-09 09:33:07.141317: Training Step 95/115: batchLoss = 4.7725, diffLoss = 7.8285, kgLoss = 0.1885
2025-04-09 09:33:07.885826: Training Step 96/115: batchLoss = 4.4165, diffLoss = 7.2402, kgLoss = 0.1809
2025-04-09 09:33:08.626391: Training Step 97/115: batchLoss = 4.3311, diffLoss = 7.1019, kgLoss = 0.1749
2025-04-09 09:33:09.369482: Training Step 98/115: batchLoss = 4.6993, diffLoss = 7.7104, kgLoss = 0.1826
2025-04-09 09:33:10.111743: Training Step 99/115: batchLoss = 4.8150, diffLoss = 7.9069, kgLoss = 0.1771
2025-04-09 09:33:10.854836: Training Step 100/115: batchLoss = 4.5573, diffLoss = 7.4861, kgLoss = 0.1641
2025-04-09 09:33:11.596278: Training Step 101/115: batchLoss = 4.3875, diffLoss = 7.2010, kgLoss = 0.1674
2025-04-09 09:33:12.335883: Training Step 102/115: batchLoss = 4.1381, diffLoss = 6.7897, kgLoss = 0.1607
2025-04-09 09:33:13.085914: Training Step 103/115: batchLoss = 4.7463, diffLoss = 7.7946, kgLoss = 0.1740
2025-04-09 09:33:13.817297: Training Step 104/115: batchLoss = 4.7247, diffLoss = 7.7497, kgLoss = 0.1873
2025-04-09 09:33:14.557362: Training Step 105/115: batchLoss = 4.6586, diffLoss = 7.6381, kgLoss = 0.1893
2025-04-09 09:33:15.293590: Training Step 106/115: batchLoss = 4.9415, diffLoss = 8.1068, kgLoss = 0.1935
2025-04-09 09:33:16.008269: Training Step 107/115: batchLoss = 4.2069, diffLoss = 6.9038, kgLoss = 0.1615
2025-04-09 09:33:16.735225: Training Step 108/115: batchLoss = 4.3326, diffLoss = 7.1101, kgLoss = 0.1665
2025-04-09 09:33:17.459430: Training Step 109/115: batchLoss = 4.6327, diffLoss = 7.6055, kgLoss = 0.1734
2025-04-09 09:33:18.184962: Training Step 110/115: batchLoss = 4.0758, diffLoss = 6.6853, kgLoss = 0.1616
2025-04-09 09:33:18.909083: Training Step 111/115: batchLoss = 4.3568, diffLoss = 7.1534, kgLoss = 0.1619
2025-04-09 09:33:19.620200: Training Step 112/115: batchLoss = 4.6105, diffLoss = 7.5645, kgLoss = 0.1795
2025-04-09 09:33:20.269464: Training Step 113/115: batchLoss = 4.1667, diffLoss = 6.8367, kgLoss = 0.1617
2025-04-09 09:33:20.903546: Training Step 114/115: batchLoss = 4.4222, diffLoss = 7.2580, kgLoss = 0.1685
2025-04-09 09:33:21.024830: 
2025-04-09 09:33:21.025892: Epoch 7/1000, Train: epLoss = 1.3361, epDfLoss = 2.1928, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:33:21.788409: Steps 0/90: batch_recall = 49.53, batch_ndcg = 36.77 
2025-04-09 09:33:22.538542: Steps 1/90: batch_recall = 51.59, batch_ndcg = 37.01 
2025-04-09 09:33:23.299035: Steps 2/90: batch_recall = 41.64, batch_ndcg = 27.39 
2025-04-09 09:33:24.045221: Steps 3/90: batch_recall = 37.97, batch_ndcg = 26.55 
2025-04-09 09:33:24.799975: Steps 4/90: batch_recall = 48.11, batch_ndcg = 34.12 
2025-04-09 09:33:25.548605: Steps 5/90: batch_recall = 30.83, batch_ndcg = 22.27 
2025-04-09 09:33:26.296730: Steps 6/90: batch_recall = 39.82, batch_ndcg = 29.27 
2025-04-09 09:33:27.060534: Steps 7/90: batch_recall = 35.52, batch_ndcg = 25.48 
2025-04-09 09:33:27.815818: Steps 8/90: batch_recall = 40.57, batch_ndcg = 28.29 
2025-04-09 09:33:28.579658: Steps 9/90: batch_recall = 36.13, batch_ndcg = 26.89 
2025-04-09 09:33:29.336564: Steps 10/90: batch_recall = 34.33, batch_ndcg = 24.29 
2025-04-09 09:33:30.087286: Steps 11/90: batch_recall = 38.79, batch_ndcg = 24.54 
2025-04-09 09:33:30.823760: Steps 12/90: batch_recall = 29.98, batch_ndcg = 20.89 
2025-04-09 09:33:31.559167: Steps 13/90: batch_recall = 29.79, batch_ndcg = 20.61 
2025-04-09 09:33:32.286006: Steps 14/90: batch_recall = 31.46, batch_ndcg = 21.59 
2025-04-09 09:33:33.003746: Steps 15/90: batch_recall = 37.80, batch_ndcg = 25.56 
2025-04-09 09:33:33.716239: Steps 16/90: batch_recall = 30.34, batch_ndcg = 21.29 
2025-04-09 09:33:34.438963: Steps 17/90: batch_recall = 29.02, batch_ndcg = 19.86 
2025-04-09 09:33:35.145242: Steps 18/90: batch_recall = 33.26, batch_ndcg = 21.92 
2025-04-09 09:33:35.871916: Steps 19/90: batch_recall = 26.87, batch_ndcg = 18.28 
2025-04-09 09:33:36.598271: Steps 20/90: batch_recall = 29.62, batch_ndcg = 19.11 
2025-04-09 09:33:37.337554: Steps 21/90: batch_recall = 31.19, batch_ndcg = 21.99 
2025-04-09 09:33:38.066598: Steps 22/90: batch_recall = 34.68, batch_ndcg = 25.23 
2025-04-09 09:33:38.797644: Steps 23/90: batch_recall = 29.05, batch_ndcg = 20.19 
2025-04-09 09:33:39.530858: Steps 24/90: batch_recall = 29.32, batch_ndcg = 20.34 
2025-04-09 09:33:40.275540: Steps 25/90: batch_recall = 30.19, batch_ndcg = 18.84 
2025-04-09 09:33:41.017553: Steps 26/90: batch_recall = 28.54, batch_ndcg = 19.36 
2025-04-09 09:33:41.735356: Steps 27/90: batch_recall = 28.24, batch_ndcg = 18.44 
2025-04-09 09:33:42.480319: Steps 28/90: batch_recall = 26.25, batch_ndcg = 19.06 
2025-04-09 09:33:43.212817: Steps 29/90: batch_recall = 26.84, batch_ndcg = 16.89 
2025-04-09 09:33:43.947883: Steps 30/90: batch_recall = 24.97, batch_ndcg = 14.96 
2025-04-09 09:33:44.704276: Steps 31/90: batch_recall = 26.67, batch_ndcg = 17.16 
2025-04-09 09:33:45.431925: Steps 32/90: batch_recall = 30.95, batch_ndcg = 19.31 
2025-04-09 09:33:46.147082: Steps 33/90: batch_recall = 25.16, batch_ndcg = 15.72 
2025-04-09 09:33:46.869675: Steps 34/90: batch_recall = 28.74, batch_ndcg = 18.97 
2025-04-09 09:33:47.591640: Steps 35/90: batch_recall = 28.53, batch_ndcg = 19.68 
2025-04-09 09:33:48.319517: Steps 36/90: batch_recall = 28.65, batch_ndcg = 19.40 
2025-04-09 09:33:49.050211: Steps 37/90: batch_recall = 25.18, batch_ndcg = 16.15 
2025-04-09 09:33:49.787160: Steps 38/90: batch_recall = 25.24, batch_ndcg = 16.81 
2025-04-09 09:33:50.522723: Steps 39/90: batch_recall = 24.43, batch_ndcg = 14.78 
2025-04-09 09:33:51.244910: Steps 40/90: batch_recall = 25.56, batch_ndcg = 17.07 
2025-04-09 09:33:51.944191: Steps 41/90: batch_recall = 24.59, batch_ndcg = 16.29 
2025-04-09 09:33:52.660773: Steps 42/90: batch_recall = 29.39, batch_ndcg = 21.64 
2025-04-09 09:33:53.364257: Steps 43/90: batch_recall = 19.65, batch_ndcg = 13.80 
2025-04-09 09:33:54.094809: Steps 44/90: batch_recall = 25.40, batch_ndcg = 17.08 
2025-04-09 09:33:54.826212: Steps 45/90: batch_recall = 25.06, batch_ndcg = 14.76 
2025-04-09 09:33:55.555923: Steps 46/90: batch_recall = 24.84, batch_ndcg = 14.10 
2025-04-09 09:33:56.295481: Steps 47/90: batch_recall = 22.30, batch_ndcg = 14.28 
2025-04-09 09:33:57.030466: Steps 48/90: batch_recall = 24.76, batch_ndcg = 15.20 
2025-04-09 09:33:57.747016: Steps 49/90: batch_recall = 17.60, batch_ndcg = 12.24 
2025-04-09 09:33:58.454033: Steps 50/90: batch_recall = 27.87, batch_ndcg = 17.84 
2025-04-09 09:33:59.185881: Steps 51/90: batch_recall = 29.15, batch_ndcg = 18.27 
2025-04-09 09:33:59.920160: Steps 52/90: batch_recall = 25.98, batch_ndcg = 15.66 
2025-04-09 09:34:00.638411: Steps 53/90: batch_recall = 27.24, batch_ndcg = 16.34 
2025-04-09 09:34:01.356433: Steps 54/90: batch_recall = 21.22, batch_ndcg = 14.75 
2025-04-09 09:34:02.075677: Steps 55/90: batch_recall = 25.47, batch_ndcg = 14.93 
2025-04-09 09:34:02.793071: Steps 56/90: batch_recall = 21.41, batch_ndcg = 12.95 
2025-04-09 09:34:03.518320: Steps 57/90: batch_recall = 25.64, batch_ndcg = 15.65 
2025-04-09 09:34:04.211016: Steps 58/90: batch_recall = 30.51, batch_ndcg = 19.09 
2025-04-09 09:34:04.921045: Steps 59/90: batch_recall = 21.48, batch_ndcg = 13.29 
2025-04-09 09:34:05.622888: Steps 60/90: batch_recall = 20.70, batch_ndcg = 13.42 
2025-04-09 09:34:06.340338: Steps 61/90: batch_recall = 30.45, batch_ndcg = 19.52 
2025-04-09 09:34:07.054945: Steps 62/90: batch_recall = 28.16, batch_ndcg = 18.18 
2025-04-09 09:34:07.764240: Steps 63/90: batch_recall = 27.13, batch_ndcg = 16.16 
2025-04-09 09:34:08.470538: Steps 64/90: batch_recall = 26.18, batch_ndcg = 14.88 
2025-04-09 09:34:09.162399: Steps 65/90: batch_recall = 27.80, batch_ndcg = 17.50 
2025-04-09 09:34:09.870357: Steps 66/90: batch_recall = 25.95, batch_ndcg = 16.77 
2025-04-09 09:34:10.588627: Steps 67/90: batch_recall = 20.48, batch_ndcg = 12.28 
2025-04-09 09:34:11.315499: Steps 68/90: batch_recall = 21.71, batch_ndcg = 14.76 
2025-04-09 09:34:12.047727: Steps 69/90: batch_recall = 25.75, batch_ndcg = 16.25 
2025-04-09 09:34:12.796100: Steps 70/90: batch_recall = 26.40, batch_ndcg = 16.25 
2025-04-09 09:34:13.519236: Steps 71/90: batch_recall = 29.78, batch_ndcg = 17.71 
2025-04-09 09:34:14.246061: Steps 72/90: batch_recall = 20.29, batch_ndcg = 12.90 
2025-04-09 09:34:14.974673: Steps 73/90: batch_recall = 26.00, batch_ndcg = 16.74 
2025-04-09 09:34:15.705516: Steps 74/90: batch_recall = 26.45, batch_ndcg = 17.52 
2025-04-09 09:34:16.448678: Steps 75/90: batch_recall = 26.93, batch_ndcg = 16.55 
2025-04-09 09:34:17.177960: Steps 76/90: batch_recall = 27.10, batch_ndcg = 15.68 
2025-04-09 09:34:17.931891: Steps 77/90: batch_recall = 24.79, batch_ndcg = 15.43 
2025-04-09 09:34:18.666963: Steps 78/90: batch_recall = 21.05, batch_ndcg = 13.58 
2025-04-09 09:34:19.373279: Steps 79/90: batch_recall = 29.09, batch_ndcg = 19.92 
2025-04-09 09:34:20.104419: Steps 80/90: batch_recall = 31.66, batch_ndcg = 19.12 
2025-04-09 09:34:20.850421: Steps 81/90: batch_recall = 29.52, batch_ndcg = 18.15 
2025-04-09 09:34:21.574493: Steps 82/90: batch_recall = 25.88, batch_ndcg = 17.28 
2025-04-09 09:34:22.281793: Steps 83/90: batch_recall = 23.21, batch_ndcg = 14.36 
2025-04-09 09:34:22.987851: Steps 84/90: batch_recall = 23.16, batch_ndcg = 14.02 
2025-04-09 09:34:23.695987: Steps 85/90: batch_recall = 24.53, batch_ndcg = 15.13 
2025-04-09 09:34:24.401367: Steps 86/90: batch_recall = 32.56, batch_ndcg = 19.13 
2025-04-09 09:34:25.114635: Steps 87/90: batch_recall = 30.16, batch_ndcg = 20.16 
2025-04-09 09:34:25.831932: Steps 88/90: batch_recall = 34.14, batch_ndcg = 20.07 
2025-04-09 09:34:26.401163: Steps 89/90: batch_recall = 24.15, batch_ndcg = 13.65 
2025-04-09 09:34:26.401812: Epoch 7/1000, Test: Recall = 0.0562, NDCG = 0.0369  

2025-04-09 09:34:27.444143: Training Step 0/115: batchLoss = 4.4897, diffLoss = 7.3677, kgLoss = 0.1726
2025-04-09 09:34:28.198547: Training Step 1/115: batchLoss = 4.7330, diffLoss = 7.7700, kgLoss = 0.1775
2025-04-09 09:34:28.942836: Training Step 2/115: batchLoss = 4.1971, diffLoss = 6.8809, kgLoss = 0.1714
2025-04-09 09:34:29.689554: Training Step 3/115: batchLoss = 4.5978, diffLoss = 7.5425, kgLoss = 0.1808
2025-04-09 09:34:30.430212: Training Step 4/115: batchLoss = 4.2711, diffLoss = 7.0089, kgLoss = 0.1644
2025-04-09 09:34:31.168126: Training Step 5/115: batchLoss = 4.2147, diffLoss = 6.9178, kgLoss = 0.1602
2025-04-09 09:34:31.914515: Training Step 6/115: batchLoss = 4.3868, diffLoss = 7.1939, kgLoss = 0.1762
2025-04-09 09:34:32.663985: Training Step 7/115: batchLoss = 4.2857, diffLoss = 7.0355, kgLoss = 0.1611
2025-04-09 09:34:33.406658: Training Step 8/115: batchLoss = 4.3304, diffLoss = 7.1042, kgLoss = 0.1699
2025-04-09 09:34:34.148378: Training Step 9/115: batchLoss = 4.1500, diffLoss = 6.8113, kgLoss = 0.1580
2025-04-09 09:34:34.906253: Training Step 10/115: batchLoss = 4.3395, diffLoss = 7.1239, kgLoss = 0.1629
2025-04-09 09:34:35.649892: Training Step 11/115: batchLoss = 4.2126, diffLoss = 6.9104, kgLoss = 0.1659
2025-04-09 09:34:36.378216: Training Step 12/115: batchLoss = 5.2986, diffLoss = 8.7063, kgLoss = 0.1871
2025-04-09 09:34:37.104182: Training Step 13/115: batchLoss = 4.5569, diffLoss = 7.4681, kgLoss = 0.1901
2025-04-09 09:34:37.824762: Training Step 14/115: batchLoss = 4.7426, diffLoss = 7.7833, kgLoss = 0.1816
2025-04-09 09:34:38.553603: Training Step 15/115: batchLoss = 4.7071, diffLoss = 7.7231, kgLoss = 0.1832
2025-04-09 09:34:39.267650: Training Step 16/115: batchLoss = 5.0784, diffLoss = 8.3394, kgLoss = 0.1870
2025-04-09 09:34:39.993647: Training Step 17/115: batchLoss = 4.9647, diffLoss = 8.1500, kgLoss = 0.1866
2025-04-09 09:34:40.716282: Training Step 18/115: batchLoss = 4.4846, diffLoss = 7.3611, kgLoss = 0.1699
2025-04-09 09:34:41.457418: Training Step 19/115: batchLoss = 4.7504, diffLoss = 7.7982, kgLoss = 0.1786
2025-04-09 09:34:42.191067: Training Step 20/115: batchLoss = 4.4825, diffLoss = 7.3548, kgLoss = 0.1740
2025-04-09 09:34:42.913078: Training Step 21/115: batchLoss = 4.6067, diffLoss = 7.5645, kgLoss = 0.1702
2025-04-09 09:34:43.652462: Training Step 22/115: batchLoss = 4.5980, diffLoss = 7.5430, kgLoss = 0.1804
2025-04-09 09:34:44.394366: Training Step 23/115: batchLoss = 4.4763, diffLoss = 7.3485, kgLoss = 0.1680
2025-04-09 09:34:45.140178: Training Step 24/115: batchLoss = 4.5897, diffLoss = 7.5305, kgLoss = 0.1784
2025-04-09 09:34:45.878669: Training Step 25/115: batchLoss = 4.1859, diffLoss = 6.8529, kgLoss = 0.1852
2025-04-09 09:34:46.621471: Training Step 26/115: batchLoss = 4.9584, diffLoss = 8.1368, kgLoss = 0.1909
2025-04-09 09:34:47.369850: Training Step 27/115: batchLoss = 4.6170, diffLoss = 7.5742, kgLoss = 0.1812
2025-04-09 09:34:48.105051: Training Step 28/115: batchLoss = 4.6585, diffLoss = 7.6465, kgLoss = 0.1764
2025-04-09 09:34:48.849236: Training Step 29/115: batchLoss = 4.7742, diffLoss = 7.8396, kgLoss = 0.1762
2025-04-09 09:34:49.600520: Training Step 30/115: batchLoss = 4.5188, diffLoss = 7.4181, kgLoss = 0.1699
2025-04-09 09:34:50.330593: Training Step 31/115: batchLoss = 4.6075, diffLoss = 7.5605, kgLoss = 0.1781
2025-04-09 09:34:51.069007: Training Step 32/115: batchLoss = 4.5393, diffLoss = 7.4457, kgLoss = 0.1797
2025-04-09 09:34:51.810440: Training Step 33/115: batchLoss = 4.7125, diffLoss = 7.7360, kgLoss = 0.1772
2025-04-09 09:34:52.545210: Training Step 34/115: batchLoss = 6.3614, diffLoss = 10.4454, kgLoss = 0.2354
2025-04-09 09:34:53.293158: Training Step 35/115: batchLoss = 4.2161, diffLoss = 6.9138, kgLoss = 0.1694
2025-04-09 09:34:54.040402: Training Step 36/115: batchLoss = 4.8992, diffLoss = 8.0407, kgLoss = 0.1871
2025-04-09 09:34:54.784354: Training Step 37/115: batchLoss = 4.9191, diffLoss = 8.0691, kgLoss = 0.1942
2025-04-09 09:34:55.513900: Training Step 38/115: batchLoss = 4.9739, diffLoss = 8.1615, kgLoss = 0.1926
2025-04-09 09:34:56.250530: Training Step 39/115: batchLoss = 4.7994, diffLoss = 7.8837, kgLoss = 0.1729
2025-04-09 09:34:56.976451: Training Step 40/115: batchLoss = 5.2813, diffLoss = 8.6739, kgLoss = 0.1924
2025-04-09 09:34:57.710146: Training Step 41/115: batchLoss = 4.5026, diffLoss = 7.3943, kgLoss = 0.1650
2025-04-09 09:34:58.430035: Training Step 42/115: batchLoss = 4.5460, diffLoss = 7.4577, kgLoss = 0.1785
2025-04-09 09:34:59.174506: Training Step 43/115: batchLoss = 4.0924, diffLoss = 6.7105, kgLoss = 0.1654
2025-04-09 09:34:59.923409: Training Step 44/115: batchLoss = 4.3805, diffLoss = 7.1873, kgLoss = 0.1702
2025-04-09 09:35:00.668595: Training Step 45/115: batchLoss = 4.0717, diffLoss = 6.6719, kgLoss = 0.1714
2025-04-09 09:35:01.415946: Training Step 46/115: batchLoss = 4.6659, diffLoss = 7.6578, kgLoss = 0.1781
2025-04-09 09:35:02.162027: Training Step 47/115: batchLoss = 4.2759, diffLoss = 7.0168, kgLoss = 0.1645
2025-04-09 09:35:02.917562: Training Step 48/115: batchLoss = 5.4954, diffLoss = 9.0258, kgLoss = 0.1998
2025-04-09 09:35:03.679344: Training Step 49/115: batchLoss = 4.3539, diffLoss = 7.1412, kgLoss = 0.1729
2025-04-09 09:35:04.442787: Training Step 50/115: batchLoss = 5.2131, diffLoss = 8.5542, kgLoss = 0.2015
2025-04-09 09:35:05.188836: Training Step 51/115: batchLoss = 4.5322, diffLoss = 7.4443, kgLoss = 0.1641
2025-04-09 09:35:05.973851: Training Step 52/115: batchLoss = 4.3892, diffLoss = 7.1990, kgLoss = 0.1746
2025-04-09 09:35:06.708812: Training Step 53/115: batchLoss = 4.6188, diffLoss = 7.5706, kgLoss = 0.1911
2025-04-09 09:35:07.438380: Training Step 54/115: batchLoss = 3.9646, diffLoss = 6.5033, kgLoss = 0.1564
2025-04-09 09:35:08.177167: Training Step 55/115: batchLoss = 4.7505, diffLoss = 7.7961, kgLoss = 0.1821
2025-04-09 09:35:08.936791: Training Step 56/115: batchLoss = 3.9445, diffLoss = 6.4685, kgLoss = 0.1584
2025-04-09 09:35:09.660321: Training Step 57/115: batchLoss = 4.4940, diffLoss = 7.3743, kgLoss = 0.1736
2025-04-09 09:35:10.382249: Training Step 58/115: batchLoss = 4.4103, diffLoss = 7.2081, kgLoss = 0.2135
2025-04-09 09:35:11.109393: Training Step 59/115: batchLoss = 4.6207, diffLoss = 7.5826, kgLoss = 0.1779
2025-04-09 09:35:11.828685: Training Step 60/115: batchLoss = 5.1815, diffLoss = 8.5039, kgLoss = 0.1979
2025-04-09 09:35:12.554787: Training Step 61/115: batchLoss = 5.2267, diffLoss = 8.5758, kgLoss = 0.2031
2025-04-09 09:35:13.281283: Training Step 62/115: batchLoss = 5.3129, diffLoss = 8.7216, kgLoss = 0.1999
2025-04-09 09:35:14.023065: Training Step 63/115: batchLoss = 4.4075, diffLoss = 7.2301, kgLoss = 0.1735
2025-04-09 09:35:14.740572: Training Step 64/115: batchLoss = 4.4982, diffLoss = 7.3799, kgLoss = 0.1757
2025-04-09 09:35:15.473308: Training Step 65/115: batchLoss = 4.2746, diffLoss = 7.0155, kgLoss = 0.1633
2025-04-09 09:35:16.215356: Training Step 66/115: batchLoss = 4.2048, diffLoss = 6.8927, kgLoss = 0.1729
2025-04-09 09:35:16.975561: Training Step 67/115: batchLoss = 4.0666, diffLoss = 6.6679, kgLoss = 0.1646
2025-04-09 09:35:17.731224: Training Step 68/115: batchLoss = 4.4245, diffLoss = 7.2542, kgLoss = 0.1800
2025-04-09 09:35:18.477702: Training Step 69/115: batchLoss = 4.2000, diffLoss = 6.8866, kgLoss = 0.1701
2025-04-09 09:35:19.227880: Training Step 70/115: batchLoss = 4.5475, diffLoss = 7.4652, kgLoss = 0.1710
2025-04-09 09:35:19.974223: Training Step 71/115: batchLoss = 4.5712, diffLoss = 7.5009, kgLoss = 0.1766
2025-04-09 09:35:20.724008: Training Step 72/115: batchLoss = 4.3978, diffLoss = 7.2133, kgLoss = 0.1745
2025-04-09 09:35:21.465321: Training Step 73/115: batchLoss = 4.9405, diffLoss = 8.1101, kgLoss = 0.1862
2025-04-09 09:35:22.201106: Training Step 74/115: batchLoss = 4.6557, diffLoss = 7.6411, kgLoss = 0.1775
2025-04-09 09:35:22.946907: Training Step 75/115: batchLoss = 4.0845, diffLoss = 6.6940, kgLoss = 0.1703
2025-04-09 09:35:23.684869: Training Step 76/115: batchLoss = 4.7879, diffLoss = 7.8572, kgLoss = 0.1839
2025-04-09 09:35:24.426718: Training Step 77/115: batchLoss = 4.1056, diffLoss = 6.7366, kgLoss = 0.1591
2025-04-09 09:35:25.169590: Training Step 78/115: batchLoss = 4.2581, diffLoss = 6.9770, kgLoss = 0.1798
2025-04-09 09:35:25.900399: Training Step 79/115: batchLoss = 5.1476, diffLoss = 8.4523, kgLoss = 0.1905
2025-04-09 09:35:26.622472: Training Step 80/115: batchLoss = 4.4028, diffLoss = 7.2178, kgLoss = 0.1803
2025-04-09 09:35:27.344736: Training Step 81/115: batchLoss = 4.2651, diffLoss = 6.9914, kgLoss = 0.1757
2025-04-09 09:35:28.062769: Training Step 82/115: batchLoss = 4.0176, diffLoss = 6.5879, kgLoss = 0.1621
2025-04-09 09:35:28.780442: Training Step 83/115: batchLoss = 4.6870, diffLoss = 7.6947, kgLoss = 0.1754
2025-04-09 09:35:29.496902: Training Step 84/115: batchLoss = 4.6182, diffLoss = 7.5803, kgLoss = 0.1749
2025-04-09 09:35:30.219281: Training Step 85/115: batchLoss = 4.3336, diffLoss = 7.1153, kgLoss = 0.1611
2025-04-09 09:35:30.942366: Training Step 86/115: batchLoss = 5.0405, diffLoss = 8.2749, kgLoss = 0.1889
2025-04-09 09:35:31.652507: Training Step 87/115: batchLoss = 4.6229, diffLoss = 7.5840, kgLoss = 0.1814
2025-04-09 09:35:32.383957: Training Step 88/115: batchLoss = 4.6162, diffLoss = 7.5735, kgLoss = 0.1801
2025-04-09 09:35:33.119410: Training Step 89/115: batchLoss = 4.3009, diffLoss = 7.0595, kgLoss = 0.1630
2025-04-09 09:35:33.852557: Training Step 90/115: batchLoss = 5.7761, diffLoss = 9.4822, kgLoss = 0.2170
2025-04-09 09:35:34.578870: Training Step 91/115: batchLoss = 4.2636, diffLoss = 6.9967, kgLoss = 0.1641
2025-04-09 09:35:35.309331: Training Step 92/115: batchLoss = 4.6416, diffLoss = 7.6235, kgLoss = 0.1687
2025-04-09 09:35:36.035919: Training Step 93/115: batchLoss = 4.7765, diffLoss = 7.8385, kgLoss = 0.1836
2025-04-09 09:35:36.760651: Training Step 94/115: batchLoss = 5.2213, diffLoss = 8.5748, kgLoss = 0.1910
2025-04-09 09:35:37.608405: Training Step 95/115: batchLoss = 4.5199, diffLoss = 7.4143, kgLoss = 0.1783
2025-04-09 09:35:38.334989: Training Step 96/115: batchLoss = 4.6075, diffLoss = 7.5596, kgLoss = 0.1793
2025-04-09 09:35:39.071151: Training Step 97/115: batchLoss = 3.6772, diffLoss = 6.0269, kgLoss = 0.1526
2025-04-09 09:35:39.801603: Training Step 98/115: batchLoss = 4.8136, diffLoss = 7.9027, kgLoss = 0.1799
2025-04-09 09:35:40.532703: Training Step 99/115: batchLoss = 4.7812, diffLoss = 7.8405, kgLoss = 0.1922
2025-04-09 09:35:41.263896: Training Step 100/115: batchLoss = 4.7922, diffLoss = 7.8705, kgLoss = 0.1746
2025-04-09 09:35:41.983525: Training Step 101/115: batchLoss = 4.4731, diffLoss = 7.3451, kgLoss = 0.1652
2025-04-09 09:35:42.705158: Training Step 102/115: batchLoss = 4.3841, diffLoss = 7.1951, kgLoss = 0.1675
2025-04-09 09:35:43.405947: Training Step 103/115: batchLoss = 4.6181, diffLoss = 7.5799, kgLoss = 0.1754
2025-04-09 09:35:44.139986: Training Step 104/115: batchLoss = 4.7699, diffLoss = 7.8331, kgLoss = 0.1750
2025-04-09 09:35:44.854127: Training Step 105/115: batchLoss = 4.4767, diffLoss = 7.3433, kgLoss = 0.1768
2025-04-09 09:35:45.572997: Training Step 106/115: batchLoss = 4.4311, diffLoss = 7.2667, kgLoss = 0.1778
2025-04-09 09:35:46.286276: Training Step 107/115: batchLoss = 4.3252, diffLoss = 7.0947, kgLoss = 0.1711
2025-04-09 09:35:47.007385: Training Step 108/115: batchLoss = 5.0929, diffLoss = 8.3600, kgLoss = 0.1923
2025-04-09 09:35:47.721636: Training Step 109/115: batchLoss = 4.7400, diffLoss = 7.7803, kgLoss = 0.1796
2025-04-09 09:35:48.442712: Training Step 110/115: batchLoss = 4.3501, diffLoss = 7.1405, kgLoss = 0.1646
2025-04-09 09:35:49.161167: Training Step 111/115: batchLoss = 4.3889, diffLoss = 7.2002, kgLoss = 0.1720
2025-04-09 09:35:49.877030: Training Step 112/115: batchLoss = 4.8529, diffLoss = 7.9618, kgLoss = 0.1896
2025-04-09 09:35:50.524481: Training Step 113/115: batchLoss = 5.2131, diffLoss = 8.5507, kgLoss = 0.2068
2025-04-09 09:35:51.168756: Training Step 114/115: batchLoss = 4.6179, diffLoss = 7.5817, kgLoss = 0.1722
2025-04-09 09:35:51.299366: 
2025-04-09 09:35:51.300325: Epoch 8/1000, Train: epLoss = 1.3217, epDfLoss = 2.1688, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:35:52.062490: Steps 0/90: batch_recall = 51.40, batch_ndcg = 38.64 
2025-04-09 09:35:52.788677: Steps 1/90: batch_recall = 52.37, batch_ndcg = 38.04 
2025-04-09 09:35:53.548910: Steps 2/90: batch_recall = 44.39, batch_ndcg = 29.08 
2025-04-09 09:35:54.288063: Steps 3/90: batch_recall = 39.21, batch_ndcg = 27.55 
2025-04-09 09:35:55.030966: Steps 4/90: batch_recall = 48.64, batch_ndcg = 34.38 
2025-04-09 09:35:55.788584: Steps 5/90: batch_recall = 31.53, batch_ndcg = 22.89 
2025-04-09 09:35:56.525683: Steps 6/90: batch_recall = 43.27, batch_ndcg = 30.79 
2025-04-09 09:35:57.276949: Steps 7/90: batch_recall = 36.01, batch_ndcg = 25.94 
2025-04-09 09:35:58.032107: Steps 8/90: batch_recall = 43.17, batch_ndcg = 29.72 
2025-04-09 09:35:58.770671: Steps 9/90: batch_recall = 36.98, batch_ndcg = 27.42 
2025-04-09 09:35:59.507626: Steps 10/90: batch_recall = 33.77, batch_ndcg = 24.46 
2025-04-09 09:36:00.231788: Steps 11/90: batch_recall = 38.42, batch_ndcg = 24.95 
2025-04-09 09:36:00.959650: Steps 12/90: batch_recall = 30.70, batch_ndcg = 21.95 
2025-04-09 09:36:01.684462: Steps 13/90: batch_recall = 29.89, batch_ndcg = 21.18 
2025-04-09 09:36:02.420439: Steps 14/90: batch_recall = 32.44, batch_ndcg = 22.14 
2025-04-09 09:36:03.160020: Steps 15/90: batch_recall = 39.97, batch_ndcg = 26.03 
2025-04-09 09:36:03.893407: Steps 16/90: batch_recall = 32.30, batch_ndcg = 22.11 
2025-04-09 09:36:04.627319: Steps 17/90: batch_recall = 29.57, batch_ndcg = 20.18 
2025-04-09 09:36:05.379758: Steps 18/90: batch_recall = 33.88, batch_ndcg = 22.70 
2025-04-09 09:36:06.151758: Steps 19/90: batch_recall = 28.92, batch_ndcg = 19.79 
2025-04-09 09:36:06.928615: Steps 20/90: batch_recall = 29.72, batch_ndcg = 19.47 
2025-04-09 09:36:07.692991: Steps 21/90: batch_recall = 32.33, batch_ndcg = 23.35 
2025-04-09 09:36:08.454680: Steps 22/90: batch_recall = 35.75, batch_ndcg = 25.55 
2025-04-09 09:36:09.215699: Steps 23/90: batch_recall = 30.06, batch_ndcg = 20.83 
2025-04-09 09:36:09.967672: Steps 24/90: batch_recall = 31.41, batch_ndcg = 21.11 
2025-04-09 09:36:10.701660: Steps 25/90: batch_recall = 31.62, batch_ndcg = 20.12 
2025-04-09 09:36:11.432460: Steps 26/90: batch_recall = 29.89, batch_ndcg = 20.43 
2025-04-09 09:36:12.161263: Steps 27/90: batch_recall = 28.69, batch_ndcg = 18.92 
2025-04-09 09:36:12.900594: Steps 28/90: batch_recall = 28.09, batch_ndcg = 19.56 
2025-04-09 09:36:13.647074: Steps 29/90: batch_recall = 27.55, batch_ndcg = 17.34 
2025-04-09 09:36:14.382159: Steps 30/90: batch_recall = 27.33, batch_ndcg = 16.07 
2025-04-09 09:36:15.095358: Steps 31/90: batch_recall = 26.57, batch_ndcg = 17.67 
2025-04-09 09:36:15.822127: Steps 32/90: batch_recall = 31.45, batch_ndcg = 20.06 
2025-04-09 09:36:16.538232: Steps 33/90: batch_recall = 25.16, batch_ndcg = 16.35 
2025-04-09 09:36:17.257804: Steps 34/90: batch_recall = 31.00, batch_ndcg = 20.03 
2025-04-09 09:36:17.986373: Steps 35/90: batch_recall = 29.26, batch_ndcg = 20.04 
2025-04-09 09:36:18.711724: Steps 36/90: batch_recall = 30.14, batch_ndcg = 20.49 
2025-04-09 09:36:19.433869: Steps 37/90: batch_recall = 26.94, batch_ndcg = 17.60 
2025-04-09 09:36:20.150406: Steps 38/90: batch_recall = 26.55, batch_ndcg = 17.48 
2025-04-09 09:36:20.863608: Steps 39/90: batch_recall = 26.05, batch_ndcg = 15.77 
2025-04-09 09:36:21.583312: Steps 40/90: batch_recall = 26.73, batch_ndcg = 18.02 
2025-04-09 09:36:22.318391: Steps 41/90: batch_recall = 24.74, batch_ndcg = 16.53 
2025-04-09 09:36:23.052932: Steps 42/90: batch_recall = 30.35, batch_ndcg = 22.09 
2025-04-09 09:36:23.767761: Steps 43/90: batch_recall = 21.95, batch_ndcg = 14.79 
2025-04-09 09:36:24.504829: Steps 44/90: batch_recall = 25.00, batch_ndcg = 17.51 
2025-04-09 09:36:25.227308: Steps 45/90: batch_recall = 25.63, batch_ndcg = 15.61 
2025-04-09 09:36:25.949758: Steps 46/90: batch_recall = 25.35, batch_ndcg = 14.75 
2025-04-09 09:36:26.675780: Steps 47/90: batch_recall = 23.92, batch_ndcg = 15.17 
2025-04-09 09:36:27.398064: Steps 48/90: batch_recall = 26.05, batch_ndcg = 15.87 
2025-04-09 09:36:28.116513: Steps 49/90: batch_recall = 18.13, batch_ndcg = 12.61 
2025-04-09 09:36:28.828178: Steps 50/90: batch_recall = 28.33, batch_ndcg = 18.65 
2025-04-09 09:36:29.572140: Steps 51/90: batch_recall = 31.32, batch_ndcg = 19.07 
2025-04-09 09:36:30.299451: Steps 52/90: batch_recall = 26.70, batch_ndcg = 16.19 
2025-04-09 09:36:30.996979: Steps 53/90: batch_recall = 29.49, batch_ndcg = 17.25 
2025-04-09 09:36:31.695402: Steps 54/90: batch_recall = 24.47, batch_ndcg = 16.04 
2025-04-09 09:36:32.395196: Steps 55/90: batch_recall = 27.40, batch_ndcg = 16.05 
2025-04-09 09:36:33.108295: Steps 56/90: batch_recall = 24.12, batch_ndcg = 14.13 
2025-04-09 09:36:33.842795: Steps 57/90: batch_recall = 26.71, batch_ndcg = 16.05 
2025-04-09 09:36:34.550040: Steps 58/90: batch_recall = 31.99, batch_ndcg = 19.78 
2025-04-09 09:36:35.258729: Steps 59/90: batch_recall = 22.40, batch_ndcg = 13.93 
2025-04-09 09:36:35.962428: Steps 60/90: batch_recall = 21.61, batch_ndcg = 14.18 
2025-04-09 09:36:36.663571: Steps 61/90: batch_recall = 31.99, batch_ndcg = 20.79 
2025-04-09 09:36:37.374901: Steps 62/90: batch_recall = 28.42, batch_ndcg = 18.53 
2025-04-09 09:36:38.088436: Steps 63/90: batch_recall = 28.95, batch_ndcg = 17.23 
2025-04-09 09:36:38.805350: Steps 64/90: batch_recall = 27.35, batch_ndcg = 15.79 
2025-04-09 09:36:39.512838: Steps 65/90: batch_recall = 29.10, batch_ndcg = 18.13 
2025-04-09 09:36:40.227581: Steps 66/90: batch_recall = 26.30, batch_ndcg = 17.09 
2025-04-09 09:36:40.940331: Steps 67/90: batch_recall = 23.39, batch_ndcg = 13.84 
2025-04-09 09:36:41.644833: Steps 68/90: batch_recall = 22.63, batch_ndcg = 15.50 
2025-04-09 09:36:42.365929: Steps 69/90: batch_recall = 27.71, batch_ndcg = 17.41 
2025-04-09 09:36:43.111326: Steps 70/90: batch_recall = 27.96, batch_ndcg = 17.37 
2025-04-09 09:36:43.836013: Steps 71/90: batch_recall = 30.14, batch_ndcg = 18.14 
2025-04-09 09:36:44.557005: Steps 72/90: batch_recall = 21.01, batch_ndcg = 13.64 
2025-04-09 09:36:45.288043: Steps 73/90: batch_recall = 27.27, batch_ndcg = 17.26 
2025-04-09 09:36:45.989497: Steps 74/90: batch_recall = 28.27, batch_ndcg = 18.95 
2025-04-09 09:36:46.731611: Steps 75/90: batch_recall = 28.01, batch_ndcg = 16.96 
2025-04-09 09:36:47.456730: Steps 76/90: batch_recall = 29.18, batch_ndcg = 16.82 
2025-04-09 09:36:48.188996: Steps 77/90: batch_recall = 27.19, batch_ndcg = 16.47 
2025-04-09 09:36:48.975746: Steps 78/90: batch_recall = 22.68, batch_ndcg = 13.94 
2025-04-09 09:36:49.708196: Steps 79/90: batch_recall = 30.21, batch_ndcg = 20.88 
2025-04-09 09:36:50.393334: Steps 80/90: batch_recall = 35.17, batch_ndcg = 20.78 
2025-04-09 09:36:51.105205: Steps 81/90: batch_recall = 30.64, batch_ndcg = 19.04 
2025-04-09 09:36:51.813076: Steps 82/90: batch_recall = 27.99, batch_ndcg = 18.00 
2025-04-09 09:36:52.536704: Steps 83/90: batch_recall = 25.06, batch_ndcg = 15.27 
2025-04-09 09:36:53.253127: Steps 84/90: batch_recall = 24.95, batch_ndcg = 15.32 
2025-04-09 09:36:53.984764: Steps 85/90: batch_recall = 26.13, batch_ndcg = 15.47 
2025-04-09 09:36:54.695789: Steps 86/90: batch_recall = 34.64, batch_ndcg = 19.96 
2025-04-09 09:36:55.401285: Steps 87/90: batch_recall = 30.89, batch_ndcg = 20.67 
2025-04-09 09:36:56.144116: Steps 88/90: batch_recall = 35.99, batch_ndcg = 20.99 
2025-04-09 09:36:56.693578: Steps 89/90: batch_recall = 26.73, batch_ndcg = 15.02 
2025-04-09 09:36:56.694385: Epoch 8/1000, Test: Recall = 0.0588, NDCG = 0.0385  

2025-04-09 09:36:57.738757: Training Step 0/115: batchLoss = 4.6075, diffLoss = 7.5611, kgLoss = 0.1771
2025-04-09 09:36:58.478113: Training Step 1/115: batchLoss = 4.7888, diffLoss = 7.8632, kgLoss = 0.1772
2025-04-09 09:36:59.214000: Training Step 2/115: batchLoss = 4.0075, diffLoss = 6.5766, kgLoss = 0.1539
2025-04-09 09:36:59.950914: Training Step 3/115: batchLoss = 4.4871, diffLoss = 7.3567, kgLoss = 0.1828
2025-04-09 09:37:00.696409: Training Step 4/115: batchLoss = 5.3015, diffLoss = 8.6970, kgLoss = 0.2082
2025-04-09 09:37:01.429650: Training Step 5/115: batchLoss = 4.2120, diffLoss = 6.9109, kgLoss = 0.1636
2025-04-09 09:37:02.168917: Training Step 6/115: batchLoss = 5.5051, diffLoss = 9.0394, kgLoss = 0.2036
2025-04-09 09:37:02.906501: Training Step 7/115: batchLoss = 4.4842, diffLoss = 7.3547, kgLoss = 0.1784
2025-04-09 09:37:03.647505: Training Step 8/115: batchLoss = 4.7102, diffLoss = 7.7315, kgLoss = 0.1782
2025-04-09 09:37:04.379842: Training Step 9/115: batchLoss = 4.6201, diffLoss = 7.5816, kgLoss = 0.1778
2025-04-09 09:37:05.110475: Training Step 10/115: batchLoss = 4.6468, diffLoss = 7.6327, kgLoss = 0.1679
2025-04-09 09:37:05.824020: Training Step 11/115: batchLoss = 4.1077, diffLoss = 6.7413, kgLoss = 0.1573
2025-04-09 09:37:06.547661: Training Step 12/115: batchLoss = 4.5714, diffLoss = 7.4974, kgLoss = 0.1823
2025-04-09 09:37:07.262001: Training Step 13/115: batchLoss = 4.3460, diffLoss = 7.1319, kgLoss = 0.1671
2025-04-09 09:37:07.993658: Training Step 14/115: batchLoss = 4.5028, diffLoss = 7.3835, kgLoss = 0.1818
2025-04-09 09:37:08.720924: Training Step 15/115: batchLoss = 4.4467, diffLoss = 7.2922, kgLoss = 0.1785
2025-04-09 09:37:09.443794: Training Step 16/115: batchLoss = 5.0996, diffLoss = 8.3645, kgLoss = 0.2023
2025-04-09 09:37:10.180630: Training Step 17/115: batchLoss = 4.5538, diffLoss = 7.4651, kgLoss = 0.1868
2025-04-09 09:37:11.006803: Training Step 18/115: batchLoss = 4.2527, diffLoss = 6.9812, kgLoss = 0.1600
2025-04-09 09:37:11.749504: Training Step 19/115: batchLoss = 4.9753, diffLoss = 8.1617, kgLoss = 0.1958
2025-04-09 09:37:12.493477: Training Step 20/115: batchLoss = 4.0923, diffLoss = 6.7112, kgLoss = 0.1641
2025-04-09 09:37:13.236208: Training Step 21/115: batchLoss = 3.9963, diffLoss = 6.5441, kgLoss = 0.1746
2025-04-09 09:37:13.978246: Training Step 22/115: batchLoss = 4.4339, diffLoss = 7.2761, kgLoss = 0.1707
2025-04-09 09:37:14.713842: Training Step 23/115: batchLoss = 4.4661, diffLoss = 7.3266, kgLoss = 0.1754
2025-04-09 09:37:15.459522: Training Step 24/115: batchLoss = 4.2352, diffLoss = 6.9523, kgLoss = 0.1596
2025-04-09 09:37:16.194019: Training Step 25/115: batchLoss = 4.4807, diffLoss = 7.3531, kgLoss = 0.1722
2025-04-09 09:37:16.932170: Training Step 26/115: batchLoss = 4.4908, diffLoss = 7.3706, kgLoss = 0.1711
2025-04-09 09:37:17.679077: Training Step 27/115: batchLoss = 4.6435, diffLoss = 7.6230, kgLoss = 0.1742
2025-04-09 09:37:18.429457: Training Step 28/115: batchLoss = 4.5296, diffLoss = 7.4384, kgLoss = 0.1666
2025-04-09 09:37:19.172502: Training Step 29/115: batchLoss = 4.6985, diffLoss = 7.7112, kgLoss = 0.1795
2025-04-09 09:37:19.920878: Training Step 30/115: batchLoss = 4.7954, diffLoss = 7.8714, kgLoss = 0.1814
2025-04-09 09:37:20.656117: Training Step 31/115: batchLoss = 4.6696, diffLoss = 7.6685, kgLoss = 0.1712
2025-04-09 09:37:21.483732: Training Step 32/115: batchLoss = 3.9378, diffLoss = 6.4571, kgLoss = 0.1589
2025-04-09 09:37:22.209625: Training Step 33/115: batchLoss = 4.0571, diffLoss = 6.6443, kgLoss = 0.1763
2025-04-09 09:37:22.941509: Training Step 34/115: batchLoss = 4.7163, diffLoss = 7.7417, kgLoss = 0.1782
2025-04-09 09:37:23.654155: Training Step 35/115: batchLoss = 4.4019, diffLoss = 7.2222, kgLoss = 0.1715
2025-04-09 09:37:24.376949: Training Step 36/115: batchLoss = 5.3783, diffLoss = 8.8301, kgLoss = 0.2005
2025-04-09 09:37:25.099654: Training Step 37/115: batchLoss = 5.6592, diffLoss = 9.2897, kgLoss = 0.2133
2025-04-09 09:37:25.825420: Training Step 38/115: batchLoss = 4.1868, diffLoss = 6.8707, kgLoss = 0.1610
2025-04-09 09:37:26.551978: Training Step 39/115: batchLoss = 5.4360, diffLoss = 8.9216, kgLoss = 0.2076
2025-04-09 09:37:27.273928: Training Step 40/115: batchLoss = 4.4410, diffLoss = 7.2644, kgLoss = 0.2059
2025-04-09 09:37:28.007030: Training Step 41/115: batchLoss = 4.3835, diffLoss = 7.1947, kgLoss = 0.1666
2025-04-09 09:37:28.747398: Training Step 42/115: batchLoss = 5.1831, diffLoss = 8.5044, kgLoss = 0.2012
2025-04-09 09:37:29.484014: Training Step 43/115: batchLoss = 4.4477, diffLoss = 7.3020, kgLoss = 0.1663
2025-04-09 09:37:30.223274: Training Step 44/115: batchLoss = 3.9606, diffLoss = 6.4925, kgLoss = 0.1628
2025-04-09 09:37:30.953717: Training Step 45/115: batchLoss = 4.5329, diffLoss = 7.4313, kgLoss = 0.1853
2025-04-09 09:37:31.697935: Training Step 46/115: batchLoss = 4.5007, diffLoss = 7.3845, kgLoss = 0.1749
2025-04-09 09:37:32.433003: Training Step 47/115: batchLoss = 4.2511, diffLoss = 6.9708, kgLoss = 0.1714
2025-04-09 09:37:33.171027: Training Step 48/115: batchLoss = 4.9763, diffLoss = 8.1703, kgLoss = 0.1854
2025-04-09 09:37:33.911410: Training Step 49/115: batchLoss = 4.5648, diffLoss = 7.4927, kgLoss = 0.1729
2025-04-09 09:37:34.649286: Training Step 50/115: batchLoss = 5.0446, diffLoss = 8.2833, kgLoss = 0.1865
2025-04-09 09:37:35.391706: Training Step 51/115: batchLoss = 4.8665, diffLoss = 7.9906, kgLoss = 0.1804
2025-04-09 09:37:36.146703: Training Step 52/115: batchLoss = 4.7728, diffLoss = 7.8305, kgLoss = 0.1862
2025-04-09 09:37:36.904989: Training Step 53/115: batchLoss = 4.4773, diffLoss = 7.3496, kgLoss = 0.1687
2025-04-09 09:37:37.648554: Training Step 54/115: batchLoss = 4.6156, diffLoss = 7.5730, kgLoss = 0.1795
2025-04-09 09:37:38.388946: Training Step 55/115: batchLoss = 4.6678, diffLoss = 7.6606, kgLoss = 0.1787
2025-04-09 09:37:39.116623: Training Step 56/115: batchLoss = 4.6032, diffLoss = 7.5513, kgLoss = 0.1811
2025-04-09 09:37:39.846166: Training Step 57/115: batchLoss = 4.8548, diffLoss = 7.9706, kgLoss = 0.1810
2025-04-09 09:37:40.572101: Training Step 58/115: batchLoss = 4.3445, diffLoss = 7.1232, kgLoss = 0.1763
2025-04-09 09:37:41.293820: Training Step 59/115: batchLoss = 4.6860, diffLoss = 7.6888, kgLoss = 0.1817
2025-04-09 09:37:42.022064: Training Step 60/115: batchLoss = 4.8834, diffLoss = 8.0142, kgLoss = 0.1871
2025-04-09 09:37:42.754277: Training Step 61/115: batchLoss = 4.3135, diffLoss = 7.0719, kgLoss = 0.1758
2025-04-09 09:37:43.477523: Training Step 62/115: batchLoss = 3.9370, diffLoss = 6.4549, kgLoss = 0.1601
2025-04-09 09:37:44.217745: Training Step 63/115: batchLoss = 5.2361, diffLoss = 8.5944, kgLoss = 0.1987
2025-04-09 09:37:44.983073: Training Step 64/115: batchLoss = 4.2881, diffLoss = 7.0323, kgLoss = 0.1717
2025-04-09 09:37:45.741517: Training Step 65/115: batchLoss = 4.3832, diffLoss = 7.1966, kgLoss = 0.1630
2025-04-09 09:37:46.493612: Training Step 66/115: batchLoss = 4.1075, diffLoss = 6.7369, kgLoss = 0.1632
2025-04-09 09:37:47.232757: Training Step 67/115: batchLoss = 4.0981, diffLoss = 6.7227, kgLoss = 0.1611
2025-04-09 09:37:47.978822: Training Step 68/115: batchLoss = 4.4049, diffLoss = 7.2291, kgLoss = 0.1686
2025-04-09 09:37:48.714282: Training Step 69/115: batchLoss = 4.2010, diffLoss = 6.8890, kgLoss = 0.1691
2025-04-09 09:37:49.463761: Training Step 70/115: batchLoss = 4.9787, diffLoss = 8.1717, kgLoss = 0.1890
2025-04-09 09:37:50.199150: Training Step 71/115: batchLoss = 4.1913, diffLoss = 6.8810, kgLoss = 0.1566
2025-04-09 09:37:50.933224: Training Step 72/115: batchLoss = 4.5803, diffLoss = 7.5156, kgLoss = 0.1772
2025-04-09 09:37:51.669559: Training Step 73/115: batchLoss = 4.8467, diffLoss = 7.9542, kgLoss = 0.1855
2025-04-09 09:37:52.407135: Training Step 74/115: batchLoss = 4.5592, diffLoss = 7.4832, kgLoss = 0.1730
2025-04-09 09:37:53.153422: Training Step 75/115: batchLoss = 4.4202, diffLoss = 7.2537, kgLoss = 0.1700
2025-04-09 09:37:53.874234: Training Step 76/115: batchLoss = 4.4280, diffLoss = 7.2714, kgLoss = 0.1630
2025-04-09 09:37:54.609817: Training Step 77/115: batchLoss = 4.0478, diffLoss = 6.6407, kgLoss = 0.1586
2025-04-09 09:37:55.355706: Training Step 78/115: batchLoss = 4.6403, diffLoss = 7.6179, kgLoss = 0.1739
2025-04-09 09:37:56.078972: Training Step 79/115: batchLoss = 4.5465, diffLoss = 7.4605, kgLoss = 0.1756
2025-04-09 09:37:56.800739: Training Step 80/115: batchLoss = 4.4206, diffLoss = 7.2547, kgLoss = 0.1694
2025-04-09 09:37:57.518560: Training Step 81/115: batchLoss = 4.8650, diffLoss = 7.9761, kgLoss = 0.1984
2025-04-09 09:37:58.248520: Training Step 82/115: batchLoss = 3.9753, diffLoss = 6.5151, kgLoss = 0.1655
2025-04-09 09:37:58.972186: Training Step 83/115: batchLoss = 5.2085, diffLoss = 8.5479, kgLoss = 0.1994
2025-04-09 09:37:59.705292: Training Step 84/115: batchLoss = 4.7145, diffLoss = 7.7370, kgLoss = 0.1807
2025-04-09 09:38:00.431862: Training Step 85/115: batchLoss = 4.5229, diffLoss = 7.4149, kgLoss = 0.1848
2025-04-09 09:38:01.169976: Training Step 86/115: batchLoss = 4.7648, diffLoss = 7.8258, kgLoss = 0.1733
2025-04-09 09:38:01.913669: Training Step 87/115: batchLoss = 4.1800, diffLoss = 6.8573, kgLoss = 0.1640
2025-04-09 09:38:02.652069: Training Step 88/115: batchLoss = 4.7375, diffLoss = 7.7761, kgLoss = 0.1796
2025-04-09 09:38:03.417431: Training Step 89/115: batchLoss = 4.6550, diffLoss = 7.6418, kgLoss = 0.1748
2025-04-09 09:38:04.169898: Training Step 90/115: batchLoss = 4.5060, diffLoss = 7.3977, kgLoss = 0.1685
2025-04-09 09:38:04.907206: Training Step 91/115: batchLoss = 4.9838, diffLoss = 8.1767, kgLoss = 0.1946
2025-04-09 09:38:05.641771: Training Step 92/115: batchLoss = 4.8804, diffLoss = 8.0086, kgLoss = 0.1882
2025-04-09 09:38:06.376536: Training Step 93/115: batchLoss = 4.5560, diffLoss = 7.4763, kgLoss = 0.1754
2025-04-09 09:38:07.106325: Training Step 94/115: batchLoss = 4.2863, diffLoss = 7.0276, kgLoss = 0.1743
2025-04-09 09:38:07.832295: Training Step 95/115: batchLoss = 4.5657, diffLoss = 7.4935, kgLoss = 0.1740
2025-04-09 09:38:08.573354: Training Step 96/115: batchLoss = 4.8675, diffLoss = 7.9928, kgLoss = 0.1796
2025-04-09 09:38:09.311335: Training Step 97/115: batchLoss = 4.3287, diffLoss = 7.1082, kgLoss = 0.1594
2025-04-09 09:38:10.038179: Training Step 98/115: batchLoss = 5.3008, diffLoss = 8.7040, kgLoss = 0.1960
2025-04-09 09:38:10.766766: Training Step 99/115: batchLoss = 4.3152, diffLoss = 7.0726, kgLoss = 0.1793
2025-04-09 09:38:11.488645: Training Step 100/115: batchLoss = 4.6583, diffLoss = 7.6450, kgLoss = 0.1782
2025-04-09 09:38:12.215970: Training Step 101/115: batchLoss = 4.4675, diffLoss = 7.3265, kgLoss = 0.1789
2025-04-09 09:38:12.946873: Training Step 102/115: batchLoss = 4.7571, diffLoss = 7.8060, kgLoss = 0.1837
2025-04-09 09:38:13.666184: Training Step 103/115: batchLoss = 4.1734, diffLoss = 6.8433, kgLoss = 0.1687
2025-04-09 09:38:14.379878: Training Step 104/115: batchLoss = 4.9841, diffLoss = 8.1720, kgLoss = 0.2022
2025-04-09 09:38:15.102689: Training Step 105/115: batchLoss = 4.5530, diffLoss = 7.4688, kgLoss = 0.1793
2025-04-09 09:38:15.826129: Training Step 106/115: batchLoss = 4.8005, diffLoss = 7.8754, kgLoss = 0.1881
2025-04-09 09:38:16.552420: Training Step 107/115: batchLoss = 4.8328, diffLoss = 7.9309, kgLoss = 0.1857
2025-04-09 09:38:17.291093: Training Step 108/115: batchLoss = 4.5226, diffLoss = 7.4156, kgLoss = 0.1831
2025-04-09 09:38:18.040843: Training Step 109/115: batchLoss = 4.9213, diffLoss = 8.0839, kgLoss = 0.1775
2025-04-09 09:38:18.769381: Training Step 110/115: batchLoss = 4.9433, diffLoss = 8.1139, kgLoss = 0.1875
2025-04-09 09:38:19.510456: Training Step 111/115: batchLoss = 4.7386, diffLoss = 7.7737, kgLoss = 0.1859
2025-04-09 09:38:20.239654: Training Step 112/115: batchLoss = 4.8734, diffLoss = 7.9990, kgLoss = 0.1849
2025-04-09 09:38:20.893812: Training Step 113/115: batchLoss = 4.3515, diffLoss = 7.1328, kgLoss = 0.1795
2025-04-09 09:38:21.511104: Training Step 114/115: batchLoss = 4.1975, diffLoss = 6.8837, kgLoss = 0.1683
2025-04-09 09:38:21.629755: 
2025-04-09 09:38:21.630785: Epoch 9/1000, Train: epLoss = 1.3180, epDfLoss = 2.1626, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:38:22.395877: Steps 0/90: batch_recall = 52.22, batch_ndcg = 40.06 
2025-04-09 09:38:23.151889: Steps 1/90: batch_recall = 53.26, batch_ndcg = 38.57 
2025-04-09 09:38:23.906599: Steps 2/90: batch_recall = 45.00, batch_ndcg = 30.35 
2025-04-09 09:38:24.657290: Steps 3/90: batch_recall = 40.28, batch_ndcg = 28.06 
2025-04-09 09:38:25.407925: Steps 4/90: batch_recall = 48.29, batch_ndcg = 34.46 
2025-04-09 09:38:26.158957: Steps 5/90: batch_recall = 31.80, batch_ndcg = 22.97 
2025-04-09 09:38:26.886621: Steps 6/90: batch_recall = 44.59, batch_ndcg = 32.01 
2025-04-09 09:38:27.617998: Steps 7/90: batch_recall = 38.17, batch_ndcg = 27.27 
2025-04-09 09:38:28.354356: Steps 8/90: batch_recall = 43.66, batch_ndcg = 30.40 
2025-04-09 09:38:29.095839: Steps 9/90: batch_recall = 38.49, batch_ndcg = 28.71 
2025-04-09 09:38:29.835448: Steps 10/90: batch_recall = 35.78, batch_ndcg = 26.13 
2025-04-09 09:38:30.573765: Steps 11/90: batch_recall = 39.67, batch_ndcg = 25.29 
2025-04-09 09:38:31.297181: Steps 12/90: batch_recall = 32.53, batch_ndcg = 22.76 
2025-04-09 09:38:32.029946: Steps 13/90: batch_recall = 30.98, batch_ndcg = 21.91 
2025-04-09 09:38:32.765681: Steps 14/90: batch_recall = 33.44, batch_ndcg = 22.80 
2025-04-09 09:38:33.507972: Steps 15/90: batch_recall = 39.08, batch_ndcg = 25.79 
2025-04-09 09:38:34.252559: Steps 16/90: batch_recall = 32.08, batch_ndcg = 22.33 
2025-04-09 09:38:35.010863: Steps 17/90: batch_recall = 30.50, batch_ndcg = 20.44 
2025-04-09 09:38:35.745451: Steps 18/90: batch_recall = 34.31, batch_ndcg = 23.17 
2025-04-09 09:38:36.501605: Steps 19/90: batch_recall = 28.29, batch_ndcg = 19.47 
2025-04-09 09:38:37.228920: Steps 20/90: batch_recall = 30.98, batch_ndcg = 20.24 
2025-04-09 09:38:37.970042: Steps 21/90: batch_recall = 32.91, batch_ndcg = 24.02 
2025-04-09 09:38:38.708088: Steps 22/90: batch_recall = 36.75, batch_ndcg = 26.54 
2025-04-09 09:38:39.445661: Steps 23/90: batch_recall = 32.19, batch_ndcg = 21.89 
2025-04-09 09:38:40.170922: Steps 24/90: batch_recall = 32.65, batch_ndcg = 21.87 
2025-04-09 09:38:40.897040: Steps 25/90: batch_recall = 32.37, batch_ndcg = 20.68 
2025-04-09 09:38:41.623444: Steps 26/90: batch_recall = 31.63, batch_ndcg = 21.60 
2025-04-09 09:38:42.344105: Steps 27/90: batch_recall = 29.25, batch_ndcg = 19.14 
2025-04-09 09:38:43.074771: Steps 28/90: batch_recall = 30.01, batch_ndcg = 20.47 
2025-04-09 09:38:43.797454: Steps 29/90: batch_recall = 28.16, batch_ndcg = 18.01 
2025-04-09 09:38:44.529736: Steps 30/90: batch_recall = 27.48, batch_ndcg = 16.22 
2025-04-09 09:38:45.265958: Steps 31/90: batch_recall = 27.09, batch_ndcg = 18.27 
2025-04-09 09:38:46.014447: Steps 32/90: batch_recall = 31.79, batch_ndcg = 20.80 
2025-04-09 09:38:46.725730: Steps 33/90: batch_recall = 26.88, batch_ndcg = 17.12 
2025-04-09 09:38:47.445113: Steps 34/90: batch_recall = 31.42, batch_ndcg = 20.73 
2025-04-09 09:38:48.160010: Steps 35/90: batch_recall = 30.12, batch_ndcg = 20.97 
2025-04-09 09:38:48.890814: Steps 36/90: batch_recall = 30.71, batch_ndcg = 20.58 
2025-04-09 09:38:49.625894: Steps 37/90: batch_recall = 26.48, batch_ndcg = 17.90 
2025-04-09 09:38:50.353677: Steps 38/90: batch_recall = 27.09, batch_ndcg = 18.25 
2025-04-09 09:38:51.082987: Steps 39/90: batch_recall = 27.22, batch_ndcg = 16.56 
2025-04-09 09:38:51.832335: Steps 40/90: batch_recall = 27.18, batch_ndcg = 18.34 
2025-04-09 09:38:52.564853: Steps 41/90: batch_recall = 26.44, batch_ndcg = 17.81 
2025-04-09 09:38:53.294690: Steps 42/90: batch_recall = 31.87, batch_ndcg = 22.92 
2025-04-09 09:38:54.041433: Steps 43/90: batch_recall = 24.77, batch_ndcg = 16.20 
2025-04-09 09:38:54.788614: Steps 44/90: batch_recall = 25.30, batch_ndcg = 17.76 
2025-04-09 09:38:55.513090: Steps 45/90: batch_recall = 26.51, batch_ndcg = 16.17 
2025-04-09 09:38:56.241745: Steps 46/90: batch_recall = 26.48, batch_ndcg = 15.38 
2025-04-09 09:38:56.961366: Steps 47/90: batch_recall = 24.65, batch_ndcg = 15.70 
2025-04-09 09:38:57.687084: Steps 48/90: batch_recall = 27.39, batch_ndcg = 16.77 
2025-04-09 09:38:58.418017: Steps 49/90: batch_recall = 19.57, batch_ndcg = 13.24 
2025-04-09 09:38:59.123438: Steps 50/90: batch_recall = 28.74, batch_ndcg = 18.98 
2025-04-09 09:38:59.832785: Steps 51/90: batch_recall = 31.91, batch_ndcg = 19.66 
2025-04-09 09:39:00.545225: Steps 52/90: batch_recall = 26.17, batch_ndcg = 16.63 
2025-04-09 09:39:01.255036: Steps 53/90: batch_recall = 29.98, batch_ndcg = 17.88 
2025-04-09 09:39:01.961819: Steps 54/90: batch_recall = 24.98, batch_ndcg = 16.46 
2025-04-09 09:39:02.668086: Steps 55/90: batch_recall = 30.08, batch_ndcg = 17.60 
2025-04-09 09:39:03.377421: Steps 56/90: batch_recall = 23.29, batch_ndcg = 14.40 
2025-04-09 09:39:04.090772: Steps 57/90: batch_recall = 29.03, batch_ndcg = 17.33 
2025-04-09 09:39:04.805725: Steps 58/90: batch_recall = 32.71, batch_ndcg = 19.99 
2025-04-09 09:39:05.542110: Steps 59/90: batch_recall = 22.32, batch_ndcg = 14.13 
2025-04-09 09:39:06.292956: Steps 60/90: batch_recall = 23.35, batch_ndcg = 14.61 
2025-04-09 09:39:07.019758: Steps 61/90: batch_recall = 34.15, batch_ndcg = 22.32 
2025-04-09 09:39:07.739894: Steps 62/90: batch_recall = 30.03, batch_ndcg = 19.51 
2025-04-09 09:39:08.474246: Steps 63/90: batch_recall = 31.06, batch_ndcg = 18.58 
2025-04-09 09:39:09.200990: Steps 64/90: batch_recall = 27.76, batch_ndcg = 16.17 
2025-04-09 09:39:09.917721: Steps 65/90: batch_recall = 28.98, batch_ndcg = 18.52 
2025-04-09 09:39:10.651214: Steps 66/90: batch_recall = 28.44, batch_ndcg = 17.95 
2025-04-09 09:39:11.390068: Steps 67/90: batch_recall = 23.79, batch_ndcg = 14.32 
2025-04-09 09:39:12.118193: Steps 68/90: batch_recall = 25.81, batch_ndcg = 16.49 
2025-04-09 09:39:12.850831: Steps 69/90: batch_recall = 28.08, batch_ndcg = 17.43 
2025-04-09 09:39:13.602304: Steps 70/90: batch_recall = 29.04, batch_ndcg = 17.99 
2025-04-09 09:39:14.326813: Steps 71/90: batch_recall = 29.15, batch_ndcg = 18.06 
2025-04-09 09:39:15.040842: Steps 72/90: batch_recall = 20.57, batch_ndcg = 13.80 
2025-04-09 09:39:15.747406: Steps 73/90: batch_recall = 27.26, batch_ndcg = 17.55 
2025-04-09 09:39:16.459887: Steps 74/90: batch_recall = 30.40, batch_ndcg = 20.10 
2025-04-09 09:39:17.160925: Steps 75/90: batch_recall = 28.96, batch_ndcg = 17.75 
2025-04-09 09:39:17.868015: Steps 76/90: batch_recall = 30.25, batch_ndcg = 17.46 
2025-04-09 09:39:18.607221: Steps 77/90: batch_recall = 30.23, batch_ndcg = 17.74 
2025-04-09 09:39:19.308552: Steps 78/90: batch_recall = 25.32, batch_ndcg = 14.89 
2025-04-09 09:39:20.035106: Steps 79/90: batch_recall = 29.59, batch_ndcg = 20.74 
2025-04-09 09:39:20.775128: Steps 80/90: batch_recall = 34.08, batch_ndcg = 21.06 
2025-04-09 09:39:21.516425: Steps 81/90: batch_recall = 32.76, batch_ndcg = 20.25 
2025-04-09 09:39:22.256393: Steps 82/90: batch_recall = 29.02, batch_ndcg = 18.81 
2025-04-09 09:39:22.978973: Steps 83/90: batch_recall = 25.63, batch_ndcg = 16.22 
2025-04-09 09:39:23.699048: Steps 84/90: batch_recall = 26.01, batch_ndcg = 15.79 
2025-04-09 09:39:24.426112: Steps 85/90: batch_recall = 27.67, batch_ndcg = 16.12 
2025-04-09 09:39:25.164579: Steps 86/90: batch_recall = 36.33, batch_ndcg = 20.92 
2025-04-09 09:39:25.911484: Steps 87/90: batch_recall = 31.31, batch_ndcg = 21.41 
2025-04-09 09:39:26.658922: Steps 88/90: batch_recall = 37.83, batch_ndcg = 21.75 
2025-04-09 09:39:27.222548: Steps 89/90: batch_recall = 29.68, batch_ndcg = 16.17 
2025-04-09 09:39:27.223394: Epoch 9/1000, Test: Recall = 0.0607, NDCG = 0.0398  

2025-04-09 09:39:28.261939: Training Step 0/115: batchLoss = 4.7224, diffLoss = 7.7536, kgLoss = 0.1756
2025-04-09 09:39:29.010278: Training Step 1/115: batchLoss = 4.5091, diffLoss = 7.4023, kgLoss = 0.1694
2025-04-09 09:39:29.751746: Training Step 2/115: batchLoss = 4.8599, diffLoss = 7.9743, kgLoss = 0.1883
2025-04-09 09:39:30.486902: Training Step 3/115: batchLoss = 4.0632, diffLoss = 6.6585, kgLoss = 0.1702
2025-04-09 09:39:31.234180: Training Step 4/115: batchLoss = 4.9981, diffLoss = 8.2023, kgLoss = 0.1917
2025-04-09 09:39:31.987818: Training Step 5/115: batchLoss = 5.0204, diffLoss = 8.2415, kgLoss = 0.1887
2025-04-09 09:39:32.737973: Training Step 6/115: batchLoss = 4.6066, diffLoss = 7.5653, kgLoss = 0.1685
2025-04-09 09:39:33.497847: Training Step 7/115: batchLoss = 4.2387, diffLoss = 6.9567, kgLoss = 0.1618
2025-04-09 09:39:34.237169: Training Step 8/115: batchLoss = 5.0400, diffLoss = 8.2622, kgLoss = 0.2068
2025-04-09 09:39:34.982398: Training Step 9/115: batchLoss = 4.5597, diffLoss = 7.4860, kgLoss = 0.1703
2025-04-09 09:39:35.714981: Training Step 10/115: batchLoss = 4.6299, diffLoss = 7.5964, kgLoss = 0.1801
2025-04-09 09:39:36.466348: Training Step 11/115: batchLoss = 4.2926, diffLoss = 7.0341, kgLoss = 0.1805
2025-04-09 09:39:37.197851: Training Step 12/115: batchLoss = 4.3207, diffLoss = 7.0923, kgLoss = 0.1632
2025-04-09 09:39:37.941587: Training Step 13/115: batchLoss = 4.6269, diffLoss = 7.5910, kgLoss = 0.1807
2025-04-09 09:39:38.678656: Training Step 14/115: batchLoss = 4.7151, diffLoss = 7.7408, kgLoss = 0.1765
2025-04-09 09:39:39.425441: Training Step 15/115: batchLoss = 4.2170, diffLoss = 6.9169, kgLoss = 0.1671
2025-04-09 09:39:40.179473: Training Step 16/115: batchLoss = 4.6184, diffLoss = 7.5805, kgLoss = 0.1752
2025-04-09 09:39:40.938132: Training Step 17/115: batchLoss = 4.2125, diffLoss = 6.9182, kgLoss = 0.1540
2025-04-09 09:39:41.690650: Training Step 18/115: batchLoss = 5.0392, diffLoss = 8.2709, kgLoss = 0.1917
2025-04-09 09:39:42.456179: Training Step 19/115: batchLoss = 4.7415, diffLoss = 7.7831, kgLoss = 0.1789
2025-04-09 09:39:43.218380: Training Step 20/115: batchLoss = 4.9025, diffLoss = 8.0543, kgLoss = 0.1749
2025-04-09 09:39:43.979039: Training Step 21/115: batchLoss = 4.1230, diffLoss = 6.7602, kgLoss = 0.1672
2025-04-09 09:39:44.738256: Training Step 22/115: batchLoss = 4.4124, diffLoss = 7.2346, kgLoss = 0.1793
2025-04-09 09:39:45.498681: Training Step 23/115: batchLoss = 5.2354, diffLoss = 8.5987, kgLoss = 0.1906
2025-04-09 09:39:46.259685: Training Step 24/115: batchLoss = 4.2841, diffLoss = 7.0248, kgLoss = 0.1730
2025-04-09 09:39:47.011698: Training Step 25/115: batchLoss = 4.6661, diffLoss = 7.6598, kgLoss = 0.1756
2025-04-09 09:39:47.762889: Training Step 26/115: batchLoss = 4.8370, diffLoss = 7.9377, kgLoss = 0.1858
2025-04-09 09:39:48.526551: Training Step 27/115: batchLoss = 4.4897, diffLoss = 7.3631, kgLoss = 0.1797
2025-04-09 09:39:49.270963: Training Step 28/115: batchLoss = 4.4984, diffLoss = 7.3869, kgLoss = 0.1656
2025-04-09 09:39:50.019680: Training Step 29/115: batchLoss = 4.9855, diffLoss = 8.1765, kgLoss = 0.1990
2025-04-09 09:39:50.754711: Training Step 30/115: batchLoss = 4.0559, diffLoss = 6.6548, kgLoss = 0.1576
2025-04-09 09:39:51.485539: Training Step 31/115: batchLoss = 4.7110, diffLoss = 7.7287, kgLoss = 0.1843
2025-04-09 09:39:52.226904: Training Step 32/115: batchLoss = 4.3822, diffLoss = 7.1943, kgLoss = 0.1641
2025-04-09 09:39:52.958272: Training Step 33/115: batchLoss = 3.8447, diffLoss = 6.3082, kgLoss = 0.1494
2025-04-09 09:39:53.704921: Training Step 34/115: batchLoss = 4.1916, diffLoss = 6.8772, kgLoss = 0.1633
2025-04-09 09:39:54.468496: Training Step 35/115: batchLoss = 4.4237, diffLoss = 7.2577, kgLoss = 0.1727
2025-04-09 09:39:55.242472: Training Step 36/115: batchLoss = 4.0003, diffLoss = 6.5605, kgLoss = 0.1601
2025-04-09 09:39:56.029039: Training Step 37/115: batchLoss = 5.1092, diffLoss = 8.3905, kgLoss = 0.1872
2025-04-09 09:39:56.824188: Training Step 38/115: batchLoss = 4.8650, diffLoss = 7.9844, kgLoss = 0.1859
2025-04-09 09:39:57.629965: Training Step 39/115: batchLoss = 4.0412, diffLoss = 6.6245, kgLoss = 0.1662
2025-04-09 09:39:58.431463: Training Step 40/115: batchLoss = 4.4428, diffLoss = 7.2984, kgLoss = 0.1593
2025-04-09 09:39:59.205871: Training Step 41/115: batchLoss = 5.4087, diffLoss = 8.8818, kgLoss = 0.1989
2025-04-09 09:39:59.992929: Training Step 42/115: batchLoss = 4.4898, diffLoss = 7.3638, kgLoss = 0.1788
2025-04-09 09:40:00.788758: Training Step 43/115: batchLoss = 4.9226, diffLoss = 8.0798, kgLoss = 0.1868
2025-04-09 09:40:01.531799: Training Step 44/115: batchLoss = 4.4141, diffLoss = 7.2414, kgLoss = 0.1731
2025-04-09 09:40:02.272982: Training Step 45/115: batchLoss = 4.0242, diffLoss = 6.5979, kgLoss = 0.1635
2025-04-09 09:40:03.015969: Training Step 46/115: batchLoss = 4.9668, diffLoss = 8.1395, kgLoss = 0.2078
2025-04-09 09:40:03.771104: Training Step 47/115: batchLoss = 4.5469, diffLoss = 7.4611, kgLoss = 0.1757
2025-04-09 09:40:04.510390: Training Step 48/115: batchLoss = 5.2825, diffLoss = 8.6708, kgLoss = 0.2000
2025-04-09 09:40:05.259307: Training Step 49/115: batchLoss = 3.8542, diffLoss = 6.3164, kgLoss = 0.1609
2025-04-09 09:40:06.000974: Training Step 50/115: batchLoss = 4.6273, diffLoss = 7.5907, kgLoss = 0.1823
2025-04-09 09:40:06.743189: Training Step 51/115: batchLoss = 5.2376, diffLoss = 8.5998, kgLoss = 0.1942
2025-04-09 09:40:07.490346: Training Step 52/115: batchLoss = 4.6231, diffLoss = 7.5842, kgLoss = 0.1814
2025-04-09 09:40:08.240157: Training Step 53/115: batchLoss = 5.0454, diffLoss = 8.2837, kgLoss = 0.1881
2025-04-09 09:40:08.976018: Training Step 54/115: batchLoss = 4.2179, diffLoss = 6.9196, kgLoss = 0.1655
2025-04-09 09:40:09.711189: Training Step 55/115: batchLoss = 4.7062, diffLoss = 7.7233, kgLoss = 0.1806
2025-04-09 09:40:10.444962: Training Step 56/115: batchLoss = 4.0339, diffLoss = 6.6109, kgLoss = 0.1685
2025-04-09 09:40:11.176384: Training Step 57/115: batchLoss = 4.2929, diffLoss = 7.0419, kgLoss = 0.1694
2025-04-09 09:40:11.922110: Training Step 58/115: batchLoss = 5.0496, diffLoss = 8.2938, kgLoss = 0.1832
2025-04-09 09:40:12.653492: Training Step 59/115: batchLoss = 4.1944, diffLoss = 6.8775, kgLoss = 0.1699
2025-04-09 09:40:13.404350: Training Step 60/115: batchLoss = 4.2849, diffLoss = 7.0237, kgLoss = 0.1766
2025-04-09 09:40:14.158400: Training Step 61/115: batchLoss = 4.6826, diffLoss = 7.6833, kgLoss = 0.1816
2025-04-09 09:40:14.935930: Training Step 62/115: batchLoss = 4.8887, diffLoss = 8.0171, kgLoss = 0.1962
2025-04-09 09:40:15.687089: Training Step 63/115: batchLoss = 4.4578, diffLoss = 7.3045, kgLoss = 0.1879
2025-04-09 09:40:16.479045: Training Step 64/115: batchLoss = 4.6286, diffLoss = 7.5998, kgLoss = 0.1719
2025-04-09 09:40:17.225092: Training Step 65/115: batchLoss = 4.0671, diffLoss = 6.6703, kgLoss = 0.1623
2025-04-09 09:40:17.956751: Training Step 66/115: batchLoss = 4.9778, diffLoss = 8.1763, kgLoss = 0.1801
2025-04-09 09:40:18.702772: Training Step 67/115: batchLoss = 4.9045, diffLoss = 8.0551, kgLoss = 0.1785
2025-04-09 09:40:19.437877: Training Step 68/115: batchLoss = 4.2211, diffLoss = 6.9240, kgLoss = 0.1668
2025-04-09 09:40:20.170425: Training Step 69/115: batchLoss = 4.7290, diffLoss = 7.7686, kgLoss = 0.1695
2025-04-09 09:40:20.905044: Training Step 70/115: batchLoss = 4.9703, diffLoss = 8.1490, kgLoss = 0.2021
2025-04-09 09:40:21.631745: Training Step 71/115: batchLoss = 4.5180, diffLoss = 7.4166, kgLoss = 0.1701
2025-04-09 09:40:22.362416: Training Step 72/115: batchLoss = 4.6327, diffLoss = 7.6020, kgLoss = 0.1788
2025-04-09 09:40:23.113515: Training Step 73/115: batchLoss = 4.7094, diffLoss = 7.7297, kgLoss = 0.1790
2025-04-09 09:40:23.844186: Training Step 74/115: batchLoss = 4.9927, diffLoss = 8.1988, kgLoss = 0.1837
2025-04-09 09:40:24.582345: Training Step 75/115: batchLoss = 4.8451, diffLoss = 7.9536, kgLoss = 0.1825
2025-04-09 09:40:25.316831: Training Step 76/115: batchLoss = 4.7685, diffLoss = 7.8275, kgLoss = 0.1801
2025-04-09 09:40:26.038720: Training Step 77/115: batchLoss = 4.3070, diffLoss = 7.0594, kgLoss = 0.1784
2025-04-09 09:40:26.759011: Training Step 78/115: batchLoss = 4.8523, diffLoss = 7.9636, kgLoss = 0.1852
2025-04-09 09:40:27.485488: Training Step 79/115: batchLoss = 5.4134, diffLoss = 8.8786, kgLoss = 0.2156
2025-04-09 09:40:28.203426: Training Step 80/115: batchLoss = 4.8683, diffLoss = 7.9919, kgLoss = 0.1829
2025-04-09 09:40:28.918486: Training Step 81/115: batchLoss = 4.3632, diffLoss = 7.1595, kgLoss = 0.1687
2025-04-09 09:40:29.633967: Training Step 82/115: batchLoss = 4.2045, diffLoss = 6.8991, kgLoss = 0.1627
2025-04-09 09:40:30.355377: Training Step 83/115: batchLoss = 5.1610, diffLoss = 8.4670, kgLoss = 0.2021
2025-04-09 09:40:31.083672: Training Step 84/115: batchLoss = 4.3661, diffLoss = 7.1693, kgLoss = 0.1613
2025-04-09 09:40:31.800699: Training Step 85/115: batchLoss = 4.7888, diffLoss = 7.8615, kgLoss = 0.1798
2025-04-09 09:40:32.514026: Training Step 86/115: batchLoss = 4.6385, diffLoss = 7.6128, kgLoss = 0.1770
2025-04-09 09:40:33.227894: Training Step 87/115: batchLoss = 3.9038, diffLoss = 6.4007, kgLoss = 0.1583
2025-04-09 09:40:33.943007: Training Step 88/115: batchLoss = 4.5087, diffLoss = 7.4017, kgLoss = 0.1692
2025-04-09 09:40:34.673280: Training Step 89/115: batchLoss = 4.7994, diffLoss = 7.8749, kgLoss = 0.1863
2025-04-09 09:40:35.389629: Training Step 90/115: batchLoss = 4.4448, diffLoss = 7.2868, kgLoss = 0.1817
2025-04-09 09:40:36.103969: Training Step 91/115: batchLoss = 4.6263, diffLoss = 7.5902, kgLoss = 0.1805
2025-04-09 09:40:36.821011: Training Step 92/115: batchLoss = 4.4226, diffLoss = 7.2575, kgLoss = 0.1703
2025-04-09 09:40:37.540913: Training Step 93/115: batchLoss = 4.8096, diffLoss = 7.8928, kgLoss = 0.1847
2025-04-09 09:40:38.259214: Training Step 94/115: batchLoss = 4.6983, diffLoss = 7.7078, kgLoss = 0.1841
2025-04-09 09:40:38.978105: Training Step 95/115: batchLoss = 4.0614, diffLoss = 6.6654, kgLoss = 0.1554
2025-04-09 09:40:39.699099: Training Step 96/115: batchLoss = 4.1841, diffLoss = 6.8531, kgLoss = 0.1805
2025-04-09 09:40:40.423970: Training Step 97/115: batchLoss = 4.6192, diffLoss = 7.5734, kgLoss = 0.1877
2025-04-09 09:40:41.160940: Training Step 98/115: batchLoss = 4.4323, diffLoss = 7.2711, kgLoss = 0.1741
2025-04-09 09:40:41.885399: Training Step 99/115: batchLoss = 4.4744, diffLoss = 7.3382, kgLoss = 0.1786
2025-04-09 09:40:42.609464: Training Step 100/115: batchLoss = 4.8898, diffLoss = 8.0158, kgLoss = 0.2009
2025-04-09 09:40:43.327613: Training Step 101/115: batchLoss = 4.3879, diffLoss = 7.1917, kgLoss = 0.1822
2025-04-09 09:40:44.049880: Training Step 102/115: batchLoss = 4.3898, diffLoss = 7.2068, kgLoss = 0.1642
2025-04-09 09:40:44.774554: Training Step 103/115: batchLoss = 4.6634, diffLoss = 7.6572, kgLoss = 0.1726
2025-04-09 09:40:45.529264: Training Step 104/115: batchLoss = 4.4392, diffLoss = 7.2809, kgLoss = 0.1766
2025-04-09 09:40:46.266807: Training Step 105/115: batchLoss = 4.4193, diffLoss = 7.2486, kgLoss = 0.1753
2025-04-09 09:40:47.000288: Training Step 106/115: batchLoss = 5.0856, diffLoss = 8.3495, kgLoss = 0.1896
2025-04-09 09:40:47.730954: Training Step 107/115: batchLoss = 4.2442, diffLoss = 6.9632, kgLoss = 0.1658
2025-04-09 09:40:48.468273: Training Step 108/115: batchLoss = 4.6647, diffLoss = 7.6562, kgLoss = 0.1775
2025-04-09 09:40:49.206158: Training Step 109/115: batchLoss = 5.1227, diffLoss = 8.4098, kgLoss = 0.1921
2025-04-09 09:40:49.949091: Training Step 110/115: batchLoss = 4.0476, diffLoss = 6.6310, kgLoss = 0.1725
2025-04-09 09:40:50.692552: Training Step 111/115: batchLoss = 4.9503, diffLoss = 8.1292, kgLoss = 0.1820
2025-04-09 09:40:51.426326: Training Step 112/115: batchLoss = 4.5456, diffLoss = 7.4611, kgLoss = 0.1724
2025-04-09 09:40:52.072990: Training Step 113/115: batchLoss = 5.0175, diffLoss = 8.2337, kgLoss = 0.1933
2025-04-09 09:40:52.715247: Training Step 114/115: batchLoss = 4.5129, diffLoss = 7.4051, kgLoss = 0.1745
2025-04-09 09:40:52.844636: 
2025-04-09 09:40:52.845809: Epoch 10/1000, Train: epLoss = 1.3202, epDfLoss = 2.1662, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:40:53.593831: Steps 0/90: batch_recall = 53.86, batch_ndcg = 41.51 
2025-04-09 09:40:54.323926: Steps 1/90: batch_recall = 54.74, batch_ndcg = 39.31 
2025-04-09 09:40:55.057099: Steps 2/90: batch_recall = 43.81, batch_ndcg = 30.42 
2025-04-09 09:40:55.790906: Steps 3/90: batch_recall = 40.85, batch_ndcg = 28.58 
2025-04-09 09:40:56.517621: Steps 4/90: batch_recall = 50.60, batch_ndcg = 34.51 
2025-04-09 09:40:57.246821: Steps 5/90: batch_recall = 32.65, batch_ndcg = 23.28 
2025-04-09 09:40:57.977331: Steps 6/90: batch_recall = 45.27, batch_ndcg = 32.74 
2025-04-09 09:40:58.739465: Steps 7/90: batch_recall = 39.58, batch_ndcg = 28.33 
2025-04-09 09:40:59.481900: Steps 8/90: batch_recall = 44.57, batch_ndcg = 30.85 
2025-04-09 09:41:00.236840: Steps 9/90: batch_recall = 38.44, batch_ndcg = 29.03 
2025-04-09 09:41:00.990007: Steps 10/90: batch_recall = 36.13, batch_ndcg = 26.48 
2025-04-09 09:41:01.727251: Steps 11/90: batch_recall = 41.28, batch_ndcg = 26.26 
2025-04-09 09:41:02.459925: Steps 12/90: batch_recall = 34.23, batch_ndcg = 23.45 
2025-04-09 09:41:03.215237: Steps 13/90: batch_recall = 30.01, batch_ndcg = 21.56 
2025-04-09 09:41:03.958068: Steps 14/90: batch_recall = 34.47, batch_ndcg = 23.52 
2025-04-09 09:41:04.706670: Steps 15/90: batch_recall = 42.11, batch_ndcg = 27.23 
2025-04-09 09:41:05.453927: Steps 16/90: batch_recall = 32.76, batch_ndcg = 22.77 
2025-04-09 09:41:06.182976: Steps 17/90: batch_recall = 31.48, batch_ndcg = 20.97 
2025-04-09 09:41:06.912522: Steps 18/90: batch_recall = 35.34, batch_ndcg = 23.49 
2025-04-09 09:41:07.662309: Steps 19/90: batch_recall = 29.19, batch_ndcg = 20.01 
2025-04-09 09:41:08.393024: Steps 20/90: batch_recall = 31.93, batch_ndcg = 20.87 
2025-04-09 09:41:09.149612: Steps 21/90: batch_recall = 33.20, batch_ndcg = 24.54 
2025-04-09 09:41:09.890587: Steps 22/90: batch_recall = 36.64, batch_ndcg = 26.91 
2025-04-09 09:41:10.638854: Steps 23/90: batch_recall = 31.46, batch_ndcg = 21.66 
2025-04-09 09:41:11.361141: Steps 24/90: batch_recall = 33.83, batch_ndcg = 22.73 
2025-04-09 09:41:12.079277: Steps 25/90: batch_recall = 32.96, batch_ndcg = 21.23 
2025-04-09 09:41:12.815717: Steps 26/90: batch_recall = 31.23, batch_ndcg = 21.99 
2025-04-09 09:41:13.522456: Steps 27/90: batch_recall = 29.70, batch_ndcg = 20.04 
2025-04-09 09:41:14.244193: Steps 28/90: batch_recall = 29.90, batch_ndcg = 20.59 
2025-04-09 09:41:14.962624: Steps 29/90: batch_recall = 28.68, batch_ndcg = 18.53 
2025-04-09 09:41:15.698202: Steps 30/90: batch_recall = 26.29, batch_ndcg = 16.01 
2025-04-09 09:41:16.418490: Steps 31/90: batch_recall = 26.29, batch_ndcg = 18.74 
2025-04-09 09:41:17.142118: Steps 32/90: batch_recall = 32.90, batch_ndcg = 21.12 
2025-04-09 09:41:17.864614: Steps 33/90: batch_recall = 27.38, batch_ndcg = 17.55 
2025-04-09 09:41:18.582484: Steps 34/90: batch_recall = 32.55, batch_ndcg = 21.34 
2025-04-09 09:41:19.304008: Steps 35/90: batch_recall = 30.27, batch_ndcg = 21.25 
2025-04-09 09:41:20.039384: Steps 36/90: batch_recall = 31.61, batch_ndcg = 21.12 
2025-04-09 09:41:20.766280: Steps 37/90: batch_recall = 27.11, batch_ndcg = 18.33 
2025-04-09 09:41:21.480404: Steps 38/90: batch_recall = 27.33, batch_ndcg = 18.46 
2025-04-09 09:41:22.201188: Steps 39/90: batch_recall = 26.66, batch_ndcg = 16.12 
2025-04-09 09:41:22.921946: Steps 40/90: batch_recall = 28.73, batch_ndcg = 18.97 
2025-04-09 09:41:23.648466: Steps 41/90: batch_recall = 26.51, batch_ndcg = 18.46 
2025-04-09 09:41:24.394237: Steps 42/90: batch_recall = 31.26, batch_ndcg = 22.80 
2025-04-09 09:41:25.130511: Steps 43/90: batch_recall = 24.52, batch_ndcg = 16.68 
2025-04-09 09:41:25.860725: Steps 44/90: batch_recall = 26.28, batch_ndcg = 18.10 
2025-04-09 09:41:26.566296: Steps 45/90: batch_recall = 28.20, batch_ndcg = 16.95 
2025-04-09 09:41:27.284710: Steps 46/90: batch_recall = 27.96, batch_ndcg = 16.34 
2025-04-09 09:41:27.990511: Steps 47/90: batch_recall = 24.68, batch_ndcg = 15.90 
2025-04-09 09:41:28.688050: Steps 48/90: batch_recall = 27.95, batch_ndcg = 17.36 
2025-04-09 09:41:29.408283: Steps 49/90: batch_recall = 21.27, batch_ndcg = 14.36 
2025-04-09 09:41:30.126288: Steps 50/90: batch_recall = 30.18, batch_ndcg = 19.38 
2025-04-09 09:41:30.859553: Steps 51/90: batch_recall = 32.93, batch_ndcg = 20.82 
2025-04-09 09:41:31.575188: Steps 52/90: batch_recall = 28.24, batch_ndcg = 18.13 
2025-04-09 09:41:32.295300: Steps 53/90: batch_recall = 30.74, batch_ndcg = 18.39 
2025-04-09 09:41:33.006571: Steps 54/90: batch_recall = 24.76, batch_ndcg = 16.47 
2025-04-09 09:41:33.726565: Steps 55/90: batch_recall = 31.04, batch_ndcg = 18.23 
2025-04-09 09:41:34.458690: Steps 56/90: batch_recall = 24.94, batch_ndcg = 14.99 
2025-04-09 09:41:35.169400: Steps 57/90: batch_recall = 28.74, batch_ndcg = 17.74 
2025-04-09 09:41:35.885162: Steps 58/90: batch_recall = 33.87, batch_ndcg = 20.66 
2025-04-09 09:41:36.595493: Steps 59/90: batch_recall = 22.08, batch_ndcg = 14.08 
2025-04-09 09:41:37.304102: Steps 60/90: batch_recall = 23.39, batch_ndcg = 14.64 
2025-04-09 09:41:38.016760: Steps 61/90: batch_recall = 36.12, batch_ndcg = 23.21 
2025-04-09 09:41:38.721251: Steps 62/90: batch_recall = 31.29, batch_ndcg = 20.75 
2025-04-09 09:41:39.443631: Steps 63/90: batch_recall = 31.38, batch_ndcg = 19.19 
2025-04-09 09:41:40.166192: Steps 64/90: batch_recall = 30.92, batch_ndcg = 17.44 
2025-04-09 09:41:40.892850: Steps 65/90: batch_recall = 29.77, batch_ndcg = 18.79 
2025-04-09 09:41:41.607068: Steps 66/90: batch_recall = 28.25, batch_ndcg = 18.13 
2025-04-09 09:41:42.311544: Steps 67/90: batch_recall = 26.16, batch_ndcg = 15.35 
2025-04-09 09:41:43.012281: Steps 68/90: batch_recall = 26.55, batch_ndcg = 17.10 
2025-04-09 09:41:43.724391: Steps 69/90: batch_recall = 30.35, batch_ndcg = 18.71 
2025-04-09 09:41:44.427454: Steps 70/90: batch_recall = 30.02, batch_ndcg = 18.60 
2025-04-09 09:41:45.136396: Steps 71/90: batch_recall = 30.50, batch_ndcg = 18.33 
2025-04-09 09:41:45.847763: Steps 72/90: batch_recall = 23.36, batch_ndcg = 14.99 
2025-04-09 09:41:46.548928: Steps 73/90: batch_recall = 27.46, batch_ndcg = 17.39 
2025-04-09 09:41:47.244335: Steps 74/90: batch_recall = 30.52, batch_ndcg = 20.36 
2025-04-09 09:41:47.955466: Steps 75/90: batch_recall = 31.23, batch_ndcg = 18.74 
2025-04-09 09:41:48.647395: Steps 76/90: batch_recall = 31.69, batch_ndcg = 18.45 
2025-04-09 09:41:49.351740: Steps 77/90: batch_recall = 30.36, batch_ndcg = 17.80 
2025-04-09 09:41:50.050274: Steps 78/90: batch_recall = 25.75, batch_ndcg = 15.37 
2025-04-09 09:41:50.763477: Steps 79/90: batch_recall = 30.65, batch_ndcg = 21.26 
2025-04-09 09:41:51.467838: Steps 80/90: batch_recall = 35.52, batch_ndcg = 21.95 
2025-04-09 09:41:52.203426: Steps 81/90: batch_recall = 34.73, batch_ndcg = 21.36 
2025-04-09 09:41:52.944817: Steps 82/90: batch_recall = 31.05, batch_ndcg = 19.67 
2025-04-09 09:41:53.660838: Steps 83/90: batch_recall = 27.39, batch_ndcg = 17.13 
2025-04-09 09:41:54.374651: Steps 84/90: batch_recall = 27.44, batch_ndcg = 16.73 
2025-04-09 09:41:55.083608: Steps 85/90: batch_recall = 27.33, batch_ndcg = 16.23 
2025-04-09 09:41:55.815225: Steps 86/90: batch_recall = 38.16, batch_ndcg = 22.06 
2025-04-09 09:41:56.522387: Steps 87/90: batch_recall = 33.73, batch_ndcg = 22.84 
2025-04-09 09:41:57.235484: Steps 88/90: batch_recall = 37.97, batch_ndcg = 22.29 
2025-04-09 09:41:57.793142: Steps 89/90: batch_recall = 31.97, batch_ndcg = 17.16 
2025-04-09 09:41:57.793924: Epoch 10/1000, Test: Recall = 0.0624, NDCG = 0.0409  

2025-04-09 09:41:58.833664: Training Step 0/115: batchLoss = 3.6897, diffLoss = 6.0543, kgLoss = 0.1427
2025-04-09 09:41:59.588248: Training Step 1/115: batchLoss = 4.3244, diffLoss = 7.0956, kgLoss = 0.1676
2025-04-09 09:42:00.332084: Training Step 2/115: batchLoss = 5.0298, diffLoss = 8.2520, kgLoss = 0.1966
2025-04-09 09:42:01.075223: Training Step 3/115: batchLoss = 4.4064, diffLoss = 7.2333, kgLoss = 0.1659
2025-04-09 09:42:01.807238: Training Step 4/115: batchLoss = 4.1907, diffLoss = 6.8658, kgLoss = 0.1780
2025-04-09 09:42:02.544148: Training Step 5/115: batchLoss = 4.0320, diffLoss = 6.6092, kgLoss = 0.1663
2025-04-09 09:42:03.283130: Training Step 6/115: batchLoss = 4.3145, diffLoss = 7.0746, kgLoss = 0.1744
2025-04-09 09:42:04.026607: Training Step 7/115: batchLoss = 4.2113, diffLoss = 6.9119, kgLoss = 0.1605
2025-04-09 09:42:04.768869: Training Step 8/115: batchLoss = 4.6010, diffLoss = 7.5520, kgLoss = 0.1745
2025-04-09 09:42:05.498600: Training Step 9/115: batchLoss = 4.0918, diffLoss = 6.7126, kgLoss = 0.1606
2025-04-09 09:42:06.243329: Training Step 10/115: batchLoss = 3.7547, diffLoss = 6.1522, kgLoss = 0.1585
2025-04-09 09:42:06.984925: Training Step 11/115: batchLoss = 4.4354, diffLoss = 7.2783, kgLoss = 0.1711
2025-04-09 09:42:07.720296: Training Step 12/115: batchLoss = 4.8859, diffLoss = 8.0212, kgLoss = 0.1830
2025-04-09 09:42:08.455232: Training Step 13/115: batchLoss = 4.2664, diffLoss = 6.9962, kgLoss = 0.1717
2025-04-09 09:42:09.197997: Training Step 14/115: batchLoss = 3.9754, diffLoss = 6.5166, kgLoss = 0.1636
2025-04-09 09:42:09.942720: Training Step 15/115: batchLoss = 4.9757, diffLoss = 8.1644, kgLoss = 0.1928
2025-04-09 09:42:10.680895: Training Step 16/115: batchLoss = 4.9569, diffLoss = 8.1346, kgLoss = 0.1904
2025-04-09 09:42:11.418814: Training Step 17/115: batchLoss = 4.1754, diffLoss = 6.8499, kgLoss = 0.1637
2025-04-09 09:42:12.154356: Training Step 18/115: batchLoss = 4.4194, diffLoss = 7.2538, kgLoss = 0.1677
2025-04-09 09:42:12.879473: Training Step 19/115: batchLoss = 4.9174, diffLoss = 8.0631, kgLoss = 0.1987
2025-04-09 09:42:13.606140: Training Step 20/115: batchLoss = 4.3695, diffLoss = 7.1626, kgLoss = 0.1799
2025-04-09 09:42:14.333914: Training Step 21/115: batchLoss = 4.4745, diffLoss = 7.3392, kgLoss = 0.1775
2025-04-09 09:42:15.058482: Training Step 22/115: batchLoss = 4.4936, diffLoss = 7.3763, kgLoss = 0.1695
2025-04-09 09:42:15.780421: Training Step 23/115: batchLoss = 4.5586, diffLoss = 7.4838, kgLoss = 0.1709
2025-04-09 09:42:16.519378: Training Step 24/115: batchLoss = 4.7895, diffLoss = 7.8647, kgLoss = 0.1767
2025-04-09 09:42:17.255193: Training Step 25/115: batchLoss = 4.4569, diffLoss = 7.3146, kgLoss = 0.1703
2025-04-09 09:42:17.981245: Training Step 26/115: batchLoss = 4.7521, diffLoss = 7.8013, kgLoss = 0.1784
2025-04-09 09:42:18.702777: Training Step 27/115: batchLoss = 4.9692, diffLoss = 8.1485, kgLoss = 0.2001
2025-04-09 09:42:19.436570: Training Step 28/115: batchLoss = 4.5990, diffLoss = 7.5434, kgLoss = 0.1823
2025-04-09 09:42:20.168185: Training Step 29/115: batchLoss = 4.3766, diffLoss = 7.1823, kgLoss = 0.1680
2025-04-09 09:42:20.903127: Training Step 30/115: batchLoss = 4.2295, diffLoss = 6.9380, kgLoss = 0.1668
2025-04-09 09:42:21.628970: Training Step 31/115: batchLoss = 4.6660, diffLoss = 7.6626, kgLoss = 0.1713
2025-04-09 09:42:22.361855: Training Step 32/115: batchLoss = 5.0195, diffLoss = 8.2368, kgLoss = 0.1934
2025-04-09 09:42:23.100825: Training Step 33/115: batchLoss = 4.8100, diffLoss = 7.8886, kgLoss = 0.1920
2025-04-09 09:42:23.845718: Training Step 34/115: batchLoss = 4.4207, diffLoss = 7.2526, kgLoss = 0.1730
2025-04-09 09:42:24.581647: Training Step 35/115: batchLoss = 4.5610, diffLoss = 7.4868, kgLoss = 0.1724
2025-04-09 09:42:25.326281: Training Step 36/115: batchLoss = 4.3488, diffLoss = 7.1264, kgLoss = 0.1824
2025-04-09 09:42:26.066581: Training Step 37/115: batchLoss = 5.8221, diffLoss = 9.5533, kgLoss = 0.2253
2025-04-09 09:42:26.812518: Training Step 38/115: batchLoss = 5.1988, diffLoss = 8.5375, kgLoss = 0.1906
2025-04-09 09:42:27.566537: Training Step 39/115: batchLoss = 4.4449, diffLoss = 7.2941, kgLoss = 0.1711
2025-04-09 09:42:28.309773: Training Step 40/115: batchLoss = 3.8271, diffLoss = 6.2739, kgLoss = 0.1571
2025-04-09 09:42:29.038125: Training Step 41/115: batchLoss = 5.1905, diffLoss = 8.5202, kgLoss = 0.1959
2025-04-09 09:42:29.767589: Training Step 42/115: batchLoss = 4.1627, diffLoss = 6.8270, kgLoss = 0.1662
2025-04-09 09:42:30.508637: Training Step 43/115: batchLoss = 5.4523, diffLoss = 8.9463, kgLoss = 0.2113
2025-04-09 09:42:31.248505: Training Step 44/115: batchLoss = 4.1483, diffLoss = 6.8089, kgLoss = 0.1575
2025-04-09 09:42:31.971867: Training Step 45/115: batchLoss = 4.7139, diffLoss = 7.7371, kgLoss = 0.1790
2025-04-09 09:42:32.709535: Training Step 46/115: batchLoss = 4.7116, diffLoss = 7.7292, kgLoss = 0.1852
2025-04-09 09:42:33.439822: Training Step 47/115: batchLoss = 4.2057, diffLoss = 6.8936, kgLoss = 0.1739
2025-04-09 09:42:34.180269: Training Step 48/115: batchLoss = 4.9304, diffLoss = 8.0884, kgLoss = 0.1934
2025-04-09 09:42:34.917299: Training Step 49/115: batchLoss = 4.2936, diffLoss = 7.0404, kgLoss = 0.1734
2025-04-09 09:42:35.645139: Training Step 50/115: batchLoss = 4.3046, diffLoss = 7.0652, kgLoss = 0.1638
2025-04-09 09:42:36.377157: Training Step 51/115: batchLoss = 4.8916, diffLoss = 8.0343, kgLoss = 0.1776
2025-04-09 09:42:37.149023: Training Step 52/115: batchLoss = 4.6412, diffLoss = 7.6125, kgLoss = 0.1843
2025-04-09 09:42:37.899191: Training Step 53/115: batchLoss = 3.9952, diffLoss = 6.5551, kgLoss = 0.1553
2025-04-09 09:42:38.633537: Training Step 54/115: batchLoss = 4.0504, diffLoss = 6.6446, kgLoss = 0.1590
2025-04-09 09:42:39.379983: Training Step 55/115: batchLoss = 4.6805, diffLoss = 7.6806, kgLoss = 0.1804
2025-04-09 09:42:40.112349: Training Step 56/115: batchLoss = 5.0370, diffLoss = 8.2748, kgLoss = 0.1802
2025-04-09 09:42:40.846747: Training Step 57/115: batchLoss = 4.5982, diffLoss = 7.5452, kgLoss = 0.1778
2025-04-09 09:42:41.587492: Training Step 58/115: batchLoss = 4.8006, diffLoss = 7.8836, kgLoss = 0.1762
2025-04-09 09:42:42.331655: Training Step 59/115: batchLoss = 4.0919, diffLoss = 6.7041, kgLoss = 0.1738
2025-04-09 09:42:43.055421: Training Step 60/115: batchLoss = 4.4503, diffLoss = 7.3009, kgLoss = 0.1745
2025-04-09 09:42:43.803698: Training Step 61/115: batchLoss = 4.2929, diffLoss = 7.0435, kgLoss = 0.1671
2025-04-09 09:42:44.531642: Training Step 62/115: batchLoss = 4.4327, diffLoss = 7.2638, kgLoss = 0.1861
2025-04-09 09:42:45.264944: Training Step 63/115: batchLoss = 4.7301, diffLoss = 7.7673, kgLoss = 0.1744
2025-04-09 09:42:45.993930: Training Step 64/115: batchLoss = 4.6652, diffLoss = 7.6541, kgLoss = 0.1820
2025-04-09 09:42:46.714343: Training Step 65/115: batchLoss = 4.0977, diffLoss = 6.7250, kgLoss = 0.1567
2025-04-09 09:42:47.440209: Training Step 66/115: batchLoss = 4.7693, diffLoss = 7.8277, kgLoss = 0.1817
2025-04-09 09:42:48.181831: Training Step 67/115: batchLoss = 4.7737, diffLoss = 7.8272, kgLoss = 0.1933
2025-04-09 09:42:48.925815: Training Step 68/115: batchLoss = 5.4909, diffLoss = 9.0037, kgLoss = 0.2217
2025-04-09 09:42:49.660703: Training Step 69/115: batchLoss = 4.2181, diffLoss = 6.9189, kgLoss = 0.1669
2025-04-09 09:42:50.410883: Training Step 70/115: batchLoss = 4.8366, diffLoss = 7.9350, kgLoss = 0.1890
2025-04-09 09:42:51.156978: Training Step 71/115: batchLoss = 3.8117, diffLoss = 6.2502, kgLoss = 0.1538
2025-04-09 09:42:51.897010: Training Step 72/115: batchLoss = 4.2435, diffLoss = 6.9601, kgLoss = 0.1686
2025-04-09 09:42:52.632076: Training Step 73/115: batchLoss = 4.8682, diffLoss = 7.9923, kgLoss = 0.1820
2025-04-09 09:42:53.363652: Training Step 74/115: batchLoss = 4.4621, diffLoss = 7.3194, kgLoss = 0.1761
2025-04-09 09:42:54.110040: Training Step 75/115: batchLoss = 5.0935, diffLoss = 8.3534, kgLoss = 0.2037
2025-04-09 09:42:54.838565: Training Step 76/115: batchLoss = 4.6334, diffLoss = 7.5911, kgLoss = 0.1968
2025-04-09 09:42:55.581953: Training Step 77/115: batchLoss = 4.2341, diffLoss = 6.9428, kgLoss = 0.1710
2025-04-09 09:42:56.330204: Training Step 78/115: batchLoss = 4.3433, diffLoss = 7.1268, kgLoss = 0.1680
2025-04-09 09:42:57.057346: Training Step 79/115: batchLoss = 4.2569, diffLoss = 6.9763, kgLoss = 0.1778
2025-04-09 09:42:57.795953: Training Step 80/115: batchLoss = 5.1208, diffLoss = 8.4093, kgLoss = 0.1880
2025-04-09 09:42:58.535973: Training Step 81/115: batchLoss = 4.7059, diffLoss = 7.7262, kgLoss = 0.1755
2025-04-09 09:42:59.273562: Training Step 82/115: batchLoss = 4.6909, diffLoss = 7.6948, kgLoss = 0.1851
2025-04-09 09:43:00.002407: Training Step 83/115: batchLoss = 3.9364, diffLoss = 6.4554, kgLoss = 0.1580
2025-04-09 09:43:00.729276: Training Step 84/115: batchLoss = 4.3861, diffLoss = 7.1990, kgLoss = 0.1667
2025-04-09 09:43:01.454467: Training Step 85/115: batchLoss = 4.3824, diffLoss = 7.1849, kgLoss = 0.1786
2025-04-09 09:43:02.186420: Training Step 86/115: batchLoss = 4.2554, diffLoss = 6.9700, kgLoss = 0.1837
2025-04-09 09:43:02.912158: Training Step 87/115: batchLoss = 4.2334, diffLoss = 6.9458, kgLoss = 0.1648
2025-04-09 09:43:03.650311: Training Step 88/115: batchLoss = 4.3152, diffLoss = 7.0802, kgLoss = 0.1678
2025-04-09 09:43:04.397421: Training Step 89/115: batchLoss = 5.0554, diffLoss = 8.2969, kgLoss = 0.1930
2025-04-09 09:43:05.139360: Training Step 90/115: batchLoss = 4.6062, diffLoss = 7.5590, kgLoss = 0.1770
2025-04-09 09:43:05.876628: Training Step 91/115: batchLoss = 4.5176, diffLoss = 7.4030, kgLoss = 0.1896
2025-04-09 09:43:06.612444: Training Step 92/115: batchLoss = 5.3336, diffLoss = 8.7500, kgLoss = 0.2090
2025-04-09 09:43:07.342471: Training Step 93/115: batchLoss = 3.9512, diffLoss = 6.4831, kgLoss = 0.1534
2025-04-09 09:43:08.078979: Training Step 94/115: batchLoss = 4.3104, diffLoss = 7.0670, kgLoss = 0.1755
2025-04-09 09:43:08.797529: Training Step 95/115: batchLoss = 4.9635, diffLoss = 8.1463, kgLoss = 0.1893
2025-04-09 09:43:09.536054: Training Step 96/115: batchLoss = 4.9909, diffLoss = 8.1907, kgLoss = 0.1913
2025-04-09 09:43:10.270349: Training Step 97/115: batchLoss = 4.4227, diffLoss = 7.2565, kgLoss = 0.1719
2025-04-09 09:43:10.995237: Training Step 98/115: batchLoss = 4.3804, diffLoss = 7.1841, kgLoss = 0.1749
2025-04-09 09:43:11.726556: Training Step 99/115: batchLoss = 4.5515, diffLoss = 7.4666, kgLoss = 0.1790
2025-04-09 09:43:12.466471: Training Step 100/115: batchLoss = 4.6960, diffLoss = 7.7038, kgLoss = 0.1843
2025-04-09 09:43:13.198246: Training Step 101/115: batchLoss = 3.6740, diffLoss = 6.0202, kgLoss = 0.1547
2025-04-09 09:43:13.941668: Training Step 102/115: batchLoss = 3.8214, diffLoss = 6.2683, kgLoss = 0.1510
2025-04-09 09:43:14.685241: Training Step 103/115: batchLoss = 4.3124, diffLoss = 7.0705, kgLoss = 0.1751
2025-04-09 09:43:15.430057: Training Step 104/115: batchLoss = 4.4502, diffLoss = 7.3027, kgLoss = 0.1714
2025-04-09 09:43:16.164278: Training Step 105/115: batchLoss = 4.7832, diffLoss = 7.8478, kgLoss = 0.1862
2025-04-09 09:43:16.911637: Training Step 106/115: batchLoss = 4.8368, diffLoss = 7.9323, kgLoss = 0.1936
2025-04-09 09:43:17.636674: Training Step 107/115: batchLoss = 4.6158, diffLoss = 7.5702, kgLoss = 0.1842
2025-04-09 09:43:18.367004: Training Step 108/115: batchLoss = 5.6369, diffLoss = 9.2514, kgLoss = 0.2152
2025-04-09 09:43:19.090689: Training Step 109/115: batchLoss = 4.5205, diffLoss = 7.4141, kgLoss = 0.1801
2025-04-09 09:43:19.816206: Training Step 110/115: batchLoss = 4.4577, diffLoss = 7.3150, kgLoss = 0.1716
2025-04-09 09:43:20.550092: Training Step 111/115: batchLoss = 4.3207, diffLoss = 7.0906, kgLoss = 0.1658
2025-04-09 09:43:21.270289: Training Step 112/115: batchLoss = 4.8394, diffLoss = 7.9332, kgLoss = 0.1987
2025-04-09 09:43:21.924270: Training Step 113/115: batchLoss = 4.8933, diffLoss = 8.0292, kgLoss = 0.1895
2025-04-09 09:43:22.539373: Training Step 114/115: batchLoss = 4.5167, diffLoss = 7.4025, kgLoss = 0.1880
2025-04-09 09:43:22.661667: 
2025-04-09 09:43:22.662569: Epoch 11/1000, Train: epLoss = 1.3048, epDfLoss = 2.1406, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:43:23.457029: Steps 0/90: batch_recall = 53.60, batch_ndcg = 41.26 
2025-04-09 09:43:24.195193: Steps 1/90: batch_recall = 55.64, batch_ndcg = 39.77 
2025-04-09 09:43:24.981499: Steps 2/90: batch_recall = 45.82, batch_ndcg = 31.70 
2025-04-09 09:43:25.747751: Steps 3/90: batch_recall = 43.96, batch_ndcg = 30.24 
2025-04-09 09:43:26.511284: Steps 4/90: batch_recall = 51.29, batch_ndcg = 35.00 
2025-04-09 09:43:27.282555: Steps 5/90: batch_recall = 33.67, batch_ndcg = 23.53 
2025-04-09 09:43:28.084387: Steps 6/90: batch_recall = 45.96, batch_ndcg = 32.97 
2025-04-09 09:43:28.908713: Steps 7/90: batch_recall = 40.24, batch_ndcg = 28.87 
2025-04-09 09:43:29.715265: Steps 8/90: batch_recall = 43.50, batch_ndcg = 30.50 
2025-04-09 09:43:30.518645: Steps 9/90: batch_recall = 39.72, batch_ndcg = 29.57 
2025-04-09 09:43:31.303445: Steps 10/90: batch_recall = 36.38, batch_ndcg = 26.65 
2025-04-09 09:43:32.119253: Steps 11/90: batch_recall = 41.03, batch_ndcg = 26.20 
2025-04-09 09:43:32.921625: Steps 12/90: batch_recall = 34.36, batch_ndcg = 23.87 
2025-04-09 09:43:33.731308: Steps 13/90: batch_recall = 29.13, batch_ndcg = 21.55 
2025-04-09 09:43:34.471581: Steps 14/90: batch_recall = 33.33, batch_ndcg = 23.42 
2025-04-09 09:43:35.209627: Steps 15/90: batch_recall = 42.57, batch_ndcg = 28.05 
2025-04-09 09:43:35.949524: Steps 16/90: batch_recall = 31.73, batch_ndcg = 23.07 
2025-04-09 09:43:36.686590: Steps 17/90: batch_recall = 32.36, batch_ndcg = 21.39 
2025-04-09 09:43:37.414090: Steps 18/90: batch_recall = 36.27, batch_ndcg = 23.88 
2025-04-09 09:43:38.167851: Steps 19/90: batch_recall = 30.31, batch_ndcg = 20.73 
2025-04-09 09:43:38.889675: Steps 20/90: batch_recall = 34.02, batch_ndcg = 21.81 
2025-04-09 09:43:39.628625: Steps 21/90: batch_recall = 33.65, batch_ndcg = 24.69 
2025-04-09 09:43:40.365525: Steps 22/90: batch_recall = 36.08, batch_ndcg = 26.73 
2025-04-09 09:43:41.110610: Steps 23/90: batch_recall = 32.12, batch_ndcg = 22.06 
2025-04-09 09:43:41.848688: Steps 24/90: batch_recall = 34.09, batch_ndcg = 23.17 
2025-04-09 09:43:42.573438: Steps 25/90: batch_recall = 33.59, batch_ndcg = 21.82 
2025-04-09 09:43:43.314883: Steps 26/90: batch_recall = 32.99, batch_ndcg = 22.91 
2025-04-09 09:43:44.034144: Steps 27/90: batch_recall = 29.89, batch_ndcg = 20.13 
2025-04-09 09:43:44.761222: Steps 28/90: batch_recall = 30.82, batch_ndcg = 21.13 
2025-04-09 09:43:45.492906: Steps 29/90: batch_recall = 28.70, batch_ndcg = 19.03 
2025-04-09 09:43:46.228629: Steps 30/90: batch_recall = 26.15, batch_ndcg = 16.42 
2025-04-09 09:43:46.952353: Steps 31/90: batch_recall = 26.32, batch_ndcg = 18.98 
2025-04-09 09:43:47.670223: Steps 32/90: batch_recall = 32.41, batch_ndcg = 21.45 
2025-04-09 09:43:48.386108: Steps 33/90: batch_recall = 28.29, batch_ndcg = 18.45 
2025-04-09 09:43:49.097511: Steps 34/90: batch_recall = 32.16, batch_ndcg = 21.58 
2025-04-09 09:43:49.805323: Steps 35/90: batch_recall = 32.04, batch_ndcg = 22.78 
2025-04-09 09:43:50.508408: Steps 36/90: batch_recall = 32.21, batch_ndcg = 21.61 
2025-04-09 09:43:51.207284: Steps 37/90: batch_recall = 27.32, batch_ndcg = 18.50 
2025-04-09 09:43:51.922337: Steps 38/90: batch_recall = 28.61, batch_ndcg = 19.21 
2025-04-09 09:43:52.642424: Steps 39/90: batch_recall = 26.99, batch_ndcg = 16.63 
2025-04-09 09:43:53.363658: Steps 40/90: batch_recall = 29.26, batch_ndcg = 19.46 
2025-04-09 09:43:54.087827: Steps 41/90: batch_recall = 26.50, batch_ndcg = 18.38 
2025-04-09 09:43:54.799858: Steps 42/90: batch_recall = 32.34, batch_ndcg = 23.60 
2025-04-09 09:43:55.508419: Steps 43/90: batch_recall = 26.14, batch_ndcg = 17.34 
2025-04-09 09:43:56.220795: Steps 44/90: batch_recall = 27.19, batch_ndcg = 18.49 
2025-04-09 09:43:56.924928: Steps 45/90: batch_recall = 28.71, batch_ndcg = 17.21 
2025-04-09 09:43:57.641464: Steps 46/90: batch_recall = 28.74, batch_ndcg = 16.88 
2025-04-09 09:43:58.356835: Steps 47/90: batch_recall = 25.52, batch_ndcg = 16.81 
2025-04-09 09:43:59.072356: Steps 48/90: batch_recall = 28.30, batch_ndcg = 18.19 
2025-04-09 09:43:59.787460: Steps 49/90: batch_recall = 21.98, batch_ndcg = 14.53 
2025-04-09 09:44:00.492061: Steps 50/90: batch_recall = 31.91, batch_ndcg = 20.39 
2025-04-09 09:44:01.203781: Steps 51/90: batch_recall = 33.47, batch_ndcg = 21.26 
2025-04-09 09:44:01.922684: Steps 52/90: batch_recall = 28.41, batch_ndcg = 18.31 
2025-04-09 09:44:02.635924: Steps 53/90: batch_recall = 33.17, batch_ndcg = 19.53 
2025-04-09 09:44:03.339846: Steps 54/90: batch_recall = 26.28, batch_ndcg = 17.55 
2025-04-09 09:44:04.045520: Steps 55/90: batch_recall = 31.28, batch_ndcg = 18.86 
2025-04-09 09:44:04.741510: Steps 56/90: batch_recall = 26.88, batch_ndcg = 15.82 
2025-04-09 09:44:05.453558: Steps 57/90: batch_recall = 28.62, batch_ndcg = 18.41 
2025-04-09 09:44:06.148404: Steps 58/90: batch_recall = 34.87, batch_ndcg = 21.69 
2025-04-09 09:44:06.852999: Steps 59/90: batch_recall = 23.44, batch_ndcg = 14.92 
2025-04-09 09:44:07.567085: Steps 60/90: batch_recall = 22.73, batch_ndcg = 14.83 
2025-04-09 09:44:08.299603: Steps 61/90: batch_recall = 35.82, batch_ndcg = 23.75 
2025-04-09 09:44:09.008894: Steps 62/90: batch_recall = 32.06, batch_ndcg = 20.88 
2025-04-09 09:44:09.713286: Steps 63/90: batch_recall = 32.05, batch_ndcg = 20.09 
2025-04-09 09:44:10.422118: Steps 64/90: batch_recall = 31.20, batch_ndcg = 17.90 
2025-04-09 09:44:11.123829: Steps 65/90: batch_recall = 29.96, batch_ndcg = 19.19 
2025-04-09 09:44:11.844214: Steps 66/90: batch_recall = 30.47, batch_ndcg = 19.08 
2025-04-09 09:44:12.556827: Steps 67/90: batch_recall = 28.35, batch_ndcg = 16.31 
2025-04-09 09:44:13.285032: Steps 68/90: batch_recall = 28.01, batch_ndcg = 17.79 
2025-04-09 09:44:14.007182: Steps 69/90: batch_recall = 30.16, batch_ndcg = 19.16 
2025-04-09 09:44:14.714844: Steps 70/90: batch_recall = 30.13, batch_ndcg = 18.48 
2025-04-09 09:44:15.415742: Steps 71/90: batch_recall = 31.36, batch_ndcg = 18.93 
2025-04-09 09:44:16.129295: Steps 72/90: batch_recall = 25.71, batch_ndcg = 15.89 
2025-04-09 09:44:16.824652: Steps 73/90: batch_recall = 30.05, batch_ndcg = 18.45 
2025-04-09 09:44:17.535624: Steps 74/90: batch_recall = 30.55, batch_ndcg = 20.67 
2025-04-09 09:44:18.231689: Steps 75/90: batch_recall = 34.48, batch_ndcg = 20.46 
2025-04-09 09:44:18.931040: Steps 76/90: batch_recall = 32.69, batch_ndcg = 19.15 
2025-04-09 09:44:19.617753: Steps 77/90: batch_recall = 32.79, batch_ndcg = 18.87 
2025-04-09 09:44:20.321734: Steps 78/90: batch_recall = 28.14, batch_ndcg = 16.62 
2025-04-09 09:44:21.008562: Steps 79/90: batch_recall = 31.82, batch_ndcg = 22.17 
2025-04-09 09:44:21.701082: Steps 80/90: batch_recall = 36.80, batch_ndcg = 22.93 
2025-04-09 09:44:22.406527: Steps 81/90: batch_recall = 35.17, batch_ndcg = 21.35 
2025-04-09 09:44:23.118596: Steps 82/90: batch_recall = 30.68, batch_ndcg = 19.86 
2025-04-09 09:44:23.839314: Steps 83/90: batch_recall = 31.07, batch_ndcg = 18.99 
2025-04-09 09:44:24.568265: Steps 84/90: batch_recall = 29.44, batch_ndcg = 17.65 
2025-04-09 09:44:25.305356: Steps 85/90: batch_recall = 28.00, batch_ndcg = 16.84 
2025-04-09 09:44:26.052395: Steps 86/90: batch_recall = 38.46, batch_ndcg = 22.26 
2025-04-09 09:44:26.764742: Steps 87/90: batch_recall = 36.05, batch_ndcg = 23.95 
2025-04-09 09:44:27.474363: Steps 88/90: batch_recall = 38.44, batch_ndcg = 22.69 
2025-04-09 09:44:28.026575: Steps 89/90: batch_recall = 32.61, batch_ndcg = 17.61 
2025-04-09 09:44:28.027501: Epoch 11/1000, Test: Recall = 0.0645, NDCG = 0.0421  

2025-04-09 09:44:29.031911: Training Step 0/115: batchLoss = 4.7129, diffLoss = 7.7265, kgLoss = 0.1925
2025-04-09 09:44:29.769885: Training Step 1/115: batchLoss = 4.6720, diffLoss = 7.6663, kgLoss = 0.1807
2025-04-09 09:44:30.501906: Training Step 2/115: batchLoss = 4.6291, diffLoss = 7.6011, kgLoss = 0.1711
2025-04-09 09:44:31.226301: Training Step 3/115: batchLoss = 4.1509, diffLoss = 6.7995, kgLoss = 0.1780
2025-04-09 09:44:31.963069: Training Step 4/115: batchLoss = 5.1126, diffLoss = 8.3857, kgLoss = 0.2029
2025-04-09 09:44:32.682382: Training Step 5/115: batchLoss = 4.0844, diffLoss = 6.6983, kgLoss = 0.1636
2025-04-09 09:44:33.410240: Training Step 6/115: batchLoss = 4.3711, diffLoss = 7.1691, kgLoss = 0.1741
2025-04-09 09:44:34.138636: Training Step 7/115: batchLoss = 4.2411, diffLoss = 6.9527, kgLoss = 0.1738
2025-04-09 09:44:34.872504: Training Step 8/115: batchLoss = 4.0854, diffLoss = 6.6938, kgLoss = 0.1728
2025-04-09 09:44:35.595890: Training Step 9/115: batchLoss = 4.4919, diffLoss = 7.3735, kgLoss = 0.1696
2025-04-09 09:44:36.330355: Training Step 10/115: batchLoss = 4.6894, diffLoss = 7.6959, kgLoss = 0.1796
2025-04-09 09:44:37.069798: Training Step 11/115: batchLoss = 4.2845, diffLoss = 7.0254, kgLoss = 0.1731
2025-04-09 09:44:37.802528: Training Step 12/115: batchLoss = 4.4380, diffLoss = 7.2815, kgLoss = 0.1729
2025-04-09 09:44:38.539204: Training Step 13/115: batchLoss = 4.5597, diffLoss = 7.4816, kgLoss = 0.1768
2025-04-09 09:44:39.268289: Training Step 14/115: batchLoss = 4.1706, diffLoss = 6.8394, kgLoss = 0.1674
2025-04-09 09:44:39.999419: Training Step 15/115: batchLoss = 4.8878, diffLoss = 8.0181, kgLoss = 0.1925
2025-04-09 09:44:40.743256: Training Step 16/115: batchLoss = 3.8586, diffLoss = 6.3215, kgLoss = 0.1643
2025-04-09 09:44:41.483871: Training Step 17/115: batchLoss = 4.7215, diffLoss = 7.7464, kgLoss = 0.1842
2025-04-09 09:44:42.212942: Training Step 18/115: batchLoss = 4.3825, diffLoss = 7.1767, kgLoss = 0.1912
2025-04-09 09:44:42.958955: Training Step 19/115: batchLoss = 4.0721, diffLoss = 6.6701, kgLoss = 0.1750
2025-04-09 09:44:43.689933: Training Step 20/115: batchLoss = 4.3863, diffLoss = 7.2006, kgLoss = 0.1650
2025-04-09 09:44:44.427656: Training Step 21/115: batchLoss = 4.4083, diffLoss = 7.2276, kgLoss = 0.1795
2025-04-09 09:44:45.170551: Training Step 22/115: batchLoss = 4.8360, diffLoss = 7.9414, kgLoss = 0.1779
2025-04-09 09:44:45.915836: Training Step 23/115: batchLoss = 4.8002, diffLoss = 7.8836, kgLoss = 0.1750
2025-04-09 09:44:46.647254: Training Step 24/115: batchLoss = 4.7774, diffLoss = 7.8401, kgLoss = 0.1833
2025-04-09 09:44:47.377211: Training Step 25/115: batchLoss = 4.2858, diffLoss = 7.0334, kgLoss = 0.1645
2025-04-09 09:44:48.130645: Training Step 26/115: batchLoss = 4.7197, diffLoss = 7.7377, kgLoss = 0.1926
2025-04-09 09:44:48.860760: Training Step 27/115: batchLoss = 4.2808, diffLoss = 7.0212, kgLoss = 0.1702
2025-04-09 09:44:49.598623: Training Step 28/115: batchLoss = 4.1820, diffLoss = 6.8571, kgLoss = 0.1693
2025-04-09 09:44:50.326166: Training Step 29/115: batchLoss = 4.3986, diffLoss = 7.2197, kgLoss = 0.1670
2025-04-09 09:44:51.070925: Training Step 30/115: batchLoss = 4.3415, diffLoss = 7.1233, kgLoss = 0.1688
2025-04-09 09:44:51.811049: Training Step 31/115: batchLoss = 4.2988, diffLoss = 7.0569, kgLoss = 0.1617
2025-04-09 09:44:52.543689: Training Step 32/115: batchLoss = 5.1761, diffLoss = 8.4904, kgLoss = 0.2047
2025-04-09 09:44:53.280938: Training Step 33/115: batchLoss = 4.4054, diffLoss = 7.2307, kgLoss = 0.1675
2025-04-09 09:44:54.038696: Training Step 34/115: batchLoss = 4.0743, diffLoss = 6.6793, kgLoss = 0.1668
2025-04-09 09:44:54.798479: Training Step 35/115: batchLoss = 4.7890, diffLoss = 7.8603, kgLoss = 0.1821
2025-04-09 09:44:55.539445: Training Step 36/115: batchLoss = 4.5527, diffLoss = 7.4766, kgLoss = 0.1668
2025-04-09 09:44:56.282923: Training Step 37/115: batchLoss = 4.5760, diffLoss = 7.4995, kgLoss = 0.1909
2025-04-09 09:44:57.024320: Training Step 38/115: batchLoss = 4.6634, diffLoss = 7.6514, kgLoss = 0.1814
2025-04-09 09:44:57.760714: Training Step 39/115: batchLoss = 4.9621, diffLoss = 8.1334, kgLoss = 0.2052
2025-04-09 09:44:58.488428: Training Step 40/115: batchLoss = 4.6236, diffLoss = 7.5890, kgLoss = 0.1755
2025-04-09 09:44:59.211733: Training Step 41/115: batchLoss = 4.3204, diffLoss = 7.0892, kgLoss = 0.1671
2025-04-09 09:44:59.952826: Training Step 42/115: batchLoss = 4.2997, diffLoss = 7.0561, kgLoss = 0.1651
2025-04-09 09:45:00.687815: Training Step 43/115: batchLoss = 4.3935, diffLoss = 7.2049, kgLoss = 0.1763
2025-04-09 09:45:01.417672: Training Step 44/115: batchLoss = 4.3758, diffLoss = 7.1758, kgLoss = 0.1759
2025-04-09 09:45:02.142927: Training Step 45/115: batchLoss = 5.1616, diffLoss = 8.4733, kgLoss = 0.1940
2025-04-09 09:45:02.875166: Training Step 46/115: batchLoss = 4.4768, diffLoss = 7.3463, kgLoss = 0.1726
2025-04-09 09:45:03.600007: Training Step 47/115: batchLoss = 4.5227, diffLoss = 7.4143, kgLoss = 0.1852
2025-04-09 09:45:04.331576: Training Step 48/115: batchLoss = 4.8450, diffLoss = 7.9499, kgLoss = 0.1878
2025-04-09 09:45:05.067499: Training Step 49/115: batchLoss = 4.4606, diffLoss = 7.3205, kgLoss = 0.1707
2025-04-09 09:45:05.795292: Training Step 50/115: batchLoss = 4.2725, diffLoss = 7.0020, kgLoss = 0.1784
2025-04-09 09:45:06.535862: Training Step 51/115: batchLoss = 5.1947, diffLoss = 8.5262, kgLoss = 0.1973
2025-04-09 09:45:07.262326: Training Step 52/115: batchLoss = 4.2277, diffLoss = 6.9327, kgLoss = 0.1703
2025-04-09 09:45:08.011387: Training Step 53/115: batchLoss = 4.4755, diffLoss = 7.3422, kgLoss = 0.1755
2025-04-09 09:45:08.772875: Training Step 54/115: batchLoss = 4.6204, diffLoss = 7.5770, kgLoss = 0.1855
2025-04-09 09:45:09.520314: Training Step 55/115: batchLoss = 4.1519, diffLoss = 6.8151, kgLoss = 0.1571
2025-04-09 09:45:10.259274: Training Step 56/115: batchLoss = 4.4448, diffLoss = 7.2957, kgLoss = 0.1683
2025-04-09 09:45:11.007267: Training Step 57/115: batchLoss = 4.0980, diffLoss = 6.7206, kgLoss = 0.1642
2025-04-09 09:45:11.745785: Training Step 58/115: batchLoss = 4.7604, diffLoss = 7.8019, kgLoss = 0.1981
2025-04-09 09:45:12.479526: Training Step 59/115: batchLoss = 5.2130, diffLoss = 8.5566, kgLoss = 0.1975
2025-04-09 09:45:13.211862: Training Step 60/115: batchLoss = 5.9350, diffLoss = 9.7315, kgLoss = 0.2402
2025-04-09 09:45:13.958004: Training Step 61/115: batchLoss = 4.3022, diffLoss = 7.0596, kgLoss = 0.1660
2025-04-09 09:45:14.686369: Training Step 62/115: batchLoss = 4.8031, diffLoss = 7.8898, kgLoss = 0.1731
2025-04-09 09:45:15.421988: Training Step 63/115: batchLoss = 4.5446, diffLoss = 7.4540, kgLoss = 0.1806
2025-04-09 09:45:16.157405: Training Step 64/115: batchLoss = 3.9289, diffLoss = 6.4462, kgLoss = 0.1529
2025-04-09 09:45:16.907411: Training Step 65/115: batchLoss = 4.8360, diffLoss = 7.9263, kgLoss = 0.2006
2025-04-09 09:45:17.642203: Training Step 66/115: batchLoss = 4.3643, diffLoss = 7.1646, kgLoss = 0.1640
2025-04-09 09:45:18.379778: Training Step 67/115: batchLoss = 4.2425, diffLoss = 6.9598, kgLoss = 0.1667
2025-04-09 09:45:19.124297: Training Step 68/115: batchLoss = 4.9990, diffLoss = 8.2076, kgLoss = 0.1861
2025-04-09 09:45:19.847546: Training Step 69/115: batchLoss = 4.1383, diffLoss = 6.7919, kgLoss = 0.1579
2025-04-09 09:45:20.583399: Training Step 70/115: batchLoss = 4.1810, diffLoss = 6.8614, kgLoss = 0.1603
2025-04-09 09:45:21.317590: Training Step 71/115: batchLoss = 4.6812, diffLoss = 7.6832, kgLoss = 0.1784
2025-04-09 09:45:22.047268: Training Step 72/115: batchLoss = 4.6492, diffLoss = 7.6262, kgLoss = 0.1837
2025-04-09 09:45:22.791078: Training Step 73/115: batchLoss = 4.5167, diffLoss = 7.4011, kgLoss = 0.1900
2025-04-09 09:45:23.510732: Training Step 74/115: batchLoss = 3.8007, diffLoss = 6.2321, kgLoss = 0.1536
2025-04-09 09:45:24.239712: Training Step 75/115: batchLoss = 4.1923, diffLoss = 6.8741, kgLoss = 0.1697
2025-04-09 09:45:24.969972: Training Step 76/115: batchLoss = 4.6989, diffLoss = 7.7124, kgLoss = 0.1788
2025-04-09 09:45:25.699542: Training Step 77/115: batchLoss = 4.2915, diffLoss = 7.0433, kgLoss = 0.1638
2025-04-09 09:45:26.443678: Training Step 78/115: batchLoss = 4.3496, diffLoss = 7.1355, kgLoss = 0.1707
2025-04-09 09:45:27.182630: Training Step 79/115: batchLoss = 5.1681, diffLoss = 8.4850, kgLoss = 0.1926
2025-04-09 09:45:27.912728: Training Step 80/115: batchLoss = 4.6256, diffLoss = 7.5913, kgLoss = 0.1771
2025-04-09 09:45:28.634087: Training Step 81/115: batchLoss = 4.7477, diffLoss = 7.7993, kgLoss = 0.1702
2025-04-09 09:45:29.352400: Training Step 82/115: batchLoss = 4.4012, diffLoss = 7.2147, kgLoss = 0.1809
2025-04-09 09:45:30.060112: Training Step 83/115: batchLoss = 5.1464, diffLoss = 8.4356, kgLoss = 0.2127
2025-04-09 09:45:30.782970: Training Step 84/115: batchLoss = 5.2728, diffLoss = 8.6588, kgLoss = 0.1937
2025-04-09 09:45:31.502399: Training Step 85/115: batchLoss = 4.5384, diffLoss = 7.4508, kgLoss = 0.1698
2025-04-09 09:45:32.219553: Training Step 86/115: batchLoss = 4.1488, diffLoss = 6.8038, kgLoss = 0.1665
2025-04-09 09:45:32.934289: Training Step 87/115: batchLoss = 4.1019, diffLoss = 6.7264, kgLoss = 0.1653
2025-04-09 09:45:33.664224: Training Step 88/115: batchLoss = 4.3558, diffLoss = 7.1463, kgLoss = 0.1700
2025-04-09 09:45:34.387578: Training Step 89/115: batchLoss = 4.4752, diffLoss = 7.3434, kgLoss = 0.1730
2025-04-09 09:45:35.119710: Training Step 90/115: batchLoss = 4.5781, diffLoss = 7.5126, kgLoss = 0.1763
2025-04-09 09:45:35.845806: Training Step 91/115: batchLoss = 4.4943, diffLoss = 7.3773, kgLoss = 0.1698
2025-04-09 09:45:36.583643: Training Step 92/115: batchLoss = 4.8762, diffLoss = 7.9986, kgLoss = 0.1927
2025-04-09 09:45:37.305860: Training Step 93/115: batchLoss = 4.5055, diffLoss = 7.3888, kgLoss = 0.1806
2025-04-09 09:45:38.128797: Training Step 94/115: batchLoss = 4.1468, diffLoss = 6.7933, kgLoss = 0.1769
2025-04-09 09:45:38.840914: Training Step 95/115: batchLoss = 5.3656, diffLoss = 8.8022, kgLoss = 0.2107
2025-04-09 09:45:39.557133: Training Step 96/115: batchLoss = 4.6515, diffLoss = 7.6329, kgLoss = 0.1793
2025-04-09 09:45:40.279309: Training Step 97/115: batchLoss = 5.2374, diffLoss = 8.6005, kgLoss = 0.1928
2025-04-09 09:45:41.000748: Training Step 98/115: batchLoss = 4.1495, diffLoss = 6.8076, kgLoss = 0.1623
2025-04-09 09:45:41.730460: Training Step 99/115: batchLoss = 5.0546, diffLoss = 8.3004, kgLoss = 0.1858
2025-04-09 09:45:42.446310: Training Step 100/115: batchLoss = 4.1272, diffLoss = 6.7701, kgLoss = 0.1629
2025-04-09 09:45:43.169253: Training Step 101/115: batchLoss = 4.6344, diffLoss = 7.6064, kgLoss = 0.1763
2025-04-09 09:45:43.886073: Training Step 102/115: batchLoss = 4.3458, diffLoss = 7.1281, kgLoss = 0.1724
2025-04-09 09:45:44.607534: Training Step 103/115: batchLoss = 4.4917, diffLoss = 7.3738, kgLoss = 0.1686
2025-04-09 09:45:45.330446: Training Step 104/115: batchLoss = 4.8095, diffLoss = 7.8924, kgLoss = 0.1852
2025-04-09 09:45:46.048067: Training Step 105/115: batchLoss = 4.3935, diffLoss = 7.2026, kgLoss = 0.1797
2025-04-09 09:45:46.768535: Training Step 106/115: batchLoss = 4.0110, diffLoss = 6.5789, kgLoss = 0.1592
2025-04-09 09:45:47.494718: Training Step 107/115: batchLoss = 4.9927, diffLoss = 8.1866, kgLoss = 0.2019
2025-04-09 09:45:48.224350: Training Step 108/115: batchLoss = 4.3496, diffLoss = 7.1284, kgLoss = 0.1814
2025-04-09 09:45:48.953235: Training Step 109/115: batchLoss = 4.2948, diffLoss = 7.0300, kgLoss = 0.1920
2025-04-09 09:45:49.699141: Training Step 110/115: batchLoss = 4.4584, diffLoss = 7.3174, kgLoss = 0.1699
2025-04-09 09:45:50.434521: Training Step 111/115: batchLoss = 4.0829, diffLoss = 6.6926, kgLoss = 0.1683
2025-04-09 09:45:51.148473: Training Step 112/115: batchLoss = 5.0235, diffLoss = 8.2489, kgLoss = 0.1853
2025-04-09 09:45:51.805303: Training Step 113/115: batchLoss = 4.2963, diffLoss = 7.0398, kgLoss = 0.1812
2025-04-09 09:45:52.456770: Training Step 114/115: batchLoss = 4.3316, diffLoss = 7.1063, kgLoss = 0.1697
2025-04-09 09:45:52.585652: 
2025-04-09 09:45:52.587041: Epoch 12/1000, Train: epLoss = 1.3015, epDfLoss = 2.1351, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:45:53.351579: Steps 0/90: batch_recall = 53.49, batch_ndcg = 41.20 
2025-04-09 09:45:54.092672: Steps 1/90: batch_recall = 55.44, batch_ndcg = 39.40 
2025-04-09 09:45:54.837480: Steps 2/90: batch_recall = 47.37, batch_ndcg = 32.70 
2025-04-09 09:45:55.587310: Steps 3/90: batch_recall = 45.17, batch_ndcg = 31.22 
2025-04-09 09:45:56.331929: Steps 4/90: batch_recall = 51.43, batch_ndcg = 35.58 
2025-04-09 09:45:57.082416: Steps 5/90: batch_recall = 33.45, batch_ndcg = 23.79 
2025-04-09 09:45:57.827258: Steps 6/90: batch_recall = 46.63, batch_ndcg = 33.30 
2025-04-09 09:45:58.564284: Steps 7/90: batch_recall = 39.90, batch_ndcg = 29.45 
2025-04-09 09:45:59.307122: Steps 8/90: batch_recall = 43.68, batch_ndcg = 31.14 
2025-04-09 09:46:00.138825: Steps 9/90: batch_recall = 40.74, batch_ndcg = 29.99 
2025-04-09 09:46:00.872078: Steps 10/90: batch_recall = 36.67, batch_ndcg = 27.10 
2025-04-09 09:46:01.611276: Steps 11/90: batch_recall = 41.80, batch_ndcg = 26.18 
2025-04-09 09:46:02.331306: Steps 12/90: batch_recall = 35.90, batch_ndcg = 24.48 
2025-04-09 09:46:03.052142: Steps 13/90: batch_recall = 30.70, batch_ndcg = 22.01 
2025-04-09 09:46:03.791872: Steps 14/90: batch_recall = 33.77, batch_ndcg = 23.94 
2025-04-09 09:46:04.514903: Steps 15/90: batch_recall = 42.85, batch_ndcg = 28.24 
2025-04-09 09:46:05.256780: Steps 16/90: batch_recall = 32.15, batch_ndcg = 23.42 
2025-04-09 09:46:05.987405: Steps 17/90: batch_recall = 32.81, batch_ndcg = 21.61 
2025-04-09 09:46:06.715957: Steps 18/90: batch_recall = 35.77, batch_ndcg = 23.81 
2025-04-09 09:46:07.479852: Steps 19/90: batch_recall = 29.83, batch_ndcg = 20.95 
2025-04-09 09:46:08.219250: Steps 20/90: batch_recall = 33.21, batch_ndcg = 21.50 
2025-04-09 09:46:08.956372: Steps 21/90: batch_recall = 35.08, batch_ndcg = 25.31 
2025-04-09 09:46:09.701819: Steps 22/90: batch_recall = 37.65, batch_ndcg = 27.47 
2025-04-09 09:46:10.436977: Steps 23/90: batch_recall = 32.24, batch_ndcg = 21.97 
2025-04-09 09:46:11.163228: Steps 24/90: batch_recall = 35.71, batch_ndcg = 23.80 
2025-04-09 09:46:11.873458: Steps 25/90: batch_recall = 34.75, batch_ndcg = 22.82 
2025-04-09 09:46:12.589283: Steps 26/90: batch_recall = 33.67, batch_ndcg = 23.51 
2025-04-09 09:46:13.297486: Steps 27/90: batch_recall = 30.24, batch_ndcg = 20.68 
2025-04-09 09:46:14.013805: Steps 28/90: batch_recall = 31.00, batch_ndcg = 21.35 
2025-04-09 09:46:14.731690: Steps 29/90: batch_recall = 29.56, batch_ndcg = 19.54 
2025-04-09 09:46:15.463142: Steps 30/90: batch_recall = 27.74, batch_ndcg = 17.11 
2025-04-09 09:46:16.190173: Steps 31/90: batch_recall = 28.19, batch_ndcg = 19.39 
2025-04-09 09:46:16.916428: Steps 32/90: batch_recall = 32.43, batch_ndcg = 21.69 
2025-04-09 09:46:17.647396: Steps 33/90: batch_recall = 28.68, batch_ndcg = 18.70 
2025-04-09 09:46:18.394397: Steps 34/90: batch_recall = 33.31, batch_ndcg = 22.13 
2025-04-09 09:46:19.110084: Steps 35/90: batch_recall = 32.84, batch_ndcg = 23.37 
2025-04-09 09:46:19.847985: Steps 36/90: batch_recall = 33.95, batch_ndcg = 22.26 
2025-04-09 09:46:20.558421: Steps 37/90: batch_recall = 28.29, batch_ndcg = 19.14 
2025-04-09 09:46:21.270315: Steps 38/90: batch_recall = 28.69, batch_ndcg = 19.20 
2025-04-09 09:46:22.001083: Steps 39/90: batch_recall = 28.50, batch_ndcg = 17.28 
2025-04-09 09:46:22.711034: Steps 40/90: batch_recall = 29.68, batch_ndcg = 19.64 
2025-04-09 09:46:23.420661: Steps 41/90: batch_recall = 27.78, batch_ndcg = 19.45 
2025-04-09 09:46:24.145737: Steps 42/90: batch_recall = 33.15, batch_ndcg = 24.01 
2025-04-09 09:46:24.873318: Steps 43/90: batch_recall = 26.66, batch_ndcg = 18.01 
2025-04-09 09:46:25.592808: Steps 44/90: batch_recall = 28.72, batch_ndcg = 19.18 
2025-04-09 09:46:26.310716: Steps 45/90: batch_recall = 30.92, batch_ndcg = 18.54 
2025-04-09 09:46:27.034988: Steps 46/90: batch_recall = 29.15, batch_ndcg = 17.45 
2025-04-09 09:46:27.746353: Steps 47/90: batch_recall = 26.41, batch_ndcg = 17.12 
2025-04-09 09:46:28.446660: Steps 48/90: batch_recall = 31.39, batch_ndcg = 19.67 
2025-04-09 09:46:29.152733: Steps 49/90: batch_recall = 22.19, batch_ndcg = 15.18 
2025-04-09 09:46:29.847832: Steps 50/90: batch_recall = 32.80, batch_ndcg = 20.81 
2025-04-09 09:46:30.567073: Steps 51/90: batch_recall = 33.67, batch_ndcg = 21.91 
2025-04-09 09:46:31.276277: Steps 52/90: batch_recall = 28.49, batch_ndcg = 18.84 
2025-04-09 09:46:31.993713: Steps 53/90: batch_recall = 32.80, batch_ndcg = 19.61 
2025-04-09 09:46:32.735353: Steps 54/90: batch_recall = 26.02, batch_ndcg = 17.62 
2025-04-09 09:46:33.459111: Steps 55/90: batch_recall = 31.62, batch_ndcg = 19.47 
2025-04-09 09:46:34.179782: Steps 56/90: batch_recall = 26.93, batch_ndcg = 16.35 
2025-04-09 09:46:34.890653: Steps 57/90: batch_recall = 30.32, batch_ndcg = 19.09 
2025-04-09 09:46:35.607370: Steps 58/90: batch_recall = 36.93, batch_ndcg = 22.31 
2025-04-09 09:46:36.328775: Steps 59/90: batch_recall = 24.69, batch_ndcg = 15.54 
2025-04-09 09:46:37.039458: Steps 60/90: batch_recall = 23.57, batch_ndcg = 15.17 
2025-04-09 09:46:37.747580: Steps 61/90: batch_recall = 36.93, batch_ndcg = 24.64 
2025-04-09 09:46:38.437464: Steps 62/90: batch_recall = 32.79, batch_ndcg = 21.10 
2025-04-09 09:46:39.149313: Steps 63/90: batch_recall = 32.34, batch_ndcg = 20.40 
2025-04-09 09:46:39.844934: Steps 64/90: batch_recall = 32.13, batch_ndcg = 18.62 
2025-04-09 09:46:40.555207: Steps 65/90: batch_recall = 30.51, batch_ndcg = 19.49 
2025-04-09 09:46:41.272138: Steps 66/90: batch_recall = 30.70, batch_ndcg = 19.71 
2025-04-09 09:46:41.986990: Steps 67/90: batch_recall = 28.92, batch_ndcg = 16.87 
2025-04-09 09:46:42.692212: Steps 68/90: batch_recall = 30.13, batch_ndcg = 18.88 
2025-04-09 09:46:43.397119: Steps 69/90: batch_recall = 30.53, batch_ndcg = 19.40 
2025-04-09 09:46:44.098979: Steps 70/90: batch_recall = 30.46, batch_ndcg = 18.76 
2025-04-09 09:46:44.802463: Steps 71/90: batch_recall = 32.37, batch_ndcg = 19.62 
2025-04-09 09:46:45.511234: Steps 72/90: batch_recall = 26.59, batch_ndcg = 16.40 
2025-04-09 09:46:46.226771: Steps 73/90: batch_recall = 30.68, batch_ndcg = 19.01 
2025-04-09 09:46:46.940946: Steps 74/90: batch_recall = 31.00, batch_ndcg = 20.77 
2025-04-09 09:46:47.652412: Steps 75/90: batch_recall = 34.37, batch_ndcg = 21.36 
2025-04-09 09:46:48.353549: Steps 76/90: batch_recall = 33.72, batch_ndcg = 19.79 
2025-04-09 09:46:49.072268: Steps 77/90: batch_recall = 33.20, batch_ndcg = 19.35 
2025-04-09 09:46:49.791112: Steps 78/90: batch_recall = 27.75, batch_ndcg = 16.64 
2025-04-09 09:46:50.504002: Steps 79/90: batch_recall = 32.31, batch_ndcg = 22.41 
2025-04-09 09:46:51.215272: Steps 80/90: batch_recall = 36.94, batch_ndcg = 23.68 
2025-04-09 09:46:51.913271: Steps 81/90: batch_recall = 36.99, batch_ndcg = 22.13 
2025-04-09 09:46:52.627003: Steps 82/90: batch_recall = 31.77, batch_ndcg = 20.60 
2025-04-09 09:46:53.332787: Steps 83/90: batch_recall = 33.44, batch_ndcg = 20.23 
2025-04-09 09:46:54.094173: Steps 84/90: batch_recall = 30.04, batch_ndcg = 17.86 
2025-04-09 09:46:54.831365: Steps 85/90: batch_recall = 30.30, batch_ndcg = 17.69 
2025-04-09 09:46:55.549851: Steps 86/90: batch_recall = 38.90, batch_ndcg = 23.33 
2025-04-09 09:46:56.249727: Steps 87/90: batch_recall = 36.88, batch_ndcg = 24.61 
2025-04-09 09:46:56.963281: Steps 88/90: batch_recall = 39.12, batch_ndcg = 22.89 
2025-04-09 09:46:57.525023: Steps 89/90: batch_recall = 34.59, batch_ndcg = 19.12 
2025-04-09 09:46:57.525768: Epoch 12/1000, Test: Recall = 0.0658, NDCG = 0.0431  

2025-04-09 09:46:58.563497: Training Step 0/115: batchLoss = 4.2675, diffLoss = 7.0032, kgLoss = 0.1639
2025-04-09 09:46:59.304925: Training Step 1/115: batchLoss = 3.9952, diffLoss = 6.5467, kgLoss = 0.1680
2025-04-09 09:47:00.042092: Training Step 2/115: batchLoss = 4.0292, diffLoss = 6.6117, kgLoss = 0.1554
2025-04-09 09:47:00.783031: Training Step 3/115: batchLoss = 4.5528, diffLoss = 7.4734, kgLoss = 0.1719
2025-04-09 09:47:01.532157: Training Step 4/115: batchLoss = 4.2534, diffLoss = 6.9803, kgLoss = 0.1630
2025-04-09 09:47:02.270190: Training Step 5/115: batchLoss = 4.9219, diffLoss = 8.0748, kgLoss = 0.1925
2025-04-09 09:47:03.011165: Training Step 6/115: batchLoss = 4.5504, diffLoss = 7.4702, kgLoss = 0.1706
2025-04-09 09:47:03.752803: Training Step 7/115: batchLoss = 4.5960, diffLoss = 7.5369, kgLoss = 0.1845
2025-04-09 09:47:04.493016: Training Step 8/115: batchLoss = 4.7455, diffLoss = 7.7882, kgLoss = 0.1816
2025-04-09 09:47:05.236324: Training Step 9/115: batchLoss = 4.5728, diffLoss = 7.4973, kgLoss = 0.1862
2025-04-09 09:47:05.974188: Training Step 10/115: batchLoss = 4.3896, diffLoss = 7.2041, kgLoss = 0.1679
2025-04-09 09:47:06.709961: Training Step 11/115: batchLoss = 4.0700, diffLoss = 6.6689, kgLoss = 0.1716
2025-04-09 09:47:07.449456: Training Step 12/115: batchLoss = 4.5903, diffLoss = 7.5354, kgLoss = 0.1726
2025-04-09 09:47:08.203399: Training Step 13/115: batchLoss = 4.9056, diffLoss = 8.0514, kgLoss = 0.1870
2025-04-09 09:47:08.954005: Training Step 14/115: batchLoss = 4.6743, diffLoss = 7.6661, kgLoss = 0.1867
2025-04-09 09:47:09.704123: Training Step 15/115: batchLoss = 5.1328, diffLoss = 8.4276, kgLoss = 0.1905
2025-04-09 09:47:10.445017: Training Step 16/115: batchLoss = 4.4362, diffLoss = 7.2708, kgLoss = 0.1842
2025-04-09 09:47:11.266925: Training Step 17/115: batchLoss = 4.6926, diffLoss = 7.6954, kgLoss = 0.1883
2025-04-09 09:47:12.000607: Training Step 18/115: batchLoss = 4.4617, diffLoss = 7.3239, kgLoss = 0.1685
2025-04-09 09:47:12.729009: Training Step 19/115: batchLoss = 4.1214, diffLoss = 6.7588, kgLoss = 0.1653
2025-04-09 09:47:13.457998: Training Step 20/115: batchLoss = 4.6853, diffLoss = 7.6897, kgLoss = 0.1786
2025-04-09 09:47:14.203058: Training Step 21/115: batchLoss = 4.8538, diffLoss = 7.9597, kgLoss = 0.1950
2025-04-09 09:47:14.930977: Training Step 22/115: batchLoss = 4.4741, diffLoss = 7.3375, kgLoss = 0.1789
2025-04-09 09:47:15.683370: Training Step 23/115: batchLoss = 4.4498, diffLoss = 7.2987, kgLoss = 0.1766
2025-04-09 09:47:16.426398: Training Step 24/115: batchLoss = 4.7606, diffLoss = 7.8122, kgLoss = 0.1832
2025-04-09 09:47:17.169378: Training Step 25/115: batchLoss = 4.4737, diffLoss = 7.3399, kgLoss = 0.1743
2025-04-09 09:47:17.904743: Training Step 26/115: batchLoss = 4.3662, diffLoss = 7.1599, kgLoss = 0.1756
2025-04-09 09:47:18.660233: Training Step 27/115: batchLoss = 4.4717, diffLoss = 7.3382, kgLoss = 0.1721
2025-04-09 09:47:19.414084: Training Step 28/115: batchLoss = 4.8964, diffLoss = 8.0324, kgLoss = 0.1923
2025-04-09 09:47:20.168142: Training Step 29/115: batchLoss = 4.6640, diffLoss = 7.6495, kgLoss = 0.1858
2025-04-09 09:47:20.919331: Training Step 30/115: batchLoss = 4.6033, diffLoss = 7.5611, kgLoss = 0.1665
2025-04-09 09:47:21.755835: Training Step 31/115: batchLoss = 4.0628, diffLoss = 6.6494, kgLoss = 0.1829
2025-04-09 09:47:22.497066: Training Step 32/115: batchLoss = 4.2062, diffLoss = 6.8963, kgLoss = 0.1711
2025-04-09 09:47:23.256519: Training Step 33/115: batchLoss = 4.2295, diffLoss = 6.9391, kgLoss = 0.1652
2025-04-09 09:47:23.992435: Training Step 34/115: batchLoss = 4.2504, diffLoss = 6.9722, kgLoss = 0.1678
2025-04-09 09:47:24.730392: Training Step 35/115: batchLoss = 4.3479, diffLoss = 7.1320, kgLoss = 0.1719
2025-04-09 09:47:25.489355: Training Step 36/115: batchLoss = 4.9333, diffLoss = 8.0894, kgLoss = 0.1991
2025-04-09 09:47:26.249035: Training Step 37/115: batchLoss = 4.8832, diffLoss = 8.0165, kgLoss = 0.1833
2025-04-09 09:47:26.992251: Training Step 38/115: batchLoss = 4.3019, diffLoss = 7.0596, kgLoss = 0.1653
2025-04-09 09:47:27.727721: Training Step 39/115: batchLoss = 5.0923, diffLoss = 8.3530, kgLoss = 0.2011
2025-04-09 09:47:28.472113: Training Step 40/115: batchLoss = 4.3926, diffLoss = 7.2012, kgLoss = 0.1795
2025-04-09 09:47:29.198316: Training Step 41/115: batchLoss = 4.5712, diffLoss = 7.5007, kgLoss = 0.1769
2025-04-09 09:47:29.923035: Training Step 42/115: batchLoss = 5.2811, diffLoss = 8.6654, kgLoss = 0.2047
2025-04-09 09:47:30.666140: Training Step 43/115: batchLoss = 4.6815, diffLoss = 7.6785, kgLoss = 0.1860
2025-04-09 09:47:31.410756: Training Step 44/115: batchLoss = 4.8360, diffLoss = 7.9379, kgLoss = 0.1831
2025-04-09 09:47:32.144213: Training Step 45/115: batchLoss = 4.0601, diffLoss = 6.6542, kgLoss = 0.1690
2025-04-09 09:47:32.880844: Training Step 46/115: batchLoss = 4.8689, diffLoss = 7.9904, kgLoss = 0.1866
2025-04-09 09:47:33.638334: Training Step 47/115: batchLoss = 4.2779, diffLoss = 7.0121, kgLoss = 0.1765
2025-04-09 09:47:34.375903: Training Step 48/115: batchLoss = 4.7429, diffLoss = 7.7854, kgLoss = 0.1792
2025-04-09 09:47:35.108252: Training Step 49/115: batchLoss = 4.4429, diffLoss = 7.2838, kgLoss = 0.1815
2025-04-09 09:47:35.842969: Training Step 50/115: batchLoss = 4.5976, diffLoss = 7.5447, kgLoss = 0.1770
2025-04-09 09:47:36.579529: Training Step 51/115: batchLoss = 4.5501, diffLoss = 7.4671, kgLoss = 0.1746
2025-04-09 09:47:37.310084: Training Step 52/115: batchLoss = 4.3693, diffLoss = 7.1629, kgLoss = 0.1789
2025-04-09 09:47:38.045249: Training Step 53/115: batchLoss = 4.5004, diffLoss = 7.3830, kgLoss = 0.1765
2025-04-09 09:47:38.785430: Training Step 54/115: batchLoss = 4.2508, diffLoss = 6.9646, kgLoss = 0.1800
2025-04-09 09:47:39.533730: Training Step 55/115: batchLoss = 4.9560, diffLoss = 8.1369, kgLoss = 0.1846
2025-04-09 09:47:40.282030: Training Step 56/115: batchLoss = 4.1420, diffLoss = 6.7938, kgLoss = 0.1644
2025-04-09 09:47:41.024609: Training Step 57/115: batchLoss = 4.4582, diffLoss = 7.3168, kgLoss = 0.1702
2025-04-09 09:47:41.761627: Training Step 58/115: batchLoss = 5.0300, diffLoss = 8.2543, kgLoss = 0.1934
2025-04-09 09:47:42.501542: Training Step 59/115: batchLoss = 4.5551, diffLoss = 7.4687, kgLoss = 0.1847
2025-04-09 09:47:43.237040: Training Step 60/115: batchLoss = 4.6946, diffLoss = 7.7002, kgLoss = 0.1862
2025-04-09 09:47:43.969596: Training Step 61/115: batchLoss = 4.3279, diffLoss = 7.0968, kgLoss = 0.1745
2025-04-09 09:47:44.701923: Training Step 62/115: batchLoss = 4.6887, diffLoss = 7.6921, kgLoss = 0.1836
2025-04-09 09:47:45.455818: Training Step 63/115: batchLoss = 4.2961, diffLoss = 7.0424, kgLoss = 0.1767
2025-04-09 09:47:46.199989: Training Step 64/115: batchLoss = 4.4360, diffLoss = 7.2752, kgLoss = 0.1772
2025-04-09 09:47:46.964846: Training Step 65/115: batchLoss = 4.4199, diffLoss = 7.2514, kgLoss = 0.1727
2025-04-09 09:47:47.720135: Training Step 66/115: batchLoss = 4.3358, diffLoss = 7.1130, kgLoss = 0.1701
2025-04-09 09:47:48.459435: Training Step 67/115: batchLoss = 4.6528, diffLoss = 7.6308, kgLoss = 0.1857
2025-04-09 09:47:49.215041: Training Step 68/115: batchLoss = 4.9584, diffLoss = 8.1343, kgLoss = 0.1946
2025-04-09 09:47:49.944499: Training Step 69/115: batchLoss = 4.3053, diffLoss = 7.0663, kgLoss = 0.1639
2025-04-09 09:47:50.687355: Training Step 70/115: batchLoss = 4.1659, diffLoss = 6.8264, kgLoss = 0.1752
2025-04-09 09:47:51.456056: Training Step 71/115: batchLoss = 4.9940, diffLoss = 8.1971, kgLoss = 0.1893
2025-04-09 09:47:52.197221: Training Step 72/115: batchLoss = 4.5245, diffLoss = 7.4207, kgLoss = 0.1802
2025-04-09 09:47:52.933978: Training Step 73/115: batchLoss = 4.3648, diffLoss = 7.1496, kgLoss = 0.1877
2025-04-09 09:47:53.669734: Training Step 74/115: batchLoss = 3.8633, diffLoss = 6.3235, kgLoss = 0.1729
2025-04-09 09:47:54.411736: Training Step 75/115: batchLoss = 4.1987, diffLoss = 6.8848, kgLoss = 0.1695
2025-04-09 09:47:55.148022: Training Step 76/115: batchLoss = 4.3879, diffLoss = 7.1982, kgLoss = 0.1723
2025-04-09 09:47:55.878784: Training Step 77/115: batchLoss = 4.3733, diffLoss = 7.1822, kgLoss = 0.1600
2025-04-09 09:47:56.622161: Training Step 78/115: batchLoss = 4.0076, diffLoss = 6.5750, kgLoss = 0.1566
2025-04-09 09:47:57.357252: Training Step 79/115: batchLoss = 4.1593, diffLoss = 6.8163, kgLoss = 0.1737
2025-04-09 09:47:58.077511: Training Step 80/115: batchLoss = 4.7210, diffLoss = 7.7403, kgLoss = 0.1921
2025-04-09 09:47:58.811422: Training Step 81/115: batchLoss = 4.6758, diffLoss = 7.6720, kgLoss = 0.1814
2025-04-09 09:47:59.539014: Training Step 82/115: batchLoss = 3.9552, diffLoss = 6.4868, kgLoss = 0.1578
2025-04-09 09:48:00.274180: Training Step 83/115: batchLoss = 5.0880, diffLoss = 8.3496, kgLoss = 0.1956
2025-04-09 09:48:01.011349: Training Step 84/115: batchLoss = 4.0844, diffLoss = 6.6910, kgLoss = 0.1745
2025-04-09 09:48:01.740111: Training Step 85/115: batchLoss = 4.5902, diffLoss = 7.5316, kgLoss = 0.1781
2025-04-09 09:48:02.473781: Training Step 86/115: batchLoss = 4.9988, diffLoss = 8.2073, kgLoss = 0.1862
2025-04-09 09:48:03.208459: Training Step 87/115: batchLoss = 4.7385, diffLoss = 7.7716, kgLoss = 0.1887
2025-04-09 09:48:03.948327: Training Step 88/115: batchLoss = 4.6034, diffLoss = 7.5579, kgLoss = 0.1718
2025-04-09 09:48:04.683536: Training Step 89/115: batchLoss = 4.1726, diffLoss = 6.8457, kgLoss = 0.1629
2025-04-09 09:48:05.421558: Training Step 90/115: batchLoss = 4.2774, diffLoss = 7.0149, kgLoss = 0.1710
2025-04-09 09:48:06.162234: Training Step 91/115: batchLoss = 4.0115, diffLoss = 6.5784, kgLoss = 0.1611
2025-04-09 09:48:06.904676: Training Step 92/115: batchLoss = 4.3930, diffLoss = 7.2049, kgLoss = 0.1751
2025-04-09 09:48:07.639004: Training Step 93/115: batchLoss = 3.9523, diffLoss = 6.4786, kgLoss = 0.1627
2025-04-09 09:48:08.387603: Training Step 94/115: batchLoss = 4.2732, diffLoss = 7.0094, kgLoss = 0.1688
2025-04-09 09:48:09.120279: Training Step 95/115: batchLoss = 4.8086, diffLoss = 7.8882, kgLoss = 0.1894
2025-04-09 09:48:09.852596: Training Step 96/115: batchLoss = 4.0655, diffLoss = 6.6627, kgLoss = 0.1697
2025-04-09 09:48:10.580064: Training Step 97/115: batchLoss = 4.7395, diffLoss = 7.7765, kgLoss = 0.1840
2025-04-09 09:48:11.302878: Training Step 98/115: batchLoss = 4.2052, diffLoss = 6.8856, kgLoss = 0.1846
2025-04-09 09:48:12.026689: Training Step 99/115: batchLoss = 5.0786, diffLoss = 8.3400, kgLoss = 0.1866
2025-04-09 09:48:12.756687: Training Step 100/115: batchLoss = 4.3919, diffLoss = 7.1986, kgLoss = 0.1818
2025-04-09 09:48:13.482085: Training Step 101/115: batchLoss = 4.4556, diffLoss = 7.3107, kgLoss = 0.1730
2025-04-09 09:48:14.209695: Training Step 102/115: batchLoss = 4.1413, diffLoss = 6.7887, kgLoss = 0.1702
2025-04-09 09:48:14.945963: Training Step 103/115: batchLoss = 4.3395, diffLoss = 7.1168, kgLoss = 0.1736
2025-04-09 09:48:15.678708: Training Step 104/115: batchLoss = 4.0479, diffLoss = 6.6390, kgLoss = 0.1612
2025-04-09 09:48:16.413555: Training Step 105/115: batchLoss = 4.7343, diffLoss = 7.7613, kgLoss = 0.1937
2025-04-09 09:48:17.155850: Training Step 106/115: batchLoss = 5.0634, diffLoss = 8.3015, kgLoss = 0.2062
2025-04-09 09:48:17.886206: Training Step 107/115: batchLoss = 4.4698, diffLoss = 7.3339, kgLoss = 0.1738
2025-04-09 09:48:18.616107: Training Step 108/115: batchLoss = 5.2917, diffLoss = 8.6840, kgLoss = 0.2033
2025-04-09 09:48:19.347100: Training Step 109/115: batchLoss = 4.3031, diffLoss = 7.0576, kgLoss = 0.1714
2025-04-09 09:48:20.080436: Training Step 110/115: batchLoss = 4.6978, diffLoss = 7.7085, kgLoss = 0.1816
2025-04-09 09:48:20.820656: Training Step 111/115: batchLoss = 4.2391, diffLoss = 6.9571, kgLoss = 0.1621
2025-04-09 09:48:21.555078: Training Step 112/115: batchLoss = 4.4487, diffLoss = 7.2991, kgLoss = 0.1729
2025-04-09 09:48:22.214046: Training Step 113/115: batchLoss = 4.1420, diffLoss = 6.7922, kgLoss = 0.1666
2025-04-09 09:48:22.858876: Training Step 114/115: batchLoss = 4.6129, diffLoss = 7.5713, kgLoss = 0.1753
2025-04-09 09:48:22.981339: 
2025-04-09 09:48:22.982509: Epoch 13/1000, Train: epLoss = 1.2937, epDfLoss = 2.1222, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:48:23.745166: Steps 0/90: batch_recall = 53.72, batch_ndcg = 41.83 
2025-04-09 09:48:24.497690: Steps 1/90: batch_recall = 55.19, batch_ndcg = 39.30 
2025-04-09 09:48:25.244907: Steps 2/90: batch_recall = 47.85, batch_ndcg = 33.85 
2025-04-09 09:48:25.994074: Steps 3/90: batch_recall = 45.66, batch_ndcg = 31.74 
2025-04-09 09:48:26.728838: Steps 4/90: batch_recall = 49.91, batch_ndcg = 35.32 
2025-04-09 09:48:27.473427: Steps 5/90: batch_recall = 33.91, batch_ndcg = 24.49 
2025-04-09 09:48:28.206913: Steps 6/90: batch_recall = 46.55, batch_ndcg = 34.02 
2025-04-09 09:48:28.959606: Steps 7/90: batch_recall = 40.64, batch_ndcg = 29.97 
2025-04-09 09:48:29.705894: Steps 8/90: batch_recall = 43.57, batch_ndcg = 31.55 
2025-04-09 09:48:30.467394: Steps 9/90: batch_recall = 40.44, batch_ndcg = 29.97 
2025-04-09 09:48:31.220075: Steps 10/90: batch_recall = 36.39, batch_ndcg = 27.07 
2025-04-09 09:48:31.956084: Steps 11/90: batch_recall = 44.58, batch_ndcg = 27.48 
2025-04-09 09:48:32.695896: Steps 12/90: batch_recall = 35.94, batch_ndcg = 24.85 
2025-04-09 09:48:33.425234: Steps 13/90: batch_recall = 32.73, batch_ndcg = 22.73 
2025-04-09 09:48:34.160976: Steps 14/90: batch_recall = 33.15, batch_ndcg = 24.82 
2025-04-09 09:48:34.901391: Steps 15/90: batch_recall = 43.15, batch_ndcg = 28.44 
2025-04-09 09:48:35.644652: Steps 16/90: batch_recall = 32.73, batch_ndcg = 23.78 
2025-04-09 09:48:36.370249: Steps 17/90: batch_recall = 31.55, batch_ndcg = 21.09 
2025-04-09 09:48:37.098838: Steps 18/90: batch_recall = 35.47, batch_ndcg = 23.66 
2025-04-09 09:48:37.831071: Steps 19/90: batch_recall = 30.51, batch_ndcg = 21.25 
2025-04-09 09:48:38.544803: Steps 20/90: batch_recall = 33.77, batch_ndcg = 21.95 
2025-04-09 09:48:39.270185: Steps 21/90: batch_recall = 35.70, batch_ndcg = 25.34 
2025-04-09 09:48:39.986529: Steps 22/90: batch_recall = 38.79, batch_ndcg = 28.18 
2025-04-09 09:48:40.698300: Steps 23/90: batch_recall = 32.45, batch_ndcg = 22.08 
2025-04-09 09:48:41.406989: Steps 24/90: batch_recall = 35.91, batch_ndcg = 24.26 
2025-04-09 09:48:42.134446: Steps 25/90: batch_recall = 34.22, batch_ndcg = 22.73 
2025-04-09 09:48:42.848418: Steps 26/90: batch_recall = 33.84, batch_ndcg = 23.76 
2025-04-09 09:48:43.562673: Steps 27/90: batch_recall = 31.18, batch_ndcg = 21.65 
2025-04-09 09:48:44.291893: Steps 28/90: batch_recall = 30.45, batch_ndcg = 21.89 
2025-04-09 09:48:45.028740: Steps 29/90: batch_recall = 30.26, batch_ndcg = 20.34 
2025-04-09 09:48:45.762834: Steps 30/90: batch_recall = 28.57, batch_ndcg = 17.60 
2025-04-09 09:48:46.510617: Steps 31/90: batch_recall = 29.22, batch_ndcg = 20.12 
2025-04-09 09:48:47.247037: Steps 32/90: batch_recall = 33.34, batch_ndcg = 21.99 
2025-04-09 09:48:47.963036: Steps 33/90: batch_recall = 28.88, batch_ndcg = 19.03 
2025-04-09 09:48:48.685836: Steps 34/90: batch_recall = 33.63, batch_ndcg = 22.49 
2025-04-09 09:48:49.399026: Steps 35/90: batch_recall = 31.60, batch_ndcg = 22.77 
2025-04-09 09:48:50.113799: Steps 36/90: batch_recall = 32.41, batch_ndcg = 21.97 
2025-04-09 09:48:50.827818: Steps 37/90: batch_recall = 29.43, batch_ndcg = 19.46 
2025-04-09 09:48:51.566558: Steps 38/90: batch_recall = 30.37, batch_ndcg = 19.91 
2025-04-09 09:48:52.309588: Steps 39/90: batch_recall = 29.96, batch_ndcg = 18.18 
2025-04-09 09:48:53.027706: Steps 40/90: batch_recall = 30.40, batch_ndcg = 19.97 
2025-04-09 09:48:53.752134: Steps 41/90: batch_recall = 30.06, batch_ndcg = 19.99 
2025-04-09 09:48:54.469721: Steps 42/90: batch_recall = 34.82, batch_ndcg = 24.91 
2025-04-09 09:48:55.176187: Steps 43/90: batch_recall = 28.69, batch_ndcg = 19.00 
2025-04-09 09:48:55.884172: Steps 44/90: batch_recall = 30.76, batch_ndcg = 19.60 
2025-04-09 09:48:56.575530: Steps 45/90: batch_recall = 31.24, batch_ndcg = 18.50 
2025-04-09 09:48:57.298931: Steps 46/90: batch_recall = 28.62, batch_ndcg = 17.30 
2025-04-09 09:48:57.991395: Steps 47/90: batch_recall = 27.04, batch_ndcg = 17.11 
2025-04-09 09:48:58.705674: Steps 48/90: batch_recall = 33.36, batch_ndcg = 20.24 
2025-04-09 09:48:59.428047: Steps 49/90: batch_recall = 23.42, batch_ndcg = 15.62 
2025-04-09 09:49:00.138847: Steps 50/90: batch_recall = 32.06, batch_ndcg = 20.43 
2025-04-09 09:49:00.847884: Steps 51/90: batch_recall = 35.10, batch_ndcg = 22.43 
2025-04-09 09:49:01.559239: Steps 52/90: batch_recall = 27.97, batch_ndcg = 18.88 
2025-04-09 09:49:02.276405: Steps 53/90: batch_recall = 34.61, batch_ndcg = 20.39 
2025-04-09 09:49:03.007909: Steps 54/90: batch_recall = 25.10, batch_ndcg = 17.51 
2025-04-09 09:49:03.734444: Steps 55/90: batch_recall = 32.26, batch_ndcg = 19.93 
2025-04-09 09:49:04.458925: Steps 56/90: batch_recall = 27.41, batch_ndcg = 16.53 
2025-04-09 09:49:05.183280: Steps 57/90: batch_recall = 30.98, batch_ndcg = 19.40 
2025-04-09 09:49:05.892963: Steps 58/90: batch_recall = 36.21, batch_ndcg = 22.25 
2025-04-09 09:49:06.602564: Steps 59/90: batch_recall = 24.06, batch_ndcg = 15.57 
2025-04-09 09:49:07.310148: Steps 60/90: batch_recall = 24.39, batch_ndcg = 15.72 
2025-04-09 09:49:08.035955: Steps 61/90: batch_recall = 37.31, batch_ndcg = 24.68 
2025-04-09 09:49:08.752539: Steps 62/90: batch_recall = 33.76, batch_ndcg = 21.72 
2025-04-09 09:49:09.469790: Steps 63/90: batch_recall = 34.09, batch_ndcg = 21.03 
2025-04-09 09:49:10.177059: Steps 64/90: batch_recall = 32.74, batch_ndcg = 19.04 
2025-04-09 09:49:10.887279: Steps 65/90: batch_recall = 30.46, batch_ndcg = 19.39 
2025-04-09 09:49:11.584255: Steps 66/90: batch_recall = 31.71, batch_ndcg = 20.44 
2025-04-09 09:49:12.282935: Steps 67/90: batch_recall = 28.95, batch_ndcg = 17.11 
2025-04-09 09:49:12.977981: Steps 68/90: batch_recall = 29.73, batch_ndcg = 18.96 
2025-04-09 09:49:13.685223: Steps 69/90: batch_recall = 31.98, batch_ndcg = 20.18 
2025-04-09 09:49:14.393676: Steps 70/90: batch_recall = 31.23, batch_ndcg = 18.94 
2025-04-09 09:49:15.095421: Steps 71/90: batch_recall = 32.31, batch_ndcg = 20.12 
2025-04-09 09:49:15.800013: Steps 72/90: batch_recall = 26.21, batch_ndcg = 16.59 
2025-04-09 09:49:16.518791: Steps 73/90: batch_recall = 31.99, batch_ndcg = 19.50 
2025-04-09 09:49:17.220982: Steps 74/90: batch_recall = 33.21, batch_ndcg = 21.31 
2025-04-09 09:49:17.936979: Steps 75/90: batch_recall = 35.02, batch_ndcg = 21.94 
2025-04-09 09:49:18.673831: Steps 76/90: batch_recall = 34.96, batch_ndcg = 20.76 
2025-04-09 09:49:19.391990: Steps 77/90: batch_recall = 33.69, batch_ndcg = 19.91 
2025-04-09 09:49:20.101169: Steps 78/90: batch_recall = 27.10, batch_ndcg = 16.92 
2025-04-09 09:49:20.801516: Steps 79/90: batch_recall = 33.80, batch_ndcg = 23.25 
2025-04-09 09:49:21.517904: Steps 80/90: batch_recall = 38.70, batch_ndcg = 24.16 
2025-04-09 09:49:22.240506: Steps 81/90: batch_recall = 37.56, batch_ndcg = 22.63 
2025-04-09 09:49:22.948143: Steps 82/90: batch_recall = 32.06, batch_ndcg = 21.05 
2025-04-09 09:49:23.642542: Steps 83/90: batch_recall = 32.60, batch_ndcg = 20.25 
2025-04-09 09:49:24.369185: Steps 84/90: batch_recall = 30.96, batch_ndcg = 18.38 
2025-04-09 09:49:25.073546: Steps 85/90: batch_recall = 33.57, batch_ndcg = 18.69 
2025-04-09 09:49:25.773735: Steps 86/90: batch_recall = 39.79, batch_ndcg = 23.90 
2025-04-09 09:49:26.481805: Steps 87/90: batch_recall = 37.46, batch_ndcg = 24.73 
2025-04-09 09:49:27.194458: Steps 88/90: batch_recall = 40.25, batch_ndcg = 23.74 
2025-04-09 09:49:27.755127: Steps 89/90: batch_recall = 35.52, batch_ndcg = 19.66 
2025-04-09 09:49:27.756181: Epoch 13/1000, Test: Recall = 0.0668, NDCG = 0.0438  

2025-04-09 09:49:28.795196: Training Step 0/115: batchLoss = 4.4809, diffLoss = 7.3535, kgLoss = 0.1721
2025-04-09 09:49:29.548827: Training Step 1/115: batchLoss = 4.2592, diffLoss = 6.9922, kgLoss = 0.1596
2025-04-09 09:49:30.296066: Training Step 2/115: batchLoss = 4.3921, diffLoss = 7.2072, kgLoss = 0.1695
2025-04-09 09:49:31.032272: Training Step 3/115: batchLoss = 4.8837, diffLoss = 8.0142, kgLoss = 0.1880
2025-04-09 09:49:31.776172: Training Step 4/115: batchLoss = 4.4573, diffLoss = 7.3033, kgLoss = 0.1884
2025-04-09 09:49:32.520138: Training Step 5/115: batchLoss = 4.6852, diffLoss = 7.6757, kgLoss = 0.1994
2025-04-09 09:49:33.255287: Training Step 6/115: batchLoss = 4.5419, diffLoss = 7.4546, kgLoss = 0.1729
2025-04-09 09:49:33.993522: Training Step 7/115: batchLoss = 4.3981, diffLoss = 7.2149, kgLoss = 0.1729
2025-04-09 09:49:34.747876: Training Step 8/115: batchLoss = 4.1699, diffLoss = 6.8423, kgLoss = 0.1613
2025-04-09 09:49:35.492803: Training Step 9/115: batchLoss = 4.4590, diffLoss = 7.3165, kgLoss = 0.1727
2025-04-09 09:49:36.236171: Training Step 10/115: batchLoss = 4.7063, diffLoss = 7.7103, kgLoss = 0.2003
2025-04-09 09:49:36.978733: Training Step 11/115: batchLoss = 4.4342, diffLoss = 7.2771, kgLoss = 0.1698
2025-04-09 09:49:37.721220: Training Step 12/115: batchLoss = 4.7070, diffLoss = 7.7259, kgLoss = 0.1787
2025-04-09 09:49:38.459499: Training Step 13/115: batchLoss = 4.7105, diffLoss = 7.7260, kgLoss = 0.1873
2025-04-09 09:49:39.196828: Training Step 14/115: batchLoss = 4.0903, diffLoss = 6.7040, kgLoss = 0.1698
2025-04-09 09:49:39.939608: Training Step 15/115: batchLoss = 4.6628, diffLoss = 7.6425, kgLoss = 0.1932
2025-04-09 09:49:40.671093: Training Step 16/115: batchLoss = 4.5352, diffLoss = 7.4412, kgLoss = 0.1760
2025-04-09 09:49:41.410494: Training Step 17/115: batchLoss = 4.4514, diffLoss = 7.2930, kgLoss = 0.1890
2025-04-09 09:49:42.150197: Training Step 18/115: batchLoss = 4.3032, diffLoss = 7.0669, kgLoss = 0.1577
2025-04-09 09:49:42.907514: Training Step 19/115: batchLoss = 4.5547, diffLoss = 7.4777, kgLoss = 0.1703
2025-04-09 09:49:43.664479: Training Step 20/115: batchLoss = 4.7689, diffLoss = 7.8252, kgLoss = 0.1844
2025-04-09 09:49:44.409307: Training Step 21/115: batchLoss = 4.3720, diffLoss = 7.1651, kgLoss = 0.1824
2025-04-09 09:49:45.161955: Training Step 22/115: batchLoss = 4.5911, diffLoss = 7.5356, kgLoss = 0.1743
2025-04-09 09:49:45.906986: Training Step 23/115: batchLoss = 4.9627, diffLoss = 8.1354, kgLoss = 0.2036
2025-04-09 09:49:46.655670: Training Step 24/115: batchLoss = 4.6965, diffLoss = 7.7038, kgLoss = 0.1857
2025-04-09 09:49:47.397266: Training Step 25/115: batchLoss = 4.7276, diffLoss = 7.7585, kgLoss = 0.1812
2025-04-09 09:49:48.131423: Training Step 26/115: batchLoss = 4.8398, diffLoss = 7.9409, kgLoss = 0.1881
2025-04-09 09:49:48.862146: Training Step 27/115: batchLoss = 4.4704, diffLoss = 7.3326, kgLoss = 0.1772
2025-04-09 09:49:49.593935: Training Step 28/115: batchLoss = 4.9261, diffLoss = 8.0813, kgLoss = 0.1932
2025-04-09 09:49:50.332592: Training Step 29/115: batchLoss = 4.2788, diffLoss = 7.0232, kgLoss = 0.1623
2025-04-09 09:49:51.072752: Training Step 30/115: batchLoss = 4.2389, diffLoss = 6.9524, kgLoss = 0.1685
2025-04-09 09:49:51.803054: Training Step 31/115: batchLoss = 4.6428, diffLoss = 7.6200, kgLoss = 0.1770
2025-04-09 09:49:52.540752: Training Step 32/115: batchLoss = 4.6164, diffLoss = 7.5744, kgLoss = 0.1794
2025-04-09 09:49:53.274553: Training Step 33/115: batchLoss = 4.1529, diffLoss = 6.8028, kgLoss = 0.1781
2025-04-09 09:49:54.015381: Training Step 34/115: batchLoss = 4.8365, diffLoss = 7.9216, kgLoss = 0.2088
2025-04-09 09:49:54.755583: Training Step 35/115: batchLoss = 4.5276, diffLoss = 7.4283, kgLoss = 0.1765
2025-04-09 09:49:55.482398: Training Step 36/115: batchLoss = 4.3005, diffLoss = 7.0518, kgLoss = 0.1737
2025-04-09 09:49:56.215749: Training Step 37/115: batchLoss = 4.3948, diffLoss = 7.2061, kgLoss = 0.1779
2025-04-09 09:49:56.976435: Training Step 38/115: batchLoss = 4.5426, diffLoss = 7.4544, kgLoss = 0.1750
2025-04-09 09:49:57.735193: Training Step 39/115: batchLoss = 5.3963, diffLoss = 8.8595, kgLoss = 0.2015
2025-04-09 09:49:58.474015: Training Step 40/115: batchLoss = 4.6657, diffLoss = 7.6569, kgLoss = 0.1789
2025-04-09 09:49:59.214639: Training Step 41/115: batchLoss = 4.5328, diffLoss = 7.4343, kgLoss = 0.1805
2025-04-09 09:49:59.958086: Training Step 42/115: batchLoss = 3.8570, diffLoss = 6.3127, kgLoss = 0.1733
2025-04-09 09:50:00.708986: Training Step 43/115: batchLoss = 4.6762, diffLoss = 7.6803, kgLoss = 0.1700
2025-04-09 09:50:01.445882: Training Step 44/115: batchLoss = 4.7110, diffLoss = 7.7341, kgLoss = 0.1764
2025-04-09 09:50:02.183042: Training Step 45/115: batchLoss = 4.1166, diffLoss = 6.7503, kgLoss = 0.1661
2025-04-09 09:50:02.907774: Training Step 46/115: batchLoss = 4.7644, diffLoss = 7.8220, kgLoss = 0.1780
2025-04-09 09:50:03.637253: Training Step 47/115: batchLoss = 4.6808, diffLoss = 7.6831, kgLoss = 0.1774
2025-04-09 09:50:04.370895: Training Step 48/115: batchLoss = 4.6155, diffLoss = 7.5698, kgLoss = 0.1840
2025-04-09 09:50:05.111070: Training Step 49/115: batchLoss = 4.9752, diffLoss = 8.1685, kgLoss = 0.1853
2025-04-09 09:50:05.844758: Training Step 50/115: batchLoss = 4.5035, diffLoss = 7.3854, kgLoss = 0.1806
2025-04-09 09:50:06.587366: Training Step 51/115: batchLoss = 3.9900, diffLoss = 6.5390, kgLoss = 0.1665
2025-04-09 09:50:07.320959: Training Step 52/115: batchLoss = 4.3578, diffLoss = 7.1458, kgLoss = 0.1757
2025-04-09 09:50:08.049513: Training Step 53/115: batchLoss = 3.8654, diffLoss = 6.3383, kgLoss = 0.1562
2025-04-09 09:50:08.786979: Training Step 54/115: batchLoss = 4.4357, diffLoss = 7.2827, kgLoss = 0.1651
2025-04-09 09:50:09.527318: Training Step 55/115: batchLoss = 4.0656, diffLoss = 6.6703, kgLoss = 0.1586
2025-04-09 09:50:10.272565: Training Step 56/115: batchLoss = 4.4393, diffLoss = 7.2788, kgLoss = 0.1802
2025-04-09 09:50:11.011187: Training Step 57/115: batchLoss = 3.9981, diffLoss = 6.5543, kgLoss = 0.1639
2025-04-09 09:50:11.743450: Training Step 58/115: batchLoss = 4.3299, diffLoss = 7.1005, kgLoss = 0.1741
2025-04-09 09:50:12.484515: Training Step 59/115: batchLoss = 4.6801, diffLoss = 7.6809, kgLoss = 0.1789
2025-04-09 09:50:13.221031: Training Step 60/115: batchLoss = 4.4232, diffLoss = 7.2567, kgLoss = 0.1730
2025-04-09 09:50:13.965710: Training Step 61/115: batchLoss = 4.0639, diffLoss = 6.6649, kgLoss = 0.1624
2025-04-09 09:50:14.706172: Training Step 62/115: batchLoss = 4.2829, diffLoss = 7.0212, kgLoss = 0.1754
2025-04-09 09:50:15.450751: Training Step 63/115: batchLoss = 4.7743, diffLoss = 7.8275, kgLoss = 0.1944
2025-04-09 09:50:16.183233: Training Step 64/115: batchLoss = 4.0784, diffLoss = 6.6920, kgLoss = 0.1581
2025-04-09 09:50:16.920321: Training Step 65/115: batchLoss = 4.5905, diffLoss = 7.5301, kgLoss = 0.1811
2025-04-09 09:50:17.679661: Training Step 66/115: batchLoss = 4.5044, diffLoss = 7.3927, kgLoss = 0.1720
2025-04-09 09:50:18.443360: Training Step 67/115: batchLoss = 4.3621, diffLoss = 7.1610, kgLoss = 0.1638
2025-04-09 09:50:19.187721: Training Step 68/115: batchLoss = 4.7362, diffLoss = 7.7756, kgLoss = 0.1770
2025-04-09 09:50:19.939210: Training Step 69/115: batchLoss = 4.3731, diffLoss = 7.1701, kgLoss = 0.1775
2025-04-09 09:50:20.669679: Training Step 70/115: batchLoss = 4.9278, diffLoss = 8.0901, kgLoss = 0.1842
2025-04-09 09:50:21.416811: Training Step 71/115: batchLoss = 4.1209, diffLoss = 6.7573, kgLoss = 0.1662
2025-04-09 09:50:22.145011: Training Step 72/115: batchLoss = 5.1361, diffLoss = 8.4309, kgLoss = 0.1941
2025-04-09 09:50:22.871232: Training Step 73/115: batchLoss = 4.6486, diffLoss = 7.6269, kgLoss = 0.1810
2025-04-09 09:50:23.604115: Training Step 74/115: batchLoss = 4.6615, diffLoss = 7.6304, kgLoss = 0.2080
2025-04-09 09:50:24.328049: Training Step 75/115: batchLoss = 4.4034, diffLoss = 7.2218, kgLoss = 0.1757
2025-04-09 09:50:25.060684: Training Step 76/115: batchLoss = 4.5495, diffLoss = 7.4600, kgLoss = 0.1837
2025-04-09 09:50:25.787119: Training Step 77/115: batchLoss = 4.4753, diffLoss = 7.3464, kgLoss = 0.1687
2025-04-09 09:50:26.516994: Training Step 78/115: batchLoss = 4.8525, diffLoss = 7.9586, kgLoss = 0.1934
2025-04-09 09:50:27.250279: Training Step 79/115: batchLoss = 4.7472, diffLoss = 7.7906, kgLoss = 0.1821
2025-04-09 09:50:27.994194: Training Step 80/115: batchLoss = 4.6897, diffLoss = 7.6952, kgLoss = 0.1816
2025-04-09 09:50:28.739948: Training Step 81/115: batchLoss = 3.8437, diffLoss = 6.3066, kgLoss = 0.1493
2025-04-09 09:50:29.472249: Training Step 82/115: batchLoss = 4.1225, diffLoss = 6.7605, kgLoss = 0.1655
2025-04-09 09:50:30.197388: Training Step 83/115: batchLoss = 5.2107, diffLoss = 8.5494, kgLoss = 0.2026
2025-04-09 09:50:30.931279: Training Step 84/115: batchLoss = 4.4911, diffLoss = 7.3647, kgLoss = 0.1806
2025-04-09 09:50:31.653385: Training Step 85/115: batchLoss = 4.7143, diffLoss = 7.7333, kgLoss = 0.1858
2025-04-09 09:50:32.375954: Training Step 86/115: batchLoss = 3.8759, diffLoss = 6.3532, kgLoss = 0.1601
2025-04-09 09:50:33.098841: Training Step 87/115: batchLoss = 4.0253, diffLoss = 6.6013, kgLoss = 0.1612
2025-04-09 09:50:33.821684: Training Step 88/115: batchLoss = 4.7406, diffLoss = 7.7780, kgLoss = 0.1845
2025-04-09 09:50:34.542818: Training Step 89/115: batchLoss = 3.9719, diffLoss = 6.5177, kgLoss = 0.1532
2025-04-09 09:50:35.270893: Training Step 90/115: batchLoss = 3.9947, diffLoss = 6.5453, kgLoss = 0.1688
2025-04-09 09:50:36.003323: Training Step 91/115: batchLoss = 4.2155, diffLoss = 6.9129, kgLoss = 0.1694
2025-04-09 09:50:36.719041: Training Step 92/115: batchLoss = 4.2023, diffLoss = 6.8879, kgLoss = 0.1740
2025-04-09 09:50:37.437020: Training Step 93/115: batchLoss = 4.3815, diffLoss = 7.1812, kgLoss = 0.1821
2025-04-09 09:50:38.161914: Training Step 94/115: batchLoss = 5.3294, diffLoss = 8.7337, kgLoss = 0.2229
2025-04-09 09:50:38.885295: Training Step 95/115: batchLoss = 4.2910, diffLoss = 7.0373, kgLoss = 0.1715
2025-04-09 09:50:39.612656: Training Step 96/115: batchLoss = 5.3359, diffLoss = 8.7469, kgLoss = 0.2195
2025-04-09 09:50:40.343346: Training Step 97/115: batchLoss = 4.6496, diffLoss = 7.6237, kgLoss = 0.1885
2025-04-09 09:50:41.058788: Training Step 98/115: batchLoss = 3.9623, diffLoss = 6.4934, kgLoss = 0.1657
2025-04-09 09:50:41.778853: Training Step 99/115: batchLoss = 4.7745, diffLoss = 7.8305, kgLoss = 0.1906
2025-04-09 09:50:42.510200: Training Step 100/115: batchLoss = 4.3778, diffLoss = 7.1835, kgLoss = 0.1692
2025-04-09 09:50:43.241667: Training Step 101/115: batchLoss = 4.2904, diffLoss = 7.0365, kgLoss = 0.1713
2025-04-09 09:50:43.964565: Training Step 102/115: batchLoss = 4.2129, diffLoss = 6.9087, kgLoss = 0.1692
2025-04-09 09:50:44.692100: Training Step 103/115: batchLoss = 3.9189, diffLoss = 6.4173, kgLoss = 0.1712
2025-04-09 09:50:45.421344: Training Step 104/115: batchLoss = 4.2803, diffLoss = 7.0219, kgLoss = 0.1678
2025-04-09 09:50:46.149232: Training Step 105/115: batchLoss = 4.3279, diffLoss = 7.0927, kgLoss = 0.1808
2025-04-09 09:50:46.890932: Training Step 106/115: batchLoss = 4.2558, diffLoss = 6.9804, kgLoss = 0.1690
2025-04-09 09:50:47.615962: Training Step 107/115: batchLoss = 4.8553, diffLoss = 7.9654, kgLoss = 0.1902
2025-04-09 09:50:48.336106: Training Step 108/115: batchLoss = 4.5878, diffLoss = 7.5294, kgLoss = 0.1755
2025-04-09 09:50:49.061873: Training Step 109/115: batchLoss = 4.8735, diffLoss = 7.9947, kgLoss = 0.1917
2025-04-09 09:50:49.786285: Training Step 110/115: batchLoss = 4.3273, diffLoss = 7.0984, kgLoss = 0.1706
2025-04-09 09:50:50.508285: Training Step 111/115: batchLoss = 4.4299, diffLoss = 7.2726, kgLoss = 0.1657
2025-04-09 09:50:51.216771: Training Step 112/115: batchLoss = 4.6420, diffLoss = 7.6172, kgLoss = 0.1792
2025-04-09 09:50:51.859857: Training Step 113/115: batchLoss = 4.2628, diffLoss = 6.9937, kgLoss = 0.1665
2025-04-09 09:50:52.485002: Training Step 114/115: batchLoss = 4.7646, diffLoss = 7.8157, kgLoss = 0.1879
2025-04-09 09:50:52.604644: 
2025-04-09 09:50:52.605543: Epoch 14/1000, Train: epLoss = 1.2909, epDfLoss = 2.1175, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:50:53.394333: Steps 0/90: batch_recall = 54.23, batch_ndcg = 41.52 
2025-04-09 09:50:54.138767: Steps 1/90: batch_recall = 54.89, batch_ndcg = 39.57 
2025-04-09 09:50:54.896388: Steps 2/90: batch_recall = 50.46, batch_ndcg = 35.34 
2025-04-09 09:50:55.637981: Steps 3/90: batch_recall = 46.79, batch_ndcg = 32.70 
2025-04-09 09:50:56.381364: Steps 4/90: batch_recall = 49.39, batch_ndcg = 35.64 
2025-04-09 09:50:57.116608: Steps 5/90: batch_recall = 34.31, batch_ndcg = 24.54 
2025-04-09 09:50:57.847988: Steps 6/90: batch_recall = 47.37, batch_ndcg = 34.41 
2025-04-09 09:50:58.595263: Steps 7/90: batch_recall = 41.59, batch_ndcg = 30.17 
2025-04-09 09:50:59.333639: Steps 8/90: batch_recall = 45.49, batch_ndcg = 31.99 
2025-04-09 09:51:00.089303: Steps 9/90: batch_recall = 40.52, batch_ndcg = 30.23 
2025-04-09 09:51:00.861551: Steps 10/90: batch_recall = 37.53, batch_ndcg = 27.65 
2025-04-09 09:51:01.605621: Steps 11/90: batch_recall = 44.33, batch_ndcg = 27.16 
2025-04-09 09:51:02.334917: Steps 12/90: batch_recall = 36.06, batch_ndcg = 24.86 
2025-04-09 09:51:03.066061: Steps 13/90: batch_recall = 33.79, batch_ndcg = 22.87 
2025-04-09 09:51:03.806583: Steps 14/90: batch_recall = 33.30, batch_ndcg = 24.95 
2025-04-09 09:51:04.534406: Steps 15/90: batch_recall = 42.80, batch_ndcg = 27.72 
2025-04-09 09:51:05.270815: Steps 16/90: batch_recall = 34.48, batch_ndcg = 24.38 
2025-04-09 09:51:05.998599: Steps 17/90: batch_recall = 31.68, batch_ndcg = 21.71 
2025-04-09 09:51:06.705177: Steps 18/90: batch_recall = 36.27, batch_ndcg = 24.29 
2025-04-09 09:51:07.439993: Steps 19/90: batch_recall = 31.12, batch_ndcg = 21.60 
2025-04-09 09:51:08.152299: Steps 20/90: batch_recall = 34.81, batch_ndcg = 21.73 
2025-04-09 09:51:08.889130: Steps 21/90: batch_recall = 36.55, batch_ndcg = 25.96 
2025-04-09 09:51:09.625688: Steps 22/90: batch_recall = 39.26, batch_ndcg = 28.40 
2025-04-09 09:51:10.347523: Steps 23/90: batch_recall = 32.46, batch_ndcg = 22.30 
2025-04-09 09:51:11.065006: Steps 24/90: batch_recall = 36.70, batch_ndcg = 24.49 
2025-04-09 09:51:11.813062: Steps 25/90: batch_recall = 35.14, batch_ndcg = 22.70 
2025-04-09 09:51:12.546930: Steps 26/90: batch_recall = 35.49, batch_ndcg = 24.56 
2025-04-09 09:51:13.277873: Steps 27/90: batch_recall = 31.92, batch_ndcg = 22.13 
2025-04-09 09:51:14.026970: Steps 28/90: batch_recall = 31.17, batch_ndcg = 22.21 
2025-04-09 09:51:14.746082: Steps 29/90: batch_recall = 29.96, batch_ndcg = 20.43 
2025-04-09 09:51:15.466558: Steps 30/90: batch_recall = 28.76, batch_ndcg = 17.94 
2025-04-09 09:51:16.203903: Steps 31/90: batch_recall = 28.94, batch_ndcg = 20.04 
2025-04-09 09:51:16.921782: Steps 32/90: batch_recall = 33.85, batch_ndcg = 21.63 
2025-04-09 09:51:17.643778: Steps 33/90: batch_recall = 30.21, batch_ndcg = 19.28 
2025-04-09 09:51:18.367265: Steps 34/90: batch_recall = 33.22, batch_ndcg = 22.50 
2025-04-09 09:51:19.070511: Steps 35/90: batch_recall = 32.88, batch_ndcg = 23.27 
2025-04-09 09:51:19.785506: Steps 36/90: batch_recall = 32.78, batch_ndcg = 22.03 
2025-04-09 09:51:20.494268: Steps 37/90: batch_recall = 29.63, batch_ndcg = 20.10 
2025-04-09 09:51:21.198645: Steps 38/90: batch_recall = 31.28, batch_ndcg = 20.37 
2025-04-09 09:51:21.916525: Steps 39/90: batch_recall = 30.48, batch_ndcg = 18.63 
2025-04-09 09:51:22.633346: Steps 40/90: batch_recall = 31.24, batch_ndcg = 20.57 
2025-04-09 09:51:23.346637: Steps 41/90: batch_recall = 30.65, batch_ndcg = 20.33 
2025-04-09 09:51:24.076383: Steps 42/90: batch_recall = 34.60, batch_ndcg = 25.06 
2025-04-09 09:51:24.781685: Steps 43/90: batch_recall = 28.43, batch_ndcg = 18.74 
2025-04-09 09:51:25.490679: Steps 44/90: batch_recall = 31.72, batch_ndcg = 19.89 
2025-04-09 09:51:26.190831: Steps 45/90: batch_recall = 31.33, batch_ndcg = 19.23 
2025-04-09 09:51:26.926787: Steps 46/90: batch_recall = 29.82, batch_ndcg = 17.71 
2025-04-09 09:51:27.644788: Steps 47/90: batch_recall = 26.75, batch_ndcg = 17.19 
2025-04-09 09:51:28.357631: Steps 48/90: batch_recall = 34.10, batch_ndcg = 20.67 
2025-04-09 09:51:29.080782: Steps 49/90: batch_recall = 23.67, batch_ndcg = 16.14 
2025-04-09 09:51:29.813980: Steps 50/90: batch_recall = 33.25, batch_ndcg = 21.30 
2025-04-09 09:51:30.537324: Steps 51/90: batch_recall = 36.76, batch_ndcg = 23.41 
2025-04-09 09:51:31.250255: Steps 52/90: batch_recall = 29.23, batch_ndcg = 19.24 
2025-04-09 09:51:31.951677: Steps 53/90: batch_recall = 34.55, batch_ndcg = 20.91 
2025-04-09 09:51:32.655426: Steps 54/90: batch_recall = 25.77, batch_ndcg = 17.62 
2025-04-09 09:51:33.371903: Steps 55/90: batch_recall = 33.32, batch_ndcg = 20.46 
2025-04-09 09:51:34.078899: Steps 56/90: batch_recall = 28.09, batch_ndcg = 16.97 
2025-04-09 09:51:34.794453: Steps 57/90: batch_recall = 31.24, batch_ndcg = 19.75 
2025-04-09 09:51:35.499432: Steps 58/90: batch_recall = 35.60, batch_ndcg = 21.98 
2025-04-09 09:51:36.217636: Steps 59/90: batch_recall = 24.63, batch_ndcg = 16.31 
2025-04-09 09:51:36.922833: Steps 60/90: batch_recall = 25.39, batch_ndcg = 16.38 
2025-04-09 09:51:37.620314: Steps 61/90: batch_recall = 38.54, batch_ndcg = 25.18 
2025-04-09 09:51:38.320416: Steps 62/90: batch_recall = 33.56, batch_ndcg = 21.68 
2025-04-09 09:51:39.040272: Steps 63/90: batch_recall = 34.21, batch_ndcg = 21.09 
2025-04-09 09:51:39.759390: Steps 64/90: batch_recall = 32.63, batch_ndcg = 19.30 
2025-04-09 09:51:40.467528: Steps 65/90: batch_recall = 30.25, batch_ndcg = 19.49 
2025-04-09 09:51:41.173887: Steps 66/90: batch_recall = 32.79, batch_ndcg = 20.75 
2025-04-09 09:51:41.882013: Steps 67/90: batch_recall = 29.61, batch_ndcg = 17.58 
2025-04-09 09:51:42.603439: Steps 68/90: batch_recall = 31.34, batch_ndcg = 19.37 
2025-04-09 09:51:43.337040: Steps 69/90: batch_recall = 33.11, batch_ndcg = 21.05 
2025-04-09 09:51:44.063267: Steps 70/90: batch_recall = 32.31, batch_ndcg = 19.76 
2025-04-09 09:51:44.812119: Steps 71/90: batch_recall = 31.78, batch_ndcg = 19.91 
2025-04-09 09:51:45.532670: Steps 72/90: batch_recall = 28.17, batch_ndcg = 17.52 
2025-04-09 09:51:46.249144: Steps 73/90: batch_recall = 33.37, batch_ndcg = 20.17 
2025-04-09 09:51:46.948146: Steps 74/90: batch_recall = 32.54, batch_ndcg = 21.18 
2025-04-09 09:51:47.675533: Steps 75/90: batch_recall = 36.07, batch_ndcg = 22.57 
2025-04-09 09:51:48.372155: Steps 76/90: batch_recall = 35.39, batch_ndcg = 21.48 
2025-04-09 09:51:49.062663: Steps 77/90: batch_recall = 34.72, batch_ndcg = 20.67 
2025-04-09 09:51:49.759533: Steps 78/90: batch_recall = 28.04, batch_ndcg = 17.44 
2025-04-09 09:51:50.459338: Steps 79/90: batch_recall = 33.97, batch_ndcg = 23.22 
2025-04-09 09:51:51.146798: Steps 80/90: batch_recall = 40.63, batch_ndcg = 25.19 
2025-04-09 09:51:51.859252: Steps 81/90: batch_recall = 38.32, batch_ndcg = 23.38 
2025-04-09 09:51:52.574501: Steps 82/90: batch_recall = 33.02, batch_ndcg = 21.06 
2025-04-09 09:51:53.288334: Steps 83/90: batch_recall = 34.19, batch_ndcg = 21.12 
2025-04-09 09:51:53.989069: Steps 84/90: batch_recall = 31.20, batch_ndcg = 18.67 
2025-04-09 09:51:54.678923: Steps 85/90: batch_recall = 33.43, batch_ndcg = 18.82 
2025-04-09 09:51:55.396064: Steps 86/90: batch_recall = 39.69, batch_ndcg = 23.81 
2025-04-09 09:51:56.113756: Steps 87/90: batch_recall = 37.51, batch_ndcg = 24.87 
2025-04-09 09:51:56.826270: Steps 88/90: batch_recall = 41.02, batch_ndcg = 24.52 
2025-04-09 09:51:57.390411: Steps 89/90: batch_recall = 36.27, batch_ndcg = 20.45 
2025-04-09 09:51:57.391321: Epoch 14/1000, Test: Recall = 0.0679, NDCG = 0.0445  

2025-04-09 09:51:58.426331: Training Step 0/115: batchLoss = 4.6235, diffLoss = 7.5907, kgLoss = 0.1727
2025-04-09 09:51:59.168100: Training Step 1/115: batchLoss = 4.6320, diffLoss = 7.5995, kgLoss = 0.1807
2025-04-09 09:51:59.903731: Training Step 2/115: batchLoss = 4.2284, diffLoss = 6.9371, kgLoss = 0.1653
2025-04-09 09:52:00.638850: Training Step 3/115: batchLoss = 4.3747, diffLoss = 7.1804, kgLoss = 0.1662
2025-04-09 09:52:01.368997: Training Step 4/115: batchLoss = 4.6578, diffLoss = 7.6366, kgLoss = 0.1896
2025-04-09 09:52:02.100931: Training Step 5/115: batchLoss = 4.8917, diffLoss = 8.0340, kgLoss = 0.1782
2025-04-09 09:52:02.861792: Training Step 6/115: batchLoss = 4.4914, diffLoss = 7.3649, kgLoss = 0.1812
2025-04-09 09:52:03.624152: Training Step 7/115: batchLoss = 4.5284, diffLoss = 7.4330, kgLoss = 0.1715
2025-04-09 09:52:04.377492: Training Step 8/115: batchLoss = 4.7469, diffLoss = 7.7874, kgLoss = 0.1862
2025-04-09 09:52:05.120406: Training Step 9/115: batchLoss = 4.2975, diffLoss = 7.0495, kgLoss = 0.1696
2025-04-09 09:52:05.854805: Training Step 10/115: batchLoss = 5.1408, diffLoss = 8.4308, kgLoss = 0.2058
2025-04-09 09:52:06.591474: Training Step 11/115: batchLoss = 4.7523, diffLoss = 7.7897, kgLoss = 0.1960
2025-04-09 09:52:07.331401: Training Step 12/115: batchLoss = 4.4762, diffLoss = 7.3471, kgLoss = 0.1698
2025-04-09 09:52:08.068959: Training Step 13/115: batchLoss = 4.1077, diffLoss = 6.7244, kgLoss = 0.1826
2025-04-09 09:52:08.812134: Training Step 14/115: batchLoss = 4.2476, diffLoss = 6.9700, kgLoss = 0.1640
2025-04-09 09:52:09.548102: Training Step 15/115: batchLoss = 4.6125, diffLoss = 7.5718, kgLoss = 0.1735
2025-04-09 09:52:10.283328: Training Step 16/115: batchLoss = 4.5909, diffLoss = 7.5335, kgLoss = 0.1771
2025-04-09 09:52:11.028695: Training Step 17/115: batchLoss = 4.1760, diffLoss = 6.8509, kgLoss = 0.1638
2025-04-09 09:52:11.779244: Training Step 18/115: batchLoss = 4.3442, diffLoss = 7.1124, kgLoss = 0.1919
2025-04-09 09:52:12.505302: Training Step 19/115: batchLoss = 3.8993, diffLoss = 6.3882, kgLoss = 0.1660
2025-04-09 09:52:13.234102: Training Step 20/115: batchLoss = 4.0748, diffLoss = 6.6748, kgLoss = 0.1748
2025-04-09 09:52:13.967772: Training Step 21/115: batchLoss = 5.0455, diffLoss = 8.2798, kgLoss = 0.1941
2025-04-09 09:52:14.706139: Training Step 22/115: batchLoss = 5.1219, diffLoss = 8.4029, kgLoss = 0.2004
2025-04-09 09:52:15.451668: Training Step 23/115: batchLoss = 4.9617, diffLoss = 8.1429, kgLoss = 0.1898
2025-04-09 09:52:16.205954: Training Step 24/115: batchLoss = 4.5693, diffLoss = 7.4992, kgLoss = 0.1745
2025-04-09 09:52:16.945099: Training Step 25/115: batchLoss = 4.5879, diffLoss = 7.5244, kgLoss = 0.1830
2025-04-09 09:52:17.671378: Training Step 26/115: batchLoss = 4.3025, diffLoss = 7.0620, kgLoss = 0.1633
2025-04-09 09:52:18.406561: Training Step 27/115: batchLoss = 3.9224, diffLoss = 6.4290, kgLoss = 0.1625
2025-04-09 09:52:19.148392: Training Step 28/115: batchLoss = 4.7312, diffLoss = 7.7669, kgLoss = 0.1776
2025-04-09 09:52:19.891389: Training Step 29/115: batchLoss = 4.5455, diffLoss = 7.4457, kgLoss = 0.1953
2025-04-09 09:52:20.629236: Training Step 30/115: batchLoss = 4.7252, diffLoss = 7.7530, kgLoss = 0.1835
2025-04-09 09:52:21.367437: Training Step 31/115: batchLoss = 4.6060, diffLoss = 7.5627, kgLoss = 0.1711
2025-04-09 09:52:22.095487: Training Step 32/115: batchLoss = 4.4489, diffLoss = 7.2949, kgLoss = 0.1798
2025-04-09 09:52:22.833077: Training Step 33/115: batchLoss = 4.5584, diffLoss = 7.4750, kgLoss = 0.1836
2025-04-09 09:52:23.577167: Training Step 34/115: batchLoss = 3.8966, diffLoss = 6.3891, kgLoss = 0.1578
2025-04-09 09:52:24.315455: Training Step 35/115: batchLoss = 4.7171, diffLoss = 7.7403, kgLoss = 0.1824
2025-04-09 09:52:25.043310: Training Step 36/115: batchLoss = 4.7596, diffLoss = 7.8120, kgLoss = 0.1810
2025-04-09 09:52:25.783154: Training Step 37/115: batchLoss = 4.8439, diffLoss = 7.9500, kgLoss = 0.1846
2025-04-09 09:52:26.519511: Training Step 38/115: batchLoss = 4.3311, diffLoss = 7.1045, kgLoss = 0.1710
2025-04-09 09:52:27.268069: Training Step 39/115: batchLoss = 4.6278, diffLoss = 7.5823, kgLoss = 0.1962
2025-04-09 09:52:28.021048: Training Step 40/115: batchLoss = 4.4863, diffLoss = 7.3570, kgLoss = 0.1803
2025-04-09 09:52:28.763512: Training Step 41/115: batchLoss = 4.5542, diffLoss = 7.4711, kgLoss = 0.1788
2025-04-09 09:52:29.502249: Training Step 42/115: batchLoss = 4.7997, diffLoss = 7.8781, kgLoss = 0.1821
2025-04-09 09:52:30.237362: Training Step 43/115: batchLoss = 4.5047, diffLoss = 7.3795, kgLoss = 0.1925
2025-04-09 09:52:30.981929: Training Step 44/115: batchLoss = 4.4820, diffLoss = 7.3510, kgLoss = 0.1786
2025-04-09 09:52:31.722702: Training Step 45/115: batchLoss = 4.5340, diffLoss = 7.4391, kgLoss = 0.1763
2025-04-09 09:52:32.460571: Training Step 46/115: batchLoss = 5.2925, diffLoss = 8.6854, kgLoss = 0.2031
2025-04-09 09:52:33.204036: Training Step 47/115: batchLoss = 5.0175, diffLoss = 8.2380, kgLoss = 0.1868
2025-04-09 09:52:33.939622: Training Step 48/115: batchLoss = 4.0151, diffLoss = 6.5862, kgLoss = 0.1583
2025-04-09 09:52:34.679063: Training Step 49/115: batchLoss = 4.0263, diffLoss = 6.6083, kgLoss = 0.1533
2025-04-09 09:52:35.423859: Training Step 50/115: batchLoss = 4.7584, diffLoss = 7.8054, kgLoss = 0.1878
2025-04-09 09:52:36.183390: Training Step 51/115: batchLoss = 4.2633, diffLoss = 6.9932, kgLoss = 0.1685
2025-04-09 09:52:36.933840: Training Step 52/115: batchLoss = 4.5620, diffLoss = 7.4844, kgLoss = 0.1783
2025-04-09 09:52:37.681895: Training Step 53/115: batchLoss = 4.3814, diffLoss = 7.1873, kgLoss = 0.1725
2025-04-09 09:52:38.428679: Training Step 54/115: batchLoss = 4.2344, diffLoss = 6.9498, kgLoss = 0.1614
2025-04-09 09:52:39.151391: Training Step 55/115: batchLoss = 4.6849, diffLoss = 7.6832, kgLoss = 0.1875
2025-04-09 09:52:39.881139: Training Step 56/115: batchLoss = 4.2708, diffLoss = 7.0112, kgLoss = 0.1602
2025-04-09 09:52:40.625952: Training Step 57/115: batchLoss = 4.2283, diffLoss = 6.9372, kgLoss = 0.1649
2025-04-09 09:52:41.348222: Training Step 58/115: batchLoss = 4.5429, diffLoss = 7.4511, kgLoss = 0.1806
2025-04-09 09:52:42.075684: Training Step 59/115: batchLoss = 4.5252, diffLoss = 7.4154, kgLoss = 0.1899
2025-04-09 09:52:42.813357: Training Step 60/115: batchLoss = 4.0404, diffLoss = 6.6201, kgLoss = 0.1709
2025-04-09 09:52:43.552741: Training Step 61/115: batchLoss = 4.1232, diffLoss = 6.7561, kgLoss = 0.1739
2025-04-09 09:52:44.304904: Training Step 62/115: batchLoss = 4.0592, diffLoss = 6.6554, kgLoss = 0.1649
2025-04-09 09:52:45.040390: Training Step 63/115: batchLoss = 4.2364, diffLoss = 6.9505, kgLoss = 0.1651
2025-04-09 09:52:45.788625: Training Step 64/115: batchLoss = 4.8192, diffLoss = 7.9053, kgLoss = 0.1901
2025-04-09 09:52:46.532034: Training Step 65/115: batchLoss = 4.5072, diffLoss = 7.3937, kgLoss = 0.1775
2025-04-09 09:52:47.279188: Training Step 66/115: batchLoss = 4.7856, diffLoss = 7.8496, kgLoss = 0.1896
2025-04-09 09:52:48.027681: Training Step 67/115: batchLoss = 4.2586, diffLoss = 6.9868, kgLoss = 0.1662
2025-04-09 09:52:48.765942: Training Step 68/115: batchLoss = 4.0348, diffLoss = 6.6113, kgLoss = 0.1699
2025-04-09 09:52:49.499509: Training Step 69/115: batchLoss = 4.3448, diffLoss = 7.1265, kgLoss = 0.1722
2025-04-09 09:52:50.233008: Training Step 70/115: batchLoss = 4.6004, diffLoss = 7.5502, kgLoss = 0.1758
2025-04-09 09:52:50.975304: Training Step 71/115: batchLoss = 4.3595, diffLoss = 7.1504, kgLoss = 0.1732
2025-04-09 09:52:51.724356: Training Step 72/115: batchLoss = 4.8043, diffLoss = 7.8839, kgLoss = 0.1850
2025-04-09 09:52:52.459224: Training Step 73/115: batchLoss = 4.6101, diffLoss = 7.5690, kgLoss = 0.1716
2025-04-09 09:52:53.201137: Training Step 74/115: batchLoss = 4.6281, diffLoss = 7.5890, kgLoss = 0.1868
2025-04-09 09:52:53.953053: Training Step 75/115: batchLoss = 4.4242, diffLoss = 7.2574, kgLoss = 0.1744
2025-04-09 09:52:54.715297: Training Step 76/115: batchLoss = 4.1928, diffLoss = 6.8773, kgLoss = 0.1661
2025-04-09 09:52:55.464414: Training Step 77/115: batchLoss = 4.2197, diffLoss = 6.9241, kgLoss = 0.1631
2025-04-09 09:52:56.193352: Training Step 78/115: batchLoss = 4.3266, diffLoss = 7.0972, kgLoss = 0.1708
2025-04-09 09:52:56.933073: Training Step 79/115: batchLoss = 4.2546, diffLoss = 6.9815, kgLoss = 0.1643
2025-04-09 09:52:57.656994: Training Step 80/115: batchLoss = 4.8984, diffLoss = 8.0258, kgLoss = 0.2073
2025-04-09 09:52:58.388132: Training Step 81/115: batchLoss = 4.2354, diffLoss = 6.9471, kgLoss = 0.1679
2025-04-09 09:52:59.121706: Training Step 82/115: batchLoss = 4.3751, diffLoss = 7.1712, kgLoss = 0.1810
2025-04-09 09:52:59.855519: Training Step 83/115: batchLoss = 4.1554, diffLoss = 6.8122, kgLoss = 0.1703
2025-04-09 09:53:00.571819: Training Step 84/115: batchLoss = 4.9242, diffLoss = 8.0804, kgLoss = 0.1897
2025-04-09 09:53:01.304856: Training Step 85/115: batchLoss = 4.4353, diffLoss = 7.2785, kgLoss = 0.1705
2025-04-09 09:53:02.029852: Training Step 86/115: batchLoss = 4.8066, diffLoss = 7.8861, kgLoss = 0.1874
2025-04-09 09:53:02.762929: Training Step 87/115: batchLoss = 4.1860, diffLoss = 6.8614, kgLoss = 0.1728
2025-04-09 09:53:03.498402: Training Step 88/115: batchLoss = 4.4138, diffLoss = 7.2483, kgLoss = 0.1621
2025-04-09 09:53:04.224328: Training Step 89/115: batchLoss = 4.2014, diffLoss = 6.8878, kgLoss = 0.1719
2025-04-09 09:53:04.958748: Training Step 90/115: batchLoss = 3.9572, diffLoss = 6.4889, kgLoss = 0.1596
2025-04-09 09:53:05.696769: Training Step 91/115: batchLoss = 4.1776, diffLoss = 6.8542, kgLoss = 0.1626
2025-04-09 09:53:06.457712: Training Step 92/115: batchLoss = 4.4823, diffLoss = 7.3543, kgLoss = 0.1742
2025-04-09 09:53:07.214446: Training Step 93/115: batchLoss = 4.4261, diffLoss = 7.2660, kgLoss = 0.1663
2025-04-09 09:53:07.966284: Training Step 94/115: batchLoss = 5.3418, diffLoss = 8.7623, kgLoss = 0.2112
2025-04-09 09:53:08.714687: Training Step 95/115: batchLoss = 4.1120, diffLoss = 6.7457, kgLoss = 0.1614
2025-04-09 09:53:09.429430: Training Step 96/115: batchLoss = 4.7102, diffLoss = 7.7297, kgLoss = 0.1810
2025-04-09 09:53:10.148764: Training Step 97/115: batchLoss = 4.9652, diffLoss = 8.1425, kgLoss = 0.1992
2025-04-09 09:53:10.877575: Training Step 98/115: batchLoss = 4.2124, diffLoss = 6.9056, kgLoss = 0.1726
2025-04-09 09:53:11.613737: Training Step 99/115: batchLoss = 4.4526, diffLoss = 7.2960, kgLoss = 0.1875
2025-04-09 09:53:12.358784: Training Step 100/115: batchLoss = 4.2067, diffLoss = 6.9013, kgLoss = 0.1648
2025-04-09 09:53:13.079119: Training Step 101/115: batchLoss = 4.3143, diffLoss = 7.0819, kgLoss = 0.1631
2025-04-09 09:53:13.796208: Training Step 102/115: batchLoss = 4.4804, diffLoss = 7.3452, kgLoss = 0.1832
2025-04-09 09:53:14.519371: Training Step 103/115: batchLoss = 4.5880, diffLoss = 7.5271, kgLoss = 0.1793
2025-04-09 09:53:15.254014: Training Step 104/115: batchLoss = 4.4685, diffLoss = 7.3356, kgLoss = 0.1679
2025-04-09 09:53:15.998140: Training Step 105/115: batchLoss = 3.8582, diffLoss = 6.3204, kgLoss = 0.1648
2025-04-09 09:53:16.722647: Training Step 106/115: batchLoss = 4.6810, diffLoss = 7.6758, kgLoss = 0.1888
2025-04-09 09:53:17.452805: Training Step 107/115: batchLoss = 4.6371, diffLoss = 7.6131, kgLoss = 0.1732
2025-04-09 09:53:18.178828: Training Step 108/115: batchLoss = 5.1472, diffLoss = 8.4410, kgLoss = 0.2064
2025-04-09 09:53:18.912771: Training Step 109/115: batchLoss = 4.2834, diffLoss = 7.0271, kgLoss = 0.1677
2025-04-09 09:53:19.658636: Training Step 110/115: batchLoss = 4.1890, diffLoss = 6.8650, kgLoss = 0.1751
2025-04-09 09:53:20.408624: Training Step 111/115: batchLoss = 5.0749, diffLoss = 8.3200, kgLoss = 0.2071
2025-04-09 09:53:21.163779: Training Step 112/115: batchLoss = 4.7634, diffLoss = 7.8182, kgLoss = 0.1811
2025-04-09 09:53:21.807580: Training Step 113/115: batchLoss = 4.8137, diffLoss = 7.8930, kgLoss = 0.1947
2025-04-09 09:53:22.450488: Training Step 114/115: batchLoss = 4.6586, diffLoss = 7.6400, kgLoss = 0.1866
2025-04-09 09:53:22.566202: 
2025-04-09 09:53:22.566856: Epoch 15/1000, Train: epLoss = 1.2916, epDfLoss = 2.1185, epfTransLoss = 0.0000, epKgLoss = 0.0511  
2025-04-09 09:53:23.327127: Steps 0/90: batch_recall = 55.08, batch_ndcg = 42.22 
2025-04-09 09:53:24.071414: Steps 1/90: batch_recall = 56.08, batch_ndcg = 39.40 
2025-04-09 09:53:24.814192: Steps 2/90: batch_recall = 51.42, batch_ndcg = 36.55 
2025-04-09 09:53:25.554184: Steps 3/90: batch_recall = 46.19, batch_ndcg = 32.28 
2025-04-09 09:53:26.313198: Steps 4/90: batch_recall = 49.41, batch_ndcg = 35.79 
2025-04-09 09:53:27.060577: Steps 5/90: batch_recall = 34.62, batch_ndcg = 24.60 
2025-04-09 09:53:27.786815: Steps 6/90: batch_recall = 46.24, batch_ndcg = 34.25 
2025-04-09 09:53:28.535332: Steps 7/90: batch_recall = 40.13, batch_ndcg = 29.48 
2025-04-09 09:53:29.276737: Steps 8/90: batch_recall = 45.70, batch_ndcg = 32.32 
2025-04-09 09:53:30.025630: Steps 9/90: batch_recall = 40.40, batch_ndcg = 30.57 
2025-04-09 09:53:30.765984: Steps 10/90: batch_recall = 37.20, batch_ndcg = 27.51 
2025-04-09 09:53:31.493496: Steps 11/90: batch_recall = 44.34, batch_ndcg = 27.04 
2025-04-09 09:53:32.209763: Steps 12/90: batch_recall = 35.55, batch_ndcg = 25.36 
2025-04-09 09:53:32.951795: Steps 13/90: batch_recall = 34.89, batch_ndcg = 23.69 
2025-04-09 09:53:33.686844: Steps 14/90: batch_recall = 32.98, batch_ndcg = 25.00 
2025-04-09 09:53:34.420696: Steps 15/90: batch_recall = 42.16, batch_ndcg = 27.89 
2025-04-09 09:53:35.162505: Steps 16/90: batch_recall = 35.24, batch_ndcg = 24.45 
2025-04-09 09:53:35.899758: Steps 17/90: batch_recall = 32.24, batch_ndcg = 21.98 
2025-04-09 09:53:36.612808: Steps 18/90: batch_recall = 36.38, batch_ndcg = 24.58 
2025-04-09 09:53:37.351903: Steps 19/90: batch_recall = 32.54, batch_ndcg = 22.30 
2025-04-09 09:53:38.070261: Steps 20/90: batch_recall = 34.29, batch_ndcg = 22.03 
2025-04-09 09:53:38.795549: Steps 21/90: batch_recall = 37.44, batch_ndcg = 26.03 
2025-04-09 09:53:39.520423: Steps 22/90: batch_recall = 39.99, batch_ndcg = 28.49 
2025-04-09 09:53:40.230161: Steps 23/90: batch_recall = 33.36, batch_ndcg = 22.58 
2025-04-09 09:53:40.951844: Steps 24/90: batch_recall = 37.42, batch_ndcg = 24.90 
2025-04-09 09:53:41.669444: Steps 25/90: batch_recall = 34.93, batch_ndcg = 22.75 
2025-04-09 09:53:42.389306: Steps 26/90: batch_recall = 36.12, batch_ndcg = 24.98 
2025-04-09 09:53:43.099549: Steps 27/90: batch_recall = 32.44, batch_ndcg = 22.41 
2025-04-09 09:53:43.824442: Steps 28/90: batch_recall = 31.90, batch_ndcg = 22.68 
2025-04-09 09:53:44.550147: Steps 29/90: batch_recall = 30.43, batch_ndcg = 20.73 
2025-04-09 09:53:45.273758: Steps 30/90: batch_recall = 29.15, batch_ndcg = 18.32 
2025-04-09 09:53:46.011283: Steps 31/90: batch_recall = 30.57, batch_ndcg = 20.51 
2025-04-09 09:53:46.749377: Steps 32/90: batch_recall = 33.36, batch_ndcg = 22.04 
2025-04-09 09:53:47.461054: Steps 33/90: batch_recall = 30.20, batch_ndcg = 19.68 
2025-04-09 09:53:48.181132: Steps 34/90: batch_recall = 32.50, batch_ndcg = 22.34 
2025-04-09 09:53:48.899421: Steps 35/90: batch_recall = 34.94, batch_ndcg = 24.58 
2025-04-09 09:53:49.615207: Steps 36/90: batch_recall = 35.28, batch_ndcg = 22.79 
2025-04-09 09:53:50.324343: Steps 37/90: batch_recall = 29.17, batch_ndcg = 19.69 
2025-04-09 09:53:51.053978: Steps 38/90: batch_recall = 32.25, batch_ndcg = 20.52 
2025-04-09 09:53:51.782204: Steps 39/90: batch_recall = 30.62, batch_ndcg = 18.60 
2025-04-09 09:53:52.505329: Steps 40/90: batch_recall = 31.69, batch_ndcg = 20.81 
2025-04-09 09:53:53.223338: Steps 41/90: batch_recall = 30.16, batch_ndcg = 20.36 
2025-04-09 09:53:53.940863: Steps 42/90: batch_recall = 35.17, batch_ndcg = 25.92 
2025-04-09 09:53:54.658496: Steps 43/90: batch_recall = 30.42, batch_ndcg = 19.52 
2025-04-09 09:53:55.361746: Steps 44/90: batch_recall = 31.42, batch_ndcg = 19.89 
2025-04-09 09:53:56.070946: Steps 45/90: batch_recall = 31.85, batch_ndcg = 19.23 
2025-04-09 09:53:56.787213: Steps 46/90: batch_recall = 28.87, batch_ndcg = 17.68 
2025-04-09 09:53:57.491224: Steps 47/90: batch_recall = 27.04, batch_ndcg = 17.19 
2025-04-09 09:53:58.193700: Steps 48/90: batch_recall = 33.19, batch_ndcg = 20.65 
2025-04-09 09:53:58.888004: Steps 49/90: batch_recall = 23.94, batch_ndcg = 16.34 
2025-04-09 09:53:59.586182: Steps 50/90: batch_recall = 34.21, batch_ndcg = 22.12 
2025-04-09 09:54:00.283567: Steps 51/90: batch_recall = 35.42, batch_ndcg = 22.81 
2025-04-09 09:54:00.992324: Steps 52/90: batch_recall = 30.37, batch_ndcg = 19.65 
2025-04-09 09:54:01.713564: Steps 53/90: batch_recall = 37.09, batch_ndcg = 21.99 
2025-04-09 09:54:02.428860: Steps 54/90: batch_recall = 26.26, batch_ndcg = 17.86 
2025-04-09 09:54:03.149777: Steps 55/90: batch_recall = 34.61, batch_ndcg = 21.12 
2025-04-09 09:54:03.860643: Steps 56/90: batch_recall = 28.13, batch_ndcg = 17.27 
2025-04-09 09:54:04.593578: Steps 57/90: batch_recall = 32.80, batch_ndcg = 20.64 
2025-04-09 09:54:05.296347: Steps 58/90: batch_recall = 36.79, batch_ndcg = 22.88 
2025-04-09 09:54:06.000543: Steps 59/90: batch_recall = 25.89, batch_ndcg = 17.03 
2025-04-09 09:54:06.719494: Steps 60/90: batch_recall = 26.84, batch_ndcg = 17.07 
2025-04-09 09:54:07.433540: Steps 61/90: batch_recall = 38.70, batch_ndcg = 25.27 
2025-04-09 09:54:08.147231: Steps 62/90: batch_recall = 34.82, batch_ndcg = 22.28 
2025-04-09 09:54:08.864647: Steps 63/90: batch_recall = 36.10, batch_ndcg = 21.84 
2025-04-09 09:54:09.572765: Steps 64/90: batch_recall = 33.43, batch_ndcg = 19.78 
2025-04-09 09:54:10.272298: Steps 65/90: batch_recall = 30.33, batch_ndcg = 19.46 
2025-04-09 09:54:10.978805: Steps 66/90: batch_recall = 33.16, batch_ndcg = 20.53 
2025-04-09 09:54:11.672530: Steps 67/90: batch_recall = 30.38, batch_ndcg = 18.01 
2025-04-09 09:54:12.376807: Steps 68/90: batch_recall = 31.37, batch_ndcg = 20.12 
2025-04-09 09:54:13.063338: Steps 69/90: batch_recall = 33.86, batch_ndcg = 21.11 
2025-04-09 09:54:13.767044: Steps 70/90: batch_recall = 32.38, batch_ndcg = 19.88 
2025-04-09 09:54:14.459705: Steps 71/90: batch_recall = 32.18, batch_ndcg = 20.06 
2025-04-09 09:54:15.161813: Steps 72/90: batch_recall = 30.52, batch_ndcg = 18.63 
2025-04-09 09:54:15.872360: Steps 73/90: batch_recall = 34.82, batch_ndcg = 21.26 
2025-04-09 09:54:16.588061: Steps 74/90: batch_recall = 34.17, batch_ndcg = 21.98 
2025-04-09 09:54:17.287567: Steps 75/90: batch_recall = 36.59, batch_ndcg = 22.79 
2025-04-09 09:54:17.984778: Steps 76/90: batch_recall = 36.67, batch_ndcg = 21.86 
2025-04-09 09:54:18.674650: Steps 77/90: batch_recall = 36.48, batch_ndcg = 22.15 
2025-04-09 09:54:19.375573: Steps 78/90: batch_recall = 28.85, batch_ndcg = 17.83 
2025-04-09 09:54:20.090191: Steps 79/90: batch_recall = 35.17, batch_ndcg = 23.75 
2025-04-09 09:54:20.809792: Steps 80/90: batch_recall = 39.22, batch_ndcg = 24.62 
2025-04-09 09:54:21.517988: Steps 81/90: batch_recall = 40.39, batch_ndcg = 23.94 
2025-04-09 09:54:22.221730: Steps 82/90: batch_recall = 34.36, batch_ndcg = 21.42 
2025-04-09 09:54:22.914153: Steps 83/90: batch_recall = 34.74, batch_ndcg = 21.86 
2025-04-09 09:54:23.627894: Steps 84/90: batch_recall = 32.11, batch_ndcg = 19.16 
2025-04-09 09:54:24.319396: Steps 85/90: batch_recall = 33.76, batch_ndcg = 19.45 
2025-04-09 09:54:25.010543: Steps 86/90: batch_recall = 40.15, batch_ndcg = 24.63 
2025-04-09 09:54:25.719978: Steps 87/90: batch_recall = 36.80, batch_ndcg = 24.90 
2025-04-09 09:54:26.432813: Steps 88/90: batch_recall = 41.14, batch_ndcg = 24.65 
2025-04-09 09:54:26.993456: Steps 89/90: batch_recall = 36.59, batch_ndcg = 21.21 
2025-04-09 09:54:26.994274: Epoch 15/1000, Test: Recall = 0.0690, NDCG = 0.0452  

2025-04-09 09:54:28.007803: Training Step 0/115: batchLoss = 4.3274, diffLoss = 7.0958, kgLoss = 0.1747
2025-04-09 09:54:28.736049: Training Step 1/115: batchLoss = 4.5244, diffLoss = 7.4187, kgLoss = 0.1829
2025-04-09 09:54:29.464075: Training Step 2/115: batchLoss = 4.4860, diffLoss = 7.3628, kgLoss = 0.1709
2025-04-09 09:54:30.192680: Training Step 3/115: batchLoss = 4.7712, diffLoss = 7.8271, kgLoss = 0.1872
2025-04-09 09:54:30.914805: Training Step 4/115: batchLoss = 4.9428, diffLoss = 8.1078, kgLoss = 0.1953
2025-04-09 09:54:31.639787: Training Step 5/115: batchLoss = 4.4069, diffLoss = 7.2337, kgLoss = 0.1667
2025-04-09 09:54:32.370530: Training Step 6/115: batchLoss = 4.6246, diffLoss = 7.5932, kgLoss = 0.1717
2025-04-09 09:54:33.098950: Training Step 7/115: batchLoss = 4.6066, diffLoss = 7.5543, kgLoss = 0.1850
2025-04-09 09:54:33.831315: Training Step 8/115: batchLoss = 4.0381, diffLoss = 6.6240, kgLoss = 0.1594
2025-04-09 09:54:34.561638: Training Step 9/115: batchLoss = 4.5903, diffLoss = 7.5297, kgLoss = 0.1812
2025-04-09 09:54:35.287001: Training Step 10/115: batchLoss = 4.2688, diffLoss = 6.9998, kgLoss = 0.1724
2025-04-09 09:54:36.022185: Training Step 11/115: batchLoss = 3.9406, diffLoss = 6.4624, kgLoss = 0.1579
2025-04-09 09:54:36.760139: Training Step 12/115: batchLoss = 4.1797, diffLoss = 6.8521, kgLoss = 0.1710
2025-04-09 09:54:37.485617: Training Step 13/115: batchLoss = 5.0120, diffLoss = 8.2252, kgLoss = 0.1922
2025-04-09 09:54:38.215214: Training Step 14/115: batchLoss = 4.7147, diffLoss = 7.7442, kgLoss = 0.1705
2025-04-09 09:54:38.944711: Training Step 15/115: batchLoss = 4.4280, diffLoss = 7.2646, kgLoss = 0.1730
2025-04-09 09:54:39.682575: Training Step 16/115: batchLoss = 4.3749, diffLoss = 7.1835, kgLoss = 0.1620
2025-04-09 09:54:40.416342: Training Step 17/115: batchLoss = 4.2707, diffLoss = 7.0036, kgLoss = 0.1713
2025-04-09 09:54:41.147867: Training Step 18/115: batchLoss = 4.5414, diffLoss = 7.4448, kgLoss = 0.1864
2025-04-09 09:54:41.879275: Training Step 19/115: batchLoss = 3.8731, diffLoss = 6.3511, kgLoss = 0.1560
2025-04-09 09:54:42.612889: Training Step 20/115: batchLoss = 4.4524, diffLoss = 7.3070, kgLoss = 0.1705
2025-04-09 09:54:43.344578: Training Step 21/115: batchLoss = 4.6147, diffLoss = 7.5658, kgLoss = 0.1881
2025-04-09 09:54:44.073414: Training Step 22/115: batchLoss = 4.4014, diffLoss = 7.2151, kgLoss = 0.1807
2025-04-09 09:54:44.800710: Training Step 23/115: batchLoss = 4.2003, diffLoss = 6.8845, kgLoss = 0.1740
2025-04-09 09:54:45.522081: Training Step 24/115: batchLoss = 5.0492, diffLoss = 8.2746, kgLoss = 0.2111
2025-04-09 09:54:46.246199: Training Step 25/115: batchLoss = 4.0532, diffLoss = 6.6453, kgLoss = 0.1651
2025-04-09 09:54:46.973681: Training Step 26/115: batchLoss = 4.4410, diffLoss = 7.2788, kgLoss = 0.1843
2025-04-09 09:54:47.708482: Training Step 27/115: batchLoss = 4.2527, diffLoss = 6.9778, kgLoss = 0.1651
2025-04-09 09:54:48.445104: Training Step 28/115: batchLoss = 4.7517, diffLoss = 7.7961, kgLoss = 0.1851
2025-04-09 09:54:49.178533: Training Step 29/115: batchLoss = 4.0105, diffLoss = 6.5776, kgLoss = 0.1599
2025-04-09 09:54:49.928186: Training Step 30/115: batchLoss = 4.1942, diffLoss = 6.8776, kgLoss = 0.1693
2025-04-09 09:54:50.679692: Training Step 31/115: batchLoss = 4.2734, diffLoss = 6.9992, kgLoss = 0.1848
2025-04-09 09:54:51.421984: Training Step 32/115: batchLoss = 4.9919, diffLoss = 8.1884, kgLoss = 0.1970
2025-04-09 09:54:52.166795: Training Step 33/115: batchLoss = 4.6249, diffLoss = 7.5883, kgLoss = 0.1799
2025-04-09 09:54:52.908721: Training Step 34/115: batchLoss = 4.8549, diffLoss = 7.9632, kgLoss = 0.1924
2025-04-09 09:54:53.647315: Training Step 35/115: batchLoss = 5.0849, diffLoss = 8.3417, kgLoss = 0.1996
2025-04-09 09:54:54.379117: Training Step 36/115: batchLoss = 4.7220, diffLoss = 7.7501, kgLoss = 0.1798
2025-04-09 09:54:55.108034: Training Step 37/115: batchLoss = 4.2556, diffLoss = 6.9725, kgLoss = 0.1802
2025-04-09 09:54:55.842527: Training Step 38/115: batchLoss = 4.4161, diffLoss = 7.2412, kgLoss = 0.1785
2025-04-09 09:54:56.569524: Training Step 39/115: batchLoss = 4.4906, diffLoss = 7.3580, kgLoss = 0.1894
2025-04-09 09:54:57.306622: Training Step 40/115: batchLoss = 4.6625, diffLoss = 7.6515, kgLoss = 0.1791
2025-04-09 09:54:58.050112: Training Step 41/115: batchLoss = 4.7472, diffLoss = 7.7951, kgLoss = 0.1754
2025-04-09 09:54:58.778235: Training Step 42/115: batchLoss = 4.3240, diffLoss = 7.0927, kgLoss = 0.1710
2025-04-09 09:54:59.510149: Training Step 43/115: batchLoss = 4.6284, diffLoss = 7.5979, kgLoss = 0.1743
2025-04-09 09:55:00.258361: Training Step 44/115: batchLoss = 4.8465, diffLoss = 7.9576, kgLoss = 0.1797
2025-04-09 09:55:01.009334: Training Step 45/115: batchLoss = 4.1476, diffLoss = 6.8017, kgLoss = 0.1665
2025-04-09 09:55:01.742316: Training Step 46/115: batchLoss = 4.7638, diffLoss = 7.8089, kgLoss = 0.1961
2025-04-09 09:55:02.478606: Training Step 47/115: batchLoss = 4.3534, diffLoss = 7.1430, kgLoss = 0.1691
2025-04-09 09:55:03.203259: Training Step 48/115: batchLoss = 4.5926, diffLoss = 7.5345, kgLoss = 0.1799
2025-04-09 09:55:03.940329: Training Step 49/115: batchLoss = 4.6503, diffLoss = 7.6263, kgLoss = 0.1864
2025-04-09 09:55:04.679778: Training Step 50/115: batchLoss = 4.3458, diffLoss = 7.1247, kgLoss = 0.1774
2025-04-09 09:55:05.409412: Training Step 51/115: batchLoss = 4.3604, diffLoss = 7.1492, kgLoss = 0.1773
2025-04-09 09:55:06.138925: Training Step 52/115: batchLoss = 4.6319, diffLoss = 7.6043, kgLoss = 0.1734
2025-04-09 09:55:06.877749: Training Step 53/115: batchLoss = 4.7352, diffLoss = 7.7636, kgLoss = 0.1926
2025-04-09 09:55:07.610945: Training Step 54/115: batchLoss = 4.5322, diffLoss = 7.4376, kgLoss = 0.1740
2025-04-09 09:55:08.345591: Training Step 55/115: batchLoss = 5.8001, diffLoss = 9.5190, kgLoss = 0.2218
2025-04-09 09:55:09.102515: Training Step 56/115: batchLoss = 4.4102, diffLoss = 7.2326, kgLoss = 0.1767
2025-04-09 09:55:09.834484: Training Step 57/115: batchLoss = 4.3673, diffLoss = 7.1608, kgLoss = 0.1769
2025-04-09 09:55:10.570695: Training Step 58/115: batchLoss = 4.3972, diffLoss = 7.2050, kgLoss = 0.1855
2025-04-09 09:55:11.299017: Training Step 59/115: batchLoss = 4.7202, diffLoss = 7.7491, kgLoss = 0.1768
2025-04-09 09:55:12.048982: Training Step 60/115: batchLoss = 4.5277, diffLoss = 7.4240, kgLoss = 0.1832
2025-04-09 09:55:12.794558: Training Step 61/115: batchLoss = 4.7617, diffLoss = 7.8101, kgLoss = 0.1891
2025-04-09 09:55:13.526192: Training Step 62/115: batchLoss = 4.6182, diffLoss = 7.5719, kgLoss = 0.1876
2025-04-09 09:55:14.258733: Training Step 63/115: batchLoss = 4.3081, diffLoss = 7.0702, kgLoss = 0.1649
2025-04-09 09:55:14.989169: Training Step 64/115: batchLoss = 4.1287, diffLoss = 6.7656, kgLoss = 0.1734
2025-04-09 09:55:15.716175: Training Step 65/115: batchLoss = 4.3563, diffLoss = 7.1348, kgLoss = 0.1885
2025-04-09 09:55:16.451178: Training Step 66/115: batchLoss = 4.0022, diffLoss = 6.5593, kgLoss = 0.1665
2025-04-09 09:55:17.192550: Training Step 67/115: batchLoss = 4.8039, diffLoss = 7.8828, kgLoss = 0.1855
2025-04-09 09:55:17.939407: Training Step 68/115: batchLoss = 4.1846, diffLoss = 6.8622, kgLoss = 0.1682
2025-04-09 09:55:18.667650: Training Step 69/115: batchLoss = 4.5078, diffLoss = 7.3965, kgLoss = 0.1746
2025-04-09 09:55:19.416324: Training Step 70/115: batchLoss = 5.0039, diffLoss = 8.2013, kgLoss = 0.2078
2025-04-09 09:55:20.173001: Training Step 71/115: batchLoss = 4.8799, diffLoss = 8.0088, kgLoss = 0.1865
2025-04-09 09:55:20.908215: Training Step 72/115: batchLoss = 4.6591, diffLoss = 7.6465, kgLoss = 0.1779
2025-04-09 09:55:21.647553: Training Step 73/115: batchLoss = 4.9228, diffLoss = 8.0704, kgLoss = 0.2014
2025-04-09 09:55:22.387266: Training Step 74/115: batchLoss = 4.2509, diffLoss = 6.9808, kgLoss = 0.1560
2025-04-09 09:55:23.112172: Training Step 75/115: batchLoss = 3.9685, diffLoss = 6.5039, kgLoss = 0.1653
2025-04-09 09:55:23.844649: Training Step 76/115: batchLoss = 4.8788, diffLoss = 8.0102, kgLoss = 0.1816
2025-04-09 09:55:24.594982: Training Step 77/115: batchLoss = 4.4059, diffLoss = 7.2358, kgLoss = 0.1612
2025-04-09 09:55:25.340216: Training Step 78/115: batchLoss = 4.1602, diffLoss = 6.8205, kgLoss = 0.1697
2025-04-09 09:55:26.077849: Training Step 79/115: batchLoss = 4.4728, diffLoss = 7.3354, kgLoss = 0.1790
2025-04-09 09:55:26.803524: Training Step 80/115: batchLoss = 4.1728, diffLoss = 6.8321, kgLoss = 0.1838
2025-04-09 09:55:27.535226: Training Step 81/115: batchLoss = 4.4818, diffLoss = 7.3554, kgLoss = 0.1715
2025-04-09 09:55:28.268970: Training Step 82/115: batchLoss = 4.4029, diffLoss = 7.2243, kgLoss = 0.1709
2025-04-09 09:55:28.990656: Training Step 83/115: batchLoss = 4.1430, diffLoss = 6.7942, kgLoss = 0.1664
2025-04-09 09:55:29.724475: Training Step 84/115: batchLoss = 4.0026, diffLoss = 6.5636, kgLoss = 0.1612
2025-04-09 09:55:30.456884: Training Step 85/115: batchLoss = 3.9763, diffLoss = 6.5189, kgLoss = 0.1625
2025-04-09 09:55:31.181345: Training Step 86/115: batchLoss = 4.2342, diffLoss = 6.9452, kgLoss = 0.1675
2025-04-09 09:55:31.903193: Training Step 87/115: batchLoss = 4.9117, diffLoss = 8.0583, kgLoss = 0.1918
2025-04-09 09:55:32.632771: Training Step 88/115: batchLoss = 4.5361, diffLoss = 7.4255, kgLoss = 0.2019
2025-04-09 09:55:33.363595: Training Step 89/115: batchLoss = 4.6983, diffLoss = 7.7076, kgLoss = 0.1843
2025-04-09 09:55:34.085418: Training Step 90/115: batchLoss = 5.0240, diffLoss = 8.2375, kgLoss = 0.2037
2025-04-09 09:55:34.814217: Training Step 91/115: batchLoss = 4.6026, diffLoss = 7.5403, kgLoss = 0.1959
2025-04-09 09:55:35.543180: Training Step 92/115: batchLoss = 4.2501, diffLoss = 6.9765, kgLoss = 0.1606
2025-04-09 09:55:36.359546: Training Step 93/115: batchLoss = 4.1165, diffLoss = 6.7488, kgLoss = 0.1680
2025-04-09 09:55:37.080370: Training Step 94/115: batchLoss = 4.1988, diffLoss = 6.8782, kgLoss = 0.1797
2025-04-09 09:55:37.796092: Training Step 95/115: batchLoss = 4.3457, diffLoss = 7.1224, kgLoss = 0.1805
2025-04-09 09:55:38.517219: Training Step 96/115: batchLoss = 4.8547, diffLoss = 7.9640, kgLoss = 0.1907
2025-04-09 09:55:39.251894: Training Step 97/115: batchLoss = 4.3687, diffLoss = 7.1609, kgLoss = 0.1804
2025-04-09 09:55:39.972680: Training Step 98/115: batchLoss = 4.1544, diffLoss = 6.8158, kgLoss = 0.1623
2025-04-09 09:55:40.690988: Training Step 99/115: batchLoss = 4.4963, diffLoss = 7.3779, kgLoss = 0.1740
2025-04-09 09:55:41.413618: Training Step 100/115: batchLoss = 4.1432, diffLoss = 6.7967, kgLoss = 0.1629
2025-04-09 09:55:42.129293: Training Step 101/115: batchLoss = 4.2441, diffLoss = 6.9620, kgLoss = 0.1672
2025-04-09 09:55:42.847373: Training Step 102/115: batchLoss = 4.5769, diffLoss = 7.5128, kgLoss = 0.1730
2025-04-09 09:55:43.564940: Training Step 103/115: batchLoss = 4.5511, diffLoss = 7.4687, kgLoss = 0.1745
2025-04-09 09:55:44.271041: Training Step 104/115: batchLoss = 4.7990, diffLoss = 7.8765, kgLoss = 0.1828
2025-04-09 09:55:44.991470: Training Step 105/115: batchLoss = 4.5830, diffLoss = 7.5223, kgLoss = 0.1740
2025-04-09 09:55:45.713904: Training Step 106/115: batchLoss = 4.2104, diffLoss = 6.9076, kgLoss = 0.1646
2025-04-09 09:55:46.448822: Training Step 107/115: batchLoss = 4.4485, diffLoss = 7.3016, kgLoss = 0.1689
2025-04-09 09:55:47.173009: Training Step 108/115: batchLoss = 4.5448, diffLoss = 7.4582, kgLoss = 0.1748
2025-04-09 09:55:47.900091: Training Step 109/115: batchLoss = 4.5577, diffLoss = 7.4750, kgLoss = 0.1817
2025-04-09 09:55:48.627997: Training Step 110/115: batchLoss = 4.4942, diffLoss = 7.3738, kgLoss = 0.1747
2025-04-09 09:55:49.364063: Training Step 111/115: batchLoss = 3.9181, diffLoss = 6.4279, kgLoss = 0.1533
2025-04-09 09:55:50.075076: Training Step 112/115: batchLoss = 4.1284, diffLoss = 6.7681, kgLoss = 0.1688
2025-04-09 09:55:50.713080: Training Step 113/115: batchLoss = 3.8489, diffLoss = 6.3048, kgLoss = 0.1652
2025-04-09 09:55:51.325462: Training Step 114/115: batchLoss = 4.0097, diffLoss = 6.5731, kgLoss = 0.1645
2025-04-09 09:55:51.449992: 
2025-04-09 09:55:51.450723: Epoch 16/1000, Train: epLoss = 1.2848, epDfLoss = 2.1073, epfTransLoss = 0.0000, epKgLoss = 0.0510  
2025-04-09 09:55:52.229010: Steps 0/90: batch_recall = 56.18, batch_ndcg = 41.90 
2025-04-09 09:55:52.975382: Steps 1/90: batch_recall = 57.45, batch_ndcg = 40.05 
2025-04-09 09:55:53.730172: Steps 2/90: batch_recall = 51.26, batch_ndcg = 36.21 
2025-04-09 09:55:54.470965: Steps 3/90: batch_recall = 46.85, batch_ndcg = 32.58 
2025-04-09 09:55:55.220481: Steps 4/90: batch_recall = 49.58, batch_ndcg = 36.17 
2025-04-09 09:55:55.967883: Steps 5/90: batch_recall = 35.47, batch_ndcg = 24.89 
2025-04-09 09:55:56.716928: Steps 6/90: batch_recall = 47.67, batch_ndcg = 34.55 
2025-04-09 09:55:57.477183: Steps 7/90: batch_recall = 40.98, batch_ndcg = 29.83 
2025-04-09 09:55:58.224543: Steps 8/90: batch_recall = 47.27, batch_ndcg = 33.10 
2025-04-09 09:55:58.989871: Steps 9/90: batch_recall = 40.16, batch_ndcg = 30.36 
2025-04-09 09:55:59.733838: Steps 10/90: batch_recall = 36.35, batch_ndcg = 27.45 
2025-04-09 09:56:00.474290: Steps 11/90: batch_recall = 44.89, batch_ndcg = 27.34 
2025-04-09 09:56:01.212154: Steps 12/90: batch_recall = 36.39, batch_ndcg = 25.94 
2025-04-09 09:56:01.955266: Steps 13/90: batch_recall = 34.90, batch_ndcg = 24.18 
2025-04-09 09:56:02.693846: Steps 14/90: batch_recall = 31.65, batch_ndcg = 24.60 
2025-04-09 09:56:03.432684: Steps 15/90: batch_recall = 43.50, batch_ndcg = 28.15 
2025-04-09 09:56:04.178210: Steps 16/90: batch_recall = 35.01, batch_ndcg = 24.61 
2025-04-09 09:56:04.924428: Steps 17/90: batch_recall = 32.33, batch_ndcg = 22.29 
2025-04-09 09:56:05.673273: Steps 18/90: batch_recall = 35.91, batch_ndcg = 24.45 
2025-04-09 09:56:06.410972: Steps 19/90: batch_recall = 32.31, batch_ndcg = 22.46 
2025-04-09 09:56:07.136433: Steps 20/90: batch_recall = 33.77, batch_ndcg = 21.79 
2025-04-09 09:56:07.866119: Steps 21/90: batch_recall = 37.29, batch_ndcg = 26.31 
2025-04-09 09:56:08.605880: Steps 22/90: batch_recall = 40.08, batch_ndcg = 28.67 
2025-04-09 09:56:09.327008: Steps 23/90: batch_recall = 33.02, batch_ndcg = 22.67 
2025-04-09 09:56:10.045019: Steps 24/90: batch_recall = 37.57, batch_ndcg = 25.09 
2025-04-09 09:56:10.757457: Steps 25/90: batch_recall = 35.32, batch_ndcg = 23.02 
2025-04-09 09:56:11.469147: Steps 26/90: batch_recall = 37.73, batch_ndcg = 25.74 
2025-04-09 09:56:12.193754: Steps 27/90: batch_recall = 31.84, batch_ndcg = 22.56 
2025-04-09 09:56:12.910493: Steps 28/90: batch_recall = 33.23, batch_ndcg = 22.98 
2025-04-09 09:56:13.627564: Steps 29/90: batch_recall = 31.37, batch_ndcg = 21.24 
2025-04-09 09:56:14.367104: Steps 30/90: batch_recall = 29.91, batch_ndcg = 18.59 
2025-04-09 09:56:15.117763: Steps 31/90: batch_recall = 29.93, batch_ndcg = 20.59 
2025-04-09 09:56:15.845139: Steps 32/90: batch_recall = 32.98, batch_ndcg = 22.03 
2025-04-09 09:56:16.561964: Steps 33/90: batch_recall = 31.06, batch_ndcg = 20.08 
2025-04-09 09:56:17.275327: Steps 34/90: batch_recall = 34.53, batch_ndcg = 22.89 
2025-04-09 09:56:17.988383: Steps 35/90: batch_recall = 34.95, batch_ndcg = 24.77 
2025-04-09 09:56:18.714197: Steps 36/90: batch_recall = 34.42, batch_ndcg = 22.61 
2025-04-09 09:56:19.424461: Steps 37/90: batch_recall = 30.30, batch_ndcg = 19.88 
2025-04-09 09:56:20.151305: Steps 38/90: batch_recall = 33.00, batch_ndcg = 20.79 
2025-04-09 09:56:20.878552: Steps 39/90: batch_recall = 32.79, batch_ndcg = 19.34 
2025-04-09 09:56:21.596539: Steps 40/90: batch_recall = 32.25, batch_ndcg = 20.61 
2025-04-09 09:56:22.306158: Steps 41/90: batch_recall = 31.36, batch_ndcg = 20.98 
2025-04-09 09:56:23.000744: Steps 42/90: batch_recall = 36.02, batch_ndcg = 26.02 
2025-04-09 09:56:23.708647: Steps 43/90: batch_recall = 30.36, batch_ndcg = 20.07 
2025-04-09 09:56:24.424090: Steps 44/90: batch_recall = 31.80, batch_ndcg = 19.88 
2025-04-09 09:56:25.142852: Steps 45/90: batch_recall = 31.11, batch_ndcg = 19.08 
2025-04-09 09:56:25.847041: Steps 46/90: batch_recall = 30.01, batch_ndcg = 18.41 
2025-04-09 09:56:26.546563: Steps 47/90: batch_recall = 27.57, batch_ndcg = 17.52 
2025-04-09 09:56:27.257643: Steps 48/90: batch_recall = 34.30, batch_ndcg = 21.15 
2025-04-09 09:56:27.960480: Steps 49/90: batch_recall = 25.97, batch_ndcg = 17.35 
2025-04-09 09:56:28.678268: Steps 50/90: batch_recall = 35.42, batch_ndcg = 22.53 
2025-04-09 09:56:29.387792: Steps 51/90: batch_recall = 33.77, batch_ndcg = 22.32 
2025-04-09 09:56:30.092463: Steps 52/90: batch_recall = 29.94, batch_ndcg = 19.82 
2025-04-09 09:56:30.794287: Steps 53/90: batch_recall = 39.06, batch_ndcg = 23.21 
2025-04-09 09:56:31.503613: Steps 54/90: batch_recall = 26.18, batch_ndcg = 18.05 
2025-04-09 09:56:32.214388: Steps 55/90: batch_recall = 35.00, batch_ndcg = 20.92 
2025-04-09 09:56:32.917986: Steps 56/90: batch_recall = 28.18, batch_ndcg = 17.50 
2025-04-09 09:56:33.627660: Steps 57/90: batch_recall = 32.53, batch_ndcg = 20.55 
2025-04-09 09:56:34.339366: Steps 58/90: batch_recall = 37.67, batch_ndcg = 23.71 
2025-04-09 09:56:35.040280: Steps 59/90: batch_recall = 26.41, batch_ndcg = 17.41 
2025-04-09 09:56:35.750428: Steps 60/90: batch_recall = 27.93, batch_ndcg = 17.62 
2025-04-09 09:56:36.459873: Steps 61/90: batch_recall = 38.28, batch_ndcg = 25.53 
2025-04-09 09:56:37.181647: Steps 62/90: batch_recall = 34.99, batch_ndcg = 22.78 
2025-04-09 09:56:37.915615: Steps 63/90: batch_recall = 37.56, batch_ndcg = 22.26 
2025-04-09 09:56:38.621501: Steps 64/90: batch_recall = 33.52, batch_ndcg = 20.45 
2025-04-09 09:56:39.330659: Steps 65/90: batch_recall = 29.40, batch_ndcg = 19.25 
2025-04-09 09:56:40.029492: Steps 66/90: batch_recall = 31.93, batch_ndcg = 20.49 
2025-04-09 09:56:40.740678: Steps 67/90: batch_recall = 31.40, batch_ndcg = 18.81 
2025-04-09 09:56:41.446958: Steps 68/90: batch_recall = 31.17, batch_ndcg = 19.72 
2025-04-09 09:56:42.166280: Steps 69/90: batch_recall = 35.68, batch_ndcg = 21.78 
2025-04-09 09:56:42.889844: Steps 70/90: batch_recall = 32.88, batch_ndcg = 20.31 
2025-04-09 09:56:43.582929: Steps 71/90: batch_recall = 33.37, batch_ndcg = 20.50 
2025-04-09 09:56:44.284252: Steps 72/90: batch_recall = 30.76, batch_ndcg = 18.95 
2025-04-09 09:56:44.992955: Steps 73/90: batch_recall = 34.18, batch_ndcg = 20.79 
2025-04-09 09:56:45.695542: Steps 74/90: batch_recall = 35.89, batch_ndcg = 22.50 
2025-04-09 09:56:46.402214: Steps 75/90: batch_recall = 35.26, batch_ndcg = 22.54 
2025-04-09 09:56:47.092637: Steps 76/90: batch_recall = 36.29, batch_ndcg = 22.20 
2025-04-09 09:56:47.800726: Steps 77/90: batch_recall = 37.76, batch_ndcg = 22.91 
2025-04-09 09:56:48.495363: Steps 78/90: batch_recall = 29.74, batch_ndcg = 18.37 
2025-04-09 09:56:49.209783: Steps 79/90: batch_recall = 36.28, batch_ndcg = 24.45 
2025-04-09 09:56:49.917900: Steps 80/90: batch_recall = 39.20, batch_ndcg = 24.39 
2025-04-09 09:56:50.632762: Steps 81/90: batch_recall = 40.13, batch_ndcg = 24.30 
2025-04-09 09:56:51.335128: Steps 82/90: batch_recall = 35.01, batch_ndcg = 22.12 
2025-04-09 09:56:52.035244: Steps 83/90: batch_recall = 35.12, batch_ndcg = 22.37 
2025-04-09 09:56:52.740097: Steps 84/90: batch_recall = 32.54, batch_ndcg = 19.01 
2025-04-09 09:56:53.440146: Steps 85/90: batch_recall = 33.53, batch_ndcg = 19.64 
2025-04-09 09:56:54.142418: Steps 86/90: batch_recall = 41.15, batch_ndcg = 25.05 
2025-04-09 09:56:54.845670: Steps 87/90: batch_recall = 38.18, batch_ndcg = 25.28 
2025-04-09 09:56:55.538877: Steps 88/90: batch_recall = 44.85, batch_ndcg = 25.82 
2025-04-09 09:56:56.086052: Steps 89/90: batch_recall = 36.85, batch_ndcg = 21.63 
2025-04-09 09:56:56.086821: Epoch 16/1000, Test: Recall = 0.0706, NDCG = 0.0458  

2025-04-09 09:56:57.104481: Training Step 0/115: batchLoss = 3.8561, diffLoss = 6.3190, kgLoss = 0.1619
2025-04-09 09:56:57.837544: Training Step 1/115: batchLoss = 4.5131, diffLoss = 7.4036, kgLoss = 0.1774
2025-04-09 09:56:58.579044: Training Step 2/115: batchLoss = 4.5388, diffLoss = 7.4448, kgLoss = 0.1798
2025-04-09 09:56:59.324451: Training Step 3/115: batchLoss = 4.6368, diffLoss = 7.6084, kgLoss = 0.1793
2025-04-09 09:57:00.078683: Training Step 4/115: batchLoss = 4.8759, diffLoss = 8.0029, kgLoss = 0.1854
2025-04-09 09:57:00.830128: Training Step 5/115: batchLoss = 4.3124, diffLoss = 7.0684, kgLoss = 0.1784
2025-04-09 09:57:01.585997: Training Step 6/115: batchLoss = 4.1370, diffLoss = 6.7817, kgLoss = 0.1699
2025-04-09 09:57:02.328107: Training Step 7/115: batchLoss = 4.8932, diffLoss = 8.0247, kgLoss = 0.1959
2025-04-09 09:57:03.069760: Training Step 8/115: batchLoss = 4.4852, diffLoss = 7.3588, kgLoss = 0.1748
2025-04-09 09:57:03.807152: Training Step 9/115: batchLoss = 4.4915, diffLoss = 7.3681, kgLoss = 0.1764
2025-04-09 09:57:04.550660: Training Step 10/115: batchLoss = 4.5153, diffLoss = 7.4030, kgLoss = 0.1837
2025-04-09 09:57:05.291318: Training Step 11/115: batchLoss = 4.8939, diffLoss = 8.0282, kgLoss = 0.1923
2025-04-09 09:57:06.022780: Training Step 12/115: batchLoss = 4.4914, diffLoss = 7.3715, kgLoss = 0.1711
2025-04-09 09:57:06.763274: Training Step 13/115: batchLoss = 4.7018, diffLoss = 7.7074, kgLoss = 0.1935
2025-04-09 09:57:07.509269: Training Step 14/115: batchLoss = 5.1142, diffLoss = 8.3886, kgLoss = 0.2025
2025-04-09 09:57:08.363631: Training Step 15/115: batchLoss = 4.2173, diffLoss = 6.9229, kgLoss = 0.1591
2025-04-09 09:57:09.130188: Training Step 16/115: batchLoss = 4.4540, diffLoss = 7.3086, kgLoss = 0.1721
2025-04-09 09:57:09.876218: Training Step 17/115: batchLoss = 4.1682, diffLoss = 6.8408, kgLoss = 0.1594
2025-04-09 09:57:10.618209: Training Step 18/115: batchLoss = 4.6278, diffLoss = 7.5963, kgLoss = 0.1751
2025-04-09 09:57:11.367243: Training Step 19/115: batchLoss = 5.0651, diffLoss = 8.3101, kgLoss = 0.1978
2025-04-09 09:57:12.102276: Training Step 20/115: batchLoss = 4.8252, diffLoss = 7.9169, kgLoss = 0.1876
2025-04-09 09:57:12.835112: Training Step 21/115: batchLoss = 4.7488, diffLoss = 7.7921, kgLoss = 0.1840
2025-04-09 09:57:13.573803: Training Step 22/115: batchLoss = 4.0343, diffLoss = 6.6134, kgLoss = 0.1657
2025-04-09 09:57:14.315381: Training Step 23/115: batchLoss = 5.2952, diffLoss = 8.6786, kgLoss = 0.2202
2025-04-09 09:57:15.056398: Training Step 24/115: batchLoss = 4.4023, diffLoss = 7.2233, kgLoss = 0.1709
2025-04-09 09:57:15.786235: Training Step 25/115: batchLoss = 4.1410, diffLoss = 6.7894, kgLoss = 0.1684
2025-04-09 09:57:16.501591: Training Step 26/115: batchLoss = 4.5483, diffLoss = 7.4627, kgLoss = 0.1767
2025-04-09 09:57:17.240855: Training Step 27/115: batchLoss = 4.4423, diffLoss = 7.2830, kgLoss = 0.1812
2025-04-09 09:57:17.977842: Training Step 28/115: batchLoss = 4.5659, diffLoss = 7.4915, kgLoss = 0.1774
2025-04-09 09:57:18.732907: Training Step 29/115: batchLoss = 3.9956, diffLoss = 6.5501, kgLoss = 0.1639
2025-04-09 09:57:19.557748: Training Step 30/115: batchLoss = 4.4674, diffLoss = 7.3324, kgLoss = 0.1700
2025-04-09 09:57:20.280287: Training Step 31/115: batchLoss = 4.0759, diffLoss = 6.6743, kgLoss = 0.1783
2025-04-09 09:57:21.015797: Training Step 32/115: batchLoss = 4.0012, diffLoss = 6.5622, kgLoss = 0.1597
2025-04-09 09:57:21.761629: Training Step 33/115: batchLoss = 4.5012, diffLoss = 7.3832, kgLoss = 0.1783
2025-04-09 09:57:22.497777: Training Step 34/115: batchLoss = 4.4156, diffLoss = 7.2461, kgLoss = 0.1699
2025-04-09 09:57:23.219125: Training Step 35/115: batchLoss = 4.0054, diffLoss = 6.5708, kgLoss = 0.1573
2025-04-09 09:57:23.945325: Training Step 36/115: batchLoss = 4.7256, diffLoss = 7.7556, kgLoss = 0.1805
2025-04-09 09:57:24.681433: Training Step 37/115: batchLoss = 4.0672, diffLoss = 6.6586, kgLoss = 0.1800
2025-04-09 09:57:25.417685: Training Step 38/115: batchLoss = 4.5704, diffLoss = 7.4959, kgLoss = 0.1821
2025-04-09 09:57:26.155674: Training Step 39/115: batchLoss = 4.5727, diffLoss = 7.5003, kgLoss = 0.1813
2025-04-09 09:57:26.895442: Training Step 40/115: batchLoss = 4.6607, diffLoss = 7.6495, kgLoss = 0.1777
2025-04-09 09:57:27.640019: Training Step 41/115: batchLoss = 4.7232, diffLoss = 7.7597, kgLoss = 0.1684
2025-04-09 09:57:28.379375: Training Step 42/115: batchLoss = 3.8815, diffLoss = 6.3621, kgLoss = 0.1606
2025-04-09 09:57:29.113121: Training Step 43/115: batchLoss = 4.5319, diffLoss = 7.4346, kgLoss = 0.1778
2025-04-09 09:57:29.848732: Training Step 44/115: batchLoss = 3.9902, diffLoss = 6.5443, kgLoss = 0.1592
2025-04-09 09:57:30.595776: Training Step 45/115: batchLoss = 4.2245, diffLoss = 6.9306, kgLoss = 0.1654
2025-04-09 09:57:31.340592: Training Step 46/115: batchLoss = 4.2472, diffLoss = 6.9667, kgLoss = 0.1680
2025-04-09 09:57:32.071526: Training Step 47/115: batchLoss = 4.2731, diffLoss = 7.0087, kgLoss = 0.1696
2025-04-09 09:57:32.800997: Training Step 48/115: batchLoss = 4.8265, diffLoss = 7.9113, kgLoss = 0.1993
2025-04-09 09:57:33.550030: Training Step 49/115: batchLoss = 4.1665, diffLoss = 6.8311, kgLoss = 0.1696
2025-04-09 09:57:34.285743: Training Step 50/115: batchLoss = 4.1998, diffLoss = 6.8862, kgLoss = 0.1702
2025-04-09 09:57:35.022949: Training Step 51/115: batchLoss = 4.8043, diffLoss = 7.8865, kgLoss = 0.1809
2025-04-09 09:57:35.755224: Training Step 52/115: batchLoss = 4.8293, diffLoss = 7.9267, kgLoss = 0.1832
2025-04-09 09:57:36.490438: Training Step 53/115: batchLoss = 4.5649, diffLoss = 7.4895, kgLoss = 0.1780
2025-04-09 09:57:37.251033: Training Step 54/115: batchLoss = 4.4884, diffLoss = 7.3567, kgLoss = 0.1859
2025-04-09 09:57:38.020532: Training Step 55/115: batchLoss = 4.6467, diffLoss = 7.6277, kgLoss = 0.1753
2025-04-09 09:57:38.785375: Training Step 56/115: batchLoss = 4.4497, diffLoss = 7.3023, kgLoss = 0.1709
2025-04-09 09:57:39.519944: Training Step 57/115: batchLoss = 4.2769, diffLoss = 7.0165, kgLoss = 0.1675
2025-04-09 09:57:40.250794: Training Step 58/115: batchLoss = 4.5223, diffLoss = 7.4135, kgLoss = 0.1855
2025-04-09 09:57:40.995281: Training Step 59/115: batchLoss = 4.4427, diffLoss = 7.2906, kgLoss = 0.1708
2025-04-09 09:57:41.719357: Training Step 60/115: batchLoss = 4.2312, diffLoss = 6.9384, kgLoss = 0.1703
2025-04-09 09:57:42.448577: Training Step 61/115: batchLoss = 4.5961, diffLoss = 7.5376, kgLoss = 0.1838
2025-04-09 09:57:43.178921: Training Step 62/115: batchLoss = 4.2399, diffLoss = 6.9579, kgLoss = 0.1630
2025-04-09 09:57:43.911049: Training Step 63/115: batchLoss = 4.8612, diffLoss = 7.9742, kgLoss = 0.1917
2025-04-09 09:57:44.631924: Training Step 64/115: batchLoss = 4.2276, diffLoss = 6.9322, kgLoss = 0.1707
2025-04-09 09:57:45.363549: Training Step 65/115: batchLoss = 4.2279, diffLoss = 6.9383, kgLoss = 0.1624
2025-04-09 09:57:46.098781: Training Step 66/115: batchLoss = 4.3755, diffLoss = 7.1733, kgLoss = 0.1788
2025-04-09 09:57:46.823641: Training Step 67/115: batchLoss = 4.4764, diffLoss = 7.3469, kgLoss = 0.1706
2025-04-09 09:57:47.562571: Training Step 68/115: batchLoss = 4.6163, diffLoss = 7.5710, kgLoss = 0.1842
2025-04-09 09:57:48.295570: Training Step 69/115: batchLoss = 4.5311, diffLoss = 7.4212, kgLoss = 0.1960
2025-04-09 09:57:49.020961: Training Step 70/115: batchLoss = 4.7831, diffLoss = 7.8502, kgLoss = 0.1824
2025-04-09 09:57:49.758989: Training Step 71/115: batchLoss = 4.3809, diffLoss = 7.1897, kgLoss = 0.1677
2025-04-09 09:57:50.491301: Training Step 72/115: batchLoss = 4.4390, diffLoss = 7.2792, kgLoss = 0.1787
2025-04-09 09:57:51.236006: Training Step 73/115: batchLoss = 3.9742, diffLoss = 6.5165, kgLoss = 0.1607
2025-04-09 09:57:51.975439: Training Step 74/115: batchLoss = 5.1214, diffLoss = 8.4003, kgLoss = 0.2030
2025-04-09 09:57:52.713579: Training Step 75/115: batchLoss = 4.5934, diffLoss = 7.5329, kgLoss = 0.1842
2025-04-09 09:57:53.447576: Training Step 76/115: batchLoss = 4.2469, diffLoss = 6.9551, kgLoss = 0.1846
2025-04-09 09:57:54.182226: Training Step 77/115: batchLoss = 4.4300, diffLoss = 7.2688, kgLoss = 0.1719
2025-04-09 09:57:54.908752: Training Step 78/115: batchLoss = 4.4029, diffLoss = 7.2123, kgLoss = 0.1889
2025-04-09 09:57:55.643830: Training Step 79/115: batchLoss = 4.5942, diffLoss = 7.5393, kgLoss = 0.1765
2025-04-09 09:57:56.381185: Training Step 80/115: batchLoss = 4.9220, diffLoss = 8.0719, kgLoss = 0.1973
2025-04-09 09:57:57.107239: Training Step 81/115: batchLoss = 4.8846, diffLoss = 8.0137, kgLoss = 0.1908
2025-04-09 09:57:57.849596: Training Step 82/115: batchLoss = 4.4652, diffLoss = 7.3216, kgLoss = 0.1807
2025-04-09 09:57:58.599504: Training Step 83/115: batchLoss = 4.2110, diffLoss = 6.9086, kgLoss = 0.1646
2025-04-09 09:57:59.329950: Training Step 84/115: batchLoss = 5.0625, diffLoss = 8.2942, kgLoss = 0.2149
2025-04-09 09:58:00.074996: Training Step 85/115: batchLoss = 4.9346, diffLoss = 8.0947, kgLoss = 0.1946
2025-04-09 09:58:00.804258: Training Step 86/115: batchLoss = 4.7758, diffLoss = 7.8280, kgLoss = 0.1974
2025-04-09 09:58:01.536113: Training Step 87/115: batchLoss = 4.8029, diffLoss = 7.8831, kgLoss = 0.1827
2025-04-09 09:58:02.258946: Training Step 88/115: batchLoss = 4.8906, diffLoss = 8.0273, kgLoss = 0.1855
2025-04-09 09:58:02.982008: Training Step 89/115: batchLoss = 4.3131, diffLoss = 7.0741, kgLoss = 0.1718
2025-04-09 09:58:03.702607: Training Step 90/115: batchLoss = 5.0294, diffLoss = 8.2549, kgLoss = 0.1912
2025-04-09 09:58:04.435427: Training Step 91/115: batchLoss = 4.7012, diffLoss = 7.7129, kgLoss = 0.1836
2025-04-09 09:58:05.162824: Training Step 92/115: batchLoss = 4.1991, diffLoss = 6.8862, kgLoss = 0.1685
2025-04-09 09:58:05.896582: Training Step 93/115: batchLoss = 4.7319, diffLoss = 7.7625, kgLoss = 0.1860
2025-04-09 09:58:06.634508: Training Step 94/115: batchLoss = 4.2714, diffLoss = 7.0064, kgLoss = 0.1688
2025-04-09 09:58:07.374200: Training Step 95/115: batchLoss = 4.0618, diffLoss = 6.6623, kgLoss = 0.1611
2025-04-09 09:58:08.105469: Training Step 96/115: batchLoss = 4.0681, diffLoss = 6.6755, kgLoss = 0.1570
2025-04-09 09:58:08.837381: Training Step 97/115: batchLoss = 4.3772, diffLoss = 7.1750, kgLoss = 0.1804
2025-04-09 09:58:09.561664: Training Step 98/115: batchLoss = 4.2729, diffLoss = 7.0028, kgLoss = 0.1780
2025-04-09 09:58:10.283959: Training Step 99/115: batchLoss = 4.2274, diffLoss = 6.9363, kgLoss = 0.1640
2025-04-09 09:58:11.006839: Training Step 100/115: batchLoss = 4.5351, diffLoss = 7.4426, kgLoss = 0.1740
2025-04-09 09:58:11.719656: Training Step 101/115: batchLoss = 4.1247, diffLoss = 6.7664, kgLoss = 0.1620
2025-04-09 09:58:12.443168: Training Step 102/115: batchLoss = 4.2646, diffLoss = 6.9976, kgLoss = 0.1652
2025-04-09 09:58:13.160787: Training Step 103/115: batchLoss = 5.0859, diffLoss = 8.3496, kgLoss = 0.1904
2025-04-09 09:58:13.883811: Training Step 104/115: batchLoss = 4.1081, diffLoss = 6.7358, kgLoss = 0.1666
2025-04-09 09:58:14.601328: Training Step 105/115: batchLoss = 4.4559, diffLoss = 7.3102, kgLoss = 0.1744
2025-04-09 09:58:15.336452: Training Step 106/115: batchLoss = 4.2964, diffLoss = 7.0522, kgLoss = 0.1626
2025-04-09 09:58:16.057030: Training Step 107/115: batchLoss = 4.4970, diffLoss = 7.3796, kgLoss = 0.1731
2025-04-09 09:58:16.788822: Training Step 108/115: batchLoss = 4.5437, diffLoss = 7.4526, kgLoss = 0.1803
2025-04-09 09:58:17.509854: Training Step 109/115: batchLoss = 4.2742, diffLoss = 7.0108, kgLoss = 0.1694
2025-04-09 09:58:18.230569: Training Step 110/115: batchLoss = 4.6227, diffLoss = 7.5812, kgLoss = 0.1850
2025-04-09 09:58:18.948695: Training Step 111/115: batchLoss = 4.1498, diffLoss = 6.7985, kgLoss = 0.1768
2025-04-09 09:58:19.660887: Training Step 112/115: batchLoss = 4.4583, diffLoss = 7.3133, kgLoss = 0.1757
2025-04-09 09:58:20.307049: Training Step 113/115: batchLoss = 3.6542, diffLoss = 5.9907, kgLoss = 0.1495
2025-04-09 09:58:20.940343: Training Step 114/115: batchLoss = 4.3994, diffLoss = 7.2064, kgLoss = 0.1888
2025-04-09 09:58:21.065275: 
2025-04-09 09:58:21.065943: Epoch 17/1000, Train: epLoss = 1.2854, epDfLoss = 2.1084, epfTransLoss = 0.0000, epKgLoss = 0.0510  
2025-04-09 09:58:21.813067: Steps 0/90: batch_recall = 56.57, batch_ndcg = 42.54 
2025-04-09 09:58:22.548676: Steps 1/90: batch_recall = 57.48, batch_ndcg = 40.14 
2025-04-09 09:58:23.282451: Steps 2/90: batch_recall = 50.65, batch_ndcg = 36.52 
2025-04-09 09:58:24.019013: Steps 3/90: batch_recall = 46.95, batch_ndcg = 33.11 
2025-04-09 09:58:24.767850: Steps 4/90: batch_recall = 48.78, batch_ndcg = 35.57 
2025-04-09 09:58:25.498952: Steps 5/90: batch_recall = 35.62, batch_ndcg = 25.62 
2025-04-09 09:58:26.225144: Steps 6/90: batch_recall = 48.68, batch_ndcg = 34.28 
2025-04-09 09:58:26.964565: Steps 7/90: batch_recall = 41.74, batch_ndcg = 30.00 
2025-04-09 09:58:27.702854: Steps 8/90: batch_recall = 47.39, batch_ndcg = 33.58 
2025-04-09 09:58:28.467179: Steps 9/90: batch_recall = 42.01, batch_ndcg = 31.66 
2025-04-09 09:58:29.210294: Steps 10/90: batch_recall = 37.07, batch_ndcg = 27.90 
2025-04-09 09:58:29.954333: Steps 11/90: batch_recall = 43.83, batch_ndcg = 27.54 
2025-04-09 09:58:30.684795: Steps 12/90: batch_recall = 37.32, batch_ndcg = 26.38 
2025-04-09 09:58:31.426200: Steps 13/90: batch_recall = 36.53, batch_ndcg = 24.92 
2025-04-09 09:58:32.167483: Steps 14/90: batch_recall = 33.89, batch_ndcg = 25.62 
2025-04-09 09:58:32.893154: Steps 15/90: batch_recall = 41.61, batch_ndcg = 27.41 
2025-04-09 09:58:33.630637: Steps 16/90: batch_recall = 35.57, batch_ndcg = 25.05 
2025-04-09 09:58:34.359407: Steps 17/90: batch_recall = 33.33, batch_ndcg = 22.41 
2025-04-09 09:58:35.071451: Steps 18/90: batch_recall = 36.60, batch_ndcg = 24.50 
2025-04-09 09:58:35.801603: Steps 19/90: batch_recall = 32.55, batch_ndcg = 23.01 
2025-04-09 09:58:36.542655: Steps 20/90: batch_recall = 34.15, batch_ndcg = 22.18 
2025-04-09 09:58:37.274319: Steps 21/90: batch_recall = 37.95, batch_ndcg = 26.69 
2025-04-09 09:58:38.005691: Steps 22/90: batch_recall = 39.59, batch_ndcg = 28.66 
2025-04-09 09:58:38.733418: Steps 23/90: batch_recall = 34.09, batch_ndcg = 23.06 
2025-04-09 09:58:39.463162: Steps 24/90: batch_recall = 36.70, batch_ndcg = 24.88 
2025-04-09 09:58:40.192954: Steps 25/90: batch_recall = 35.76, batch_ndcg = 23.21 
2025-04-09 09:58:40.906822: Steps 26/90: batch_recall = 37.97, batch_ndcg = 26.10 
2025-04-09 09:58:41.614417: Steps 27/90: batch_recall = 32.52, batch_ndcg = 22.74 
2025-04-09 09:58:42.335935: Steps 28/90: batch_recall = 34.10, batch_ndcg = 23.56 
2025-04-09 09:58:43.048074: Steps 29/90: batch_recall = 31.78, batch_ndcg = 21.15 
2025-04-09 09:58:43.755975: Steps 30/90: batch_recall = 30.57, batch_ndcg = 18.98 
2025-04-09 09:58:44.480175: Steps 31/90: batch_recall = 30.43, batch_ndcg = 20.86 
2025-04-09 09:58:45.206740: Steps 32/90: batch_recall = 32.26, batch_ndcg = 22.13 
2025-04-09 09:58:45.922553: Steps 33/90: batch_recall = 31.70, batch_ndcg = 20.28 
2025-04-09 09:58:46.648362: Steps 34/90: batch_recall = 34.08, batch_ndcg = 22.72 
2025-04-09 09:58:47.378357: Steps 35/90: batch_recall = 35.46, batch_ndcg = 25.01 
2025-04-09 09:58:48.102825: Steps 36/90: batch_recall = 34.73, batch_ndcg = 22.68 
2025-04-09 09:58:48.825996: Steps 37/90: batch_recall = 30.45, batch_ndcg = 19.95 
2025-04-09 09:58:49.531142: Steps 38/90: batch_recall = 32.50, batch_ndcg = 20.82 
2025-04-09 09:58:50.249576: Steps 39/90: batch_recall = 34.35, batch_ndcg = 20.36 
2025-04-09 09:58:50.973764: Steps 40/90: batch_recall = 31.85, batch_ndcg = 20.88 
2025-04-09 09:58:51.687023: Steps 41/90: batch_recall = 31.72, batch_ndcg = 21.45 
2025-04-09 09:58:52.399768: Steps 42/90: batch_recall = 36.26, batch_ndcg = 25.76 
2025-04-09 09:58:53.093561: Steps 43/90: batch_recall = 30.49, batch_ndcg = 20.25 
2025-04-09 09:58:53.802513: Steps 44/90: batch_recall = 33.10, batch_ndcg = 20.66 
2025-04-09 09:58:54.495554: Steps 45/90: batch_recall = 30.77, batch_ndcg = 18.79 
2025-04-09 09:58:55.202619: Steps 46/90: batch_recall = 30.15, batch_ndcg = 18.75 
2025-04-09 09:58:55.910722: Steps 47/90: batch_recall = 28.26, batch_ndcg = 17.89 
2025-04-09 09:58:56.624275: Steps 48/90: batch_recall = 32.48, batch_ndcg = 20.61 
2025-04-09 09:58:57.336932: Steps 49/90: batch_recall = 26.45, batch_ndcg = 17.42 
2025-04-09 09:58:58.035255: Steps 50/90: batch_recall = 35.99, batch_ndcg = 22.76 
2025-04-09 09:58:58.739196: Steps 51/90: batch_recall = 33.63, batch_ndcg = 23.16 
2025-04-09 09:58:59.456938: Steps 52/90: batch_recall = 31.98, batch_ndcg = 20.51 
2025-04-09 09:59:00.161019: Steps 53/90: batch_recall = 40.24, batch_ndcg = 23.71 
2025-04-09 09:59:00.871585: Steps 54/90: batch_recall = 27.25, batch_ndcg = 17.97 
2025-04-09 09:59:01.598189: Steps 55/90: batch_recall = 35.54, batch_ndcg = 21.67 
2025-04-09 09:59:02.335166: Steps 56/90: batch_recall = 28.91, batch_ndcg = 17.63 
2025-04-09 09:59:03.054847: Steps 57/90: batch_recall = 35.17, batch_ndcg = 21.82 
2025-04-09 09:59:03.754512: Steps 58/90: batch_recall = 36.79, batch_ndcg = 23.60 
2025-04-09 09:59:04.462982: Steps 59/90: batch_recall = 27.24, batch_ndcg = 18.01 
2025-04-09 09:59:05.169534: Steps 60/90: batch_recall = 29.80, batch_ndcg = 18.45 
2025-04-09 09:59:05.882624: Steps 61/90: batch_recall = 40.26, batch_ndcg = 26.67 
2025-04-09 09:59:06.581354: Steps 62/90: batch_recall = 36.32, batch_ndcg = 23.22 
2025-04-09 09:59:07.282017: Steps 63/90: batch_recall = 36.23, batch_ndcg = 22.28 
2025-04-09 09:59:07.995085: Steps 64/90: batch_recall = 34.88, batch_ndcg = 20.93 
2025-04-09 09:59:08.702602: Steps 65/90: batch_recall = 30.08, batch_ndcg = 19.67 
2025-04-09 09:59:09.394190: Steps 66/90: batch_recall = 34.39, batch_ndcg = 21.34 
2025-04-09 09:59:10.099864: Steps 67/90: batch_recall = 31.42, batch_ndcg = 19.07 
2025-04-09 09:59:10.808605: Steps 68/90: batch_recall = 30.80, batch_ndcg = 19.80 
2025-04-09 09:59:11.526051: Steps 69/90: batch_recall = 34.72, batch_ndcg = 21.32 
2025-04-09 09:59:12.233620: Steps 70/90: batch_recall = 34.14, batch_ndcg = 21.32 
2025-04-09 09:59:12.931339: Steps 71/90: batch_recall = 32.92, batch_ndcg = 20.32 
2025-04-09 09:59:13.631023: Steps 72/90: batch_recall = 31.95, batch_ndcg = 19.63 
2025-04-09 09:59:14.335836: Steps 73/90: batch_recall = 34.17, batch_ndcg = 21.00 
2025-04-09 09:59:15.056421: Steps 74/90: batch_recall = 35.68, batch_ndcg = 22.88 
2025-04-09 09:59:15.778511: Steps 75/90: batch_recall = 36.70, batch_ndcg = 22.81 
2025-04-09 09:59:16.492887: Steps 76/90: batch_recall = 36.00, batch_ndcg = 21.95 
2025-04-09 09:59:17.224215: Steps 77/90: batch_recall = 39.14, batch_ndcg = 23.73 
2025-04-09 09:59:17.929034: Steps 78/90: batch_recall = 30.28, batch_ndcg = 18.69 
2025-04-09 09:59:18.636538: Steps 79/90: batch_recall = 37.12, batch_ndcg = 25.29 
2025-04-09 09:59:19.332930: Steps 80/90: batch_recall = 41.70, batch_ndcg = 25.65 
2025-04-09 09:59:20.041061: Steps 81/90: batch_recall = 39.47, batch_ndcg = 24.32 
2025-04-09 09:59:20.734880: Steps 82/90: batch_recall = 36.23, batch_ndcg = 22.89 
2025-04-09 09:59:21.442338: Steps 83/90: batch_recall = 35.79, batch_ndcg = 22.47 
2025-04-09 09:59:22.148025: Steps 84/90: batch_recall = 33.05, batch_ndcg = 19.55 
2025-04-09 09:59:22.863131: Steps 85/90: batch_recall = 34.97, batch_ndcg = 20.12 
2025-04-09 09:59:23.595331: Steps 86/90: batch_recall = 42.20, batch_ndcg = 25.40 
2025-04-09 09:59:24.311605: Steps 87/90: batch_recall = 39.13, batch_ndcg = 25.82 
2025-04-09 09:59:25.039261: Steps 88/90: batch_recall = 44.95, batch_ndcg = 26.03 
2025-04-09 09:59:25.589484: Steps 89/90: batch_recall = 38.25, batch_ndcg = 22.67 
2025-04-09 09:59:25.590438: Epoch 17/1000, Test: Recall = 0.0716, NDCG = 0.0464  

2025-04-09 09:59:26.601687: Training Step 0/115: batchLoss = 4.5273, diffLoss = 7.4237, kgLoss = 0.1826
2025-04-09 09:59:27.329679: Training Step 1/115: batchLoss = 4.3401, diffLoss = 7.1240, kgLoss = 0.1643
2025-04-09 09:59:28.056905: Training Step 2/115: batchLoss = 4.8559, diffLoss = 7.9708, kgLoss = 0.1836
2025-04-09 09:59:28.786910: Training Step 3/115: batchLoss = 4.2175, diffLoss = 6.9143, kgLoss = 0.1723
2025-04-09 09:59:29.504665: Training Step 4/115: batchLoss = 5.1027, diffLoss = 8.3714, kgLoss = 0.1997
2025-04-09 09:59:30.231439: Training Step 5/115: batchLoss = 4.2945, diffLoss = 7.0362, kgLoss = 0.1818
2025-04-09 09:59:30.963830: Training Step 6/115: batchLoss = 4.7373, diffLoss = 7.7729, kgLoss = 0.1839
2025-04-09 09:59:31.698602: Training Step 7/115: batchLoss = 4.4767, diffLoss = 7.3331, kgLoss = 0.1921
2025-04-09 09:59:32.430643: Training Step 8/115: batchLoss = 3.8169, diffLoss = 6.2567, kgLoss = 0.1571
2025-04-09 09:59:33.171992: Training Step 9/115: batchLoss = 4.8187, diffLoss = 7.9088, kgLoss = 0.1836
2025-04-09 09:59:33.912427: Training Step 10/115: batchLoss = 4.5295, diffLoss = 7.4199, kgLoss = 0.1937
2025-04-09 09:59:34.644477: Training Step 11/115: batchLoss = 5.0739, diffLoss = 8.3213, kgLoss = 0.2029
2025-04-09 09:59:35.379798: Training Step 12/115: batchLoss = 4.2932, diffLoss = 7.0414, kgLoss = 0.1710
2025-04-09 09:59:36.111265: Training Step 13/115: batchLoss = 4.1910, diffLoss = 6.8670, kgLoss = 0.1769
2025-04-09 09:59:36.845945: Training Step 14/115: batchLoss = 4.6116, diffLoss = 7.5675, kgLoss = 0.1776
2025-04-09 09:59:37.577294: Training Step 15/115: batchLoss = 4.3462, diffLoss = 7.1314, kgLoss = 0.1684
2025-04-09 09:59:38.315368: Training Step 16/115: batchLoss = 4.2466, diffLoss = 6.9671, kgLoss = 0.1659
2025-04-09 09:59:39.042941: Training Step 17/115: batchLoss = 4.9498, diffLoss = 8.1196, kgLoss = 0.1951
2025-04-09 09:59:39.787218: Training Step 18/115: batchLoss = 4.0470, diffLoss = 6.6263, kgLoss = 0.1781
2025-04-09 09:59:40.516163: Training Step 19/115: batchLoss = 4.6225, diffLoss = 7.5889, kgLoss = 0.1730
2025-04-09 09:59:41.247605: Training Step 20/115: batchLoss = 4.6530, diffLoss = 7.6349, kgLoss = 0.1801
2025-04-09 09:59:41.987456: Training Step 21/115: batchLoss = 3.7454, diffLoss = 6.1421, kgLoss = 0.1505
2025-04-09 09:59:42.720797: Training Step 22/115: batchLoss = 4.2352, diffLoss = 6.9506, kgLoss = 0.1623
2025-04-09 09:59:43.449660: Training Step 23/115: batchLoss = 4.3124, diffLoss = 7.0794, kgLoss = 0.1619
2025-04-09 09:59:44.200728: Training Step 24/115: batchLoss = 4.8486, diffLoss = 7.9568, kgLoss = 0.1863
2025-04-09 09:59:44.928448: Training Step 25/115: batchLoss = 4.4468, diffLoss = 7.2940, kgLoss = 0.1761
2025-04-09 09:59:45.650869: Training Step 26/115: batchLoss = 5.0220, diffLoss = 8.2356, kgLoss = 0.2016
2025-04-09 09:59:46.382836: Training Step 27/115: batchLoss = 4.2524, diffLoss = 6.9763, kgLoss = 0.1666
2025-04-09 09:59:47.127492: Training Step 28/115: batchLoss = 4.2331, diffLoss = 6.9378, kgLoss = 0.1759
2025-04-09 09:59:47.888539: Training Step 29/115: batchLoss = 4.7494, diffLoss = 7.7928, kgLoss = 0.1844
2025-04-09 09:59:48.624702: Training Step 30/115: batchLoss = 3.9841, diffLoss = 6.5341, kgLoss = 0.1589
2025-04-09 09:59:49.353534: Training Step 31/115: batchLoss = 4.6676, diffLoss = 7.6621, kgLoss = 0.1759
2025-04-09 09:59:50.088495: Training Step 32/115: batchLoss = 4.2102, diffLoss = 6.9070, kgLoss = 0.1650
2025-04-09 09:59:50.836958: Training Step 33/115: batchLoss = 4.2595, diffLoss = 6.9805, kgLoss = 0.1780
2025-04-09 09:59:51.574881: Training Step 34/115: batchLoss = 5.2162, diffLoss = 8.5611, kgLoss = 0.1989
2025-04-09 09:59:52.315869: Training Step 35/115: batchLoss = 4.1101, diffLoss = 6.7393, kgLoss = 0.1662
2025-04-09 09:59:53.043273: Training Step 36/115: batchLoss = 3.8151, diffLoss = 6.2525, kgLoss = 0.1590
2025-04-09 09:59:53.770116: Training Step 37/115: batchLoss = 4.5165, diffLoss = 7.4115, kgLoss = 0.1739
2025-04-09 09:59:54.509699: Training Step 38/115: batchLoss = 4.1678, diffLoss = 6.8332, kgLoss = 0.1698
2025-04-09 09:59:55.253306: Training Step 39/115: batchLoss = 4.0023, diffLoss = 6.5611, kgLoss = 0.1641
2025-04-09 09:59:55.995668: Training Step 40/115: batchLoss = 4.9738, diffLoss = 8.1585, kgLoss = 0.1967
2025-04-09 09:59:56.733838: Training Step 41/115: batchLoss = 4.1983, diffLoss = 6.8853, kgLoss = 0.1677
2025-04-09 09:59:57.467065: Training Step 42/115: batchLoss = 4.4337, diffLoss = 7.2744, kgLoss = 0.1726
2025-04-09 09:59:58.225797: Training Step 43/115: batchLoss = 4.0840, diffLoss = 6.7016, kgLoss = 0.1577
2025-04-09 09:59:58.962668: Training Step 44/115: batchLoss = 4.7327, diffLoss = 7.7627, kgLoss = 0.1877
2025-04-09 09:59:59.694747: Training Step 45/115: batchLoss = 4.4207, diffLoss = 7.2522, kgLoss = 0.1736
2025-04-09 10:00:00.427026: Training Step 46/115: batchLoss = 4.2843, diffLoss = 7.0295, kgLoss = 0.1666
2025-04-09 10:00:01.167690: Training Step 47/115: batchLoss = 4.8234, diffLoss = 7.9118, kgLoss = 0.1909
2025-04-09 10:00:01.904257: Training Step 48/115: batchLoss = 5.0806, diffLoss = 8.3362, kgLoss = 0.1972
2025-04-09 10:00:02.638663: Training Step 49/115: batchLoss = 4.1678, diffLoss = 6.8285, kgLoss = 0.1769
2025-04-09 10:00:03.382313: Training Step 50/115: batchLoss = 4.0627, diffLoss = 6.6564, kgLoss = 0.1722
2025-04-09 10:00:04.134918: Training Step 51/115: batchLoss = 4.6396, diffLoss = 7.6123, kgLoss = 0.1804
2025-04-09 10:00:04.874088: Training Step 52/115: batchLoss = 4.8612, diffLoss = 7.9748, kgLoss = 0.1909
2025-04-09 10:00:05.609977: Training Step 53/115: batchLoss = 5.0682, diffLoss = 8.3062, kgLoss = 0.2112
2025-04-09 10:00:06.348419: Training Step 54/115: batchLoss = 4.2335, diffLoss = 6.9479, kgLoss = 0.1619
2025-04-09 10:00:07.108836: Training Step 55/115: batchLoss = 4.1526, diffLoss = 6.8023, kgLoss = 0.1781
2025-04-09 10:00:07.864371: Training Step 56/115: batchLoss = 4.1724, diffLoss = 6.8422, kgLoss = 0.1678
2025-04-09 10:00:08.601973: Training Step 57/115: batchLoss = 3.8479, diffLoss = 6.3038, kgLoss = 0.1642
2025-04-09 10:00:09.344664: Training Step 58/115: batchLoss = 4.1476, diffLoss = 6.8022, kgLoss = 0.1656
2025-04-09 10:00:10.078020: Training Step 59/115: batchLoss = 4.1586, diffLoss = 6.8226, kgLoss = 0.1625
2025-04-09 10:00:10.801573: Training Step 60/115: batchLoss = 3.9212, diffLoss = 6.4115, kgLoss = 0.1856
2025-04-09 10:00:11.547027: Training Step 61/115: batchLoss = 4.4266, diffLoss = 7.2614, kgLoss = 0.1743
2025-04-09 10:00:12.301860: Training Step 62/115: batchLoss = 5.1360, diffLoss = 8.4285, kgLoss = 0.1971
2025-04-09 10:00:13.025213: Training Step 63/115: batchLoss = 4.2541, diffLoss = 6.9773, kgLoss = 0.1693
2025-04-09 10:00:13.750899: Training Step 64/115: batchLoss = 4.5212, diffLoss = 7.4121, kgLoss = 0.1850
2025-04-09 10:00:14.491185: Training Step 65/115: batchLoss = 4.2834, diffLoss = 7.0026, kgLoss = 0.2046
2025-04-09 10:00:15.224894: Training Step 66/115: batchLoss = 4.7489, diffLoss = 7.7866, kgLoss = 0.1923
2025-04-09 10:00:15.960480: Training Step 67/115: batchLoss = 3.9620, diffLoss = 6.4972, kgLoss = 0.1591
2025-04-09 10:00:16.699380: Training Step 68/115: batchLoss = 4.2232, diffLoss = 6.9256, kgLoss = 0.1696
2025-04-09 10:00:17.437211: Training Step 69/115: batchLoss = 4.4730, diffLoss = 7.3379, kgLoss = 0.1755
2025-04-09 10:00:18.184015: Training Step 70/115: batchLoss = 4.6884, diffLoss = 7.6891, kgLoss = 0.1874
2025-04-09 10:00:18.934965: Training Step 71/115: batchLoss = 4.4589, diffLoss = 7.3087, kgLoss = 0.1842
2025-04-09 10:00:19.694428: Training Step 72/115: batchLoss = 4.3791, diffLoss = 7.1808, kgLoss = 0.1767
2025-04-09 10:00:20.434977: Training Step 73/115: batchLoss = 4.6398, diffLoss = 7.6156, kgLoss = 0.1763
2025-04-09 10:00:21.172990: Training Step 74/115: batchLoss = 4.3708, diffLoss = 7.1714, kgLoss = 0.1699
2025-04-09 10:00:21.911717: Training Step 75/115: batchLoss = 4.7612, diffLoss = 7.8134, kgLoss = 0.1828
2025-04-09 10:00:22.648698: Training Step 76/115: batchLoss = 4.2744, diffLoss = 7.0044, kgLoss = 0.1794
2025-04-09 10:00:23.404848: Training Step 77/115: batchLoss = 4.4424, diffLoss = 7.2833, kgLoss = 0.1810
2025-04-09 10:00:24.148527: Training Step 78/115: batchLoss = 4.6111, diffLoss = 7.5654, kgLoss = 0.1797
2025-04-09 10:00:24.878596: Training Step 79/115: batchLoss = 4.6487, diffLoss = 7.6299, kgLoss = 0.1768
2025-04-09 10:00:25.615661: Training Step 80/115: batchLoss = 3.9566, diffLoss = 6.4791, kgLoss = 0.1730
2025-04-09 10:00:26.373923: Training Step 81/115: batchLoss = 4.6338, diffLoss = 7.5983, kgLoss = 0.1870
2025-04-09 10:00:27.105173: Training Step 82/115: batchLoss = 4.6057, diffLoss = 7.5537, kgLoss = 0.1837
2025-04-09 10:00:27.851841: Training Step 83/115: batchLoss = 4.4898, diffLoss = 7.3672, kgLoss = 0.1738
2025-04-09 10:00:28.579875: Training Step 84/115: batchLoss = 4.4049, diffLoss = 7.2301, kgLoss = 0.1670
2025-04-09 10:00:29.310890: Training Step 85/115: batchLoss = 4.2728, diffLoss = 6.9955, kgLoss = 0.1887
2025-04-09 10:00:30.057868: Training Step 86/115: batchLoss = 5.2835, diffLoss = 8.6758, kgLoss = 0.1952
2025-04-09 10:00:30.804813: Training Step 87/115: batchLoss = 3.9872, diffLoss = 6.5404, kgLoss = 0.1575
2025-04-09 10:00:31.531317: Training Step 88/115: batchLoss = 4.2574, diffLoss = 6.9783, kgLoss = 0.1760
2025-04-09 10:00:32.256361: Training Step 89/115: batchLoss = 5.1299, diffLoss = 8.4060, kgLoss = 0.2157
2025-04-09 10:00:32.982246: Training Step 90/115: batchLoss = 4.3703, diffLoss = 7.1708, kgLoss = 0.1695
2025-04-09 10:00:33.708909: Training Step 91/115: batchLoss = 4.3976, diffLoss = 7.2090, kgLoss = 0.1805
2025-04-09 10:00:34.443502: Training Step 92/115: batchLoss = 3.8939, diffLoss = 6.3861, kgLoss = 0.1557
2025-04-09 10:00:35.174397: Training Step 93/115: batchLoss = 4.7368, diffLoss = 7.7694, kgLoss = 0.1879
2025-04-09 10:00:35.905882: Training Step 94/115: batchLoss = 4.4184, diffLoss = 7.2458, kgLoss = 0.1773
2025-04-09 10:00:36.635682: Training Step 95/115: batchLoss = 5.1086, diffLoss = 8.3862, kgLoss = 0.1924
2025-04-09 10:00:37.370224: Training Step 96/115: batchLoss = 4.4132, diffLoss = 7.2365, kgLoss = 0.1783
2025-04-09 10:00:38.103387: Training Step 97/115: batchLoss = 4.1888, diffLoss = 6.8715, kgLoss = 0.1648
2025-04-09 10:00:38.848494: Training Step 98/115: batchLoss = 4.5178, diffLoss = 7.4108, kgLoss = 0.1783
2025-04-09 10:00:39.581967: Training Step 99/115: batchLoss = 4.4347, diffLoss = 7.2737, kgLoss = 0.1762
2025-04-09 10:00:40.322176: Training Step 100/115: batchLoss = 4.4756, diffLoss = 7.3394, kgLoss = 0.1799
2025-04-09 10:00:41.055985: Training Step 101/115: batchLoss = 3.9992, diffLoss = 6.5608, kgLoss = 0.1568
2025-04-09 10:00:41.779959: Training Step 102/115: batchLoss = 4.5986, diffLoss = 7.5475, kgLoss = 0.1752
2025-04-09 10:00:42.503291: Training Step 103/115: batchLoss = 4.1494, diffLoss = 6.8050, kgLoss = 0.1660
2025-04-09 10:00:43.233952: Training Step 104/115: batchLoss = 4.4673, diffLoss = 7.3304, kgLoss = 0.1725
2025-04-09 10:00:43.955945: Training Step 105/115: batchLoss = 4.3293, diffLoss = 7.1024, kgLoss = 0.1696
2025-04-09 10:00:44.693816: Training Step 106/115: batchLoss = 4.2378, diffLoss = 6.9547, kgLoss = 0.1626
2025-04-09 10:00:45.418164: Training Step 107/115: batchLoss = 4.4665, diffLoss = 7.3213, kgLoss = 0.1844
2025-04-09 10:00:46.142158: Training Step 108/115: batchLoss = 3.9964, diffLoss = 6.5571, kgLoss = 0.1554
2025-04-09 10:00:46.868079: Training Step 109/115: batchLoss = 4.9915, diffLoss = 8.1842, kgLoss = 0.2024
2025-04-09 10:00:47.596333: Training Step 110/115: batchLoss = 4.2432, diffLoss = 6.9629, kgLoss = 0.1636
2025-04-09 10:00:48.335559: Training Step 111/115: batchLoss = 3.8997, diffLoss = 6.3920, kgLoss = 0.1614
2025-04-09 10:00:49.057045: Training Step 112/115: batchLoss = 4.3113, diffLoss = 7.0708, kgLoss = 0.1722
2025-04-09 10:00:49.717210: Training Step 113/115: batchLoss = 4.3108, diffLoss = 7.0734, kgLoss = 0.1670
2025-04-09 10:00:50.338458: Training Step 114/115: batchLoss = 4.1929, diffLoss = 6.8795, kgLoss = 0.1629
2025-04-09 10:00:50.454958: 
2025-04-09 10:00:50.455633: Epoch 18/1000, Train: epLoss = 1.2737, epDfLoss = 2.0890, epfTransLoss = 0.0000, epKgLoss = 0.0508  
2025-04-09 10:00:51.235834: Steps 0/90: batch_recall = 55.63, batch_ndcg = 42.04 
2025-04-09 10:00:51.980816: Steps 1/90: batch_recall = 57.45, batch_ndcg = 40.33 
2025-04-09 10:00:52.736362: Steps 2/90: batch_recall = 51.40, batch_ndcg = 36.87 
2025-04-09 10:00:53.486599: Steps 3/90: batch_recall = 46.30, batch_ndcg = 32.64 
2025-04-09 10:00:54.244216: Steps 4/90: batch_recall = 49.74, batch_ndcg = 35.87 
2025-04-09 10:00:54.993010: Steps 5/90: batch_recall = 35.61, batch_ndcg = 25.71 
2025-04-09 10:00:55.733532: Steps 6/90: batch_recall = 48.43, batch_ndcg = 34.60 
2025-04-09 10:00:56.488701: Steps 7/90: batch_recall = 42.28, batch_ndcg = 30.52 
2025-04-09 10:00:57.229642: Steps 8/90: batch_recall = 46.93, batch_ndcg = 33.32 
2025-04-09 10:00:57.980013: Steps 9/90: batch_recall = 41.85, batch_ndcg = 32.07 
2025-04-09 10:00:58.723027: Steps 10/90: batch_recall = 37.22, batch_ndcg = 27.89 
2025-04-09 10:00:59.456431: Steps 11/90: batch_recall = 43.55, batch_ndcg = 27.60 
2025-04-09 10:01:00.190764: Steps 12/90: batch_recall = 36.65, batch_ndcg = 26.90 
2025-04-09 10:01:00.926191: Steps 13/90: batch_recall = 37.27, batch_ndcg = 25.36 
2025-04-09 10:01:01.660926: Steps 14/90: batch_recall = 33.36, batch_ndcg = 25.54 
2025-04-09 10:01:02.451842: Steps 15/90: batch_recall = 41.70, batch_ndcg = 27.67 
2025-04-09 10:01:03.198030: Steps 16/90: batch_recall = 35.24, batch_ndcg = 25.17 
2025-04-09 10:01:03.932207: Steps 17/90: batch_recall = 33.89, batch_ndcg = 22.70 
2025-04-09 10:01:04.653335: Steps 18/90: batch_recall = 37.74, batch_ndcg = 25.10 
2025-04-09 10:01:05.391645: Steps 19/90: batch_recall = 32.10, batch_ndcg = 22.81 
2025-04-09 10:01:06.122183: Steps 20/90: batch_recall = 33.80, batch_ndcg = 22.07 
2025-04-09 10:01:06.848395: Steps 21/90: batch_recall = 37.44, batch_ndcg = 26.53 
2025-04-09 10:01:07.578762: Steps 22/90: batch_recall = 41.06, batch_ndcg = 29.35 
2025-04-09 10:01:08.307298: Steps 23/90: batch_recall = 34.08, batch_ndcg = 23.71 
2025-04-09 10:01:09.016985: Steps 24/90: batch_recall = 37.42, batch_ndcg = 25.06 
2025-04-09 10:01:09.730432: Steps 25/90: batch_recall = 36.38, batch_ndcg = 23.31 
2025-04-09 10:01:10.452411: Steps 26/90: batch_recall = 38.44, batch_ndcg = 26.46 
2025-04-09 10:01:11.158737: Steps 27/90: batch_recall = 33.42, batch_ndcg = 23.45 
2025-04-09 10:01:11.878153: Steps 28/90: batch_recall = 35.33, batch_ndcg = 23.83 
2025-04-09 10:01:12.594050: Steps 29/90: batch_recall = 31.45, batch_ndcg = 21.31 
2025-04-09 10:01:13.299399: Steps 30/90: batch_recall = 30.39, batch_ndcg = 19.18 
2025-04-09 10:01:14.020583: Steps 31/90: batch_recall = 30.94, batch_ndcg = 21.15 
2025-04-09 10:01:14.734209: Steps 32/90: batch_recall = 32.95, batch_ndcg = 22.21 
2025-04-09 10:01:15.459110: Steps 33/90: batch_recall = 32.88, batch_ndcg = 20.72 
2025-04-09 10:01:16.184343: Steps 34/90: batch_recall = 33.56, batch_ndcg = 22.90 
2025-04-09 10:01:16.890403: Steps 35/90: batch_recall = 36.43, batch_ndcg = 25.26 
2025-04-09 10:01:17.604011: Steps 36/90: batch_recall = 35.55, batch_ndcg = 23.28 
2025-04-09 10:01:18.329999: Steps 37/90: batch_recall = 30.36, batch_ndcg = 20.08 
2025-04-09 10:01:19.042665: Steps 38/90: batch_recall = 32.64, batch_ndcg = 21.09 
2025-04-09 10:01:19.752508: Steps 39/90: batch_recall = 34.53, batch_ndcg = 20.79 
2025-04-09 10:01:20.462815: Steps 40/90: batch_recall = 32.21, batch_ndcg = 21.36 
2025-04-09 10:01:21.178540: Steps 41/90: batch_recall = 32.15, batch_ndcg = 21.79 
2025-04-09 10:01:21.894089: Steps 42/90: batch_recall = 37.99, batch_ndcg = 26.56 
2025-04-09 10:01:22.617854: Steps 43/90: batch_recall = 31.86, batch_ndcg = 20.89 
2025-04-09 10:01:23.349923: Steps 44/90: batch_recall = 34.61, batch_ndcg = 21.31 
2025-04-09 10:01:24.057476: Steps 45/90: batch_recall = 31.28, batch_ndcg = 19.53 
2025-04-09 10:01:24.758961: Steps 46/90: batch_recall = 30.59, batch_ndcg = 19.36 
2025-04-09 10:01:25.469485: Steps 47/90: batch_recall = 28.19, batch_ndcg = 17.91 
2025-04-09 10:01:26.169194: Steps 48/90: batch_recall = 35.23, batch_ndcg = 21.50 
2025-04-09 10:01:26.870725: Steps 49/90: batch_recall = 26.63, batch_ndcg = 17.60 
2025-04-09 10:01:27.569462: Steps 50/90: batch_recall = 35.80, batch_ndcg = 23.01 
2025-04-09 10:01:28.262553: Steps 51/90: batch_recall = 34.63, batch_ndcg = 23.48 
2025-04-09 10:01:28.977652: Steps 52/90: batch_recall = 32.84, batch_ndcg = 20.93 
2025-04-09 10:01:29.662865: Steps 53/90: batch_recall = 39.97, batch_ndcg = 24.25 
2025-04-09 10:01:30.369936: Steps 54/90: batch_recall = 27.90, batch_ndcg = 17.88 
2025-04-09 10:01:31.072446: Steps 55/90: batch_recall = 36.34, batch_ndcg = 22.24 
2025-04-09 10:01:31.805053: Steps 56/90: batch_recall = 29.86, batch_ndcg = 18.13 
2025-04-09 10:01:32.516751: Steps 57/90: batch_recall = 35.99, batch_ndcg = 22.29 
2025-04-09 10:01:33.221297: Steps 58/90: batch_recall = 37.46, batch_ndcg = 24.37 
2025-04-09 10:01:33.934812: Steps 59/90: batch_recall = 27.85, batch_ndcg = 18.33 
2025-04-09 10:01:34.661618: Steps 60/90: batch_recall = 29.94, batch_ndcg = 18.68 
2025-04-09 10:01:35.373462: Steps 61/90: batch_recall = 40.35, batch_ndcg = 27.10 
2025-04-09 10:01:36.073898: Steps 62/90: batch_recall = 36.41, batch_ndcg = 24.25 
2025-04-09 10:01:36.787930: Steps 63/90: batch_recall = 36.50, batch_ndcg = 22.64 
2025-04-09 10:01:37.502648: Steps 64/90: batch_recall = 34.73, batch_ndcg = 21.49 
2025-04-09 10:01:38.223983: Steps 65/90: batch_recall = 29.85, batch_ndcg = 19.40 
2025-04-09 10:01:38.942280: Steps 66/90: batch_recall = 34.91, batch_ndcg = 21.47 
2025-04-09 10:01:39.653754: Steps 67/90: batch_recall = 31.73, batch_ndcg = 19.00 
2025-04-09 10:01:40.358081: Steps 68/90: batch_recall = 32.07, batch_ndcg = 20.41 
2025-04-09 10:01:41.057790: Steps 69/90: batch_recall = 38.45, batch_ndcg = 22.70 
2025-04-09 10:01:41.763561: Steps 70/90: batch_recall = 36.03, batch_ndcg = 22.40 
2025-04-09 10:01:42.479034: Steps 71/90: batch_recall = 33.71, batch_ndcg = 20.69 
2025-04-09 10:01:43.166466: Steps 72/90: batch_recall = 32.31, batch_ndcg = 20.27 
2025-04-09 10:01:43.867781: Steps 73/90: batch_recall = 33.19, batch_ndcg = 20.75 
2025-04-09 10:01:44.555748: Steps 74/90: batch_recall = 37.89, batch_ndcg = 23.40 
2025-04-09 10:01:45.265709: Steps 75/90: batch_recall = 36.58, batch_ndcg = 22.67 
2025-04-09 10:01:45.986639: Steps 76/90: batch_recall = 37.56, batch_ndcg = 23.08 
2025-04-09 10:01:46.703512: Steps 77/90: batch_recall = 40.37, batch_ndcg = 24.25 
2025-04-09 10:01:47.408451: Steps 78/90: batch_recall = 31.77, batch_ndcg = 19.73 
2025-04-09 10:01:48.115104: Steps 79/90: batch_recall = 38.14, batch_ndcg = 25.89 
2025-04-09 10:01:48.809942: Steps 80/90: batch_recall = 41.04, batch_ndcg = 25.55 
2025-04-09 10:01:49.511116: Steps 81/90: batch_recall = 40.36, batch_ndcg = 24.53 
2025-04-09 10:01:50.213697: Steps 82/90: batch_recall = 37.02, batch_ndcg = 23.21 
2025-04-09 10:01:50.925728: Steps 83/90: batch_recall = 35.28, batch_ndcg = 22.65 
2025-04-09 10:01:51.637809: Steps 84/90: batch_recall = 33.88, batch_ndcg = 20.01 
2025-04-09 10:01:52.339286: Steps 85/90: batch_recall = 36.71, batch_ndcg = 20.45 
2025-04-09 10:01:53.029518: Steps 86/90: batch_recall = 41.70, batch_ndcg = 25.49 
2025-04-09 10:01:53.737112: Steps 87/90: batch_recall = 38.92, batch_ndcg = 26.28 
2025-04-09 10:01:54.444156: Steps 88/90: batch_recall = 47.47, batch_ndcg = 26.85 
2025-04-09 10:01:54.991543: Steps 89/90: batch_recall = 37.22, batch_ndcg = 22.89 
2025-04-09 10:01:54.992472: Epoch 18/1000, Test: Recall = 0.0735, NDCG = 0.0471  

2025-04-09 10:01:56.000300: Training Step 0/115: batchLoss = 4.3603, diffLoss = 7.1423, kgLoss = 0.1874
2025-04-09 10:01:56.724743: Training Step 1/115: batchLoss = 3.9346, diffLoss = 6.4490, kgLoss = 0.1631
2025-04-09 10:01:57.452478: Training Step 2/115: batchLoss = 3.8455, diffLoss = 6.3030, kgLoss = 0.1593
2025-04-09 10:01:58.189719: Training Step 3/115: batchLoss = 4.3683, diffLoss = 7.1600, kgLoss = 0.1809
2025-04-09 10:01:58.934446: Training Step 4/115: batchLoss = 4.4585, diffLoss = 7.3029, kgLoss = 0.1918
2025-04-09 10:01:59.664867: Training Step 5/115: batchLoss = 4.3043, diffLoss = 7.0595, kgLoss = 0.1715
2025-04-09 10:02:00.405100: Training Step 6/115: batchLoss = 4.4221, diffLoss = 7.2539, kgLoss = 0.1745
2025-04-09 10:02:01.149525: Training Step 7/115: batchLoss = 4.4847, diffLoss = 7.3606, kgLoss = 0.1708
2025-04-09 10:02:01.895540: Training Step 8/115: batchLoss = 4.8849, diffLoss = 7.9968, kgLoss = 0.2172
2025-04-09 10:02:02.629982: Training Step 9/115: batchLoss = 4.4667, diffLoss = 7.3217, kgLoss = 0.1843
2025-04-09 10:02:03.365244: Training Step 10/115: batchLoss = 4.3127, diffLoss = 7.0775, kgLoss = 0.1655
2025-04-09 10:02:04.098462: Training Step 11/115: batchLoss = 4.0370, diffLoss = 6.6198, kgLoss = 0.1627
2025-04-09 10:02:04.833943: Training Step 12/115: batchLoss = 3.5704, diffLoss = 5.8483, kgLoss = 0.1535
2025-04-09 10:02:05.570467: Training Step 13/115: batchLoss = 4.2733, diffLoss = 6.9980, kgLoss = 0.1863
2025-04-09 10:02:06.298286: Training Step 14/115: batchLoss = 5.1116, diffLoss = 8.3805, kgLoss = 0.2081
2025-04-09 10:02:07.020328: Training Step 15/115: batchLoss = 5.1660, diffLoss = 8.4745, kgLoss = 0.2032
2025-04-09 10:02:07.748864: Training Step 16/115: batchLoss = 4.2452, diffLoss = 6.9656, kgLoss = 0.1645
2025-04-09 10:02:08.485067: Training Step 17/115: batchLoss = 4.5780, diffLoss = 7.5108, kgLoss = 0.1788
2025-04-09 10:02:09.203505: Training Step 18/115: batchLoss = 4.3101, diffLoss = 7.0695, kgLoss = 0.1709
2025-04-09 10:02:09.928932: Training Step 19/115: batchLoss = 4.9253, diffLoss = 8.0788, kgLoss = 0.1950
2025-04-09 10:02:10.656374: Training Step 20/115: batchLoss = 4.4650, diffLoss = 7.3189, kgLoss = 0.1840
2025-04-09 10:02:11.385577: Training Step 21/115: batchLoss = 4.2460, diffLoss = 6.9667, kgLoss = 0.1649
2025-04-09 10:02:12.108508: Training Step 22/115: batchLoss = 4.3267, diffLoss = 7.0976, kgLoss = 0.1703
2025-04-09 10:02:12.829292: Training Step 23/115: batchLoss = 4.5257, diffLoss = 7.4216, kgLoss = 0.1818
2025-04-09 10:02:13.549059: Training Step 24/115: batchLoss = 4.0370, diffLoss = 6.6119, kgLoss = 0.1746
2025-04-09 10:02:14.281813: Training Step 25/115: batchLoss = 4.4093, diffLoss = 7.2332, kgLoss = 0.1736
2025-04-09 10:02:15.015159: Training Step 26/115: batchLoss = 4.1404, diffLoss = 6.7923, kgLoss = 0.1625
2025-04-09 10:02:15.755208: Training Step 27/115: batchLoss = 4.3759, diffLoss = 7.1713, kgLoss = 0.1829
2025-04-09 10:02:16.498666: Training Step 28/115: batchLoss = 4.4883, diffLoss = 7.3622, kgLoss = 0.1775
2025-04-09 10:02:17.248592: Training Step 29/115: batchLoss = 4.8824, diffLoss = 8.0047, kgLoss = 0.1990
2025-04-09 10:02:17.994196: Training Step 30/115: batchLoss = 4.6370, diffLoss = 7.6067, kgLoss = 0.1825
2025-04-09 10:02:18.742530: Training Step 31/115: batchLoss = 4.2935, diffLoss = 7.0444, kgLoss = 0.1672
2025-04-09 10:02:19.493658: Training Step 32/115: batchLoss = 3.9395, diffLoss = 6.4569, kgLoss = 0.1633
2025-04-09 10:02:20.246934: Training Step 33/115: batchLoss = 4.6144, diffLoss = 7.5713, kgLoss = 0.1791
2025-04-09 10:02:20.989363: Training Step 34/115: batchLoss = 4.1297, diffLoss = 6.7719, kgLoss = 0.1665
2025-04-09 10:02:21.737911: Training Step 35/115: batchLoss = 4.1941, diffLoss = 6.8782, kgLoss = 0.1678
2025-04-09 10:02:22.479672: Training Step 36/115: batchLoss = 5.2078, diffLoss = 8.5486, kgLoss = 0.1967
2025-04-09 10:02:23.211668: Training Step 37/115: batchLoss = 4.1602, diffLoss = 6.8214, kgLoss = 0.1683
2025-04-09 10:02:23.949136: Training Step 38/115: batchLoss = 4.6091, diffLoss = 7.5572, kgLoss = 0.1871
2025-04-09 10:02:24.683511: Training Step 39/115: batchLoss = 4.7681, diffLoss = 7.8266, kgLoss = 0.1802
2025-04-09 10:02:25.416985: Training Step 40/115: batchLoss = 4.6206, diffLoss = 7.5827, kgLoss = 0.1773
2025-04-09 10:02:26.154278: Training Step 41/115: batchLoss = 4.3836, diffLoss = 7.1910, kgLoss = 0.1725
2025-04-09 10:02:26.873317: Training Step 42/115: batchLoss = 4.1669, diffLoss = 6.8356, kgLoss = 0.1638
2025-04-09 10:02:27.595793: Training Step 43/115: batchLoss = 4.3710, diffLoss = 7.1659, kgLoss = 0.1786
2025-04-09 10:02:28.319918: Training Step 44/115: batchLoss = 4.2441, diffLoss = 6.9617, kgLoss = 0.1676
2025-04-09 10:02:29.064163: Training Step 45/115: batchLoss = 4.3750, diffLoss = 7.1769, kgLoss = 0.1722
2025-04-09 10:02:29.793262: Training Step 46/115: batchLoss = 4.1307, diffLoss = 6.7669, kgLoss = 0.1765
2025-04-09 10:02:30.523163: Training Step 47/115: batchLoss = 4.3325, diffLoss = 7.1130, kgLoss = 0.1618
2025-04-09 10:02:31.269675: Training Step 48/115: batchLoss = 4.4635, diffLoss = 7.3285, kgLoss = 0.1659
2025-04-09 10:02:32.027888: Training Step 49/115: batchLoss = 4.8119, diffLoss = 7.8937, kgLoss = 0.1891
2025-04-09 10:02:32.784868: Training Step 50/115: batchLoss = 4.1311, diffLoss = 6.7770, kgLoss = 0.1623
2025-04-09 10:02:33.525487: Training Step 51/115: batchLoss = 4.2053, diffLoss = 6.8986, kgLoss = 0.1654
2025-04-09 10:02:34.266237: Training Step 52/115: batchLoss = 4.2356, diffLoss = 6.9516, kgLoss = 0.1616
2025-04-09 10:02:35.005197: Training Step 53/115: batchLoss = 4.0856, diffLoss = 6.7028, kgLoss = 0.1596
2025-04-09 10:02:35.747664: Training Step 54/115: batchLoss = 4.1202, diffLoss = 6.7547, kgLoss = 0.1684
2025-04-09 10:02:36.479877: Training Step 55/115: batchLoss = 4.0201, diffLoss = 6.5881, kgLoss = 0.1680
2025-04-09 10:02:37.209765: Training Step 56/115: batchLoss = 4.7577, diffLoss = 7.8007, kgLoss = 0.1931
2025-04-09 10:02:37.951766: Training Step 57/115: batchLoss = 4.2026, diffLoss = 6.8889, kgLoss = 0.1732
2025-04-09 10:02:38.681540: Training Step 58/115: batchLoss = 4.3867, diffLoss = 7.2000, kgLoss = 0.1667
2025-04-09 10:02:39.408667: Training Step 59/115: batchLoss = 4.2633, diffLoss = 6.9899, kgLoss = 0.1735
2025-04-09 10:02:40.151549: Training Step 60/115: batchLoss = 4.6886, diffLoss = 7.6938, kgLoss = 0.1808
2025-04-09 10:02:40.881575: Training Step 61/115: batchLoss = 4.6762, diffLoss = 7.6749, kgLoss = 0.1782
2025-04-09 10:02:41.607504: Training Step 62/115: batchLoss = 4.1683, diffLoss = 6.8388, kgLoss = 0.1625
2025-04-09 10:02:42.333520: Training Step 63/115: batchLoss = 4.5525, diffLoss = 7.4681, kgLoss = 0.1790
2025-04-09 10:02:43.070384: Training Step 64/115: batchLoss = 4.1841, diffLoss = 6.8642, kgLoss = 0.1639
2025-04-09 10:02:43.814393: Training Step 65/115: batchLoss = 4.2466, diffLoss = 6.9578, kgLoss = 0.1798
2025-04-09 10:02:44.547540: Training Step 66/115: batchLoss = 3.9601, diffLoss = 6.4901, kgLoss = 0.1650
2025-04-09 10:02:45.279689: Training Step 67/115: batchLoss = 4.5475, diffLoss = 7.4565, kgLoss = 0.1840
2025-04-09 10:02:46.011736: Training Step 68/115: batchLoss = 4.6560, diffLoss = 7.6357, kgLoss = 0.1864
2025-04-09 10:02:46.762385: Training Step 69/115: batchLoss = 3.9318, diffLoss = 6.4441, kgLoss = 0.1634
2025-04-09 10:02:47.495195: Training Step 70/115: batchLoss = 4.9609, diffLoss = 8.1359, kgLoss = 0.1984
2025-04-09 10:02:48.227793: Training Step 71/115: batchLoss = 3.8017, diffLoss = 6.2345, kgLoss = 0.1527
2025-04-09 10:02:48.967259: Training Step 72/115: batchLoss = 4.3818, diffLoss = 7.1860, kgLoss = 0.1753
2025-04-09 10:02:49.715820: Training Step 73/115: batchLoss = 3.9139, diffLoss = 6.4193, kgLoss = 0.1558
2025-04-09 10:02:50.456743: Training Step 74/115: batchLoss = 3.9469, diffLoss = 6.4730, kgLoss = 0.1578
2025-04-09 10:02:51.223525: Training Step 75/115: batchLoss = 4.6903, diffLoss = 7.6945, kgLoss = 0.1841
2025-04-09 10:02:51.971756: Training Step 76/115: batchLoss = 4.0572, diffLoss = 6.6423, kgLoss = 0.1795
2025-04-09 10:02:52.734320: Training Step 77/115: batchLoss = 4.6391, diffLoss = 7.6091, kgLoss = 0.1840
2025-04-09 10:02:53.473427: Training Step 78/115: batchLoss = 4.6193, diffLoss = 7.5856, kgLoss = 0.1698
2025-04-09 10:02:54.206183: Training Step 79/115: batchLoss = 3.9323, diffLoss = 6.4441, kgLoss = 0.1645
2025-04-09 10:02:54.943968: Training Step 80/115: batchLoss = 4.1644, diffLoss = 6.8301, kgLoss = 0.1658
2025-04-09 10:02:55.667641: Training Step 81/115: batchLoss = 4.2911, diffLoss = 7.0394, kgLoss = 0.1685
2025-04-09 10:02:56.391669: Training Step 82/115: batchLoss = 4.4377, diffLoss = 7.2767, kgLoss = 0.1791
2025-04-09 10:02:57.117436: Training Step 83/115: batchLoss = 4.5075, diffLoss = 7.3907, kgLoss = 0.1828
2025-04-09 10:02:57.843761: Training Step 84/115: batchLoss = 3.8151, diffLoss = 6.2485, kgLoss = 0.1650
2025-04-09 10:02:58.586034: Training Step 85/115: batchLoss = 3.9751, diffLoss = 6.5151, kgLoss = 0.1650
2025-04-09 10:02:59.318765: Training Step 86/115: batchLoss = 4.2386, diffLoss = 6.9563, kgLoss = 0.1621
2025-04-09 10:03:00.066316: Training Step 87/115: batchLoss = 4.9280, diffLoss = 8.0870, kgLoss = 0.1895
2025-04-09 10:03:00.820930: Training Step 88/115: batchLoss = 4.4340, diffLoss = 7.2660, kgLoss = 0.1860
2025-04-09 10:03:01.573620: Training Step 89/115: batchLoss = 4.5164, diffLoss = 7.4099, kgLoss = 0.1761
2025-04-09 10:03:02.300878: Training Step 90/115: batchLoss = 4.7163, diffLoss = 7.7306, kgLoss = 0.1948
2025-04-09 10:03:03.027090: Training Step 91/115: batchLoss = 4.3329, diffLoss = 7.1065, kgLoss = 0.1725
2025-04-09 10:03:03.747271: Training Step 92/115: batchLoss = 4.4682, diffLoss = 7.3289, kgLoss = 0.1770
2025-04-09 10:03:04.467671: Training Step 93/115: batchLoss = 4.4946, diffLoss = 7.3757, kgLoss = 0.1728
2025-04-09 10:03:05.196595: Training Step 94/115: batchLoss = 4.2364, diffLoss = 6.9380, kgLoss = 0.1841
2025-04-09 10:03:05.932257: Training Step 95/115: batchLoss = 4.7305, diffLoss = 7.7591, kgLoss = 0.1878
2025-04-09 10:03:06.655082: Training Step 96/115: batchLoss = 4.1027, diffLoss = 6.7226, kgLoss = 0.1729
2025-04-09 10:03:07.379315: Training Step 97/115: batchLoss = 4.4804, diffLoss = 7.3539, kgLoss = 0.1701
2025-04-09 10:03:08.099055: Training Step 98/115: batchLoss = 4.2764, diffLoss = 7.0109, kgLoss = 0.1746
2025-04-09 10:03:08.823692: Training Step 99/115: batchLoss = 4.1422, diffLoss = 6.7865, kgLoss = 0.1758
2025-04-09 10:03:09.542641: Training Step 100/115: batchLoss = 4.2840, diffLoss = 7.0238, kgLoss = 0.1743
2025-04-09 10:03:10.263535: Training Step 101/115: batchLoss = 4.5153, diffLoss = 7.4027, kgLoss = 0.1843
2025-04-09 10:03:10.982329: Training Step 102/115: batchLoss = 4.2084, diffLoss = 6.8940, kgLoss = 0.1799
2025-04-09 10:03:11.703273: Training Step 103/115: batchLoss = 4.7483, diffLoss = 7.7840, kgLoss = 0.1947
2025-04-09 10:03:12.417280: Training Step 104/115: batchLoss = 4.2775, diffLoss = 7.0085, kgLoss = 0.1809
2025-04-09 10:03:13.139532: Training Step 105/115: batchLoss = 5.2220, diffLoss = 8.5662, kgLoss = 0.2058
2025-04-09 10:03:13.870775: Training Step 106/115: batchLoss = 4.9240, diffLoss = 8.0781, kgLoss = 0.1929
2025-04-09 10:03:14.611103: Training Step 107/115: batchLoss = 4.5796, diffLoss = 7.5097, kgLoss = 0.1845
2025-04-09 10:03:15.339624: Training Step 108/115: batchLoss = 4.5309, diffLoss = 7.4269, kgLoss = 0.1869
2025-04-09 10:03:16.059661: Training Step 109/115: batchLoss = 4.4574, diffLoss = 7.3105, kgLoss = 0.1777
2025-04-09 10:03:16.785544: Training Step 110/115: batchLoss = 4.7506, diffLoss = 7.7892, kgLoss = 0.1927
2025-04-09 10:03:17.505767: Training Step 111/115: batchLoss = 4.7109, diffLoss = 7.7335, kgLoss = 0.1769
2025-04-09 10:03:18.213318: Training Step 112/115: batchLoss = 4.5377, diffLoss = 7.4453, kgLoss = 0.1764
2025-04-09 10:03:18.850672: Training Step 113/115: batchLoss = 4.5656, diffLoss = 7.4933, kgLoss = 0.1742
2025-04-09 10:03:19.477193: Training Step 114/115: batchLoss = 4.9032, diffLoss = 8.0463, kgLoss = 0.1887
2025-04-09 10:03:19.600545: 
2025-04-09 10:03:19.601201: Epoch 19/1000, Train: epLoss = 1.2647, epDfLoss = 2.0740, epfTransLoss = 0.0000, epKgLoss = 0.0507  
2025-04-09 10:03:20.368485: Steps 0/90: batch_recall = 55.76, batch_ndcg = 41.34 
2025-04-09 10:03:21.115127: Steps 1/90: batch_recall = 59.21, batch_ndcg = 40.60 
2025-04-09 10:03:21.883602: Steps 2/90: batch_recall = 50.60, batch_ndcg = 36.41 
2025-04-09 10:03:22.634932: Steps 3/90: batch_recall = 46.56, batch_ndcg = 32.96 
2025-04-09 10:03:23.382203: Steps 4/90: batch_recall = 49.10, batch_ndcg = 35.81 
2025-04-09 10:03:24.132522: Steps 5/90: batch_recall = 35.16, batch_ndcg = 25.79 
2025-04-09 10:03:24.869845: Steps 6/90: batch_recall = 48.06, batch_ndcg = 34.60 
2025-04-09 10:03:25.619356: Steps 7/90: batch_recall = 43.45, batch_ndcg = 30.93 
2025-04-09 10:03:26.368950: Steps 8/90: batch_recall = 47.70, batch_ndcg = 33.43 
2025-04-09 10:03:27.129630: Steps 9/90: batch_recall = 41.77, batch_ndcg = 32.79 
2025-04-09 10:03:27.890035: Steps 10/90: batch_recall = 39.01, batch_ndcg = 28.15 
2025-04-09 10:03:28.644659: Steps 11/90: batch_recall = 44.70, batch_ndcg = 28.00 
2025-04-09 10:03:29.411927: Steps 12/90: batch_recall = 38.33, batch_ndcg = 27.34 
2025-04-09 10:03:30.162746: Steps 13/90: batch_recall = 37.84, batch_ndcg = 25.54 
2025-04-09 10:03:30.911798: Steps 14/90: batch_recall = 34.46, batch_ndcg = 25.90 
2025-04-09 10:03:31.639731: Steps 15/90: batch_recall = 43.84, batch_ndcg = 28.43 
2025-04-09 10:03:32.368616: Steps 16/90: batch_recall = 36.70, batch_ndcg = 25.69 
2025-04-09 10:03:33.095355: Steps 17/90: batch_recall = 34.78, batch_ndcg = 22.72 
2025-04-09 10:03:33.809404: Steps 18/90: batch_recall = 37.27, batch_ndcg = 25.06 
2025-04-09 10:03:34.541787: Steps 19/90: batch_recall = 31.91, batch_ndcg = 23.19 
2025-04-09 10:03:35.267744: Steps 20/90: batch_recall = 34.65, batch_ndcg = 22.79 
2025-04-09 10:03:35.996919: Steps 21/90: batch_recall = 38.39, batch_ndcg = 27.04 
2025-04-09 10:03:36.708326: Steps 22/90: batch_recall = 41.16, batch_ndcg = 28.80 
2025-04-09 10:03:37.429783: Steps 23/90: batch_recall = 35.25, batch_ndcg = 24.15 
2025-04-09 10:03:38.152022: Steps 24/90: batch_recall = 37.91, batch_ndcg = 25.46 
2025-04-09 10:03:38.867514: Steps 25/90: batch_recall = 38.13, batch_ndcg = 24.12 
2025-04-09 10:03:39.587738: Steps 26/90: batch_recall = 38.96, batch_ndcg = 27.35 
2025-04-09 10:03:40.320542: Steps 27/90: batch_recall = 34.17, batch_ndcg = 23.79 
2025-04-09 10:03:41.043348: Steps 28/90: batch_recall = 35.82, batch_ndcg = 24.10 
2025-04-09 10:03:41.762583: Steps 29/90: batch_recall = 32.29, batch_ndcg = 21.66 
2025-04-09 10:03:42.486450: Steps 30/90: batch_recall = 31.40, batch_ndcg = 19.57 
2025-04-09 10:03:43.201931: Steps 31/90: batch_recall = 32.55, batch_ndcg = 21.72 
2025-04-09 10:03:43.914773: Steps 32/90: batch_recall = 33.27, batch_ndcg = 22.57 
2025-04-09 10:03:44.628357: Steps 33/90: batch_recall = 34.07, batch_ndcg = 21.20 
2025-04-09 10:03:45.358079: Steps 34/90: batch_recall = 34.29, batch_ndcg = 23.06 
2025-04-09 10:03:46.066794: Steps 35/90: batch_recall = 37.32, batch_ndcg = 25.55 
2025-04-09 10:03:46.781193: Steps 36/90: batch_recall = 37.45, batch_ndcg = 24.11 
2025-04-09 10:03:47.510712: Steps 37/90: batch_recall = 32.24, batch_ndcg = 20.77 
2025-04-09 10:03:48.225869: Steps 38/90: batch_recall = 32.18, batch_ndcg = 21.12 
2025-04-09 10:03:48.932833: Steps 39/90: batch_recall = 36.03, batch_ndcg = 21.35 
2025-04-09 10:03:49.640829: Steps 40/90: batch_recall = 32.67, batch_ndcg = 21.80 
2025-04-09 10:03:50.356211: Steps 41/90: batch_recall = 32.05, batch_ndcg = 21.93 
2025-04-09 10:03:51.074656: Steps 42/90: batch_recall = 39.56, batch_ndcg = 27.31 
2025-04-09 10:03:51.800199: Steps 43/90: batch_recall = 33.14, batch_ndcg = 21.72 
2025-04-09 10:03:52.521206: Steps 44/90: batch_recall = 33.79, batch_ndcg = 21.40 
2025-04-09 10:03:53.221794: Steps 45/90: batch_recall = 33.02, batch_ndcg = 20.39 
2025-04-09 10:03:53.936328: Steps 46/90: batch_recall = 31.18, batch_ndcg = 19.56 
2025-04-09 10:03:54.640885: Steps 47/90: batch_recall = 29.51, batch_ndcg = 18.73 
2025-04-09 10:03:55.350294: Steps 48/90: batch_recall = 35.17, batch_ndcg = 21.70 
2025-04-09 10:03:56.063926: Steps 49/90: batch_recall = 27.59, batch_ndcg = 18.41 
2025-04-09 10:03:56.774963: Steps 50/90: batch_recall = 35.53, batch_ndcg = 22.89 
2025-04-09 10:03:57.514906: Steps 51/90: batch_recall = 34.32, batch_ndcg = 23.56 
2025-04-09 10:03:58.235099: Steps 52/90: batch_recall = 33.09, batch_ndcg = 21.41 
2025-04-09 10:03:58.950693: Steps 53/90: batch_recall = 39.39, batch_ndcg = 24.08 
2025-04-09 10:03:59.653596: Steps 54/90: batch_recall = 28.18, batch_ndcg = 18.08 
2025-04-09 10:04:00.358469: Steps 55/90: batch_recall = 37.55, batch_ndcg = 23.04 
2025-04-09 10:04:01.072584: Steps 56/90: batch_recall = 31.46, batch_ndcg = 18.84 
2025-04-09 10:04:01.801823: Steps 57/90: batch_recall = 37.40, batch_ndcg = 23.29 
2025-04-09 10:04:02.508726: Steps 58/90: batch_recall = 37.61, batch_ndcg = 24.33 
2025-04-09 10:04:03.208228: Steps 59/90: batch_recall = 29.76, batch_ndcg = 18.95 
2025-04-09 10:04:03.915678: Steps 60/90: batch_recall = 29.69, batch_ndcg = 18.77 
2025-04-09 10:04:04.612061: Steps 61/90: batch_recall = 40.83, batch_ndcg = 27.29 
2025-04-09 10:04:05.310468: Steps 62/90: batch_recall = 36.32, batch_ndcg = 24.50 
2025-04-09 10:04:06.019387: Steps 63/90: batch_recall = 37.51, batch_ndcg = 22.64 
2025-04-09 10:04:06.737283: Steps 64/90: batch_recall = 34.79, batch_ndcg = 21.57 
2025-04-09 10:04:07.440643: Steps 65/90: batch_recall = 29.64, batch_ndcg = 19.34 
2025-04-09 10:04:08.141975: Steps 66/90: batch_recall = 34.12, batch_ndcg = 21.17 
2025-04-09 10:04:08.838817: Steps 67/90: batch_recall = 32.09, batch_ndcg = 19.56 
2025-04-09 10:04:09.535244: Steps 68/90: batch_recall = 32.99, batch_ndcg = 20.24 
2025-04-09 10:04:10.235733: Steps 69/90: batch_recall = 39.03, batch_ndcg = 23.12 
2025-04-09 10:04:10.947195: Steps 70/90: batch_recall = 35.97, batch_ndcg = 22.88 
2025-04-09 10:04:11.671250: Steps 71/90: batch_recall = 33.63, batch_ndcg = 21.25 
2025-04-09 10:04:12.369570: Steps 72/90: batch_recall = 34.20, batch_ndcg = 21.64 
2025-04-09 10:04:13.075064: Steps 73/90: batch_recall = 33.12, batch_ndcg = 20.87 
2025-04-09 10:04:13.784867: Steps 74/90: batch_recall = 37.89, batch_ndcg = 23.74 
2025-04-09 10:04:14.488243: Steps 75/90: batch_recall = 37.22, batch_ndcg = 23.05 
2025-04-09 10:04:15.187319: Steps 76/90: batch_recall = 39.26, batch_ndcg = 23.83 
2025-04-09 10:04:15.885786: Steps 77/90: batch_recall = 42.85, batch_ndcg = 25.28 
2025-04-09 10:04:16.597792: Steps 78/90: batch_recall = 30.81, batch_ndcg = 19.24 
2025-04-09 10:04:17.297312: Steps 79/90: batch_recall = 38.09, batch_ndcg = 26.28 
2025-04-09 10:04:18.009644: Steps 80/90: batch_recall = 43.12, batch_ndcg = 26.43 
2025-04-09 10:04:18.724288: Steps 81/90: batch_recall = 42.93, batch_ndcg = 26.03 
2025-04-09 10:04:19.448003: Steps 82/90: batch_recall = 38.05, batch_ndcg = 23.88 
2025-04-09 10:04:20.147408: Steps 83/90: batch_recall = 36.94, batch_ndcg = 23.16 
2025-04-09 10:04:20.845153: Steps 84/90: batch_recall = 33.91, batch_ndcg = 20.16 
2025-04-09 10:04:21.542179: Steps 85/90: batch_recall = 37.27, batch_ndcg = 20.69 
2025-04-09 10:04:22.241574: Steps 86/90: batch_recall = 41.89, batch_ndcg = 25.75 
2025-04-09 10:04:22.949793: Steps 87/90: batch_recall = 40.16, batch_ndcg = 26.76 
2025-04-09 10:04:23.657990: Steps 88/90: batch_recall = 46.73, batch_ndcg = 26.68 
2025-04-09 10:04:24.221575: Steps 89/90: batch_recall = 37.63, batch_ndcg = 23.49 
2025-04-09 10:04:24.222396: Epoch 19/1000, Test: Recall = 0.0742, NDCG = 0.0478  

2025-04-09 10:04:25.228165: Training Step 0/115: batchLoss = 3.6809, diffLoss = 6.0342, kgLoss = 0.1510
2025-04-09 10:04:25.955188: Training Step 1/115: batchLoss = 4.1350, diffLoss = 6.7752, kgLoss = 0.1747
2025-04-09 10:04:26.689925: Training Step 2/115: batchLoss = 5.1635, diffLoss = 8.4745, kgLoss = 0.1970
2025-04-09 10:04:27.431711: Training Step 3/115: batchLoss = 4.9136, diffLoss = 8.0607, kgLoss = 0.1929
2025-04-09 10:04:28.176631: Training Step 4/115: batchLoss = 4.6610, diffLoss = 7.6383, kgLoss = 0.1950
2025-04-09 10:04:28.911532: Training Step 5/115: batchLoss = 4.5198, diffLoss = 7.4143, kgLoss = 0.1780
2025-04-09 10:04:29.654059: Training Step 6/115: batchLoss = 5.1011, diffLoss = 8.3680, kgLoss = 0.2007
2025-04-09 10:04:30.400680: Training Step 7/115: batchLoss = 4.5461, diffLoss = 7.4498, kgLoss = 0.1905
2025-04-09 10:04:31.137579: Training Step 8/115: batchLoss = 3.8899, diffLoss = 6.3726, kgLoss = 0.1660
2025-04-09 10:04:31.869327: Training Step 9/115: batchLoss = 4.3695, diffLoss = 7.1576, kgLoss = 0.1874
2025-04-09 10:04:32.601682: Training Step 10/115: batchLoss = 3.9175, diffLoss = 6.4198, kgLoss = 0.1640
2025-04-09 10:04:33.355506: Training Step 11/115: batchLoss = 4.0540, diffLoss = 6.6498, kgLoss = 0.1603
2025-04-09 10:04:34.094681: Training Step 12/115: batchLoss = 3.6480, diffLoss = 5.9749, kgLoss = 0.1577
2025-04-09 10:04:34.829131: Training Step 13/115: batchLoss = 4.8589, diffLoss = 7.9742, kgLoss = 0.1859
2025-04-09 10:04:35.557390: Training Step 14/115: batchLoss = 4.6454, diffLoss = 7.6204, kgLoss = 0.1830
2025-04-09 10:04:36.288736: Training Step 15/115: batchLoss = 3.8616, diffLoss = 6.3251, kgLoss = 0.1662
2025-04-09 10:04:37.017062: Training Step 16/115: batchLoss = 4.3480, diffLoss = 7.1300, kgLoss = 0.1748
2025-04-09 10:04:37.747364: Training Step 17/115: batchLoss = 4.2686, diffLoss = 6.9974, kgLoss = 0.1753
2025-04-09 10:04:38.469323: Training Step 18/115: batchLoss = 4.2447, diffLoss = 6.9608, kgLoss = 0.1706
2025-04-09 10:04:39.209281: Training Step 19/115: batchLoss = 4.2228, diffLoss = 6.9227, kgLoss = 0.1730
2025-04-09 10:04:39.942138: Training Step 20/115: batchLoss = 4.1256, diffLoss = 6.7592, kgLoss = 0.1751
2025-04-09 10:04:40.678926: Training Step 21/115: batchLoss = 4.9002, diffLoss = 8.0353, kgLoss = 0.1974
2025-04-09 10:04:41.414957: Training Step 22/115: batchLoss = 4.1482, diffLoss = 6.8007, kgLoss = 0.1695
2025-04-09 10:04:42.150664: Training Step 23/115: batchLoss = 4.8967, diffLoss = 8.0252, kgLoss = 0.2039
2025-04-09 10:04:42.890923: Training Step 24/115: batchLoss = 4.2131, diffLoss = 6.9090, kgLoss = 0.1693
2025-04-09 10:04:43.646694: Training Step 25/115: batchLoss = 4.5816, diffLoss = 7.5210, kgLoss = 0.1725
2025-04-09 10:04:44.397045: Training Step 26/115: batchLoss = 4.0400, diffLoss = 6.6259, kgLoss = 0.1612
2025-04-09 10:04:45.130227: Training Step 27/115: batchLoss = 4.8556, diffLoss = 7.9616, kgLoss = 0.1967
2025-04-09 10:04:45.874034: Training Step 28/115: batchLoss = 4.1198, diffLoss = 6.7521, kgLoss = 0.1714
2025-04-09 10:04:46.613993: Training Step 29/115: batchLoss = 4.9088, diffLoss = 8.0477, kgLoss = 0.2005
2025-04-09 10:04:47.339708: Training Step 30/115: batchLoss = 4.6191, diffLoss = 7.5761, kgLoss = 0.1836
2025-04-09 10:04:48.096646: Training Step 31/115: batchLoss = 4.0666, diffLoss = 6.6633, kgLoss = 0.1716
2025-04-09 10:04:48.835212: Training Step 32/115: batchLoss = 4.7484, diffLoss = 7.7852, kgLoss = 0.1931
2025-04-09 10:04:49.573881: Training Step 33/115: batchLoss = 4.1957, diffLoss = 6.8744, kgLoss = 0.1777
2025-04-09 10:04:50.310908: Training Step 34/115: batchLoss = 4.5871, diffLoss = 7.5272, kgLoss = 0.1768
2025-04-09 10:04:51.044790: Training Step 35/115: batchLoss = 4.1629, diffLoss = 6.8229, kgLoss = 0.1729
2025-04-09 10:04:51.793252: Training Step 36/115: batchLoss = 4.1617, diffLoss = 6.8306, kgLoss = 0.1583
2025-04-09 10:04:52.547592: Training Step 37/115: batchLoss = 4.2279, diffLoss = 6.9300, kgLoss = 0.1746
2025-04-09 10:04:53.301038: Training Step 38/115: batchLoss = 4.4209, diffLoss = 7.2440, kgLoss = 0.1864
2025-04-09 10:04:54.031837: Training Step 39/115: batchLoss = 4.5106, diffLoss = 7.3930, kgLoss = 0.1870
2025-04-09 10:04:54.760613: Training Step 40/115: batchLoss = 3.6105, diffLoss = 5.9205, kgLoss = 0.1455
2025-04-09 10:04:55.498854: Training Step 41/115: batchLoss = 4.6485, diffLoss = 7.6274, kgLoss = 0.1803
2025-04-09 10:04:56.249039: Training Step 42/115: batchLoss = 3.9514, diffLoss = 6.4794, kgLoss = 0.1593
2025-04-09 10:04:56.980239: Training Step 43/115: batchLoss = 4.5191, diffLoss = 7.4030, kgLoss = 0.1933
2025-04-09 10:04:57.712434: Training Step 44/115: batchLoss = 4.3179, diffLoss = 7.0853, kgLoss = 0.1668
2025-04-09 10:04:58.448958: Training Step 45/115: batchLoss = 4.3036, diffLoss = 7.0604, kgLoss = 0.1685
2025-04-09 10:04:59.186478: Training Step 46/115: batchLoss = 3.7425, diffLoss = 6.1258, kgLoss = 0.1676
2025-04-09 10:04:59.921256: Training Step 47/115: batchLoss = 4.0335, diffLoss = 6.6157, kgLoss = 0.1603
2025-04-09 10:05:00.647373: Training Step 48/115: batchLoss = 4.7537, diffLoss = 7.8013, kgLoss = 0.1823
2025-04-09 10:05:01.375841: Training Step 49/115: batchLoss = 4.0409, diffLoss = 6.6275, kgLoss = 0.1609
2025-04-09 10:05:02.108770: Training Step 50/115: batchLoss = 4.1416, diffLoss = 6.7907, kgLoss = 0.1680
2025-04-09 10:05:02.833416: Training Step 51/115: batchLoss = 4.5958, diffLoss = 7.5396, kgLoss = 0.1802
2025-04-09 10:05:03.576118: Training Step 52/115: batchLoss = 4.5693, diffLoss = 7.4917, kgLoss = 0.1856
2025-04-09 10:05:04.314224: Training Step 53/115: batchLoss = 4.2082, diffLoss = 6.9017, kgLoss = 0.1680
2025-04-09 10:05:05.044590: Training Step 54/115: batchLoss = 4.2475, diffLoss = 6.9657, kgLoss = 0.1701
2025-04-09 10:05:05.787948: Training Step 55/115: batchLoss = 4.9875, diffLoss = 8.1812, kgLoss = 0.1969
2025-04-09 10:05:06.528036: Training Step 56/115: batchLoss = 4.0560, diffLoss = 6.6515, kgLoss = 0.1627
2025-04-09 10:05:07.278964: Training Step 57/115: batchLoss = 5.0543, diffLoss = 8.2966, kgLoss = 0.1909
2025-04-09 10:05:08.021461: Training Step 58/115: batchLoss = 4.2398, diffLoss = 6.9594, kgLoss = 0.1604
2025-04-09 10:05:08.754916: Training Step 59/115: batchLoss = 4.7754, diffLoss = 7.8370, kgLoss = 0.1830
2025-04-09 10:05:09.483949: Training Step 60/115: batchLoss = 4.3095, diffLoss = 7.0685, kgLoss = 0.1710
2025-04-09 10:05:10.220882: Training Step 61/115: batchLoss = 4.0786, diffLoss = 6.6885, kgLoss = 0.1637
2025-04-09 10:05:10.957518: Training Step 62/115: batchLoss = 4.6560, diffLoss = 7.6294, kgLoss = 0.1958
2025-04-09 10:05:11.689846: Training Step 63/115: batchLoss = 4.0777, diffLoss = 6.6810, kgLoss = 0.1728
2025-04-09 10:05:12.426534: Training Step 64/115: batchLoss = 4.6185, diffLoss = 7.5735, kgLoss = 0.1861
2025-04-09 10:05:13.169495: Training Step 65/115: batchLoss = 4.3913, diffLoss = 7.2052, kgLoss = 0.1706
2025-04-09 10:05:13.911293: Training Step 66/115: batchLoss = 4.2621, diffLoss = 6.9934, kgLoss = 0.1652
2025-04-09 10:05:14.655014: Training Step 67/115: batchLoss = 4.4127, diffLoss = 7.2401, kgLoss = 0.1717
2025-04-09 10:05:15.416390: Training Step 68/115: batchLoss = 4.0065, diffLoss = 6.5663, kgLoss = 0.1667
2025-04-09 10:05:16.156662: Training Step 69/115: batchLoss = 5.0307, diffLoss = 8.2536, kgLoss = 0.1962
2025-04-09 10:05:16.892225: Training Step 70/115: batchLoss = 3.6508, diffLoss = 5.9886, kgLoss = 0.1442
2025-04-09 10:05:17.626019: Training Step 71/115: batchLoss = 4.0683, diffLoss = 6.6734, kgLoss = 0.1605
2025-04-09 10:05:18.354281: Training Step 72/115: batchLoss = 4.2249, diffLoss = 6.9173, kgLoss = 0.1863
2025-04-09 10:05:19.099452: Training Step 73/115: batchLoss = 4.9530, diffLoss = 8.1161, kgLoss = 0.2084
2025-04-09 10:05:19.825826: Training Step 74/115: batchLoss = 4.5306, diffLoss = 7.4339, kgLoss = 0.1756
2025-04-09 10:05:20.547868: Training Step 75/115: batchLoss = 4.2823, diffLoss = 7.0193, kgLoss = 0.1769
2025-04-09 10:05:21.286168: Training Step 76/115: batchLoss = 4.9530, diffLoss = 8.1225, kgLoss = 0.1988
2025-04-09 10:05:22.010754: Training Step 77/115: batchLoss = 4.3014, diffLoss = 7.0589, kgLoss = 0.1652
2025-04-09 10:05:22.740515: Training Step 78/115: batchLoss = 4.5318, diffLoss = 7.4315, kgLoss = 0.1822
2025-04-09 10:05:23.468674: Training Step 79/115: batchLoss = 4.4345, diffLoss = 7.2737, kgLoss = 0.1758
2025-04-09 10:05:24.203142: Training Step 80/115: batchLoss = 4.5507, diffLoss = 7.4681, kgLoss = 0.1745
2025-04-09 10:05:24.928154: Training Step 81/115: batchLoss = 4.3015, diffLoss = 7.0550, kgLoss = 0.1711
2025-04-09 10:05:25.665531: Training Step 82/115: batchLoss = 4.5933, diffLoss = 7.5280, kgLoss = 0.1914
2025-04-09 10:05:26.407239: Training Step 83/115: batchLoss = 4.4995, diffLoss = 7.3792, kgLoss = 0.1799
2025-04-09 10:05:27.153918: Training Step 84/115: batchLoss = 5.1240, diffLoss = 8.4028, kgLoss = 0.2058
2025-04-09 10:05:27.889573: Training Step 85/115: batchLoss = 4.3648, diffLoss = 7.1575, kgLoss = 0.1757
2025-04-09 10:05:28.631527: Training Step 86/115: batchLoss = 4.2558, diffLoss = 6.9832, kgLoss = 0.1647
2025-04-09 10:05:29.368574: Training Step 87/115: batchLoss = 4.6910, diffLoss = 7.6987, kgLoss = 0.1795
2025-04-09 10:05:30.094027: Training Step 88/115: batchLoss = 4.4358, diffLoss = 7.2772, kgLoss = 0.1738
2025-04-09 10:05:30.825317: Training Step 89/115: batchLoss = 4.1376, diffLoss = 6.7887, kgLoss = 0.1611
2025-04-09 10:05:31.573101: Training Step 90/115: batchLoss = 4.3608, diffLoss = 7.1564, kgLoss = 0.1674
2025-04-09 10:05:32.433958: Training Step 91/115: batchLoss = 4.0884, diffLoss = 6.6935, kgLoss = 0.1807
2025-04-09 10:05:33.160573: Training Step 92/115: batchLoss = 4.0758, diffLoss = 6.6829, kgLoss = 0.1651
2025-04-09 10:05:33.891884: Training Step 93/115: batchLoss = 4.6825, diffLoss = 7.6804, kgLoss = 0.1857
2025-04-09 10:05:34.621725: Training Step 94/115: batchLoss = 4.5997, diffLoss = 7.5441, kgLoss = 0.1831
2025-04-09 10:05:35.348761: Training Step 95/115: batchLoss = 3.8290, diffLoss = 6.2729, kgLoss = 0.1631
2025-04-09 10:05:36.074334: Training Step 96/115: batchLoss = 4.2084, diffLoss = 6.9011, kgLoss = 0.1693
2025-04-09 10:05:36.808163: Training Step 97/115: batchLoss = 4.3303, diffLoss = 7.1021, kgLoss = 0.1727
2025-04-09 10:05:37.532323: Training Step 98/115: batchLoss = 4.5101, diffLoss = 7.3974, kgLoss = 0.1792
2025-04-09 10:05:38.266225: Training Step 99/115: batchLoss = 4.3684, diffLoss = 7.1602, kgLoss = 0.1807
2025-04-09 10:05:38.989876: Training Step 100/115: batchLoss = 4.1798, diffLoss = 6.8552, kgLoss = 0.1667
2025-04-09 10:05:39.715071: Training Step 101/115: batchLoss = 5.0854, diffLoss = 8.3420, kgLoss = 0.2004
2025-04-09 10:05:40.462522: Training Step 102/115: batchLoss = 3.8728, diffLoss = 6.3491, kgLoss = 0.1583
2025-04-09 10:05:41.192452: Training Step 103/115: batchLoss = 4.4772, diffLoss = 7.3437, kgLoss = 0.1775
2025-04-09 10:05:41.933462: Training Step 104/115: batchLoss = 4.3614, diffLoss = 7.1485, kgLoss = 0.1807
2025-04-09 10:05:42.664741: Training Step 105/115: batchLoss = 4.9500, diffLoss = 8.1230, kgLoss = 0.1905
2025-04-09 10:05:43.397160: Training Step 106/115: batchLoss = 4.1489, diffLoss = 6.8039, kgLoss = 0.1665
2025-04-09 10:05:44.137365: Training Step 107/115: batchLoss = 4.2091, diffLoss = 6.9087, kgLoss = 0.1596
2025-04-09 10:05:44.865520: Training Step 108/115: batchLoss = 3.8700, diffLoss = 6.3500, kgLoss = 0.1500
2025-04-09 10:05:45.589519: Training Step 109/115: batchLoss = 4.8867, diffLoss = 8.0162, kgLoss = 0.1924
2025-04-09 10:05:46.319209: Training Step 110/115: batchLoss = 4.6532, diffLoss = 7.6394, kgLoss = 0.1739
2025-04-09 10:05:47.059663: Training Step 111/115: batchLoss = 4.3397, diffLoss = 7.1233, kgLoss = 0.1642
2025-04-09 10:05:47.782454: Training Step 112/115: batchLoss = 3.8587, diffLoss = 6.3263, kgLoss = 0.1573
2025-04-09 10:05:48.430372: Training Step 113/115: batchLoss = 5.0248, diffLoss = 8.2461, kgLoss = 0.1928
2025-04-09 10:05:49.070367: Training Step 114/115: batchLoss = 3.9212, diffLoss = 6.4336, kgLoss = 0.1524
2025-04-09 10:05:49.186005: 
2025-04-09 10:05:49.186720: Epoch 20/1000, Train: epLoss = 1.2599, epDfLoss = 2.0661, epfTransLoss = 0.0000, epKgLoss = 0.0506  
2025-04-09 10:05:49.940650: Steps 0/90: batch_recall = 55.68, batch_ndcg = 41.87 
2025-04-09 10:05:50.673841: Steps 1/90: batch_recall = 59.07, batch_ndcg = 40.92 
2025-04-09 10:05:51.412400: Steps 2/90: batch_recall = 51.35, batch_ndcg = 36.79 
2025-04-09 10:05:52.141924: Steps 3/90: batch_recall = 46.18, batch_ndcg = 32.82 
2025-04-09 10:05:52.866780: Steps 4/90: batch_recall = 48.51, batch_ndcg = 35.87 
2025-04-09 10:05:53.594889: Steps 5/90: batch_recall = 35.67, batch_ndcg = 26.09 
2025-04-09 10:05:54.321186: Steps 6/90: batch_recall = 47.86, batch_ndcg = 33.98 
2025-04-09 10:05:55.072099: Steps 7/90: batch_recall = 42.55, batch_ndcg = 31.19 
2025-04-09 10:05:55.799528: Steps 8/90: batch_recall = 47.79, batch_ndcg = 33.86 
2025-04-09 10:05:56.544428: Steps 9/90: batch_recall = 42.73, batch_ndcg = 33.49 
2025-04-09 10:05:57.287075: Steps 10/90: batch_recall = 38.19, batch_ndcg = 28.20 
2025-04-09 10:05:58.003563: Steps 11/90: batch_recall = 43.29, batch_ndcg = 27.96 
2025-04-09 10:05:58.725505: Steps 12/90: batch_recall = 38.02, batch_ndcg = 27.59 
2025-04-09 10:05:59.458777: Steps 13/90: batch_recall = 37.83, batch_ndcg = 26.02 
2025-04-09 10:06:00.213837: Steps 14/90: batch_recall = 34.15, batch_ndcg = 25.93 
2025-04-09 10:06:00.961621: Steps 15/90: batch_recall = 43.09, batch_ndcg = 28.19 
2025-04-09 10:06:01.705927: Steps 16/90: batch_recall = 37.42, batch_ndcg = 26.29 
2025-04-09 10:06:02.446551: Steps 17/90: batch_recall = 35.11, batch_ndcg = 23.01 
2025-04-09 10:06:03.158176: Steps 18/90: batch_recall = 38.16, batch_ndcg = 25.20 
2025-04-09 10:06:03.890405: Steps 19/90: batch_recall = 33.29, batch_ndcg = 23.57 
2025-04-09 10:06:04.615520: Steps 20/90: batch_recall = 35.22, batch_ndcg = 23.36 
2025-04-09 10:06:05.342215: Steps 21/90: batch_recall = 39.30, batch_ndcg = 27.63 
2025-04-09 10:06:06.063446: Steps 22/90: batch_recall = 39.54, batch_ndcg = 28.00 
2025-04-09 10:06:06.797341: Steps 23/90: batch_recall = 36.31, batch_ndcg = 24.65 
2025-04-09 10:06:07.505111: Steps 24/90: batch_recall = 38.87, batch_ndcg = 26.12 
2025-04-09 10:06:08.224663: Steps 25/90: batch_recall = 37.99, batch_ndcg = 24.24 
2025-04-09 10:06:08.950271: Steps 26/90: batch_recall = 39.14, batch_ndcg = 27.22 
2025-04-09 10:06:09.656615: Steps 27/90: batch_recall = 33.98, batch_ndcg = 24.07 
2025-04-09 10:06:10.368602: Steps 28/90: batch_recall = 36.11, batch_ndcg = 24.29 
2025-04-09 10:06:11.096360: Steps 29/90: batch_recall = 31.95, batch_ndcg = 21.50 
2025-04-09 10:06:11.828002: Steps 30/90: batch_recall = 32.98, batch_ndcg = 20.62 
2025-04-09 10:06:12.566700: Steps 31/90: batch_recall = 31.44, batch_ndcg = 22.12 
2025-04-09 10:06:13.291616: Steps 32/90: batch_recall = 34.08, batch_ndcg = 22.97 
2025-04-09 10:06:14.021213: Steps 33/90: batch_recall = 35.47, batch_ndcg = 21.88 
2025-04-09 10:06:14.750600: Steps 34/90: batch_recall = 34.66, batch_ndcg = 23.32 
2025-04-09 10:06:15.467536: Steps 35/90: batch_recall = 37.61, batch_ndcg = 25.88 
2025-04-09 10:06:16.187021: Steps 36/90: batch_recall = 37.31, batch_ndcg = 24.42 
2025-04-09 10:06:16.900354: Steps 37/90: batch_recall = 33.00, batch_ndcg = 21.21 
2025-04-09 10:06:17.609904: Steps 38/90: batch_recall = 32.67, batch_ndcg = 21.07 
2025-04-09 10:06:18.326301: Steps 39/90: batch_recall = 37.07, batch_ndcg = 21.90 
2025-04-09 10:06:19.037912: Steps 40/90: batch_recall = 32.52, batch_ndcg = 22.38 
2025-04-09 10:06:19.754849: Steps 41/90: batch_recall = 33.38, batch_ndcg = 22.47 
2025-04-09 10:06:20.472489: Steps 42/90: batch_recall = 40.67, batch_ndcg = 27.67 
2025-04-09 10:06:21.197383: Steps 43/90: batch_recall = 33.77, batch_ndcg = 21.98 
2025-04-09 10:06:21.908288: Steps 44/90: batch_recall = 34.96, batch_ndcg = 21.99 
2025-04-09 10:06:22.620490: Steps 45/90: batch_recall = 32.47, batch_ndcg = 20.57 
2025-04-09 10:06:23.318842: Steps 46/90: batch_recall = 32.26, batch_ndcg = 20.22 
2025-04-09 10:06:24.022719: Steps 47/90: batch_recall = 30.30, batch_ndcg = 19.07 
2025-04-09 10:06:24.720110: Steps 48/90: batch_recall = 36.35, batch_ndcg = 22.18 
2025-04-09 10:06:25.431344: Steps 49/90: batch_recall = 28.55, batch_ndcg = 18.59 
2025-04-09 10:06:26.144632: Steps 50/90: batch_recall = 35.62, batch_ndcg = 23.38 
2025-04-09 10:06:26.852901: Steps 51/90: batch_recall = 33.78, batch_ndcg = 23.29 
2025-04-09 10:06:27.574015: Steps 52/90: batch_recall = 33.08, batch_ndcg = 21.31 
2025-04-09 10:06:28.297145: Steps 53/90: batch_recall = 40.37, batch_ndcg = 24.61 
2025-04-09 10:06:29.024363: Steps 54/90: batch_recall = 30.98, batch_ndcg = 19.21 
2025-04-09 10:06:29.731974: Steps 55/90: batch_recall = 38.09, batch_ndcg = 23.76 
2025-04-09 10:06:30.444755: Steps 56/90: batch_recall = 31.24, batch_ndcg = 18.72 
2025-04-09 10:06:31.168939: Steps 57/90: batch_recall = 35.92, batch_ndcg = 22.95 
2025-04-09 10:06:31.873040: Steps 58/90: batch_recall = 38.43, batch_ndcg = 25.06 
2025-04-09 10:06:32.589840: Steps 59/90: batch_recall = 29.65, batch_ndcg = 19.12 
2025-04-09 10:06:33.299673: Steps 60/90: batch_recall = 29.57, batch_ndcg = 18.92 
2025-04-09 10:06:34.013654: Steps 61/90: batch_recall = 40.81, batch_ndcg = 27.43 
2025-04-09 10:06:34.732614: Steps 62/90: batch_recall = 37.43, batch_ndcg = 24.62 
2025-04-09 10:06:35.432698: Steps 63/90: batch_recall = 36.75, batch_ndcg = 22.93 
2025-04-09 10:06:36.122970: Steps 64/90: batch_recall = 35.83, batch_ndcg = 21.89 
2025-04-09 10:06:36.826011: Steps 65/90: batch_recall = 31.04, batch_ndcg = 20.03 
2025-04-09 10:06:37.521971: Steps 66/90: batch_recall = 35.55, batch_ndcg = 21.51 
2025-04-09 10:06:38.220720: Steps 67/90: batch_recall = 32.64, batch_ndcg = 19.86 
2025-04-09 10:06:38.920351: Steps 68/90: batch_recall = 33.41, batch_ndcg = 20.50 
2025-04-09 10:06:39.623466: Steps 69/90: batch_recall = 40.37, batch_ndcg = 23.28 
2025-04-09 10:06:40.329034: Steps 70/90: batch_recall = 35.67, batch_ndcg = 22.78 
2025-04-09 10:06:41.029662: Steps 71/90: batch_recall = 33.46, batch_ndcg = 21.05 
2025-04-09 10:06:41.738628: Steps 72/90: batch_recall = 35.04, batch_ndcg = 22.11 
2025-04-09 10:06:42.455663: Steps 73/90: batch_recall = 34.86, batch_ndcg = 21.61 
2025-04-09 10:06:43.166595: Steps 74/90: batch_recall = 38.21, batch_ndcg = 23.84 
2025-04-09 10:06:43.887129: Steps 75/90: batch_recall = 36.59, batch_ndcg = 22.77 
2025-04-09 10:06:44.588923: Steps 76/90: batch_recall = 40.00, batch_ndcg = 24.38 
2025-04-09 10:06:45.288079: Steps 77/90: batch_recall = 42.92, batch_ndcg = 25.55 
2025-04-09 10:06:45.995679: Steps 78/90: batch_recall = 31.89, batch_ndcg = 19.66 
2025-04-09 10:06:46.701458: Steps 79/90: batch_recall = 39.44, batch_ndcg = 26.60 
2025-04-09 10:06:47.412516: Steps 80/90: batch_recall = 43.17, batch_ndcg = 26.69 
2025-04-09 10:06:48.111689: Steps 81/90: batch_recall = 42.02, batch_ndcg = 25.53 
2025-04-09 10:06:48.810569: Steps 82/90: batch_recall = 38.24, batch_ndcg = 24.22 
2025-04-09 10:06:49.498502: Steps 83/90: batch_recall = 37.41, batch_ndcg = 23.46 
2025-04-09 10:06:50.211290: Steps 84/90: batch_recall = 34.18, batch_ndcg = 20.50 
2025-04-09 10:06:50.900120: Steps 85/90: batch_recall = 37.47, batch_ndcg = 21.24 
2025-04-09 10:06:51.600765: Steps 86/90: batch_recall = 42.06, batch_ndcg = 26.01 
2025-04-09 10:06:52.297694: Steps 87/90: batch_recall = 40.56, batch_ndcg = 26.98 
2025-04-09 10:06:53.012219: Steps 88/90: batch_recall = 47.59, batch_ndcg = 26.82 
2025-04-09 10:06:53.558969: Steps 89/90: batch_recall = 37.17, batch_ndcg = 23.50 
2025-04-09 10:06:53.560342: Epoch 20/1000, Test: Recall = 0.0748, NDCG = 0.0483  

2025-04-09 10:06:54.589446: Training Step 0/115: batchLoss = 4.5752, diffLoss = 7.5047, kgLoss = 0.1811
2025-04-09 10:06:55.310527: Training Step 1/115: batchLoss = 4.4475, diffLoss = 7.3000, kgLoss = 0.1687
2025-04-09 10:06:56.030952: Training Step 2/115: batchLoss = 4.4585, diffLoss = 7.3120, kgLoss = 0.1783
2025-04-09 10:06:56.751560: Training Step 3/115: batchLoss = 4.1204, diffLoss = 6.7543, kgLoss = 0.1695
2025-04-09 10:06:57.472261: Training Step 4/115: batchLoss = 4.9979, diffLoss = 8.2029, kgLoss = 0.1903
2025-04-09 10:06:58.195908: Training Step 5/115: batchLoss = 3.8755, diffLoss = 6.3522, kgLoss = 0.1604
2025-04-09 10:06:58.921712: Training Step 6/115: batchLoss = 4.2969, diffLoss = 7.0443, kgLoss = 0.1757
2025-04-09 10:06:59.655555: Training Step 7/115: batchLoss = 5.5784, diffLoss = 9.1485, kgLoss = 0.2232
2025-04-09 10:07:00.391891: Training Step 8/115: batchLoss = 4.8001, diffLoss = 7.8774, kgLoss = 0.1841
2025-04-09 10:07:01.130440: Training Step 9/115: batchLoss = 3.7834, diffLoss = 6.1969, kgLoss = 0.1630
2025-04-09 10:07:01.865341: Training Step 10/115: batchLoss = 3.5014, diffLoss = 5.7300, kgLoss = 0.1584
2025-04-09 10:07:02.600205: Training Step 11/115: batchLoss = 4.4814, diffLoss = 7.3465, kgLoss = 0.1837
2025-04-09 10:07:03.335139: Training Step 12/115: batchLoss = 4.3026, diffLoss = 7.0623, kgLoss = 0.1630
2025-04-09 10:07:04.064664: Training Step 13/115: batchLoss = 4.1982, diffLoss = 6.8848, kgLoss = 0.1684
2025-04-09 10:07:04.895026: Training Step 14/115: batchLoss = 4.6698, diffLoss = 7.6610, kgLoss = 0.1831
2025-04-09 10:07:05.624551: Training Step 15/115: batchLoss = 4.3576, diffLoss = 7.1422, kgLoss = 0.1806
2025-04-09 10:07:06.349376: Training Step 16/115: batchLoss = 4.1781, diffLoss = 6.8501, kgLoss = 0.1701
2025-04-09 10:07:07.080477: Training Step 17/115: batchLoss = 4.5517, diffLoss = 7.4712, kgLoss = 0.1725
2025-04-09 10:07:07.806935: Training Step 18/115: batchLoss = 4.9731, diffLoss = 8.1605, kgLoss = 0.1920
2025-04-09 10:07:08.541478: Training Step 19/115: batchLoss = 4.3254, diffLoss = 7.0900, kgLoss = 0.1785
2025-04-09 10:07:09.280703: Training Step 20/115: batchLoss = 5.0072, diffLoss = 8.2188, kgLoss = 0.1897
2025-04-09 10:07:10.016782: Training Step 21/115: batchLoss = 4.5431, diffLoss = 7.4433, kgLoss = 0.1927
2025-04-09 10:07:10.752055: Training Step 22/115: batchLoss = 5.1596, diffLoss = 8.4667, kgLoss = 0.1990
2025-04-09 10:07:11.500896: Training Step 23/115: batchLoss = 4.1277, diffLoss = 6.7676, kgLoss = 0.1680
2025-04-09 10:07:12.235107: Training Step 24/115: batchLoss = 4.4798, diffLoss = 7.3494, kgLoss = 0.1753
2025-04-09 10:07:12.969131: Training Step 25/115: batchLoss = 4.2954, diffLoss = 7.0436, kgLoss = 0.1731
2025-04-09 10:07:13.701248: Training Step 26/115: batchLoss = 4.1187, diffLoss = 6.7502, kgLoss = 0.1714
2025-04-09 10:07:14.432306: Training Step 27/115: batchLoss = 4.2655, diffLoss = 6.9941, kgLoss = 0.1726
2025-04-09 10:07:15.162209: Training Step 28/115: batchLoss = 4.3929, diffLoss = 7.2091, kgLoss = 0.1686
2025-04-09 10:07:16.001604: Training Step 29/115: batchLoss = 4.3367, diffLoss = 7.1101, kgLoss = 0.1766
2025-04-09 10:07:16.744706: Training Step 30/115: batchLoss = 4.4313, diffLoss = 7.2694, kgLoss = 0.1741
2025-04-09 10:07:17.480233: Training Step 31/115: batchLoss = 4.1642, diffLoss = 6.8274, kgLoss = 0.1693
2025-04-09 10:07:18.219416: Training Step 32/115: batchLoss = 4.6732, diffLoss = 7.6685, kgLoss = 0.1803
2025-04-09 10:07:18.962178: Training Step 33/115: batchLoss = 4.2693, diffLoss = 7.0042, kgLoss = 0.1669
2025-04-09 10:07:19.706030: Training Step 34/115: batchLoss = 4.4587, diffLoss = 7.3112, kgLoss = 0.1798
2025-04-09 10:07:20.445655: Training Step 35/115: batchLoss = 4.3576, diffLoss = 7.1523, kgLoss = 0.1656
2025-04-09 10:07:21.182648: Training Step 36/115: batchLoss = 4.2001, diffLoss = 6.8811, kgLoss = 0.1787
2025-04-09 10:07:21.917812: Training Step 37/115: batchLoss = 4.4671, diffLoss = 7.3254, kgLoss = 0.1796
2025-04-09 10:07:22.668420: Training Step 38/115: batchLoss = 4.3637, diffLoss = 7.1584, kgLoss = 0.1716
2025-04-09 10:07:23.396905: Training Step 39/115: batchLoss = 4.6358, diffLoss = 7.6016, kgLoss = 0.1870
2025-04-09 10:07:24.133981: Training Step 40/115: batchLoss = 3.9026, diffLoss = 6.3970, kgLoss = 0.1610
2025-04-09 10:07:24.858684: Training Step 41/115: batchLoss = 4.0634, diffLoss = 6.6607, kgLoss = 0.1674
2025-04-09 10:07:25.585218: Training Step 42/115: batchLoss = 4.3640, diffLoss = 7.1526, kgLoss = 0.1810
2025-04-09 10:07:26.317347: Training Step 43/115: batchLoss = 4.3716, diffLoss = 7.1713, kgLoss = 0.1720
2025-04-09 10:07:27.059698: Training Step 44/115: batchLoss = 4.9693, diffLoss = 8.1576, kgLoss = 0.1870
2025-04-09 10:07:27.803833: Training Step 45/115: batchLoss = 4.3334, diffLoss = 7.1102, kgLoss = 0.1681
2025-04-09 10:07:28.527965: Training Step 46/115: batchLoss = 4.3117, diffLoss = 7.0749, kgLoss = 0.1669
2025-04-09 10:07:29.270676: Training Step 47/115: batchLoss = 4.5320, diffLoss = 7.4268, kgLoss = 0.1898
2025-04-09 10:07:30.033015: Training Step 48/115: batchLoss = 3.9400, diffLoss = 6.4604, kgLoss = 0.1594
2025-04-09 10:07:30.790589: Training Step 49/115: batchLoss = 4.6327, diffLoss = 7.5969, kgLoss = 0.1864
2025-04-09 10:07:31.525972: Training Step 50/115: batchLoss = 4.7237, diffLoss = 7.7473, kgLoss = 0.1882
2025-04-09 10:07:32.260251: Training Step 51/115: batchLoss = 4.4803, diffLoss = 7.3476, kgLoss = 0.1795
2025-04-09 10:07:32.996366: Training Step 52/115: batchLoss = 4.1314, diffLoss = 6.7662, kgLoss = 0.1793
2025-04-09 10:07:33.723046: Training Step 53/115: batchLoss = 4.2658, diffLoss = 6.9965, kgLoss = 0.1697
2025-04-09 10:07:34.455800: Training Step 54/115: batchLoss = 4.4862, diffLoss = 7.3584, kgLoss = 0.1780
2025-04-09 10:07:35.185261: Training Step 55/115: batchLoss = 4.1984, diffLoss = 6.8811, kgLoss = 0.1745
2025-04-09 10:07:35.905818: Training Step 56/115: batchLoss = 4.3526, diffLoss = 7.1435, kgLoss = 0.1663
2025-04-09 10:07:36.638692: Training Step 57/115: batchLoss = 4.6954, diffLoss = 7.7039, kgLoss = 0.1827
2025-04-09 10:07:37.366485: Training Step 58/115: batchLoss = 4.6539, diffLoss = 7.6396, kgLoss = 0.1755
2025-04-09 10:07:38.091449: Training Step 59/115: batchLoss = 4.4024, diffLoss = 7.2231, kgLoss = 0.1712
2025-04-09 10:07:38.828252: Training Step 60/115: batchLoss = 4.3819, diffLoss = 7.1874, kgLoss = 0.1735
2025-04-09 10:07:39.561143: Training Step 61/115: batchLoss = 4.2143, diffLoss = 6.9125, kgLoss = 0.1668
2025-04-09 10:07:40.290085: Training Step 62/115: batchLoss = 4.4077, diffLoss = 7.2265, kgLoss = 0.1795
2025-04-09 10:07:41.017212: Training Step 63/115: batchLoss = 4.5547, diffLoss = 7.4684, kgLoss = 0.1842
2025-04-09 10:07:41.747905: Training Step 64/115: batchLoss = 4.2111, diffLoss = 6.8932, kgLoss = 0.1879
2025-04-09 10:07:42.484052: Training Step 65/115: batchLoss = 4.4566, diffLoss = 7.3093, kgLoss = 0.1776
2025-04-09 10:07:43.221809: Training Step 66/115: batchLoss = 4.7878, diffLoss = 7.8642, kgLoss = 0.1733
2025-04-09 10:07:43.968314: Training Step 67/115: batchLoss = 3.9436, diffLoss = 6.4652, kgLoss = 0.1612
2025-04-09 10:07:44.708055: Training Step 68/115: batchLoss = 4.4882, diffLoss = 7.3663, kgLoss = 0.1711
2025-04-09 10:07:45.439427: Training Step 69/115: batchLoss = 3.6190, diffLoss = 5.9320, kgLoss = 0.1497
2025-04-09 10:07:46.165125: Training Step 70/115: batchLoss = 4.1301, diffLoss = 6.7742, kgLoss = 0.1640
2025-04-09 10:07:46.896747: Training Step 71/115: batchLoss = 4.4606, diffLoss = 7.3172, kgLoss = 0.1757
2025-04-09 10:07:47.631043: Training Step 72/115: batchLoss = 4.3387, diffLoss = 7.1095, kgLoss = 0.1824
2025-04-09 10:07:48.362562: Training Step 73/115: batchLoss = 4.3258, diffLoss = 7.0829, kgLoss = 0.1900
2025-04-09 10:07:49.092717: Training Step 74/115: batchLoss = 4.2229, diffLoss = 6.9207, kgLoss = 0.1760
2025-04-09 10:07:49.828340: Training Step 75/115: batchLoss = 3.8106, diffLoss = 6.2451, kgLoss = 0.1587
2025-04-09 10:07:50.551962: Training Step 76/115: batchLoss = 3.9663, diffLoss = 6.5022, kgLoss = 0.1625
2025-04-09 10:07:51.297719: Training Step 77/115: batchLoss = 4.2398, diffLoss = 6.9576, kgLoss = 0.1631
2025-04-09 10:07:52.053744: Training Step 78/115: batchLoss = 4.2057, diffLoss = 6.9024, kgLoss = 0.1605
2025-04-09 10:07:52.787231: Training Step 79/115: batchLoss = 4.0078, diffLoss = 6.5769, kgLoss = 0.1543
2025-04-09 10:07:53.517239: Training Step 80/115: batchLoss = 4.6357, diffLoss = 7.6074, kgLoss = 0.1782
2025-04-09 10:07:54.244507: Training Step 81/115: batchLoss = 4.4971, diffLoss = 7.3740, kgLoss = 0.1819
2025-04-09 10:07:54.974888: Training Step 82/115: batchLoss = 4.4329, diffLoss = 7.2682, kgLoss = 0.1799
2025-04-09 10:07:55.719652: Training Step 83/115: batchLoss = 4.3201, diffLoss = 7.0814, kgLoss = 0.1782
2025-04-09 10:07:56.458670: Training Step 84/115: batchLoss = 4.4197, diffLoss = 7.2525, kgLoss = 0.1706
2025-04-09 10:07:57.191178: Training Step 85/115: batchLoss = 4.7541, diffLoss = 7.8013, kgLoss = 0.1832
2025-04-09 10:07:57.919782: Training Step 86/115: batchLoss = 4.3753, diffLoss = 7.1748, kgLoss = 0.1761
2025-04-09 10:07:58.669295: Training Step 87/115: batchLoss = 4.5260, diffLoss = 7.4266, kgLoss = 0.1750
2025-04-09 10:07:59.402578: Training Step 88/115: batchLoss = 4.0763, diffLoss = 6.6877, kgLoss = 0.1592
2025-04-09 10:08:00.132662: Training Step 89/115: batchLoss = 4.7111, diffLoss = 7.7310, kgLoss = 0.1814
2025-04-09 10:08:00.858077: Training Step 90/115: batchLoss = 5.1901, diffLoss = 8.5163, kgLoss = 0.2007
2025-04-09 10:08:01.601167: Training Step 91/115: batchLoss = 4.5772, diffLoss = 7.5127, kgLoss = 0.1738
2025-04-09 10:08:02.337213: Training Step 92/115: batchLoss = 4.1990, diffLoss = 6.8930, kgLoss = 0.1579
2025-04-09 10:08:03.070031: Training Step 93/115: batchLoss = 4.3909, diffLoss = 7.1994, kgLoss = 0.1780
2025-04-09 10:08:03.789937: Training Step 94/115: batchLoss = 4.4524, diffLoss = 7.3030, kgLoss = 0.1766
2025-04-09 10:08:04.511867: Training Step 95/115: batchLoss = 4.8398, diffLoss = 7.9388, kgLoss = 0.1914
2025-04-09 10:08:05.241949: Training Step 96/115: batchLoss = 4.2101, diffLoss = 6.9069, kgLoss = 0.1651
2025-04-09 10:08:05.954445: Training Step 97/115: batchLoss = 4.5461, diffLoss = 7.4573, kgLoss = 0.1792
2025-04-09 10:08:06.666902: Training Step 98/115: batchLoss = 3.8765, diffLoss = 6.3561, kgLoss = 0.1570
2025-04-09 10:08:07.379566: Training Step 99/115: batchLoss = 4.2775, diffLoss = 7.0124, kgLoss = 0.1752
2025-04-09 10:08:08.097498: Training Step 100/115: batchLoss = 4.2724, diffLoss = 7.0126, kgLoss = 0.1621
2025-04-09 10:08:08.815580: Training Step 101/115: batchLoss = 4.5272, diffLoss = 7.4257, kgLoss = 0.1794
2025-04-09 10:08:09.538815: Training Step 102/115: batchLoss = 4.8493, diffLoss = 7.9497, kgLoss = 0.1988
2025-04-09 10:08:10.264816: Training Step 103/115: batchLoss = 4.6363, diffLoss = 7.6080, kgLoss = 0.1789
2025-04-09 10:08:10.993074: Training Step 104/115: batchLoss = 4.5861, diffLoss = 7.5254, kgLoss = 0.1772
2025-04-09 10:08:11.716184: Training Step 105/115: batchLoss = 4.2267, diffLoss = 6.9211, kgLoss = 0.1851
2025-04-09 10:08:12.454509: Training Step 106/115: batchLoss = 4.6301, diffLoss = 7.5933, kgLoss = 0.1852
2025-04-09 10:08:13.177575: Training Step 107/115: batchLoss = 4.2684, diffLoss = 7.0054, kgLoss = 0.1628
2025-04-09 10:08:13.902023: Training Step 108/115: batchLoss = 4.1613, diffLoss = 6.8264, kgLoss = 0.1635
2025-04-09 10:08:14.633105: Training Step 109/115: batchLoss = 4.4631, diffLoss = 7.3192, kgLoss = 0.1789
2025-04-09 10:08:15.349116: Training Step 110/115: batchLoss = 3.9746, diffLoss = 6.5154, kgLoss = 0.1632
2025-04-09 10:08:16.069114: Training Step 111/115: batchLoss = 4.3131, diffLoss = 7.0696, kgLoss = 0.1784
2025-04-09 10:08:16.785112: Training Step 112/115: batchLoss = 4.6014, diffLoss = 7.5269, kgLoss = 0.2132
2025-04-09 10:08:17.426138: Training Step 113/115: batchLoss = 4.2037, diffLoss = 6.8951, kgLoss = 0.1666
2025-04-09 10:08:18.057793: Training Step 114/115: batchLoss = 4.3873, diffLoss = 7.1913, kgLoss = 0.1812
2025-04-09 10:08:18.177561: 
2025-04-09 10:08:18.178309: Epoch 21/1000, Train: epLoss = 1.2630, epDfLoss = 2.0714, epfTransLoss = 0.0000, epKgLoss = 0.0505  
2025-04-09 10:08:18.956429: Steps 0/90: batch_recall = 54.90, batch_ndcg = 41.86 
2025-04-09 10:08:19.704828: Steps 1/90: batch_recall = 59.45, batch_ndcg = 41.17 
2025-04-09 10:08:20.460967: Steps 2/90: batch_recall = 52.82, batch_ndcg = 37.35 
2025-04-09 10:08:21.206563: Steps 3/90: batch_recall = 47.26, batch_ndcg = 33.12 
2025-04-09 10:08:21.960327: Steps 4/90: batch_recall = 49.10, batch_ndcg = 35.98 
2025-04-09 10:08:22.707768: Steps 5/90: batch_recall = 36.63, batch_ndcg = 26.72 
2025-04-09 10:08:23.450922: Steps 6/90: batch_recall = 47.21, batch_ndcg = 33.40 
2025-04-09 10:08:24.235279: Steps 7/90: batch_recall = 43.35, batch_ndcg = 31.47 
2025-04-09 10:08:25.005499: Steps 8/90: batch_recall = 46.39, batch_ndcg = 33.16 
2025-04-09 10:08:25.777747: Steps 9/90: batch_recall = 42.68, batch_ndcg = 33.42 
2025-04-09 10:08:26.524032: Steps 10/90: batch_recall = 38.36, batch_ndcg = 28.34 
2025-04-09 10:08:27.277054: Steps 11/90: batch_recall = 43.84, batch_ndcg = 28.14 
2025-04-09 10:08:28.002252: Steps 12/90: batch_recall = 39.15, batch_ndcg = 28.06 
2025-04-09 10:08:28.740135: Steps 13/90: batch_recall = 36.11, batch_ndcg = 25.18 
2025-04-09 10:08:29.493476: Steps 14/90: batch_recall = 34.34, batch_ndcg = 26.59 
2025-04-09 10:08:30.230973: Steps 15/90: batch_recall = 44.22, batch_ndcg = 28.64 
2025-04-09 10:08:30.981712: Steps 16/90: batch_recall = 37.57, batch_ndcg = 26.79 
2025-04-09 10:08:31.719638: Steps 17/90: batch_recall = 34.73, batch_ndcg = 23.22 
2025-04-09 10:08:32.446726: Steps 18/90: batch_recall = 38.66, batch_ndcg = 25.50 
2025-04-09 10:08:33.181542: Steps 19/90: batch_recall = 33.83, batch_ndcg = 23.65 
2025-04-09 10:08:33.906664: Steps 20/90: batch_recall = 35.96, batch_ndcg = 24.05 
2025-04-09 10:08:34.632269: Steps 21/90: batch_recall = 40.09, batch_ndcg = 27.99 
2025-04-09 10:08:35.353511: Steps 22/90: batch_recall = 39.81, batch_ndcg = 28.05 
2025-04-09 10:08:36.073693: Steps 23/90: batch_recall = 36.03, batch_ndcg = 24.84 
2025-04-09 10:08:36.792804: Steps 24/90: batch_recall = 39.42, batch_ndcg = 26.38 
2025-04-09 10:08:37.502362: Steps 25/90: batch_recall = 37.35, batch_ndcg = 24.13 
2025-04-09 10:08:38.220442: Steps 26/90: batch_recall = 39.83, batch_ndcg = 27.47 
2025-04-09 10:08:38.936720: Steps 27/90: batch_recall = 35.09, batch_ndcg = 24.18 
2025-04-09 10:08:39.677032: Steps 28/90: batch_recall = 36.25, batch_ndcg = 24.50 
2025-04-09 10:08:40.397651: Steps 29/90: batch_recall = 32.65, batch_ndcg = 21.86 
2025-04-09 10:08:41.119148: Steps 30/90: batch_recall = 34.19, batch_ndcg = 21.50 
2025-04-09 10:08:41.851521: Steps 31/90: batch_recall = 32.40, batch_ndcg = 22.75 
2025-04-09 10:08:42.600808: Steps 32/90: batch_recall = 35.01, batch_ndcg = 23.33 
2025-04-09 10:08:43.331599: Steps 33/90: batch_recall = 36.40, batch_ndcg = 22.24 
2025-04-09 10:08:44.045896: Steps 34/90: batch_recall = 35.57, batch_ndcg = 23.93 
2025-04-09 10:08:44.750290: Steps 35/90: batch_recall = 38.47, batch_ndcg = 26.40 
2025-04-09 10:08:45.469443: Steps 36/90: batch_recall = 38.00, batch_ndcg = 24.41 
2025-04-09 10:08:46.190570: Steps 37/90: batch_recall = 32.87, batch_ndcg = 21.01 
2025-04-09 10:08:46.901343: Steps 38/90: batch_recall = 33.09, batch_ndcg = 21.54 
2025-04-09 10:08:47.614664: Steps 39/90: batch_recall = 37.70, batch_ndcg = 22.46 
2025-04-09 10:08:48.323675: Steps 40/90: batch_recall = 31.72, batch_ndcg = 22.24 
2025-04-09 10:08:49.017194: Steps 41/90: batch_recall = 33.63, batch_ndcg = 22.69 
2025-04-09 10:08:49.730851: Steps 42/90: batch_recall = 40.53, batch_ndcg = 28.14 
2025-04-09 10:08:50.435201: Steps 43/90: batch_recall = 34.36, batch_ndcg = 22.64 
2025-04-09 10:08:51.134083: Steps 44/90: batch_recall = 34.30, batch_ndcg = 22.08 
2025-04-09 10:08:51.837520: Steps 45/90: batch_recall = 32.97, batch_ndcg = 21.14 
2025-04-09 10:08:52.568016: Steps 46/90: batch_recall = 32.17, batch_ndcg = 20.13 
2025-04-09 10:08:53.282534: Steps 47/90: batch_recall = 30.46, batch_ndcg = 19.19 
2025-04-09 10:08:53.989297: Steps 48/90: batch_recall = 36.34, batch_ndcg = 22.31 
2025-04-09 10:08:54.710104: Steps 49/90: batch_recall = 29.30, batch_ndcg = 19.18 
2025-04-09 10:08:55.410432: Steps 50/90: batch_recall = 36.64, batch_ndcg = 23.81 
2025-04-09 10:08:56.118601: Steps 51/90: batch_recall = 35.91, batch_ndcg = 23.91 
2025-04-09 10:08:56.843088: Steps 52/90: batch_recall = 34.23, batch_ndcg = 21.74 
2025-04-09 10:08:57.556327: Steps 53/90: batch_recall = 40.03, batch_ndcg = 24.66 
2025-04-09 10:08:58.289977: Steps 54/90: batch_recall = 30.43, batch_ndcg = 19.14 
2025-04-09 10:08:59.000962: Steps 55/90: batch_recall = 36.75, batch_ndcg = 23.21 
2025-04-09 10:08:59.723698: Steps 56/90: batch_recall = 31.36, batch_ndcg = 18.82 
2025-04-09 10:09:00.435591: Steps 57/90: batch_recall = 36.63, batch_ndcg = 23.67 
2025-04-09 10:09:01.143360: Steps 58/90: batch_recall = 38.89, batch_ndcg = 24.82 
2025-04-09 10:09:01.881016: Steps 59/90: batch_recall = 29.79, batch_ndcg = 19.78 
2025-04-09 10:09:02.593468: Steps 60/90: batch_recall = 31.03, batch_ndcg = 19.39 
2025-04-09 10:09:03.305174: Steps 61/90: batch_recall = 40.18, batch_ndcg = 27.25 
2025-04-09 10:09:03.992899: Steps 62/90: batch_recall = 37.95, batch_ndcg = 25.25 
2025-04-09 10:09:04.691217: Steps 63/90: batch_recall = 36.97, batch_ndcg = 23.08 
2025-04-09 10:09:05.383494: Steps 64/90: batch_recall = 35.77, batch_ndcg = 22.07 
2025-04-09 10:09:06.071337: Steps 65/90: batch_recall = 30.04, batch_ndcg = 20.14 
2025-04-09 10:09:06.769560: Steps 66/90: batch_recall = 34.81, batch_ndcg = 21.50 
2025-04-09 10:09:07.483238: Steps 67/90: batch_recall = 33.91, batch_ndcg = 20.32 
2025-04-09 10:09:08.189834: Steps 68/90: batch_recall = 33.25, batch_ndcg = 20.72 
2025-04-09 10:09:08.887300: Steps 69/90: batch_recall = 40.41, batch_ndcg = 23.92 
2025-04-09 10:09:09.597416: Steps 70/90: batch_recall = 36.26, batch_ndcg = 23.02 
2025-04-09 10:09:10.301764: Steps 71/90: batch_recall = 35.29, batch_ndcg = 21.81 
2025-04-09 10:09:11.011226: Steps 72/90: batch_recall = 34.95, batch_ndcg = 22.32 
2025-04-09 10:09:11.729159: Steps 73/90: batch_recall = 35.13, batch_ndcg = 21.84 
2025-04-09 10:09:12.459922: Steps 74/90: batch_recall = 39.01, batch_ndcg = 23.86 
2025-04-09 10:09:13.173499: Steps 75/90: batch_recall = 37.47, batch_ndcg = 23.20 
2025-04-09 10:09:13.885881: Steps 76/90: batch_recall = 41.85, batch_ndcg = 25.58 
2025-04-09 10:09:14.580607: Steps 77/90: batch_recall = 43.18, batch_ndcg = 25.98 
2025-04-09 10:09:15.289407: Steps 78/90: batch_recall = 31.67, batch_ndcg = 19.58 
2025-04-09 10:09:15.997086: Steps 79/90: batch_recall = 40.57, batch_ndcg = 27.26 
2025-04-09 10:09:16.688775: Steps 80/90: batch_recall = 43.65, batch_ndcg = 27.06 
2025-04-09 10:09:17.393644: Steps 81/90: batch_recall = 41.51, batch_ndcg = 25.82 
2025-04-09 10:09:18.088482: Steps 82/90: batch_recall = 39.28, batch_ndcg = 24.90 
2025-04-09 10:09:18.772547: Steps 83/90: batch_recall = 38.87, batch_ndcg = 24.19 
2025-04-09 10:09:19.489163: Steps 84/90: batch_recall = 34.30, batch_ndcg = 20.56 
2025-04-09 10:09:20.191662: Steps 85/90: batch_recall = 37.43, batch_ndcg = 21.78 
2025-04-09 10:09:20.893900: Steps 86/90: batch_recall = 42.65, batch_ndcg = 26.60 
2025-04-09 10:09:21.594961: Steps 87/90: batch_recall = 40.95, batch_ndcg = 27.00 
2025-04-09 10:09:22.275901: Steps 88/90: batch_recall = 45.99, batch_ndcg = 26.35 
2025-04-09 10:09:22.819993: Steps 89/90: batch_recall = 38.28, batch_ndcg = 23.90 
2025-04-09 10:09:22.820701: Epoch 21/1000, Test: Recall = 0.0752, NDCG = 0.0489  

2025-04-09 10:09:23.850394: Training Step 0/115: batchLoss = 4.8124, diffLoss = 7.8927, kgLoss = 0.1920
2025-04-09 10:09:24.577122: Training Step 1/115: batchLoss = 4.3503, diffLoss = 7.1372, kgLoss = 0.1700
2025-04-09 10:09:25.299000: Training Step 2/115: batchLoss = 4.1852, diffLoss = 6.8663, kgLoss = 0.1635
2025-04-09 10:09:26.021601: Training Step 3/115: batchLoss = 4.1592, diffLoss = 6.8174, kgLoss = 0.1718
2025-04-09 10:09:26.757441: Training Step 4/115: batchLoss = 3.9633, diffLoss = 6.4989, kgLoss = 0.1600
2025-04-09 10:09:27.488252: Training Step 5/115: batchLoss = 4.2205, diffLoss = 6.9204, kgLoss = 0.1706
2025-04-09 10:09:28.210215: Training Step 6/115: batchLoss = 4.5313, diffLoss = 7.4329, kgLoss = 0.1790
2025-04-09 10:09:28.936362: Training Step 7/115: batchLoss = 3.6464, diffLoss = 5.9776, kgLoss = 0.1495
2025-04-09 10:09:29.668124: Training Step 8/115: batchLoss = 4.6477, diffLoss = 7.6239, kgLoss = 0.1834
2025-04-09 10:09:30.407008: Training Step 9/115: batchLoss = 4.7524, diffLoss = 7.7991, kgLoss = 0.1823
2025-04-09 10:09:31.144338: Training Step 10/115: batchLoss = 4.4555, diffLoss = 7.3100, kgLoss = 0.1737
2025-04-09 10:09:31.890928: Training Step 11/115: batchLoss = 3.9216, diffLoss = 6.4264, kgLoss = 0.1644
2025-04-09 10:09:32.626696: Training Step 12/115: batchLoss = 3.7971, diffLoss = 6.2263, kgLoss = 0.1534
2025-04-09 10:09:33.367970: Training Step 13/115: batchLoss = 4.7436, diffLoss = 7.7765, kgLoss = 0.1941
2025-04-09 10:09:34.103287: Training Step 14/115: batchLoss = 4.5806, diffLoss = 7.5157, kgLoss = 0.1780
2025-04-09 10:09:34.844708: Training Step 15/115: batchLoss = 4.6689, diffLoss = 7.6583, kgLoss = 0.1848
2025-04-09 10:09:35.579815: Training Step 16/115: batchLoss = 4.3224, diffLoss = 7.0870, kgLoss = 0.1754
2025-04-09 10:09:36.314511: Training Step 17/115: batchLoss = 4.0450, diffLoss = 6.6334, kgLoss = 0.1624
2025-04-09 10:09:37.050358: Training Step 18/115: batchLoss = 5.2367, diffLoss = 8.5887, kgLoss = 0.2086
2025-04-09 10:09:37.782117: Training Step 19/115: batchLoss = 4.1074, diffLoss = 6.7400, kgLoss = 0.1584
2025-04-09 10:09:38.518670: Training Step 20/115: batchLoss = 4.4852, diffLoss = 7.3569, kgLoss = 0.1778
2025-04-09 10:09:39.257554: Training Step 21/115: batchLoss = 4.3649, diffLoss = 7.1543, kgLoss = 0.1808
2025-04-09 10:09:40.008936: Training Step 22/115: batchLoss = 4.9622, diffLoss = 8.1400, kgLoss = 0.1955
2025-04-09 10:09:40.760725: Training Step 23/115: batchLoss = 4.5296, diffLoss = 7.4312, kgLoss = 0.1771
2025-04-09 10:09:41.511913: Training Step 24/115: batchLoss = 4.0516, diffLoss = 6.6418, kgLoss = 0.1664
2025-04-09 10:09:42.245445: Training Step 25/115: batchLoss = 4.7412, diffLoss = 7.7729, kgLoss = 0.1937
2025-04-09 10:09:42.973377: Training Step 26/115: batchLoss = 4.3410, diffLoss = 7.1162, kgLoss = 0.1782
2025-04-09 10:09:43.699677: Training Step 27/115: batchLoss = 4.6309, diffLoss = 7.5970, kgLoss = 0.1816
2025-04-09 10:09:44.422819: Training Step 28/115: batchLoss = 4.6392, diffLoss = 7.6051, kgLoss = 0.1904
2025-04-09 10:09:45.163047: Training Step 29/115: batchLoss = 4.0871, diffLoss = 6.7031, kgLoss = 0.1632
2025-04-09 10:09:45.910142: Training Step 30/115: batchLoss = 4.3151, diffLoss = 7.0702, kgLoss = 0.1823
2025-04-09 10:09:46.641928: Training Step 31/115: batchLoss = 4.3670, diffLoss = 7.1617, kgLoss = 0.1749
2025-04-09 10:09:47.380462: Training Step 32/115: batchLoss = 4.5220, diffLoss = 7.4152, kgLoss = 0.1822
2025-04-09 10:09:48.113542: Training Step 33/115: batchLoss = 4.1908, diffLoss = 6.8744, kgLoss = 0.1654
2025-04-09 10:09:48.844893: Training Step 34/115: batchLoss = 4.4982, diffLoss = 7.3831, kgLoss = 0.1708
2025-04-09 10:09:49.576036: Training Step 35/115: batchLoss = 3.9045, diffLoss = 6.3996, kgLoss = 0.1618
2025-04-09 10:09:50.305720: Training Step 36/115: batchLoss = 3.6382, diffLoss = 5.9619, kgLoss = 0.1526
2025-04-09 10:09:51.041814: Training Step 37/115: batchLoss = 4.2130, diffLoss = 6.9106, kgLoss = 0.1666
2025-04-09 10:09:51.784170: Training Step 38/115: batchLoss = 4.0404, diffLoss = 6.6249, kgLoss = 0.1637
2025-04-09 10:09:52.513519: Training Step 39/115: batchLoss = 4.4949, diffLoss = 7.3680, kgLoss = 0.1852
2025-04-09 10:09:53.233383: Training Step 40/115: batchLoss = 4.4159, diffLoss = 7.2403, kgLoss = 0.1792
2025-04-09 10:09:53.958726: Training Step 41/115: batchLoss = 3.6972, diffLoss = 6.0635, kgLoss = 0.1479
2025-04-09 10:09:54.702609: Training Step 42/115: batchLoss = 4.1745, diffLoss = 6.8399, kgLoss = 0.1765
2025-04-09 10:09:55.441427: Training Step 43/115: batchLoss = 4.2035, diffLoss = 6.8984, kgLoss = 0.1611
2025-04-09 10:09:56.170975: Training Step 44/115: batchLoss = 4.6181, diffLoss = 7.5687, kgLoss = 0.1924
2025-04-09 10:09:56.897841: Training Step 45/115: batchLoss = 5.2588, diffLoss = 8.6309, kgLoss = 0.2008
2025-04-09 10:09:57.621969: Training Step 46/115: batchLoss = 3.6760, diffLoss = 6.0297, kgLoss = 0.1455
2025-04-09 10:09:58.342652: Training Step 47/115: batchLoss = 4.3317, diffLoss = 7.1023, kgLoss = 0.1757
2025-04-09 10:09:59.073827: Training Step 48/115: batchLoss = 3.7480, diffLoss = 6.1422, kgLoss = 0.1566
2025-04-09 10:09:59.802013: Training Step 49/115: batchLoss = 4.4074, diffLoss = 7.2329, kgLoss = 0.1692
2025-04-09 10:10:00.553293: Training Step 50/115: batchLoss = 4.5078, diffLoss = 7.3975, kgLoss = 0.1733
2025-04-09 10:10:01.298332: Training Step 51/115: batchLoss = 4.6074, diffLoss = 7.5521, kgLoss = 0.1902
2025-04-09 10:10:02.046456: Training Step 52/115: batchLoss = 4.2967, diffLoss = 7.0499, kgLoss = 0.1668
2025-04-09 10:10:02.785939: Training Step 53/115: batchLoss = 4.2280, diffLoss = 6.9367, kgLoss = 0.1650
2025-04-09 10:10:03.519285: Training Step 54/115: batchLoss = 3.9856, diffLoss = 6.5348, kgLoss = 0.1618
2025-04-09 10:10:04.248682: Training Step 55/115: batchLoss = 4.5045, diffLoss = 7.3815, kgLoss = 0.1889
2025-04-09 10:10:04.986680: Training Step 56/115: batchLoss = 4.8087, diffLoss = 7.8882, kgLoss = 0.1895
2025-04-09 10:10:05.715953: Training Step 57/115: batchLoss = 4.6828, diffLoss = 7.6785, kgLoss = 0.1894
2025-04-09 10:10:06.434544: Training Step 58/115: batchLoss = 3.9526, diffLoss = 6.4815, kgLoss = 0.1593
2025-04-09 10:10:07.169492: Training Step 59/115: batchLoss = 3.8465, diffLoss = 6.3100, kgLoss = 0.1514
2025-04-09 10:10:07.918510: Training Step 60/115: batchLoss = 4.4005, diffLoss = 7.1969, kgLoss = 0.2060
2025-04-09 10:10:08.656409: Training Step 61/115: batchLoss = 4.4412, diffLoss = 7.2757, kgLoss = 0.1894
2025-04-09 10:10:09.384478: Training Step 62/115: batchLoss = 4.5389, diffLoss = 7.4346, kgLoss = 0.1953
2025-04-09 10:10:10.124756: Training Step 63/115: batchLoss = 4.4235, diffLoss = 7.2602, kgLoss = 0.1684
2025-04-09 10:10:10.884475: Training Step 64/115: batchLoss = 4.2685, diffLoss = 7.0021, kgLoss = 0.1681
2025-04-09 10:10:11.638550: Training Step 65/115: batchLoss = 4.0173, diffLoss = 6.5736, kgLoss = 0.1829
2025-04-09 10:10:12.375269: Training Step 66/115: batchLoss = 4.3855, diffLoss = 7.1936, kgLoss = 0.1732
2025-04-09 10:10:13.128994: Training Step 67/115: batchLoss = 4.7897, diffLoss = 7.8504, kgLoss = 0.1988
2025-04-09 10:10:13.872688: Training Step 68/115: batchLoss = 4.4881, diffLoss = 7.3668, kgLoss = 0.1701
2025-04-09 10:10:14.594265: Training Step 69/115: batchLoss = 4.1299, diffLoss = 6.7699, kgLoss = 0.1698
2025-04-09 10:10:15.327045: Training Step 70/115: batchLoss = 5.0405, diffLoss = 8.2684, kgLoss = 0.1987
2025-04-09 10:10:16.070185: Training Step 71/115: batchLoss = 3.9412, diffLoss = 6.4655, kgLoss = 0.1546
2025-04-09 10:10:16.797900: Training Step 72/115: batchLoss = 3.6173, diffLoss = 5.9366, kgLoss = 0.1385
2025-04-09 10:10:17.531908: Training Step 73/115: batchLoss = 4.7424, diffLoss = 7.7762, kgLoss = 0.1917
2025-04-09 10:10:18.259989: Training Step 74/115: batchLoss = 4.7326, diffLoss = 7.7660, kgLoss = 0.1826
2025-04-09 10:10:18.994540: Training Step 75/115: batchLoss = 4.3275, diffLoss = 7.1009, kgLoss = 0.1674
2025-04-09 10:10:19.726456: Training Step 76/115: batchLoss = 4.5085, diffLoss = 7.3984, kgLoss = 0.1736
2025-04-09 10:10:20.450310: Training Step 77/115: batchLoss = 3.9969, diffLoss = 6.5543, kgLoss = 0.1608
2025-04-09 10:10:21.184300: Training Step 78/115: batchLoss = 4.5327, diffLoss = 7.4288, kgLoss = 0.1885
2025-04-09 10:10:21.913412: Training Step 79/115: batchLoss = 3.9908, diffLoss = 6.5363, kgLoss = 0.1725
2025-04-09 10:10:22.674504: Training Step 80/115: batchLoss = 4.1240, diffLoss = 6.7577, kgLoss = 0.1735
2025-04-09 10:10:23.423893: Training Step 81/115: batchLoss = 4.5692, diffLoss = 7.4940, kgLoss = 0.1819
2025-04-09 10:10:24.166307: Training Step 82/115: batchLoss = 4.3690, diffLoss = 7.1650, kgLoss = 0.1749
2025-04-09 10:10:24.905486: Training Step 83/115: batchLoss = 4.4458, diffLoss = 7.2882, kgLoss = 0.1821
2025-04-09 10:10:25.635514: Training Step 84/115: batchLoss = 4.3691, diffLoss = 7.1690, kgLoss = 0.1692
2025-04-09 10:10:26.377686: Training Step 85/115: batchLoss = 4.3588, diffLoss = 7.1513, kgLoss = 0.1700
2025-04-09 10:10:27.115142: Training Step 86/115: batchLoss = 4.5163, diffLoss = 7.4066, kgLoss = 0.1807
2025-04-09 10:10:27.844057: Training Step 87/115: batchLoss = 4.5289, diffLoss = 7.4266, kgLoss = 0.1825
2025-04-09 10:10:28.587492: Training Step 88/115: batchLoss = 4.3186, diffLoss = 7.0867, kgLoss = 0.1664
2025-04-09 10:10:29.354034: Training Step 89/115: batchLoss = 4.1040, diffLoss = 6.7283, kgLoss = 0.1676
2025-04-09 10:10:30.091398: Training Step 90/115: batchLoss = 3.8973, diffLoss = 6.3888, kgLoss = 0.1601
2025-04-09 10:10:30.839240: Training Step 91/115: batchLoss = 4.1173, diffLoss = 6.7509, kgLoss = 0.1669
2025-04-09 10:10:31.578443: Training Step 92/115: batchLoss = 4.4369, diffLoss = 7.2742, kgLoss = 0.1809
2025-04-09 10:10:32.318062: Training Step 93/115: batchLoss = 4.6796, diffLoss = 7.6802, kgLoss = 0.1786
2025-04-09 10:10:33.053340: Training Step 94/115: batchLoss = 5.2765, diffLoss = 8.6608, kgLoss = 0.2001
2025-04-09 10:10:33.779929: Training Step 95/115: batchLoss = 4.3722, diffLoss = 7.1703, kgLoss = 0.1750
2025-04-09 10:10:34.502928: Training Step 96/115: batchLoss = 4.8255, diffLoss = 7.9194, kgLoss = 0.1847
2025-04-09 10:10:35.225334: Training Step 97/115: batchLoss = 3.6526, diffLoss = 5.9870, kgLoss = 0.1509
2025-04-09 10:10:35.946577: Training Step 98/115: batchLoss = 4.9631, diffLoss = 8.1495, kgLoss = 0.1836
2025-04-09 10:10:36.684710: Training Step 99/115: batchLoss = 4.4302, diffLoss = 7.2584, kgLoss = 0.1880
2025-04-09 10:10:37.405448: Training Step 100/115: batchLoss = 4.3416, diffLoss = 7.1201, kgLoss = 0.1737
2025-04-09 10:10:38.133346: Training Step 101/115: batchLoss = 4.4309, diffLoss = 7.2629, kgLoss = 0.1827
2025-04-09 10:10:38.860493: Training Step 102/115: batchLoss = 4.5730, diffLoss = 7.4980, kgLoss = 0.1854
2025-04-09 10:10:39.596552: Training Step 103/115: batchLoss = 4.4567, diffLoss = 7.3040, kgLoss = 0.1857
2025-04-09 10:10:40.339232: Training Step 104/115: batchLoss = 4.0272, diffLoss = 6.6072, kgLoss = 0.1573
2025-04-09 10:10:41.066536: Training Step 105/115: batchLoss = 4.6187, diffLoss = 7.5791, kgLoss = 0.1781
2025-04-09 10:10:41.787521: Training Step 106/115: batchLoss = 4.8092, diffLoss = 7.8904, kgLoss = 0.1874
2025-04-09 10:10:42.518993: Training Step 107/115: batchLoss = 4.3729, diffLoss = 7.1710, kgLoss = 0.1758
2025-04-09 10:10:43.247080: Training Step 108/115: batchLoss = 4.3904, diffLoss = 7.1967, kgLoss = 0.1810
2025-04-09 10:10:43.988335: Training Step 109/115: batchLoss = 4.9107, diffLoss = 8.0539, kgLoss = 0.1958
2025-04-09 10:10:44.721561: Training Step 110/115: batchLoss = 4.3386, diffLoss = 7.1126, kgLoss = 0.1776
2025-04-09 10:10:45.453106: Training Step 111/115: batchLoss = 4.2517, diffLoss = 6.9797, kgLoss = 0.1596
2025-04-09 10:10:46.202886: Training Step 112/115: batchLoss = 4.3448, diffLoss = 7.1307, kgLoss = 0.1658
2025-04-09 10:10:46.857337: Training Step 113/115: batchLoss = 4.3808, diffLoss = 7.1885, kgLoss = 0.1694
2025-04-09 10:10:47.495620: Training Step 114/115: batchLoss = 5.0366, diffLoss = 8.2715, kgLoss = 0.1842
2025-04-09 10:10:47.613186: 
2025-04-09 10:10:47.613841: Epoch 22/1000, Train: epLoss = 1.2577, epDfLoss = 2.0625, epfTransLoss = 0.0000, epKgLoss = 0.0504  
2025-04-09 10:10:48.369010: Steps 0/90: batch_recall = 54.91, batch_ndcg = 41.83 
2025-04-09 10:10:49.114686: Steps 1/90: batch_recall = 60.52, batch_ndcg = 41.31 
2025-04-09 10:10:49.873191: Steps 2/90: batch_recall = 52.03, batch_ndcg = 37.04 
2025-04-09 10:10:50.610631: Steps 3/90: batch_recall = 46.91, batch_ndcg = 33.15 
2025-04-09 10:10:51.350648: Steps 4/90: batch_recall = 48.91, batch_ndcg = 36.12 
2025-04-09 10:10:52.090448: Steps 5/90: batch_recall = 36.92, batch_ndcg = 26.84 
2025-04-09 10:10:52.830743: Steps 6/90: batch_recall = 45.54, batch_ndcg = 32.87 
2025-04-09 10:10:53.578193: Steps 7/90: batch_recall = 43.65, batch_ndcg = 31.45 
2025-04-09 10:10:54.310336: Steps 8/90: batch_recall = 45.60, batch_ndcg = 33.19 
2025-04-09 10:10:55.062653: Steps 9/90: batch_recall = 43.31, batch_ndcg = 33.77 
2025-04-09 10:10:55.813807: Steps 10/90: batch_recall = 38.33, batch_ndcg = 28.35 
2025-04-09 10:10:56.575043: Steps 11/90: batch_recall = 44.74, batch_ndcg = 28.61 
2025-04-09 10:10:57.312662: Steps 12/90: batch_recall = 38.68, batch_ndcg = 27.76 
2025-04-09 10:10:58.040065: Steps 13/90: batch_recall = 37.62, batch_ndcg = 25.84 
2025-04-09 10:10:58.774590: Steps 14/90: batch_recall = 34.84, batch_ndcg = 27.11 
2025-04-09 10:10:59.495166: Steps 15/90: batch_recall = 45.39, batch_ndcg = 29.09 
2025-04-09 10:11:00.230419: Steps 16/90: batch_recall = 37.55, batch_ndcg = 26.30 
2025-04-09 10:11:00.963575: Steps 17/90: batch_recall = 35.07, batch_ndcg = 23.23 
2025-04-09 10:11:01.687933: Steps 18/90: batch_recall = 38.94, batch_ndcg = 25.29 
2025-04-09 10:11:02.411974: Steps 19/90: batch_recall = 34.52, batch_ndcg = 23.72 
2025-04-09 10:11:03.129422: Steps 20/90: batch_recall = 36.81, batch_ndcg = 24.54 
2025-04-09 10:11:03.855357: Steps 21/90: batch_recall = 38.59, batch_ndcg = 28.03 
2025-04-09 10:11:04.572752: Steps 22/90: batch_recall = 41.75, batch_ndcg = 29.01 
2025-04-09 10:11:05.277109: Steps 23/90: batch_recall = 36.28, batch_ndcg = 24.86 
2025-04-09 10:11:05.997397: Steps 24/90: batch_recall = 39.82, batch_ndcg = 26.53 
2025-04-09 10:11:06.716513: Steps 25/90: batch_recall = 38.62, batch_ndcg = 24.58 
2025-04-09 10:11:07.419797: Steps 26/90: batch_recall = 38.13, batch_ndcg = 26.95 
2025-04-09 10:11:08.135736: Steps 27/90: batch_recall = 35.62, batch_ndcg = 24.38 
2025-04-09 10:11:08.864659: Steps 28/90: batch_recall = 36.99, batch_ndcg = 24.95 
2025-04-09 10:11:09.596595: Steps 29/90: batch_recall = 33.06, batch_ndcg = 22.02 
2025-04-09 10:11:10.326185: Steps 30/90: batch_recall = 34.03, batch_ndcg = 21.63 
2025-04-09 10:11:11.066340: Steps 31/90: batch_recall = 32.47, batch_ndcg = 22.53 
2025-04-09 10:11:11.792994: Steps 32/90: batch_recall = 35.24, batch_ndcg = 23.37 
2025-04-09 10:11:12.506294: Steps 33/90: batch_recall = 35.53, batch_ndcg = 22.10 
2025-04-09 10:11:13.218389: Steps 34/90: batch_recall = 35.92, batch_ndcg = 24.51 
2025-04-09 10:11:13.941161: Steps 35/90: batch_recall = 39.58, batch_ndcg = 26.87 
2025-04-09 10:11:14.666811: Steps 36/90: batch_recall = 37.31, batch_ndcg = 24.28 
2025-04-09 10:11:15.400426: Steps 37/90: batch_recall = 32.52, batch_ndcg = 20.88 
2025-04-09 10:11:16.146433: Steps 38/90: batch_recall = 33.52, batch_ndcg = 21.58 
2025-04-09 10:11:16.870752: Steps 39/90: batch_recall = 37.42, batch_ndcg = 22.83 
2025-04-09 10:11:17.589998: Steps 40/90: batch_recall = 31.74, batch_ndcg = 22.24 
2025-04-09 10:11:18.337044: Steps 41/90: batch_recall = 35.51, batch_ndcg = 23.53 
2025-04-09 10:11:19.052541: Steps 42/90: batch_recall = 39.52, batch_ndcg = 27.91 
2025-04-09 10:11:19.764834: Steps 43/90: batch_recall = 35.30, batch_ndcg = 23.14 
2025-04-09 10:11:20.470188: Steps 44/90: batch_recall = 34.37, batch_ndcg = 22.83 
2025-04-09 10:11:21.175099: Steps 45/90: batch_recall = 32.80, batch_ndcg = 21.39 
2025-04-09 10:11:21.876901: Steps 46/90: batch_recall = 32.86, batch_ndcg = 20.28 
2025-04-09 10:11:22.583112: Steps 47/90: batch_recall = 30.19, batch_ndcg = 19.43 
2025-04-09 10:11:23.277526: Steps 48/90: batch_recall = 37.55, batch_ndcg = 22.70 
2025-04-09 10:11:23.988726: Steps 49/90: batch_recall = 29.28, batch_ndcg = 19.91 
2025-04-09 10:11:24.704279: Steps 50/90: batch_recall = 35.72, batch_ndcg = 23.68 
2025-04-09 10:11:25.408466: Steps 51/90: batch_recall = 37.09, batch_ndcg = 24.48 
2025-04-09 10:11:26.128588: Steps 52/90: batch_recall = 35.44, batch_ndcg = 22.37 
2025-04-09 10:11:26.856758: Steps 53/90: batch_recall = 40.42, batch_ndcg = 24.85 
2025-04-09 10:11:27.585311: Steps 54/90: batch_recall = 31.15, batch_ndcg = 19.74 
2025-04-09 10:11:28.295697: Steps 55/90: batch_recall = 36.43, batch_ndcg = 23.67 
2025-04-09 10:11:29.010746: Steps 56/90: batch_recall = 31.80, batch_ndcg = 18.98 
2025-04-09 10:11:29.721028: Steps 57/90: batch_recall = 35.67, batch_ndcg = 23.26 
2025-04-09 10:11:30.416738: Steps 58/90: batch_recall = 39.23, batch_ndcg = 24.71 
2025-04-09 10:11:31.136141: Steps 59/90: batch_recall = 30.97, batch_ndcg = 20.29 
2025-04-09 10:11:31.850395: Steps 60/90: batch_recall = 30.86, batch_ndcg = 19.52 
2025-04-09 10:11:32.582979: Steps 61/90: batch_recall = 39.88, batch_ndcg = 27.00 
2025-04-09 10:11:33.288721: Steps 62/90: batch_recall = 37.53, batch_ndcg = 25.46 
2025-04-09 10:11:33.995635: Steps 63/90: batch_recall = 36.26, batch_ndcg = 22.87 
2025-04-09 10:11:34.690267: Steps 64/90: batch_recall = 35.42, batch_ndcg = 22.20 
2025-04-09 10:11:35.404782: Steps 65/90: batch_recall = 28.86, batch_ndcg = 19.81 
2025-04-09 10:11:36.102144: Steps 66/90: batch_recall = 35.37, batch_ndcg = 22.07 
2025-04-09 10:11:36.801106: Steps 67/90: batch_recall = 32.99, batch_ndcg = 20.32 
2025-04-09 10:11:37.514267: Steps 68/90: batch_recall = 34.61, batch_ndcg = 21.32 
2025-04-09 10:11:38.216082: Steps 69/90: batch_recall = 40.38, batch_ndcg = 23.95 
2025-04-09 10:11:38.932407: Steps 70/90: batch_recall = 37.05, batch_ndcg = 23.33 
2025-04-09 10:11:39.625733: Steps 71/90: batch_recall = 34.92, batch_ndcg = 21.74 
2025-04-09 10:11:40.335839: Steps 72/90: batch_recall = 35.46, batch_ndcg = 22.63 
2025-04-09 10:11:41.053075: Steps 73/90: batch_recall = 34.73, batch_ndcg = 21.67 
2025-04-09 10:11:41.774159: Steps 74/90: batch_recall = 39.17, batch_ndcg = 24.00 
2025-04-09 10:11:42.500166: Steps 75/90: batch_recall = 38.07, batch_ndcg = 23.85 
2025-04-09 10:11:43.213488: Steps 76/90: batch_recall = 39.97, batch_ndcg = 25.40 
2025-04-09 10:11:43.924433: Steps 77/90: batch_recall = 43.52, batch_ndcg = 26.79 
2025-04-09 10:11:44.630510: Steps 78/90: batch_recall = 30.76, batch_ndcg = 19.58 
2025-04-09 10:11:45.343054: Steps 79/90: batch_recall = 41.20, batch_ndcg = 27.37 
2025-04-09 10:11:46.051530: Steps 80/90: batch_recall = 44.00, batch_ndcg = 27.53 
2025-04-09 10:11:46.784596: Steps 81/90: batch_recall = 41.51, batch_ndcg = 25.69 
2025-04-09 10:11:47.485563: Steps 82/90: batch_recall = 39.05, batch_ndcg = 24.94 
2025-04-09 10:11:48.175826: Steps 83/90: batch_recall = 39.38, batch_ndcg = 24.74 
2025-04-09 10:11:48.877053: Steps 84/90: batch_recall = 32.41, batch_ndcg = 20.23 
2025-04-09 10:11:49.561333: Steps 85/90: batch_recall = 38.04, batch_ndcg = 22.23 
2025-04-09 10:11:50.273485: Steps 86/90: batch_recall = 44.17, batch_ndcg = 27.57 
2025-04-09 10:11:50.950913: Steps 87/90: batch_recall = 42.82, batch_ndcg = 28.29 
2025-04-09 10:11:51.646869: Steps 88/90: batch_recall = 47.12, batch_ndcg = 26.95 
2025-04-09 10:11:52.191895: Steps 89/90: batch_recall = 38.46, batch_ndcg = 24.39 
2025-04-09 10:11:52.192674: Epoch 22/1000, Test: Recall = 0.0761, NDCG = 0.0493  

2025-04-09 10:11:53.210534: Training Step 0/115: batchLoss = 4.3598, diffLoss = 7.1517, kgLoss = 0.1720
2025-04-09 10:11:53.933307: Training Step 1/115: batchLoss = 3.9389, diffLoss = 6.4539, kgLoss = 0.1664
2025-04-09 10:11:54.655596: Training Step 2/115: batchLoss = 4.3572, diffLoss = 7.1419, kgLoss = 0.1801
2025-04-09 10:11:55.378583: Training Step 3/115: batchLoss = 4.4501, diffLoss = 7.2995, kgLoss = 0.1759
2025-04-09 10:11:56.101668: Training Step 4/115: batchLoss = 4.0539, diffLoss = 6.6472, kgLoss = 0.1639
2025-04-09 10:11:56.830023: Training Step 5/115: batchLoss = 4.8044, diffLoss = 7.8832, kgLoss = 0.1863
2025-04-09 10:11:57.549731: Training Step 6/115: batchLoss = 4.2926, diffLoss = 7.0379, kgLoss = 0.1746
2025-04-09 10:11:58.264501: Training Step 7/115: batchLoss = 4.0032, diffLoss = 6.5619, kgLoss = 0.1651
2025-04-09 10:11:58.980353: Training Step 8/115: batchLoss = 4.1060, diffLoss = 6.7288, kgLoss = 0.1719
2025-04-09 10:11:59.703000: Training Step 9/115: batchLoss = 4.6057, diffLoss = 7.5560, kgLoss = 0.1803
2025-04-09 10:12:00.430800: Training Step 10/115: batchLoss = 4.0299, diffLoss = 6.6054, kgLoss = 0.1666
2025-04-09 10:12:01.176788: Training Step 11/115: batchLoss = 4.3035, diffLoss = 7.0642, kgLoss = 0.1625
2025-04-09 10:12:01.913198: Training Step 12/115: batchLoss = 4.7301, diffLoss = 7.7605, kgLoss = 0.1845
2025-04-09 10:12:02.652526: Training Step 13/115: batchLoss = 4.6532, diffLoss = 7.6390, kgLoss = 0.1743
2025-04-09 10:12:03.373176: Training Step 14/115: batchLoss = 4.3368, diffLoss = 7.1068, kgLoss = 0.1819
2025-04-09 10:12:04.093268: Training Step 15/115: batchLoss = 4.3459, diffLoss = 7.1327, kgLoss = 0.1657
2025-04-09 10:12:04.816676: Training Step 16/115: batchLoss = 4.8418, diffLoss = 7.9404, kgLoss = 0.1939
2025-04-09 10:12:05.550479: Training Step 17/115: batchLoss = 4.5935, diffLoss = 7.5355, kgLoss = 0.1804
2025-04-09 10:12:06.277380: Training Step 18/115: batchLoss = 4.9354, diffLoss = 8.0989, kgLoss = 0.1901
2025-04-09 10:12:07.010814: Training Step 19/115: batchLoss = 4.6120, diffLoss = 7.5588, kgLoss = 0.1919
2025-04-09 10:12:07.752208: Training Step 20/115: batchLoss = 4.2657, diffLoss = 7.0008, kgLoss = 0.1631
2025-04-09 10:12:08.480072: Training Step 21/115: batchLoss = 4.4191, diffLoss = 7.2479, kgLoss = 0.1759
2025-04-09 10:12:09.218965: Training Step 22/115: batchLoss = 4.4359, diffLoss = 7.2785, kgLoss = 0.1721
2025-04-09 10:12:09.953791: Training Step 23/115: batchLoss = 4.8757, diffLoss = 7.9971, kgLoss = 0.1936
2025-04-09 10:12:10.688439: Training Step 24/115: batchLoss = 4.6034, diffLoss = 7.5547, kgLoss = 0.1764
2025-04-09 10:12:11.419413: Training Step 25/115: batchLoss = 4.0427, diffLoss = 6.6291, kgLoss = 0.1630
2025-04-09 10:12:12.145104: Training Step 26/115: batchLoss = 4.2701, diffLoss = 7.0024, kgLoss = 0.1715
2025-04-09 10:12:12.876137: Training Step 27/115: batchLoss = 4.4418, diffLoss = 7.2816, kgLoss = 0.1822
2025-04-09 10:12:13.608188: Training Step 28/115: batchLoss = 4.3163, diffLoss = 7.0843, kgLoss = 0.1642
2025-04-09 10:12:14.340867: Training Step 29/115: batchLoss = 4.1168, diffLoss = 6.7462, kgLoss = 0.1726
2025-04-09 10:12:15.073113: Training Step 30/115: batchLoss = 4.6981, diffLoss = 7.7021, kgLoss = 0.1921
2025-04-09 10:12:15.812663: Training Step 31/115: batchLoss = 4.2258, diffLoss = 6.9312, kgLoss = 0.1677
2025-04-09 10:12:16.540142: Training Step 32/115: batchLoss = 4.1372, diffLoss = 6.7815, kgLoss = 0.1707
2025-04-09 10:12:17.264807: Training Step 33/115: batchLoss = 4.3449, diffLoss = 7.1062, kgLoss = 0.2029
2025-04-09 10:12:17.998359: Training Step 34/115: batchLoss = 4.6546, diffLoss = 7.6397, kgLoss = 0.1769
2025-04-09 10:12:18.732331: Training Step 35/115: batchLoss = 4.1677, diffLoss = 6.8346, kgLoss = 0.1673
2025-04-09 10:12:19.460201: Training Step 36/115: batchLoss = 4.4380, diffLoss = 7.2814, kgLoss = 0.1730
2025-04-09 10:12:20.191177: Training Step 37/115: batchLoss = 4.2395, diffLoss = 6.9573, kgLoss = 0.1627
2025-04-09 10:12:20.919784: Training Step 38/115: batchLoss = 3.9104, diffLoss = 6.4135, kgLoss = 0.1559
2025-04-09 10:12:21.647080: Training Step 39/115: batchLoss = 3.8557, diffLoss = 6.3111, kgLoss = 0.1725
2025-04-09 10:12:22.381155: Training Step 40/115: batchLoss = 4.5967, diffLoss = 7.5348, kgLoss = 0.1897
2025-04-09 10:12:23.110836: Training Step 41/115: batchLoss = 4.6881, diffLoss = 7.6909, kgLoss = 0.1839
2025-04-09 10:12:23.841665: Training Step 42/115: batchLoss = 4.2727, diffLoss = 7.0094, kgLoss = 0.1677
2025-04-09 10:12:24.571350: Training Step 43/115: batchLoss = 4.5677, diffLoss = 7.4985, kgLoss = 0.1716
2025-04-09 10:12:25.316607: Training Step 44/115: batchLoss = 4.0853, diffLoss = 6.7044, kgLoss = 0.1566
2025-04-09 10:12:26.062369: Training Step 45/115: batchLoss = 4.7666, diffLoss = 7.8271, kgLoss = 0.1758
2025-04-09 10:12:26.806510: Training Step 46/115: batchLoss = 4.5440, diffLoss = 7.4579, kgLoss = 0.1731
2025-04-09 10:12:27.545662: Training Step 47/115: batchLoss = 4.3615, diffLoss = 7.1582, kgLoss = 0.1665
2025-04-09 10:12:28.290214: Training Step 48/115: batchLoss = 4.3528, diffLoss = 7.1420, kgLoss = 0.1691
2025-04-09 10:12:29.044913: Training Step 49/115: batchLoss = 3.9451, diffLoss = 6.4614, kgLoss = 0.1705
2025-04-09 10:12:29.792825: Training Step 50/115: batchLoss = 4.7409, diffLoss = 7.7730, kgLoss = 0.1927
2025-04-09 10:12:30.524522: Training Step 51/115: batchLoss = 4.1700, diffLoss = 6.8398, kgLoss = 0.1653
2025-04-09 10:12:31.260733: Training Step 52/115: batchLoss = 3.8248, diffLoss = 6.2709, kgLoss = 0.1557
2025-04-09 10:12:32.002556: Training Step 53/115: batchLoss = 4.3003, diffLoss = 7.0535, kgLoss = 0.1705
2025-04-09 10:12:32.737694: Training Step 54/115: batchLoss = 4.4234, diffLoss = 7.2593, kgLoss = 0.1695
2025-04-09 10:12:33.462339: Training Step 55/115: batchLoss = 4.3932, diffLoss = 7.2099, kgLoss = 0.1682
2025-04-09 10:12:34.191661: Training Step 56/115: batchLoss = 4.4274, diffLoss = 7.2584, kgLoss = 0.1808
2025-04-09 10:12:34.928942: Training Step 57/115: batchLoss = 4.7659, diffLoss = 7.8168, kgLoss = 0.1896
2025-04-09 10:12:35.652287: Training Step 58/115: batchLoss = 4.9043, diffLoss = 8.0343, kgLoss = 0.2092
2025-04-09 10:12:36.388987: Training Step 59/115: batchLoss = 4.7791, diffLoss = 7.8311, kgLoss = 0.2011
2025-04-09 10:12:37.130099: Training Step 60/115: batchLoss = 3.9219, diffLoss = 6.4299, kgLoss = 0.1600
2025-04-09 10:12:37.886709: Training Step 61/115: batchLoss = 4.1898, diffLoss = 6.8595, kgLoss = 0.1853
2025-04-09 10:12:38.638239: Training Step 62/115: batchLoss = 4.1581, diffLoss = 6.8243, kgLoss = 0.1588
2025-04-09 10:12:39.376830: Training Step 63/115: batchLoss = 4.3505, diffLoss = 7.1398, kgLoss = 0.1666
2025-04-09 10:12:40.110993: Training Step 64/115: batchLoss = 4.6806, diffLoss = 7.6803, kgLoss = 0.1812
2025-04-09 10:12:40.842511: Training Step 65/115: batchLoss = 4.5986, diffLoss = 7.5396, kgLoss = 0.1870
2025-04-09 10:12:41.578286: Training Step 66/115: batchLoss = 4.3807, diffLoss = 7.1749, kgLoss = 0.1894
2025-04-09 10:12:42.322520: Training Step 67/115: batchLoss = 4.8413, diffLoss = 7.9401, kgLoss = 0.1930
2025-04-09 10:12:43.055226: Training Step 68/115: batchLoss = 4.3710, diffLoss = 7.1692, kgLoss = 0.1738
2025-04-09 10:12:43.785026: Training Step 69/115: batchLoss = 4.1257, diffLoss = 6.7686, kgLoss = 0.1613
2025-04-09 10:12:44.527400: Training Step 70/115: batchLoss = 3.8221, diffLoss = 6.2674, kgLoss = 0.1542
2025-04-09 10:12:45.262206: Training Step 71/115: batchLoss = 4.9384, diffLoss = 8.1046, kgLoss = 0.1892
2025-04-09 10:12:46.002411: Training Step 72/115: batchLoss = 3.9896, diffLoss = 6.5379, kgLoss = 0.1671
2025-04-09 10:12:46.730892: Training Step 73/115: batchLoss = 4.2844, diffLoss = 7.0316, kgLoss = 0.1635
2025-04-09 10:12:47.459671: Training Step 74/115: batchLoss = 4.5579, diffLoss = 7.4756, kgLoss = 0.1813
2025-04-09 10:12:48.190163: Training Step 75/115: batchLoss = 4.0104, diffLoss = 6.5787, kgLoss = 0.1581
2025-04-09 10:12:48.936700: Training Step 76/115: batchLoss = 4.6756, diffLoss = 7.6727, kgLoss = 0.1800
2025-04-09 10:12:49.690354: Training Step 77/115: batchLoss = 4.1041, diffLoss = 6.7080, kgLoss = 0.1981
2025-04-09 10:12:50.429364: Training Step 78/115: batchLoss = 4.7979, diffLoss = 7.8684, kgLoss = 0.1922
2025-04-09 10:12:51.164334: Training Step 79/115: batchLoss = 4.0958, diffLoss = 6.7148, kgLoss = 0.1674
2025-04-09 10:12:51.902018: Training Step 80/115: batchLoss = 3.9683, diffLoss = 6.5000, kgLoss = 0.1707
2025-04-09 10:12:52.631230: Training Step 81/115: batchLoss = 4.2956, diffLoss = 7.0510, kgLoss = 0.1626
2025-04-09 10:12:53.357533: Training Step 82/115: batchLoss = 4.2450, diffLoss = 6.9573, kgLoss = 0.1767
2025-04-09 10:12:54.101515: Training Step 83/115: batchLoss = 4.1475, diffLoss = 6.8035, kgLoss = 0.1636
2025-04-09 10:12:54.846026: Training Step 84/115: batchLoss = 4.8164, diffLoss = 7.9057, kgLoss = 0.1823
2025-04-09 10:12:55.580079: Training Step 85/115: batchLoss = 4.6477, diffLoss = 7.6235, kgLoss = 0.1840
2025-04-09 10:12:56.316464: Training Step 86/115: batchLoss = 3.9499, diffLoss = 6.4784, kgLoss = 0.1572
2025-04-09 10:12:57.089502: Training Step 87/115: batchLoss = 4.1120, diffLoss = 6.7321, kgLoss = 0.1818
2025-04-09 10:12:57.826588: Training Step 88/115: batchLoss = 4.2546, diffLoss = 6.9818, kgLoss = 0.1639
2025-04-09 10:12:58.569239: Training Step 89/115: batchLoss = 4.1954, diffLoss = 6.8823, kgLoss = 0.1651
2025-04-09 10:12:59.301407: Training Step 90/115: batchLoss = 4.2440, diffLoss = 6.9598, kgLoss = 0.1703
2025-04-09 10:13:00.039443: Training Step 91/115: batchLoss = 4.3074, diffLoss = 7.0644, kgLoss = 0.1720
2025-04-09 10:13:00.791048: Training Step 92/115: batchLoss = 3.8574, diffLoss = 6.3252, kgLoss = 0.1558
2025-04-09 10:13:01.514829: Training Step 93/115: batchLoss = 4.7884, diffLoss = 7.8570, kgLoss = 0.1855
2025-04-09 10:13:02.249759: Training Step 94/115: batchLoss = 4.2647, diffLoss = 6.9937, kgLoss = 0.1713
2025-04-09 10:13:02.983646: Training Step 95/115: batchLoss = 5.2623, diffLoss = 8.6410, kgLoss = 0.1943
2025-04-09 10:13:03.714625: Training Step 96/115: batchLoss = 4.4244, diffLoss = 7.2591, kgLoss = 0.1723
2025-04-09 10:13:04.445143: Training Step 97/115: batchLoss = 5.0800, diffLoss = 8.3333, kgLoss = 0.2001
2025-04-09 10:13:05.172632: Training Step 98/115: batchLoss = 4.4136, diffLoss = 7.2407, kgLoss = 0.1729
2025-04-09 10:13:05.897709: Training Step 99/115: batchLoss = 4.5180, diffLoss = 7.4140, kgLoss = 0.1740
2025-04-09 10:13:06.627944: Training Step 100/115: batchLoss = 4.8533, diffLoss = 7.9699, kgLoss = 0.1784
2025-04-09 10:13:07.368457: Training Step 101/115: batchLoss = 4.4984, diffLoss = 7.3759, kgLoss = 0.1822
2025-04-09 10:13:08.086895: Training Step 102/115: batchLoss = 3.8681, diffLoss = 6.3394, kgLoss = 0.1612
2025-04-09 10:13:08.808201: Training Step 103/115: batchLoss = 4.8446, diffLoss = 7.9468, kgLoss = 0.1912
2025-04-09 10:13:09.539170: Training Step 104/115: batchLoss = 4.0206, diffLoss = 6.5940, kgLoss = 0.1604
2025-04-09 10:13:10.268535: Training Step 105/115: batchLoss = 4.4099, diffLoss = 7.2322, kgLoss = 0.1764
2025-04-09 10:13:10.997597: Training Step 106/115: batchLoss = 3.9824, diffLoss = 6.5246, kgLoss = 0.1692
2025-04-09 10:13:11.735836: Training Step 107/115: batchLoss = 4.2544, diffLoss = 6.9752, kgLoss = 0.1733
2025-04-09 10:13:12.475146: Training Step 108/115: batchLoss = 4.2595, diffLoss = 6.9846, kgLoss = 0.1719
2025-04-09 10:13:13.215774: Training Step 109/115: batchLoss = 4.6552, diffLoss = 7.6417, kgLoss = 0.1753
2025-04-09 10:13:13.952322: Training Step 110/115: batchLoss = 4.3998, diffLoss = 7.2042, kgLoss = 0.1931
2025-04-09 10:13:14.687164: Training Step 111/115: batchLoss = 4.0442, diffLoss = 6.6304, kgLoss = 0.1648
2025-04-09 10:13:15.404605: Training Step 112/115: batchLoss = 3.4462, diffLoss = 5.6533, kgLoss = 0.1355
2025-04-09 10:13:16.045010: Training Step 113/115: batchLoss = 4.6419, diffLoss = 7.6087, kgLoss = 0.1918
2025-04-09 10:13:16.658433: Training Step 114/115: batchLoss = 4.5593, diffLoss = 7.4777, kgLoss = 0.1816
2025-04-09 10:13:16.777939: 
2025-04-09 10:13:16.778648: Epoch 23/1000, Train: epLoss = 1.2579, epDfLoss = 2.0629, epfTransLoss = 0.0000, epKgLoss = 0.0503  
2025-04-09 10:13:17.543684: Steps 0/90: batch_recall = 53.42, batch_ndcg = 41.21 
2025-04-09 10:13:18.266573: Steps 1/90: batch_recall = 58.95, batch_ndcg = 40.44 
2025-04-09 10:13:19.010185: Steps 2/90: batch_recall = 51.35, batch_ndcg = 36.67 
2025-04-09 10:13:19.745449: Steps 3/90: batch_recall = 45.97, batch_ndcg = 32.77 
2025-04-09 10:13:20.474631: Steps 4/90: batch_recall = 48.59, batch_ndcg = 35.64 
2025-04-09 10:13:21.217817: Steps 5/90: batch_recall = 36.82, batch_ndcg = 27.06 
2025-04-09 10:13:21.961116: Steps 6/90: batch_recall = 46.32, batch_ndcg = 33.54 
2025-04-09 10:13:22.716592: Steps 7/90: batch_recall = 43.80, batch_ndcg = 31.98 
2025-04-09 10:13:23.460744: Steps 8/90: batch_recall = 45.95, batch_ndcg = 33.53 
2025-04-09 10:13:24.210086: Steps 9/90: batch_recall = 42.67, batch_ndcg = 33.53 
2025-04-09 10:13:24.959610: Steps 10/90: batch_recall = 37.96, batch_ndcg = 28.27 
2025-04-09 10:13:25.687598: Steps 11/90: batch_recall = 43.94, batch_ndcg = 28.67 
2025-04-09 10:13:26.431149: Steps 12/90: batch_recall = 38.63, batch_ndcg = 27.61 
2025-04-09 10:13:27.175921: Steps 13/90: batch_recall = 37.39, batch_ndcg = 25.90 
2025-04-09 10:13:27.925352: Steps 14/90: batch_recall = 35.25, batch_ndcg = 26.97 
2025-04-09 10:13:28.662896: Steps 15/90: batch_recall = 45.80, batch_ndcg = 29.08 
2025-04-09 10:13:29.397587: Steps 16/90: batch_recall = 37.85, batch_ndcg = 26.57 
2025-04-09 10:13:30.136556: Steps 17/90: batch_recall = 36.41, batch_ndcg = 23.64 
2025-04-09 10:13:30.873582: Steps 18/90: batch_recall = 38.09, batch_ndcg = 25.39 
2025-04-09 10:13:31.613177: Steps 19/90: batch_recall = 34.24, batch_ndcg = 23.59 
2025-04-09 10:13:32.344053: Steps 20/90: batch_recall = 36.63, batch_ndcg = 24.34 
2025-04-09 10:13:33.093316: Steps 21/90: batch_recall = 39.56, batch_ndcg = 28.21 
2025-04-09 10:13:33.818887: Steps 22/90: batch_recall = 42.10, batch_ndcg = 28.56 
2025-04-09 10:13:34.539661: Steps 23/90: batch_recall = 37.96, batch_ndcg = 25.70 
2025-04-09 10:13:35.262140: Steps 24/90: batch_recall = 40.03, batch_ndcg = 26.67 
2025-04-09 10:13:35.972048: Steps 25/90: batch_recall = 37.92, batch_ndcg = 24.53 
2025-04-09 10:13:36.697638: Steps 26/90: batch_recall = 37.68, batch_ndcg = 26.91 
2025-04-09 10:13:37.429990: Steps 27/90: batch_recall = 35.24, batch_ndcg = 24.35 
2025-04-09 10:13:38.155643: Steps 28/90: batch_recall = 36.34, batch_ndcg = 24.42 
2025-04-09 10:13:38.871108: Steps 29/90: batch_recall = 33.93, batch_ndcg = 22.53 
2025-04-09 10:13:39.594236: Steps 30/90: batch_recall = 34.23, batch_ndcg = 21.66 
2025-04-09 10:13:40.318481: Steps 31/90: batch_recall = 33.83, batch_ndcg = 23.46 
2025-04-09 10:13:41.029357: Steps 32/90: batch_recall = 35.57, batch_ndcg = 23.41 
2025-04-09 10:13:41.734896: Steps 33/90: batch_recall = 36.17, batch_ndcg = 22.50 
2025-04-09 10:13:42.455704: Steps 34/90: batch_recall = 36.20, batch_ndcg = 24.62 
2025-04-09 10:13:43.160176: Steps 35/90: batch_recall = 39.52, batch_ndcg = 26.77 
2025-04-09 10:13:43.877653: Steps 36/90: batch_recall = 38.91, batch_ndcg = 25.07 
2025-04-09 10:13:44.605329: Steps 37/90: batch_recall = 32.44, batch_ndcg = 21.27 
2025-04-09 10:13:45.342772: Steps 38/90: batch_recall = 34.85, batch_ndcg = 22.24 
2025-04-09 10:13:46.070119: Steps 39/90: batch_recall = 37.08, batch_ndcg = 22.83 
2025-04-09 10:13:46.799251: Steps 40/90: batch_recall = 33.32, batch_ndcg = 22.76 
2025-04-09 10:13:47.519596: Steps 41/90: batch_recall = 36.18, batch_ndcg = 23.85 
2025-04-09 10:13:48.233178: Steps 42/90: batch_recall = 40.71, batch_ndcg = 28.16 
2025-04-09 10:13:48.936917: Steps 43/90: batch_recall = 35.42, batch_ndcg = 23.33 
2025-04-09 10:13:49.644973: Steps 44/90: batch_recall = 34.74, batch_ndcg = 22.90 
2025-04-09 10:13:50.345147: Steps 45/90: batch_recall = 32.25, batch_ndcg = 21.47 
2025-04-09 10:13:51.059849: Steps 46/90: batch_recall = 33.10, batch_ndcg = 20.64 
2025-04-09 10:13:51.773049: Steps 47/90: batch_recall = 30.54, batch_ndcg = 19.48 
2025-04-09 10:13:52.493872: Steps 48/90: batch_recall = 37.21, batch_ndcg = 22.86 
2025-04-09 10:13:53.239905: Steps 49/90: batch_recall = 31.45, batch_ndcg = 20.82 
2025-04-09 10:13:53.951481: Steps 50/90: batch_recall = 37.79, batch_ndcg = 24.32 
2025-04-09 10:13:54.670907: Steps 51/90: batch_recall = 38.89, batch_ndcg = 25.31 
2025-04-09 10:13:55.368010: Steps 52/90: batch_recall = 35.29, batch_ndcg = 22.47 
2025-04-09 10:13:56.103147: Steps 53/90: batch_recall = 40.80, batch_ndcg = 25.03 
2025-04-09 10:13:56.813840: Steps 54/90: batch_recall = 30.85, batch_ndcg = 19.40 
2025-04-09 10:13:57.520447: Steps 55/90: batch_recall = 37.16, batch_ndcg = 24.21 
2025-04-09 10:13:58.232477: Steps 56/90: batch_recall = 32.07, batch_ndcg = 18.84 
2025-04-09 10:13:58.942764: Steps 57/90: batch_recall = 37.87, batch_ndcg = 24.28 
2025-04-09 10:13:59.653295: Steps 58/90: batch_recall = 38.06, batch_ndcg = 24.23 
2025-04-09 10:14:00.357942: Steps 59/90: batch_recall = 31.71, batch_ndcg = 20.79 
2025-04-09 10:14:01.059239: Steps 60/90: batch_recall = 31.67, batch_ndcg = 19.91 
2025-04-09 10:14:01.774235: Steps 61/90: batch_recall = 39.04, batch_ndcg = 26.85 
2025-04-09 10:14:02.485018: Steps 62/90: batch_recall = 37.87, batch_ndcg = 25.49 
2025-04-09 10:14:03.192809: Steps 63/90: batch_recall = 35.90, batch_ndcg = 22.91 
2025-04-09 10:14:03.895430: Steps 64/90: batch_recall = 35.13, batch_ndcg = 22.17 
2025-04-09 10:14:04.600392: Steps 65/90: batch_recall = 30.29, batch_ndcg = 20.44 
2025-04-09 10:14:05.301851: Steps 66/90: batch_recall = 35.53, batch_ndcg = 22.13 
2025-04-09 10:14:06.022548: Steps 67/90: batch_recall = 33.43, batch_ndcg = 21.11 
2025-04-09 10:14:06.731331: Steps 68/90: batch_recall = 34.66, batch_ndcg = 21.31 
2025-04-09 10:14:07.447798: Steps 69/90: batch_recall = 40.23, batch_ndcg = 24.00 
2025-04-09 10:14:08.176500: Steps 70/90: batch_recall = 36.60, batch_ndcg = 23.10 
2025-04-09 10:14:08.884728: Steps 71/90: batch_recall = 36.06, batch_ndcg = 21.96 
2025-04-09 10:14:09.574188: Steps 72/90: batch_recall = 36.01, batch_ndcg = 23.00 
2025-04-09 10:14:10.283295: Steps 73/90: batch_recall = 35.70, batch_ndcg = 22.20 
2025-04-09 10:14:10.977867: Steps 74/90: batch_recall = 39.40, batch_ndcg = 24.21 
2025-04-09 10:14:11.688480: Steps 75/90: batch_recall = 38.19, batch_ndcg = 23.77 
2025-04-09 10:14:12.393721: Steps 76/90: batch_recall = 40.53, batch_ndcg = 25.65 
2025-04-09 10:14:13.099062: Steps 77/90: batch_recall = 45.25, batch_ndcg = 27.49 
2025-04-09 10:14:13.786395: Steps 78/90: batch_recall = 32.14, batch_ndcg = 20.12 
2025-04-09 10:14:14.496261: Steps 79/90: batch_recall = 40.02, batch_ndcg = 27.25 
2025-04-09 10:14:15.213563: Steps 80/90: batch_recall = 44.54, batch_ndcg = 27.79 
2025-04-09 10:14:15.924474: Steps 81/90: batch_recall = 42.89, batch_ndcg = 25.93 
2025-04-09 10:14:16.623076: Steps 82/90: batch_recall = 39.31, batch_ndcg = 25.46 
2025-04-09 10:14:17.321556: Steps 83/90: batch_recall = 39.67, batch_ndcg = 24.93 
2025-04-09 10:14:18.016893: Steps 84/90: batch_recall = 33.09, batch_ndcg = 20.61 
2025-04-09 10:14:18.728905: Steps 85/90: batch_recall = 39.43, batch_ndcg = 23.06 
2025-04-09 10:14:19.437273: Steps 86/90: batch_recall = 43.52, batch_ndcg = 27.28 
2025-04-09 10:14:20.143178: Steps 87/90: batch_recall = 42.17, batch_ndcg = 28.41 
2025-04-09 10:14:20.847622: Steps 88/90: batch_recall = 47.93, batch_ndcg = 27.36 
2025-04-09 10:14:21.397537: Steps 89/90: batch_recall = 37.87, batch_ndcg = 24.12 
2025-04-09 10:14:21.398174: Epoch 23/1000, Test: Recall = 0.0768, NDCG = 0.0496  

2025-04-09 10:14:22.402854: Training Step 0/115: batchLoss = 4.5197, diffLoss = 7.4053, kgLoss = 0.1912
2025-04-09 10:14:23.145621: Training Step 1/115: batchLoss = 4.4380, diffLoss = 7.2824, kgLoss = 0.1714
2025-04-09 10:14:23.888779: Training Step 2/115: batchLoss = 4.4287, diffLoss = 7.2562, kgLoss = 0.1874
2025-04-09 10:14:24.628791: Training Step 3/115: batchLoss = 4.6610, diffLoss = 7.6406, kgLoss = 0.1915
2025-04-09 10:14:25.358113: Training Step 4/115: batchLoss = 4.7225, diffLoss = 7.7403, kgLoss = 0.1958
2025-04-09 10:14:26.091383: Training Step 5/115: batchLoss = 4.3120, diffLoss = 7.0706, kgLoss = 0.1741
2025-04-09 10:14:26.828344: Training Step 6/115: batchLoss = 4.6948, diffLoss = 7.7047, kgLoss = 0.1801
2025-04-09 10:14:27.557119: Training Step 7/115: batchLoss = 4.2739, diffLoss = 7.0042, kgLoss = 0.1784
2025-04-09 10:14:28.295542: Training Step 8/115: batchLoss = 4.4154, diffLoss = 7.2443, kgLoss = 0.1720
2025-04-09 10:14:29.028934: Training Step 9/115: batchLoss = 4.3613, diffLoss = 7.1469, kgLoss = 0.1830
2025-04-09 10:14:29.768387: Training Step 10/115: batchLoss = 4.7382, diffLoss = 7.7661, kgLoss = 0.1964
2025-04-09 10:14:30.500022: Training Step 11/115: batchLoss = 4.4931, diffLoss = 7.3708, kgLoss = 0.1766
2025-04-09 10:14:31.236496: Training Step 12/115: batchLoss = 4.1933, diffLoss = 6.8779, kgLoss = 0.1663
2025-04-09 10:14:31.966031: Training Step 13/115: batchLoss = 4.0323, diffLoss = 6.6164, kgLoss = 0.1562
2025-04-09 10:14:32.704358: Training Step 14/115: batchLoss = 4.2301, diffLoss = 6.9305, kgLoss = 0.1795
2025-04-09 10:14:33.438304: Training Step 15/115: batchLoss = 4.3489, diffLoss = 7.1330, kgLoss = 0.1727
2025-04-09 10:14:34.174963: Training Step 16/115: batchLoss = 4.2115, diffLoss = 6.8827, kgLoss = 0.2046
2025-04-09 10:14:34.904738: Training Step 17/115: batchLoss = 4.3314, diffLoss = 7.0993, kgLoss = 0.1797
2025-04-09 10:14:35.634023: Training Step 18/115: batchLoss = 4.6060, diffLoss = 7.5574, kgLoss = 0.1789
2025-04-09 10:14:36.357601: Training Step 19/115: batchLoss = 4.1199, diffLoss = 6.7525, kgLoss = 0.1710
2025-04-09 10:14:37.089887: Training Step 20/115: batchLoss = 3.9069, diffLoss = 6.4027, kgLoss = 0.1632
2025-04-09 10:14:37.824455: Training Step 21/115: batchLoss = 4.2903, diffLoss = 7.0340, kgLoss = 0.1748
2025-04-09 10:14:38.563819: Training Step 22/115: batchLoss = 4.5891, diffLoss = 7.5251, kgLoss = 0.1851
2025-04-09 10:14:39.300680: Training Step 23/115: batchLoss = 3.8109, diffLoss = 6.2461, kgLoss = 0.1581
2025-04-09 10:14:40.040251: Training Step 24/115: batchLoss = 4.1151, diffLoss = 6.7467, kgLoss = 0.1678
2025-04-09 10:14:40.777691: Training Step 25/115: batchLoss = 3.8569, diffLoss = 6.3240, kgLoss = 0.1561
2025-04-09 10:14:41.510368: Training Step 26/115: batchLoss = 4.4015, diffLoss = 7.2097, kgLoss = 0.1892
2025-04-09 10:14:42.244824: Training Step 27/115: batchLoss = 4.3780, diffLoss = 7.1702, kgLoss = 0.1897
2025-04-09 10:14:42.988196: Training Step 28/115: batchLoss = 4.6216, diffLoss = 7.5758, kgLoss = 0.1904
2025-04-09 10:14:43.728434: Training Step 29/115: batchLoss = 4.1827, diffLoss = 6.8521, kgLoss = 0.1786
2025-04-09 10:14:44.463040: Training Step 30/115: batchLoss = 4.0188, diffLoss = 6.5937, kgLoss = 0.1565
2025-04-09 10:14:45.202515: Training Step 31/115: batchLoss = 4.9958, diffLoss = 8.2002, kgLoss = 0.1892
2025-04-09 10:14:45.941477: Training Step 32/115: batchLoss = 4.1256, diffLoss = 6.7651, kgLoss = 0.1663
2025-04-09 10:14:46.672393: Training Step 33/115: batchLoss = 4.2790, diffLoss = 7.0194, kgLoss = 0.1684
2025-04-09 10:14:47.399704: Training Step 34/115: batchLoss = 4.7366, diffLoss = 7.7646, kgLoss = 0.1947
2025-04-09 10:14:48.142048: Training Step 35/115: batchLoss = 3.9263, diffLoss = 6.4383, kgLoss = 0.1585
2025-04-09 10:14:48.874073: Training Step 36/115: batchLoss = 4.3680, diffLoss = 7.1670, kgLoss = 0.1695
2025-04-09 10:14:49.620159: Training Step 37/115: batchLoss = 4.2539, diffLoss = 6.9717, kgLoss = 0.1772
2025-04-09 10:14:50.362310: Training Step 38/115: batchLoss = 4.4707, diffLoss = 7.3279, kgLoss = 0.1851
2025-04-09 10:14:51.105121: Training Step 39/115: batchLoss = 4.1486, diffLoss = 6.8031, kgLoss = 0.1670
2025-04-09 10:14:51.840603: Training Step 40/115: batchLoss = 4.6755, diffLoss = 7.6703, kgLoss = 0.1833
2025-04-09 10:14:52.567962: Training Step 41/115: batchLoss = 3.9133, diffLoss = 6.4119, kgLoss = 0.1654
2025-04-09 10:14:53.300019: Training Step 42/115: batchLoss = 4.0773, diffLoss = 6.6835, kgLoss = 0.1679
2025-04-09 10:14:54.047598: Training Step 43/115: batchLoss = 4.2524, diffLoss = 6.9723, kgLoss = 0.1724
2025-04-09 10:14:54.802500: Training Step 44/115: batchLoss = 4.3589, diffLoss = 7.1492, kgLoss = 0.1736
2025-04-09 10:14:55.555574: Training Step 45/115: batchLoss = 4.6373, diffLoss = 7.6085, kgLoss = 0.1805
2025-04-09 10:14:56.293355: Training Step 46/115: batchLoss = 3.6922, diffLoss = 6.0530, kgLoss = 0.1510
2025-04-09 10:14:57.026804: Training Step 47/115: batchLoss = 3.9845, diffLoss = 6.5327, kgLoss = 0.1621
2025-04-09 10:14:57.770054: Training Step 48/115: batchLoss = 3.8991, diffLoss = 6.4018, kgLoss = 0.1452
2025-04-09 10:14:58.510784: Training Step 49/115: batchLoss = 3.8297, diffLoss = 6.2775, kgLoss = 0.1579
2025-04-09 10:14:59.238463: Training Step 50/115: batchLoss = 4.4984, diffLoss = 7.3801, kgLoss = 0.1758
2025-04-09 10:14:59.980942: Training Step 51/115: batchLoss = 4.6047, diffLoss = 7.5542, kgLoss = 0.1805
2025-04-09 10:15:00.726720: Training Step 52/115: batchLoss = 4.1940, diffLoss = 6.8760, kgLoss = 0.1711
2025-04-09 10:15:01.453090: Training Step 53/115: batchLoss = 3.7035, diffLoss = 6.0652, kgLoss = 0.1610
2025-04-09 10:15:02.186340: Training Step 54/115: batchLoss = 4.4606, diffLoss = 7.3123, kgLoss = 0.1830
2025-04-09 10:15:02.915476: Training Step 55/115: batchLoss = 3.7167, diffLoss = 6.0892, kgLoss = 0.1579
2025-04-09 10:15:03.648795: Training Step 56/115: batchLoss = 4.4904, diffLoss = 7.3613, kgLoss = 0.1841
2025-04-09 10:15:04.374305: Training Step 57/115: batchLoss = 4.4379, diffLoss = 7.2778, kgLoss = 0.1779
2025-04-09 10:15:05.107988: Training Step 58/115: batchLoss = 4.7315, diffLoss = 7.7606, kgLoss = 0.1879
2025-04-09 10:15:05.835842: Training Step 59/115: batchLoss = 4.2555, diffLoss = 6.9770, kgLoss = 0.1732
2025-04-09 10:15:06.583598: Training Step 60/115: batchLoss = 4.2669, diffLoss = 6.9931, kgLoss = 0.1776
2025-04-09 10:15:07.330997: Training Step 61/115: batchLoss = 4.5309, diffLoss = 7.4291, kgLoss = 0.1836
2025-04-09 10:15:08.079623: Training Step 62/115: batchLoss = 4.3235, diffLoss = 7.0896, kgLoss = 0.1742
2025-04-09 10:15:08.822282: Training Step 63/115: batchLoss = 4.0499, diffLoss = 6.6388, kgLoss = 0.1666
2025-04-09 10:15:09.572324: Training Step 64/115: batchLoss = 4.3054, diffLoss = 7.0626, kgLoss = 0.1697
2025-04-09 10:15:10.308534: Training Step 65/115: batchLoss = 4.6159, diffLoss = 7.5705, kgLoss = 0.1840
2025-04-09 10:15:11.050118: Training Step 66/115: batchLoss = 3.9472, diffLoss = 6.4606, kgLoss = 0.1772
2025-04-09 10:15:11.785953: Training Step 67/115: batchLoss = 4.7951, diffLoss = 7.8725, kgLoss = 0.1791
2025-04-09 10:15:12.528961: Training Step 68/115: batchLoss = 4.1229, diffLoss = 6.7622, kgLoss = 0.1638
2025-04-09 10:15:13.260985: Training Step 69/115: batchLoss = 4.4322, diffLoss = 7.2570, kgLoss = 0.1951
2025-04-09 10:15:13.993736: Training Step 70/115: batchLoss = 3.8828, diffLoss = 6.3652, kgLoss = 0.1591
2025-04-09 10:15:14.735877: Training Step 71/115: batchLoss = 4.3058, diffLoss = 7.0466, kgLoss = 0.1946
2025-04-09 10:15:15.464848: Training Step 72/115: batchLoss = 3.9608, diffLoss = 6.4985, kgLoss = 0.1542
2025-04-09 10:15:16.192552: Training Step 73/115: batchLoss = 4.5206, diffLoss = 7.4181, kgLoss = 0.1743
2025-04-09 10:15:16.937679: Training Step 74/115: batchLoss = 3.8001, diffLoss = 6.2344, kgLoss = 0.1487
2025-04-09 10:15:17.689112: Training Step 75/115: batchLoss = 4.2128, diffLoss = 6.9121, kgLoss = 0.1638
2025-04-09 10:15:18.436500: Training Step 76/115: batchLoss = 4.0584, diffLoss = 6.6573, kgLoss = 0.1600
2025-04-09 10:15:19.174805: Training Step 77/115: batchLoss = 5.0334, diffLoss = 8.2620, kgLoss = 0.1905
2025-04-09 10:15:19.902059: Training Step 78/115: batchLoss = 4.2467, diffLoss = 6.9696, kgLoss = 0.1623
2025-04-09 10:15:20.637384: Training Step 79/115: batchLoss = 4.4023, diffLoss = 7.2210, kgLoss = 0.1742
2025-04-09 10:15:21.383915: Training Step 80/115: batchLoss = 4.6525, diffLoss = 7.6264, kgLoss = 0.1916
2025-04-09 10:15:22.097606: Training Step 81/115: batchLoss = 4.2373, diffLoss = 6.9501, kgLoss = 0.1682
2025-04-09 10:15:22.827545: Training Step 82/115: batchLoss = 4.6609, diffLoss = 7.6425, kgLoss = 0.1884
2025-04-09 10:15:23.558528: Training Step 83/115: batchLoss = 4.3557, diffLoss = 7.1492, kgLoss = 0.1655
2025-04-09 10:15:24.284383: Training Step 84/115: batchLoss = 3.9276, diffLoss = 6.4382, kgLoss = 0.1618
2025-04-09 10:15:25.018747: Training Step 85/115: batchLoss = 4.6869, diffLoss = 7.6871, kgLoss = 0.1867
2025-04-09 10:15:25.754691: Training Step 86/115: batchLoss = 4.3441, diffLoss = 7.1195, kgLoss = 0.1809
2025-04-09 10:15:26.485658: Training Step 87/115: batchLoss = 3.9878, diffLoss = 6.5418, kgLoss = 0.1567
2025-04-09 10:15:27.218505: Training Step 88/115: batchLoss = 4.4108, diffLoss = 7.2355, kgLoss = 0.1737
2025-04-09 10:15:27.976746: Training Step 89/115: batchLoss = 4.6771, diffLoss = 7.6746, kgLoss = 0.1809
2025-04-09 10:15:28.729471: Training Step 90/115: batchLoss = 4.7251, diffLoss = 7.7467, kgLoss = 0.1926
2025-04-09 10:15:29.558837: Training Step 91/115: batchLoss = 4.4442, diffLoss = 7.2871, kgLoss = 0.1798
2025-04-09 10:15:30.292491: Training Step 92/115: batchLoss = 3.9115, diffLoss = 6.4084, kgLoss = 0.1660
2025-04-09 10:15:31.025063: Training Step 93/115: batchLoss = 4.9522, diffLoss = 8.1297, kgLoss = 0.1859
2025-04-09 10:15:31.759783: Training Step 94/115: batchLoss = 4.5450, diffLoss = 7.4590, kgLoss = 0.1739
2025-04-09 10:15:32.482635: Training Step 95/115: batchLoss = 4.5586, diffLoss = 7.4817, kgLoss = 0.1739
2025-04-09 10:15:33.236506: Training Step 96/115: batchLoss = 4.6425, diffLoss = 7.6206, kgLoss = 0.1753
2025-04-09 10:15:33.973370: Training Step 97/115: batchLoss = 4.6552, diffLoss = 7.6339, kgLoss = 0.1872
2025-04-09 10:15:34.697368: Training Step 98/115: batchLoss = 3.9577, diffLoss = 6.4909, kgLoss = 0.1578
2025-04-09 10:15:35.455962: Training Step 99/115: batchLoss = 4.2317, diffLoss = 6.9355, kgLoss = 0.1760
2025-04-09 10:15:36.183824: Training Step 100/115: batchLoss = 3.8761, diffLoss = 6.3605, kgLoss = 0.1495
2025-04-09 10:15:36.914020: Training Step 101/115: batchLoss = 3.8719, diffLoss = 6.3500, kgLoss = 0.1547
2025-04-09 10:15:37.646255: Training Step 102/115: batchLoss = 4.0228, diffLoss = 6.5967, kgLoss = 0.1619
2025-04-09 10:15:38.373285: Training Step 103/115: batchLoss = 4.4013, diffLoss = 7.2227, kgLoss = 0.1691
2025-04-09 10:15:39.109079: Training Step 104/115: batchLoss = 4.5981, diffLoss = 7.5439, kgLoss = 0.1792
2025-04-09 10:15:39.842768: Training Step 105/115: batchLoss = 4.5744, diffLoss = 7.5025, kgLoss = 0.1822
2025-04-09 10:15:40.580533: Training Step 106/115: batchLoss = 3.9869, diffLoss = 6.5322, kgLoss = 0.1689
2025-04-09 10:15:41.302701: Training Step 107/115: batchLoss = 5.1052, diffLoss = 8.3726, kgLoss = 0.2041
2025-04-09 10:15:42.030457: Training Step 108/115: batchLoss = 4.1046, diffLoss = 6.7306, kgLoss = 0.1655
2025-04-09 10:15:42.758798: Training Step 109/115: batchLoss = 4.4808, diffLoss = 7.3496, kgLoss = 0.1777
2025-04-09 10:15:43.502169: Training Step 110/115: batchLoss = 4.3296, diffLoss = 7.1054, kgLoss = 0.1659
2025-04-09 10:15:44.231297: Training Step 111/115: batchLoss = 4.3171, diffLoss = 7.0827, kgLoss = 0.1687
2025-04-09 10:15:44.942558: Training Step 112/115: batchLoss = 4.0834, diffLoss = 6.6927, kgLoss = 0.1695
2025-04-09 10:15:45.593138: Training Step 113/115: batchLoss = 5.3906, diffLoss = 8.8427, kgLoss = 0.2124
2025-04-09 10:15:46.225412: Training Step 114/115: batchLoss = 4.1147, diffLoss = 6.7334, kgLoss = 0.1867
2025-04-09 10:15:46.340749: 
2025-04-09 10:15:46.341384: Epoch 24/1000, Train: epLoss = 1.2441, epDfLoss = 2.0401, epfTransLoss = 0.0000, epKgLoss = 0.0502  
2025-04-09 10:15:47.091408: Steps 0/90: batch_recall = 54.44, batch_ndcg = 41.85 
2025-04-09 10:15:47.819439: Steps 1/90: batch_recall = 60.57, batch_ndcg = 41.57 
2025-04-09 10:15:48.562975: Steps 2/90: batch_recall = 51.59, batch_ndcg = 36.73 
2025-04-09 10:15:49.295152: Steps 3/90: batch_recall = 46.21, batch_ndcg = 32.98 
2025-04-09 10:15:50.031335: Steps 4/90: batch_recall = 48.61, batch_ndcg = 35.74 
2025-04-09 10:15:50.763805: Steps 5/90: batch_recall = 36.82, batch_ndcg = 27.25 
2025-04-09 10:15:51.491129: Steps 6/90: batch_recall = 47.11, batch_ndcg = 33.41 
2025-04-09 10:15:52.234735: Steps 7/90: batch_recall = 43.91, batch_ndcg = 31.90 
2025-04-09 10:15:52.965902: Steps 8/90: batch_recall = 46.89, batch_ndcg = 33.70 
2025-04-09 10:15:53.720345: Steps 9/90: batch_recall = 42.73, batch_ndcg = 33.56 
2025-04-09 10:15:54.467862: Steps 10/90: batch_recall = 38.50, batch_ndcg = 28.53 
2025-04-09 10:15:55.194207: Steps 11/90: batch_recall = 44.93, batch_ndcg = 29.25 
2025-04-09 10:15:55.923414: Steps 12/90: batch_recall = 38.63, batch_ndcg = 27.60 
2025-04-09 10:15:56.661606: Steps 13/90: batch_recall = 37.55, batch_ndcg = 26.33 
2025-04-09 10:15:57.394946: Steps 14/90: batch_recall = 35.36, batch_ndcg = 27.17 
2025-04-09 10:15:58.115355: Steps 15/90: batch_recall = 46.69, batch_ndcg = 29.60 
2025-04-09 10:15:58.843611: Steps 16/90: batch_recall = 37.19, batch_ndcg = 26.48 
2025-04-09 10:15:59.570297: Steps 17/90: batch_recall = 36.11, batch_ndcg = 23.37 
2025-04-09 10:16:00.283170: Steps 18/90: batch_recall = 38.58, batch_ndcg = 25.59 
2025-04-09 10:16:01.021827: Steps 19/90: batch_recall = 35.07, batch_ndcg = 24.42 
2025-04-09 10:16:01.747772: Steps 20/90: batch_recall = 37.96, batch_ndcg = 25.18 
2025-04-09 10:16:02.464336: Steps 21/90: batch_recall = 39.69, batch_ndcg = 28.50 
2025-04-09 10:16:03.178397: Steps 22/90: batch_recall = 41.70, batch_ndcg = 28.50 
2025-04-09 10:16:03.889212: Steps 23/90: batch_recall = 38.09, batch_ndcg = 25.48 
2025-04-09 10:16:04.603503: Steps 24/90: batch_recall = 41.08, batch_ndcg = 27.20 
2025-04-09 10:16:05.414421: Steps 25/90: batch_recall = 37.48, batch_ndcg = 24.65 
2025-04-09 10:16:06.134601: Steps 26/90: batch_recall = 37.53, batch_ndcg = 26.95 
2025-04-09 10:16:06.859195: Steps 27/90: batch_recall = 35.44, batch_ndcg = 24.08 
2025-04-09 10:16:07.593170: Steps 28/90: batch_recall = 36.28, batch_ndcg = 24.26 
2025-04-09 10:16:08.327738: Steps 29/90: batch_recall = 34.50, batch_ndcg = 23.12 
2025-04-09 10:16:09.060074: Steps 30/90: batch_recall = 34.67, batch_ndcg = 21.85 
2025-04-09 10:16:09.801454: Steps 31/90: batch_recall = 34.01, batch_ndcg = 23.52 
2025-04-09 10:16:10.541382: Steps 32/90: batch_recall = 35.65, batch_ndcg = 23.34 
2025-04-09 10:16:11.258206: Steps 33/90: batch_recall = 37.09, batch_ndcg = 22.71 
2025-04-09 10:16:11.977416: Steps 34/90: batch_recall = 36.38, batch_ndcg = 24.78 
2025-04-09 10:16:12.697148: Steps 35/90: batch_recall = 39.04, batch_ndcg = 26.68 
2025-04-09 10:16:13.424162: Steps 36/90: batch_recall = 38.23, batch_ndcg = 24.60 
2025-04-09 10:16:14.141975: Steps 37/90: batch_recall = 33.41, batch_ndcg = 21.57 
2025-04-09 10:16:14.867856: Steps 38/90: batch_recall = 33.64, batch_ndcg = 21.87 
2025-04-09 10:16:15.582666: Steps 39/90: batch_recall = 38.61, batch_ndcg = 23.51 
2025-04-09 10:16:16.304659: Steps 40/90: batch_recall = 33.16, batch_ndcg = 22.73 
2025-04-09 10:16:17.041337: Steps 41/90: batch_recall = 35.64, batch_ndcg = 23.52 
2025-04-09 10:16:17.753977: Steps 42/90: batch_recall = 40.52, batch_ndcg = 28.34 
2025-04-09 10:16:18.467468: Steps 43/90: batch_recall = 35.62, batch_ndcg = 23.42 
2025-04-09 10:16:19.189852: Steps 44/90: batch_recall = 33.78, batch_ndcg = 22.48 
2025-04-09 10:16:19.896740: Steps 45/90: batch_recall = 32.75, batch_ndcg = 21.87 
2025-04-09 10:16:20.606909: Steps 46/90: batch_recall = 33.78, batch_ndcg = 20.91 
2025-04-09 10:16:21.314744: Steps 47/90: batch_recall = 32.15, batch_ndcg = 20.30 
2025-04-09 10:16:22.019501: Steps 48/90: batch_recall = 36.90, batch_ndcg = 22.88 
2025-04-09 10:16:22.729488: Steps 49/90: batch_recall = 31.61, batch_ndcg = 20.85 
2025-04-09 10:16:23.441982: Steps 50/90: batch_recall = 37.24, batch_ndcg = 24.51 
2025-04-09 10:16:24.168925: Steps 51/90: batch_recall = 38.90, batch_ndcg = 25.45 
2025-04-09 10:16:24.893891: Steps 52/90: batch_recall = 35.11, batch_ndcg = 22.59 
2025-04-09 10:16:25.607017: Steps 53/90: batch_recall = 40.10, batch_ndcg = 25.28 
2025-04-09 10:16:26.313126: Steps 54/90: batch_recall = 31.30, batch_ndcg = 19.69 
2025-04-09 10:16:27.016753: Steps 55/90: batch_recall = 37.08, batch_ndcg = 24.12 
2025-04-09 10:16:27.726183: Steps 56/90: batch_recall = 33.02, batch_ndcg = 19.24 
2025-04-09 10:16:28.444891: Steps 57/90: batch_recall = 37.61, batch_ndcg = 24.05 
2025-04-09 10:16:29.158393: Steps 58/90: batch_recall = 38.15, batch_ndcg = 23.90 
2025-04-09 10:16:29.874683: Steps 59/90: batch_recall = 32.03, batch_ndcg = 21.16 
2025-04-09 10:16:30.608266: Steps 60/90: batch_recall = 32.54, batch_ndcg = 20.66 
2025-04-09 10:16:31.312185: Steps 61/90: batch_recall = 38.90, batch_ndcg = 26.66 
2025-04-09 10:16:32.001276: Steps 62/90: batch_recall = 37.90, batch_ndcg = 25.57 
2025-04-09 10:16:32.706863: Steps 63/90: batch_recall = 36.39, batch_ndcg = 23.20 
2025-04-09 10:16:33.411050: Steps 64/90: batch_recall = 36.73, batch_ndcg = 22.83 
2025-04-09 10:16:34.115963: Steps 65/90: batch_recall = 30.19, batch_ndcg = 19.79 
2025-04-09 10:16:34.817201: Steps 66/90: batch_recall = 37.11, batch_ndcg = 22.98 
2025-04-09 10:16:35.524239: Steps 67/90: batch_recall = 34.16, batch_ndcg = 21.25 
2025-04-09 10:16:36.226235: Steps 68/90: batch_recall = 35.11, batch_ndcg = 21.75 
2025-04-09 10:16:36.930115: Steps 69/90: batch_recall = 41.57, batch_ndcg = 24.43 
2025-04-09 10:16:37.646391: Steps 70/90: batch_recall = 37.54, batch_ndcg = 23.74 
2025-04-09 10:16:38.372065: Steps 71/90: batch_recall = 37.49, batch_ndcg = 22.79 
2025-04-09 10:16:39.093171: Steps 72/90: batch_recall = 35.68, batch_ndcg = 22.87 
2025-04-09 10:16:39.798744: Steps 73/90: batch_recall = 35.19, batch_ndcg = 21.98 
2025-04-09 10:16:40.502311: Steps 74/90: batch_recall = 40.06, batch_ndcg = 24.68 
2025-04-09 10:16:41.198628: Steps 75/90: batch_recall = 38.10, batch_ndcg = 23.90 
2025-04-09 10:16:41.908246: Steps 76/90: batch_recall = 40.90, batch_ndcg = 25.80 
2025-04-09 10:16:42.619713: Steps 77/90: batch_recall = 43.32, batch_ndcg = 26.94 
2025-04-09 10:16:43.320184: Steps 78/90: batch_recall = 32.39, batch_ndcg = 20.34 
2025-04-09 10:16:44.016756: Steps 79/90: batch_recall = 40.45, batch_ndcg = 27.01 
2025-04-09 10:16:44.719565: Steps 80/90: batch_recall = 44.12, batch_ndcg = 27.65 
2025-04-09 10:16:45.413343: Steps 81/90: batch_recall = 42.90, batch_ndcg = 26.59 
2025-04-09 10:16:46.105607: Steps 82/90: batch_recall = 39.91, batch_ndcg = 25.72 
2025-04-09 10:16:46.795234: Steps 83/90: batch_recall = 40.56, batch_ndcg = 25.46 
2025-04-09 10:16:47.479704: Steps 84/90: batch_recall = 32.98, batch_ndcg = 20.67 
2025-04-09 10:16:48.189862: Steps 85/90: batch_recall = 40.64, batch_ndcg = 23.15 
2025-04-09 10:16:48.904409: Steps 86/90: batch_recall = 45.07, batch_ndcg = 27.73 
2025-04-09 10:16:49.601875: Steps 87/90: batch_recall = 40.98, batch_ndcg = 28.15 
2025-04-09 10:16:50.299356: Steps 88/90: batch_recall = 49.05, batch_ndcg = 27.94 
2025-04-09 10:16:50.841708: Steps 89/90: batch_recall = 38.77, batch_ndcg = 24.54 
2025-04-09 10:16:50.842425: Epoch 24/1000, Test: Recall = 0.0769, NDCG = 0.0499  

2025-04-09 10:16:51.848990: Training Step 0/115: batchLoss = 4.1133, diffLoss = 6.7422, kgLoss = 0.1700
2025-04-09 10:16:52.583903: Training Step 1/115: batchLoss = 4.1398, diffLoss = 6.7879, kgLoss = 0.1676
2025-04-09 10:16:53.318229: Training Step 2/115: batchLoss = 3.9109, diffLoss = 6.4136, kgLoss = 0.1568
2025-04-09 10:16:54.052080: Training Step 3/115: batchLoss = 4.0965, diffLoss = 6.7172, kgLoss = 0.1655
2025-04-09 10:16:54.785994: Training Step 4/115: batchLoss = 4.1613, diffLoss = 6.8257, kgLoss = 0.1646
2025-04-09 10:16:55.511141: Training Step 5/115: batchLoss = 4.2307, diffLoss = 6.9432, kgLoss = 0.1620
2025-04-09 10:16:56.252765: Training Step 6/115: batchLoss = 4.8570, diffLoss = 7.9618, kgLoss = 0.1997
2025-04-09 10:16:56.982246: Training Step 7/115: batchLoss = 4.5898, diffLoss = 7.5319, kgLoss = 0.1767
2025-04-09 10:16:57.707930: Training Step 8/115: batchLoss = 4.2089, diffLoss = 6.9013, kgLoss = 0.1704
2025-04-09 10:16:58.428871: Training Step 9/115: batchLoss = 4.6990, diffLoss = 7.7032, kgLoss = 0.1926
2025-04-09 10:16:59.156775: Training Step 10/115: batchLoss = 4.1993, diffLoss = 6.8745, kgLoss = 0.1865
2025-04-09 10:16:59.875895: Training Step 11/115: batchLoss = 4.1326, diffLoss = 6.7779, kgLoss = 0.1647
2025-04-09 10:17:00.609121: Training Step 12/115: batchLoss = 4.2744, diffLoss = 7.0133, kgLoss = 0.1660
2025-04-09 10:17:01.434698: Training Step 13/115: batchLoss = 4.7002, diffLoss = 7.7134, kgLoss = 0.1803
2025-04-09 10:17:02.175315: Training Step 14/115: batchLoss = 4.2631, diffLoss = 6.9892, kgLoss = 0.1740
2025-04-09 10:17:02.903777: Training Step 15/115: batchLoss = 4.5043, diffLoss = 7.3891, kgLoss = 0.1770
2025-04-09 10:17:03.725852: Training Step 16/115: batchLoss = 4.1463, diffLoss = 6.8009, kgLoss = 0.1644
2025-04-09 10:17:04.471675: Training Step 17/115: batchLoss = 5.0950, diffLoss = 8.3564, kgLoss = 0.2028
2025-04-09 10:17:05.212589: Training Step 18/115: batchLoss = 4.4468, diffLoss = 7.2917, kgLoss = 0.1793
2025-04-09 10:17:05.956151: Training Step 19/115: batchLoss = 4.4737, diffLoss = 7.3375, kgLoss = 0.1780
2025-04-09 10:17:06.687476: Training Step 20/115: batchLoss = 4.1605, diffLoss = 6.8265, kgLoss = 0.1614
2025-04-09 10:17:07.421978: Training Step 21/115: batchLoss = 4.7348, diffLoss = 7.7631, kgLoss = 0.1922
2025-04-09 10:17:08.152383: Training Step 22/115: batchLoss = 4.6747, diffLoss = 7.6682, kgLoss = 0.1846
2025-04-09 10:17:08.883671: Training Step 23/115: batchLoss = 3.9894, diffLoss = 6.5396, kgLoss = 0.1642
2025-04-09 10:17:09.618151: Training Step 24/115: batchLoss = 4.1311, diffLoss = 6.7765, kgLoss = 0.1631
2025-04-09 10:17:10.349330: Training Step 25/115: batchLoss = 4.4631, diffLoss = 7.3202, kgLoss = 0.1776
2025-04-09 10:17:11.067542: Training Step 26/115: batchLoss = 4.4932, diffLoss = 7.3662, kgLoss = 0.1835
2025-04-09 10:17:11.792043: Training Step 27/115: batchLoss = 4.2393, diffLoss = 6.9535, kgLoss = 0.1680
2025-04-09 10:17:12.528253: Training Step 28/115: batchLoss = 4.2482, diffLoss = 6.9571, kgLoss = 0.1848
2025-04-09 10:17:13.251682: Training Step 29/115: batchLoss = 5.0153, diffLoss = 8.2189, kgLoss = 0.2100
2025-04-09 10:17:13.983144: Training Step 30/115: batchLoss = 4.4699, diffLoss = 7.3363, kgLoss = 0.1703
2025-04-09 10:17:14.713684: Training Step 31/115: batchLoss = 4.4374, diffLoss = 7.2806, kgLoss = 0.1727
2025-04-09 10:17:15.456002: Training Step 32/115: batchLoss = 3.9560, diffLoss = 6.4829, kgLoss = 0.1658
2025-04-09 10:17:16.187804: Training Step 33/115: batchLoss = 3.9927, diffLoss = 6.5451, kgLoss = 0.1640
2025-04-09 10:17:16.923032: Training Step 34/115: batchLoss = 4.5305, diffLoss = 7.4287, kgLoss = 0.1834
2025-04-09 10:17:17.652243: Training Step 35/115: batchLoss = 4.1968, diffLoss = 6.8794, kgLoss = 0.1730
2025-04-09 10:17:18.378625: Training Step 36/115: batchLoss = 4.1355, diffLoss = 6.7709, kgLoss = 0.1823
2025-04-09 10:17:19.110019: Training Step 37/115: batchLoss = 4.8590, diffLoss = 7.9777, kgLoss = 0.1808
2025-04-09 10:17:19.843217: Training Step 38/115: batchLoss = 4.5677, diffLoss = 7.4989, kgLoss = 0.1709
2025-04-09 10:17:20.576206: Training Step 39/115: batchLoss = 4.3377, diffLoss = 7.1116, kgLoss = 0.1769
2025-04-09 10:17:21.313568: Training Step 40/115: batchLoss = 4.8909, diffLoss = 8.0247, kgLoss = 0.1901
2025-04-09 10:17:22.053541: Training Step 41/115: batchLoss = 4.5999, diffLoss = 7.5436, kgLoss = 0.1844
2025-04-09 10:17:22.785333: Training Step 42/115: batchLoss = 4.4485, diffLoss = 7.2971, kgLoss = 0.1755
2025-04-09 10:17:23.517418: Training Step 43/115: batchLoss = 4.5541, diffLoss = 7.4722, kgLoss = 0.1769
2025-04-09 10:17:24.255336: Training Step 44/115: batchLoss = 4.8123, diffLoss = 7.8938, kgLoss = 0.1899
2025-04-09 10:17:24.988983: Training Step 45/115: batchLoss = 4.5184, diffLoss = 7.4040, kgLoss = 0.1900
2025-04-09 10:17:25.731144: Training Step 46/115: batchLoss = 4.5009, diffLoss = 7.3763, kgLoss = 0.1879
2025-04-09 10:17:26.477966: Training Step 47/115: batchLoss = 4.1565, diffLoss = 6.8213, kgLoss = 0.1593
2025-04-09 10:17:27.225963: Training Step 48/115: batchLoss = 4.9739, diffLoss = 8.1623, kgLoss = 0.1913
2025-04-09 10:17:27.981087: Training Step 49/115: batchLoss = 3.9564, diffLoss = 6.4809, kgLoss = 0.1695
2025-04-09 10:17:28.717418: Training Step 50/115: batchLoss = 3.9795, diffLoss = 6.5255, kgLoss = 0.1604
2025-04-09 10:17:29.447684: Training Step 51/115: batchLoss = 4.8495, diffLoss = 7.9534, kgLoss = 0.1938
2025-04-09 10:17:30.181713: Training Step 52/115: batchLoss = 4.5777, diffLoss = 7.5058, kgLoss = 0.1856
2025-04-09 10:17:30.916867: Training Step 53/115: batchLoss = 3.8098, diffLoss = 6.2463, kgLoss = 0.1551
2025-04-09 10:17:31.657014: Training Step 54/115: batchLoss = 4.4495, diffLoss = 7.2985, kgLoss = 0.1759
2025-04-09 10:17:32.387090: Training Step 55/115: batchLoss = 4.2691, diffLoss = 6.9979, kgLoss = 0.1759
2025-04-09 10:17:33.119537: Training Step 56/115: batchLoss = 4.3540, diffLoss = 7.1402, kgLoss = 0.1746
2025-04-09 10:17:33.855375: Training Step 57/115: batchLoss = 4.5455, diffLoss = 7.4481, kgLoss = 0.1915
2025-04-09 10:17:34.600013: Training Step 58/115: batchLoss = 4.3190, diffLoss = 7.0850, kgLoss = 0.1698
2025-04-09 10:17:35.334924: Training Step 59/115: batchLoss = 4.6372, diffLoss = 7.5998, kgLoss = 0.1932
2025-04-09 10:17:36.084314: Training Step 60/115: batchLoss = 4.3604, diffLoss = 7.1479, kgLoss = 0.1792
2025-04-09 10:17:36.828410: Training Step 61/115: batchLoss = 4.0583, diffLoss = 6.6478, kgLoss = 0.1741
2025-04-09 10:17:37.584367: Training Step 62/115: batchLoss = 4.4719, diffLoss = 7.3288, kgLoss = 0.1865
2025-04-09 10:17:38.324554: Training Step 63/115: batchLoss = 4.5470, diffLoss = 7.4564, kgLoss = 0.1827
2025-04-09 10:17:39.057811: Training Step 64/115: batchLoss = 4.7019, diffLoss = 7.7156, kgLoss = 0.1814
2025-04-09 10:17:39.800647: Training Step 65/115: batchLoss = 4.1718, diffLoss = 6.8412, kgLoss = 0.1678
2025-04-09 10:17:40.523798: Training Step 66/115: batchLoss = 4.2012, diffLoss = 6.8904, kgLoss = 0.1674
2025-04-09 10:17:41.257599: Training Step 67/115: batchLoss = 4.2074, diffLoss = 6.9024, kgLoss = 0.1650
2025-04-09 10:17:41.994105: Training Step 68/115: batchLoss = 4.5102, diffLoss = 7.4000, kgLoss = 0.1756
2025-04-09 10:17:42.743019: Training Step 69/115: batchLoss = 4.2089, diffLoss = 6.9019, kgLoss = 0.1695
2025-04-09 10:17:43.467689: Training Step 70/115: batchLoss = 4.2904, diffLoss = 7.0209, kgLoss = 0.1946
2025-04-09 10:17:44.198205: Training Step 71/115: batchLoss = 4.2277, diffLoss = 6.9366, kgLoss = 0.1644
2025-04-09 10:17:44.931870: Training Step 72/115: batchLoss = 3.6508, diffLoss = 5.9877, kgLoss = 0.1455
2025-04-09 10:17:45.659965: Training Step 73/115: batchLoss = 4.1155, diffLoss = 6.7506, kgLoss = 0.1628
2025-04-09 10:17:46.388854: Training Step 74/115: batchLoss = 4.4300, diffLoss = 7.2656, kgLoss = 0.1765
2025-04-09 10:17:47.112760: Training Step 75/115: batchLoss = 4.1990, diffLoss = 6.8823, kgLoss = 0.1740
2025-04-09 10:17:47.852267: Training Step 76/115: batchLoss = 4.2964, diffLoss = 7.0486, kgLoss = 0.1681
2025-04-09 10:17:48.580830: Training Step 77/115: batchLoss = 4.1515, diffLoss = 6.8087, kgLoss = 0.1657
2025-04-09 10:17:49.323378: Training Step 78/115: batchLoss = 4.2384, diffLoss = 6.9413, kgLoss = 0.1840
2025-04-09 10:17:50.069940: Training Step 79/115: batchLoss = 4.3712, diffLoss = 7.1672, kgLoss = 0.1771
2025-04-09 10:17:50.821491: Training Step 80/115: batchLoss = 4.1110, diffLoss = 6.7429, kgLoss = 0.1632
2025-04-09 10:17:51.571477: Training Step 81/115: batchLoss = 4.3815, diffLoss = 7.1889, kgLoss = 0.1705
2025-04-09 10:17:52.311557: Training Step 82/115: batchLoss = 4.6680, diffLoss = 7.6615, kgLoss = 0.1778
2025-04-09 10:17:53.051012: Training Step 83/115: batchLoss = 3.9465, diffLoss = 6.4719, kgLoss = 0.1585
2025-04-09 10:17:53.799378: Training Step 84/115: batchLoss = 4.2045, diffLoss = 6.8906, kgLoss = 0.1754
2025-04-09 10:17:54.539909: Training Step 85/115: batchLoss = 4.1449, diffLoss = 6.7938, kgLoss = 0.1716
2025-04-09 10:17:55.273856: Training Step 86/115: batchLoss = 4.2580, diffLoss = 6.9858, kgLoss = 0.1664
2025-04-09 10:17:56.010400: Training Step 87/115: batchLoss = 3.7195, diffLoss = 6.1000, kgLoss = 0.1488
2025-04-09 10:17:56.739466: Training Step 88/115: batchLoss = 3.9060, diffLoss = 6.4014, kgLoss = 0.1629
2025-04-09 10:17:57.473065: Training Step 89/115: batchLoss = 4.1596, diffLoss = 6.8263, kgLoss = 0.1597
2025-04-09 10:17:58.220528: Training Step 90/115: batchLoss = 4.5735, diffLoss = 7.4979, kgLoss = 0.1869
2025-04-09 10:17:58.958082: Training Step 91/115: batchLoss = 4.4310, diffLoss = 7.2701, kgLoss = 0.1723
2025-04-09 10:17:59.710720: Training Step 92/115: batchLoss = 4.2759, diffLoss = 7.0138, kgLoss = 0.1692
2025-04-09 10:18:00.433660: Training Step 93/115: batchLoss = 4.1935, diffLoss = 6.8770, kgLoss = 0.1682
2025-04-09 10:18:01.165395: Training Step 94/115: batchLoss = 4.0670, diffLoss = 6.6651, kgLoss = 0.1699
2025-04-09 10:18:01.893268: Training Step 95/115: batchLoss = 4.4271, diffLoss = 7.2637, kgLoss = 0.1722
2025-04-09 10:18:02.625356: Training Step 96/115: batchLoss = 4.2538, diffLoss = 6.9738, kgLoss = 0.1740
2025-04-09 10:18:03.357602: Training Step 97/115: batchLoss = 4.5922, diffLoss = 7.5376, kgLoss = 0.1740
2025-04-09 10:18:04.088167: Training Step 98/115: batchLoss = 4.1174, diffLoss = 6.7459, kgLoss = 0.1746
2025-04-09 10:18:04.817173: Training Step 99/115: batchLoss = 4.4865, diffLoss = 7.3563, kgLoss = 0.1818
2025-04-09 10:18:05.545632: Training Step 100/115: batchLoss = 4.5375, diffLoss = 7.4376, kgLoss = 0.1874
2025-04-09 10:18:06.287334: Training Step 101/115: batchLoss = 4.5317, diffLoss = 7.4351, kgLoss = 0.1766
2025-04-09 10:18:07.006877: Training Step 102/115: batchLoss = 3.7756, diffLoss = 6.1882, kgLoss = 0.1567
2025-04-09 10:18:07.746704: Training Step 103/115: batchLoss = 4.3271, diffLoss = 7.1016, kgLoss = 0.1654
2025-04-09 10:18:08.486105: Training Step 104/115: batchLoss = 4.4889, diffLoss = 7.3619, kgLoss = 0.1793
2025-04-09 10:18:09.222588: Training Step 105/115: batchLoss = 4.3620, diffLoss = 7.1499, kgLoss = 0.1803
2025-04-09 10:18:09.949630: Training Step 106/115: batchLoss = 4.4949, diffLoss = 7.3697, kgLoss = 0.1828
2025-04-09 10:18:10.685103: Training Step 107/115: batchLoss = 3.9197, diffLoss = 6.4180, kgLoss = 0.1722
2025-04-09 10:18:11.410190: Training Step 108/115: batchLoss = 4.4092, diffLoss = 7.2342, kgLoss = 0.1718
2025-04-09 10:18:12.131367: Training Step 109/115: batchLoss = 3.7593, diffLoss = 6.1618, kgLoss = 0.1556
2025-04-09 10:18:12.853394: Training Step 110/115: batchLoss = 4.0571, diffLoss = 6.6503, kgLoss = 0.1672
2025-04-09 10:18:13.572008: Training Step 111/115: batchLoss = 4.0534, diffLoss = 6.6504, kgLoss = 0.1581
2025-04-09 10:18:14.306777: Training Step 112/115: batchLoss = 4.0887, diffLoss = 6.7040, kgLoss = 0.1657
2025-04-09 10:18:14.948168: Training Step 113/115: batchLoss = 4.8528, diffLoss = 7.9598, kgLoss = 0.1923
2025-04-09 10:18:15.586859: Training Step 114/115: batchLoss = 4.6233, diffLoss = 7.5838, kgLoss = 0.1826
2025-04-09 10:18:15.722113: 
2025-04-09 10:18:15.722810: Epoch 25/1000, Train: epLoss = 1.2467, epDfLoss = 2.0444, epfTransLoss = 0.0000, epKgLoss = 0.0502  
2025-04-09 10:18:16.473222: Steps 0/90: batch_recall = 54.38, batch_ndcg = 41.97 
2025-04-09 10:18:17.204999: Steps 1/90: batch_recall = 58.81, batch_ndcg = 40.87 
2025-04-09 10:18:17.978701: Steps 2/90: batch_recall = 51.97, batch_ndcg = 37.11 
2025-04-09 10:18:18.715762: Steps 3/90: batch_recall = 46.52, batch_ndcg = 32.92 
2025-04-09 10:18:19.452506: Steps 4/90: batch_recall = 48.11, batch_ndcg = 35.80 
2025-04-09 10:18:20.190254: Steps 5/90: batch_recall = 36.60, batch_ndcg = 27.18 
2025-04-09 10:18:20.928862: Steps 6/90: batch_recall = 45.81, batch_ndcg = 33.01 
2025-04-09 10:18:21.677494: Steps 7/90: batch_recall = 43.79, batch_ndcg = 31.53 
2025-04-09 10:18:22.414204: Steps 8/90: batch_recall = 47.49, batch_ndcg = 33.81 
2025-04-09 10:18:23.173454: Steps 9/90: batch_recall = 43.26, batch_ndcg = 33.48 
2025-04-09 10:18:23.935374: Steps 10/90: batch_recall = 38.12, batch_ndcg = 28.47 
2025-04-09 10:18:24.684056: Steps 11/90: batch_recall = 43.18, batch_ndcg = 28.83 
2025-04-09 10:18:25.429215: Steps 12/90: batch_recall = 39.97, batch_ndcg = 28.26 
2025-04-09 10:18:26.194257: Steps 13/90: batch_recall = 38.44, batch_ndcg = 26.65 
2025-04-09 10:18:26.949762: Steps 14/90: batch_recall = 34.37, batch_ndcg = 26.50 
2025-04-09 10:18:27.684617: Steps 15/90: batch_recall = 46.53, batch_ndcg = 29.46 
2025-04-09 10:18:28.411925: Steps 16/90: batch_recall = 36.98, batch_ndcg = 26.51 
2025-04-09 10:18:29.140444: Steps 17/90: batch_recall = 36.64, batch_ndcg = 23.67 
2025-04-09 10:18:29.869241: Steps 18/90: batch_recall = 40.15, batch_ndcg = 26.16 
2025-04-09 10:18:30.601843: Steps 19/90: batch_recall = 34.73, batch_ndcg = 24.46 
2025-04-09 10:18:31.313193: Steps 20/90: batch_recall = 36.28, batch_ndcg = 24.74 
2025-04-09 10:18:32.035044: Steps 21/90: batch_recall = 40.26, batch_ndcg = 28.64 
2025-04-09 10:18:32.758067: Steps 22/90: batch_recall = 41.08, batch_ndcg = 28.36 
2025-04-09 10:18:33.477517: Steps 23/90: batch_recall = 38.36, batch_ndcg = 25.55 
2025-04-09 10:18:34.198587: Steps 24/90: batch_recall = 41.50, batch_ndcg = 27.50 
2025-04-09 10:18:34.921170: Steps 25/90: batch_recall = 38.19, batch_ndcg = 24.77 
2025-04-09 10:18:35.658647: Steps 26/90: batch_recall = 38.52, batch_ndcg = 27.60 
2025-04-09 10:18:36.378602: Steps 27/90: batch_recall = 37.90, batch_ndcg = 25.00 
2025-04-09 10:18:37.124929: Steps 28/90: batch_recall = 36.66, batch_ndcg = 24.81 
2025-04-09 10:18:37.852284: Steps 29/90: batch_recall = 34.70, batch_ndcg = 23.14 
2025-04-09 10:18:38.565430: Steps 30/90: batch_recall = 35.42, batch_ndcg = 22.28 
2025-04-09 10:18:39.298347: Steps 31/90: batch_recall = 32.96, batch_ndcg = 23.11 
2025-04-09 10:18:40.002649: Steps 32/90: batch_recall = 34.97, batch_ndcg = 23.41 
2025-04-09 10:18:40.719783: Steps 33/90: batch_recall = 36.78, batch_ndcg = 22.43 
2025-04-09 10:18:41.441841: Steps 34/90: batch_recall = 36.06, batch_ndcg = 24.77 
2025-04-09 10:18:42.135770: Steps 35/90: batch_recall = 38.44, batch_ndcg = 26.56 
2025-04-09 10:18:42.853256: Steps 36/90: batch_recall = 38.40, batch_ndcg = 25.06 
2025-04-09 10:18:43.558641: Steps 37/90: batch_recall = 31.27, batch_ndcg = 21.18 
2025-04-09 10:18:44.283237: Steps 38/90: batch_recall = 33.96, batch_ndcg = 22.22 
2025-04-09 10:18:44.992790: Steps 39/90: batch_recall = 37.36, batch_ndcg = 23.15 
2025-04-09 10:18:45.691018: Steps 40/90: batch_recall = 33.18, batch_ndcg = 22.66 
2025-04-09 10:18:46.399827: Steps 41/90: batch_recall = 35.76, batch_ndcg = 23.62 
2025-04-09 10:18:47.118971: Steps 42/90: batch_recall = 40.23, batch_ndcg = 28.25 
2025-04-09 10:18:47.838728: Steps 43/90: batch_recall = 36.57, batch_ndcg = 23.35 
2025-04-09 10:18:48.567852: Steps 44/90: batch_recall = 34.12, batch_ndcg = 22.47 
2025-04-09 10:18:49.272435: Steps 45/90: batch_recall = 33.60, batch_ndcg = 22.54 
2025-04-09 10:18:49.986147: Steps 46/90: batch_recall = 34.90, batch_ndcg = 21.44 
2025-04-09 10:18:50.685328: Steps 47/90: batch_recall = 31.87, batch_ndcg = 20.46 
2025-04-09 10:18:51.388182: Steps 48/90: batch_recall = 37.79, batch_ndcg = 23.20 
2025-04-09 10:18:52.108964: Steps 49/90: batch_recall = 31.61, batch_ndcg = 20.51 
2025-04-09 10:18:52.817363: Steps 50/90: batch_recall = 38.52, batch_ndcg = 25.33 
2025-04-09 10:18:53.550098: Steps 51/90: batch_recall = 38.73, batch_ndcg = 25.57 
2025-04-09 10:18:54.258774: Steps 52/90: batch_recall = 36.35, batch_ndcg = 23.04 
2025-04-09 10:18:54.972809: Steps 53/90: batch_recall = 38.86, batch_ndcg = 25.17 
2025-04-09 10:18:55.680688: Steps 54/90: batch_recall = 30.76, batch_ndcg = 19.86 
2025-04-09 10:18:56.382260: Steps 55/90: batch_recall = 36.32, batch_ndcg = 24.31 
2025-04-09 10:18:57.079387: Steps 56/90: batch_recall = 32.89, batch_ndcg = 19.22 
2025-04-09 10:18:57.793405: Steps 57/90: batch_recall = 38.61, batch_ndcg = 24.59 
2025-04-09 10:18:58.495295: Steps 58/90: batch_recall = 38.19, batch_ndcg = 24.44 
2025-04-09 10:18:59.200046: Steps 59/90: batch_recall = 32.43, batch_ndcg = 21.12 
2025-04-09 10:18:59.904371: Steps 60/90: batch_recall = 32.11, batch_ndcg = 20.09 
2025-04-09 10:19:00.592641: Steps 61/90: batch_recall = 39.87, batch_ndcg = 27.48 
2025-04-09 10:19:01.314225: Steps 62/90: batch_recall = 38.60, batch_ndcg = 25.88 
2025-04-09 10:19:02.035006: Steps 63/90: batch_recall = 36.72, batch_ndcg = 23.37 
2025-04-09 10:19:02.756498: Steps 64/90: batch_recall = 36.99, batch_ndcg = 23.43 
2025-04-09 10:19:03.461874: Steps 65/90: batch_recall = 31.45, batch_ndcg = 20.80 
2025-04-09 10:19:04.166727: Steps 66/90: batch_recall = 36.75, batch_ndcg = 22.92 
2025-04-09 10:19:04.865611: Steps 67/90: batch_recall = 34.79, batch_ndcg = 21.60 
2025-04-09 10:19:05.564313: Steps 68/90: batch_recall = 36.18, batch_ndcg = 22.14 
2025-04-09 10:19:06.268970: Steps 69/90: batch_recall = 41.31, batch_ndcg = 24.31 
2025-04-09 10:19:06.980004: Steps 70/90: batch_recall = 38.42, batch_ndcg = 24.31 
2025-04-09 10:19:07.691507: Steps 71/90: batch_recall = 37.30, batch_ndcg = 22.77 
2025-04-09 10:19:08.394192: Steps 72/90: batch_recall = 35.84, batch_ndcg = 22.69 
2025-04-09 10:19:09.088815: Steps 73/90: batch_recall = 34.83, batch_ndcg = 22.12 
2025-04-09 10:19:09.799770: Steps 74/90: batch_recall = 39.32, batch_ndcg = 24.61 
2025-04-09 10:19:10.491222: Steps 75/90: batch_recall = 39.21, batch_ndcg = 24.36 
2025-04-09 10:19:11.182946: Steps 76/90: batch_recall = 40.22, batch_ndcg = 25.52 
2025-04-09 10:19:11.878563: Steps 77/90: batch_recall = 43.75, batch_ndcg = 27.12 
2025-04-09 10:19:12.588934: Steps 78/90: batch_recall = 34.47, batch_ndcg = 20.94 
2025-04-09 10:19:13.288323: Steps 79/90: batch_recall = 39.86, batch_ndcg = 26.78 
2025-04-09 10:19:13.983742: Steps 80/90: batch_recall = 43.99, batch_ndcg = 28.03 
2025-04-09 10:19:14.695050: Steps 81/90: batch_recall = 42.15, batch_ndcg = 26.33 
2025-04-09 10:19:15.406840: Steps 82/90: batch_recall = 40.27, batch_ndcg = 26.20 
2025-04-09 10:19:16.108498: Steps 83/90: batch_recall = 43.14, batch_ndcg = 26.37 
2025-04-09 10:19:16.821080: Steps 84/90: batch_recall = 33.12, batch_ndcg = 20.67 
2025-04-09 10:19:17.510653: Steps 85/90: batch_recall = 40.38, batch_ndcg = 23.07 
2025-04-09 10:19:18.213240: Steps 86/90: batch_recall = 44.70, batch_ndcg = 27.96 
2025-04-09 10:19:18.919022: Steps 87/90: batch_recall = 42.23, batch_ndcg = 28.61 
2025-04-09 10:19:19.618593: Steps 88/90: batch_recall = 49.21, batch_ndcg = 28.33 
2025-04-09 10:19:20.174258: Steps 89/90: batch_recall = 40.94, batch_ndcg = 25.49 
2025-04-09 10:19:20.174981: Epoch 25/1000, Test: Recall = 0.0770, NDCG = 0.0502  

2025-04-09 10:19:21.180714: Training Step 0/115: batchLoss = 3.8028, diffLoss = 6.2270, kgLoss = 0.1667
2025-04-09 10:19:21.926084: Training Step 1/115: batchLoss = 4.2604, diffLoss = 6.9895, kgLoss = 0.1669
2025-04-09 10:19:22.687221: Training Step 2/115: batchLoss = 4.3834, diffLoss = 7.1861, kgLoss = 0.1793
2025-04-09 10:19:23.435575: Training Step 3/115: batchLoss = 4.6537, diffLoss = 7.6318, kgLoss = 0.1865
2025-04-09 10:19:24.180221: Training Step 4/115: batchLoss = 4.2485, diffLoss = 6.9667, kgLoss = 0.1712
2025-04-09 10:19:24.918823: Training Step 5/115: batchLoss = 3.8730, diffLoss = 6.3510, kgLoss = 0.1561
2025-04-09 10:19:25.663496: Training Step 6/115: batchLoss = 4.4141, diffLoss = 7.2372, kgLoss = 0.1795
2025-04-09 10:19:26.389533: Training Step 7/115: batchLoss = 4.4009, diffLoss = 7.2169, kgLoss = 0.1770
2025-04-09 10:19:27.131290: Training Step 8/115: batchLoss = 4.5877, diffLoss = 7.5268, kgLoss = 0.1790
2025-04-09 10:19:27.872480: Training Step 9/115: batchLoss = 4.1213, diffLoss = 6.7577, kgLoss = 0.1665
2025-04-09 10:19:28.623185: Training Step 10/115: batchLoss = 3.9771, diffLoss = 6.5169, kgLoss = 0.1674
2025-04-09 10:19:29.378892: Training Step 11/115: batchLoss = 3.7902, diffLoss = 6.2111, kgLoss = 0.1588
2025-04-09 10:19:30.125426: Training Step 12/115: batchLoss = 4.5138, diffLoss = 7.4044, kgLoss = 0.1779
2025-04-09 10:19:30.896069: Training Step 13/115: batchLoss = 4.6312, diffLoss = 7.5951, kgLoss = 0.1853
2025-04-09 10:19:31.655067: Training Step 14/115: batchLoss = 4.6997, diffLoss = 7.7116, kgLoss = 0.1819
2025-04-09 10:19:32.386515: Training Step 15/115: batchLoss = 3.8867, diffLoss = 6.3727, kgLoss = 0.1576
2025-04-09 10:19:33.127714: Training Step 16/115: batchLoss = 4.1597, diffLoss = 6.8235, kgLoss = 0.1641
2025-04-09 10:19:33.860590: Training Step 17/115: batchLoss = 3.9643, diffLoss = 6.5035, kgLoss = 0.1554
2025-04-09 10:19:34.608245: Training Step 18/115: batchLoss = 5.2243, diffLoss = 8.5747, kgLoss = 0.1988
2025-04-09 10:19:35.342929: Training Step 19/115: batchLoss = 4.7326, diffLoss = 7.7636, kgLoss = 0.1861
2025-04-09 10:19:36.078512: Training Step 20/115: batchLoss = 4.1919, diffLoss = 6.8744, kgLoss = 0.1682
2025-04-09 10:19:36.809598: Training Step 21/115: batchLoss = 4.3822, diffLoss = 7.1916, kgLoss = 0.1680
2025-04-09 10:19:37.536887: Training Step 22/115: batchLoss = 4.3743, diffLoss = 7.1777, kgLoss = 0.1693
2025-04-09 10:19:38.280502: Training Step 23/115: batchLoss = 4.7328, diffLoss = 7.7686, kgLoss = 0.1792
2025-04-09 10:19:39.013821: Training Step 24/115: batchLoss = 4.9667, diffLoss = 8.1476, kgLoss = 0.1953
2025-04-09 10:19:39.763528: Training Step 25/115: batchLoss = 4.1909, diffLoss = 6.8713, kgLoss = 0.1704
2025-04-09 10:19:40.495240: Training Step 26/115: batchLoss = 3.6061, diffLoss = 5.9025, kgLoss = 0.1614
2025-04-09 10:19:41.244048: Training Step 27/115: batchLoss = 3.7751, diffLoss = 6.1894, kgLoss = 0.1535
2025-04-09 10:19:41.999815: Training Step 28/115: batchLoss = 4.2111, diffLoss = 6.8984, kgLoss = 0.1802
2025-04-09 10:19:42.739044: Training Step 29/115: batchLoss = 4.5112, diffLoss = 7.4007, kgLoss = 0.1770
2025-04-09 10:19:43.475455: Training Step 30/115: batchLoss = 4.4811, diffLoss = 7.3425, kgLoss = 0.1889
2025-04-09 10:19:44.218400: Training Step 31/115: batchLoss = 4.2346, diffLoss = 6.9425, kgLoss = 0.1727
2025-04-09 10:19:44.957172: Training Step 32/115: batchLoss = 4.6562, diffLoss = 7.6398, kgLoss = 0.1808
2025-04-09 10:19:45.704815: Training Step 33/115: batchLoss = 4.2595, diffLoss = 6.9848, kgLoss = 0.1714
2025-04-09 10:19:46.451818: Training Step 34/115: batchLoss = 4.0628, diffLoss = 6.6590, kgLoss = 0.1686
2025-04-09 10:19:47.197665: Training Step 35/115: batchLoss = 4.2324, diffLoss = 6.9420, kgLoss = 0.1679
2025-04-09 10:19:47.930175: Training Step 36/115: batchLoss = 4.6062, diffLoss = 7.5414, kgLoss = 0.2034
2025-04-09 10:19:48.654869: Training Step 37/115: batchLoss = 4.6738, diffLoss = 7.6644, kgLoss = 0.1878
2025-04-09 10:19:49.388925: Training Step 38/115: batchLoss = 4.2415, diffLoss = 6.9555, kgLoss = 0.1706
2025-04-09 10:19:50.127242: Training Step 39/115: batchLoss = 3.8585, diffLoss = 6.3199, kgLoss = 0.1664
2025-04-09 10:19:50.878916: Training Step 40/115: batchLoss = 4.2199, diffLoss = 6.9150, kgLoss = 0.1772
2025-04-09 10:19:51.618921: Training Step 41/115: batchLoss = 4.4421, diffLoss = 7.2867, kgLoss = 0.1753
2025-04-09 10:19:52.362121: Training Step 42/115: batchLoss = 4.1049, diffLoss = 6.7274, kgLoss = 0.1712
2025-04-09 10:19:53.103486: Training Step 43/115: batchLoss = 4.4157, diffLoss = 7.2473, kgLoss = 0.1682
2025-04-09 10:19:53.855020: Training Step 44/115: batchLoss = 4.6591, diffLoss = 7.6372, kgLoss = 0.1921
2025-04-09 10:19:54.606684: Training Step 45/115: batchLoss = 3.8920, diffLoss = 6.3810, kgLoss = 0.1584
2025-04-09 10:19:55.360371: Training Step 46/115: batchLoss = 4.6573, diffLoss = 7.6414, kgLoss = 0.1811
2025-04-09 10:19:56.122128: Training Step 47/115: batchLoss = 4.2709, diffLoss = 7.0004, kgLoss = 0.1765
2025-04-09 10:19:56.867862: Training Step 48/115: batchLoss = 4.9217, diffLoss = 8.0745, kgLoss = 0.1925
2025-04-09 10:19:57.606513: Training Step 49/115: batchLoss = 3.9191, diffLoss = 6.4275, kgLoss = 0.1566
2025-04-09 10:19:58.348077: Training Step 50/115: batchLoss = 3.9701, diffLoss = 6.5055, kgLoss = 0.1670
2025-04-09 10:19:59.088985: Training Step 51/115: batchLoss = 4.6223, diffLoss = 7.5837, kgLoss = 0.1802
2025-04-09 10:19:59.820167: Training Step 52/115: batchLoss = 5.1234, diffLoss = 8.4032, kgLoss = 0.2037
2025-04-09 10:20:00.552883: Training Step 53/115: batchLoss = 4.2335, diffLoss = 6.9357, kgLoss = 0.1802
2025-04-09 10:20:01.285757: Training Step 54/115: batchLoss = 4.2647, diffLoss = 6.9831, kgLoss = 0.1870
2025-04-09 10:20:02.035968: Training Step 55/115: batchLoss = 4.1936, diffLoss = 6.8654, kgLoss = 0.1861
2025-04-09 10:20:02.783520: Training Step 56/115: batchLoss = 4.3848, diffLoss = 7.1942, kgLoss = 0.1708
2025-04-09 10:20:03.505738: Training Step 57/115: batchLoss = 3.9663, diffLoss = 6.5074, kgLoss = 0.1546
2025-04-09 10:20:04.243695: Training Step 58/115: batchLoss = 4.1406, diffLoss = 6.7920, kgLoss = 0.1637
2025-04-09 10:20:04.976396: Training Step 59/115: batchLoss = 4.8531, diffLoss = 7.9616, kgLoss = 0.1904
2025-04-09 10:20:05.712888: Training Step 60/115: batchLoss = 4.2013, diffLoss = 6.8924, kgLoss = 0.1648
2025-04-09 10:20:06.452124: Training Step 61/115: batchLoss = 3.9968, diffLoss = 6.5493, kgLoss = 0.1681
2025-04-09 10:20:07.195022: Training Step 62/115: batchLoss = 3.9392, diffLoss = 6.4564, kgLoss = 0.1634
2025-04-09 10:20:07.931670: Training Step 63/115: batchLoss = 4.4504, diffLoss = 7.2954, kgLoss = 0.1828
2025-04-09 10:20:08.667393: Training Step 64/115: batchLoss = 4.3483, diffLoss = 7.1337, kgLoss = 0.1704
2025-04-09 10:20:09.407151: Training Step 65/115: batchLoss = 4.2144, diffLoss = 6.9077, kgLoss = 0.1745
2025-04-09 10:20:10.160803: Training Step 66/115: batchLoss = 3.8431, diffLoss = 6.3014, kgLoss = 0.1557
2025-04-09 10:20:10.917602: Training Step 67/115: batchLoss = 3.9666, diffLoss = 6.5014, kgLoss = 0.1643
2025-04-09 10:20:11.667876: Training Step 68/115: batchLoss = 4.9536, diffLoss = 8.1252, kgLoss = 0.1962
2025-04-09 10:20:12.401671: Training Step 69/115: batchLoss = 3.9952, diffLoss = 6.5488, kgLoss = 0.1648
2025-04-09 10:20:13.152923: Training Step 70/115: batchLoss = 4.4158, diffLoss = 7.2461, kgLoss = 0.1705
2025-04-09 10:20:13.910090: Training Step 71/115: batchLoss = 4.8028, diffLoss = 7.8800, kgLoss = 0.1871
2025-04-09 10:20:14.650113: Training Step 72/115: batchLoss = 4.0919, diffLoss = 6.7086, kgLoss = 0.1668
2025-04-09 10:20:15.386075: Training Step 73/115: batchLoss = 4.1902, diffLoss = 6.8691, kgLoss = 0.1718
2025-04-09 10:20:16.125039: Training Step 74/115: batchLoss = 4.1280, diffLoss = 6.7708, kgLoss = 0.1639
2025-04-09 10:20:16.881082: Training Step 75/115: batchLoss = 4.6127, diffLoss = 7.5648, kgLoss = 0.1847
2025-04-09 10:20:17.616598: Training Step 76/115: batchLoss = 4.0757, diffLoss = 6.6779, kgLoss = 0.1724
2025-04-09 10:20:18.355688: Training Step 77/115: batchLoss = 4.2590, diffLoss = 6.9804, kgLoss = 0.1768
2025-04-09 10:20:19.135898: Training Step 78/115: batchLoss = 4.5595, diffLoss = 7.4787, kgLoss = 0.1807
2025-04-09 10:20:19.875801: Training Step 79/115: batchLoss = 4.7215, diffLoss = 7.7440, kgLoss = 0.1878
2025-04-09 10:20:20.621498: Training Step 80/115: batchLoss = 4.2910, diffLoss = 7.0380, kgLoss = 0.1705
2025-04-09 10:20:21.356715: Training Step 81/115: batchLoss = 5.1873, diffLoss = 8.5054, kgLoss = 0.2102
2025-04-09 10:20:22.157399: Training Step 82/115: batchLoss = 4.1473, diffLoss = 6.7992, kgLoss = 0.1694
2025-04-09 10:20:22.895998: Training Step 83/115: batchLoss = 4.0239, diffLoss = 6.5944, kgLoss = 0.1681
2025-04-09 10:20:23.642786: Training Step 84/115: batchLoss = 4.1451, diffLoss = 6.7962, kgLoss = 0.1685
2025-04-09 10:20:24.389721: Training Step 85/115: batchLoss = 4.3582, diffLoss = 7.1477, kgLoss = 0.1741
2025-04-09 10:20:25.126301: Training Step 86/115: batchLoss = 4.1919, diffLoss = 6.8789, kgLoss = 0.1615
2025-04-09 10:20:25.870253: Training Step 87/115: batchLoss = 4.2142, diffLoss = 6.9079, kgLoss = 0.1737
2025-04-09 10:20:26.619404: Training Step 88/115: batchLoss = 4.0994, diffLoss = 6.7252, kgLoss = 0.1607
2025-04-09 10:20:27.367461: Training Step 89/115: batchLoss = 4.1357, diffLoss = 6.7807, kgLoss = 0.1682
2025-04-09 10:20:28.217074: Training Step 90/115: batchLoss = 4.1507, diffLoss = 6.8117, kgLoss = 0.1591
2025-04-09 10:20:28.976620: Training Step 91/115: batchLoss = 3.8903, diffLoss = 6.3793, kgLoss = 0.1567
2025-04-09 10:20:29.729578: Training Step 92/115: batchLoss = 4.3784, diffLoss = 7.1730, kgLoss = 0.1865
2025-04-09 10:20:30.482992: Training Step 93/115: batchLoss = 4.8759, diffLoss = 7.9912, kgLoss = 0.2031
2025-04-09 10:20:31.229259: Training Step 94/115: batchLoss = 4.3544, diffLoss = 7.1344, kgLoss = 0.1844
2025-04-09 10:20:31.973152: Training Step 95/115: batchLoss = 4.4417, diffLoss = 7.2715, kgLoss = 0.1971
2025-04-09 10:20:32.733511: Training Step 96/115: batchLoss = 3.7416, diffLoss = 6.1334, kgLoss = 0.1538
2025-04-09 10:20:33.475971: Training Step 97/115: batchLoss = 4.3737, diffLoss = 7.1688, kgLoss = 0.1810
2025-04-09 10:20:34.219601: Training Step 98/115: batchLoss = 4.2445, diffLoss = 6.9550, kgLoss = 0.1789
2025-04-09 10:20:34.944716: Training Step 99/115: batchLoss = 4.7293, diffLoss = 7.7514, kgLoss = 0.1963
2025-04-09 10:20:35.674925: Training Step 100/115: batchLoss = 3.9876, diffLoss = 6.5350, kgLoss = 0.1666
2025-04-09 10:20:36.406705: Training Step 101/115: batchLoss = 3.8344, diffLoss = 6.2902, kgLoss = 0.1507
2025-04-09 10:20:37.148190: Training Step 102/115: batchLoss = 4.1127, diffLoss = 6.7373, kgLoss = 0.1758
2025-04-09 10:20:37.889153: Training Step 103/115: batchLoss = 4.4869, diffLoss = 7.3500, kgLoss = 0.1924
2025-04-09 10:20:38.631177: Training Step 104/115: batchLoss = 4.0128, diffLoss = 6.5759, kgLoss = 0.1681
2025-04-09 10:20:39.367944: Training Step 105/115: batchLoss = 4.2381, diffLoss = 6.9483, kgLoss = 0.1727
2025-04-09 10:20:40.103299: Training Step 106/115: batchLoss = 3.7771, diffLoss = 6.1925, kgLoss = 0.1540
2025-04-09 10:20:40.844534: Training Step 107/115: batchLoss = 3.9790, diffLoss = 6.5260, kgLoss = 0.1585
2025-04-09 10:20:41.598170: Training Step 108/115: batchLoss = 4.0840, diffLoss = 6.6819, kgLoss = 0.1873
2025-04-09 10:20:42.347916: Training Step 109/115: batchLoss = 4.5145, diffLoss = 7.4096, kgLoss = 0.1719
2025-04-09 10:20:43.098171: Training Step 110/115: batchLoss = 3.9994, diffLoss = 6.5563, kgLoss = 0.1641
2025-04-09 10:20:43.867019: Training Step 111/115: batchLoss = 4.2104, diffLoss = 6.9059, kgLoss = 0.1670
2025-04-09 10:20:44.612391: Training Step 112/115: batchLoss = 3.9066, diffLoss = 6.4021, kgLoss = 0.1633
2025-04-09 10:20:45.268671: Training Step 113/115: batchLoss = 4.3873, diffLoss = 7.1953, kgLoss = 0.1753
2025-04-09 10:20:45.886868: Training Step 114/115: batchLoss = 5.3270, diffLoss = 8.7385, kgLoss = 0.2097
2025-04-09 10:20:45.999769: 
2025-04-09 10:20:46.000485: Epoch 26/1000, Train: epLoss = 1.2361, epDfLoss = 2.0267, epfTransLoss = 0.0000, epKgLoss = 0.0502  
2025-04-09 10:20:46.768220: Steps 0/90: batch_recall = 53.51, batch_ndcg = 41.93 
2025-04-09 10:20:47.521233: Steps 1/90: batch_recall = 58.61, batch_ndcg = 40.99 
2025-04-09 10:20:48.266914: Steps 2/90: batch_recall = 50.82, batch_ndcg = 37.11 
2025-04-09 10:20:49.023368: Steps 3/90: batch_recall = 47.71, batch_ndcg = 33.15 
2025-04-09 10:20:49.778208: Steps 4/90: batch_recall = 47.24, batch_ndcg = 36.12 
2025-04-09 10:20:50.519225: Steps 5/90: batch_recall = 37.42, batch_ndcg = 27.77 
2025-04-09 10:20:51.263656: Steps 6/90: batch_recall = 46.94, batch_ndcg = 33.56 
2025-04-09 10:20:52.057125: Steps 7/90: batch_recall = 43.96, batch_ndcg = 31.86 
2025-04-09 10:20:52.811260: Steps 8/90: batch_recall = 46.48, batch_ndcg = 33.65 
2025-04-09 10:20:53.578507: Steps 9/90: batch_recall = 43.57, batch_ndcg = 33.45 
2025-04-09 10:20:54.339902: Steps 10/90: batch_recall = 37.72, batch_ndcg = 28.45 
2025-04-09 10:20:55.085306: Steps 11/90: batch_recall = 43.28, batch_ndcg = 28.86 
2025-04-09 10:20:55.836362: Steps 12/90: batch_recall = 39.31, batch_ndcg = 27.98 
2025-04-09 10:20:56.574764: Steps 13/90: batch_recall = 38.12, batch_ndcg = 26.53 
2025-04-09 10:20:57.336736: Steps 14/90: batch_recall = 35.07, batch_ndcg = 26.68 
2025-04-09 10:20:58.089961: Steps 15/90: batch_recall = 47.28, batch_ndcg = 29.80 
2025-04-09 10:20:58.825815: Steps 16/90: batch_recall = 37.18, batch_ndcg = 26.49 
2025-04-09 10:20:59.554817: Steps 17/90: batch_recall = 35.86, batch_ndcg = 23.61 
2025-04-09 10:21:00.282033: Steps 18/90: batch_recall = 39.08, batch_ndcg = 26.13 
2025-04-09 10:21:01.024111: Steps 19/90: batch_recall = 34.14, batch_ndcg = 24.55 
2025-04-09 10:21:01.749798: Steps 20/90: batch_recall = 36.90, batch_ndcg = 25.12 
2025-04-09 10:21:02.465274: Steps 21/90: batch_recall = 41.79, batch_ndcg = 29.27 
2025-04-09 10:21:03.182810: Steps 22/90: batch_recall = 41.59, batch_ndcg = 28.95 
2025-04-09 10:21:03.901091: Steps 23/90: batch_recall = 38.32, batch_ndcg = 25.55 
2025-04-09 10:21:04.620134: Steps 24/90: batch_recall = 42.01, batch_ndcg = 27.76 
2025-04-09 10:21:05.333241: Steps 25/90: batch_recall = 37.19, batch_ndcg = 24.43 
2025-04-09 10:21:06.057502: Steps 26/90: batch_recall = 37.73, batch_ndcg = 27.78 
2025-04-09 10:21:06.780541: Steps 27/90: batch_recall = 35.94, batch_ndcg = 24.40 
2025-04-09 10:21:07.504315: Steps 28/90: batch_recall = 38.24, batch_ndcg = 25.23 
2025-04-09 10:21:08.224542: Steps 29/90: batch_recall = 33.92, batch_ndcg = 23.00 
2025-04-09 10:21:08.953385: Steps 30/90: batch_recall = 34.45, batch_ndcg = 21.94 
2025-04-09 10:21:09.692742: Steps 31/90: batch_recall = 33.53, batch_ndcg = 23.80 
2025-04-09 10:21:10.420788: Steps 32/90: batch_recall = 35.04, batch_ndcg = 23.44 
2025-04-09 10:21:11.156528: Steps 33/90: batch_recall = 36.49, batch_ndcg = 22.61 
2025-04-09 10:21:11.875542: Steps 34/90: batch_recall = 36.72, batch_ndcg = 24.78 
2025-04-09 10:21:12.584415: Steps 35/90: batch_recall = 38.41, batch_ndcg = 26.76 
2025-04-09 10:21:13.300587: Steps 36/90: batch_recall = 37.82, batch_ndcg = 24.86 
2025-04-09 10:21:14.016109: Steps 37/90: batch_recall = 31.57, batch_ndcg = 21.37 
2025-04-09 10:21:14.741004: Steps 38/90: batch_recall = 33.93, batch_ndcg = 22.42 
2025-04-09 10:21:15.465848: Steps 39/90: batch_recall = 37.38, batch_ndcg = 23.36 
2025-04-09 10:21:16.178126: Steps 40/90: batch_recall = 32.87, batch_ndcg = 22.73 
2025-04-09 10:21:16.893986: Steps 41/90: batch_recall = 35.65, batch_ndcg = 23.69 
2025-04-09 10:21:17.627716: Steps 42/90: batch_recall = 42.17, batch_ndcg = 28.99 
2025-04-09 10:21:18.345574: Steps 43/90: batch_recall = 36.96, batch_ndcg = 23.24 
2025-04-09 10:21:19.041474: Steps 44/90: batch_recall = 34.50, batch_ndcg = 22.62 
2025-04-09 10:21:19.737598: Steps 45/90: batch_recall = 33.49, batch_ndcg = 22.58 
2025-04-09 10:21:20.461989: Steps 46/90: batch_recall = 35.61, batch_ndcg = 21.82 
2025-04-09 10:21:21.161083: Steps 47/90: batch_recall = 31.96, batch_ndcg = 20.09 
2025-04-09 10:21:21.872038: Steps 48/90: batch_recall = 35.94, batch_ndcg = 22.76 
2025-04-09 10:21:22.574374: Steps 49/90: batch_recall = 30.89, batch_ndcg = 20.26 
2025-04-09 10:21:23.283627: Steps 50/90: batch_recall = 37.40, batch_ndcg = 25.05 
2025-04-09 10:21:23.984776: Steps 51/90: batch_recall = 38.99, batch_ndcg = 25.55 
2025-04-09 10:21:24.682753: Steps 52/90: batch_recall = 36.68, batch_ndcg = 23.32 
2025-04-09 10:21:25.404069: Steps 53/90: batch_recall = 38.47, batch_ndcg = 25.11 
2025-04-09 10:21:26.123025: Steps 54/90: batch_recall = 31.23, batch_ndcg = 19.84 
2025-04-09 10:21:26.852346: Steps 55/90: batch_recall = 35.73, batch_ndcg = 23.87 
2025-04-09 10:21:27.566583: Steps 56/90: batch_recall = 32.05, batch_ndcg = 18.87 
2025-04-09 10:21:28.272867: Steps 57/90: batch_recall = 37.52, batch_ndcg = 24.26 
2025-04-09 10:21:28.985807: Steps 58/90: batch_recall = 39.25, batch_ndcg = 24.09 
2025-04-09 10:21:29.698202: Steps 59/90: batch_recall = 32.07, batch_ndcg = 21.15 
2025-04-09 10:21:30.404378: Steps 60/90: batch_recall = 32.53, batch_ndcg = 20.42 
2025-04-09 10:21:31.125036: Steps 61/90: batch_recall = 41.02, batch_ndcg = 28.00 
2025-04-09 10:21:31.832342: Steps 62/90: batch_recall = 38.71, batch_ndcg = 25.64 
2025-04-09 10:21:32.568486: Steps 63/90: batch_recall = 36.98, batch_ndcg = 23.38 
2025-04-09 10:21:33.271733: Steps 64/90: batch_recall = 35.97, batch_ndcg = 22.93 
2025-04-09 10:21:33.987809: Steps 65/90: batch_recall = 31.31, batch_ndcg = 20.62 
2025-04-09 10:21:34.693032: Steps 66/90: batch_recall = 38.28, batch_ndcg = 23.47 
2025-04-09 10:21:35.397340: Steps 67/90: batch_recall = 34.28, batch_ndcg = 21.35 
2025-04-09 10:21:36.106759: Steps 68/90: batch_recall = 35.95, batch_ndcg = 21.96 
2025-04-09 10:21:36.820081: Steps 69/90: batch_recall = 41.94, batch_ndcg = 24.48 
2025-04-09 10:21:37.522290: Steps 70/90: batch_recall = 38.78, batch_ndcg = 24.79 
2025-04-09 10:21:38.235675: Steps 71/90: batch_recall = 38.12, batch_ndcg = 23.24 
2025-04-09 10:21:38.931926: Steps 72/90: batch_recall = 36.36, batch_ndcg = 23.06 
2025-04-09 10:21:39.641928: Steps 73/90: batch_recall = 34.35, batch_ndcg = 22.23 
2025-04-09 10:21:40.351584: Steps 74/90: batch_recall = 39.31, batch_ndcg = 25.17 
2025-04-09 10:21:41.074468: Steps 75/90: batch_recall = 37.77, batch_ndcg = 23.88 
2025-04-09 10:21:41.794459: Steps 76/90: batch_recall = 42.13, batch_ndcg = 26.03 
2025-04-09 10:21:42.504642: Steps 77/90: batch_recall = 44.37, batch_ndcg = 27.23 
2025-04-09 10:21:43.212882: Steps 78/90: batch_recall = 34.10, batch_ndcg = 20.77 
2025-04-09 10:21:43.908696: Steps 79/90: batch_recall = 39.41, batch_ndcg = 26.93 
2025-04-09 10:21:44.608013: Steps 80/90: batch_recall = 44.16, batch_ndcg = 28.10 
2025-04-09 10:21:45.327145: Steps 81/90: batch_recall = 43.93, batch_ndcg = 27.35 
2025-04-09 10:21:46.034435: Steps 82/90: batch_recall = 40.11, batch_ndcg = 26.04 
2025-04-09 10:21:46.752706: Steps 83/90: batch_recall = 41.42, batch_ndcg = 25.61 
2025-04-09 10:21:47.460318: Steps 84/90: batch_recall = 34.38, batch_ndcg = 21.37 
2025-04-09 10:21:48.152907: Steps 85/90: batch_recall = 41.25, batch_ndcg = 23.72 
2025-04-09 10:21:48.858606: Steps 86/90: batch_recall = 43.54, batch_ndcg = 27.56 
2025-04-09 10:21:49.542551: Steps 87/90: batch_recall = 42.89, batch_ndcg = 29.11 
2025-04-09 10:21:50.233685: Steps 88/90: batch_recall = 49.17, batch_ndcg = 27.87 
2025-04-09 10:21:50.781038: Steps 89/90: batch_recall = 40.94, batch_ndcg = 25.49 
2025-04-09 10:21:50.782468: Epoch 26/1000, Test: Recall = 0.0770, NDCG = 0.0504  

2025-04-09 10:21:51.808712: Training Step 0/115: batchLoss = 4.3476, diffLoss = 7.1259, kgLoss = 0.1801
2025-04-09 10:21:52.537482: Training Step 1/115: batchLoss = 3.8637, diffLoss = 6.3389, kgLoss = 0.1510
2025-04-09 10:21:53.260941: Training Step 2/115: batchLoss = 3.9732, diffLoss = 6.5169, kgLoss = 0.1576
2025-04-09 10:21:53.982537: Training Step 3/115: batchLoss = 4.5713, diffLoss = 7.5006, kgLoss = 0.1773
2025-04-09 10:21:54.711175: Training Step 4/115: batchLoss = 4.3345, diffLoss = 7.1119, kgLoss = 0.1683
2025-04-09 10:21:55.441883: Training Step 5/115: batchLoss = 4.5029, diffLoss = 7.3883, kgLoss = 0.1747
2025-04-09 10:21:56.168880: Training Step 6/115: batchLoss = 4.2209, diffLoss = 6.9213, kgLoss = 0.1702
2025-04-09 10:21:56.898646: Training Step 7/115: batchLoss = 4.1588, diffLoss = 6.8165, kgLoss = 0.1724
2025-04-09 10:21:57.628175: Training Step 8/115: batchLoss = 4.3881, diffLoss = 7.1921, kgLoss = 0.1821
2025-04-09 10:21:58.364859: Training Step 9/115: batchLoss = 3.9323, diffLoss = 6.4435, kgLoss = 0.1655
2025-04-09 10:21:59.097892: Training Step 10/115: batchLoss = 4.4734, diffLoss = 7.3368, kgLoss = 0.1782
2025-04-09 10:21:59.827179: Training Step 11/115: batchLoss = 4.9990, diffLoss = 8.1990, kgLoss = 0.1988
2025-04-09 10:22:00.558618: Training Step 12/115: batchLoss = 4.6159, diffLoss = 7.5710, kgLoss = 0.1832
2025-04-09 10:22:01.279327: Training Step 13/115: batchLoss = 4.1282, diffLoss = 6.7680, kgLoss = 0.1685
2025-04-09 10:22:02.005363: Training Step 14/115: batchLoss = 3.9718, diffLoss = 6.5089, kgLoss = 0.1661
2025-04-09 10:22:02.735898: Training Step 15/115: batchLoss = 4.3861, diffLoss = 7.1959, kgLoss = 0.1714
2025-04-09 10:22:03.467869: Training Step 16/115: batchLoss = 4.7879, diffLoss = 7.8549, kgLoss = 0.1875
2025-04-09 10:22:04.191407: Training Step 17/115: batchLoss = 3.9989, diffLoss = 6.5451, kgLoss = 0.1797
2025-04-09 10:22:04.922745: Training Step 18/115: batchLoss = 4.3666, diffLoss = 7.1641, kgLoss = 0.1703
2025-04-09 10:22:05.649432: Training Step 19/115: batchLoss = 4.4532, diffLoss = 7.3022, kgLoss = 0.1798
2025-04-09 10:22:06.380769: Training Step 20/115: batchLoss = 4.6108, diffLoss = 7.5651, kgLoss = 0.1794
2025-04-09 10:22:07.109574: Training Step 21/115: batchLoss = 4.5185, diffLoss = 7.4104, kgLoss = 0.1805
2025-04-09 10:22:07.838180: Training Step 22/115: batchLoss = 3.9298, diffLoss = 6.4396, kgLoss = 0.1651
2025-04-09 10:22:08.576218: Training Step 23/115: batchLoss = 4.4599, diffLoss = 7.3171, kgLoss = 0.1741
2025-04-09 10:22:09.308768: Training Step 24/115: batchLoss = 4.4967, diffLoss = 7.3725, kgLoss = 0.1829
2025-04-09 10:22:10.049423: Training Step 25/115: batchLoss = 4.1081, diffLoss = 6.7394, kgLoss = 0.1611
2025-04-09 10:22:10.782384: Training Step 26/115: batchLoss = 4.2297, diffLoss = 6.9358, kgLoss = 0.1706
2025-04-09 10:22:11.514842: Training Step 27/115: batchLoss = 4.2508, diffLoss = 6.9747, kgLoss = 0.1650
2025-04-09 10:22:12.249608: Training Step 28/115: batchLoss = 3.6433, diffLoss = 5.9706, kgLoss = 0.1522
2025-04-09 10:22:12.982817: Training Step 29/115: batchLoss = 4.4836, diffLoss = 7.3471, kgLoss = 0.1883
2025-04-09 10:22:13.713337: Training Step 30/115: batchLoss = 3.9468, diffLoss = 6.4698, kgLoss = 0.1622
2025-04-09 10:22:14.458784: Training Step 31/115: batchLoss = 4.1922, diffLoss = 6.8704, kgLoss = 0.1750
2025-04-09 10:22:15.190968: Training Step 32/115: batchLoss = 4.1781, diffLoss = 6.8455, kgLoss = 0.1771
2025-04-09 10:22:15.925175: Training Step 33/115: batchLoss = 4.2123, diffLoss = 6.9119, kgLoss = 0.1629
2025-04-09 10:22:16.650320: Training Step 34/115: batchLoss = 4.3869, diffLoss = 7.1991, kgLoss = 0.1685
2025-04-09 10:22:17.382196: Training Step 35/115: batchLoss = 4.1396, diffLoss = 6.7867, kgLoss = 0.1689
2025-04-09 10:22:18.113338: Training Step 36/115: batchLoss = 4.6411, diffLoss = 7.6095, kgLoss = 0.1885
2025-04-09 10:22:18.846739: Training Step 37/115: batchLoss = 3.9609, diffLoss = 6.4937, kgLoss = 0.1616
2025-04-09 10:22:19.572474: Training Step 38/115: batchLoss = 4.3146, diffLoss = 7.0739, kgLoss = 0.1758
2025-04-09 10:22:20.300991: Training Step 39/115: batchLoss = 3.9542, diffLoss = 6.4865, kgLoss = 0.1559
2025-04-09 10:22:21.038917: Training Step 40/115: batchLoss = 4.4670, diffLoss = 7.3295, kgLoss = 0.1733
2025-04-09 10:22:21.776891: Training Step 41/115: batchLoss = 3.9491, diffLoss = 6.4662, kgLoss = 0.1733
2025-04-09 10:22:22.497198: Training Step 42/115: batchLoss = 4.4278, diffLoss = 7.2589, kgLoss = 0.1811
2025-04-09 10:22:23.226749: Training Step 43/115: batchLoss = 4.0016, diffLoss = 6.5549, kgLoss = 0.1718
2025-04-09 10:22:23.964534: Training Step 44/115: batchLoss = 4.5961, diffLoss = 7.5438, kgLoss = 0.1745
2025-04-09 10:22:24.703761: Training Step 45/115: batchLoss = 4.5819, diffLoss = 7.5095, kgLoss = 0.1904
2025-04-09 10:22:25.445926: Training Step 46/115: batchLoss = 4.5251, diffLoss = 7.4250, kgLoss = 0.1754
2025-04-09 10:22:26.190183: Training Step 47/115: batchLoss = 4.2702, diffLoss = 7.0037, kgLoss = 0.1700
2025-04-09 10:22:26.938388: Training Step 48/115: batchLoss = 4.1714, diffLoss = 6.8411, kgLoss = 0.1669
2025-04-09 10:22:27.688491: Training Step 49/115: batchLoss = 4.5637, diffLoss = 7.4834, kgLoss = 0.1840
2025-04-09 10:22:28.436221: Training Step 50/115: batchLoss = 4.3004, diffLoss = 7.0444, kgLoss = 0.1844
2025-04-09 10:22:29.188006: Training Step 51/115: batchLoss = 4.6846, diffLoss = 7.6883, kgLoss = 0.1790
2025-04-09 10:22:29.942047: Training Step 52/115: batchLoss = 4.7507, diffLoss = 7.7957, kgLoss = 0.1831
2025-04-09 10:22:30.680459: Training Step 53/115: batchLoss = 4.3549, diffLoss = 7.1400, kgLoss = 0.1773
2025-04-09 10:22:31.406009: Training Step 54/115: batchLoss = 4.5710, diffLoss = 7.4958, kgLoss = 0.1837
2025-04-09 10:22:32.159551: Training Step 55/115: batchLoss = 3.8462, diffLoss = 6.3014, kgLoss = 0.1635
2025-04-09 10:22:32.896645: Training Step 56/115: batchLoss = 4.2396, diffLoss = 6.9560, kgLoss = 0.1651
2025-04-09 10:22:33.634404: Training Step 57/115: batchLoss = 4.5228, diffLoss = 7.4211, kgLoss = 0.1752
2025-04-09 10:22:34.369960: Training Step 58/115: batchLoss = 4.1990, diffLoss = 6.8883, kgLoss = 0.1651
2025-04-09 10:22:35.107216: Training Step 59/115: batchLoss = 3.9580, diffLoss = 6.4870, kgLoss = 0.1644
2025-04-09 10:22:35.863037: Training Step 60/115: batchLoss = 4.6996, diffLoss = 7.7108, kgLoss = 0.1828
2025-04-09 10:22:36.610548: Training Step 61/115: batchLoss = 4.1386, diffLoss = 6.7852, kgLoss = 0.1689
2025-04-09 10:22:37.348962: Training Step 62/115: batchLoss = 3.8843, diffLoss = 6.3668, kgLoss = 0.1606
2025-04-09 10:22:38.113004: Training Step 63/115: batchLoss = 4.1345, diffLoss = 6.7803, kgLoss = 0.1658
2025-04-09 10:22:38.889255: Training Step 64/115: batchLoss = 4.4045, diffLoss = 7.2162, kgLoss = 0.1869
2025-04-09 10:22:39.644004: Training Step 65/115: batchLoss = 4.0430, diffLoss = 6.6251, kgLoss = 0.1698
2025-04-09 10:22:40.388494: Training Step 66/115: batchLoss = 4.1544, diffLoss = 6.8158, kgLoss = 0.1622
2025-04-09 10:22:41.126150: Training Step 67/115: batchLoss = 3.9022, diffLoss = 6.3909, kgLoss = 0.1691
2025-04-09 10:22:41.861952: Training Step 68/115: batchLoss = 4.2459, diffLoss = 6.9649, kgLoss = 0.1674
2025-04-09 10:22:42.596113: Training Step 69/115: batchLoss = 4.2740, diffLoss = 7.0116, kgLoss = 0.1675
2025-04-09 10:22:43.345685: Training Step 70/115: batchLoss = 4.3897, diffLoss = 7.1958, kgLoss = 0.1807
2025-04-09 10:22:44.076277: Training Step 71/115: batchLoss = 4.2487, diffLoss = 6.9653, kgLoss = 0.1737
2025-04-09 10:22:44.819000: Training Step 72/115: batchLoss = 4.0067, diffLoss = 6.5760, kgLoss = 0.1527
2025-04-09 10:22:45.568201: Training Step 73/115: batchLoss = 4.0213, diffLoss = 6.5893, kgLoss = 0.1693
2025-04-09 10:22:46.293400: Training Step 74/115: batchLoss = 3.8656, diffLoss = 6.3405, kgLoss = 0.1532
2025-04-09 10:22:47.029386: Training Step 75/115: batchLoss = 4.0897, diffLoss = 6.7008, kgLoss = 0.1730
2025-04-09 10:22:47.762173: Training Step 76/115: batchLoss = 5.2270, diffLoss = 8.5808, kgLoss = 0.1963
2025-04-09 10:22:48.485892: Training Step 77/115: batchLoss = 4.3903, diffLoss = 7.1934, kgLoss = 0.1857
2025-04-09 10:22:49.220548: Training Step 78/115: batchLoss = 4.2978, diffLoss = 7.0430, kgLoss = 0.1799
2025-04-09 10:22:49.958788: Training Step 79/115: batchLoss = 4.6127, diffLoss = 7.5667, kgLoss = 0.1816
2025-04-09 10:22:50.709445: Training Step 80/115: batchLoss = 4.5716, diffLoss = 7.4965, kgLoss = 0.1843
2025-04-09 10:22:51.460773: Training Step 81/115: batchLoss = 4.1328, diffLoss = 6.7783, kgLoss = 0.1645
2025-04-09 10:22:52.218752: Training Step 82/115: batchLoss = 4.2183, diffLoss = 6.9170, kgLoss = 0.1702
2025-04-09 10:22:52.962391: Training Step 83/115: batchLoss = 4.2247, diffLoss = 6.9295, kgLoss = 0.1674
2025-04-09 10:22:53.715819: Training Step 84/115: batchLoss = 4.1438, diffLoss = 6.7934, kgLoss = 0.1694
2025-04-09 10:22:54.449467: Training Step 85/115: batchLoss = 4.6563, diffLoss = 7.6336, kgLoss = 0.1905
2025-04-09 10:22:55.192606: Training Step 86/115: batchLoss = 4.3546, diffLoss = 7.1451, kgLoss = 0.1689
2025-04-09 10:22:55.940084: Training Step 87/115: batchLoss = 4.5724, diffLoss = 7.5023, kgLoss = 0.1775
2025-04-09 10:22:56.686067: Training Step 88/115: batchLoss = 4.1284, diffLoss = 6.7699, kgLoss = 0.1660
2025-04-09 10:22:57.425288: Training Step 89/115: batchLoss = 4.3534, diffLoss = 7.1279, kgLoss = 0.1916
2025-04-09 10:22:58.184258: Training Step 90/115: batchLoss = 4.7792, diffLoss = 7.8411, kgLoss = 0.1865
2025-04-09 10:22:58.938646: Training Step 91/115: batchLoss = 4.4597, diffLoss = 7.3048, kgLoss = 0.1920
2025-04-09 10:22:59.686266: Training Step 92/115: batchLoss = 4.4295, diffLoss = 7.2642, kgLoss = 0.1773
2025-04-09 10:23:00.424703: Training Step 93/115: batchLoss = 4.4051, diffLoss = 7.2278, kgLoss = 0.1710
2025-04-09 10:23:01.151722: Training Step 94/115: batchLoss = 4.8701, diffLoss = 7.9847, kgLoss = 0.1982
2025-04-09 10:23:01.897324: Training Step 95/115: batchLoss = 4.5287, diffLoss = 7.4238, kgLoss = 0.1861
2025-04-09 10:23:02.623207: Training Step 96/115: batchLoss = 4.1630, diffLoss = 6.8261, kgLoss = 0.1684
2025-04-09 10:23:03.360489: Training Step 97/115: batchLoss = 4.8483, diffLoss = 7.9553, kgLoss = 0.1877
2025-04-09 10:23:04.101419: Training Step 98/115: batchLoss = 4.6196, diffLoss = 7.5787, kgLoss = 0.1810
2025-04-09 10:23:04.839214: Training Step 99/115: batchLoss = 4.0694, diffLoss = 6.6689, kgLoss = 0.1701
2025-04-09 10:23:05.580305: Training Step 100/115: batchLoss = 4.1956, diffLoss = 6.8747, kgLoss = 0.1770
2025-04-09 10:23:06.327156: Training Step 101/115: batchLoss = 4.5548, diffLoss = 7.4688, kgLoss = 0.1838
2025-04-09 10:23:07.056981: Training Step 102/115: batchLoss = 4.4400, diffLoss = 7.2841, kgLoss = 0.1738
2025-04-09 10:23:07.792394: Training Step 103/115: batchLoss = 3.8827, diffLoss = 6.3572, kgLoss = 0.1709
2025-04-09 10:23:08.521354: Training Step 104/115: batchLoss = 4.1054, diffLoss = 6.7322, kgLoss = 0.1653
2025-04-09 10:23:09.252263: Training Step 105/115: batchLoss = 4.4521, diffLoss = 7.3051, kgLoss = 0.1727
2025-04-09 10:23:09.976767: Training Step 106/115: batchLoss = 3.9749, diffLoss = 6.5136, kgLoss = 0.1670
2025-04-09 10:23:10.708428: Training Step 107/115: batchLoss = 5.0520, diffLoss = 8.2959, kgLoss = 0.1860
2025-04-09 10:23:11.438841: Training Step 108/115: batchLoss = 4.5465, diffLoss = 7.4446, kgLoss = 0.1994
2025-04-09 10:23:12.169435: Training Step 109/115: batchLoss = 4.1274, diffLoss = 6.7653, kgLoss = 0.1705
2025-04-09 10:23:12.899890: Training Step 110/115: batchLoss = 3.9634, diffLoss = 6.4908, kgLoss = 0.1722
2025-04-09 10:23:13.626937: Training Step 111/115: batchLoss = 4.3565, diffLoss = 7.1477, kgLoss = 0.1697
2025-04-09 10:23:14.345039: Training Step 112/115: batchLoss = 4.2552, diffLoss = 6.9766, kgLoss = 0.1733
2025-04-09 10:23:14.982666: Training Step 113/115: batchLoss = 3.9775, diffLoss = 6.5178, kgLoss = 0.1669
2025-04-09 10:23:15.617384: Training Step 114/115: batchLoss = 4.5909, diffLoss = 7.5331, kgLoss = 0.1775
2025-04-09 10:23:15.746922: 
2025-04-09 10:23:15.747588: Epoch 27/1000, Train: epLoss = 1.2404, epDfLoss = 2.0339, epfTransLoss = 0.0000, epKgLoss = 0.0501  
2025-04-09 10:23:16.520070: Steps 0/90: batch_recall = 54.42, batch_ndcg = 42.43 
2025-04-09 10:23:17.255212: Steps 1/90: batch_recall = 59.42, batch_ndcg = 41.49 
2025-04-09 10:23:18.012656: Steps 2/90: batch_recall = 51.15, batch_ndcg = 37.24 
2025-04-09 10:23:18.758806: Steps 3/90: batch_recall = 46.87, batch_ndcg = 32.85 
2025-04-09 10:23:19.505128: Steps 4/90: batch_recall = 48.45, batch_ndcg = 36.56 
2025-04-09 10:23:20.254968: Steps 5/90: batch_recall = 38.54, batch_ndcg = 28.38 
2025-04-09 10:23:21.009056: Steps 6/90: batch_recall = 47.13, batch_ndcg = 33.44 
2025-04-09 10:23:21.778548: Steps 7/90: batch_recall = 42.78, batch_ndcg = 31.75 
2025-04-09 10:23:22.530011: Steps 8/90: batch_recall = 46.40, batch_ndcg = 33.40 
2025-04-09 10:23:23.296023: Steps 9/90: batch_recall = 44.73, batch_ndcg = 33.72 
2025-04-09 10:23:24.058036: Steps 10/90: batch_recall = 36.80, batch_ndcg = 27.63 
2025-04-09 10:23:24.804351: Steps 11/90: batch_recall = 43.13, batch_ndcg = 28.78 
2025-04-09 10:23:25.550365: Steps 12/90: batch_recall = 39.28, batch_ndcg = 27.76 
2025-04-09 10:23:26.310088: Steps 13/90: batch_recall = 36.63, batch_ndcg = 25.90 
2025-04-09 10:23:27.073323: Steps 14/90: batch_recall = 35.35, batch_ndcg = 26.48 
2025-04-09 10:23:27.820202: Steps 15/90: batch_recall = 46.64, batch_ndcg = 29.58 
2025-04-09 10:23:28.568765: Steps 16/90: batch_recall = 37.10, batch_ndcg = 26.57 
2025-04-09 10:23:29.323458: Steps 17/90: batch_recall = 35.28, batch_ndcg = 23.55 
2025-04-09 10:23:30.056796: Steps 18/90: batch_recall = 39.15, batch_ndcg = 26.00 
2025-04-09 10:23:30.785982: Steps 19/90: batch_recall = 34.83, batch_ndcg = 24.59 
2025-04-09 10:23:31.509410: Steps 20/90: batch_recall = 37.29, batch_ndcg = 25.40 
2025-04-09 10:23:32.246609: Steps 21/90: batch_recall = 41.94, batch_ndcg = 29.56 
2025-04-09 10:23:32.971205: Steps 22/90: batch_recall = 40.34, batch_ndcg = 28.31 
2025-04-09 10:23:33.686443: Steps 23/90: batch_recall = 37.74, batch_ndcg = 25.47 
2025-04-09 10:23:34.402336: Steps 24/90: batch_recall = 41.51, batch_ndcg = 27.81 
2025-04-09 10:23:35.125166: Steps 25/90: batch_recall = 38.51, batch_ndcg = 24.99 
2025-04-09 10:23:35.857781: Steps 26/90: batch_recall = 37.43, batch_ndcg = 27.84 
2025-04-09 10:23:36.573596: Steps 27/90: batch_recall = 36.82, batch_ndcg = 24.38 
2025-04-09 10:23:37.299996: Steps 28/90: batch_recall = 38.26, batch_ndcg = 25.24 
2025-04-09 10:23:38.052829: Steps 29/90: batch_recall = 34.42, batch_ndcg = 23.03 
2025-04-09 10:23:38.771616: Steps 30/90: batch_recall = 34.67, batch_ndcg = 22.20 
2025-04-09 10:23:39.495817: Steps 31/90: batch_recall = 34.47, batch_ndcg = 24.10 
2025-04-09 10:23:40.227455: Steps 32/90: batch_recall = 35.42, batch_ndcg = 23.52 
2025-04-09 10:23:40.936264: Steps 33/90: batch_recall = 36.35, batch_ndcg = 22.55 
2025-04-09 10:23:41.660022: Steps 34/90: batch_recall = 36.85, batch_ndcg = 25.00 
2025-04-09 10:23:42.380725: Steps 35/90: batch_recall = 37.74, batch_ndcg = 26.70 
2025-04-09 10:23:43.100988: Steps 36/90: batch_recall = 37.15, batch_ndcg = 24.64 
2025-04-09 10:23:43.819939: Steps 37/90: batch_recall = 33.43, batch_ndcg = 22.13 
2025-04-09 10:23:44.526748: Steps 38/90: batch_recall = 34.43, batch_ndcg = 22.30 
2025-04-09 10:23:45.239361: Steps 39/90: batch_recall = 36.98, batch_ndcg = 23.57 
2025-04-09 10:23:45.955721: Steps 40/90: batch_recall = 33.79, batch_ndcg = 23.30 
2025-04-09 10:23:46.652826: Steps 41/90: batch_recall = 35.17, batch_ndcg = 23.30 
2025-04-09 10:23:47.364422: Steps 42/90: batch_recall = 42.00, batch_ndcg = 29.28 
2025-04-09 10:23:48.082463: Steps 43/90: batch_recall = 36.65, batch_ndcg = 23.39 
2025-04-09 10:23:48.813224: Steps 44/90: batch_recall = 35.16, batch_ndcg = 22.91 
2025-04-09 10:23:49.553552: Steps 45/90: batch_recall = 32.59, batch_ndcg = 22.32 
2025-04-09 10:23:50.273867: Steps 46/90: batch_recall = 35.44, batch_ndcg = 21.57 
2025-04-09 10:23:50.980738: Steps 47/90: batch_recall = 31.33, batch_ndcg = 20.00 
2025-04-09 10:23:51.697215: Steps 48/90: batch_recall = 38.09, batch_ndcg = 23.64 
2025-04-09 10:23:52.403090: Steps 49/90: batch_recall = 32.67, batch_ndcg = 20.43 
2025-04-09 10:23:53.121453: Steps 50/90: batch_recall = 37.17, batch_ndcg = 24.91 
2025-04-09 10:23:53.843317: Steps 51/90: batch_recall = 38.64, batch_ndcg = 25.56 
2025-04-09 10:23:54.564175: Steps 52/90: batch_recall = 38.06, batch_ndcg = 23.87 
2025-04-09 10:23:55.300775: Steps 53/90: batch_recall = 40.03, batch_ndcg = 26.10 
2025-04-09 10:23:56.026731: Steps 54/90: batch_recall = 30.35, batch_ndcg = 19.55 
2025-04-09 10:23:56.740883: Steps 55/90: batch_recall = 36.52, batch_ndcg = 24.02 
2025-04-09 10:23:57.453659: Steps 56/90: batch_recall = 31.66, batch_ndcg = 18.96 
2025-04-09 10:23:58.181769: Steps 57/90: batch_recall = 38.43, batch_ndcg = 24.97 
2025-04-09 10:23:58.904960: Steps 58/90: batch_recall = 38.35, batch_ndcg = 23.91 
2025-04-09 10:23:59.604825: Steps 59/90: batch_recall = 32.64, batch_ndcg = 21.49 
2025-04-09 10:24:00.317304: Steps 60/90: batch_recall = 33.36, batch_ndcg = 21.02 
2025-04-09 10:24:01.025040: Steps 61/90: batch_recall = 40.72, batch_ndcg = 27.95 
2025-04-09 10:24:01.723024: Steps 62/90: batch_recall = 38.49, batch_ndcg = 25.47 
2025-04-09 10:24:02.416891: Steps 63/90: batch_recall = 35.93, batch_ndcg = 23.00 
2025-04-09 10:24:03.120940: Steps 64/90: batch_recall = 36.47, batch_ndcg = 23.53 
2025-04-09 10:24:03.839731: Steps 65/90: batch_recall = 32.00, batch_ndcg = 20.92 
2025-04-09 10:24:04.571040: Steps 66/90: batch_recall = 38.38, batch_ndcg = 23.93 
2025-04-09 10:24:05.285682: Steps 67/90: batch_recall = 34.18, batch_ndcg = 21.31 
2025-04-09 10:24:05.994700: Steps 68/90: batch_recall = 35.80, batch_ndcg = 21.87 
2025-04-09 10:24:06.712159: Steps 69/90: batch_recall = 40.71, batch_ndcg = 24.25 
2025-04-09 10:24:07.410617: Steps 70/90: batch_recall = 37.80, batch_ndcg = 24.64 
2025-04-09 10:24:08.120757: Steps 71/90: batch_recall = 38.77, batch_ndcg = 23.41 
2025-04-09 10:24:08.831827: Steps 72/90: batch_recall = 37.50, batch_ndcg = 23.35 
2025-04-09 10:24:09.552334: Steps 73/90: batch_recall = 35.11, batch_ndcg = 22.47 
2025-04-09 10:24:10.278213: Steps 74/90: batch_recall = 40.89, batch_ndcg = 25.41 
2025-04-09 10:24:10.989360: Steps 75/90: batch_recall = 38.08, batch_ndcg = 24.02 
2025-04-09 10:24:11.693704: Steps 76/90: batch_recall = 42.55, batch_ndcg = 26.42 
2025-04-09 10:24:12.409133: Steps 77/90: batch_recall = 46.57, batch_ndcg = 27.95 
2025-04-09 10:24:13.111269: Steps 78/90: batch_recall = 33.29, batch_ndcg = 20.31 
2025-04-09 10:24:13.819877: Steps 79/90: batch_recall = 38.51, batch_ndcg = 26.52 
2025-04-09 10:24:14.510807: Steps 80/90: batch_recall = 44.47, batch_ndcg = 28.35 
2025-04-09 10:24:15.207507: Steps 81/90: batch_recall = 43.89, batch_ndcg = 27.35 
2025-04-09 10:24:15.905640: Steps 82/90: batch_recall = 40.40, batch_ndcg = 26.00 
2025-04-09 10:24:16.602891: Steps 83/90: batch_recall = 42.54, batch_ndcg = 26.35 
2025-04-09 10:24:17.317575: Steps 84/90: batch_recall = 34.26, batch_ndcg = 21.32 
2025-04-09 10:24:18.021970: Steps 85/90: batch_recall = 40.81, batch_ndcg = 23.58 
2025-04-09 10:24:18.726888: Steps 86/90: batch_recall = 43.97, batch_ndcg = 28.10 
2025-04-09 10:24:19.438263: Steps 87/90: batch_recall = 44.41, batch_ndcg = 29.76 
2025-04-09 10:24:20.132203: Steps 88/90: batch_recall = 49.99, batch_ndcg = 28.47 
2025-04-09 10:24:20.697938: Steps 89/90: batch_recall = 39.71, batch_ndcg = 25.38 
2025-04-09 10:24:20.698654: Epoch 27/1000, Test: Recall = 0.0771, NDCG = 0.0506  

2025-04-09 10:24:21.728128: Training Step 0/115: batchLoss = 4.7967, diffLoss = 7.8697, kgLoss = 0.1871
2025-04-09 10:24:22.468382: Training Step 1/115: batchLoss = 4.1883, diffLoss = 6.8687, kgLoss = 0.1676
2025-04-09 10:24:23.217763: Training Step 2/115: batchLoss = 4.2909, diffLoss = 7.0410, kgLoss = 0.1658
2025-04-09 10:24:23.955023: Training Step 3/115: batchLoss = 4.1068, diffLoss = 6.7359, kgLoss = 0.1632
2025-04-09 10:24:24.686956: Training Step 4/115: batchLoss = 4.1282, diffLoss = 6.7639, kgLoss = 0.1745
2025-04-09 10:24:25.410109: Training Step 5/115: batchLoss = 4.9745, diffLoss = 8.1436, kgLoss = 0.2208
2025-04-09 10:24:26.151328: Training Step 6/115: batchLoss = 5.2264, diffLoss = 8.5776, kgLoss = 0.1996
2025-04-09 10:24:26.894933: Training Step 7/115: batchLoss = 4.5868, diffLoss = 7.5224, kgLoss = 0.1836
2025-04-09 10:24:27.630356: Training Step 8/115: batchLoss = 4.3447, diffLoss = 7.1265, kgLoss = 0.1719
2025-04-09 10:24:28.384740: Training Step 9/115: batchLoss = 3.9399, diffLoss = 6.4581, kgLoss = 0.1626
2025-04-09 10:24:29.140896: Training Step 10/115: batchLoss = 4.5720, diffLoss = 7.4918, kgLoss = 0.1923
2025-04-09 10:24:29.882273: Training Step 11/115: batchLoss = 4.7892, diffLoss = 7.8541, kgLoss = 0.1918
2025-04-09 10:24:30.624995: Training Step 12/115: batchLoss = 4.2544, diffLoss = 6.9796, kgLoss = 0.1668
2025-04-09 10:24:31.356904: Training Step 13/115: batchLoss = 4.0665, diffLoss = 6.6665, kgLoss = 0.1665
2025-04-09 10:24:32.090826: Training Step 14/115: batchLoss = 4.2193, diffLoss = 6.9240, kgLoss = 0.1623
2025-04-09 10:24:32.816366: Training Step 15/115: batchLoss = 4.3601, diffLoss = 7.1493, kgLoss = 0.1762
2025-04-09 10:24:33.561823: Training Step 16/115: batchLoss = 4.0320, diffLoss = 6.6128, kgLoss = 0.1607
2025-04-09 10:24:34.297944: Training Step 17/115: batchLoss = 4.3985, diffLoss = 7.2165, kgLoss = 0.1714
2025-04-09 10:24:35.042594: Training Step 18/115: batchLoss = 4.8149, diffLoss = 7.8990, kgLoss = 0.1886
2025-04-09 10:24:35.793400: Training Step 19/115: batchLoss = 4.4221, diffLoss = 7.2569, kgLoss = 0.1698
2025-04-09 10:24:36.540746: Training Step 20/115: batchLoss = 4.0364, diffLoss = 6.6174, kgLoss = 0.1649
2025-04-09 10:24:37.284460: Training Step 21/115: batchLoss = 4.0672, diffLoss = 6.6706, kgLoss = 0.1620
2025-04-09 10:24:38.031794: Training Step 22/115: batchLoss = 3.9289, diffLoss = 6.4305, kgLoss = 0.1765
2025-04-09 10:24:38.774330: Training Step 23/115: batchLoss = 4.1311, diffLoss = 6.7722, kgLoss = 0.1694
2025-04-09 10:24:39.514999: Training Step 24/115: batchLoss = 4.1554, diffLoss = 6.8068, kgLoss = 0.1783
2025-04-09 10:24:40.252776: Training Step 25/115: batchLoss = 4.6212, diffLoss = 7.5729, kgLoss = 0.1937
2025-04-09 10:24:40.986904: Training Step 26/115: batchLoss = 4.4223, diffLoss = 7.2462, kgLoss = 0.1864
2025-04-09 10:24:41.726597: Training Step 27/115: batchLoss = 4.9756, diffLoss = 8.1602, kgLoss = 0.1987
2025-04-09 10:24:42.464644: Training Step 28/115: batchLoss = 4.0067, diffLoss = 6.5698, kgLoss = 0.1621
2025-04-09 10:24:43.205835: Training Step 29/115: batchLoss = 4.0190, diffLoss = 6.5777, kgLoss = 0.1809
2025-04-09 10:24:43.943016: Training Step 30/115: batchLoss = 4.0822, diffLoss = 6.6911, kgLoss = 0.1689
2025-04-09 10:24:44.678468: Training Step 31/115: batchLoss = 5.1248, diffLoss = 8.4130, kgLoss = 0.1925
2025-04-09 10:24:45.411530: Training Step 32/115: batchLoss = 4.5393, diffLoss = 7.4426, kgLoss = 0.1842
2025-04-09 10:24:46.147663: Training Step 33/115: batchLoss = 4.3988, diffLoss = 7.2144, kgLoss = 0.1754
2025-04-09 10:24:46.886267: Training Step 34/115: batchLoss = 4.1705, diffLoss = 6.8409, kgLoss = 0.1650
2025-04-09 10:24:47.616024: Training Step 35/115: batchLoss = 3.9593, diffLoss = 6.4895, kgLoss = 0.1639
2025-04-09 10:24:48.361060: Training Step 36/115: batchLoss = 4.4065, diffLoss = 7.2264, kgLoss = 0.1766
2025-04-09 10:24:49.098428: Training Step 37/115: batchLoss = 4.1226, diffLoss = 6.7583, kgLoss = 0.1689
2025-04-09 10:24:49.833354: Training Step 38/115: batchLoss = 3.8559, diffLoss = 6.3244, kgLoss = 0.1532
2025-04-09 10:24:50.578334: Training Step 39/115: batchLoss = 3.9094, diffLoss = 6.4106, kgLoss = 0.1576
2025-04-09 10:24:51.313090: Training Step 40/115: batchLoss = 4.1538, diffLoss = 6.8093, kgLoss = 0.1707
2025-04-09 10:24:52.053181: Training Step 41/115: batchLoss = 4.0763, diffLoss = 6.6798, kgLoss = 0.1711
2025-04-09 10:24:52.808648: Training Step 42/115: batchLoss = 4.0393, diffLoss = 6.6201, kgLoss = 0.1681
2025-04-09 10:24:53.556233: Training Step 43/115: batchLoss = 4.1862, diffLoss = 6.8651, kgLoss = 0.1679
2025-04-09 10:24:54.310650: Training Step 44/115: batchLoss = 4.2889, diffLoss = 7.0413, kgLoss = 0.1603
2025-04-09 10:24:55.062849: Training Step 45/115: batchLoss = 3.9956, diffLoss = 6.5465, kgLoss = 0.1692
2025-04-09 10:24:55.825333: Training Step 46/115: batchLoss = 4.5539, diffLoss = 7.4712, kgLoss = 0.1781
2025-04-09 10:24:56.590919: Training Step 47/115: batchLoss = 4.7710, diffLoss = 7.8155, kgLoss = 0.2043
2025-04-09 10:24:57.338723: Training Step 48/115: batchLoss = 4.4538, diffLoss = 7.3054, kgLoss = 0.1765
2025-04-09 10:24:58.083288: Training Step 49/115: batchLoss = 4.4738, diffLoss = 7.3316, kgLoss = 0.1870
2025-04-09 10:24:58.838478: Training Step 50/115: batchLoss = 4.4418, diffLoss = 7.2913, kgLoss = 0.1676
2025-04-09 10:24:59.568535: Training Step 51/115: batchLoss = 3.7588, diffLoss = 6.1563, kgLoss = 0.1625
2025-04-09 10:25:00.313705: Training Step 52/115: batchLoss = 3.9989, diffLoss = 6.5568, kgLoss = 0.1619
2025-04-09 10:25:01.040504: Training Step 53/115: batchLoss = 4.2041, diffLoss = 6.8906, kgLoss = 0.1744
2025-04-09 10:25:01.783018: Training Step 54/115: batchLoss = 4.3589, diffLoss = 7.1493, kgLoss = 0.1732
2025-04-09 10:25:02.522235: Training Step 55/115: batchLoss = 4.5755, diffLoss = 7.5083, kgLoss = 0.1763
2025-04-09 10:25:03.255003: Training Step 56/115: batchLoss = 4.3364, diffLoss = 7.1113, kgLoss = 0.1741
2025-04-09 10:25:03.992256: Training Step 57/115: batchLoss = 3.9125, diffLoss = 6.3963, kgLoss = 0.1868
2025-04-09 10:25:04.739445: Training Step 58/115: batchLoss = 4.2449, diffLoss = 6.9590, kgLoss = 0.1737
2025-04-09 10:25:05.477280: Training Step 59/115: batchLoss = 4.1554, diffLoss = 6.8116, kgLoss = 0.1710
2025-04-09 10:25:06.223038: Training Step 60/115: batchLoss = 4.4107, diffLoss = 7.2370, kgLoss = 0.1711
2025-04-09 10:25:06.964364: Training Step 61/115: batchLoss = 4.0432, diffLoss = 6.6227, kgLoss = 0.1739
2025-04-09 10:25:07.712189: Training Step 62/115: batchLoss = 3.9591, diffLoss = 6.4896, kgLoss = 0.1633
2025-04-09 10:25:08.456162: Training Step 63/115: batchLoss = 4.2377, diffLoss = 6.9501, kgLoss = 0.1691
2025-04-09 10:25:09.201068: Training Step 64/115: batchLoss = 4.9665, diffLoss = 8.1481, kgLoss = 0.1941
2025-04-09 10:25:09.955406: Training Step 65/115: batchLoss = 3.9931, diffLoss = 6.5519, kgLoss = 0.1550
2025-04-09 10:25:10.703290: Training Step 66/115: batchLoss = 3.6784, diffLoss = 6.0285, kgLoss = 0.1534
2025-04-09 10:25:11.442557: Training Step 67/115: batchLoss = 4.1627, diffLoss = 6.8250, kgLoss = 0.1692
2025-04-09 10:25:12.191522: Training Step 68/115: batchLoss = 4.2222, diffLoss = 6.9225, kgLoss = 0.1718
2025-04-09 10:25:12.935761: Training Step 69/115: batchLoss = 4.5628, diffLoss = 7.4782, kgLoss = 0.1896
2025-04-09 10:25:13.664548: Training Step 70/115: batchLoss = 4.5495, diffLoss = 7.4607, kgLoss = 0.1825
2025-04-09 10:25:14.396135: Training Step 71/115: batchLoss = 3.8282, diffLoss = 6.2792, kgLoss = 0.1517
2025-04-09 10:25:15.131918: Training Step 72/115: batchLoss = 4.5043, diffLoss = 7.3893, kgLoss = 0.1767
2025-04-09 10:25:15.875576: Training Step 73/115: batchLoss = 4.5585, diffLoss = 7.4772, kgLoss = 0.1805
2025-04-09 10:25:16.605988: Training Step 74/115: batchLoss = 3.9599, diffLoss = 6.4992, kgLoss = 0.1511
2025-04-09 10:25:17.352685: Training Step 75/115: batchLoss = 3.7586, diffLoss = 6.1573, kgLoss = 0.1604
2025-04-09 10:25:18.117932: Training Step 76/115: batchLoss = 4.4195, diffLoss = 7.2460, kgLoss = 0.1797
2025-04-09 10:25:18.865898: Training Step 77/115: batchLoss = 4.7074, diffLoss = 7.7213, kgLoss = 0.1865
2025-04-09 10:25:19.713477: Training Step 78/115: batchLoss = 4.8987, diffLoss = 8.0384, kgLoss = 0.1892
2025-04-09 10:25:20.446623: Training Step 79/115: batchLoss = 3.9437, diffLoss = 6.4600, kgLoss = 0.1693
2025-04-09 10:25:21.194026: Training Step 80/115: batchLoss = 4.2854, diffLoss = 7.0272, kgLoss = 0.1726
2025-04-09 10:25:21.937523: Training Step 81/115: batchLoss = 4.0120, diffLoss = 6.5794, kgLoss = 0.1609
2025-04-09 10:25:22.666098: Training Step 82/115: batchLoss = 4.1744, diffLoss = 6.8422, kgLoss = 0.1727
2025-04-09 10:25:23.413027: Training Step 83/115: batchLoss = 4.5533, diffLoss = 7.4658, kgLoss = 0.1845
2025-04-09 10:25:24.154765: Training Step 84/115: batchLoss = 3.9275, diffLoss = 6.4383, kgLoss = 0.1614
2025-04-09 10:25:24.888172: Training Step 85/115: batchLoss = 3.9593, diffLoss = 6.4973, kgLoss = 0.1524
2025-04-09 10:25:25.620519: Training Step 86/115: batchLoss = 4.3994, diffLoss = 7.2086, kgLoss = 0.1857
2025-04-09 10:25:26.367154: Training Step 87/115: batchLoss = 4.7538, diffLoss = 7.7992, kgLoss = 0.1856
2025-04-09 10:25:27.123209: Training Step 88/115: batchLoss = 4.2225, diffLoss = 6.9302, kgLoss = 0.1608
2025-04-09 10:25:27.858276: Training Step 89/115: batchLoss = 3.7044, diffLoss = 6.0726, kgLoss = 0.1521
2025-04-09 10:25:28.591229: Training Step 90/115: batchLoss = 4.3221, diffLoss = 7.0915, kgLoss = 0.1679
2025-04-09 10:25:29.327397: Training Step 91/115: batchLoss = 3.8900, diffLoss = 6.3779, kgLoss = 0.1581
2025-04-09 10:25:30.052432: Training Step 92/115: batchLoss = 4.2082, diffLoss = 6.8999, kgLoss = 0.1706
2025-04-09 10:25:30.789315: Training Step 93/115: batchLoss = 4.4669, diffLoss = 7.3274, kgLoss = 0.1763
2025-04-09 10:25:31.519999: Training Step 94/115: batchLoss = 3.9411, diffLoss = 6.4610, kgLoss = 0.1611
2025-04-09 10:25:32.260204: Training Step 95/115: batchLoss = 4.5190, diffLoss = 7.4130, kgLoss = 0.1779
2025-04-09 10:25:32.990752: Training Step 96/115: batchLoss = 3.9592, diffLoss = 6.4855, kgLoss = 0.1699
2025-04-09 10:25:33.746621: Training Step 97/115: batchLoss = 5.4033, diffLoss = 8.8648, kgLoss = 0.2111
2025-04-09 10:25:34.493477: Training Step 98/115: batchLoss = 4.3064, diffLoss = 7.0586, kgLoss = 0.1782
2025-04-09 10:25:35.242757: Training Step 99/115: batchLoss = 4.0577, diffLoss = 6.6548, kgLoss = 0.1620
2025-04-09 10:25:35.996372: Training Step 100/115: batchLoss = 3.9828, diffLoss = 6.5314, kgLoss = 0.1599
2025-04-09 10:25:36.735352: Training Step 101/115: batchLoss = 4.1468, diffLoss = 6.8012, kgLoss = 0.1652
2025-04-09 10:25:37.471966: Training Step 102/115: batchLoss = 4.6053, diffLoss = 7.5472, kgLoss = 0.1923
2025-04-09 10:25:38.208926: Training Step 103/115: batchLoss = 4.7695, diffLoss = 7.8271, kgLoss = 0.1832
2025-04-09 10:25:38.945842: Training Step 104/115: batchLoss = 4.6982, diffLoss = 7.7058, kgLoss = 0.1868
2025-04-09 10:25:39.682690: Training Step 105/115: batchLoss = 4.1202, diffLoss = 6.7515, kgLoss = 0.1731
2025-04-09 10:25:40.411137: Training Step 106/115: batchLoss = 4.1217, diffLoss = 6.7577, kgLoss = 0.1676
2025-04-09 10:25:41.140708: Training Step 107/115: batchLoss = 5.1145, diffLoss = 8.3955, kgLoss = 0.1930
2025-04-09 10:25:41.863780: Training Step 108/115: batchLoss = 4.7485, diffLoss = 7.7852, kgLoss = 0.1934
2025-04-09 10:25:42.592334: Training Step 109/115: batchLoss = 4.4587, diffLoss = 7.3081, kgLoss = 0.1847
2025-04-09 10:25:43.313139: Training Step 110/115: batchLoss = 4.6431, diffLoss = 7.6152, kgLoss = 0.1849
2025-04-09 10:25:44.045242: Training Step 111/115: batchLoss = 4.3857, diffLoss = 7.1935, kgLoss = 0.1741
2025-04-09 10:25:44.770325: Training Step 112/115: batchLoss = 4.0961, diffLoss = 6.7206, kgLoss = 0.1592
2025-04-09 10:25:45.417791: Training Step 113/115: batchLoss = 4.5555, diffLoss = 7.4649, kgLoss = 0.1914
2025-04-09 10:25:46.026343: Training Step 114/115: batchLoss = 4.0120, diffLoss = 6.5803, kgLoss = 0.1596
2025-04-09 10:25:46.138292: 
2025-04-09 10:25:46.139327: Epoch 28/1000, Train: epLoss = 1.2383, epDfLoss = 2.0304, epfTransLoss = 0.0000, epKgLoss = 0.0501  
2025-04-09 10:25:46.913466: Steps 0/90: batch_recall = 54.06, batch_ndcg = 42.42 
2025-04-09 10:25:47.654338: Steps 1/90: batch_recall = 58.08, batch_ndcg = 41.38 
2025-04-09 10:25:48.404612: Steps 2/90: batch_recall = 51.07, batch_ndcg = 37.25 
2025-04-09 10:25:49.143919: Steps 3/90: batch_recall = 46.37, batch_ndcg = 32.83 
2025-04-09 10:25:49.888790: Steps 4/90: batch_recall = 47.69, batch_ndcg = 36.22 
2025-04-09 10:25:50.635221: Steps 5/90: batch_recall = 38.38, batch_ndcg = 28.88 
2025-04-09 10:25:51.382834: Steps 6/90: batch_recall = 46.69, batch_ndcg = 33.25 
2025-04-09 10:25:52.153382: Steps 7/90: batch_recall = 43.09, batch_ndcg = 31.80 
2025-04-09 10:25:52.896975: Steps 8/90: batch_recall = 48.21, batch_ndcg = 33.95 
2025-04-09 10:25:53.651637: Steps 9/90: batch_recall = 45.63, batch_ndcg = 33.82 
2025-04-09 10:25:54.395454: Steps 10/90: batch_recall = 37.17, batch_ndcg = 28.18 
2025-04-09 10:25:55.142985: Steps 11/90: batch_recall = 43.86, batch_ndcg = 29.32 
2025-04-09 10:25:55.876040: Steps 12/90: batch_recall = 38.58, batch_ndcg = 27.87 
2025-04-09 10:25:56.611546: Steps 13/90: batch_recall = 36.64, batch_ndcg = 25.84 
2025-04-09 10:25:57.346995: Steps 14/90: batch_recall = 35.43, batch_ndcg = 26.33 
2025-04-09 10:25:58.084102: Steps 15/90: batch_recall = 45.73, batch_ndcg = 29.39 
2025-04-09 10:25:58.837576: Steps 16/90: batch_recall = 37.98, batch_ndcg = 26.89 
2025-04-09 10:25:59.594968: Steps 17/90: batch_recall = 35.96, batch_ndcg = 23.58 
2025-04-09 10:26:00.323736: Steps 18/90: batch_recall = 40.02, batch_ndcg = 26.73 
2025-04-09 10:26:01.060060: Steps 19/90: batch_recall = 34.21, batch_ndcg = 23.84 
2025-04-09 10:26:01.783725: Steps 20/90: batch_recall = 36.59, batch_ndcg = 25.20 
2025-04-09 10:26:02.508501: Steps 21/90: batch_recall = 42.10, batch_ndcg = 29.66 
2025-04-09 10:26:03.222070: Steps 22/90: batch_recall = 40.16, batch_ndcg = 28.37 
2025-04-09 10:26:03.954723: Steps 23/90: batch_recall = 38.29, batch_ndcg = 25.84 
2025-04-09 10:26:04.687673: Steps 24/90: batch_recall = 40.59, batch_ndcg = 27.62 
2025-04-09 10:26:05.396787: Steps 25/90: batch_recall = 38.69, batch_ndcg = 25.44 
2025-04-09 10:26:06.118081: Steps 26/90: batch_recall = 38.28, batch_ndcg = 28.22 
2025-04-09 10:26:06.850656: Steps 27/90: batch_recall = 36.38, batch_ndcg = 24.24 
2025-04-09 10:26:07.585350: Steps 28/90: batch_recall = 40.17, batch_ndcg = 25.93 
2025-04-09 10:26:08.308235: Steps 29/90: batch_recall = 34.43, batch_ndcg = 23.38 
2025-04-09 10:26:09.026292: Steps 30/90: batch_recall = 34.49, batch_ndcg = 22.27 
2025-04-09 10:26:09.765668: Steps 31/90: batch_recall = 32.99, batch_ndcg = 23.69 
2025-04-09 10:26:10.483259: Steps 32/90: batch_recall = 36.65, batch_ndcg = 24.13 
2025-04-09 10:26:11.197644: Steps 33/90: batch_recall = 36.83, batch_ndcg = 22.54 
2025-04-09 10:26:11.919823: Steps 34/90: batch_recall = 37.25, batch_ndcg = 25.24 
2025-04-09 10:26:12.643753: Steps 35/90: batch_recall = 38.43, batch_ndcg = 26.94 
2025-04-09 10:26:13.376131: Steps 36/90: batch_recall = 38.42, batch_ndcg = 25.06 
2025-04-09 10:26:14.103715: Steps 37/90: batch_recall = 33.88, batch_ndcg = 22.51 
2025-04-09 10:26:14.825992: Steps 38/90: batch_recall = 34.73, batch_ndcg = 22.75 
2025-04-09 10:26:15.534276: Steps 39/90: batch_recall = 36.69, batch_ndcg = 23.50 
2025-04-09 10:26:16.245693: Steps 40/90: batch_recall = 34.15, batch_ndcg = 23.28 
2025-04-09 10:26:16.947193: Steps 41/90: batch_recall = 35.83, batch_ndcg = 23.27 
2025-04-09 10:26:17.695468: Steps 42/90: batch_recall = 42.28, batch_ndcg = 29.35 
2025-04-09 10:26:18.409846: Steps 43/90: batch_recall = 37.39, batch_ndcg = 23.70 
2025-04-09 10:26:19.129500: Steps 44/90: batch_recall = 34.09, batch_ndcg = 22.49 
2025-04-09 10:26:19.833629: Steps 45/90: batch_recall = 32.10, batch_ndcg = 22.10 
2025-04-09 10:26:20.547075: Steps 46/90: batch_recall = 36.25, batch_ndcg = 22.03 
2025-04-09 10:26:21.273167: Steps 47/90: batch_recall = 31.66, batch_ndcg = 20.14 
2025-04-09 10:26:21.978611: Steps 48/90: batch_recall = 37.05, batch_ndcg = 23.51 
2025-04-09 10:26:22.694550: Steps 49/90: batch_recall = 32.24, batch_ndcg = 20.08 
2025-04-09 10:26:23.394442: Steps 50/90: batch_recall = 38.68, batch_ndcg = 25.88 
2025-04-09 10:26:24.115233: Steps 51/90: batch_recall = 39.70, batch_ndcg = 25.86 
2025-04-09 10:26:24.821141: Steps 52/90: batch_recall = 38.21, batch_ndcg = 24.08 
2025-04-09 10:26:25.531781: Steps 53/90: batch_recall = 40.38, batch_ndcg = 26.24 
2025-04-09 10:26:26.246667: Steps 54/90: batch_recall = 30.40, batch_ndcg = 19.72 
2025-04-09 10:26:26.957304: Steps 55/90: batch_recall = 36.30, batch_ndcg = 24.18 
2025-04-09 10:26:27.675527: Steps 56/90: batch_recall = 31.72, batch_ndcg = 19.12 
2025-04-09 10:26:28.381648: Steps 57/90: batch_recall = 38.36, batch_ndcg = 24.82 
2025-04-09 10:26:29.085270: Steps 58/90: batch_recall = 38.17, batch_ndcg = 24.12 
2025-04-09 10:26:29.786526: Steps 59/90: batch_recall = 33.66, batch_ndcg = 21.87 
2025-04-09 10:26:30.491554: Steps 60/90: batch_recall = 33.62, batch_ndcg = 21.29 
2025-04-09 10:26:31.198708: Steps 61/90: batch_recall = 41.05, batch_ndcg = 28.06 
2025-04-09 10:26:31.910061: Steps 62/90: batch_recall = 38.17, batch_ndcg = 25.47 
2025-04-09 10:26:32.651859: Steps 63/90: batch_recall = 36.10, batch_ndcg = 22.96 
2025-04-09 10:26:33.361918: Steps 64/90: batch_recall = 36.93, batch_ndcg = 23.17 
2025-04-09 10:26:34.076045: Steps 65/90: batch_recall = 33.02, batch_ndcg = 21.33 
2025-04-09 10:26:34.777883: Steps 66/90: batch_recall = 38.54, batch_ndcg = 24.09 
2025-04-09 10:26:35.471054: Steps 67/90: batch_recall = 33.60, batch_ndcg = 21.42 
2025-04-09 10:26:36.175194: Steps 68/90: batch_recall = 36.01, batch_ndcg = 21.92 
2025-04-09 10:26:36.879393: Steps 69/90: batch_recall = 41.54, batch_ndcg = 24.77 
2025-04-09 10:26:37.585658: Steps 70/90: batch_recall = 38.37, batch_ndcg = 24.99 
2025-04-09 10:26:38.290956: Steps 71/90: batch_recall = 38.30, batch_ndcg = 23.36 
2025-04-09 10:26:38.979488: Steps 72/90: batch_recall = 36.37, batch_ndcg = 23.05 
2025-04-09 10:26:39.693087: Steps 73/90: batch_recall = 35.09, batch_ndcg = 22.58 
2025-04-09 10:26:40.412717: Steps 74/90: batch_recall = 40.61, batch_ndcg = 25.53 
2025-04-09 10:26:41.134577: Steps 75/90: batch_recall = 38.51, batch_ndcg = 24.02 
2025-04-09 10:26:41.832176: Steps 76/90: batch_recall = 42.00, batch_ndcg = 26.66 
2025-04-09 10:26:42.531959: Steps 77/90: batch_recall = 45.40, batch_ndcg = 27.85 
2025-04-09 10:26:43.230799: Steps 78/90: batch_recall = 34.46, batch_ndcg = 20.77 
2025-04-09 10:26:43.946594: Steps 79/90: batch_recall = 39.27, batch_ndcg = 26.87 
2025-04-09 10:26:44.646076: Steps 80/90: batch_recall = 45.42, batch_ndcg = 28.50 
2025-04-09 10:26:45.358252: Steps 81/90: batch_recall = 45.53, batch_ndcg = 27.69 
2025-04-09 10:26:46.060396: Steps 82/90: batch_recall = 41.54, batch_ndcg = 26.70 
2025-04-09 10:26:46.747231: Steps 83/90: batch_recall = 43.66, batch_ndcg = 26.95 
2025-04-09 10:26:47.461856: Steps 84/90: batch_recall = 35.81, batch_ndcg = 21.78 
2025-04-09 10:26:48.155194: Steps 85/90: batch_recall = 41.70, batch_ndcg = 24.09 
2025-04-09 10:26:48.862356: Steps 86/90: batch_recall = 43.16, batch_ndcg = 27.93 
2025-04-09 10:26:49.571687: Steps 87/90: batch_recall = 42.39, batch_ndcg = 28.81 
2025-04-09 10:26:50.282702: Steps 88/90: batch_recall = 49.37, batch_ndcg = 28.71 
2025-04-09 10:26:50.831080: Steps 89/90: batch_recall = 40.66, batch_ndcg = 25.62 
2025-04-09 10:26:50.832083: Epoch 28/1000, Test: Recall = 0.0771, NDCG = 0.0509  

2025-04-09 10:26:51.861103: Training Step 0/115: batchLoss = 4.1510, diffLoss = 6.8030, kgLoss = 0.1731
2025-04-09 10:26:52.692332: Training Step 1/115: batchLoss = 4.3743, diffLoss = 7.1693, kgLoss = 0.1817
2025-04-09 10:26:53.423208: Training Step 2/115: batchLoss = 4.6949, diffLoss = 7.7022, kgLoss = 0.1839
2025-04-09 10:26:54.246777: Training Step 3/115: batchLoss = 4.4945, diffLoss = 7.3739, kgLoss = 0.1755
2025-04-09 10:26:54.978002: Training Step 4/115: batchLoss = 4.7959, diffLoss = 7.8676, kgLoss = 0.1885
2025-04-09 10:26:55.700238: Training Step 5/115: batchLoss = 4.0373, diffLoss = 6.6267, kgLoss = 0.1533
2025-04-09 10:26:56.425531: Training Step 6/115: batchLoss = 4.2165, diffLoss = 6.9124, kgLoss = 0.1726
2025-04-09 10:26:57.144958: Training Step 7/115: batchLoss = 4.3901, diffLoss = 7.1959, kgLoss = 0.1815
2025-04-09 10:26:57.876330: Training Step 8/115: batchLoss = 4.2859, diffLoss = 7.0285, kgLoss = 0.1720
2025-04-09 10:26:58.603072: Training Step 9/115: batchLoss = 3.9726, diffLoss = 6.5095, kgLoss = 0.1672
2025-04-09 10:26:59.324596: Training Step 10/115: batchLoss = 4.1213, diffLoss = 6.7608, kgLoss = 0.1621
2025-04-09 10:27:00.048781: Training Step 11/115: batchLoss = 4.0514, diffLoss = 6.6386, kgLoss = 0.1705
2025-04-09 10:27:00.768610: Training Step 12/115: batchLoss = 4.4073, diffLoss = 7.2262, kgLoss = 0.1789
2025-04-09 10:27:01.495635: Training Step 13/115: batchLoss = 4.2953, diffLoss = 7.0436, kgLoss = 0.1728
2025-04-09 10:27:02.214372: Training Step 14/115: batchLoss = 4.3161, diffLoss = 7.0771, kgLoss = 0.1745
2025-04-09 10:27:02.944046: Training Step 15/115: batchLoss = 4.4838, diffLoss = 7.3485, kgLoss = 0.1866
2025-04-09 10:27:03.679180: Training Step 16/115: batchLoss = 4.0295, diffLoss = 6.6031, kgLoss = 0.1691
2025-04-09 10:27:04.407836: Training Step 17/115: batchLoss = 4.1325, diffLoss = 6.7779, kgLoss = 0.1643
2025-04-09 10:27:05.143161: Training Step 18/115: batchLoss = 4.0687, diffLoss = 6.6690, kgLoss = 0.1682
2025-04-09 10:27:05.868386: Training Step 19/115: batchLoss = 3.9599, diffLoss = 6.4894, kgLoss = 0.1657
2025-04-09 10:27:06.612635: Training Step 20/115: batchLoss = 4.6273, diffLoss = 7.5866, kgLoss = 0.1884
2025-04-09 10:27:07.346680: Training Step 21/115: batchLoss = 5.0083, diffLoss = 8.2127, kgLoss = 0.2018
2025-04-09 10:27:08.082245: Training Step 22/115: batchLoss = 4.4071, diffLoss = 7.2266, kgLoss = 0.1778
2025-04-09 10:27:08.822372: Training Step 23/115: batchLoss = 4.6442, diffLoss = 7.6211, kgLoss = 0.1790
2025-04-09 10:27:09.549242: Training Step 24/115: batchLoss = 4.1363, diffLoss = 6.7793, kgLoss = 0.1717
2025-04-09 10:27:10.276265: Training Step 25/115: batchLoss = 3.9179, diffLoss = 6.4241, kgLoss = 0.1587
2025-04-09 10:27:11.015887: Training Step 26/115: batchLoss = 4.1070, diffLoss = 6.7313, kgLoss = 0.1705
2025-04-09 10:27:11.750894: Training Step 27/115: batchLoss = 4.2948, diffLoss = 7.0465, kgLoss = 0.1673
2025-04-09 10:27:12.489699: Training Step 28/115: batchLoss = 4.2696, diffLoss = 7.0016, kgLoss = 0.1717
2025-04-09 10:27:13.220161: Training Step 29/115: batchLoss = 4.1979, diffLoss = 6.8812, kgLoss = 0.1729
2025-04-09 10:27:13.961143: Training Step 30/115: batchLoss = 4.0818, diffLoss = 6.6805, kgLoss = 0.1838
2025-04-09 10:27:14.695619: Training Step 31/115: batchLoss = 3.8680, diffLoss = 6.3425, kgLoss = 0.1562
2025-04-09 10:27:15.431183: Training Step 32/115: batchLoss = 4.3038, diffLoss = 7.0454, kgLoss = 0.1913
2025-04-09 10:27:16.159784: Training Step 33/115: batchLoss = 4.5692, diffLoss = 7.4868, kgLoss = 0.1926
2025-04-09 10:27:16.896053: Training Step 34/115: batchLoss = 4.0861, diffLoss = 6.6960, kgLoss = 0.1714
2025-04-09 10:27:17.633482: Training Step 35/115: batchLoss = 4.1542, diffLoss = 6.8111, kgLoss = 0.1688
2025-04-09 10:27:18.389425: Training Step 36/115: batchLoss = 4.3959, diffLoss = 7.2055, kgLoss = 0.1814
2025-04-09 10:27:19.133266: Training Step 37/115: batchLoss = 4.2627, diffLoss = 6.9866, kgLoss = 0.1769
2025-04-09 10:27:19.871332: Training Step 38/115: batchLoss = 4.1089, diffLoss = 6.7338, kgLoss = 0.1716
2025-04-09 10:27:20.611618: Training Step 39/115: batchLoss = 4.1855, diffLoss = 6.8613, kgLoss = 0.1717
2025-04-09 10:27:21.354157: Training Step 40/115: batchLoss = 4.3247, diffLoss = 7.0943, kgLoss = 0.1703
2025-04-09 10:27:22.088197: Training Step 41/115: batchLoss = 4.6277, diffLoss = 7.5844, kgLoss = 0.1927
2025-04-09 10:27:22.836305: Training Step 42/115: batchLoss = 4.1077, diffLoss = 6.7389, kgLoss = 0.1609
2025-04-09 10:27:23.585164: Training Step 43/115: batchLoss = 4.6203, diffLoss = 7.5792, kgLoss = 0.1820
2025-04-09 10:27:24.342507: Training Step 44/115: batchLoss = 4.0915, diffLoss = 6.7014, kgLoss = 0.1767
2025-04-09 10:27:25.085856: Training Step 45/115: batchLoss = 3.9822, diffLoss = 6.5320, kgLoss = 0.1575
2025-04-09 10:27:25.836201: Training Step 46/115: batchLoss = 4.4122, diffLoss = 7.2312, kgLoss = 0.1838
2025-04-09 10:27:26.594069: Training Step 47/115: batchLoss = 4.1519, diffLoss = 6.8082, kgLoss = 0.1675
2025-04-09 10:27:27.342532: Training Step 48/115: batchLoss = 4.5531, diffLoss = 7.4683, kgLoss = 0.1803
2025-04-09 10:27:28.085202: Training Step 49/115: batchLoss = 4.6104, diffLoss = 7.5555, kgLoss = 0.1927
2025-04-09 10:27:28.825583: Training Step 50/115: batchLoss = 4.2367, diffLoss = 6.9520, kgLoss = 0.1638
2025-04-09 10:27:29.561483: Training Step 51/115: batchLoss = 4.6026, diffLoss = 7.5462, kgLoss = 0.1872
2025-04-09 10:27:30.306369: Training Step 52/115: batchLoss = 4.2200, diffLoss = 6.9186, kgLoss = 0.1721
2025-04-09 10:27:31.047136: Training Step 53/115: batchLoss = 3.7556, diffLoss = 6.1604, kgLoss = 0.1483
2025-04-09 10:27:31.786415: Training Step 54/115: batchLoss = 4.5418, diffLoss = 7.4488, kgLoss = 0.1812
2025-04-09 10:27:32.526865: Training Step 55/115: batchLoss = 3.9499, diffLoss = 6.4780, kgLoss = 0.1577
2025-04-09 10:27:33.271392: Training Step 56/115: batchLoss = 4.5796, diffLoss = 7.5085, kgLoss = 0.1863
2025-04-09 10:27:34.019100: Training Step 57/115: batchLoss = 3.6779, diffLoss = 6.0262, kgLoss = 0.1554
2025-04-09 10:27:34.764053: Training Step 58/115: batchLoss = 4.4641, diffLoss = 7.3181, kgLoss = 0.1832
2025-04-09 10:27:35.504371: Training Step 59/115: batchLoss = 4.8024, diffLoss = 7.8788, kgLoss = 0.1879
2025-04-09 10:27:36.245655: Training Step 60/115: batchLoss = 4.3549, diffLoss = 7.1384, kgLoss = 0.1797
2025-04-09 10:27:37.007358: Training Step 61/115: batchLoss = 4.3640, diffLoss = 7.1569, kgLoss = 0.1747
2025-04-09 10:27:37.776222: Training Step 62/115: batchLoss = 4.1868, diffLoss = 6.8671, kgLoss = 0.1665
2025-04-09 10:27:38.527164: Training Step 63/115: batchLoss = 4.0398, diffLoss = 6.6207, kgLoss = 0.1683
2025-04-09 10:27:39.274828: Training Step 64/115: batchLoss = 4.0102, diffLoss = 6.5683, kgLoss = 0.1731
2025-04-09 10:27:40.026238: Training Step 65/115: batchLoss = 4.3833, diffLoss = 7.1902, kgLoss = 0.1731
2025-04-09 10:27:40.774729: Training Step 66/115: batchLoss = 4.0771, diffLoss = 6.6813, kgLoss = 0.1708
2025-04-09 10:27:41.517726: Training Step 67/115: batchLoss = 3.9296, diffLoss = 6.4365, kgLoss = 0.1691
2025-04-09 10:27:42.257536: Training Step 68/115: batchLoss = 3.9237, diffLoss = 6.4349, kgLoss = 0.1568
2025-04-09 10:27:42.985249: Training Step 69/115: batchLoss = 4.4482, diffLoss = 7.2988, kgLoss = 0.1723
2025-04-09 10:27:43.703913: Training Step 70/115: batchLoss = 3.8898, diffLoss = 6.3747, kgLoss = 0.1623
2025-04-09 10:27:44.431274: Training Step 71/115: batchLoss = 4.0884, diffLoss = 6.6951, kgLoss = 0.1783
2025-04-09 10:27:45.154721: Training Step 72/115: batchLoss = 4.3653, diffLoss = 7.1654, kgLoss = 0.1653
2025-04-09 10:27:45.890974: Training Step 73/115: batchLoss = 4.2113, diffLoss = 6.9046, kgLoss = 0.1714
2025-04-09 10:27:46.625327: Training Step 74/115: batchLoss = 3.7251, diffLoss = 6.1001, kgLoss = 0.1626
2025-04-09 10:27:47.355009: Training Step 75/115: batchLoss = 3.9367, diffLoss = 6.4520, kgLoss = 0.1636
2025-04-09 10:27:48.095907: Training Step 76/115: batchLoss = 4.3238, diffLoss = 7.0899, kgLoss = 0.1746
2025-04-09 10:27:48.837189: Training Step 77/115: batchLoss = 4.4000, diffLoss = 7.2162, kgLoss = 0.1757
2025-04-09 10:27:49.588325: Training Step 78/115: batchLoss = 4.3558, diffLoss = 7.1339, kgLoss = 0.1887
2025-04-09 10:27:50.348317: Training Step 79/115: batchLoss = 4.4772, diffLoss = 7.3452, kgLoss = 0.1752
2025-04-09 10:27:51.093143: Training Step 80/115: batchLoss = 3.9746, diffLoss = 6.5121, kgLoss = 0.1683
2025-04-09 10:27:51.839358: Training Step 81/115: batchLoss = 4.2634, diffLoss = 6.9913, kgLoss = 0.1714
2025-04-09 10:27:52.575664: Training Step 82/115: batchLoss = 4.1957, diffLoss = 6.8823, kgLoss = 0.1658
2025-04-09 10:27:53.330378: Training Step 83/115: batchLoss = 4.2054, diffLoss = 6.8901, kgLoss = 0.1784
2025-04-09 10:27:54.067801: Training Step 84/115: batchLoss = 4.8966, diffLoss = 8.0291, kgLoss = 0.1979
2025-04-09 10:27:54.805326: Training Step 85/115: batchLoss = 4.2213, diffLoss = 6.9215, kgLoss = 0.1710
2025-04-09 10:27:55.543303: Training Step 86/115: batchLoss = 4.6090, diffLoss = 7.5587, kgLoss = 0.1843
2025-04-09 10:27:56.279925: Training Step 87/115: batchLoss = 4.2243, diffLoss = 6.9227, kgLoss = 0.1767
2025-04-09 10:27:57.020388: Training Step 88/115: batchLoss = 4.0359, diffLoss = 6.6136, kgLoss = 0.1694
2025-04-09 10:27:57.777871: Training Step 89/115: batchLoss = 4.3094, diffLoss = 7.0699, kgLoss = 0.1687
2025-04-09 10:27:58.515607: Training Step 90/115: batchLoss = 4.0546, diffLoss = 6.6454, kgLoss = 0.1684
2025-04-09 10:27:59.248711: Training Step 91/115: batchLoss = 4.2053, diffLoss = 6.8984, kgLoss = 0.1657
2025-04-09 10:27:59.984727: Training Step 92/115: batchLoss = 4.4865, diffLoss = 7.3504, kgLoss = 0.1907
2025-04-09 10:28:00.730346: Training Step 93/115: batchLoss = 4.8214, diffLoss = 7.9090, kgLoss = 0.1900
2025-04-09 10:28:01.463092: Training Step 94/115: batchLoss = 3.9711, diffLoss = 6.5113, kgLoss = 0.1609
2025-04-09 10:28:02.205792: Training Step 95/115: batchLoss = 4.1126, diffLoss = 6.7433, kgLoss = 0.1665
2025-04-09 10:28:02.950406: Training Step 96/115: batchLoss = 4.3936, diffLoss = 7.2042, kgLoss = 0.1777
2025-04-09 10:28:03.690794: Training Step 97/115: batchLoss = 4.5232, diffLoss = 7.4076, kgLoss = 0.1965
2025-04-09 10:28:04.426253: Training Step 98/115: batchLoss = 4.2828, diffLoss = 7.0183, kgLoss = 0.1794
2025-04-09 10:28:05.156360: Training Step 99/115: batchLoss = 3.7678, diffLoss = 6.1772, kgLoss = 0.1537
2025-04-09 10:28:05.882913: Training Step 100/115: batchLoss = 3.9802, diffLoss = 6.5280, kgLoss = 0.1585
2025-04-09 10:28:06.612041: Training Step 101/115: batchLoss = 4.6946, diffLoss = 7.6879, kgLoss = 0.2045
2025-04-09 10:28:07.361888: Training Step 102/115: batchLoss = 4.0652, diffLoss = 6.6647, kgLoss = 0.1659
2025-04-09 10:28:08.118455: Training Step 103/115: batchLoss = 4.0920, diffLoss = 6.7094, kgLoss = 0.1658
2025-04-09 10:28:08.853725: Training Step 104/115: batchLoss = 4.3973, diffLoss = 7.2167, kgLoss = 0.1683
2025-04-09 10:28:09.590356: Training Step 105/115: batchLoss = 4.5657, diffLoss = 7.4911, kgLoss = 0.1777
2025-04-09 10:28:10.306628: Training Step 106/115: batchLoss = 4.3016, diffLoss = 7.0461, kgLoss = 0.1849
2025-04-09 10:28:11.040471: Training Step 107/115: batchLoss = 4.6521, diffLoss = 7.6336, kgLoss = 0.1798
2025-04-09 10:28:11.765517: Training Step 108/115: batchLoss = 3.9601, diffLoss = 6.4969, kgLoss = 0.1549
2025-04-09 10:28:12.490856: Training Step 109/115: batchLoss = 4.1324, diffLoss = 6.7733, kgLoss = 0.1711
2025-04-09 10:28:13.217906: Training Step 110/115: batchLoss = 4.3338, diffLoss = 7.1087, kgLoss = 0.1714
2025-04-09 10:28:13.941658: Training Step 111/115: batchLoss = 3.9475, diffLoss = 6.4666, kgLoss = 0.1687
2025-04-09 10:28:14.650964: Training Step 112/115: batchLoss = 5.0123, diffLoss = 8.2245, kgLoss = 0.1940
2025-04-09 10:28:15.286107: Training Step 113/115: batchLoss = 4.3654, diffLoss = 7.1544, kgLoss = 0.1821
2025-04-09 10:28:15.917989: Training Step 114/115: batchLoss = 4.3671, diffLoss = 7.1618, kgLoss = 0.1751
2025-04-09 10:28:16.047378: 
2025-04-09 10:28:16.048302: Epoch 29/1000, Train: epLoss = 1.2278, epDfLoss = 2.0130, epfTransLoss = 0.0000, epKgLoss = 0.0501  
2025-04-09 10:28:16.794953: Steps 0/90: batch_recall = 54.90, batch_ndcg = 42.85 
2025-04-09 10:28:17.534563: Steps 1/90: batch_recall = 58.44, batch_ndcg = 41.42 
2025-04-09 10:28:18.290862: Steps 2/90: batch_recall = 50.45, batch_ndcg = 37.08 
2025-04-09 10:28:19.026238: Steps 3/90: batch_recall = 46.27, batch_ndcg = 32.85 
2025-04-09 10:28:19.768893: Steps 4/90: batch_recall = 46.06, batch_ndcg = 35.74 
2025-04-09 10:28:20.514348: Steps 5/90: batch_recall = 38.99, batch_ndcg = 29.05 
2025-04-09 10:28:21.249733: Steps 6/90: batch_recall = 47.22, batch_ndcg = 33.27 
2025-04-09 10:28:22.000662: Steps 7/90: batch_recall = 42.74, batch_ndcg = 31.73 
2025-04-09 10:28:22.742694: Steps 8/90: batch_recall = 48.96, batch_ndcg = 34.52 
2025-04-09 10:28:23.502002: Steps 9/90: batch_recall = 44.66, batch_ndcg = 33.65 
2025-04-09 10:28:24.247524: Steps 10/90: batch_recall = 37.57, batch_ndcg = 28.11 
2025-04-09 10:28:24.984537: Steps 11/90: batch_recall = 44.59, batch_ndcg = 29.27 
2025-04-09 10:28:25.731863: Steps 12/90: batch_recall = 39.39, batch_ndcg = 27.84 
2025-04-09 10:28:26.457581: Steps 13/90: batch_recall = 37.17, batch_ndcg = 26.15 
2025-04-09 10:28:27.191163: Steps 14/90: batch_recall = 35.86, batch_ndcg = 26.27 
2025-04-09 10:28:27.916899: Steps 15/90: batch_recall = 46.27, batch_ndcg = 29.52 
2025-04-09 10:28:28.636024: Steps 16/90: batch_recall = 36.68, batch_ndcg = 26.49 
2025-04-09 10:28:29.351694: Steps 17/90: batch_recall = 36.08, batch_ndcg = 23.48 
2025-04-09 10:28:30.065451: Steps 18/90: batch_recall = 39.85, batch_ndcg = 26.48 
2025-04-09 10:28:30.803770: Steps 19/90: batch_recall = 34.57, batch_ndcg = 23.90 
2025-04-09 10:28:31.532370: Steps 20/90: batch_recall = 37.35, batch_ndcg = 25.41 
2025-04-09 10:28:32.280072: Steps 21/90: batch_recall = 41.06, batch_ndcg = 29.24 
2025-04-09 10:28:33.018328: Steps 22/90: batch_recall = 39.70, batch_ndcg = 27.90 
2025-04-09 10:28:33.757945: Steps 23/90: batch_recall = 38.22, batch_ndcg = 25.79 
2025-04-09 10:28:34.501433: Steps 24/90: batch_recall = 39.32, batch_ndcg = 27.13 
2025-04-09 10:28:35.232403: Steps 25/90: batch_recall = 38.16, batch_ndcg = 25.06 
2025-04-09 10:28:35.953164: Steps 26/90: batch_recall = 37.59, batch_ndcg = 27.78 
2025-04-09 10:28:36.667350: Steps 27/90: batch_recall = 35.73, batch_ndcg = 23.98 
2025-04-09 10:28:37.403355: Steps 28/90: batch_recall = 39.26, batch_ndcg = 25.57 
2025-04-09 10:28:38.127278: Steps 29/90: batch_recall = 34.12, batch_ndcg = 23.19 
2025-04-09 10:28:38.843406: Steps 30/90: batch_recall = 34.91, batch_ndcg = 22.52 
2025-04-09 10:28:39.574664: Steps 31/90: batch_recall = 34.47, batch_ndcg = 24.03 
2025-04-09 10:28:40.290815: Steps 32/90: batch_recall = 36.05, batch_ndcg = 23.95 
2025-04-09 10:28:41.020706: Steps 33/90: batch_recall = 36.59, batch_ndcg = 22.33 
2025-04-09 10:28:41.732023: Steps 34/90: batch_recall = 37.34, batch_ndcg = 25.12 
2025-04-09 10:28:42.446274: Steps 35/90: batch_recall = 38.62, batch_ndcg = 26.95 
2025-04-09 10:28:43.188178: Steps 36/90: batch_recall = 38.35, batch_ndcg = 25.18 
2025-04-09 10:28:43.905371: Steps 37/90: batch_recall = 34.09, batch_ndcg = 22.54 
2025-04-09 10:28:44.637264: Steps 38/90: batch_recall = 35.44, batch_ndcg = 23.31 
2025-04-09 10:28:45.339819: Steps 39/90: batch_recall = 36.36, batch_ndcg = 23.68 
2025-04-09 10:28:46.047435: Steps 40/90: batch_recall = 33.81, batch_ndcg = 23.00 
2025-04-09 10:28:46.767577: Steps 41/90: batch_recall = 36.08, batch_ndcg = 23.08 
2025-04-09 10:28:47.478810: Steps 42/90: batch_recall = 42.63, batch_ndcg = 29.41 
2025-04-09 10:28:48.194197: Steps 43/90: batch_recall = 37.01, batch_ndcg = 23.50 
2025-04-09 10:28:48.906363: Steps 44/90: batch_recall = 33.68, batch_ndcg = 22.65 
2025-04-09 10:28:49.617762: Steps 45/90: batch_recall = 31.11, batch_ndcg = 21.88 
2025-04-09 10:28:50.339842: Steps 46/90: batch_recall = 36.11, batch_ndcg = 22.14 
2025-04-09 10:28:51.049592: Steps 47/90: batch_recall = 32.47, batch_ndcg = 20.43 
2025-04-09 10:28:51.769227: Steps 48/90: batch_recall = 37.24, batch_ndcg = 23.88 
2025-04-09 10:28:52.494360: Steps 49/90: batch_recall = 31.26, batch_ndcg = 19.55 
2025-04-09 10:28:53.213900: Steps 50/90: batch_recall = 39.38, batch_ndcg = 25.89 
2025-04-09 10:28:53.932737: Steps 51/90: batch_recall = 39.66, batch_ndcg = 26.10 
2025-04-09 10:28:54.655716: Steps 52/90: batch_recall = 38.83, batch_ndcg = 24.40 
2025-04-09 10:28:55.368602: Steps 53/90: batch_recall = 39.57, batch_ndcg = 26.42 
2025-04-09 10:28:56.074284: Steps 54/90: batch_recall = 29.50, batch_ndcg = 19.06 
2025-04-09 10:28:56.792753: Steps 55/90: batch_recall = 34.85, batch_ndcg = 23.48 
2025-04-09 10:28:57.500989: Steps 56/90: batch_recall = 31.93, batch_ndcg = 19.10 
2025-04-09 10:28:58.231336: Steps 57/90: batch_recall = 37.46, batch_ndcg = 24.44 
2025-04-09 10:28:58.943803: Steps 58/90: batch_recall = 39.30, batch_ndcg = 24.55 
2025-04-09 10:28:59.656020: Steps 59/90: batch_recall = 33.76, batch_ndcg = 22.04 
2025-04-09 10:29:00.360451: Steps 60/90: batch_recall = 34.10, batch_ndcg = 21.33 
2025-04-09 10:29:01.072134: Steps 61/90: batch_recall = 40.73, batch_ndcg = 28.51 
2025-04-09 10:29:01.780366: Steps 62/90: batch_recall = 38.92, batch_ndcg = 25.92 
2025-04-09 10:29:02.490376: Steps 63/90: batch_recall = 35.31, batch_ndcg = 22.71 
2025-04-09 10:29:03.209450: Steps 64/90: batch_recall = 36.29, batch_ndcg = 23.34 
2025-04-09 10:29:03.915129: Steps 65/90: batch_recall = 33.40, batch_ndcg = 21.70 
2025-04-09 10:29:04.616312: Steps 66/90: batch_recall = 38.80, batch_ndcg = 23.58 
2025-04-09 10:29:05.324725: Steps 67/90: batch_recall = 33.92, batch_ndcg = 21.65 
2025-04-09 10:29:06.045671: Steps 68/90: batch_recall = 37.11, batch_ndcg = 22.46 
2025-04-09 10:29:06.777075: Steps 69/90: batch_recall = 40.72, batch_ndcg = 24.21 
2025-04-09 10:29:07.510426: Steps 70/90: batch_recall = 40.01, batch_ndcg = 25.61 
2025-04-09 10:29:08.219688: Steps 71/90: batch_recall = 38.91, batch_ndcg = 23.82 
2025-04-09 10:29:08.931042: Steps 72/90: batch_recall = 36.73, batch_ndcg = 23.07 
2025-04-09 10:29:09.633102: Steps 73/90: batch_recall = 35.68, batch_ndcg = 22.45 
2025-04-09 10:29:10.335617: Steps 74/90: batch_recall = 40.92, batch_ndcg = 25.71 
2025-04-09 10:29:11.041933: Steps 75/90: batch_recall = 39.19, batch_ndcg = 24.22 
2025-04-09 10:29:11.759713: Steps 76/90: batch_recall = 42.30, batch_ndcg = 26.92 
2025-04-09 10:29:12.467301: Steps 77/90: batch_recall = 45.68, batch_ndcg = 28.20 
2025-04-09 10:29:13.175452: Steps 78/90: batch_recall = 33.69, batch_ndcg = 20.38 
2025-04-09 10:29:13.865929: Steps 79/90: batch_recall = 39.43, batch_ndcg = 27.06 
2025-04-09 10:29:14.571460: Steps 80/90: batch_recall = 45.01, batch_ndcg = 28.75 
2025-04-09 10:29:15.259836: Steps 81/90: batch_recall = 46.69, batch_ndcg = 28.47 
2025-04-09 10:29:15.964995: Steps 82/90: batch_recall = 42.21, batch_ndcg = 26.94 
2025-04-09 10:29:16.666051: Steps 83/90: batch_recall = 42.10, batch_ndcg = 26.21 
2025-04-09 10:29:17.375426: Steps 84/90: batch_recall = 34.27, batch_ndcg = 21.15 
2025-04-09 10:29:18.069476: Steps 85/90: batch_recall = 41.17, batch_ndcg = 24.31 
2025-04-09 10:29:18.783650: Steps 86/90: batch_recall = 43.69, batch_ndcg = 28.26 
2025-04-09 10:29:19.502175: Steps 87/90: batch_recall = 43.64, batch_ndcg = 29.42 
2025-04-09 10:29:20.234245: Steps 88/90: batch_recall = 50.27, batch_ndcg = 28.96 
2025-04-09 10:29:20.785569: Steps 89/90: batch_recall = 40.92, batch_ndcg = 25.78 
2025-04-09 10:29:20.786285: Epoch 29/1000, Test: Recall = 0.0771, NDCG = 0.0509  

2025-04-09 10:29:21.828181: Training Step 0/115: batchLoss = 5.1045, diffLoss = 8.3759, kgLoss = 0.1973
2025-04-09 10:29:22.568523: Training Step 1/115: batchLoss = 3.9968, diffLoss = 6.5510, kgLoss = 0.1655
2025-04-09 10:29:23.306641: Training Step 2/115: batchLoss = 5.3865, diffLoss = 8.8347, kgLoss = 0.2143
2025-04-09 10:29:24.032227: Training Step 3/115: batchLoss = 4.7941, diffLoss = 7.8658, kgLoss = 0.1865
2025-04-09 10:29:24.766874: Training Step 4/115: batchLoss = 4.1640, diffLoss = 6.8277, kgLoss = 0.1686
2025-04-09 10:29:25.501872: Training Step 5/115: batchLoss = 4.5858, diffLoss = 7.5287, kgLoss = 0.1714
2025-04-09 10:29:26.231355: Training Step 6/115: batchLoss = 4.2194, diffLoss = 6.9192, kgLoss = 0.1697
2025-04-09 10:29:26.959460: Training Step 7/115: batchLoss = 4.1456, diffLoss = 6.7973, kgLoss = 0.1681
2025-04-09 10:29:27.687136: Training Step 8/115: batchLoss = 4.3065, diffLoss = 7.0643, kgLoss = 0.1700
2025-04-09 10:29:28.415976: Training Step 9/115: batchLoss = 4.7019, diffLoss = 7.7152, kgLoss = 0.1820
2025-04-09 10:29:29.145065: Training Step 10/115: batchLoss = 4.1422, diffLoss = 6.7920, kgLoss = 0.1674
2025-04-09 10:29:29.870972: Training Step 11/115: batchLoss = 4.3759, diffLoss = 7.1742, kgLoss = 0.1786
2025-04-09 10:29:30.596480: Training Step 12/115: batchLoss = 3.9886, diffLoss = 6.5368, kgLoss = 0.1662
2025-04-09 10:29:31.318088: Training Step 13/115: batchLoss = 4.5412, diffLoss = 7.4457, kgLoss = 0.1843
2025-04-09 10:29:32.060009: Training Step 14/115: batchLoss = 4.0308, diffLoss = 6.6150, kgLoss = 0.1545
2025-04-09 10:29:32.786050: Training Step 15/115: batchLoss = 4.1065, diffLoss = 6.7279, kgLoss = 0.1744
2025-04-09 10:29:33.515320: Training Step 16/115: batchLoss = 4.2423, diffLoss = 6.9585, kgLoss = 0.1680
2025-04-09 10:29:34.254259: Training Step 17/115: batchLoss = 4.1048, diffLoss = 6.7335, kgLoss = 0.1617
2025-04-09 10:29:34.988788: Training Step 18/115: batchLoss = 4.2386, diffLoss = 6.9499, kgLoss = 0.1716
2025-04-09 10:29:35.731975: Training Step 19/115: batchLoss = 3.9219, diffLoss = 6.4291, kgLoss = 0.1610
2025-04-09 10:29:36.486974: Training Step 20/115: batchLoss = 4.4291, diffLoss = 7.2538, kgLoss = 0.1920
2025-04-09 10:29:37.235385: Training Step 21/115: batchLoss = 4.4253, diffLoss = 7.2530, kgLoss = 0.1839
2025-04-09 10:29:37.987129: Training Step 22/115: batchLoss = 4.0764, diffLoss = 6.6801, kgLoss = 0.1709
2025-04-09 10:29:38.752593: Training Step 23/115: batchLoss = 4.5023, diffLoss = 7.3871, kgLoss = 0.1752
2025-04-09 10:29:39.505196: Training Step 24/115: batchLoss = 4.1640, diffLoss = 6.8283, kgLoss = 0.1676
2025-04-09 10:29:40.249424: Training Step 25/115: batchLoss = 4.2063, diffLoss = 6.8890, kgLoss = 0.1823
2025-04-09 10:29:41.002734: Training Step 26/115: batchLoss = 3.9513, diffLoss = 6.4783, kgLoss = 0.1609
2025-04-09 10:29:41.737403: Training Step 27/115: batchLoss = 4.1833, diffLoss = 6.8471, kgLoss = 0.1875
2025-04-09 10:29:42.460626: Training Step 28/115: batchLoss = 3.6745, diffLoss = 6.0204, kgLoss = 0.1556
2025-04-09 10:29:43.187658: Training Step 29/115: batchLoss = 4.3336, diffLoss = 7.1012, kgLoss = 0.1822
2025-04-09 10:29:43.925469: Training Step 30/115: batchLoss = 4.4645, diffLoss = 7.3201, kgLoss = 0.1809
2025-04-09 10:29:44.666577: Training Step 31/115: batchLoss = 4.0001, diffLoss = 6.5487, kgLoss = 0.1771
2025-04-09 10:29:45.388314: Training Step 32/115: batchLoss = 4.2335, diffLoss = 6.9406, kgLoss = 0.1727
2025-04-09 10:29:46.109356: Training Step 33/115: batchLoss = 4.5256, diffLoss = 7.4200, kgLoss = 0.1840
2025-04-09 10:29:46.837139: Training Step 34/115: batchLoss = 4.7230, diffLoss = 7.7451, kgLoss = 0.1898
2025-04-09 10:29:47.575593: Training Step 35/115: batchLoss = 4.1259, diffLoss = 6.7643, kgLoss = 0.1683
2025-04-09 10:29:48.310475: Training Step 36/115: batchLoss = 4.2104, diffLoss = 6.9057, kgLoss = 0.1674
2025-04-09 10:29:49.045982: Training Step 37/115: batchLoss = 3.7849, diffLoss = 6.2070, kgLoss = 0.1517
2025-04-09 10:29:49.796188: Training Step 38/115: batchLoss = 4.1905, diffLoss = 6.8708, kgLoss = 0.1701
2025-04-09 10:29:50.534553: Training Step 39/115: batchLoss = 4.5870, diffLoss = 7.5272, kgLoss = 0.1767
2025-04-09 10:29:51.282895: Training Step 40/115: batchLoss = 4.6479, diffLoss = 7.6216, kgLoss = 0.1874
2025-04-09 10:29:52.020856: Training Step 41/115: batchLoss = 4.4423, diffLoss = 7.2825, kgLoss = 0.1821
2025-04-09 10:29:52.760887: Training Step 42/115: batchLoss = 3.7993, diffLoss = 6.2225, kgLoss = 0.1644
2025-04-09 10:29:53.519913: Training Step 43/115: batchLoss = 4.0686, diffLoss = 6.6695, kgLoss = 0.1673
2025-04-09 10:29:54.255233: Training Step 44/115: batchLoss = 4.7882, diffLoss = 7.8565, kgLoss = 0.1858
2025-04-09 10:29:54.991370: Training Step 45/115: batchLoss = 4.3782, diffLoss = 7.1793, kgLoss = 0.1765
2025-04-09 10:29:55.729514: Training Step 46/115: batchLoss = 3.8022, diffLoss = 6.2287, kgLoss = 0.1625
2025-04-09 10:29:56.469321: Training Step 47/115: batchLoss = 4.5967, diffLoss = 7.5368, kgLoss = 0.1866
2025-04-09 10:29:57.204870: Training Step 48/115: batchLoss = 4.4831, diffLoss = 7.3556, kgLoss = 0.1744
2025-04-09 10:29:57.935962: Training Step 49/115: batchLoss = 4.3693, diffLoss = 7.1662, kgLoss = 0.1740
2025-04-09 10:29:58.669920: Training Step 50/115: batchLoss = 3.8215, diffLoss = 6.2657, kgLoss = 0.1552
2025-04-09 10:29:59.400848: Training Step 51/115: batchLoss = 4.0670, diffLoss = 6.6615, kgLoss = 0.1753
2025-04-09 10:30:00.135175: Training Step 52/115: batchLoss = 4.5757, diffLoss = 7.5063, kgLoss = 0.1798
2025-04-09 10:30:00.864594: Training Step 53/115: batchLoss = 4.3302, diffLoss = 7.1008, kgLoss = 0.1742
2025-04-09 10:30:01.608267: Training Step 54/115: batchLoss = 4.1596, diffLoss = 6.8201, kgLoss = 0.1688
2025-04-09 10:30:02.353543: Training Step 55/115: batchLoss = 4.0583, diffLoss = 6.6490, kgLoss = 0.1721
2025-04-09 10:30:03.096902: Training Step 56/115: batchLoss = 4.0502, diffLoss = 6.6332, kgLoss = 0.1756
2025-04-09 10:30:03.847455: Training Step 57/115: batchLoss = 4.1281, diffLoss = 6.7633, kgLoss = 0.1753
2025-04-09 10:30:04.600925: Training Step 58/115: batchLoss = 3.7018, diffLoss = 6.0713, kgLoss = 0.1475
2025-04-09 10:30:05.346929: Training Step 59/115: batchLoss = 4.4016, diffLoss = 7.2224, kgLoss = 0.1703
2025-04-09 10:30:06.098043: Training Step 60/115: batchLoss = 4.6435, diffLoss = 7.6233, kgLoss = 0.1738
2025-04-09 10:30:06.847809: Training Step 61/115: batchLoss = 3.6823, diffLoss = 6.0329, kgLoss = 0.1564
2025-04-09 10:30:07.587616: Training Step 62/115: batchLoss = 3.9837, diffLoss = 6.5304, kgLoss = 0.1636
2025-04-09 10:30:08.331649: Training Step 63/115: batchLoss = 3.9737, diffLoss = 6.5067, kgLoss = 0.1742
2025-04-09 10:30:09.075316: Training Step 64/115: batchLoss = 4.2003, diffLoss = 6.8854, kgLoss = 0.1726
2025-04-09 10:30:09.815334: Training Step 65/115: batchLoss = 4.4656, diffLoss = 7.3207, kgLoss = 0.1831
2025-04-09 10:30:10.549625: Training Step 66/115: batchLoss = 4.3540, diffLoss = 7.1383, kgLoss = 0.1775
2025-04-09 10:30:11.286461: Training Step 67/115: batchLoss = 4.7000, diffLoss = 7.7089, kgLoss = 0.1866
2025-04-09 10:30:12.037298: Training Step 68/115: batchLoss = 4.3782, diffLoss = 7.1798, kgLoss = 0.1758
2025-04-09 10:30:12.769866: Training Step 69/115: batchLoss = 4.0398, diffLoss = 6.6278, kgLoss = 0.1577
2025-04-09 10:30:13.519662: Training Step 70/115: batchLoss = 4.3663, diffLoss = 7.1680, kgLoss = 0.1636
2025-04-09 10:30:14.280870: Training Step 71/115: batchLoss = 3.7390, diffLoss = 6.1326, kgLoss = 0.1487
2025-04-09 10:30:15.025021: Training Step 72/115: batchLoss = 3.7355, diffLoss = 6.1221, kgLoss = 0.1557
2025-04-09 10:30:15.769003: Training Step 73/115: batchLoss = 4.3742, diffLoss = 7.1721, kgLoss = 0.1773
2025-04-09 10:30:16.524792: Training Step 74/115: batchLoss = 4.1889, diffLoss = 6.8711, kgLoss = 0.1657
2025-04-09 10:30:17.259678: Training Step 75/115: batchLoss = 4.0120, diffLoss = 6.5776, kgLoss = 0.1637
2025-04-09 10:30:17.992753: Training Step 76/115: batchLoss = 4.3781, diffLoss = 7.1806, kgLoss = 0.1745
2025-04-09 10:30:18.731178: Training Step 77/115: batchLoss = 4.5180, diffLoss = 7.4064, kgLoss = 0.1853
2025-04-09 10:30:19.477545: Training Step 78/115: batchLoss = 4.1558, diffLoss = 6.8110, kgLoss = 0.1730
2025-04-09 10:30:20.221414: Training Step 79/115: batchLoss = 4.4724, diffLoss = 7.3283, kgLoss = 0.1886
2025-04-09 10:30:20.962329: Training Step 80/115: batchLoss = 4.2324, diffLoss = 6.9385, kgLoss = 0.1733
2025-04-09 10:30:21.707264: Training Step 81/115: batchLoss = 3.5508, diffLoss = 5.8135, kgLoss = 0.1567
2025-04-09 10:30:22.446256: Training Step 82/115: batchLoss = 4.5601, diffLoss = 7.4754, kgLoss = 0.1871
2025-04-09 10:30:23.189211: Training Step 83/115: batchLoss = 4.1711, diffLoss = 6.8376, kgLoss = 0.1713
2025-04-09 10:30:23.925235: Training Step 84/115: batchLoss = 4.7103, diffLoss = 7.7220, kgLoss = 0.1926
2025-04-09 10:30:24.649924: Training Step 85/115: batchLoss = 4.2871, diffLoss = 7.0161, kgLoss = 0.1936
2025-04-09 10:30:25.400796: Training Step 86/115: batchLoss = 4.8075, diffLoss = 7.8885, kgLoss = 0.1860
2025-04-09 10:30:26.147886: Training Step 87/115: batchLoss = 4.6179, diffLoss = 7.5711, kgLoss = 0.1881
2025-04-09 10:30:26.901710: Training Step 88/115: batchLoss = 4.0448, diffLoss = 6.6344, kgLoss = 0.1603
2025-04-09 10:30:27.637766: Training Step 89/115: batchLoss = 3.7532, diffLoss = 6.1549, kgLoss = 0.1505
2025-04-09 10:30:28.373685: Training Step 90/115: batchLoss = 4.4571, diffLoss = 7.3115, kgLoss = 0.1754
2025-04-09 10:30:29.104818: Training Step 91/115: batchLoss = 4.1114, diffLoss = 6.7430, kgLoss = 0.1640
2025-04-09 10:30:29.837090: Training Step 92/115: batchLoss = 4.3023, diffLoss = 7.0528, kgLoss = 0.1766
2025-04-09 10:30:30.568399: Training Step 93/115: batchLoss = 4.3501, diffLoss = 7.1337, kgLoss = 0.1746
2025-04-09 10:30:31.310706: Training Step 94/115: batchLoss = 4.0810, diffLoss = 6.6903, kgLoss = 0.1671
2025-04-09 10:30:32.052078: Training Step 95/115: batchLoss = 4.0734, diffLoss = 6.6814, kgLoss = 0.1615
2025-04-09 10:30:32.811699: Training Step 96/115: batchLoss = 4.5978, diffLoss = 7.5383, kgLoss = 0.1871
2025-04-09 10:30:33.573302: Training Step 97/115: batchLoss = 4.8726, diffLoss = 7.9742, kgLoss = 0.2203
2025-04-09 10:30:34.315315: Training Step 98/115: batchLoss = 4.2472, diffLoss = 6.9688, kgLoss = 0.1648
2025-04-09 10:30:35.056879: Training Step 99/115: batchLoss = 4.2843, diffLoss = 7.0239, kgLoss = 0.1749
2025-04-09 10:30:35.788228: Training Step 100/115: batchLoss = 3.9527, diffLoss = 6.4751, kgLoss = 0.1689
2025-04-09 10:30:36.530464: Training Step 101/115: batchLoss = 4.3254, diffLoss = 7.0915, kgLoss = 0.1763
2025-04-09 10:30:37.262384: Training Step 102/115: batchLoss = 4.2098, diffLoss = 6.9015, kgLoss = 0.1722
2025-04-09 10:30:37.997340: Training Step 103/115: batchLoss = 4.8901, diffLoss = 8.0245, kgLoss = 0.1885
2025-04-09 10:30:38.743384: Training Step 104/115: batchLoss = 3.9242, diffLoss = 6.4335, kgLoss = 0.1604
2025-04-09 10:30:39.490792: Training Step 105/115: batchLoss = 4.5630, diffLoss = 7.4806, kgLoss = 0.1866
2025-04-09 10:30:40.229064: Training Step 106/115: batchLoss = 3.7467, diffLoss = 6.1424, kgLoss = 0.1532
2025-04-09 10:30:40.976981: Training Step 107/115: batchLoss = 4.2989, diffLoss = 7.0419, kgLoss = 0.1843
2025-04-09 10:30:41.706105: Training Step 108/115: batchLoss = 4.4783, diffLoss = 7.3461, kgLoss = 0.1765
2025-04-09 10:30:42.431161: Training Step 109/115: batchLoss = 4.2102, diffLoss = 6.9023, kgLoss = 0.1722
2025-04-09 10:30:43.157436: Training Step 110/115: batchLoss = 4.0379, diffLoss = 6.6151, kgLoss = 0.1721
2025-04-09 10:30:43.890859: Training Step 111/115: batchLoss = 4.5043, diffLoss = 7.3804, kgLoss = 0.1900
2025-04-09 10:30:44.609027: Training Step 112/115: batchLoss = 4.2656, diffLoss = 6.9980, kgLoss = 0.1669
2025-04-09 10:30:45.261780: Training Step 113/115: batchLoss = 4.1776, diffLoss = 6.8439, kgLoss = 0.1782
2025-04-09 10:30:45.875309: Training Step 114/115: batchLoss = 4.5715, diffLoss = 7.4915, kgLoss = 0.1914
2025-04-09 10:30:45.999992: 
2025-04-09 10:30:46.001027: Epoch 30/1000, Train: epLoss = 1.2280, epDfLoss = 2.0133, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:30:46.760997: Steps 0/90: batch_recall = 54.20, batch_ndcg = 42.89 
2025-04-09 10:30:47.504426: Steps 1/90: batch_recall = 58.42, batch_ndcg = 41.90 
2025-04-09 10:30:48.262252: Steps 2/90: batch_recall = 51.07, batch_ndcg = 37.58 
2025-04-09 10:30:49.014511: Steps 3/90: batch_recall = 45.81, batch_ndcg = 32.64 
2025-04-09 10:30:49.769548: Steps 4/90: batch_recall = 45.25, batch_ndcg = 35.28 
2025-04-09 10:30:50.533490: Steps 5/90: batch_recall = 38.24, batch_ndcg = 28.47 
2025-04-09 10:30:51.290812: Steps 6/90: batch_recall = 48.69, batch_ndcg = 33.70 
2025-04-09 10:30:52.058417: Steps 7/90: batch_recall = 43.46, batch_ndcg = 32.24 
2025-04-09 10:30:52.813998: Steps 8/90: batch_recall = 46.88, batch_ndcg = 33.84 
2025-04-09 10:30:53.582258: Steps 9/90: batch_recall = 44.66, batch_ndcg = 33.33 
2025-04-09 10:30:54.347777: Steps 10/90: batch_recall = 37.30, batch_ndcg = 28.43 
2025-04-09 10:30:55.095790: Steps 11/90: batch_recall = 43.71, batch_ndcg = 28.86 
2025-04-09 10:30:55.835865: Steps 12/90: batch_recall = 38.13, batch_ndcg = 27.30 
2025-04-09 10:30:56.584541: Steps 13/90: batch_recall = 36.47, batch_ndcg = 25.80 
2025-04-09 10:30:57.341571: Steps 14/90: batch_recall = 36.13, batch_ndcg = 26.02 
2025-04-09 10:30:58.068855: Steps 15/90: batch_recall = 45.63, batch_ndcg = 29.22 
2025-04-09 10:30:58.810988: Steps 16/90: batch_recall = 36.40, batch_ndcg = 26.25 
2025-04-09 10:30:59.551065: Steps 17/90: batch_recall = 35.12, batch_ndcg = 23.37 
2025-04-09 10:31:00.284086: Steps 18/90: batch_recall = 40.27, batch_ndcg = 26.79 
2025-04-09 10:31:01.019476: Steps 19/90: batch_recall = 33.62, batch_ndcg = 23.60 
2025-04-09 10:31:01.745600: Steps 20/90: batch_recall = 36.01, batch_ndcg = 25.24 
2025-04-09 10:31:02.481140: Steps 21/90: batch_recall = 41.02, batch_ndcg = 29.50 
2025-04-09 10:31:03.210177: Steps 22/90: batch_recall = 38.52, batch_ndcg = 27.59 
2025-04-09 10:31:03.933760: Steps 23/90: batch_recall = 38.79, batch_ndcg = 25.96 
2025-04-09 10:31:04.645038: Steps 24/90: batch_recall = 39.96, batch_ndcg = 27.51 
2025-04-09 10:31:05.371788: Steps 25/90: batch_recall = 38.74, batch_ndcg = 25.48 
2025-04-09 10:31:06.086220: Steps 26/90: batch_recall = 37.87, batch_ndcg = 27.91 
2025-04-09 10:31:06.811504: Steps 27/90: batch_recall = 35.66, batch_ndcg = 23.78 
2025-04-09 10:31:07.622983: Steps 28/90: batch_recall = 39.20, batch_ndcg = 25.75 
2025-04-09 10:31:08.346729: Steps 29/90: batch_recall = 34.52, batch_ndcg = 23.25 
2025-04-09 10:31:09.073835: Steps 30/90: batch_recall = 34.20, batch_ndcg = 22.41 
2025-04-09 10:31:09.801433: Steps 31/90: batch_recall = 33.71, batch_ndcg = 24.14 
2025-04-09 10:31:10.531221: Steps 32/90: batch_recall = 35.15, batch_ndcg = 23.95 
2025-04-09 10:31:11.262597: Steps 33/90: batch_recall = 37.55, batch_ndcg = 22.71 
2025-04-09 10:31:11.998134: Steps 34/90: batch_recall = 37.81, batch_ndcg = 25.64 
2025-04-09 10:31:12.709798: Steps 35/90: batch_recall = 40.03, batch_ndcg = 27.51 
2025-04-09 10:31:13.436933: Steps 36/90: batch_recall = 37.77, batch_ndcg = 25.15 
2025-04-09 10:31:14.146043: Steps 37/90: batch_recall = 34.57, batch_ndcg = 22.66 
2025-04-09 10:31:14.870615: Steps 38/90: batch_recall = 35.08, batch_ndcg = 22.93 
2025-04-09 10:31:15.574710: Steps 39/90: batch_recall = 36.24, batch_ndcg = 23.25 
2025-04-09 10:31:16.281446: Steps 40/90: batch_recall = 32.70, batch_ndcg = 22.62 
2025-04-09 10:31:16.990779: Steps 41/90: batch_recall = 36.06, batch_ndcg = 22.98 
2025-04-09 10:31:17.701477: Steps 42/90: batch_recall = 42.61, batch_ndcg = 29.17 
2025-04-09 10:31:18.413568: Steps 43/90: batch_recall = 36.31, batch_ndcg = 23.31 
2025-04-09 10:31:19.119692: Steps 44/90: batch_recall = 32.81, batch_ndcg = 22.30 
2025-04-09 10:31:19.834140: Steps 45/90: batch_recall = 33.04, batch_ndcg = 22.56 
2025-04-09 10:31:20.553437: Steps 46/90: batch_recall = 36.41, batch_ndcg = 22.38 
2025-04-09 10:31:21.288115: Steps 47/90: batch_recall = 32.51, batch_ndcg = 20.11 
2025-04-09 10:31:21.999057: Steps 48/90: batch_recall = 37.02, batch_ndcg = 23.92 
2025-04-09 10:31:22.715308: Steps 49/90: batch_recall = 31.51, batch_ndcg = 19.73 
2025-04-09 10:31:23.432775: Steps 50/90: batch_recall = 38.85, batch_ndcg = 25.56 
2025-04-09 10:31:24.136524: Steps 51/90: batch_recall = 38.39, batch_ndcg = 25.56 
2025-04-09 10:31:24.856064: Steps 52/90: batch_recall = 38.55, batch_ndcg = 24.41 
2025-04-09 10:31:25.568087: Steps 53/90: batch_recall = 40.89, batch_ndcg = 26.76 
2025-04-09 10:31:26.282539: Steps 54/90: batch_recall = 28.65, batch_ndcg = 18.82 
2025-04-09 10:31:27.020088: Steps 55/90: batch_recall = 35.11, batch_ndcg = 23.22 
2025-04-09 10:31:27.738939: Steps 56/90: batch_recall = 32.37, batch_ndcg = 19.10 
2025-04-09 10:31:28.450541: Steps 57/90: batch_recall = 38.63, batch_ndcg = 24.81 
2025-04-09 10:31:29.155694: Steps 58/90: batch_recall = 38.42, batch_ndcg = 24.15 
2025-04-09 10:31:29.860965: Steps 59/90: batch_recall = 34.62, batch_ndcg = 22.40 
2025-04-09 10:31:30.572233: Steps 60/90: batch_recall = 34.46, batch_ndcg = 21.74 
2025-04-09 10:31:31.270894: Steps 61/90: batch_recall = 41.37, batch_ndcg = 28.57 
2025-04-09 10:31:31.964762: Steps 62/90: batch_recall = 38.88, batch_ndcg = 25.52 
2025-04-09 10:31:32.675413: Steps 63/90: batch_recall = 35.14, batch_ndcg = 22.68 
2025-04-09 10:31:33.375438: Steps 64/90: batch_recall = 36.52, batch_ndcg = 23.17 
2025-04-09 10:31:34.090223: Steps 65/90: batch_recall = 33.18, batch_ndcg = 21.65 
2025-04-09 10:31:34.813549: Steps 66/90: batch_recall = 38.48, batch_ndcg = 23.06 
2025-04-09 10:31:35.534437: Steps 67/90: batch_recall = 33.89, batch_ndcg = 21.82 
2025-04-09 10:31:36.247890: Steps 68/90: batch_recall = 36.94, batch_ndcg = 22.53 
2025-04-09 10:31:36.963285: Steps 69/90: batch_recall = 40.46, batch_ndcg = 24.20 
2025-04-09 10:31:37.675035: Steps 70/90: batch_recall = 39.46, batch_ndcg = 25.67 
2025-04-09 10:31:38.379603: Steps 71/90: batch_recall = 40.02, batch_ndcg = 24.40 
2025-04-09 10:31:39.093545: Steps 72/90: batch_recall = 36.80, batch_ndcg = 22.78 
2025-04-09 10:31:39.808598: Steps 73/90: batch_recall = 34.87, batch_ndcg = 22.23 
2025-04-09 10:31:40.527936: Steps 74/90: batch_recall = 40.18, batch_ndcg = 25.23 
2025-04-09 10:31:41.246503: Steps 75/90: batch_recall = 39.87, batch_ndcg = 24.71 
2025-04-09 10:31:41.954651: Steps 76/90: batch_recall = 42.53, batch_ndcg = 26.72 
2025-04-09 10:31:42.650111: Steps 77/90: batch_recall = 45.88, batch_ndcg = 28.09 
2025-04-09 10:31:43.359212: Steps 78/90: batch_recall = 33.23, batch_ndcg = 20.30 
2025-04-09 10:31:44.058837: Steps 79/90: batch_recall = 39.86, batch_ndcg = 26.99 
2025-04-09 10:31:44.763290: Steps 80/90: batch_recall = 45.15, batch_ndcg = 28.81 
2025-04-09 10:31:45.452890: Steps 81/90: batch_recall = 45.64, batch_ndcg = 28.46 
2025-04-09 10:31:46.150198: Steps 82/90: batch_recall = 42.20, batch_ndcg = 27.01 
2025-04-09 10:31:46.844152: Steps 83/90: batch_recall = 42.91, batch_ndcg = 26.44 
2025-04-09 10:31:47.555960: Steps 84/90: batch_recall = 34.82, batch_ndcg = 21.58 
2025-04-09 10:31:48.277746: Steps 85/90: batch_recall = 40.99, batch_ndcg = 24.28 
2025-04-09 10:31:48.990890: Steps 86/90: batch_recall = 42.43, batch_ndcg = 27.96 
2025-04-09 10:31:49.693948: Steps 87/90: batch_recall = 42.76, batch_ndcg = 29.28 
2025-04-09 10:31:50.396838: Steps 88/90: batch_recall = 49.16, batch_ndcg = 28.60 
2025-04-09 10:31:50.956583: Steps 89/90: batch_recall = 39.51, batch_ndcg = 25.38 
2025-04-09 10:31:50.957731: Epoch 30/1000, Test: Recall = 0.0772, NDCG = 0.0508  

2025-04-09 10:31:51.986494: Training Step 0/115: batchLoss = 3.9803, diffLoss = 6.5238, kgLoss = 0.1649
2025-04-09 10:31:52.718995: Training Step 1/115: batchLoss = 4.2970, diffLoss = 7.0449, kgLoss = 0.1753
2025-04-09 10:31:53.454657: Training Step 2/115: batchLoss = 4.2809, diffLoss = 7.0240, kgLoss = 0.1661
2025-04-09 10:31:54.185676: Training Step 3/115: batchLoss = 4.0409, diffLoss = 6.6138, kgLoss = 0.1814
2025-04-09 10:31:54.916841: Training Step 4/115: batchLoss = 4.4525, diffLoss = 7.2952, kgLoss = 0.1884
2025-04-09 10:31:55.653037: Training Step 5/115: batchLoss = 3.9341, diffLoss = 6.4436, kgLoss = 0.1698
2025-04-09 10:31:56.383935: Training Step 6/115: batchLoss = 4.0517, diffLoss = 6.6394, kgLoss = 0.1701
2025-04-09 10:31:57.103432: Training Step 7/115: batchLoss = 4.5677, diffLoss = 7.4894, kgLoss = 0.1852
2025-04-09 10:31:57.824692: Training Step 8/115: batchLoss = 4.3807, diffLoss = 7.1793, kgLoss = 0.1829
2025-04-09 10:31:58.549320: Training Step 9/115: batchLoss = 4.5689, diffLoss = 7.4897, kgLoss = 0.1876
2025-04-09 10:31:59.277800: Training Step 10/115: batchLoss = 4.0795, diffLoss = 6.6944, kgLoss = 0.1570
2025-04-09 10:32:00.011091: Training Step 11/115: batchLoss = 3.8548, diffLoss = 6.3228, kgLoss = 0.1529
2025-04-09 10:32:00.751324: Training Step 12/115: batchLoss = 3.8109, diffLoss = 6.2457, kgLoss = 0.1586
2025-04-09 10:32:01.474331: Training Step 13/115: batchLoss = 4.8565, diffLoss = 7.9657, kgLoss = 0.1926
2025-04-09 10:32:02.198469: Training Step 14/115: batchLoss = 4.1970, diffLoss = 6.8794, kgLoss = 0.1734
2025-04-09 10:32:02.922698: Training Step 15/115: batchLoss = 4.0301, diffLoss = 6.6094, kgLoss = 0.1613
2025-04-09 10:32:03.656401: Training Step 16/115: batchLoss = 4.2299, diffLoss = 6.9303, kgLoss = 0.1793
2025-04-09 10:32:04.397810: Training Step 17/115: batchLoss = 4.0861, diffLoss = 6.7040, kgLoss = 0.1592
2025-04-09 10:32:05.132337: Training Step 18/115: batchLoss = 4.3186, diffLoss = 7.0807, kgLoss = 0.1756
2025-04-09 10:32:05.866599: Training Step 19/115: batchLoss = 3.9228, diffLoss = 6.4163, kgLoss = 0.1825
2025-04-09 10:32:06.614234: Training Step 20/115: batchLoss = 3.6389, diffLoss = 5.9670, kgLoss = 0.1466
2025-04-09 10:32:07.372573: Training Step 21/115: batchLoss = 4.0321, diffLoss = 6.6130, kgLoss = 0.1606
2025-04-09 10:32:08.132428: Training Step 22/115: batchLoss = 4.4020, diffLoss = 7.2193, kgLoss = 0.1760
2025-04-09 10:32:08.902063: Training Step 23/115: batchLoss = 3.9450, diffLoss = 6.4662, kgLoss = 0.1632
2025-04-09 10:32:09.659571: Training Step 24/115: batchLoss = 3.9676, diffLoss = 6.4974, kgLoss = 0.1729
2025-04-09 10:32:10.409686: Training Step 25/115: batchLoss = 4.3455, diffLoss = 7.1244, kgLoss = 0.1771
2025-04-09 10:32:11.156781: Training Step 26/115: batchLoss = 4.1818, diffLoss = 6.8584, kgLoss = 0.1668
2025-04-09 10:32:11.911268: Training Step 27/115: batchLoss = 4.3944, diffLoss = 7.2114, kgLoss = 0.1688
2025-04-09 10:32:12.647193: Training Step 28/115: batchLoss = 4.5028, diffLoss = 7.3791, kgLoss = 0.1883
2025-04-09 10:32:13.387266: Training Step 29/115: batchLoss = 4.3555, diffLoss = 7.1311, kgLoss = 0.1920
2025-04-09 10:32:14.123973: Training Step 30/115: batchLoss = 4.3190, diffLoss = 7.0781, kgLoss = 0.1803
2025-04-09 10:32:14.868387: Training Step 31/115: batchLoss = 3.8065, diffLoss = 6.2289, kgLoss = 0.1730
2025-04-09 10:32:15.607331: Training Step 32/115: batchLoss = 4.3022, diffLoss = 7.0512, kgLoss = 0.1787
2025-04-09 10:32:16.353636: Training Step 33/115: batchLoss = 4.2377, diffLoss = 6.9455, kgLoss = 0.1758
2025-04-09 10:32:17.094914: Training Step 34/115: batchLoss = 4.0192, diffLoss = 6.5898, kgLoss = 0.1633
2025-04-09 10:32:17.837696: Training Step 35/115: batchLoss = 4.4816, diffLoss = 7.3540, kgLoss = 0.1730
2025-04-09 10:32:18.577184: Training Step 36/115: batchLoss = 3.8392, diffLoss = 6.2894, kgLoss = 0.1640
2025-04-09 10:32:19.326028: Training Step 37/115: batchLoss = 4.5976, diffLoss = 7.5447, kgLoss = 0.1770
2025-04-09 10:32:20.072282: Training Step 38/115: batchLoss = 4.2815, diffLoss = 7.0220, kgLoss = 0.1707
2025-04-09 10:32:20.819324: Training Step 39/115: batchLoss = 4.3692, diffLoss = 7.1664, kgLoss = 0.1734
2025-04-09 10:32:21.567166: Training Step 40/115: batchLoss = 4.1510, diffLoss = 6.7988, kgLoss = 0.1793
2025-04-09 10:32:22.303755: Training Step 41/115: batchLoss = 4.6217, diffLoss = 7.5772, kgLoss = 0.1884
2025-04-09 10:32:23.034990: Training Step 42/115: batchLoss = 3.9523, diffLoss = 6.4753, kgLoss = 0.1678
2025-04-09 10:32:23.773733: Training Step 43/115: batchLoss = 4.2683, diffLoss = 6.9873, kgLoss = 0.1897
2025-04-09 10:32:24.516652: Training Step 44/115: batchLoss = 4.0835, diffLoss = 6.6871, kgLoss = 0.1781
2025-04-09 10:32:25.277242: Training Step 45/115: batchLoss = 4.1932, diffLoss = 6.8719, kgLoss = 0.1751
2025-04-09 10:32:26.042611: Training Step 46/115: batchLoss = 4.1013, diffLoss = 6.7213, kgLoss = 0.1714
2025-04-09 10:32:26.787159: Training Step 47/115: batchLoss = 4.1973, diffLoss = 6.8786, kgLoss = 0.1754
2025-04-09 10:32:27.525279: Training Step 48/115: batchLoss = 4.3346, diffLoss = 7.1146, kgLoss = 0.1645
2025-04-09 10:32:28.264844: Training Step 49/115: batchLoss = 4.1810, diffLoss = 6.8542, kgLoss = 0.1713
2025-04-09 10:32:29.007498: Training Step 50/115: batchLoss = 3.9939, diffLoss = 6.5484, kgLoss = 0.1622
2025-04-09 10:32:29.754342: Training Step 51/115: batchLoss = 4.0027, diffLoss = 6.5617, kgLoss = 0.1643
2025-04-09 10:32:30.484665: Training Step 52/115: batchLoss = 4.5137, diffLoss = 7.4029, kgLoss = 0.1799
2025-04-09 10:32:31.203805: Training Step 53/115: batchLoss = 4.7083, diffLoss = 7.7222, kgLoss = 0.1876
2025-04-09 10:32:31.928418: Training Step 54/115: batchLoss = 4.1492, diffLoss = 6.7988, kgLoss = 0.1750
2025-04-09 10:32:32.683623: Training Step 55/115: batchLoss = 3.9948, diffLoss = 6.5511, kgLoss = 0.1604
2025-04-09 10:32:33.408913: Training Step 56/115: batchLoss = 4.1094, diffLoss = 6.7313, kgLoss = 0.1767
2025-04-09 10:32:34.136250: Training Step 57/115: batchLoss = 4.1746, diffLoss = 6.8432, kgLoss = 0.1717
2025-04-09 10:32:34.873612: Training Step 58/115: batchLoss = 4.4639, diffLoss = 7.3243, kgLoss = 0.1735
2025-04-09 10:32:35.607079: Training Step 59/115: batchLoss = 4.5462, diffLoss = 7.4557, kgLoss = 0.1821
2025-04-09 10:32:36.349674: Training Step 60/115: batchLoss = 4.4697, diffLoss = 7.3280, kgLoss = 0.1822
2025-04-09 10:32:37.075225: Training Step 61/115: batchLoss = 3.8906, diffLoss = 6.3814, kgLoss = 0.1545
2025-04-09 10:32:37.816545: Training Step 62/115: batchLoss = 3.9535, diffLoss = 6.4833, kgLoss = 0.1587
2025-04-09 10:32:38.555220: Training Step 63/115: batchLoss = 4.3936, diffLoss = 7.2057, kgLoss = 0.1755
2025-04-09 10:32:39.296355: Training Step 64/115: batchLoss = 4.5788, diffLoss = 7.5129, kgLoss = 0.1776
2025-04-09 10:32:40.036959: Training Step 65/115: batchLoss = 4.3557, diffLoss = 7.1415, kgLoss = 0.1769
2025-04-09 10:32:40.774805: Training Step 66/115: batchLoss = 4.1213, diffLoss = 6.7576, kgLoss = 0.1668
2025-04-09 10:32:41.522606: Training Step 67/115: batchLoss = 5.1343, diffLoss = 8.4208, kgLoss = 0.2045
2025-04-09 10:32:42.274282: Training Step 68/115: batchLoss = 3.9437, diffLoss = 6.4656, kgLoss = 0.1608
2025-04-09 10:32:43.026188: Training Step 69/115: batchLoss = 4.1860, diffLoss = 6.8587, kgLoss = 0.1770
2025-04-09 10:32:43.768675: Training Step 70/115: batchLoss = 3.4993, diffLoss = 5.7318, kgLoss = 0.1505
2025-04-09 10:32:44.503869: Training Step 71/115: batchLoss = 3.8808, diffLoss = 6.3603, kgLoss = 0.1615
2025-04-09 10:32:45.248154: Training Step 72/115: batchLoss = 4.3946, diffLoss = 7.2044, kgLoss = 0.1799
2025-04-09 10:32:45.987023: Training Step 73/115: batchLoss = 4.3160, diffLoss = 7.0730, kgLoss = 0.1805
2025-04-09 10:32:46.730826: Training Step 74/115: batchLoss = 3.9941, diffLoss = 6.5468, kgLoss = 0.1651
2025-04-09 10:32:47.472804: Training Step 75/115: batchLoss = 4.0925, diffLoss = 6.7071, kgLoss = 0.1706
2025-04-09 10:32:48.210127: Training Step 76/115: batchLoss = 4.6517, diffLoss = 7.6297, kgLoss = 0.1848
2025-04-09 10:32:48.953390: Training Step 77/115: batchLoss = 4.3682, diffLoss = 7.1655, kgLoss = 0.1722
2025-04-09 10:32:49.708316: Training Step 78/115: batchLoss = 4.4776, diffLoss = 7.3379, kgLoss = 0.1872
2025-04-09 10:32:50.477381: Training Step 79/115: batchLoss = 4.3984, diffLoss = 7.2137, kgLoss = 0.1755
2025-04-09 10:32:51.228326: Training Step 80/115: batchLoss = 4.0743, diffLoss = 6.6768, kgLoss = 0.1706
2025-04-09 10:32:51.972840: Training Step 81/115: batchLoss = 4.4755, diffLoss = 7.3381, kgLoss = 0.1815
2025-04-09 10:32:52.716045: Training Step 82/115: batchLoss = 4.3262, diffLoss = 7.0992, kgLoss = 0.1667
2025-04-09 10:32:53.476162: Training Step 83/115: batchLoss = 4.3146, diffLoss = 7.0689, kgLoss = 0.1830
2025-04-09 10:32:54.227850: Training Step 84/115: batchLoss = 4.0989, diffLoss = 6.7183, kgLoss = 0.1699
2025-04-09 10:32:54.958907: Training Step 85/115: batchLoss = 4.6134, diffLoss = 7.5606, kgLoss = 0.1925
2025-04-09 10:32:55.705489: Training Step 86/115: batchLoss = 4.7727, diffLoss = 7.8239, kgLoss = 0.1958
2025-04-09 10:32:56.447600: Training Step 87/115: batchLoss = 3.8691, diffLoss = 6.3432, kgLoss = 0.1580
2025-04-09 10:32:57.180361: Training Step 88/115: batchLoss = 4.5142, diffLoss = 7.3997, kgLoss = 0.1858
2025-04-09 10:32:57.916057: Training Step 89/115: batchLoss = 4.1298, diffLoss = 6.7698, kgLoss = 0.1697
2025-04-09 10:32:58.642109: Training Step 90/115: batchLoss = 3.8766, diffLoss = 6.3487, kgLoss = 0.1683
2025-04-09 10:32:59.382267: Training Step 91/115: batchLoss = 4.4476, diffLoss = 7.2937, kgLoss = 0.1785
2025-04-09 10:33:00.126597: Training Step 92/115: batchLoss = 4.5269, diffLoss = 7.4291, kgLoss = 0.1734
2025-04-09 10:33:00.874086: Training Step 93/115: batchLoss = 4.2678, diffLoss = 6.9927, kgLoss = 0.1803
2025-04-09 10:33:01.613300: Training Step 94/115: batchLoss = 4.1387, diffLoss = 6.7799, kgLoss = 0.1769
2025-04-09 10:33:02.349608: Training Step 95/115: batchLoss = 4.6285, diffLoss = 7.5913, kgLoss = 0.1843
2025-04-09 10:33:03.088637: Training Step 96/115: batchLoss = 4.1816, diffLoss = 6.8595, kgLoss = 0.1648
2025-04-09 10:33:03.818357: Training Step 97/115: batchLoss = 4.4369, diffLoss = 7.2775, kgLoss = 0.1760
2025-04-09 10:33:04.558224: Training Step 98/115: batchLoss = 4.0473, diffLoss = 6.6349, kgLoss = 0.1658
2025-04-09 10:33:05.320615: Training Step 99/115: batchLoss = 4.1532, diffLoss = 6.8139, kgLoss = 0.1622
2025-04-09 10:33:06.070306: Training Step 100/115: batchLoss = 3.7680, diffLoss = 6.1785, kgLoss = 0.1524
2025-04-09 10:33:06.831583: Training Step 101/115: batchLoss = 4.3729, diffLoss = 7.1727, kgLoss = 0.1733
2025-04-09 10:33:07.573522: Training Step 102/115: batchLoss = 4.2863, diffLoss = 7.0268, kgLoss = 0.1755
2025-04-09 10:33:08.330783: Training Step 103/115: batchLoss = 4.7771, diffLoss = 7.8215, kgLoss = 0.2105
2025-04-09 10:33:09.059421: Training Step 104/115: batchLoss = 4.2871, diffLoss = 7.0280, kgLoss = 0.1758
2025-04-09 10:33:09.797417: Training Step 105/115: batchLoss = 4.4643, diffLoss = 7.3268, kgLoss = 0.1705
2025-04-09 10:33:10.541663: Training Step 106/115: batchLoss = 4.4222, diffLoss = 7.2520, kgLoss = 0.1775
2025-04-09 10:33:11.279266: Training Step 107/115: batchLoss = 4.7920, diffLoss = 7.8564, kgLoss = 0.1952
2025-04-09 10:33:12.017161: Training Step 108/115: batchLoss = 4.5944, diffLoss = 7.5371, kgLoss = 0.1802
2025-04-09 10:33:12.752140: Training Step 109/115: batchLoss = 3.8798, diffLoss = 6.3605, kgLoss = 0.1589
2025-04-09 10:33:13.473725: Training Step 110/115: batchLoss = 3.9602, diffLoss = 6.4950, kgLoss = 0.1580
2025-04-09 10:33:14.197180: Training Step 111/115: batchLoss = 4.3914, diffLoss = 7.1973, kgLoss = 0.1826
2025-04-09 10:33:14.911965: Training Step 112/115: batchLoss = 4.3056, diffLoss = 7.0636, kgLoss = 0.1685
2025-04-09 10:33:15.558803: Training Step 113/115: batchLoss = 3.9363, diffLoss = 6.4473, kgLoss = 0.1697
2025-04-09 10:33:16.202942: Training Step 114/115: batchLoss = 4.8291, diffLoss = 7.9171, kgLoss = 0.1971
2025-04-09 10:33:16.325136: 
2025-04-09 10:33:16.326668: Epoch 31/1000, Train: epLoss = 1.2219, epDfLoss = 2.0032, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:33:17.094230: Steps 0/90: batch_recall = 54.08, batch_ndcg = 42.58 
2025-04-09 10:33:17.829038: Steps 1/90: batch_recall = 58.96, batch_ndcg = 42.18 
2025-04-09 10:33:18.573299: Steps 2/90: batch_recall = 50.57, batch_ndcg = 37.47 
2025-04-09 10:33:19.317180: Steps 3/90: batch_recall = 46.13, batch_ndcg = 32.39 
2025-04-09 10:33:20.060836: Steps 4/90: batch_recall = 46.06, batch_ndcg = 35.56 
2025-04-09 10:33:20.801603: Steps 5/90: batch_recall = 38.02, batch_ndcg = 28.54 
2025-04-09 10:33:21.546133: Steps 6/90: batch_recall = 46.94, batch_ndcg = 32.96 
2025-04-09 10:33:22.304823: Steps 7/90: batch_recall = 44.12, batch_ndcg = 32.46 
2025-04-09 10:33:23.041968: Steps 8/90: batch_recall = 47.28, batch_ndcg = 33.76 
2025-04-09 10:33:23.797120: Steps 9/90: batch_recall = 43.89, batch_ndcg = 33.24 
2025-04-09 10:33:24.533957: Steps 10/90: batch_recall = 38.32, batch_ndcg = 29.04 
2025-04-09 10:33:25.276249: Steps 11/90: batch_recall = 44.75, batch_ndcg = 29.07 
2025-04-09 10:33:26.012179: Steps 12/90: batch_recall = 39.17, batch_ndcg = 27.87 
2025-04-09 10:33:26.761656: Steps 13/90: batch_recall = 36.95, batch_ndcg = 26.08 
2025-04-09 10:33:27.510004: Steps 14/90: batch_recall = 36.03, batch_ndcg = 25.97 
2025-04-09 10:33:28.249333: Steps 15/90: batch_recall = 45.81, batch_ndcg = 29.98 
2025-04-09 10:33:28.976648: Steps 16/90: batch_recall = 36.71, batch_ndcg = 26.29 
2025-04-09 10:33:29.696337: Steps 17/90: batch_recall = 34.32, batch_ndcg = 22.92 
2025-04-09 10:33:30.421677: Steps 18/90: batch_recall = 40.43, batch_ndcg = 26.82 
2025-04-09 10:33:31.181645: Steps 19/90: batch_recall = 34.19, batch_ndcg = 23.76 
2025-04-09 10:33:31.923136: Steps 20/90: batch_recall = 36.81, batch_ndcg = 25.25 
2025-04-09 10:33:32.661085: Steps 21/90: batch_recall = 41.52, batch_ndcg = 29.85 
2025-04-09 10:33:33.387931: Steps 22/90: batch_recall = 38.57, batch_ndcg = 27.70 
2025-04-09 10:33:34.112310: Steps 23/90: batch_recall = 40.07, batch_ndcg = 26.40 
2025-04-09 10:33:34.825615: Steps 24/90: batch_recall = 39.15, batch_ndcg = 27.09 
2025-04-09 10:33:35.543778: Steps 25/90: batch_recall = 38.92, batch_ndcg = 25.43 
2025-04-09 10:33:36.274673: Steps 26/90: batch_recall = 37.83, batch_ndcg = 27.90 
2025-04-09 10:33:37.003206: Steps 27/90: batch_recall = 35.02, batch_ndcg = 23.78 
2025-04-09 10:33:37.724614: Steps 28/90: batch_recall = 38.64, batch_ndcg = 25.40 
2025-04-09 10:33:38.444562: Steps 29/90: batch_recall = 35.53, batch_ndcg = 23.83 
2025-04-09 10:33:39.171367: Steps 30/90: batch_recall = 33.82, batch_ndcg = 22.25 
2025-04-09 10:33:39.900346: Steps 31/90: batch_recall = 32.46, batch_ndcg = 23.80 
2025-04-09 10:33:40.619449: Steps 32/90: batch_recall = 35.79, batch_ndcg = 24.16 
2025-04-09 10:33:41.339371: Steps 33/90: batch_recall = 37.70, batch_ndcg = 23.18 
2025-04-09 10:33:42.049110: Steps 34/90: batch_recall = 37.31, batch_ndcg = 25.68 
2025-04-09 10:33:42.760901: Steps 35/90: batch_recall = 39.60, batch_ndcg = 27.50 
2025-04-09 10:33:43.492133: Steps 36/90: batch_recall = 38.47, batch_ndcg = 25.24 
2025-04-09 10:33:44.227141: Steps 37/90: batch_recall = 33.88, batch_ndcg = 22.52 
2025-04-09 10:33:44.946774: Steps 38/90: batch_recall = 35.75, batch_ndcg = 23.70 
2025-04-09 10:33:45.660311: Steps 39/90: batch_recall = 36.06, batch_ndcg = 23.22 
2025-04-09 10:33:46.365056: Steps 40/90: batch_recall = 32.03, batch_ndcg = 22.55 
2025-04-09 10:33:47.069437: Steps 41/90: batch_recall = 34.89, batch_ndcg = 22.48 
2025-04-09 10:33:47.772604: Steps 42/90: batch_recall = 42.59, batch_ndcg = 29.29 
2025-04-09 10:33:48.493758: Steps 43/90: batch_recall = 36.61, batch_ndcg = 23.40 
2025-04-09 10:33:49.210954: Steps 44/90: batch_recall = 33.90, batch_ndcg = 22.58 
2025-04-09 10:33:49.944805: Steps 45/90: batch_recall = 31.95, batch_ndcg = 21.88 
2025-04-09 10:33:50.675709: Steps 46/90: batch_recall = 35.66, batch_ndcg = 22.17 
2025-04-09 10:33:51.392130: Steps 47/90: batch_recall = 33.41, batch_ndcg = 20.87 
2025-04-09 10:33:52.101090: Steps 48/90: batch_recall = 36.78, batch_ndcg = 23.55 
2025-04-09 10:33:52.821118: Steps 49/90: batch_recall = 31.83, batch_ndcg = 19.67 
2025-04-09 10:33:53.533870: Steps 50/90: batch_recall = 40.16, batch_ndcg = 26.03 
2025-04-09 10:33:54.246676: Steps 51/90: batch_recall = 38.92, batch_ndcg = 26.21 
2025-04-09 10:33:54.970740: Steps 52/90: batch_recall = 39.01, batch_ndcg = 24.58 
2025-04-09 10:33:55.678332: Steps 53/90: batch_recall = 41.79, batch_ndcg = 26.81 
2025-04-09 10:33:56.387286: Steps 54/90: batch_recall = 29.51, batch_ndcg = 19.34 
2025-04-09 10:33:57.100023: Steps 55/90: batch_recall = 36.03, batch_ndcg = 23.42 
2025-04-09 10:33:57.808207: Steps 56/90: batch_recall = 33.31, batch_ndcg = 19.45 
2025-04-09 10:33:58.540141: Steps 57/90: batch_recall = 37.87, batch_ndcg = 24.73 
2025-04-09 10:33:59.253344: Steps 58/90: batch_recall = 37.70, batch_ndcg = 23.83 
2025-04-09 10:33:59.994394: Steps 59/90: batch_recall = 34.44, batch_ndcg = 22.48 
2025-04-09 10:34:00.705085: Steps 60/90: batch_recall = 34.26, batch_ndcg = 21.97 
2025-04-09 10:34:01.414502: Steps 61/90: batch_recall = 41.51, batch_ndcg = 28.37 
2025-04-09 10:34:02.113465: Steps 62/90: batch_recall = 38.93, batch_ndcg = 25.63 
2025-04-09 10:34:02.826737: Steps 63/90: batch_recall = 35.70, batch_ndcg = 22.95 
2025-04-09 10:34:03.524015: Steps 64/90: batch_recall = 37.35, batch_ndcg = 23.51 
2025-04-09 10:34:04.235003: Steps 65/90: batch_recall = 33.33, batch_ndcg = 21.63 
2025-04-09 10:34:04.950034: Steps 66/90: batch_recall = 38.93, batch_ndcg = 23.81 
2025-04-09 10:34:05.693866: Steps 67/90: batch_recall = 32.81, batch_ndcg = 21.33 
2025-04-09 10:34:06.409550: Steps 68/90: batch_recall = 37.35, batch_ndcg = 22.75 
2025-04-09 10:34:07.131946: Steps 69/90: batch_recall = 40.84, batch_ndcg = 24.58 
2025-04-09 10:34:07.839754: Steps 70/90: batch_recall = 38.51, batch_ndcg = 25.34 
2025-04-09 10:34:08.556068: Steps 71/90: batch_recall = 40.65, batch_ndcg = 24.71 
2025-04-09 10:34:09.259039: Steps 72/90: batch_recall = 35.89, batch_ndcg = 22.51 
2025-04-09 10:34:09.975129: Steps 73/90: batch_recall = 35.67, batch_ndcg = 22.35 
2025-04-09 10:34:10.673145: Steps 74/90: batch_recall = 42.24, batch_ndcg = 26.08 
2025-04-09 10:34:11.389925: Steps 75/90: batch_recall = 39.77, batch_ndcg = 24.48 
2025-04-09 10:34:12.089229: Steps 76/90: batch_recall = 42.54, batch_ndcg = 26.76 
2025-04-09 10:34:12.794819: Steps 77/90: batch_recall = 44.45, batch_ndcg = 27.44 
2025-04-09 10:34:13.518390: Steps 78/90: batch_recall = 33.07, batch_ndcg = 20.24 
2025-04-09 10:34:14.243430: Steps 79/90: batch_recall = 39.42, batch_ndcg = 26.80 
2025-04-09 10:34:14.945808: Steps 80/90: batch_recall = 45.59, batch_ndcg = 29.19 
2025-04-09 10:34:15.639030: Steps 81/90: batch_recall = 46.40, batch_ndcg = 28.75 
2025-04-09 10:34:16.334386: Steps 82/90: batch_recall = 43.38, batch_ndcg = 27.52 
2025-04-09 10:34:17.036614: Steps 83/90: batch_recall = 43.13, batch_ndcg = 26.64 
2025-04-09 10:34:17.748652: Steps 84/90: batch_recall = 35.56, batch_ndcg = 21.94 
2025-04-09 10:34:18.469712: Steps 85/90: batch_recall = 40.78, batch_ndcg = 24.11 
2025-04-09 10:34:19.184343: Steps 86/90: batch_recall = 44.03, batch_ndcg = 28.79 
2025-04-09 10:34:19.884727: Steps 87/90: batch_recall = 42.58, batch_ndcg = 29.35 
2025-04-09 10:34:20.589733: Steps 88/90: batch_recall = 50.56, batch_ndcg = 29.42 
2025-04-09 10:34:21.149195: Steps 89/90: batch_recall = 39.64, batch_ndcg = 25.46 
2025-04-09 10:34:21.149957: Epoch 31/1000, Test: Recall = 0.0772, NDCG = 0.0510  

2025-04-09 10:34:22.193496: Training Step 0/115: batchLoss = 4.3243, diffLoss = 7.0934, kgLoss = 0.1706
2025-04-09 10:34:22.926658: Training Step 1/115: batchLoss = 4.0899, diffLoss = 6.7088, kgLoss = 0.1616
2025-04-09 10:34:23.658305: Training Step 2/115: batchLoss = 4.5257, diffLoss = 7.4216, kgLoss = 0.1818
2025-04-09 10:34:24.398184: Training Step 3/115: batchLoss = 3.9947, diffLoss = 6.5527, kgLoss = 0.1576
2025-04-09 10:34:25.147736: Training Step 4/115: batchLoss = 4.4703, diffLoss = 7.3354, kgLoss = 0.1727
2025-04-09 10:34:25.875155: Training Step 5/115: batchLoss = 4.4423, diffLoss = 7.2822, kgLoss = 0.1824
2025-04-09 10:34:26.612555: Training Step 6/115: batchLoss = 4.1015, diffLoss = 6.7271, kgLoss = 0.1630
2025-04-09 10:34:27.349455: Training Step 7/115: batchLoss = 3.6730, diffLoss = 6.0157, kgLoss = 0.1590
2025-04-09 10:34:28.082936: Training Step 8/115: batchLoss = 3.9255, diffLoss = 6.4356, kgLoss = 0.1604
2025-04-09 10:34:28.822279: Training Step 9/115: batchLoss = 4.3889, diffLoss = 7.1966, kgLoss = 0.1774
2025-04-09 10:34:29.552048: Training Step 10/115: batchLoss = 4.0349, diffLoss = 6.6072, kgLoss = 0.1766
2025-04-09 10:34:30.279249: Training Step 11/115: batchLoss = 4.4959, diffLoss = 7.3615, kgLoss = 0.1976
2025-04-09 10:34:31.007495: Training Step 12/115: batchLoss = 4.1490, diffLoss = 6.8036, kgLoss = 0.1671
2025-04-09 10:34:31.732236: Training Step 13/115: batchLoss = 4.4071, diffLoss = 7.2242, kgLoss = 0.1815
2025-04-09 10:34:32.458878: Training Step 14/115: batchLoss = 4.6022, diffLoss = 7.5387, kgLoss = 0.1975
2025-04-09 10:34:33.194696: Training Step 15/115: batchLoss = 4.1423, diffLoss = 6.7934, kgLoss = 0.1656
2025-04-09 10:34:33.922686: Training Step 16/115: batchLoss = 3.9366, diffLoss = 6.4539, kgLoss = 0.1606
2025-04-09 10:34:34.651377: Training Step 17/115: batchLoss = 4.1814, diffLoss = 6.8508, kgLoss = 0.1774
2025-04-09 10:34:35.378219: Training Step 18/115: batchLoss = 4.2658, diffLoss = 6.9880, kgLoss = 0.1824
2025-04-09 10:34:36.118061: Training Step 19/115: batchLoss = 4.9106, diffLoss = 8.0533, kgLoss = 0.1966
2025-04-09 10:34:36.871251: Training Step 20/115: batchLoss = 3.9628, diffLoss = 6.4894, kgLoss = 0.1728
2025-04-09 10:34:37.618469: Training Step 21/115: batchLoss = 3.9482, diffLoss = 6.4702, kgLoss = 0.1651
2025-04-09 10:34:38.370745: Training Step 22/115: batchLoss = 4.2813, diffLoss = 7.0194, kgLoss = 0.1742
2025-04-09 10:34:39.118411: Training Step 23/115: batchLoss = 4.0890, diffLoss = 6.7021, kgLoss = 0.1693
2025-04-09 10:34:39.864428: Training Step 24/115: batchLoss = 4.0939, diffLoss = 6.7050, kgLoss = 0.1773
2025-04-09 10:34:40.609312: Training Step 25/115: batchLoss = 4.0397, diffLoss = 6.6153, kgLoss = 0.1763
2025-04-09 10:34:41.356104: Training Step 26/115: batchLoss = 4.0374, diffLoss = 6.6228, kgLoss = 0.1591
2025-04-09 10:34:42.101239: Training Step 27/115: batchLoss = 4.7682, diffLoss = 7.8181, kgLoss = 0.1935
2025-04-09 10:34:42.835596: Training Step 28/115: batchLoss = 4.1348, diffLoss = 6.7740, kgLoss = 0.1760
2025-04-09 10:34:43.582194: Training Step 29/115: batchLoss = 4.1372, diffLoss = 6.7820, kgLoss = 0.1699
2025-04-09 10:34:44.325480: Training Step 30/115: batchLoss = 4.6712, diffLoss = 7.6618, kgLoss = 0.1852
2025-04-09 10:34:45.048884: Training Step 31/115: batchLoss = 4.5565, diffLoss = 7.4707, kgLoss = 0.1851
2025-04-09 10:34:45.783315: Training Step 32/115: batchLoss = 4.1365, diffLoss = 6.7840, kgLoss = 0.1652
2025-04-09 10:34:46.507659: Training Step 33/115: batchLoss = 4.1999, diffLoss = 6.8898, kgLoss = 0.1650
2025-04-09 10:34:47.242127: Training Step 34/115: batchLoss = 4.4551, diffLoss = 7.3055, kgLoss = 0.1795
2025-04-09 10:34:47.968286: Training Step 35/115: batchLoss = 4.3826, diffLoss = 7.1890, kgLoss = 0.1730
2025-04-09 10:34:48.690969: Training Step 36/115: batchLoss = 4.2156, diffLoss = 6.9107, kgLoss = 0.1730
2025-04-09 10:34:49.418977: Training Step 37/115: batchLoss = 4.4206, diffLoss = 7.2558, kgLoss = 0.1679
2025-04-09 10:34:50.161440: Training Step 38/115: batchLoss = 4.9431, diffLoss = 8.1082, kgLoss = 0.1954
2025-04-09 10:34:50.884438: Training Step 39/115: batchLoss = 4.2821, diffLoss = 7.0224, kgLoss = 0.1717
2025-04-09 10:34:51.617409: Training Step 40/115: batchLoss = 4.2492, diffLoss = 6.9667, kgLoss = 0.1729
2025-04-09 10:34:52.355794: Training Step 41/115: batchLoss = 4.5382, diffLoss = 7.4410, kgLoss = 0.1841
2025-04-09 10:34:53.096802: Training Step 42/115: batchLoss = 4.1896, diffLoss = 6.8637, kgLoss = 0.1785
2025-04-09 10:34:53.834772: Training Step 43/115: batchLoss = 4.0294, diffLoss = 6.6023, kgLoss = 0.1701
2025-04-09 10:34:54.575502: Training Step 44/115: batchLoss = 4.0685, diffLoss = 6.6708, kgLoss = 0.1650
2025-04-09 10:34:55.328223: Training Step 45/115: batchLoss = 3.9283, diffLoss = 6.4431, kgLoss = 0.1561
2025-04-09 10:34:56.082043: Training Step 46/115: batchLoss = 4.9791, diffLoss = 8.1651, kgLoss = 0.2002
2025-04-09 10:34:56.828677: Training Step 47/115: batchLoss = 4.4425, diffLoss = 7.2836, kgLoss = 0.1808
2025-04-09 10:34:57.576337: Training Step 48/115: batchLoss = 4.1725, diffLoss = 6.8404, kgLoss = 0.1706
2025-04-09 10:34:58.332786: Training Step 49/115: batchLoss = 4.2596, diffLoss = 6.9911, kgLoss = 0.1622
2025-04-09 10:34:59.082302: Training Step 50/115: batchLoss = 4.3449, diffLoss = 7.1252, kgLoss = 0.1743
2025-04-09 10:34:59.833400: Training Step 51/115: batchLoss = 4.0236, diffLoss = 6.5951, kgLoss = 0.1664
2025-04-09 10:35:00.577037: Training Step 52/115: batchLoss = 4.1830, diffLoss = 6.8590, kgLoss = 0.1690
2025-04-09 10:35:01.308384: Training Step 53/115: batchLoss = 4.1926, diffLoss = 6.8711, kgLoss = 0.1747
2025-04-09 10:35:02.070762: Training Step 54/115: batchLoss = 4.3861, diffLoss = 7.1876, kgLoss = 0.1839
2025-04-09 10:35:02.804934: Training Step 55/115: batchLoss = 4.4144, diffLoss = 7.2373, kgLoss = 0.1801
2025-04-09 10:35:03.541242: Training Step 56/115: batchLoss = 4.2097, diffLoss = 6.9012, kgLoss = 0.1725
2025-04-09 10:35:04.267558: Training Step 57/115: batchLoss = 4.2471, diffLoss = 6.9644, kgLoss = 0.1711
2025-04-09 10:35:05.011194: Training Step 58/115: batchLoss = 4.1820, diffLoss = 6.8588, kgLoss = 0.1669
2025-04-09 10:35:05.752278: Training Step 59/115: batchLoss = 4.5241, diffLoss = 7.4221, kgLoss = 0.1770
2025-04-09 10:35:06.494764: Training Step 60/115: batchLoss = 3.6483, diffLoss = 5.9709, kgLoss = 0.1644
2025-04-09 10:35:07.231388: Training Step 61/115: batchLoss = 4.6036, diffLoss = 7.5516, kgLoss = 0.1816
2025-04-09 10:35:07.975031: Training Step 62/115: batchLoss = 4.1278, diffLoss = 6.7666, kgLoss = 0.1696
2025-04-09 10:35:08.748681: Training Step 63/115: batchLoss = 4.0568, diffLoss = 6.6529, kgLoss = 0.1627
2025-04-09 10:35:09.505879: Training Step 64/115: batchLoss = 4.3125, diffLoss = 7.0709, kgLoss = 0.1748
2025-04-09 10:35:10.368560: Training Step 65/115: batchLoss = 4.2235, diffLoss = 6.9197, kgLoss = 0.1793
2025-04-09 10:35:11.110854: Training Step 66/115: batchLoss = 4.0418, diffLoss = 6.6270, kgLoss = 0.1640
2025-04-09 10:35:11.847750: Training Step 67/115: batchLoss = 4.6561, diffLoss = 7.6303, kgLoss = 0.1949
2025-04-09 10:35:12.591215: Training Step 68/115: batchLoss = 3.5507, diffLoss = 5.8218, kgLoss = 0.1440
2025-04-09 10:35:13.335962: Training Step 69/115: batchLoss = 4.2673, diffLoss = 6.9960, kgLoss = 0.1744
2025-04-09 10:35:14.072001: Training Step 70/115: batchLoss = 4.3874, diffLoss = 7.1904, kgLoss = 0.1829
2025-04-09 10:35:14.811711: Training Step 71/115: batchLoss = 4.1179, diffLoss = 6.7462, kgLoss = 0.1754
2025-04-09 10:35:15.547233: Training Step 72/115: batchLoss = 4.6057, diffLoss = 7.5583, kgLoss = 0.1767
2025-04-09 10:35:16.283630: Training Step 73/115: batchLoss = 3.8928, diffLoss = 6.3811, kgLoss = 0.1604
2025-04-09 10:35:17.021955: Training Step 74/115: batchLoss = 4.1552, diffLoss = 6.8106, kgLoss = 0.1721
2025-04-09 10:35:17.760802: Training Step 75/115: batchLoss = 3.8452, diffLoss = 6.3032, kgLoss = 0.1583
2025-04-09 10:35:18.515318: Training Step 76/115: batchLoss = 4.4050, diffLoss = 7.2210, kgLoss = 0.1810
2025-04-09 10:35:19.264053: Training Step 77/115: batchLoss = 4.2821, diffLoss = 7.0262, kgLoss = 0.1659
2025-04-09 10:35:20.001328: Training Step 78/115: batchLoss = 4.1475, diffLoss = 6.8032, kgLoss = 0.1639
2025-04-09 10:35:20.739903: Training Step 79/115: batchLoss = 3.8614, diffLoss = 6.3324, kgLoss = 0.1548
2025-04-09 10:35:21.487678: Training Step 80/115: batchLoss = 3.9811, diffLoss = 6.5294, kgLoss = 0.1586
2025-04-09 10:35:22.220834: Training Step 81/115: batchLoss = 3.9027, diffLoss = 6.3908, kgLoss = 0.1706
2025-04-09 10:35:22.960255: Training Step 82/115: batchLoss = 4.5612, diffLoss = 7.4799, kgLoss = 0.1833
2025-04-09 10:35:23.702975: Training Step 83/115: batchLoss = 4.1492, diffLoss = 6.8004, kgLoss = 0.1723
2025-04-09 10:35:24.462271: Training Step 84/115: batchLoss = 4.3582, diffLoss = 7.1409, kgLoss = 0.1842
2025-04-09 10:35:25.212910: Training Step 85/115: batchLoss = 4.0180, diffLoss = 6.5817, kgLoss = 0.1725
2025-04-09 10:35:25.969887: Training Step 86/115: batchLoss = 3.9093, diffLoss = 6.4048, kgLoss = 0.1660
2025-04-09 10:35:26.741219: Training Step 87/115: batchLoss = 4.4007, diffLoss = 7.2216, kgLoss = 0.1694
2025-04-09 10:35:27.489908: Training Step 88/115: batchLoss = 4.0098, diffLoss = 6.5712, kgLoss = 0.1676
2025-04-09 10:35:28.241950: Training Step 89/115: batchLoss = 4.3561, diffLoss = 7.1429, kgLoss = 0.1758
2025-04-09 10:35:28.985090: Training Step 90/115: batchLoss = 4.2391, diffLoss = 6.9502, kgLoss = 0.1724
2025-04-09 10:35:29.724667: Training Step 91/115: batchLoss = 4.3363, diffLoss = 7.1006, kgLoss = 0.1899
2025-04-09 10:35:30.472128: Training Step 92/115: batchLoss = 4.1078, diffLoss = 6.7378, kgLoss = 0.1628
2025-04-09 10:35:31.204605: Training Step 93/115: batchLoss = 4.8947, diffLoss = 8.0237, kgLoss = 0.2014
2025-04-09 10:35:31.938225: Training Step 94/115: batchLoss = 4.8553, diffLoss = 7.9644, kgLoss = 0.1916
2025-04-09 10:35:32.674885: Training Step 95/115: batchLoss = 4.7885, diffLoss = 7.8543, kgLoss = 0.1899
2025-04-09 10:35:33.406742: Training Step 96/115: batchLoss = 3.9395, diffLoss = 6.4548, kgLoss = 0.1667
2025-04-09 10:35:34.143876: Training Step 97/115: batchLoss = 3.8080, diffLoss = 6.2375, kgLoss = 0.1637
2025-04-09 10:35:34.870431: Training Step 98/115: batchLoss = 3.9957, diffLoss = 6.5523, kgLoss = 0.1608
2025-04-09 10:35:35.614891: Training Step 99/115: batchLoss = 4.1764, diffLoss = 6.8471, kgLoss = 0.1702
2025-04-09 10:35:36.367848: Training Step 100/115: batchLoss = 4.2685, diffLoss = 7.0041, kgLoss = 0.1651
2025-04-09 10:35:37.124328: Training Step 101/115: batchLoss = 3.9949, diffLoss = 6.5447, kgLoss = 0.1702
2025-04-09 10:35:37.863749: Training Step 102/115: batchLoss = 4.5421, diffLoss = 7.4569, kgLoss = 0.1700
2025-04-09 10:35:38.611424: Training Step 103/115: batchLoss = 4.4972, diffLoss = 7.3673, kgLoss = 0.1919
2025-04-09 10:35:39.361032: Training Step 104/115: batchLoss = 4.1544, diffLoss = 6.8086, kgLoss = 0.1731
2025-04-09 10:35:40.189716: Training Step 105/115: batchLoss = 3.8873, diffLoss = 6.3664, kgLoss = 0.1687
2025-04-09 10:35:40.936173: Training Step 106/115: batchLoss = 4.3540, diffLoss = 7.1346, kgLoss = 0.1829
2025-04-09 10:35:41.694070: Training Step 107/115: batchLoss = 4.4128, diffLoss = 7.2332, kgLoss = 0.1821
2025-04-09 10:35:42.427458: Training Step 108/115: batchLoss = 4.0654, diffLoss = 6.6590, kgLoss = 0.1751
2025-04-09 10:35:43.193976: Training Step 109/115: batchLoss = 4.0444, diffLoss = 6.6331, kgLoss = 0.1614
2025-04-09 10:35:43.935137: Training Step 110/115: batchLoss = 4.1824, diffLoss = 6.8591, kgLoss = 0.1674
2025-04-09 10:35:44.677423: Training Step 111/115: batchLoss = 4.6838, diffLoss = 7.6763, kgLoss = 0.1949
2025-04-09 10:35:45.408877: Training Step 112/115: batchLoss = 4.1661, diffLoss = 6.8284, kgLoss = 0.1727
2025-04-09 10:35:46.052882: Training Step 113/115: batchLoss = 4.4391, diffLoss = 7.2714, kgLoss = 0.1907
2025-04-09 10:35:46.693055: Training Step 114/115: batchLoss = 4.4440, diffLoss = 7.2837, kgLoss = 0.1844
2025-04-09 10:35:46.813184: 
2025-04-09 10:35:46.814355: Epoch 32/1000, Train: epLoss = 1.2212, epDfLoss = 2.0021, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:35:47.577860: Steps 0/90: batch_recall = 54.36, batch_ndcg = 42.94 
2025-04-09 10:35:48.310727: Steps 1/90: batch_recall = 57.70, batch_ndcg = 41.10 
2025-04-09 10:35:49.055589: Steps 2/90: batch_recall = 50.39, batch_ndcg = 37.51 
2025-04-09 10:35:49.783405: Steps 3/90: batch_recall = 46.48, batch_ndcg = 32.60 
2025-04-09 10:35:50.526772: Steps 4/90: batch_recall = 46.52, batch_ndcg = 35.76 
2025-04-09 10:35:51.267160: Steps 5/90: batch_recall = 38.02, batch_ndcg = 28.48 
2025-04-09 10:35:52.017951: Steps 6/90: batch_recall = 47.28, batch_ndcg = 32.87 
2025-04-09 10:35:52.778640: Steps 7/90: batch_recall = 43.51, batch_ndcg = 31.84 
2025-04-09 10:35:53.519773: Steps 8/90: batch_recall = 45.21, batch_ndcg = 33.16 
2025-04-09 10:35:54.266905: Steps 9/90: batch_recall = 44.15, batch_ndcg = 33.05 
2025-04-09 10:35:55.019288: Steps 10/90: batch_recall = 38.33, batch_ndcg = 29.00 
2025-04-09 10:35:55.750226: Steps 11/90: batch_recall = 44.21, batch_ndcg = 28.77 
2025-04-09 10:35:56.485611: Steps 12/90: batch_recall = 38.77, batch_ndcg = 27.57 
2025-04-09 10:35:57.216052: Steps 13/90: batch_recall = 36.65, batch_ndcg = 25.78 
2025-04-09 10:35:57.962047: Steps 14/90: batch_recall = 36.37, batch_ndcg = 26.29 
2025-04-09 10:35:58.697052: Steps 15/90: batch_recall = 46.61, batch_ndcg = 30.19 
2025-04-09 10:35:59.440866: Steps 16/90: batch_recall = 36.37, batch_ndcg = 25.67 
2025-04-09 10:36:00.168800: Steps 17/90: batch_recall = 34.57, batch_ndcg = 23.22 
2025-04-09 10:36:00.878848: Steps 18/90: batch_recall = 39.80, batch_ndcg = 26.69 
2025-04-09 10:36:01.621540: Steps 19/90: batch_recall = 33.85, batch_ndcg = 23.63 
2025-04-09 10:36:02.342774: Steps 20/90: batch_recall = 37.65, batch_ndcg = 25.61 
2025-04-09 10:36:03.075628: Steps 21/90: batch_recall = 41.15, batch_ndcg = 29.77 
2025-04-09 10:36:03.778034: Steps 22/90: batch_recall = 38.12, batch_ndcg = 27.55 
2025-04-09 10:36:04.502998: Steps 23/90: batch_recall = 39.42, batch_ndcg = 26.18 
2025-04-09 10:36:05.229321: Steps 24/90: batch_recall = 40.65, batch_ndcg = 27.89 
2025-04-09 10:36:05.947063: Steps 25/90: batch_recall = 39.18, batch_ndcg = 25.47 
2025-04-09 10:36:06.664681: Steps 26/90: batch_recall = 37.61, batch_ndcg = 27.80 
2025-04-09 10:36:07.396688: Steps 27/90: batch_recall = 35.59, batch_ndcg = 24.03 
2025-04-09 10:36:08.136941: Steps 28/90: batch_recall = 38.45, batch_ndcg = 25.85 
2025-04-09 10:36:08.869859: Steps 29/90: batch_recall = 36.14, batch_ndcg = 24.06 
2025-04-09 10:36:09.612499: Steps 30/90: batch_recall = 33.71, batch_ndcg = 22.28 
2025-04-09 10:36:10.363656: Steps 31/90: batch_recall = 32.71, batch_ndcg = 23.58 
2025-04-09 10:36:11.086477: Steps 32/90: batch_recall = 36.61, batch_ndcg = 24.29 
2025-04-09 10:36:11.802993: Steps 33/90: batch_recall = 36.72, batch_ndcg = 23.05 
2025-04-09 10:36:12.528491: Steps 34/90: batch_recall = 36.42, batch_ndcg = 25.06 
2025-04-09 10:36:13.242161: Steps 35/90: batch_recall = 39.80, batch_ndcg = 27.56 
2025-04-09 10:36:13.973261: Steps 36/90: batch_recall = 37.35, batch_ndcg = 24.95 
2025-04-09 10:36:14.699889: Steps 37/90: batch_recall = 33.29, batch_ndcg = 22.17 
2025-04-09 10:36:15.428993: Steps 38/90: batch_recall = 35.85, batch_ndcg = 23.76 
2025-04-09 10:36:16.145369: Steps 39/90: batch_recall = 36.77, batch_ndcg = 23.07 
2025-04-09 10:36:16.879112: Steps 40/90: batch_recall = 31.53, batch_ndcg = 22.24 
2025-04-09 10:36:17.599048: Steps 41/90: batch_recall = 35.45, batch_ndcg = 22.49 
2025-04-09 10:36:18.307733: Steps 42/90: batch_recall = 43.32, batch_ndcg = 29.29 
2025-04-09 10:36:19.018404: Steps 43/90: batch_recall = 36.26, batch_ndcg = 23.41 
2025-04-09 10:36:19.727341: Steps 44/90: batch_recall = 33.73, batch_ndcg = 22.58 
2025-04-09 10:36:20.444225: Steps 45/90: batch_recall = 32.38, batch_ndcg = 22.04 
2025-04-09 10:36:21.144104: Steps 46/90: batch_recall = 35.19, batch_ndcg = 22.05 
2025-04-09 10:36:21.860388: Steps 47/90: batch_recall = 32.72, batch_ndcg = 20.58 
2025-04-09 10:36:22.563868: Steps 48/90: batch_recall = 37.56, batch_ndcg = 24.08 
2025-04-09 10:36:23.276128: Steps 49/90: batch_recall = 32.20, batch_ndcg = 19.66 
2025-04-09 10:36:23.982395: Steps 50/90: batch_recall = 40.56, batch_ndcg = 26.22 
2025-04-09 10:36:24.685067: Steps 51/90: batch_recall = 39.49, batch_ndcg = 26.12 
2025-04-09 10:36:25.408359: Steps 52/90: batch_recall = 39.83, batch_ndcg = 25.35 
2025-04-09 10:36:26.127896: Steps 53/90: batch_recall = 41.19, batch_ndcg = 26.67 
2025-04-09 10:36:26.851033: Steps 54/90: batch_recall = 28.77, batch_ndcg = 19.35 
2025-04-09 10:36:27.566729: Steps 55/90: batch_recall = 35.37, batch_ndcg = 23.36 
2025-04-09 10:36:28.288379: Steps 56/90: batch_recall = 33.05, batch_ndcg = 19.17 
2025-04-09 10:36:29.007647: Steps 57/90: batch_recall = 37.64, batch_ndcg = 25.07 
2025-04-09 10:36:29.712330: Steps 58/90: batch_recall = 40.04, batch_ndcg = 24.46 
2025-04-09 10:36:30.451081: Steps 59/90: batch_recall = 34.51, batch_ndcg = 22.42 
2025-04-09 10:36:31.160800: Steps 60/90: batch_recall = 34.45, batch_ndcg = 22.30 
2025-04-09 10:36:31.876769: Steps 61/90: batch_recall = 40.60, batch_ndcg = 27.95 
2025-04-09 10:36:32.568138: Steps 62/90: batch_recall = 37.93, batch_ndcg = 25.29 
2025-04-09 10:36:33.273014: Steps 63/90: batch_recall = 36.20, batch_ndcg = 23.15 
2025-04-09 10:36:33.958658: Steps 64/90: batch_recall = 36.39, batch_ndcg = 23.25 
2025-04-09 10:36:34.659516: Steps 65/90: batch_recall = 33.82, batch_ndcg = 21.97 
2025-04-09 10:36:35.346192: Steps 66/90: batch_recall = 38.90, batch_ndcg = 23.44 
2025-04-09 10:36:36.055036: Steps 67/90: batch_recall = 32.88, batch_ndcg = 21.35 
2025-04-09 10:36:36.757638: Steps 68/90: batch_recall = 36.41, batch_ndcg = 22.39 
2025-04-09 10:36:37.467694: Steps 69/90: batch_recall = 41.01, batch_ndcg = 24.33 
2025-04-09 10:36:38.194212: Steps 70/90: batch_recall = 39.15, batch_ndcg = 25.74 
2025-04-09 10:36:38.915910: Steps 71/90: batch_recall = 40.06, batch_ndcg = 24.54 
2025-04-09 10:36:39.621202: Steps 72/90: batch_recall = 36.64, batch_ndcg = 22.72 
2025-04-09 10:36:40.333817: Steps 73/90: batch_recall = 35.18, batch_ndcg = 22.18 
2025-04-09 10:36:41.028682: Steps 74/90: batch_recall = 41.23, batch_ndcg = 25.87 
2025-04-09 10:36:41.743208: Steps 75/90: batch_recall = 39.91, batch_ndcg = 24.37 
2025-04-09 10:36:42.468386: Steps 76/90: batch_recall = 43.54, batch_ndcg = 27.04 
2025-04-09 10:36:43.176900: Steps 77/90: batch_recall = 44.25, batch_ndcg = 27.09 
2025-04-09 10:36:43.910007: Steps 78/90: batch_recall = 33.42, batch_ndcg = 20.47 
2025-04-09 10:36:44.618538: Steps 79/90: batch_recall = 40.40, batch_ndcg = 27.10 
2025-04-09 10:36:45.330290: Steps 80/90: batch_recall = 45.97, batch_ndcg = 29.30 
2025-04-09 10:36:46.033026: Steps 81/90: batch_recall = 46.22, batch_ndcg = 29.07 
2025-04-09 10:36:46.732246: Steps 82/90: batch_recall = 42.87, batch_ndcg = 27.42 
2025-04-09 10:36:47.435483: Steps 83/90: batch_recall = 42.76, batch_ndcg = 26.79 
2025-04-09 10:36:48.128603: Steps 84/90: batch_recall = 36.57, batch_ndcg = 22.21 
2025-04-09 10:36:48.824253: Steps 85/90: batch_recall = 41.41, batch_ndcg = 24.33 
2025-04-09 10:36:49.511488: Steps 86/90: batch_recall = 42.54, batch_ndcg = 28.53 
2025-04-09 10:36:50.205410: Steps 87/90: batch_recall = 42.01, batch_ndcg = 28.75 
2025-04-09 10:36:50.902121: Steps 88/90: batch_recall = 49.60, batch_ndcg = 29.03 
2025-04-09 10:36:51.451344: Steps 89/90: batch_recall = 40.07, batch_ndcg = 25.74 
2025-04-09 10:36:51.452042: Epoch 32/1000, Test: Recall = 0.0772, NDCG = 0.0510  

2025-04-09 10:36:52.482167: Training Step 0/115: batchLoss = 4.3553, diffLoss = 7.1413, kgLoss = 0.1761
2025-04-09 10:36:53.210965: Training Step 1/115: batchLoss = 4.1564, diffLoss = 6.8087, kgLoss = 0.1779
2025-04-09 10:36:53.952509: Training Step 2/115: batchLoss = 4.1440, diffLoss = 6.7899, kgLoss = 0.1752
2025-04-09 10:36:54.688008: Training Step 3/115: batchLoss = 4.0503, diffLoss = 6.6414, kgLoss = 0.1638
2025-04-09 10:36:55.426088: Training Step 4/115: batchLoss = 4.4509, diffLoss = 7.3008, kgLoss = 0.1760
2025-04-09 10:36:56.172027: Training Step 5/115: batchLoss = 4.0695, diffLoss = 6.6682, kgLoss = 0.1714
2025-04-09 10:36:56.911222: Training Step 6/115: batchLoss = 4.0512, diffLoss = 6.6375, kgLoss = 0.1719
2025-04-09 10:36:57.641716: Training Step 7/115: batchLoss = 3.9874, diffLoss = 6.5339, kgLoss = 0.1676
2025-04-09 10:36:58.381430: Training Step 8/115: batchLoss = 3.8663, diffLoss = 6.3364, kgLoss = 0.1611
2025-04-09 10:36:59.119283: Training Step 9/115: batchLoss = 4.0135, diffLoss = 6.5745, kgLoss = 0.1720
2025-04-09 10:36:59.848450: Training Step 10/115: batchLoss = 4.3981, diffLoss = 7.2080, kgLoss = 0.1831
2025-04-09 10:37:00.583911: Training Step 11/115: batchLoss = 4.4367, diffLoss = 7.2662, kgLoss = 0.1925
2025-04-09 10:37:01.310925: Training Step 12/115: batchLoss = 4.4124, diffLoss = 7.2287, kgLoss = 0.1880
2025-04-09 10:37:02.053966: Training Step 13/115: batchLoss = 4.4648, diffLoss = 7.3222, kgLoss = 0.1787
2025-04-09 10:37:02.783366: Training Step 14/115: batchLoss = 4.1969, diffLoss = 6.8745, kgLoss = 0.1806
2025-04-09 10:37:03.507083: Training Step 15/115: batchLoss = 4.2475, diffLoss = 6.9641, kgLoss = 0.1725
2025-04-09 10:37:04.231988: Training Step 16/115: batchLoss = 3.9163, diffLoss = 6.4190, kgLoss = 0.1622
2025-04-09 10:37:04.956769: Training Step 17/115: batchLoss = 4.2076, diffLoss = 6.8966, kgLoss = 0.1741
2025-04-09 10:37:05.684481: Training Step 18/115: batchLoss = 4.4387, diffLoss = 7.2774, kgLoss = 0.1806
2025-04-09 10:37:06.416574: Training Step 19/115: batchLoss = 4.4472, diffLoss = 7.2928, kgLoss = 0.1789
2025-04-09 10:37:07.155954: Training Step 20/115: batchLoss = 4.5445, diffLoss = 7.4438, kgLoss = 0.1955
2025-04-09 10:37:07.896772: Training Step 21/115: batchLoss = 4.0373, diffLoss = 6.6187, kgLoss = 0.1652
2025-04-09 10:37:08.629116: Training Step 22/115: batchLoss = 3.8359, diffLoss = 6.2853, kgLoss = 0.1619
2025-04-09 10:37:09.388383: Training Step 23/115: batchLoss = 4.1276, diffLoss = 6.7561, kgLoss = 0.1848
2025-04-09 10:37:10.153917: Training Step 24/115: batchLoss = 4.7186, diffLoss = 7.7313, kgLoss = 0.1995
2025-04-09 10:37:10.922711: Training Step 25/115: batchLoss = 4.3696, diffLoss = 7.1684, kgLoss = 0.1715
2025-04-09 10:37:11.680141: Training Step 26/115: batchLoss = 4.3969, diffLoss = 7.2025, kgLoss = 0.1885
2025-04-09 10:37:12.442517: Training Step 27/115: batchLoss = 3.7602, diffLoss = 6.1631, kgLoss = 0.1560
2025-04-09 10:37:13.194900: Training Step 28/115: batchLoss = 4.5717, diffLoss = 7.5029, kgLoss = 0.1748
2025-04-09 10:37:13.955111: Training Step 29/115: batchLoss = 4.5190, diffLoss = 7.4097, kgLoss = 0.1828
2025-04-09 10:37:14.708524: Training Step 30/115: batchLoss = 4.2475, diffLoss = 6.9547, kgLoss = 0.1868
2025-04-09 10:37:15.446289: Training Step 31/115: batchLoss = 4.1610, diffLoss = 6.8267, kgLoss = 0.1626
2025-04-09 10:37:16.191173: Training Step 32/115: batchLoss = 4.7138, diffLoss = 7.7271, kgLoss = 0.1937
2025-04-09 10:37:16.941114: Training Step 33/115: batchLoss = 4.4324, diffLoss = 7.2627, kgLoss = 0.1870
2025-04-09 10:37:17.697654: Training Step 34/115: batchLoss = 4.5391, diffLoss = 7.4442, kgLoss = 0.1813
2025-04-09 10:37:18.437929: Training Step 35/115: batchLoss = 4.1022, diffLoss = 6.7265, kgLoss = 0.1659
2025-04-09 10:37:19.175364: Training Step 36/115: batchLoss = 4.1746, diffLoss = 6.8444, kgLoss = 0.1701
2025-04-09 10:37:19.922212: Training Step 37/115: batchLoss = 3.8934, diffLoss = 6.3797, kgLoss = 0.1639
2025-04-09 10:37:20.654724: Training Step 38/115: batchLoss = 4.1400, diffLoss = 6.7874, kgLoss = 0.1688
2025-04-09 10:37:21.400964: Training Step 39/115: batchLoss = 4.4951, diffLoss = 7.3744, kgLoss = 0.1762
2025-04-09 10:37:22.143319: Training Step 40/115: batchLoss = 4.3918, diffLoss = 7.2015, kgLoss = 0.1774
2025-04-09 10:37:22.875921: Training Step 41/115: batchLoss = 4.7334, diffLoss = 7.7671, kgLoss = 0.1828
2025-04-09 10:37:23.619564: Training Step 42/115: batchLoss = 4.5462, diffLoss = 7.4522, kgLoss = 0.1872
2025-04-09 10:37:24.379017: Training Step 43/115: batchLoss = 4.0040, diffLoss = 6.5638, kgLoss = 0.1642
2025-04-09 10:37:25.132090: Training Step 44/115: batchLoss = 4.8536, diffLoss = 7.9518, kgLoss = 0.2061
2025-04-09 10:37:25.881021: Training Step 45/115: batchLoss = 4.1297, diffLoss = 6.7700, kgLoss = 0.1693
2025-04-09 10:37:26.632931: Training Step 46/115: batchLoss = 4.1940, diffLoss = 6.8729, kgLoss = 0.1756
2025-04-09 10:37:27.383899: Training Step 47/115: batchLoss = 4.5519, diffLoss = 7.4659, kgLoss = 0.1810
2025-04-09 10:37:28.133845: Training Step 48/115: batchLoss = 4.1047, diffLoss = 6.7347, kgLoss = 0.1597
2025-04-09 10:37:28.893193: Training Step 49/115: batchLoss = 3.8948, diffLoss = 6.3774, kgLoss = 0.1710
2025-04-09 10:37:29.658605: Training Step 50/115: batchLoss = 4.4777, diffLoss = 7.3413, kgLoss = 0.1821
2025-04-09 10:37:30.431934: Training Step 51/115: batchLoss = 4.6313, diffLoss = 7.5941, kgLoss = 0.1871
2025-04-09 10:37:31.178716: Training Step 52/115: batchLoss = 3.9060, diffLoss = 6.4060, kgLoss = 0.1559
2025-04-09 10:37:31.925632: Training Step 53/115: batchLoss = 4.1046, diffLoss = 6.7311, kgLoss = 0.1650
2025-04-09 10:37:32.668259: Training Step 54/115: batchLoss = 4.1155, diffLoss = 6.7454, kgLoss = 0.1707
2025-04-09 10:37:33.408489: Training Step 55/115: batchLoss = 4.1750, diffLoss = 6.8479, kgLoss = 0.1656
2025-04-09 10:37:34.151608: Training Step 56/115: batchLoss = 3.7957, diffLoss = 6.2180, kgLoss = 0.1624
2025-04-09 10:37:34.871449: Training Step 57/115: batchLoss = 4.2763, diffLoss = 7.0151, kgLoss = 0.1682
2025-04-09 10:37:35.598022: Training Step 58/115: batchLoss = 4.2559, diffLoss = 6.9812, kgLoss = 0.1679
2025-04-09 10:37:36.333899: Training Step 59/115: batchLoss = 4.2145, diffLoss = 6.9111, kgLoss = 0.1697
2025-04-09 10:37:37.080311: Training Step 60/115: batchLoss = 3.9020, diffLoss = 6.3958, kgLoss = 0.1613
2025-04-09 10:37:37.818143: Training Step 61/115: batchLoss = 4.5743, diffLoss = 7.5027, kgLoss = 0.1816
2025-04-09 10:37:38.547815: Training Step 62/115: batchLoss = 4.4397, diffLoss = 7.2790, kgLoss = 0.1806
2025-04-09 10:37:39.293838: Training Step 63/115: batchLoss = 4.1544, diffLoss = 6.8079, kgLoss = 0.1742
2025-04-09 10:37:40.041015: Training Step 64/115: batchLoss = 4.0280, diffLoss = 6.6050, kgLoss = 0.1625
2025-04-09 10:37:40.775414: Training Step 65/115: batchLoss = 4.4905, diffLoss = 7.3515, kgLoss = 0.1991
2025-04-09 10:37:41.515768: Training Step 66/115: batchLoss = 3.9250, diffLoss = 6.4366, kgLoss = 0.1576
2025-04-09 10:37:42.255765: Training Step 67/115: batchLoss = 3.9687, diffLoss = 6.4987, kgLoss = 0.1738
2025-04-09 10:37:42.989075: Training Step 68/115: batchLoss = 4.0872, diffLoss = 6.6954, kgLoss = 0.1749
2025-04-09 10:37:43.721032: Training Step 69/115: batchLoss = 3.9305, diffLoss = 6.4390, kgLoss = 0.1678
2025-04-09 10:37:44.462100: Training Step 70/115: batchLoss = 3.9679, diffLoss = 6.5025, kgLoss = 0.1660
2025-04-09 10:37:45.211026: Training Step 71/115: batchLoss = 4.4805, diffLoss = 7.3453, kgLoss = 0.1833
2025-04-09 10:37:45.967672: Training Step 72/115: batchLoss = 3.5993, diffLoss = 5.8978, kgLoss = 0.1515
2025-04-09 10:37:46.725845: Training Step 73/115: batchLoss = 4.6231, diffLoss = 7.5844, kgLoss = 0.1810
2025-04-09 10:37:47.462703: Training Step 74/115: batchLoss = 4.2798, diffLoss = 7.0205, kgLoss = 0.1687
2025-04-09 10:37:48.192655: Training Step 75/115: batchLoss = 4.5548, diffLoss = 7.4614, kgLoss = 0.1950
2025-04-09 10:37:48.935119: Training Step 76/115: batchLoss = 4.4656, diffLoss = 7.3205, kgLoss = 0.1834
2025-04-09 10:37:49.667492: Training Step 77/115: batchLoss = 4.5084, diffLoss = 7.3951, kgLoss = 0.1785
2025-04-09 10:37:50.397678: Training Step 78/115: batchLoss = 4.7404, diffLoss = 7.7733, kgLoss = 0.1910
2025-04-09 10:37:51.131142: Training Step 79/115: batchLoss = 3.6832, diffLoss = 6.0404, kgLoss = 0.1474
2025-04-09 10:37:51.880215: Training Step 80/115: batchLoss = 4.9815, diffLoss = 8.1732, kgLoss = 0.1940
2025-04-09 10:37:52.620015: Training Step 81/115: batchLoss = 4.1925, diffLoss = 6.8770, kgLoss = 0.1658
2025-04-09 10:37:53.363934: Training Step 82/115: batchLoss = 4.2031, diffLoss = 6.8866, kgLoss = 0.1779
2025-04-09 10:37:54.116626: Training Step 83/115: batchLoss = 3.9965, diffLoss = 6.5506, kgLoss = 0.1654
2025-04-09 10:37:54.878773: Training Step 84/115: batchLoss = 4.0981, diffLoss = 6.7128, kgLoss = 0.1762
2025-04-09 10:37:55.624452: Training Step 85/115: batchLoss = 3.8307, diffLoss = 6.2803, kgLoss = 0.1565
2025-04-09 10:37:56.363877: Training Step 86/115: batchLoss = 3.9137, diffLoss = 6.4140, kgLoss = 0.1633
2025-04-09 10:37:57.111393: Training Step 87/115: batchLoss = 4.7433, diffLoss = 7.7777, kgLoss = 0.1918
2025-04-09 10:37:57.836982: Training Step 88/115: batchLoss = 4.0472, diffLoss = 6.6393, kgLoss = 0.1590
2025-04-09 10:37:58.579677: Training Step 89/115: batchLoss = 3.7737, diffLoss = 6.1871, kgLoss = 0.1535
2025-04-09 10:37:59.314147: Training Step 90/115: batchLoss = 3.8655, diffLoss = 6.3281, kgLoss = 0.1715
2025-04-09 10:38:00.062415: Training Step 91/115: batchLoss = 4.1506, diffLoss = 6.8045, kgLoss = 0.1697
2025-04-09 10:38:00.802395: Training Step 92/115: batchLoss = 3.9110, diffLoss = 6.4020, kgLoss = 0.1744
2025-04-09 10:38:01.540288: Training Step 93/115: batchLoss = 4.1997, diffLoss = 6.8837, kgLoss = 0.1736
2025-04-09 10:38:02.279785: Training Step 94/115: batchLoss = 4.3338, diffLoss = 7.1070, kgLoss = 0.1739
2025-04-09 10:38:03.017033: Training Step 95/115: batchLoss = 4.5095, diffLoss = 7.3958, kgLoss = 0.1802
2025-04-09 10:38:03.751234: Training Step 96/115: batchLoss = 4.5988, diffLoss = 7.5452, kgLoss = 0.1794
2025-04-09 10:38:04.483051: Training Step 97/115: batchLoss = 4.6073, diffLoss = 7.5598, kgLoss = 0.1785
2025-04-09 10:38:05.230570: Training Step 98/115: batchLoss = 4.2175, diffLoss = 6.9085, kgLoss = 0.1810
2025-04-09 10:38:05.973443: Training Step 99/115: batchLoss = 3.9570, diffLoss = 6.4824, kgLoss = 0.1689
2025-04-09 10:38:06.725405: Training Step 100/115: batchLoss = 4.3509, diffLoss = 7.1226, kgLoss = 0.1932
2025-04-09 10:38:07.469596: Training Step 101/115: batchLoss = 4.4056, diffLoss = 7.2192, kgLoss = 0.1852
2025-04-09 10:38:08.206610: Training Step 102/115: batchLoss = 4.1678, diffLoss = 6.8386, kgLoss = 0.1615
2025-04-09 10:38:08.943298: Training Step 103/115: batchLoss = 4.3116, diffLoss = 7.0656, kgLoss = 0.1806
2025-04-09 10:38:09.679679: Training Step 104/115: batchLoss = 4.3092, diffLoss = 7.0612, kgLoss = 0.1811
2025-04-09 10:38:10.425714: Training Step 105/115: batchLoss = 4.6187, diffLoss = 7.5786, kgLoss = 0.1789
2025-04-09 10:38:11.176543: Training Step 106/115: batchLoss = 3.8688, diffLoss = 6.3455, kgLoss = 0.1537
2025-04-09 10:38:11.945804: Training Step 107/115: batchLoss = 3.9996, diffLoss = 6.5507, kgLoss = 0.1730
2025-04-09 10:38:12.689050: Training Step 108/115: batchLoss = 4.0794, diffLoss = 6.6903, kgLoss = 0.1630
2025-04-09 10:38:13.433235: Training Step 109/115: batchLoss = 4.2236, diffLoss = 6.9235, kgLoss = 0.1739
2025-04-09 10:38:14.169703: Training Step 110/115: batchLoss = 4.0066, diffLoss = 6.5717, kgLoss = 0.1588
2025-04-09 10:38:14.913215: Training Step 111/115: batchLoss = 4.2135, diffLoss = 6.9094, kgLoss = 0.1695
2025-04-09 10:38:15.637313: Training Step 112/115: batchLoss = 3.5282, diffLoss = 5.7792, kgLoss = 0.1518
2025-04-09 10:38:16.292162: Training Step 113/115: batchLoss = 3.9354, diffLoss = 6.4521, kgLoss = 0.1604
2025-04-09 10:38:16.909694: Training Step 114/115: batchLoss = 4.0788, diffLoss = 6.6902, kgLoss = 0.1616
2025-04-09 10:38:17.020483: 
2025-04-09 10:38:17.021622: Epoch 33/1000, Train: epLoss = 1.2152, epDfLoss = 1.9920, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:38:17.808560: Steps 0/90: batch_recall = 54.63, batch_ndcg = 42.96 
2025-04-09 10:38:18.566079: Steps 1/90: batch_recall = 57.00, batch_ndcg = 41.25 
2025-04-09 10:38:19.332641: Steps 2/90: batch_recall = 49.91, batch_ndcg = 37.20 
2025-04-09 10:38:20.092311: Steps 3/90: batch_recall = 45.97, batch_ndcg = 32.74 
2025-04-09 10:38:20.850362: Steps 4/90: batch_recall = 47.48, batch_ndcg = 35.76 
2025-04-09 10:38:21.608088: Steps 5/90: batch_recall = 38.52, batch_ndcg = 28.73 
2025-04-09 10:38:22.380698: Steps 6/90: batch_recall = 47.82, batch_ndcg = 33.00 
2025-04-09 10:38:23.134661: Steps 7/90: batch_recall = 43.23, batch_ndcg = 31.51 
2025-04-09 10:38:23.897637: Steps 8/90: batch_recall = 46.61, batch_ndcg = 33.71 
2025-04-09 10:38:24.675362: Steps 9/90: batch_recall = 43.90, batch_ndcg = 33.01 
2025-04-09 10:38:25.442920: Steps 10/90: batch_recall = 39.55, batch_ndcg = 29.09 
2025-04-09 10:38:26.197087: Steps 11/90: batch_recall = 42.96, batch_ndcg = 28.54 
2025-04-09 10:38:26.935306: Steps 12/90: batch_recall = 38.29, batch_ndcg = 27.06 
2025-04-09 10:38:27.687074: Steps 13/90: batch_recall = 36.94, batch_ndcg = 25.93 
2025-04-09 10:38:28.434958: Steps 14/90: batch_recall = 36.51, batch_ndcg = 26.02 
2025-04-09 10:38:29.171724: Steps 15/90: batch_recall = 44.95, batch_ndcg = 29.28 
2025-04-09 10:38:29.909148: Steps 16/90: batch_recall = 35.58, batch_ndcg = 25.11 
2025-04-09 10:38:30.660793: Steps 17/90: batch_recall = 34.88, batch_ndcg = 23.02 
2025-04-09 10:38:31.389488: Steps 18/90: batch_recall = 39.98, batch_ndcg = 26.99 
2025-04-09 10:38:32.120807: Steps 19/90: batch_recall = 34.05, batch_ndcg = 23.83 
2025-04-09 10:38:32.843310: Steps 20/90: batch_recall = 38.60, batch_ndcg = 25.47 
2025-04-09 10:38:33.584045: Steps 21/90: batch_recall = 42.21, batch_ndcg = 30.47 
2025-04-09 10:38:34.310016: Steps 22/90: batch_recall = 37.79, batch_ndcg = 27.60 
2025-04-09 10:38:35.026369: Steps 23/90: batch_recall = 39.41, batch_ndcg = 26.34 
2025-04-09 10:38:35.737473: Steps 24/90: batch_recall = 38.63, batch_ndcg = 27.34 
2025-04-09 10:38:36.458841: Steps 25/90: batch_recall = 38.89, batch_ndcg = 25.46 
2025-04-09 10:38:37.170633: Steps 26/90: batch_recall = 37.37, batch_ndcg = 27.52 
2025-04-09 10:38:37.879777: Steps 27/90: batch_recall = 34.19, batch_ndcg = 23.50 
2025-04-09 10:38:38.603671: Steps 28/90: batch_recall = 39.69, batch_ndcg = 26.44 
2025-04-09 10:38:39.327124: Steps 29/90: batch_recall = 35.40, batch_ndcg = 24.16 
2025-04-09 10:38:40.047194: Steps 30/90: batch_recall = 33.99, batch_ndcg = 22.47 
2025-04-09 10:38:40.767218: Steps 31/90: batch_recall = 33.05, batch_ndcg = 23.75 
2025-04-09 10:38:41.481103: Steps 32/90: batch_recall = 35.27, batch_ndcg = 23.77 
2025-04-09 10:38:42.206265: Steps 33/90: batch_recall = 38.15, batch_ndcg = 23.70 
2025-04-09 10:38:42.933853: Steps 34/90: batch_recall = 36.87, batch_ndcg = 24.79 
2025-04-09 10:38:43.669332: Steps 35/90: batch_recall = 38.83, batch_ndcg = 27.22 
2025-04-09 10:38:44.400200: Steps 36/90: batch_recall = 37.36, batch_ndcg = 25.11 
2025-04-09 10:38:45.119091: Steps 37/90: batch_recall = 34.01, batch_ndcg = 22.28 
2025-04-09 10:38:45.853104: Steps 38/90: batch_recall = 36.29, batch_ndcg = 23.81 
2025-04-09 10:38:46.576988: Steps 39/90: batch_recall = 36.35, batch_ndcg = 22.90 
2025-04-09 10:38:47.296841: Steps 40/90: batch_recall = 31.51, batch_ndcg = 22.37 
2025-04-09 10:38:48.006059: Steps 41/90: batch_recall = 35.68, batch_ndcg = 22.68 
2025-04-09 10:38:48.722209: Steps 42/90: batch_recall = 44.26, batch_ndcg = 29.70 
2025-04-09 10:38:49.434676: Steps 43/90: batch_recall = 35.56, batch_ndcg = 23.08 
2025-04-09 10:38:50.168596: Steps 44/90: batch_recall = 34.32, batch_ndcg = 23.20 
2025-04-09 10:38:50.883691: Steps 45/90: batch_recall = 32.72, batch_ndcg = 22.21 
2025-04-09 10:38:51.571424: Steps 46/90: batch_recall = 35.94, batch_ndcg = 22.52 
2025-04-09 10:38:52.269487: Steps 47/90: batch_recall = 31.73, batch_ndcg = 20.42 
2025-04-09 10:38:52.975505: Steps 48/90: batch_recall = 37.50, batch_ndcg = 24.05 
2025-04-09 10:38:53.688498: Steps 49/90: batch_recall = 31.86, batch_ndcg = 19.59 
2025-04-09 10:38:54.402608: Steps 50/90: batch_recall = 39.69, batch_ndcg = 25.77 
2025-04-09 10:38:55.114493: Steps 51/90: batch_recall = 38.97, batch_ndcg = 26.03 
2025-04-09 10:38:55.828269: Steps 52/90: batch_recall = 38.08, batch_ndcg = 24.67 
2025-04-09 10:38:56.552215: Steps 53/90: batch_recall = 41.69, batch_ndcg = 27.12 
2025-04-09 10:38:57.263901: Steps 54/90: batch_recall = 29.83, batch_ndcg = 19.60 
2025-04-09 10:38:57.969672: Steps 55/90: batch_recall = 35.36, batch_ndcg = 23.42 
2025-04-09 10:38:58.690738: Steps 56/90: batch_recall = 33.13, batch_ndcg = 19.09 
2025-04-09 10:38:59.421283: Steps 57/90: batch_recall = 38.80, batch_ndcg = 25.32 
2025-04-09 10:39:00.145508: Steps 58/90: batch_recall = 39.99, batch_ndcg = 24.21 
2025-04-09 10:39:00.864307: Steps 59/90: batch_recall = 35.77, batch_ndcg = 23.00 
2025-04-09 10:39:01.576536: Steps 60/90: batch_recall = 35.79, batch_ndcg = 22.73 
2025-04-09 10:39:02.292224: Steps 61/90: batch_recall = 40.55, batch_ndcg = 27.68 
2025-04-09 10:39:02.989510: Steps 62/90: batch_recall = 39.25, batch_ndcg = 26.05 
2025-04-09 10:39:03.713522: Steps 63/90: batch_recall = 35.49, batch_ndcg = 23.12 
2025-04-09 10:39:04.424643: Steps 64/90: batch_recall = 35.47, batch_ndcg = 23.05 
2025-04-09 10:39:05.145864: Steps 65/90: batch_recall = 33.41, batch_ndcg = 21.87 
2025-04-09 10:39:05.860573: Steps 66/90: batch_recall = 38.73, batch_ndcg = 23.61 
2025-04-09 10:39:06.565536: Steps 67/90: batch_recall = 32.34, batch_ndcg = 21.04 
2025-04-09 10:39:07.257995: Steps 68/90: batch_recall = 37.08, batch_ndcg = 22.46 
2025-04-09 10:39:07.972122: Steps 69/90: batch_recall = 39.73, batch_ndcg = 24.02 
2025-04-09 10:39:08.671683: Steps 70/90: batch_recall = 38.50, batch_ndcg = 25.70 
2025-04-09 10:39:09.379685: Steps 71/90: batch_recall = 39.58, batch_ndcg = 24.78 
2025-04-09 10:39:10.086826: Steps 72/90: batch_recall = 35.83, batch_ndcg = 22.35 
2025-04-09 10:39:10.795724: Steps 73/90: batch_recall = 34.67, batch_ndcg = 21.96 
2025-04-09 10:39:11.496796: Steps 74/90: batch_recall = 40.01, batch_ndcg = 25.02 
2025-04-09 10:39:12.205031: Steps 75/90: batch_recall = 39.56, batch_ndcg = 24.24 
2025-04-09 10:39:12.924511: Steps 76/90: batch_recall = 44.09, batch_ndcg = 27.51 
2025-04-09 10:39:13.654941: Steps 77/90: batch_recall = 44.11, batch_ndcg = 27.14 
2025-04-09 10:39:14.364927: Steps 78/90: batch_recall = 33.07, batch_ndcg = 20.41 
2025-04-09 10:39:15.072588: Steps 79/90: batch_recall = 40.15, batch_ndcg = 26.76 
2025-04-09 10:39:15.785199: Steps 80/90: batch_recall = 44.88, batch_ndcg = 29.06 
2025-04-09 10:39:16.492963: Steps 81/90: batch_recall = 47.28, batch_ndcg = 28.82 
2025-04-09 10:39:17.196666: Steps 82/90: batch_recall = 43.89, batch_ndcg = 27.79 
2025-04-09 10:39:17.908485: Steps 83/90: batch_recall = 43.24, batch_ndcg = 26.77 
2025-04-09 10:39:18.629073: Steps 84/90: batch_recall = 35.92, batch_ndcg = 22.32 
2025-04-09 10:39:19.320604: Steps 85/90: batch_recall = 42.18, batch_ndcg = 24.77 
2025-04-09 10:39:20.019154: Steps 86/90: batch_recall = 43.31, batch_ndcg = 28.20 
2025-04-09 10:39:20.717806: Steps 87/90: batch_recall = 42.75, batch_ndcg = 29.28 
2025-04-09 10:39:21.409538: Steps 88/90: batch_recall = 49.94, batch_ndcg = 29.72 
2025-04-09 10:39:21.947006: Steps 89/90: batch_recall = 41.57, batch_ndcg = 26.33 
2025-04-09 10:39:21.947631: Epoch 33/1000, Test: Recall = 0.0770, NDCG = 0.0510  

2025-04-09 10:39:22.963628: Training Step 0/115: batchLoss = 4.1122, diffLoss = 6.7417, kgLoss = 0.1680
2025-04-09 10:39:23.697285: Training Step 1/115: batchLoss = 4.2959, diffLoss = 7.0461, kgLoss = 0.1707
2025-04-09 10:39:24.422203: Training Step 2/115: batchLoss = 4.3629, diffLoss = 7.1576, kgLoss = 0.1710
2025-04-09 10:39:25.153345: Training Step 3/115: batchLoss = 4.4196, diffLoss = 7.2444, kgLoss = 0.1825
2025-04-09 10:39:25.890659: Training Step 4/115: batchLoss = 4.5944, diffLoss = 7.5370, kgLoss = 0.1805
2025-04-09 10:39:26.620536: Training Step 5/115: batchLoss = 3.6019, diffLoss = 5.9041, kgLoss = 0.1485
2025-04-09 10:39:27.348782: Training Step 6/115: batchLoss = 4.5755, diffLoss = 7.4998, kgLoss = 0.1890
2025-04-09 10:39:28.075334: Training Step 7/115: batchLoss = 4.0240, diffLoss = 6.5937, kgLoss = 0.1694
2025-04-09 10:39:28.797527: Training Step 8/115: batchLoss = 3.9317, diffLoss = 6.4386, kgLoss = 0.1713
2025-04-09 10:39:29.518205: Training Step 9/115: batchLoss = 3.8324, diffLoss = 6.2740, kgLoss = 0.1699
2025-04-09 10:39:30.243943: Training Step 10/115: batchLoss = 4.3232, diffLoss = 7.0862, kgLoss = 0.1786
2025-04-09 10:39:30.971506: Training Step 11/115: batchLoss = 3.9256, diffLoss = 6.4362, kgLoss = 0.1596
2025-04-09 10:39:31.697509: Training Step 12/115: batchLoss = 4.4662, diffLoss = 7.3250, kgLoss = 0.1780
2025-04-09 10:39:32.432745: Training Step 13/115: batchLoss = 4.0149, diffLoss = 6.5866, kgLoss = 0.1574
2025-04-09 10:39:33.153840: Training Step 14/115: batchLoss = 4.1027, diffLoss = 6.7235, kgLoss = 0.1715
2025-04-09 10:39:33.870253: Training Step 15/115: batchLoss = 3.7854, diffLoss = 6.1985, kgLoss = 0.1659
2025-04-09 10:39:34.595967: Training Step 16/115: batchLoss = 4.3484, diffLoss = 7.1338, kgLoss = 0.1703
2025-04-09 10:39:35.312143: Training Step 17/115: batchLoss = 4.1507, diffLoss = 6.8023, kgLoss = 0.1732
2025-04-09 10:39:36.026960: Training Step 18/115: batchLoss = 4.5104, diffLoss = 7.3988, kgLoss = 0.1777
2025-04-09 10:39:36.745646: Training Step 19/115: batchLoss = 4.8454, diffLoss = 7.9451, kgLoss = 0.1958
2025-04-09 10:39:37.464723: Training Step 20/115: batchLoss = 4.0571, diffLoss = 6.6487, kgLoss = 0.1696
2025-04-09 10:39:38.184031: Training Step 21/115: batchLoss = 4.2131, diffLoss = 6.9112, kgLoss = 0.1658
2025-04-09 10:39:38.900884: Training Step 22/115: batchLoss = 4.3778, diffLoss = 7.1843, kgLoss = 0.1681
2025-04-09 10:39:39.620479: Training Step 23/115: batchLoss = 4.1377, diffLoss = 6.7794, kgLoss = 0.1751
2025-04-09 10:39:40.354069: Training Step 24/115: batchLoss = 3.9730, diffLoss = 6.5046, kgLoss = 0.1758
2025-04-09 10:39:41.089946: Training Step 25/115: batchLoss = 4.1366, diffLoss = 6.7758, kgLoss = 0.1777
2025-04-09 10:39:41.825259: Training Step 26/115: batchLoss = 4.6813, diffLoss = 7.6793, kgLoss = 0.1842
2025-04-09 10:39:42.560891: Training Step 27/115: batchLoss = 4.3473, diffLoss = 7.1320, kgLoss = 0.1704
2025-04-09 10:39:43.305027: Training Step 28/115: batchLoss = 4.0030, diffLoss = 6.5574, kgLoss = 0.1715
2025-04-09 10:39:44.036459: Training Step 29/115: batchLoss = 4.3042, diffLoss = 7.0569, kgLoss = 0.1751
2025-04-09 10:39:44.776841: Training Step 30/115: batchLoss = 4.3791, diffLoss = 7.1752, kgLoss = 0.1850
2025-04-09 10:39:45.520395: Training Step 31/115: batchLoss = 4.3157, diffLoss = 7.0724, kgLoss = 0.1806
2025-04-09 10:39:46.262569: Training Step 32/115: batchLoss = 4.9189, diffLoss = 8.0650, kgLoss = 0.1997
2025-04-09 10:39:47.002739: Training Step 33/115: batchLoss = 4.1389, diffLoss = 6.7894, kgLoss = 0.1632
2025-04-09 10:39:47.741689: Training Step 34/115: batchLoss = 3.8448, diffLoss = 6.3013, kgLoss = 0.1599
2025-04-09 10:39:48.477058: Training Step 35/115: batchLoss = 4.3907, diffLoss = 7.2044, kgLoss = 0.1701
2025-04-09 10:39:49.208801: Training Step 36/115: batchLoss = 4.1138, diffLoss = 6.7428, kgLoss = 0.1704
2025-04-09 10:39:49.942072: Training Step 37/115: batchLoss = 4.2132, diffLoss = 6.9108, kgLoss = 0.1667
2025-04-09 10:39:50.669340: Training Step 38/115: batchLoss = 5.0754, diffLoss = 8.3251, kgLoss = 0.2008
2025-04-09 10:39:51.392237: Training Step 39/115: batchLoss = 4.3489, diffLoss = 7.1324, kgLoss = 0.1737
2025-04-09 10:39:52.128111: Training Step 40/115: batchLoss = 4.4018, diffLoss = 7.2206, kgLoss = 0.1737
2025-04-09 10:39:52.858265: Training Step 41/115: batchLoss = 3.5257, diffLoss = 5.7741, kgLoss = 0.1532
2025-04-09 10:39:53.593220: Training Step 42/115: batchLoss = 4.4918, diffLoss = 7.3660, kgLoss = 0.1806
2025-04-09 10:39:54.335356: Training Step 43/115: batchLoss = 3.8077, diffLoss = 6.2394, kgLoss = 0.1601
2025-04-09 10:39:55.075881: Training Step 44/115: batchLoss = 4.1979, diffLoss = 6.8768, kgLoss = 0.1796
2025-04-09 10:39:55.814757: Training Step 45/115: batchLoss = 3.9238, diffLoss = 6.4326, kgLoss = 0.1606
2025-04-09 10:39:56.549997: Training Step 46/115: batchLoss = 3.9224, diffLoss = 6.4270, kgLoss = 0.1656
2025-04-09 10:39:57.296131: Training Step 47/115: batchLoss = 4.4618, diffLoss = 7.3169, kgLoss = 0.1792
2025-04-09 10:39:58.042563: Training Step 48/115: batchLoss = 4.5368, diffLoss = 7.4406, kgLoss = 0.1810
2025-04-09 10:39:58.780639: Training Step 49/115: batchLoss = 4.5326, diffLoss = 7.4314, kgLoss = 0.1844
2025-04-09 10:39:59.516980: Training Step 50/115: batchLoss = 4.1665, diffLoss = 6.8364, kgLoss = 0.1618
2025-04-09 10:40:00.256538: Training Step 51/115: batchLoss = 4.6488, diffLoss = 7.6316, kgLoss = 0.1747
2025-04-09 10:40:00.996126: Training Step 52/115: batchLoss = 3.9120, diffLoss = 6.4165, kgLoss = 0.1553
2025-04-09 10:40:01.735908: Training Step 53/115: batchLoss = 4.4253, diffLoss = 7.2602, kgLoss = 0.1728
2025-04-09 10:40:02.470318: Training Step 54/115: batchLoss = 4.5109, diffLoss = 7.3979, kgLoss = 0.1804
2025-04-09 10:40:03.216066: Training Step 55/115: batchLoss = 4.3294, diffLoss = 7.1025, kgLoss = 0.1697
2025-04-09 10:40:03.959573: Training Step 56/115: batchLoss = 4.4546, diffLoss = 7.3067, kgLoss = 0.1764
2025-04-09 10:40:04.708637: Training Step 57/115: batchLoss = 4.6665, diffLoss = 7.6488, kgLoss = 0.1930
2025-04-09 10:40:05.459150: Training Step 58/115: batchLoss = 4.3855, diffLoss = 7.1914, kgLoss = 0.1766
2025-04-09 10:40:06.210166: Training Step 59/115: batchLoss = 3.9398, diffLoss = 6.4612, kgLoss = 0.1576
2025-04-09 10:40:06.941559: Training Step 60/115: batchLoss = 4.2420, diffLoss = 6.9601, kgLoss = 0.1648
2025-04-09 10:40:07.672607: Training Step 61/115: batchLoss = 4.3815, diffLoss = 7.1782, kgLoss = 0.1863
2025-04-09 10:40:08.406420: Training Step 62/115: batchLoss = 4.5002, diffLoss = 7.3821, kgLoss = 0.1774
2025-04-09 10:40:09.154016: Training Step 63/115: batchLoss = 4.4527, diffLoss = 7.2986, kgLoss = 0.1839
2025-04-09 10:40:09.896820: Training Step 64/115: batchLoss = 4.2980, diffLoss = 7.0503, kgLoss = 0.1694
2025-04-09 10:40:10.635261: Training Step 65/115: batchLoss = 4.5020, diffLoss = 7.3870, kgLoss = 0.1745
2025-04-09 10:40:11.377858: Training Step 66/115: batchLoss = 3.5923, diffLoss = 5.8843, kgLoss = 0.1544
2025-04-09 10:40:12.127130: Training Step 67/115: batchLoss = 4.5031, diffLoss = 7.3814, kgLoss = 0.1856
2025-04-09 10:40:12.869675: Training Step 68/115: batchLoss = 3.9970, diffLoss = 6.5404, kgLoss = 0.1819
2025-04-09 10:40:13.615878: Training Step 69/115: batchLoss = 4.1608, diffLoss = 6.8195, kgLoss = 0.1729
2025-04-09 10:40:14.374033: Training Step 70/115: batchLoss = 4.3292, diffLoss = 7.0970, kgLoss = 0.1776
2025-04-09 10:40:15.137715: Training Step 71/115: batchLoss = 4.1833, diffLoss = 6.8552, kgLoss = 0.1756
2025-04-09 10:40:15.903995: Training Step 72/115: batchLoss = 4.1472, diffLoss = 6.8006, kgLoss = 0.1671
2025-04-09 10:40:16.651791: Training Step 73/115: batchLoss = 4.3744, diffLoss = 7.1709, kgLoss = 0.1796
2025-04-09 10:40:17.379867: Training Step 74/115: batchLoss = 4.1518, diffLoss = 6.8144, kgLoss = 0.1578
2025-04-09 10:40:18.122198: Training Step 75/115: batchLoss = 4.1183, diffLoss = 6.7521, kgLoss = 0.1677
2025-04-09 10:40:18.870220: Training Step 76/115: batchLoss = 4.2724, diffLoss = 7.0018, kgLoss = 0.1783
2025-04-09 10:40:19.593811: Training Step 77/115: batchLoss = 4.3961, diffLoss = 7.2038, kgLoss = 0.1844
2025-04-09 10:40:20.327994: Training Step 78/115: batchLoss = 4.3773, diffLoss = 7.1669, kgLoss = 0.1930
2025-04-09 10:40:21.079841: Training Step 79/115: batchLoss = 4.1341, diffLoss = 6.7778, kgLoss = 0.1686
2025-04-09 10:40:21.798804: Training Step 80/115: batchLoss = 3.8714, diffLoss = 6.3457, kgLoss = 0.1599
2025-04-09 10:40:22.536589: Training Step 81/115: batchLoss = 4.5126, diffLoss = 7.4040, kgLoss = 0.1753
2025-04-09 10:40:23.268621: Training Step 82/115: batchLoss = 4.2897, diffLoss = 7.0226, kgLoss = 0.1904
2025-04-09 10:40:24.009841: Training Step 83/115: batchLoss = 4.6773, diffLoss = 7.6746, kgLoss = 0.1812
2025-04-09 10:40:24.761215: Training Step 84/115: batchLoss = 4.3678, diffLoss = 7.1589, kgLoss = 0.1813
2025-04-09 10:40:25.498322: Training Step 85/115: batchLoss = 4.6315, diffLoss = 7.5938, kgLoss = 0.1880
2025-04-09 10:40:26.251271: Training Step 86/115: batchLoss = 3.8177, diffLoss = 6.2586, kgLoss = 0.1564
2025-04-09 10:40:27.003837: Training Step 87/115: batchLoss = 4.2651, diffLoss = 6.9959, kgLoss = 0.1690
2025-04-09 10:40:27.761412: Training Step 88/115: batchLoss = 4.3019, diffLoss = 7.0513, kgLoss = 0.1778
2025-04-09 10:40:28.505258: Training Step 89/115: batchLoss = 3.8475, diffLoss = 6.3058, kgLoss = 0.1600
2025-04-09 10:40:29.257630: Training Step 90/115: batchLoss = 4.2344, diffLoss = 6.9412, kgLoss = 0.1743
2025-04-09 10:40:30.000620: Training Step 91/115: batchLoss = 4.2814, diffLoss = 7.0176, kgLoss = 0.1771
2025-04-09 10:40:30.738799: Training Step 92/115: batchLoss = 3.4019, diffLoss = 5.5666, kgLoss = 0.1548
2025-04-09 10:40:31.490085: Training Step 93/115: batchLoss = 3.8413, diffLoss = 6.2920, kgLoss = 0.1652
2025-04-09 10:40:32.252688: Training Step 94/115: batchLoss = 4.6095, diffLoss = 7.5559, kgLoss = 0.1900
2025-04-09 10:40:33.006193: Training Step 95/115: batchLoss = 3.7793, diffLoss = 6.1941, kgLoss = 0.1572
2025-04-09 10:40:33.770700: Training Step 96/115: batchLoss = 4.5972, diffLoss = 7.5415, kgLoss = 0.1809
2025-04-09 10:40:34.506725: Training Step 97/115: batchLoss = 3.9812, diffLoss = 6.5229, kgLoss = 0.1687
2025-04-09 10:40:35.249846: Training Step 98/115: batchLoss = 3.9500, diffLoss = 6.4709, kgLoss = 0.1687
2025-04-09 10:40:35.986173: Training Step 99/115: batchLoss = 4.1948, diffLoss = 6.8825, kgLoss = 0.1632
2025-04-09 10:40:36.719125: Training Step 100/115: batchLoss = 4.3639, diffLoss = 7.1541, kgLoss = 0.1785
2025-04-09 10:40:37.457278: Training Step 101/115: batchLoss = 3.9205, diffLoss = 6.4249, kgLoss = 0.1638
2025-04-09 10:40:38.191591: Training Step 102/115: batchLoss = 4.0674, diffLoss = 6.6687, kgLoss = 0.1654
2025-04-09 10:40:38.933027: Training Step 103/115: batchLoss = 4.2289, diffLoss = 6.9369, kgLoss = 0.1669
2025-04-09 10:40:39.672341: Training Step 104/115: batchLoss = 4.4689, diffLoss = 7.3289, kgLoss = 0.1790
2025-04-09 10:40:40.419215: Training Step 105/115: batchLoss = 4.3373, diffLoss = 7.1093, kgLoss = 0.1794
2025-04-09 10:40:41.160893: Training Step 106/115: batchLoss = 4.3454, diffLoss = 7.1263, kgLoss = 0.1740
2025-04-09 10:40:41.899112: Training Step 107/115: batchLoss = 4.3697, diffLoss = 7.1559, kgLoss = 0.1904
2025-04-09 10:40:42.639682: Training Step 108/115: batchLoss = 3.8700, diffLoss = 6.3409, kgLoss = 0.1636
2025-04-09 10:40:43.391140: Training Step 109/115: batchLoss = 3.9450, diffLoss = 6.4697, kgLoss = 0.1580
2025-04-09 10:40:44.133826: Training Step 110/115: batchLoss = 4.6015, diffLoss = 7.5253, kgLoss = 0.2159
2025-04-09 10:40:44.874704: Training Step 111/115: batchLoss = 4.2707, diffLoss = 7.0030, kgLoss = 0.1721
2025-04-09 10:40:45.600002: Training Step 112/115: batchLoss = 4.6091, diffLoss = 7.5456, kgLoss = 0.2043
2025-04-09 10:40:46.238124: Training Step 113/115: batchLoss = 4.3012, diffLoss = 7.0496, kgLoss = 0.1786
2025-04-09 10:40:46.878029: Training Step 114/115: batchLoss = 4.0207, diffLoss = 6.5876, kgLoss = 0.1704
2025-04-09 10:40:47.012048: 
2025-04-09 10:40:47.013087: Epoch 34/1000, Train: epLoss = 1.2193, epDfLoss = 1.9989, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:40:47.766158: Steps 0/90: batch_recall = 56.00, batch_ndcg = 43.62 
2025-04-09 10:40:48.492561: Steps 1/90: batch_recall = 57.35, batch_ndcg = 41.97 
2025-04-09 10:40:49.231190: Steps 2/90: batch_recall = 49.74, batch_ndcg = 37.30 
2025-04-09 10:40:49.967683: Steps 3/90: batch_recall = 45.85, batch_ndcg = 32.82 
2025-04-09 10:40:50.697764: Steps 4/90: batch_recall = 47.53, batch_ndcg = 35.85 
2025-04-09 10:40:51.429059: Steps 5/90: batch_recall = 37.60, batch_ndcg = 28.10 
2025-04-09 10:40:52.155125: Steps 6/90: batch_recall = 47.19, batch_ndcg = 32.62 
2025-04-09 10:40:52.905535: Steps 7/90: batch_recall = 44.32, batch_ndcg = 32.15 
2025-04-09 10:40:53.634416: Steps 8/90: batch_recall = 47.12, batch_ndcg = 34.09 
2025-04-09 10:40:54.383908: Steps 9/90: batch_recall = 44.32, batch_ndcg = 32.93 
2025-04-09 10:40:55.120747: Steps 10/90: batch_recall = 39.45, batch_ndcg = 29.34 
2025-04-09 10:40:55.849876: Steps 11/90: batch_recall = 42.69, batch_ndcg = 28.63 
2025-04-09 10:40:56.590556: Steps 12/90: batch_recall = 38.32, batch_ndcg = 27.24 
2025-04-09 10:40:57.329788: Steps 13/90: batch_recall = 37.50, batch_ndcg = 26.29 
2025-04-09 10:40:58.074518: Steps 14/90: batch_recall = 36.71, batch_ndcg = 26.00 
2025-04-09 10:40:58.815192: Steps 15/90: batch_recall = 45.36, batch_ndcg = 29.76 
2025-04-09 10:40:59.551695: Steps 16/90: batch_recall = 35.78, batch_ndcg = 25.52 
2025-04-09 10:41:00.281840: Steps 17/90: batch_recall = 34.32, batch_ndcg = 23.08 
2025-04-09 10:41:01.017166: Steps 18/90: batch_recall = 39.39, batch_ndcg = 26.74 
2025-04-09 10:41:01.745165: Steps 19/90: batch_recall = 33.97, batch_ndcg = 23.64 
2025-04-09 10:41:02.458597: Steps 20/90: batch_recall = 39.46, batch_ndcg = 25.72 
2025-04-09 10:41:03.187860: Steps 21/90: batch_recall = 42.72, batch_ndcg = 30.60 
2025-04-09 10:41:03.914242: Steps 22/90: batch_recall = 38.93, batch_ndcg = 27.92 
2025-04-09 10:41:04.640142: Steps 23/90: batch_recall = 39.79, batch_ndcg = 26.19 
2025-04-09 10:41:05.371924: Steps 24/90: batch_recall = 37.23, batch_ndcg = 26.88 
2025-04-09 10:41:06.102010: Steps 25/90: batch_recall = 39.30, batch_ndcg = 25.53 
2025-04-09 10:41:06.820079: Steps 26/90: batch_recall = 37.46, batch_ndcg = 27.66 
2025-04-09 10:41:07.534523: Steps 27/90: batch_recall = 33.95, batch_ndcg = 23.78 
2025-04-09 10:41:08.248242: Steps 28/90: batch_recall = 38.47, batch_ndcg = 26.10 
2025-04-09 10:41:08.969637: Steps 29/90: batch_recall = 34.76, batch_ndcg = 23.85 
2025-04-09 10:41:09.681076: Steps 30/90: batch_recall = 34.54, batch_ndcg = 22.69 
2025-04-09 10:41:10.399752: Steps 31/90: batch_recall = 32.83, batch_ndcg = 23.52 
2025-04-09 10:41:11.120383: Steps 32/90: batch_recall = 35.80, batch_ndcg = 23.80 
2025-04-09 10:41:11.846161: Steps 33/90: batch_recall = 37.75, batch_ndcg = 23.38 
2025-04-09 10:41:12.563887: Steps 34/90: batch_recall = 36.53, batch_ndcg = 24.77 
2025-04-09 10:41:13.282473: Steps 35/90: batch_recall = 39.15, batch_ndcg = 27.77 
2025-04-09 10:41:14.034197: Steps 36/90: batch_recall = 37.91, batch_ndcg = 25.53 
2025-04-09 10:41:14.752084: Steps 37/90: batch_recall = 32.77, batch_ndcg = 21.89 
2025-04-09 10:41:15.478466: Steps 38/90: batch_recall = 35.35, batch_ndcg = 23.38 
2025-04-09 10:41:16.212446: Steps 39/90: batch_recall = 36.38, batch_ndcg = 23.10 
2025-04-09 10:41:16.919131: Steps 40/90: batch_recall = 31.28, batch_ndcg = 22.18 
2025-04-09 10:41:17.630524: Steps 41/90: batch_recall = 35.74, batch_ndcg = 22.47 
2025-04-09 10:41:18.334202: Steps 42/90: batch_recall = 43.23, batch_ndcg = 29.25 
2025-04-09 10:41:19.038202: Steps 43/90: batch_recall = 35.91, batch_ndcg = 22.84 
2025-04-09 10:41:19.758410: Steps 44/90: batch_recall = 34.09, batch_ndcg = 23.02 
2025-04-09 10:41:20.458539: Steps 45/90: batch_recall = 31.54, batch_ndcg = 21.80 
2025-04-09 10:41:21.174986: Steps 46/90: batch_recall = 35.48, batch_ndcg = 22.53 
2025-04-09 10:41:21.865592: Steps 47/90: batch_recall = 32.85, batch_ndcg = 20.39 
2025-04-09 10:41:22.576228: Steps 48/90: batch_recall = 37.72, batch_ndcg = 24.00 
2025-04-09 10:41:23.285843: Steps 49/90: batch_recall = 30.15, batch_ndcg = 19.08 
2025-04-09 10:41:24.007968: Steps 50/90: batch_recall = 39.67, batch_ndcg = 25.89 
2025-04-09 10:41:24.720289: Steps 51/90: batch_recall = 38.48, batch_ndcg = 25.90 
2025-04-09 10:41:25.431800: Steps 52/90: batch_recall = 38.39, batch_ndcg = 25.15 
2025-04-09 10:41:26.153922: Steps 53/90: batch_recall = 41.69, batch_ndcg = 26.85 
2025-04-09 10:41:26.861355: Steps 54/90: batch_recall = 29.67, batch_ndcg = 19.73 
2025-04-09 10:41:27.575277: Steps 55/90: batch_recall = 35.50, batch_ndcg = 23.75 
2025-04-09 10:41:28.299999: Steps 56/90: batch_recall = 32.54, batch_ndcg = 19.32 
2025-04-09 10:41:29.026168: Steps 57/90: batch_recall = 37.28, batch_ndcg = 24.72 
2025-04-09 10:41:29.755765: Steps 58/90: batch_recall = 40.69, batch_ndcg = 24.25 
2025-04-09 10:41:30.482978: Steps 59/90: batch_recall = 35.45, batch_ndcg = 23.24 
2025-04-09 10:41:31.195689: Steps 60/90: batch_recall = 36.40, batch_ndcg = 23.23 
2025-04-09 10:41:31.915139: Steps 61/90: batch_recall = 40.71, batch_ndcg = 27.69 
2025-04-09 10:41:32.633317: Steps 62/90: batch_recall = 39.55, batch_ndcg = 25.86 
2025-04-09 10:41:33.349135: Steps 63/90: batch_recall = 33.72, batch_ndcg = 22.41 
2025-04-09 10:41:34.049258: Steps 64/90: batch_recall = 34.65, batch_ndcg = 22.51 
2025-04-09 10:41:34.767978: Steps 65/90: batch_recall = 34.44, batch_ndcg = 22.26 
2025-04-09 10:41:35.463351: Steps 66/90: batch_recall = 38.02, batch_ndcg = 23.48 
2025-04-09 10:41:36.167480: Steps 67/90: batch_recall = 32.68, batch_ndcg = 21.31 
2025-04-09 10:41:36.854948: Steps 68/90: batch_recall = 37.39, batch_ndcg = 22.53 
2025-04-09 10:41:37.569531: Steps 69/90: batch_recall = 40.03, batch_ndcg = 24.21 
2025-04-09 10:41:38.278705: Steps 70/90: batch_recall = 39.45, batch_ndcg = 25.85 
2025-04-09 10:41:38.991845: Steps 71/90: batch_recall = 41.34, batch_ndcg = 25.22 
2025-04-09 10:41:39.697236: Steps 72/90: batch_recall = 35.69, batch_ndcg = 22.21 
2025-04-09 10:41:40.400265: Steps 73/90: batch_recall = 35.30, batch_ndcg = 22.07 
2025-04-09 10:41:41.120085: Steps 74/90: batch_recall = 42.10, batch_ndcg = 25.93 
2025-04-09 10:41:41.833483: Steps 75/90: batch_recall = 39.44, batch_ndcg = 23.82 
2025-04-09 10:41:42.554710: Steps 76/90: batch_recall = 42.90, batch_ndcg = 26.94 
2025-04-09 10:41:43.265362: Steps 77/90: batch_recall = 44.97, batch_ndcg = 27.62 
2025-04-09 10:41:43.996561: Steps 78/90: batch_recall = 34.62, batch_ndcg = 21.07 
2025-04-09 10:41:44.710764: Steps 79/90: batch_recall = 39.78, batch_ndcg = 26.53 
2025-04-09 10:41:45.416552: Steps 80/90: batch_recall = 45.46, batch_ndcg = 29.43 
2025-04-09 10:41:46.118418: Steps 81/90: batch_recall = 46.67, batch_ndcg = 28.46 
2025-04-09 10:41:46.837082: Steps 82/90: batch_recall = 44.11, batch_ndcg = 27.48 
2025-04-09 10:41:47.533573: Steps 83/90: batch_recall = 43.45, batch_ndcg = 26.75 
2025-04-09 10:41:48.258913: Steps 84/90: batch_recall = 36.45, batch_ndcg = 22.25 
2025-04-09 10:41:48.956171: Steps 85/90: batch_recall = 41.96, batch_ndcg = 24.90 
2025-04-09 10:41:49.668360: Steps 86/90: batch_recall = 42.69, batch_ndcg = 28.56 
2025-04-09 10:41:50.365973: Steps 87/90: batch_recall = 42.51, batch_ndcg = 29.06 
2025-04-09 10:41:51.067668: Steps 88/90: batch_recall = 50.66, batch_ndcg = 29.99 
2025-04-09 10:41:51.608199: Steps 89/90: batch_recall = 41.26, batch_ndcg = 25.87 
2025-04-09 10:41:51.609197: Epoch 34/1000, Test: Recall = 0.0769, NDCG = 0.0510  

2025-04-09 10:41:52.625186: Training Step 0/115: batchLoss = 4.4175, diffLoss = 7.2402, kgLoss = 0.1833
2025-04-09 10:41:53.343152: Training Step 1/115: batchLoss = 3.8831, diffLoss = 6.3597, kgLoss = 0.1681
2025-04-09 10:41:54.062050: Training Step 2/115: batchLoss = 4.2812, diffLoss = 7.0235, kgLoss = 0.1677
2025-04-09 10:41:54.780971: Training Step 3/115: batchLoss = 3.7535, diffLoss = 6.1508, kgLoss = 0.1576
2025-04-09 10:41:55.505577: Training Step 4/115: batchLoss = 4.1672, diffLoss = 6.8303, kgLoss = 0.1726
2025-04-09 10:41:56.232781: Training Step 5/115: batchLoss = 3.8046, diffLoss = 6.2344, kgLoss = 0.1600
2025-04-09 10:41:56.960540: Training Step 6/115: batchLoss = 4.2604, diffLoss = 6.9802, kgLoss = 0.1808
2025-04-09 10:41:57.688213: Training Step 7/115: batchLoss = 4.6145, diffLoss = 7.5639, kgLoss = 0.1904
2025-04-09 10:41:58.422623: Training Step 8/115: batchLoss = 3.9370, diffLoss = 6.4540, kgLoss = 0.1613
2025-04-09 10:41:59.160101: Training Step 9/115: batchLoss = 4.0193, diffLoss = 6.5838, kgLoss = 0.1724
2025-04-09 10:41:59.900754: Training Step 10/115: batchLoss = 4.0331, diffLoss = 6.6114, kgLoss = 0.1657
2025-04-09 10:42:00.635197: Training Step 11/115: batchLoss = 4.2872, diffLoss = 7.0229, kgLoss = 0.1836
2025-04-09 10:42:01.363552: Training Step 12/115: batchLoss = 4.3166, diffLoss = 7.0809, kgLoss = 0.1701
2025-04-09 10:42:02.098726: Training Step 13/115: batchLoss = 3.8494, diffLoss = 6.3056, kgLoss = 0.1651
2025-04-09 10:42:02.826594: Training Step 14/115: batchLoss = 4.6793, diffLoss = 7.6696, kgLoss = 0.1939
2025-04-09 10:42:03.556077: Training Step 15/115: batchLoss = 3.7230, diffLoss = 6.1009, kgLoss = 0.1563
2025-04-09 10:42:04.289686: Training Step 16/115: batchLoss = 4.3778, diffLoss = 7.1854, kgLoss = 0.1664
2025-04-09 10:42:05.017553: Training Step 17/115: batchLoss = 4.1743, diffLoss = 6.8373, kgLoss = 0.1797
2025-04-09 10:42:05.742032: Training Step 18/115: batchLoss = 3.8771, diffLoss = 6.3519, kgLoss = 0.1649
2025-04-09 10:42:06.466010: Training Step 19/115: batchLoss = 3.8791, diffLoss = 6.3658, kgLoss = 0.1490
2025-04-09 10:42:07.196581: Training Step 20/115: batchLoss = 3.5310, diffLoss = 5.7807, kgLoss = 0.1564
2025-04-09 10:42:07.919988: Training Step 21/115: batchLoss = 4.5839, diffLoss = 7.5193, kgLoss = 0.1808
2025-04-09 10:42:08.650103: Training Step 22/115: batchLoss = 3.5926, diffLoss = 5.8873, kgLoss = 0.1506
2025-04-09 10:42:09.376453: Training Step 23/115: batchLoss = 3.8345, diffLoss = 6.2850, kgLoss = 0.1589
2025-04-09 10:42:10.103103: Training Step 24/115: batchLoss = 4.5244, diffLoss = 7.4196, kgLoss = 0.1816
2025-04-09 10:42:10.823802: Training Step 25/115: batchLoss = 4.4853, diffLoss = 7.3538, kgLoss = 0.1825
2025-04-09 10:42:11.566466: Training Step 26/115: batchLoss = 4.1263, diffLoss = 6.7643, kgLoss = 0.1694
2025-04-09 10:42:12.304958: Training Step 27/115: batchLoss = 3.7879, diffLoss = 6.2038, kgLoss = 0.1641
2025-04-09 10:42:13.036044: Training Step 28/115: batchLoss = 4.1655, diffLoss = 6.8290, kgLoss = 0.1702
2025-04-09 10:42:13.771408: Training Step 29/115: batchLoss = 4.0623, diffLoss = 6.6556, kgLoss = 0.1724
2025-04-09 10:42:14.514140: Training Step 30/115: batchLoss = 4.4847, diffLoss = 7.3552, kgLoss = 0.1789
2025-04-09 10:42:15.253420: Training Step 31/115: batchLoss = 4.2068, diffLoss = 6.8977, kgLoss = 0.1704
2025-04-09 10:42:15.994212: Training Step 32/115: batchLoss = 4.5744, diffLoss = 7.4994, kgLoss = 0.1869
2025-04-09 10:42:16.730695: Training Step 33/115: batchLoss = 4.2816, diffLoss = 7.0242, kgLoss = 0.1678
2025-04-09 10:42:17.469862: Training Step 34/115: batchLoss = 4.0239, diffLoss = 6.5953, kgLoss = 0.1667
2025-04-09 10:42:18.211758: Training Step 35/115: batchLoss = 4.8686, diffLoss = 7.9888, kgLoss = 0.1884
2025-04-09 10:42:18.969550: Training Step 36/115: batchLoss = 4.3482, diffLoss = 7.1299, kgLoss = 0.1756
2025-04-09 10:42:19.719292: Training Step 37/115: batchLoss = 4.1790, diffLoss = 6.8546, kgLoss = 0.1656
2025-04-09 10:42:20.470911: Training Step 38/115: batchLoss = 4.1205, diffLoss = 6.7520, kgLoss = 0.1732
2025-04-09 10:42:21.218227: Training Step 39/115: batchLoss = 4.3568, diffLoss = 7.1482, kgLoss = 0.1695
2025-04-09 10:42:21.967336: Training Step 40/115: batchLoss = 4.2389, diffLoss = 6.9506, kgLoss = 0.1714
2025-04-09 10:42:22.699550: Training Step 41/115: batchLoss = 4.3624, diffLoss = 7.1466, kgLoss = 0.1861
2025-04-09 10:42:23.428654: Training Step 42/115: batchLoss = 4.2227, diffLoss = 6.9253, kgLoss = 0.1688
2025-04-09 10:42:24.157944: Training Step 43/115: batchLoss = 4.1101, diffLoss = 6.7292, kgLoss = 0.1814
2025-04-09 10:42:24.882773: Training Step 44/115: batchLoss = 4.0780, diffLoss = 6.6842, kgLoss = 0.1687
2025-04-09 10:42:25.616366: Training Step 45/115: batchLoss = 4.6696, diffLoss = 7.6504, kgLoss = 0.1983
2025-04-09 10:42:26.348317: Training Step 46/115: batchLoss = 4.6197, diffLoss = 7.5633, kgLoss = 0.2043
2025-04-09 10:42:27.088996: Training Step 47/115: batchLoss = 4.5489, diffLoss = 7.4639, kgLoss = 0.1764
2025-04-09 10:42:27.846370: Training Step 48/115: batchLoss = 4.5199, diffLoss = 7.4101, kgLoss = 0.1847
2025-04-09 10:42:28.588745: Training Step 49/115: batchLoss = 4.2209, diffLoss = 6.9197, kgLoss = 0.1726
2025-04-09 10:42:29.335253: Training Step 50/115: batchLoss = 4.0463, diffLoss = 6.6368, kgLoss = 0.1606
2025-04-09 10:42:30.094855: Training Step 51/115: batchLoss = 4.5934, diffLoss = 7.5300, kgLoss = 0.1885
2025-04-09 10:42:30.830451: Training Step 52/115: batchLoss = 4.1829, diffLoss = 6.8570, kgLoss = 0.1717
2025-04-09 10:42:31.575325: Training Step 53/115: batchLoss = 4.3758, diffLoss = 7.1762, kgLoss = 0.1753
2025-04-09 10:42:32.315621: Training Step 54/115: batchLoss = 4.0017, diffLoss = 6.5604, kgLoss = 0.1636
2025-04-09 10:42:33.067975: Training Step 55/115: batchLoss = 3.8159, diffLoss = 6.2586, kgLoss = 0.1517
2025-04-09 10:42:33.814153: Training Step 56/115: batchLoss = 4.4427, diffLoss = 7.2832, kgLoss = 0.1818
2025-04-09 10:42:34.563030: Training Step 57/115: batchLoss = 4.2146, diffLoss = 6.9104, kgLoss = 0.1708
2025-04-09 10:42:35.303839: Training Step 58/115: batchLoss = 3.9443, diffLoss = 6.4589, kgLoss = 0.1722
2025-04-09 10:42:36.040598: Training Step 59/115: batchLoss = 4.1010, diffLoss = 6.7205, kgLoss = 0.1717
2025-04-09 10:42:36.781533: Training Step 60/115: batchLoss = 4.5952, diffLoss = 7.5323, kgLoss = 0.1895
2025-04-09 10:42:37.514787: Training Step 61/115: batchLoss = 3.9555, diffLoss = 6.4868, kgLoss = 0.1587
2025-04-09 10:42:38.254303: Training Step 62/115: batchLoss = 4.2407, diffLoss = 6.9463, kgLoss = 0.1824
2025-04-09 10:42:38.983106: Training Step 63/115: batchLoss = 4.3347, diffLoss = 7.1097, kgLoss = 0.1721
2025-04-09 10:42:39.723939: Training Step 64/115: batchLoss = 4.3611, diffLoss = 7.1449, kgLoss = 0.1853
2025-04-09 10:42:40.456510: Training Step 65/115: batchLoss = 4.3321, diffLoss = 7.0996, kgLoss = 0.1809
2025-04-09 10:42:41.207529: Training Step 66/115: batchLoss = 3.9033, diffLoss = 6.3945, kgLoss = 0.1664
2025-04-09 10:42:41.981046: Training Step 67/115: batchLoss = 3.8787, diffLoss = 6.3500, kgLoss = 0.1717
2025-04-09 10:42:42.741747: Training Step 68/115: batchLoss = 4.4080, diffLoss = 7.2328, kgLoss = 0.1709
2025-04-09 10:42:43.501402: Training Step 69/115: batchLoss = 4.0515, diffLoss = 6.6388, kgLoss = 0.1705
2025-04-09 10:42:44.243833: Training Step 70/115: batchLoss = 3.8149, diffLoss = 6.2464, kgLoss = 0.1677
2025-04-09 10:42:44.996360: Training Step 71/115: batchLoss = 3.8840, diffLoss = 6.3573, kgLoss = 0.1740
2025-04-09 10:42:45.746769: Training Step 72/115: batchLoss = 4.5377, diffLoss = 7.4416, kgLoss = 0.1818
2025-04-09 10:42:46.491591: Training Step 73/115: batchLoss = 4.2694, diffLoss = 6.9998, kgLoss = 0.1740
2025-04-09 10:42:47.239399: Training Step 74/115: batchLoss = 4.0556, diffLoss = 6.6458, kgLoss = 0.1704
2025-04-09 10:42:47.983725: Training Step 75/115: batchLoss = 4.2103, diffLoss = 6.9056, kgLoss = 0.1675
2025-04-09 10:42:48.736509: Training Step 76/115: batchLoss = 4.0580, diffLoss = 6.6539, kgLoss = 0.1642
2025-04-09 10:42:49.491569: Training Step 77/115: batchLoss = 4.3225, diffLoss = 7.0864, kgLoss = 0.1768
2025-04-09 10:42:50.234110: Training Step 78/115: batchLoss = 4.1789, diffLoss = 6.8535, kgLoss = 0.1669
2025-04-09 10:42:50.989912: Training Step 79/115: batchLoss = 4.6095, diffLoss = 7.5589, kgLoss = 0.1853
2025-04-09 10:42:51.753361: Training Step 80/115: batchLoss = 4.2141, diffLoss = 6.9071, kgLoss = 0.1747
2025-04-09 10:42:52.487333: Training Step 81/115: batchLoss = 4.5015, diffLoss = 7.3667, kgLoss = 0.2036
2025-04-09 10:42:53.237344: Training Step 82/115: batchLoss = 4.2859, diffLoss = 7.0217, kgLoss = 0.1822
2025-04-09 10:42:53.988081: Training Step 83/115: batchLoss = 4.1041, diffLoss = 6.7352, kgLoss = 0.1576
2025-04-09 10:42:54.716632: Training Step 84/115: batchLoss = 4.1315, diffLoss = 6.7805, kgLoss = 0.1579
2025-04-09 10:42:55.442637: Training Step 85/115: batchLoss = 4.3391, diffLoss = 7.1054, kgLoss = 0.1898
2025-04-09 10:42:56.180698: Training Step 86/115: batchLoss = 3.8896, diffLoss = 6.3730, kgLoss = 0.1643
2025-04-09 10:42:56.920360: Training Step 87/115: batchLoss = 4.7571, diffLoss = 7.8028, kgLoss = 0.1886
2025-04-09 10:42:57.659756: Training Step 88/115: batchLoss = 4.5012, diffLoss = 7.3778, kgLoss = 0.1862
2025-04-09 10:42:58.401983: Training Step 89/115: batchLoss = 4.3250, diffLoss = 7.0868, kgLoss = 0.1825
2025-04-09 10:42:59.161977: Training Step 90/115: batchLoss = 4.0997, diffLoss = 6.7140, kgLoss = 0.1783
2025-04-09 10:42:59.908232: Training Step 91/115: batchLoss = 4.1744, diffLoss = 6.8468, kgLoss = 0.1657
2025-04-09 10:43:00.657202: Training Step 92/115: batchLoss = 4.7088, diffLoss = 7.7217, kgLoss = 0.1894
2025-04-09 10:43:01.406429: Training Step 93/115: batchLoss = 4.2737, diffLoss = 7.0033, kgLoss = 0.1794
2025-04-09 10:43:02.150356: Training Step 94/115: batchLoss = 3.9983, diffLoss = 6.5526, kgLoss = 0.1668
2025-04-09 10:43:02.908726: Training Step 95/115: batchLoss = 3.9269, diffLoss = 6.4374, kgLoss = 0.1612
2025-04-09 10:43:03.670901: Training Step 96/115: batchLoss = 4.6226, diffLoss = 7.5849, kgLoss = 0.1792
2025-04-09 10:43:04.423249: Training Step 97/115: batchLoss = 4.0483, diffLoss = 6.6328, kgLoss = 0.1716
2025-04-09 10:43:05.184279: Training Step 98/115: batchLoss = 4.6744, diffLoss = 7.6607, kgLoss = 0.1949
2025-04-09 10:43:05.935011: Training Step 99/115: batchLoss = 4.4436, diffLoss = 7.2872, kgLoss = 0.1781
2025-04-09 10:43:06.666328: Training Step 100/115: batchLoss = 4.2725, diffLoss = 7.0057, kgLoss = 0.1726
2025-04-09 10:43:07.407356: Training Step 101/115: batchLoss = 4.3040, diffLoss = 7.0592, kgLoss = 0.1712
2025-04-09 10:43:08.153541: Training Step 102/115: batchLoss = 3.9251, diffLoss = 6.4308, kgLoss = 0.1666
2025-04-09 10:43:08.886360: Training Step 103/115: batchLoss = 4.3422, diffLoss = 7.1200, kgLoss = 0.1755
2025-04-09 10:43:09.627701: Training Step 104/115: batchLoss = 3.7367, diffLoss = 6.1227, kgLoss = 0.1578
2025-04-09 10:43:10.387005: Training Step 105/115: batchLoss = 4.1073, diffLoss = 6.7242, kgLoss = 0.1819
2025-04-09 10:43:11.134043: Training Step 106/115: batchLoss = 3.8675, diffLoss = 6.3412, kgLoss = 0.1569
2025-04-09 10:43:11.872863: Training Step 107/115: batchLoss = 4.6237, diffLoss = 7.5812, kgLoss = 0.1875
2025-04-09 10:43:12.629721: Training Step 108/115: batchLoss = 3.7218, diffLoss = 6.0885, kgLoss = 0.1718
2025-04-09 10:43:13.371645: Training Step 109/115: batchLoss = 4.3674, diffLoss = 7.1560, kgLoss = 0.1845
2025-04-09 10:43:14.114849: Training Step 110/115: batchLoss = 4.4404, diffLoss = 7.2811, kgLoss = 0.1793
2025-04-09 10:43:14.855289: Training Step 111/115: batchLoss = 4.2477, diffLoss = 6.9608, kgLoss = 0.1781
2025-04-09 10:43:15.590379: Training Step 112/115: batchLoss = 4.1114, diffLoss = 6.7403, kgLoss = 0.1679
2025-04-09 10:43:16.257082: Training Step 113/115: batchLoss = 3.7713, diffLoss = 6.1681, kgLoss = 0.1760
2025-04-09 10:43:16.870816: Training Step 114/115: batchLoss = 4.1686, diffLoss = 6.8353, kgLoss = 0.1684
2025-04-09 10:43:17.000913: 
2025-04-09 10:43:17.002038: Epoch 35/1000, Train: epLoss = 1.2095, epDfLoss = 1.9826, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:43:17.776809: Steps 0/90: batch_recall = 56.77, batch_ndcg = 43.67 
2025-04-09 10:43:18.526028: Steps 1/90: batch_recall = 57.74, batch_ndcg = 42.22 
2025-04-09 10:43:19.274866: Steps 2/90: batch_recall = 49.61, batch_ndcg = 37.70 
2025-04-09 10:43:20.016267: Steps 3/90: batch_recall = 45.93, batch_ndcg = 32.36 
2025-04-09 10:43:20.763992: Steps 4/90: batch_recall = 46.34, batch_ndcg = 35.36 
2025-04-09 10:43:21.511338: Steps 5/90: batch_recall = 38.28, batch_ndcg = 28.15 
2025-04-09 10:43:22.250054: Steps 6/90: batch_recall = 47.00, batch_ndcg = 32.75 
2025-04-09 10:43:22.994280: Steps 7/90: batch_recall = 43.17, batch_ndcg = 31.65 
2025-04-09 10:43:23.733978: Steps 8/90: batch_recall = 47.30, batch_ndcg = 34.27 
2025-04-09 10:43:24.475963: Steps 9/90: batch_recall = 44.48, batch_ndcg = 32.94 
2025-04-09 10:43:25.219898: Steps 10/90: batch_recall = 39.35, batch_ndcg = 29.21 
2025-04-09 10:43:25.953720: Steps 11/90: batch_recall = 43.45, batch_ndcg = 28.97 
2025-04-09 10:43:26.681379: Steps 12/90: batch_recall = 37.90, batch_ndcg = 27.06 
2025-04-09 10:43:27.424118: Steps 13/90: batch_recall = 37.31, batch_ndcg = 26.40 
2025-04-09 10:43:28.179654: Steps 14/90: batch_recall = 35.94, batch_ndcg = 25.46 
2025-04-09 10:43:28.921901: Steps 15/90: batch_recall = 45.02, batch_ndcg = 29.68 
2025-04-09 10:43:29.668996: Steps 16/90: batch_recall = 35.19, batch_ndcg = 25.31 
2025-04-09 10:43:30.409335: Steps 17/90: batch_recall = 33.72, batch_ndcg = 22.61 
2025-04-09 10:43:31.142080: Steps 18/90: batch_recall = 39.66, batch_ndcg = 27.00 
2025-04-09 10:43:31.890251: Steps 19/90: batch_recall = 33.90, batch_ndcg = 23.68 
2025-04-09 10:43:32.623065: Steps 20/90: batch_recall = 40.98, batch_ndcg = 26.42 
2025-04-09 10:43:33.357631: Steps 21/90: batch_recall = 42.91, batch_ndcg = 30.88 
2025-04-09 10:43:34.099745: Steps 22/90: batch_recall = 38.85, batch_ndcg = 27.87 
2025-04-09 10:43:34.846645: Steps 23/90: batch_recall = 40.19, batch_ndcg = 26.46 
2025-04-09 10:43:35.565215: Steps 24/90: batch_recall = 36.38, batch_ndcg = 26.47 
2025-04-09 10:43:36.305668: Steps 25/90: batch_recall = 38.94, batch_ndcg = 25.21 
2025-04-09 10:43:37.040951: Steps 26/90: batch_recall = 37.45, batch_ndcg = 27.63 
2025-04-09 10:43:37.780587: Steps 27/90: batch_recall = 34.38, batch_ndcg = 23.89 
2025-04-09 10:43:38.506664: Steps 28/90: batch_recall = 37.71, batch_ndcg = 26.11 
2025-04-09 10:43:39.224950: Steps 29/90: batch_recall = 34.73, batch_ndcg = 23.79 
2025-04-09 10:43:39.942717: Steps 30/90: batch_recall = 34.06, batch_ndcg = 22.61 
2025-04-09 10:43:40.654058: Steps 31/90: batch_recall = 32.99, batch_ndcg = 23.45 
2025-04-09 10:43:41.370879: Steps 32/90: batch_recall = 36.31, batch_ndcg = 24.09 
2025-04-09 10:43:42.081438: Steps 33/90: batch_recall = 36.91, batch_ndcg = 22.71 
2025-04-09 10:43:42.803140: Steps 34/90: batch_recall = 36.40, batch_ndcg = 24.92 
2025-04-09 10:43:43.530391: Steps 35/90: batch_recall = 38.89, batch_ndcg = 27.57 
2025-04-09 10:43:44.257453: Steps 36/90: batch_recall = 37.77, batch_ndcg = 25.33 
2025-04-09 10:43:44.986725: Steps 37/90: batch_recall = 33.50, batch_ndcg = 21.75 
2025-04-09 10:43:45.712327: Steps 38/90: batch_recall = 36.84, batch_ndcg = 23.70 
2025-04-09 10:43:46.433880: Steps 39/90: batch_recall = 37.55, batch_ndcg = 23.88 
2025-04-09 10:43:47.148608: Steps 40/90: batch_recall = 31.16, batch_ndcg = 22.10 
2025-04-09 10:43:47.870815: Steps 41/90: batch_recall = 36.07, batch_ndcg = 22.65 
2025-04-09 10:43:48.603604: Steps 42/90: batch_recall = 43.64, batch_ndcg = 29.31 
2025-04-09 10:43:49.348726: Steps 43/90: batch_recall = 35.63, batch_ndcg = 22.65 
2025-04-09 10:43:50.086601: Steps 44/90: batch_recall = 32.81, batch_ndcg = 22.72 
2025-04-09 10:43:50.799754: Steps 45/90: batch_recall = 31.79, batch_ndcg = 21.94 
2025-04-09 10:43:51.517173: Steps 46/90: batch_recall = 35.32, batch_ndcg = 22.62 
2025-04-09 10:43:52.218810: Steps 47/90: batch_recall = 32.79, batch_ndcg = 20.35 
2025-04-09 10:43:52.923888: Steps 48/90: batch_recall = 38.13, batch_ndcg = 23.79 
2025-04-09 10:43:53.645887: Steps 49/90: batch_recall = 32.14, batch_ndcg = 19.73 
2025-04-09 10:43:54.356482: Steps 50/90: batch_recall = 40.51, batch_ndcg = 26.13 
2025-04-09 10:43:55.089587: Steps 51/90: batch_recall = 38.28, batch_ndcg = 25.83 
2025-04-09 10:43:55.805212: Steps 52/90: batch_recall = 39.14, batch_ndcg = 25.37 
2025-04-09 10:43:56.519583: Steps 53/90: batch_recall = 41.46, batch_ndcg = 26.75 
2025-04-09 10:43:57.221395: Steps 54/90: batch_recall = 30.67, batch_ndcg = 20.15 
2025-04-09 10:43:57.935116: Steps 55/90: batch_recall = 36.38, batch_ndcg = 23.82 
2025-04-09 10:43:58.651477: Steps 56/90: batch_recall = 32.33, batch_ndcg = 19.38 
2025-04-09 10:43:59.366770: Steps 57/90: batch_recall = 36.58, batch_ndcg = 24.44 
2025-04-09 10:44:00.085815: Steps 58/90: batch_recall = 40.13, batch_ndcg = 24.18 
2025-04-09 10:44:00.818194: Steps 59/90: batch_recall = 36.14, batch_ndcg = 23.42 
2025-04-09 10:44:01.534802: Steps 60/90: batch_recall = 34.77, batch_ndcg = 22.77 
2025-04-09 10:44:02.254910: Steps 61/90: batch_recall = 40.78, batch_ndcg = 27.48 
2025-04-09 10:44:02.965018: Steps 62/90: batch_recall = 39.34, batch_ndcg = 25.62 
2025-04-09 10:44:03.691029: Steps 63/90: batch_recall = 34.20, batch_ndcg = 22.58 
2025-04-09 10:44:04.416994: Steps 64/90: batch_recall = 34.97, batch_ndcg = 22.53 
2025-04-09 10:44:05.148248: Steps 65/90: batch_recall = 33.53, batch_ndcg = 22.07 
2025-04-09 10:44:05.862648: Steps 66/90: batch_recall = 37.50, batch_ndcg = 23.15 
2025-04-09 10:44:06.575982: Steps 67/90: batch_recall = 32.55, batch_ndcg = 21.01 
2025-04-09 10:44:07.269466: Steps 68/90: batch_recall = 37.16, batch_ndcg = 22.56 
2025-04-09 10:44:07.980355: Steps 69/90: batch_recall = 39.49, batch_ndcg = 24.13 
2025-04-09 10:44:08.687664: Steps 70/90: batch_recall = 39.31, batch_ndcg = 25.71 
2025-04-09 10:44:09.400235: Steps 71/90: batch_recall = 41.24, batch_ndcg = 25.22 
2025-04-09 10:44:10.133131: Steps 72/90: batch_recall = 36.23, batch_ndcg = 22.38 
2025-04-09 10:44:10.836588: Steps 73/90: batch_recall = 35.01, batch_ndcg = 22.09 
2025-04-09 10:44:11.530224: Steps 74/90: batch_recall = 40.46, batch_ndcg = 25.45 
2025-04-09 10:44:12.241666: Steps 75/90: batch_recall = 40.11, batch_ndcg = 24.44 
2025-04-09 10:44:12.966272: Steps 76/90: batch_recall = 40.98, batch_ndcg = 26.32 
2025-04-09 10:44:13.676969: Steps 77/90: batch_recall = 45.09, batch_ndcg = 27.80 
2025-04-09 10:44:14.398765: Steps 78/90: batch_recall = 33.93, batch_ndcg = 20.90 
2025-04-09 10:44:15.106315: Steps 79/90: batch_recall = 39.61, batch_ndcg = 26.31 
2025-04-09 10:44:15.816050: Steps 80/90: batch_recall = 44.76, batch_ndcg = 29.30 
2025-04-09 10:44:16.516970: Steps 81/90: batch_recall = 45.55, batch_ndcg = 28.35 
2025-04-09 10:44:17.224931: Steps 82/90: batch_recall = 44.83, batch_ndcg = 27.86 
2025-04-09 10:44:17.950697: Steps 83/90: batch_recall = 44.33, batch_ndcg = 26.95 
2025-04-09 10:44:18.674128: Steps 84/90: batch_recall = 36.57, batch_ndcg = 22.48 
2025-04-09 10:44:19.389569: Steps 85/90: batch_recall = 41.11, batch_ndcg = 24.53 
2025-04-09 10:44:20.100719: Steps 86/90: batch_recall = 41.80, batch_ndcg = 28.23 
2025-04-09 10:44:20.812324: Steps 87/90: batch_recall = 44.17, batch_ndcg = 29.44 
2025-04-09 10:44:21.516788: Steps 88/90: batch_recall = 50.73, batch_ndcg = 30.17 
2025-04-09 10:44:22.088267: Steps 89/90: batch_recall = 41.07, batch_ndcg = 26.17 
2025-04-09 10:44:22.089004: Epoch 35/1000, Test: Recall = 0.0767, NDCG = 0.0510  

2025-04-09 10:44:23.111471: Training Step 0/115: batchLoss = 4.1962, diffLoss = 6.8750, kgLoss = 0.1780
2025-04-09 10:44:23.838989: Training Step 1/115: batchLoss = 4.0576, diffLoss = 6.6490, kgLoss = 0.1705
2025-04-09 10:44:24.561579: Training Step 2/115: batchLoss = 4.1104, diffLoss = 6.7356, kgLoss = 0.1725
2025-04-09 10:44:25.289092: Training Step 3/115: batchLoss = 4.0779, diffLoss = 6.6794, kgLoss = 0.1756
2025-04-09 10:44:26.029227: Training Step 4/115: batchLoss = 3.9654, diffLoss = 6.4987, kgLoss = 0.1654
2025-04-09 10:44:26.776474: Training Step 5/115: batchLoss = 3.9318, diffLoss = 6.4445, kgLoss = 0.1628
2025-04-09 10:44:27.526339: Training Step 6/115: batchLoss = 3.8618, diffLoss = 6.3283, kgLoss = 0.1620
2025-04-09 10:44:28.260896: Training Step 7/115: batchLoss = 4.2100, diffLoss = 6.9074, kgLoss = 0.1639
2025-04-09 10:44:29.001250: Training Step 8/115: batchLoss = 3.8826, diffLoss = 6.3607, kgLoss = 0.1654
2025-04-09 10:44:29.743770: Training Step 9/115: batchLoss = 3.9739, diffLoss = 6.5096, kgLoss = 0.1705
2025-04-09 10:44:30.491141: Training Step 10/115: batchLoss = 4.2664, diffLoss = 6.9962, kgLoss = 0.1717
2025-04-09 10:44:31.234972: Training Step 11/115: batchLoss = 4.3779, diffLoss = 7.1785, kgLoss = 0.1771
2025-04-09 10:44:31.973531: Training Step 12/115: batchLoss = 4.2583, diffLoss = 6.9823, kgLoss = 0.1723
2025-04-09 10:44:32.716153: Training Step 13/115: batchLoss = 3.6174, diffLoss = 5.9292, kgLoss = 0.1496
2025-04-09 10:44:33.454796: Training Step 14/115: batchLoss = 4.1588, diffLoss = 6.8232, kgLoss = 0.1621
2025-04-09 10:44:34.209734: Training Step 15/115: batchLoss = 4.4438, diffLoss = 7.2889, kgLoss = 0.1762
2025-04-09 10:44:34.978633: Training Step 16/115: batchLoss = 4.1827, diffLoss = 6.8606, kgLoss = 0.1659
2025-04-09 10:44:35.722359: Training Step 17/115: batchLoss = 4.1319, diffLoss = 6.7787, kgLoss = 0.1617
2025-04-09 10:44:36.468343: Training Step 18/115: batchLoss = 4.1945, diffLoss = 6.8756, kgLoss = 0.1728
2025-04-09 10:44:37.206159: Training Step 19/115: batchLoss = 4.0871, diffLoss = 6.6980, kgLoss = 0.1707
2025-04-09 10:44:37.947815: Training Step 20/115: batchLoss = 4.0484, diffLoss = 6.6311, kgLoss = 0.1743
2025-04-09 10:44:38.679585: Training Step 21/115: batchLoss = 3.9040, diffLoss = 6.3935, kgLoss = 0.1698
2025-04-09 10:44:39.409862: Training Step 22/115: batchLoss = 4.0330, diffLoss = 6.6081, kgLoss = 0.1704
2025-04-09 10:44:40.135599: Training Step 23/115: batchLoss = 4.2026, diffLoss = 6.8836, kgLoss = 0.1811
2025-04-09 10:44:40.872994: Training Step 24/115: batchLoss = 4.2727, diffLoss = 7.0071, kgLoss = 0.1712
2025-04-09 10:44:41.602588: Training Step 25/115: batchLoss = 4.1784, diffLoss = 6.8522, kgLoss = 0.1675
2025-04-09 10:44:42.331790: Training Step 26/115: batchLoss = 5.0127, diffLoss = 8.2175, kgLoss = 0.2054
2025-04-09 10:44:43.085027: Training Step 27/115: batchLoss = 4.1038, diffLoss = 6.7299, kgLoss = 0.1647
2025-04-09 10:44:43.829493: Training Step 28/115: batchLoss = 3.8245, diffLoss = 6.2709, kgLoss = 0.1550
2025-04-09 10:44:44.576801: Training Step 29/115: batchLoss = 3.8333, diffLoss = 6.2759, kgLoss = 0.1695
2025-04-09 10:44:45.333190: Training Step 30/115: batchLoss = 4.5184, diffLoss = 7.4052, kgLoss = 0.1883
2025-04-09 10:44:46.085557: Training Step 31/115: batchLoss = 4.2860, diffLoss = 7.0313, kgLoss = 0.1681
2025-04-09 10:44:46.826916: Training Step 32/115: batchLoss = 4.1080, diffLoss = 6.7261, kgLoss = 0.1807
2025-04-09 10:44:47.574409: Training Step 33/115: batchLoss = 4.0082, diffLoss = 6.5746, kgLoss = 0.1587
2025-04-09 10:44:48.310062: Training Step 34/115: batchLoss = 3.9646, diffLoss = 6.4899, kgLoss = 0.1766
2025-04-09 10:44:49.047399: Training Step 35/115: batchLoss = 3.9827, diffLoss = 6.5297, kgLoss = 0.1622
2025-04-09 10:44:49.783749: Training Step 36/115: batchLoss = 4.4095, diffLoss = 7.2262, kgLoss = 0.1844
2025-04-09 10:44:50.529510: Training Step 37/115: batchLoss = 4.7375, diffLoss = 7.7677, kgLoss = 0.1921
2025-04-09 10:44:51.273639: Training Step 38/115: batchLoss = 3.7878, diffLoss = 6.1918, kgLoss = 0.1818
2025-04-09 10:44:52.010214: Training Step 39/115: batchLoss = 4.2468, diffLoss = 6.9634, kgLoss = 0.1719
2025-04-09 10:44:52.745913: Training Step 40/115: batchLoss = 4.3711, diffLoss = 7.1625, kgLoss = 0.1840
2025-04-09 10:44:53.475821: Training Step 41/115: batchLoss = 4.7511, diffLoss = 7.7949, kgLoss = 0.1854
2025-04-09 10:44:54.203800: Training Step 42/115: batchLoss = 4.1256, diffLoss = 6.7591, kgLoss = 0.1755
2025-04-09 10:44:54.940470: Training Step 43/115: batchLoss = 4.1916, diffLoss = 6.8709, kgLoss = 0.1726
2025-04-09 10:44:55.670258: Training Step 44/115: batchLoss = 4.6164, diffLoss = 7.5500, kgLoss = 0.2160
2025-04-09 10:44:56.412978: Training Step 45/115: batchLoss = 4.6109, diffLoss = 7.5540, kgLoss = 0.1962
2025-04-09 10:44:57.152167: Training Step 46/115: batchLoss = 4.0151, diffLoss = 6.5806, kgLoss = 0.1669
2025-04-09 10:44:57.893538: Training Step 47/115: batchLoss = 4.2129, diffLoss = 6.9114, kgLoss = 0.1653
2025-04-09 10:44:58.638743: Training Step 48/115: batchLoss = 3.7377, diffLoss = 6.1224, kgLoss = 0.1607
2025-04-09 10:44:59.398369: Training Step 49/115: batchLoss = 4.5041, diffLoss = 7.3862, kgLoss = 0.1808
2025-04-09 10:45:00.142517: Training Step 50/115: batchLoss = 4.5971, diffLoss = 7.5378, kgLoss = 0.1860
2025-04-09 10:45:00.893077: Training Step 51/115: batchLoss = 3.8984, diffLoss = 6.3870, kgLoss = 0.1655
2025-04-09 10:45:01.637153: Training Step 52/115: batchLoss = 4.0031, diffLoss = 6.5629, kgLoss = 0.1635
2025-04-09 10:45:02.399030: Training Step 53/115: batchLoss = 4.1699, diffLoss = 6.8359, kgLoss = 0.1708
2025-04-09 10:45:03.142828: Training Step 54/115: batchLoss = 4.0644, diffLoss = 6.6627, kgLoss = 0.1669
2025-04-09 10:45:03.900358: Training Step 55/115: batchLoss = 4.0995, diffLoss = 6.7155, kgLoss = 0.1754
2025-04-09 10:45:04.669977: Training Step 56/115: batchLoss = 3.8774, diffLoss = 6.3580, kgLoss = 0.1566
2025-04-09 10:45:05.432989: Training Step 57/115: batchLoss = 4.1259, diffLoss = 6.7673, kgLoss = 0.1637
2025-04-09 10:45:06.181484: Training Step 58/115: batchLoss = 4.0796, diffLoss = 6.6873, kgLoss = 0.1681
2025-04-09 10:45:06.924966: Training Step 59/115: batchLoss = 4.2650, diffLoss = 6.9960, kgLoss = 0.1687
2025-04-09 10:45:07.666693: Training Step 60/115: batchLoss = 4.3860, diffLoss = 7.1870, kgLoss = 0.1845
2025-04-09 10:45:08.422284: Training Step 61/115: batchLoss = 3.8227, diffLoss = 6.2600, kgLoss = 0.1668
2025-04-09 10:45:09.160816: Training Step 62/115: batchLoss = 4.4819, diffLoss = 7.3487, kgLoss = 0.1817
2025-04-09 10:45:09.910249: Training Step 63/115: batchLoss = 4.4413, diffLoss = 7.2891, kgLoss = 0.1696
2025-04-09 10:45:10.734954: Training Step 64/115: batchLoss = 3.9408, diffLoss = 6.4581, kgLoss = 0.1648
2025-04-09 10:45:11.474269: Training Step 65/115: batchLoss = 4.5417, diffLoss = 7.4385, kgLoss = 0.1964
2025-04-09 10:45:12.211585: Training Step 66/115: batchLoss = 4.2882, diffLoss = 7.0280, kgLoss = 0.1784
2025-04-09 10:45:12.945939: Training Step 67/115: batchLoss = 4.2359, diffLoss = 6.9414, kgLoss = 0.1777
2025-04-09 10:45:13.695131: Training Step 68/115: batchLoss = 4.0678, diffLoss = 6.6662, kgLoss = 0.1701
2025-04-09 10:45:14.439423: Training Step 69/115: batchLoss = 4.2819, diffLoss = 7.0155, kgLoss = 0.1815
2025-04-09 10:45:15.178032: Training Step 70/115: batchLoss = 4.2469, diffLoss = 6.9583, kgLoss = 0.1799
2025-04-09 10:45:15.924289: Training Step 71/115: batchLoss = 4.4165, diffLoss = 7.2397, kgLoss = 0.1816
2025-04-09 10:45:16.680663: Training Step 72/115: batchLoss = 4.0380, diffLoss = 6.6179, kgLoss = 0.1681
2025-04-09 10:45:17.436507: Training Step 73/115: batchLoss = 4.4880, diffLoss = 7.3644, kgLoss = 0.1735
2025-04-09 10:45:18.195650: Training Step 74/115: batchLoss = 4.4114, diffLoss = 7.2313, kgLoss = 0.1817
2025-04-09 10:45:18.948336: Training Step 75/115: batchLoss = 4.3204, diffLoss = 7.0899, kgLoss = 0.1661
2025-04-09 10:45:19.705631: Training Step 76/115: batchLoss = 4.2456, diffLoss = 6.9586, kgLoss = 0.1762
2025-04-09 10:45:20.447048: Training Step 77/115: batchLoss = 4.3132, diffLoss = 7.0585, kgLoss = 0.1952
2025-04-09 10:45:21.187171: Training Step 78/115: batchLoss = 4.1519, diffLoss = 6.8082, kgLoss = 0.1674
2025-04-09 10:45:21.931396: Training Step 79/115: batchLoss = 4.0209, diffLoss = 6.5960, kgLoss = 0.1581
2025-04-09 10:45:22.686719: Training Step 80/115: batchLoss = 4.3835, diffLoss = 7.1853, kgLoss = 0.1807
2025-04-09 10:45:23.429543: Training Step 81/115: batchLoss = 4.3296, diffLoss = 7.0922, kgLoss = 0.1856
2025-04-09 10:45:24.165999: Training Step 82/115: batchLoss = 4.0508, diffLoss = 6.6283, kgLoss = 0.1846
2025-04-09 10:45:24.931495: Training Step 83/115: batchLoss = 4.0207, diffLoss = 6.5906, kgLoss = 0.1657
2025-04-09 10:45:25.671149: Training Step 84/115: batchLoss = 4.0820, diffLoss = 6.6857, kgLoss = 0.1765
2025-04-09 10:45:26.407778: Training Step 85/115: batchLoss = 4.1118, diffLoss = 6.7463, kgLoss = 0.1599
2025-04-09 10:45:27.156477: Training Step 86/115: batchLoss = 4.3544, diffLoss = 7.1407, kgLoss = 0.1749
2025-04-09 10:45:27.882339: Training Step 87/115: batchLoss = 4.2575, diffLoss = 6.9800, kgLoss = 0.1738
2025-04-09 10:45:28.625766: Training Step 88/115: batchLoss = 4.6183, diffLoss = 7.5754, kgLoss = 0.1827
2025-04-09 10:45:29.376410: Training Step 89/115: batchLoss = 4.0828, diffLoss = 6.6870, kgLoss = 0.1767
2025-04-09 10:45:30.106415: Training Step 90/115: batchLoss = 4.3802, diffLoss = 7.1808, kgLoss = 0.1794
2025-04-09 10:45:30.849236: Training Step 91/115: batchLoss = 4.1505, diffLoss = 6.8112, kgLoss = 0.1595
2025-04-09 10:45:31.597184: Training Step 92/115: batchLoss = 5.1981, diffLoss = 8.5224, kgLoss = 0.2116
2025-04-09 10:45:32.343812: Training Step 93/115: batchLoss = 4.2215, diffLoss = 6.9273, kgLoss = 0.1629
2025-04-09 10:45:33.089039: Training Step 94/115: batchLoss = 4.4013, diffLoss = 7.2121, kgLoss = 0.1850
2025-04-09 10:45:33.832273: Training Step 95/115: batchLoss = 4.4549, diffLoss = 7.3007, kgLoss = 0.1861
2025-04-09 10:45:34.568799: Training Step 96/115: batchLoss = 3.7797, diffLoss = 6.1930, kgLoss = 0.1596
2025-04-09 10:45:35.321711: Training Step 97/115: batchLoss = 3.9798, diffLoss = 6.5246, kgLoss = 0.1626
2025-04-09 10:45:36.089401: Training Step 98/115: batchLoss = 4.0417, diffLoss = 6.6228, kgLoss = 0.1700
2025-04-09 10:45:36.840312: Training Step 99/115: batchLoss = 4.5760, diffLoss = 7.5009, kgLoss = 0.1887
2025-04-09 10:45:37.583863: Training Step 100/115: batchLoss = 4.6807, diffLoss = 7.6734, kgLoss = 0.1917
2025-04-09 10:45:38.318368: Training Step 101/115: batchLoss = 4.2080, diffLoss = 6.8929, kgLoss = 0.1805
2025-04-09 10:45:39.047961: Training Step 102/115: batchLoss = 3.8406, diffLoss = 6.2954, kgLoss = 0.1583
2025-04-09 10:45:39.787808: Training Step 103/115: batchLoss = 3.9874, diffLoss = 6.5305, kgLoss = 0.1728
2025-04-09 10:45:40.614420: Training Step 104/115: batchLoss = 3.7620, diffLoss = 6.1674, kgLoss = 0.1539
2025-04-09 10:45:41.365092: Training Step 105/115: batchLoss = 3.6953, diffLoss = 6.0506, kgLoss = 0.1624
2025-04-09 10:45:42.105474: Training Step 106/115: batchLoss = 4.3861, diffLoss = 7.1889, kgLoss = 0.1818
2025-04-09 10:45:42.842650: Training Step 107/115: batchLoss = 3.9005, diffLoss = 6.3950, kgLoss = 0.1586
2025-04-09 10:45:43.587072: Training Step 108/115: batchLoss = 4.0451, diffLoss = 6.6316, kgLoss = 0.1654
2025-04-09 10:45:44.336109: Training Step 109/115: batchLoss = 4.1514, diffLoss = 6.8062, kgLoss = 0.1692
2025-04-09 10:45:45.083329: Training Step 110/115: batchLoss = 4.2401, diffLoss = 6.9492, kgLoss = 0.1764
2025-04-09 10:45:45.828548: Training Step 111/115: batchLoss = 3.6514, diffLoss = 5.9731, kgLoss = 0.1689
2025-04-09 10:45:46.564124: Training Step 112/115: batchLoss = 4.3990, diffLoss = 7.2090, kgLoss = 0.1840
2025-04-09 10:45:47.221695: Training Step 113/115: batchLoss = 4.6306, diffLoss = 7.5953, kgLoss = 0.1835
2025-04-09 10:45:47.873159: Training Step 114/115: batchLoss = 4.2385, diffLoss = 6.9440, kgLoss = 0.1802
2025-04-09 10:45:48.002028: 
2025-04-09 10:45:48.003136: Epoch 36/1000, Train: epLoss = 1.2055, epDfLoss = 1.9759, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:45:48.777176: Steps 0/90: batch_recall = 55.71, batch_ndcg = 43.04 
2025-04-09 10:45:49.530048: Steps 1/90: batch_recall = 57.86, batch_ndcg = 41.76 
2025-04-09 10:45:50.290968: Steps 2/90: batch_recall = 50.12, batch_ndcg = 37.98 
2025-04-09 10:45:51.044578: Steps 3/90: batch_recall = 44.92, batch_ndcg = 31.58 
2025-04-09 10:45:51.800579: Steps 4/90: batch_recall = 46.35, batch_ndcg = 35.26 
2025-04-09 10:45:52.560558: Steps 5/90: batch_recall = 37.85, batch_ndcg = 27.92 
2025-04-09 10:45:53.301733: Steps 6/90: batch_recall = 46.52, batch_ndcg = 32.38 
2025-04-09 10:45:54.054790: Steps 7/90: batch_recall = 44.00, batch_ndcg = 32.39 
2025-04-09 10:45:54.801611: Steps 8/90: batch_recall = 47.56, batch_ndcg = 33.89 
2025-04-09 10:45:55.555619: Steps 9/90: batch_recall = 44.65, batch_ndcg = 33.11 
2025-04-09 10:45:56.306747: Steps 10/90: batch_recall = 39.61, batch_ndcg = 29.32 
2025-04-09 10:45:57.042491: Steps 11/90: batch_recall = 42.57, batch_ndcg = 28.44 
2025-04-09 10:45:57.779977: Steps 12/90: batch_recall = 37.64, batch_ndcg = 27.14 
2025-04-09 10:45:58.534168: Steps 13/90: batch_recall = 38.13, batch_ndcg = 26.76 
2025-04-09 10:45:59.285469: Steps 14/90: batch_recall = 35.53, batch_ndcg = 25.34 
2025-04-09 10:46:00.033938: Steps 15/90: batch_recall = 44.73, batch_ndcg = 29.84 
2025-04-09 10:46:00.791334: Steps 16/90: batch_recall = 35.73, batch_ndcg = 25.55 
2025-04-09 10:46:01.541536: Steps 17/90: batch_recall = 33.64, batch_ndcg = 22.57 
2025-04-09 10:46:02.291361: Steps 18/90: batch_recall = 39.49, batch_ndcg = 27.00 
2025-04-09 10:46:03.034218: Steps 19/90: batch_recall = 34.10, batch_ndcg = 24.05 
2025-04-09 10:46:03.765121: Steps 20/90: batch_recall = 41.85, batch_ndcg = 27.23 
2025-04-09 10:46:04.498470: Steps 21/90: batch_recall = 42.67, batch_ndcg = 30.53 
2025-04-09 10:46:05.229264: Steps 22/90: batch_recall = 37.99, batch_ndcg = 27.28 
2025-04-09 10:46:05.968059: Steps 23/90: batch_recall = 39.72, batch_ndcg = 26.44 
2025-04-09 10:46:06.694669: Steps 24/90: batch_recall = 36.67, batch_ndcg = 26.44 
2025-04-09 10:46:07.405617: Steps 25/90: batch_recall = 38.54, batch_ndcg = 25.48 
2025-04-09 10:46:08.130583: Steps 26/90: batch_recall = 37.24, batch_ndcg = 27.37 
2025-04-09 10:46:08.851390: Steps 27/90: batch_recall = 32.33, batch_ndcg = 23.06 
2025-04-09 10:46:09.581001: Steps 28/90: batch_recall = 38.44, batch_ndcg = 26.26 
2025-04-09 10:46:10.310612: Steps 29/90: batch_recall = 34.91, batch_ndcg = 23.92 
2025-04-09 10:46:11.053325: Steps 30/90: batch_recall = 34.34, batch_ndcg = 22.86 
2025-04-09 10:46:11.785440: Steps 31/90: batch_recall = 33.57, batch_ndcg = 23.93 
2025-04-09 10:46:12.498725: Steps 32/90: batch_recall = 36.58, batch_ndcg = 24.26 
2025-04-09 10:46:13.224215: Steps 33/90: batch_recall = 36.41, batch_ndcg = 22.77 
2025-04-09 10:46:13.944312: Steps 34/90: batch_recall = 36.17, batch_ndcg = 24.24 
2025-04-09 10:46:14.674147: Steps 35/90: batch_recall = 38.26, batch_ndcg = 27.66 
2025-04-09 10:46:15.403431: Steps 36/90: batch_recall = 37.38, batch_ndcg = 25.16 
2025-04-09 10:46:16.115392: Steps 37/90: batch_recall = 33.04, batch_ndcg = 21.45 
2025-04-09 10:46:16.849037: Steps 38/90: batch_recall = 36.75, batch_ndcg = 23.99 
2025-04-09 10:46:17.571742: Steps 39/90: batch_recall = 37.09, batch_ndcg = 23.54 
2025-04-09 10:46:18.284107: Steps 40/90: batch_recall = 31.59, batch_ndcg = 22.32 
2025-04-09 10:46:19.010039: Steps 41/90: batch_recall = 35.52, batch_ndcg = 22.99 
2025-04-09 10:46:19.713378: Steps 42/90: batch_recall = 43.22, batch_ndcg = 29.09 
2025-04-09 10:46:20.427465: Steps 43/90: batch_recall = 34.50, batch_ndcg = 22.38 
2025-04-09 10:46:21.164304: Steps 44/90: batch_recall = 33.41, batch_ndcg = 23.04 
2025-04-09 10:46:21.888981: Steps 45/90: batch_recall = 32.54, batch_ndcg = 22.07 
2025-04-09 10:46:22.617036: Steps 46/90: batch_recall = 34.80, batch_ndcg = 22.47 
2025-04-09 10:46:23.324776: Steps 47/90: batch_recall = 32.14, batch_ndcg = 20.60 
2025-04-09 10:46:24.027631: Steps 48/90: batch_recall = 37.29, batch_ndcg = 23.90 
2025-04-09 10:46:24.731528: Steps 49/90: batch_recall = 31.91, batch_ndcg = 19.48 
2025-04-09 10:46:25.449525: Steps 50/90: batch_recall = 39.53, batch_ndcg = 26.10 
2025-04-09 10:46:26.163750: Steps 51/90: batch_recall = 39.75, batch_ndcg = 26.34 
2025-04-09 10:46:26.885367: Steps 52/90: batch_recall = 38.78, batch_ndcg = 25.45 
2025-04-09 10:46:27.593494: Steps 53/90: batch_recall = 42.03, batch_ndcg = 26.85 
2025-04-09 10:46:28.317293: Steps 54/90: batch_recall = 30.81, batch_ndcg = 20.29 
2025-04-09 10:46:29.033762: Steps 55/90: batch_recall = 36.32, batch_ndcg = 23.75 
2025-04-09 10:46:29.756332: Steps 56/90: batch_recall = 33.26, batch_ndcg = 19.59 
2025-04-09 10:46:30.478419: Steps 57/90: batch_recall = 37.09, batch_ndcg = 24.70 
2025-04-09 10:46:31.180131: Steps 58/90: batch_recall = 39.11, batch_ndcg = 24.00 
2025-04-09 10:46:31.900942: Steps 59/90: batch_recall = 35.74, batch_ndcg = 23.38 
2025-04-09 10:46:32.621170: Steps 60/90: batch_recall = 35.43, batch_ndcg = 22.85 
2025-04-09 10:46:33.332247: Steps 61/90: batch_recall = 40.03, batch_ndcg = 27.46 
2025-04-09 10:46:34.038732: Steps 62/90: batch_recall = 40.26, batch_ndcg = 25.72 
2025-04-09 10:46:34.769273: Steps 63/90: batch_recall = 34.12, batch_ndcg = 22.66 
2025-04-09 10:46:35.500184: Steps 64/90: batch_recall = 35.75, batch_ndcg = 22.94 
2025-04-09 10:46:36.211990: Steps 65/90: batch_recall = 33.96, batch_ndcg = 22.37 
2025-04-09 10:46:36.930239: Steps 66/90: batch_recall = 36.72, batch_ndcg = 22.60 
2025-04-09 10:46:37.633937: Steps 67/90: batch_recall = 32.60, batch_ndcg = 20.92 
2025-04-09 10:46:38.329745: Steps 68/90: batch_recall = 36.95, batch_ndcg = 22.58 
2025-04-09 10:46:39.030858: Steps 69/90: batch_recall = 39.21, batch_ndcg = 24.05 
2025-04-09 10:46:39.732273: Steps 70/90: batch_recall = 39.08, batch_ndcg = 25.43 
2025-04-09 10:46:40.452531: Steps 71/90: batch_recall = 41.06, batch_ndcg = 24.86 
2025-04-09 10:46:41.159454: Steps 72/90: batch_recall = 36.08, batch_ndcg = 22.29 
2025-04-09 10:46:41.872683: Steps 73/90: batch_recall = 34.41, batch_ndcg = 21.78 
2025-04-09 10:46:42.574964: Steps 74/90: batch_recall = 40.96, batch_ndcg = 25.31 
2025-04-09 10:46:43.302184: Steps 75/90: batch_recall = 38.14, batch_ndcg = 23.74 
2025-04-09 10:46:44.017476: Steps 76/90: batch_recall = 40.95, batch_ndcg = 26.34 
2025-04-09 10:46:44.717493: Steps 77/90: batch_recall = 45.37, batch_ndcg = 27.97 
2025-04-09 10:46:45.426883: Steps 78/90: batch_recall = 33.88, batch_ndcg = 20.78 
2025-04-09 10:46:46.136123: Steps 79/90: batch_recall = 40.24, batch_ndcg = 26.62 
2025-04-09 10:46:46.854454: Steps 80/90: batch_recall = 44.92, batch_ndcg = 29.59 
2025-04-09 10:46:47.586110: Steps 81/90: batch_recall = 45.90, batch_ndcg = 28.78 
2025-04-09 10:46:48.300509: Steps 82/90: batch_recall = 44.68, batch_ndcg = 28.08 
2025-04-09 10:46:49.006648: Steps 83/90: batch_recall = 43.83, batch_ndcg = 26.69 
2025-04-09 10:46:49.712849: Steps 84/90: batch_recall = 36.47, batch_ndcg = 22.46 
2025-04-09 10:46:50.416641: Steps 85/90: batch_recall = 41.36, batch_ndcg = 24.51 
2025-04-09 10:46:51.130655: Steps 86/90: batch_recall = 42.13, batch_ndcg = 28.29 
2025-04-09 10:46:51.854491: Steps 87/90: batch_recall = 43.34, batch_ndcg = 28.91 
2025-04-09 10:46:52.561296: Steps 88/90: batch_recall = 50.45, batch_ndcg = 29.98 
2025-04-09 10:46:53.105152: Steps 89/90: batch_recall = 40.94, batch_ndcg = 25.85 
2025-04-09 10:46:53.106126: Epoch 36/1000, Test: Recall = 0.0765, NDCG = 0.0510  

2025-04-09 10:46:54.102681: Training Step 0/115: batchLoss = 3.9702, diffLoss = 6.5074, kgLoss = 0.1643
2025-04-09 10:46:54.837406: Training Step 1/115: batchLoss = 4.1275, diffLoss = 6.7655, kgLoss = 0.1705
2025-04-09 10:46:55.568974: Training Step 2/115: batchLoss = 4.7241, diffLoss = 7.7481, kgLoss = 0.1881
2025-04-09 10:46:56.305045: Training Step 3/115: batchLoss = 4.2438, diffLoss = 6.9577, kgLoss = 0.1729
2025-04-09 10:46:57.042078: Training Step 4/115: batchLoss = 4.4006, diffLoss = 7.2100, kgLoss = 0.1866
2025-04-09 10:46:57.779233: Training Step 5/115: batchLoss = 3.8533, diffLoss = 6.3116, kgLoss = 0.1659
2025-04-09 10:46:58.512559: Training Step 6/115: batchLoss = 4.1788, diffLoss = 6.8516, kgLoss = 0.1697
2025-04-09 10:46:59.256671: Training Step 7/115: batchLoss = 4.0496, diffLoss = 6.6360, kgLoss = 0.1702
2025-04-09 10:46:59.993647: Training Step 8/115: batchLoss = 4.2495, diffLoss = 6.9702, kgLoss = 0.1685
2025-04-09 10:47:00.730416: Training Step 9/115: batchLoss = 4.1170, diffLoss = 6.7491, kgLoss = 0.1688
2025-04-09 10:47:01.478488: Training Step 10/115: batchLoss = 4.2820, diffLoss = 7.0187, kgLoss = 0.1770
2025-04-09 10:47:02.238221: Training Step 11/115: batchLoss = 4.5384, diffLoss = 7.4400, kgLoss = 0.1860
2025-04-09 10:47:02.972275: Training Step 12/115: batchLoss = 3.9166, diffLoss = 6.4126, kgLoss = 0.1724
2025-04-09 10:47:03.706134: Training Step 13/115: batchLoss = 3.9648, diffLoss = 6.4977, kgLoss = 0.1655
2025-04-09 10:47:04.434084: Training Step 14/115: batchLoss = 3.9092, diffLoss = 6.4073, kgLoss = 0.1622
2025-04-09 10:47:05.174364: Training Step 15/115: batchLoss = 4.1126, diffLoss = 6.7407, kgLoss = 0.1703
2025-04-09 10:47:05.925886: Training Step 16/115: batchLoss = 4.7398, diffLoss = 7.7679, kgLoss = 0.1978
2025-04-09 10:47:06.658687: Training Step 17/115: batchLoss = 4.7849, diffLoss = 7.8409, kgLoss = 0.2010
2025-04-09 10:47:07.396572: Training Step 18/115: batchLoss = 4.3488, diffLoss = 7.1220, kgLoss = 0.1892
2025-04-09 10:47:08.120364: Training Step 19/115: batchLoss = 3.6746, diffLoss = 6.0252, kgLoss = 0.1489
2025-04-09 10:47:08.840253: Training Step 20/115: batchLoss = 4.3271, diffLoss = 7.1028, kgLoss = 0.1636
2025-04-09 10:47:09.565284: Training Step 21/115: batchLoss = 4.4632, diffLoss = 7.3238, kgLoss = 0.1722
2025-04-09 10:47:10.301086: Training Step 22/115: batchLoss = 4.3637, diffLoss = 7.1511, kgLoss = 0.1826
2025-04-09 10:47:11.034310: Training Step 23/115: batchLoss = 4.0759, diffLoss = 6.6760, kgLoss = 0.1758
2025-04-09 10:47:11.760281: Training Step 24/115: batchLoss = 3.8713, diffLoss = 6.3438, kgLoss = 0.1625
2025-04-09 10:47:12.486524: Training Step 25/115: batchLoss = 3.9003, diffLoss = 6.3904, kgLoss = 0.1652
2025-04-09 10:47:13.210793: Training Step 26/115: batchLoss = 4.1224, diffLoss = 6.7599, kgLoss = 0.1662
2025-04-09 10:47:13.949068: Training Step 27/115: batchLoss = 4.2106, diffLoss = 6.9008, kgLoss = 0.1754
2025-04-09 10:47:14.712743: Training Step 28/115: batchLoss = 3.7175, diffLoss = 6.0886, kgLoss = 0.1607
2025-04-09 10:47:15.461537: Training Step 29/115: batchLoss = 3.9211, diffLoss = 6.4290, kgLoss = 0.1593
2025-04-09 10:47:16.203940: Training Step 30/115: batchLoss = 4.1408, diffLoss = 6.7810, kgLoss = 0.1805
2025-04-09 10:47:16.958910: Training Step 31/115: batchLoss = 4.2761, diffLoss = 7.0111, kgLoss = 0.1735
2025-04-09 10:47:17.705592: Training Step 32/115: batchLoss = 4.0168, diffLoss = 6.5850, kgLoss = 0.1646
2025-04-09 10:47:18.460491: Training Step 33/115: batchLoss = 4.3040, diffLoss = 7.0562, kgLoss = 0.1758
2025-04-09 10:47:19.206736: Training Step 34/115: batchLoss = 4.0734, diffLoss = 6.6765, kgLoss = 0.1687
2025-04-09 10:47:19.948829: Training Step 35/115: batchLoss = 4.4281, diffLoss = 7.2588, kgLoss = 0.1820
2025-04-09 10:47:20.696043: Training Step 36/115: batchLoss = 4.4458, diffLoss = 7.2755, kgLoss = 0.2013
2025-04-09 10:47:21.440399: Training Step 37/115: batchLoss = 4.0768, diffLoss = 6.6843, kgLoss = 0.1655
2025-04-09 10:47:22.170058: Training Step 38/115: batchLoss = 4.4714, diffLoss = 7.3231, kgLoss = 0.1939
2025-04-09 10:47:22.911892: Training Step 39/115: batchLoss = 3.6980, diffLoss = 6.0537, kgLoss = 0.1645
2025-04-09 10:47:23.652474: Training Step 40/115: batchLoss = 4.0348, diffLoss = 6.6139, kgLoss = 0.1661
2025-04-09 10:47:24.398968: Training Step 41/115: batchLoss = 4.4160, diffLoss = 7.2433, kgLoss = 0.1750
2025-04-09 10:47:25.141535: Training Step 42/115: batchLoss = 4.1108, diffLoss = 6.7392, kgLoss = 0.1682
2025-04-09 10:47:25.900399: Training Step 43/115: batchLoss = 4.3773, diffLoss = 7.1739, kgLoss = 0.1823
2025-04-09 10:47:26.647193: Training Step 44/115: batchLoss = 3.7473, diffLoss = 6.1428, kgLoss = 0.1540
2025-04-09 10:47:27.389450: Training Step 45/115: batchLoss = 4.0883, diffLoss = 6.7056, kgLoss = 0.1624
2025-04-09 10:47:28.141109: Training Step 46/115: batchLoss = 3.9368, diffLoss = 6.4535, kgLoss = 0.1618
2025-04-09 10:47:28.893112: Training Step 47/115: batchLoss = 4.6259, diffLoss = 7.5863, kgLoss = 0.1854
2025-04-09 10:47:29.641032: Training Step 48/115: batchLoss = 4.6964, diffLoss = 7.7028, kgLoss = 0.1868
2025-04-09 10:47:30.403403: Training Step 49/115: batchLoss = 4.2111, diffLoss = 6.9033, kgLoss = 0.1728
2025-04-09 10:47:31.151367: Training Step 50/115: batchLoss = 4.0477, diffLoss = 6.6291, kgLoss = 0.1756
2025-04-09 10:47:31.914064: Training Step 51/115: batchLoss = 4.1446, diffLoss = 6.7969, kgLoss = 0.1660
2025-04-09 10:47:32.656225: Training Step 52/115: batchLoss = 3.7859, diffLoss = 6.2033, kgLoss = 0.1598
2025-04-09 10:47:33.403039: Training Step 53/115: batchLoss = 4.1237, diffLoss = 6.7572, kgLoss = 0.1734
2025-04-09 10:47:34.148222: Training Step 54/115: batchLoss = 4.4083, diffLoss = 7.2251, kgLoss = 0.1830
2025-04-09 10:47:34.894226: Training Step 55/115: batchLoss = 4.1325, diffLoss = 6.7746, kgLoss = 0.1692
2025-04-09 10:47:35.644304: Training Step 56/115: batchLoss = 4.7568, diffLoss = 7.7986, kgLoss = 0.1939
2025-04-09 10:47:36.380595: Training Step 57/115: batchLoss = 4.4088, diffLoss = 7.2251, kgLoss = 0.1844
2025-04-09 10:47:37.118445: Training Step 58/115: batchLoss = 3.6976, diffLoss = 6.0581, kgLoss = 0.1569
2025-04-09 10:47:37.847483: Training Step 59/115: batchLoss = 4.5697, diffLoss = 7.4907, kgLoss = 0.1882
2025-04-09 10:47:38.578709: Training Step 60/115: batchLoss = 4.4219, diffLoss = 7.2568, kgLoss = 0.1696
2025-04-09 10:47:39.317699: Training Step 61/115: batchLoss = 3.9454, diffLoss = 6.4673, kgLoss = 0.1625
2025-04-09 10:47:40.057899: Training Step 62/115: batchLoss = 4.3953, diffLoss = 7.2090, kgLoss = 0.1747
2025-04-09 10:47:40.808316: Training Step 63/115: batchLoss = 4.1352, diffLoss = 6.7783, kgLoss = 0.1706
2025-04-09 10:47:41.534995: Training Step 64/115: batchLoss = 3.7821, diffLoss = 6.1986, kgLoss = 0.1573
2025-04-09 10:47:42.273970: Training Step 65/115: batchLoss = 4.1563, diffLoss = 6.8152, kgLoss = 0.1679
2025-04-09 10:47:43.024256: Training Step 66/115: batchLoss = 4.4217, diffLoss = 7.2496, kgLoss = 0.1800
2025-04-09 10:47:43.797994: Training Step 67/115: batchLoss = 4.1850, diffLoss = 6.8579, kgLoss = 0.1756
2025-04-09 10:47:44.564595: Training Step 68/115: batchLoss = 4.2370, diffLoss = 6.9426, kgLoss = 0.1786
2025-04-09 10:47:45.314766: Training Step 69/115: batchLoss = 4.5303, diffLoss = 7.4235, kgLoss = 0.1906
2025-04-09 10:47:46.056199: Training Step 70/115: batchLoss = 3.7378, diffLoss = 6.1186, kgLoss = 0.1665
2025-04-09 10:47:46.810531: Training Step 71/115: batchLoss = 4.5412, diffLoss = 7.4426, kgLoss = 0.1892
2025-04-09 10:47:47.543797: Training Step 72/115: batchLoss = 4.5817, diffLoss = 7.5184, kgLoss = 0.1767
2025-04-09 10:47:48.287278: Training Step 73/115: batchLoss = 4.2310, diffLoss = 6.9357, kgLoss = 0.1739
2025-04-09 10:47:49.041204: Training Step 74/115: batchLoss = 4.4019, diffLoss = 7.2072, kgLoss = 0.1939
2025-04-09 10:47:49.779609: Training Step 75/115: batchLoss = 4.2927, diffLoss = 7.0358, kgLoss = 0.1780
2025-04-09 10:47:50.513068: Training Step 76/115: batchLoss = 4.1420, diffLoss = 6.7935, kgLoss = 0.1647
2025-04-09 10:47:51.260034: Training Step 77/115: batchLoss = 3.9452, diffLoss = 6.4671, kgLoss = 0.1624
2025-04-09 10:47:51.992338: Training Step 78/115: batchLoss = 4.1408, diffLoss = 6.7803, kgLoss = 0.1816
2025-04-09 10:47:52.733606: Training Step 79/115: batchLoss = 3.6517, diffLoss = 5.9825, kgLoss = 0.1554
2025-04-09 10:47:53.468046: Training Step 80/115: batchLoss = 4.3931, diffLoss = 7.2000, kgLoss = 0.1826
2025-04-09 10:47:54.201541: Training Step 81/115: batchLoss = 5.1387, diffLoss = 8.4313, kgLoss = 0.1998
2025-04-09 10:47:54.927799: Training Step 82/115: batchLoss = 4.4220, diffLoss = 7.2566, kgLoss = 0.1701
2025-04-09 10:47:55.664843: Training Step 83/115: batchLoss = 4.4873, diffLoss = 7.3545, kgLoss = 0.1866
2025-04-09 10:47:56.407919: Training Step 84/115: batchLoss = 3.7977, diffLoss = 6.2200, kgLoss = 0.1644
2025-04-09 10:47:57.156666: Training Step 85/115: batchLoss = 3.6847, diffLoss = 6.0367, kgLoss = 0.1567
2025-04-09 10:47:57.912753: Training Step 86/115: batchLoss = 4.3339, diffLoss = 7.1055, kgLoss = 0.1766
2025-04-09 10:47:58.656911: Training Step 87/115: batchLoss = 3.9710, diffLoss = 6.5098, kgLoss = 0.1628
2025-04-09 10:47:59.408283: Training Step 88/115: batchLoss = 4.1227, diffLoss = 6.7612, kgLoss = 0.1649
2025-04-09 10:48:00.150586: Training Step 89/115: batchLoss = 4.0233, diffLoss = 6.5933, kgLoss = 0.1683
2025-04-09 10:48:00.894866: Training Step 90/115: batchLoss = 3.5519, diffLoss = 5.8240, kgLoss = 0.1437
2025-04-09 10:48:01.653276: Training Step 91/115: batchLoss = 4.3834, diffLoss = 7.1884, kgLoss = 0.1759
2025-04-09 10:48:02.400891: Training Step 92/115: batchLoss = 4.3067, diffLoss = 7.0575, kgLoss = 0.1805
2025-04-09 10:48:03.145000: Training Step 93/115: batchLoss = 4.1792, diffLoss = 6.8578, kgLoss = 0.1613
2025-04-09 10:48:03.893452: Training Step 94/115: batchLoss = 4.5074, diffLoss = 7.3852, kgLoss = 0.1907
2025-04-09 10:48:04.661829: Training Step 95/115: batchLoss = 4.2080, diffLoss = 6.9015, kgLoss = 0.1677
2025-04-09 10:48:05.419142: Training Step 96/115: batchLoss = 4.0724, diffLoss = 6.6760, kgLoss = 0.1669
2025-04-09 10:48:06.176154: Training Step 97/115: batchLoss = 3.9524, diffLoss = 6.4798, kgLoss = 0.1614
2025-04-09 10:48:06.908559: Training Step 98/115: batchLoss = 4.2783, diffLoss = 7.0034, kgLoss = 0.1905
2025-04-09 10:48:07.631654: Training Step 99/115: batchLoss = 4.2806, diffLoss = 7.0184, kgLoss = 0.1740
2025-04-09 10:48:08.386284: Training Step 100/115: batchLoss = 4.0165, diffLoss = 6.5832, kgLoss = 0.1664
2025-04-09 10:48:09.119652: Training Step 101/115: batchLoss = 3.9015, diffLoss = 6.3904, kgLoss = 0.1682
2025-04-09 10:48:09.851633: Training Step 102/115: batchLoss = 4.1195, diffLoss = 6.7545, kgLoss = 0.1670
2025-04-09 10:48:10.577144: Training Step 103/115: batchLoss = 4.3573, diffLoss = 7.1330, kgLoss = 0.1938
2025-04-09 10:48:11.312374: Training Step 104/115: batchLoss = 4.9660, diffLoss = 8.1427, kgLoss = 0.2009
2025-04-09 10:48:12.047027: Training Step 105/115: batchLoss = 3.9163, diffLoss = 6.4224, kgLoss = 0.1572
2025-04-09 10:48:12.793374: Training Step 106/115: batchLoss = 4.2829, diffLoss = 7.0155, kgLoss = 0.1840
2025-04-09 10:48:13.534129: Training Step 107/115: batchLoss = 4.5143, diffLoss = 7.3979, kgLoss = 0.1889
2025-04-09 10:48:14.290984: Training Step 108/115: batchLoss = 4.0655, diffLoss = 6.6551, kgLoss = 0.1811
2025-04-09 10:48:15.051047: Training Step 109/115: batchLoss = 4.3282, diffLoss = 7.1004, kgLoss = 0.1700
2025-04-09 10:48:15.810053: Training Step 110/115: batchLoss = 5.0114, diffLoss = 8.2207, kgLoss = 0.1973
2025-04-09 10:48:16.555759: Training Step 111/115: batchLoss = 4.0439, diffLoss = 6.6325, kgLoss = 0.1611
2025-04-09 10:48:17.286953: Training Step 112/115: batchLoss = 4.3014, diffLoss = 7.0565, kgLoss = 0.1687
2025-04-09 10:48:17.947847: Training Step 113/115: batchLoss = 3.5974, diffLoss = 5.8897, kgLoss = 0.1591
2025-04-09 10:48:18.577351: Training Step 114/115: batchLoss = 4.6676, diffLoss = 7.6542, kgLoss = 0.1878
2025-04-09 10:48:18.700679: 
2025-04-09 10:48:18.701973: Epoch 37/1000, Train: epLoss = 1.2087, epDfLoss = 1.9812, epfTransLoss = 0.0000, epKgLoss = 0.0500  
2025-04-09 10:48:19.487086: Steps 0/90: batch_recall = 56.27, batch_ndcg = 43.18 
2025-04-09 10:48:20.245187: Steps 1/90: batch_recall = 58.12, batch_ndcg = 41.22 
2025-04-09 10:48:21.002829: Steps 2/90: batch_recall = 51.07, batch_ndcg = 38.26 
2025-04-09 10:48:21.752609: Steps 3/90: batch_recall = 46.68, batch_ndcg = 32.83 
2025-04-09 10:48:22.512874: Steps 4/90: batch_recall = 48.56, batch_ndcg = 35.81 
2025-04-09 10:48:23.258255: Steps 5/90: batch_recall = 39.26, batch_ndcg = 28.71 
2025-04-09 10:48:23.993800: Steps 6/90: batch_recall = 47.58, batch_ndcg = 32.91 
2025-04-09 10:48:24.744886: Steps 7/90: batch_recall = 43.11, batch_ndcg = 32.05 
2025-04-09 10:48:25.478752: Steps 8/90: batch_recall = 46.31, batch_ndcg = 33.56 
2025-04-09 10:48:26.216670: Steps 9/90: batch_recall = 45.00, batch_ndcg = 33.29 
2025-04-09 10:48:26.948945: Steps 10/90: batch_recall = 39.53, batch_ndcg = 29.42 
2025-04-09 10:48:27.679628: Steps 11/90: batch_recall = 43.63, batch_ndcg = 29.23 
2025-04-09 10:48:28.412977: Steps 12/90: batch_recall = 35.74, batch_ndcg = 26.74 
2025-04-09 10:48:29.152810: Steps 13/90: batch_recall = 36.63, batch_ndcg = 26.48 
2025-04-09 10:48:29.895516: Steps 14/90: batch_recall = 35.34, batch_ndcg = 25.17 
2025-04-09 10:48:30.635645: Steps 15/90: batch_recall = 44.56, batch_ndcg = 29.82 
2025-04-09 10:48:31.379026: Steps 16/90: batch_recall = 35.02, batch_ndcg = 25.12 
2025-04-09 10:48:32.109499: Steps 17/90: batch_recall = 32.47, batch_ndcg = 22.45 
2025-04-09 10:48:32.838681: Steps 18/90: batch_recall = 39.91, batch_ndcg = 27.21 
2025-04-09 10:48:33.578560: Steps 19/90: batch_recall = 33.50, batch_ndcg = 23.87 
2025-04-09 10:48:34.309518: Steps 20/90: batch_recall = 40.95, batch_ndcg = 26.89 
2025-04-09 10:48:35.046928: Steps 21/90: batch_recall = 43.12, batch_ndcg = 30.33 
2025-04-09 10:48:35.763147: Steps 22/90: batch_recall = 38.24, batch_ndcg = 27.58 
2025-04-09 10:48:36.482966: Steps 23/90: batch_recall = 39.93, batch_ndcg = 26.85 
2025-04-09 10:48:37.194931: Steps 24/90: batch_recall = 36.25, batch_ndcg = 26.84 
2025-04-09 10:48:37.914076: Steps 25/90: batch_recall = 37.03, batch_ndcg = 25.06 
2025-04-09 10:48:38.645180: Steps 26/90: batch_recall = 37.79, batch_ndcg = 27.72 
2025-04-09 10:48:39.375152: Steps 27/90: batch_recall = 33.85, batch_ndcg = 23.60 
2025-04-09 10:48:40.114385: Steps 28/90: batch_recall = 38.63, batch_ndcg = 26.13 
2025-04-09 10:48:40.840908: Steps 29/90: batch_recall = 33.91, batch_ndcg = 23.73 
2025-04-09 10:48:41.561080: Steps 30/90: batch_recall = 33.23, batch_ndcg = 22.60 
2025-04-09 10:48:42.291905: Steps 31/90: batch_recall = 33.92, batch_ndcg = 23.98 
2025-04-09 10:48:43.020589: Steps 32/90: batch_recall = 35.21, batch_ndcg = 23.78 
2025-04-09 10:48:43.735040: Steps 33/90: batch_recall = 35.48, batch_ndcg = 22.44 
2025-04-09 10:48:44.445513: Steps 34/90: batch_recall = 36.11, batch_ndcg = 24.55 
2025-04-09 10:48:45.176945: Steps 35/90: batch_recall = 38.84, batch_ndcg = 27.96 
2025-04-09 10:48:45.906380: Steps 36/90: batch_recall = 38.11, batch_ndcg = 25.28 
2025-04-09 10:48:46.630745: Steps 37/90: batch_recall = 33.56, batch_ndcg = 21.65 
2025-04-09 10:48:47.376124: Steps 38/90: batch_recall = 35.94, batch_ndcg = 23.75 
2025-04-09 10:48:48.110617: Steps 39/90: batch_recall = 37.58, batch_ndcg = 23.96 
2025-04-09 10:48:48.837899: Steps 40/90: batch_recall = 32.69, batch_ndcg = 22.85 
2025-04-09 10:48:49.550262: Steps 41/90: batch_recall = 34.68, batch_ndcg = 22.44 
2025-04-09 10:48:50.269169: Steps 42/90: batch_recall = 44.34, batch_ndcg = 29.53 
2025-04-09 10:48:50.991544: Steps 43/90: batch_recall = 35.60, batch_ndcg = 22.68 
2025-04-09 10:48:51.703712: Steps 44/90: batch_recall = 33.90, batch_ndcg = 23.18 
2025-04-09 10:48:52.409402: Steps 45/90: batch_recall = 31.38, batch_ndcg = 21.80 
2025-04-09 10:48:53.116804: Steps 46/90: batch_recall = 34.56, batch_ndcg = 22.49 
2025-04-09 10:48:53.820202: Steps 47/90: batch_recall = 32.74, batch_ndcg = 20.79 
2025-04-09 10:48:54.521786: Steps 48/90: batch_recall = 37.10, batch_ndcg = 23.72 
2025-04-09 10:48:55.225931: Steps 49/90: batch_recall = 31.17, batch_ndcg = 19.40 
2025-04-09 10:48:55.938278: Steps 50/90: batch_recall = 40.48, batch_ndcg = 26.43 
2025-04-09 10:48:56.673855: Steps 51/90: batch_recall = 39.38, batch_ndcg = 26.19 
2025-04-09 10:48:57.394363: Steps 52/90: batch_recall = 38.53, batch_ndcg = 25.55 
2025-04-09 10:48:58.102877: Steps 53/90: batch_recall = 41.52, batch_ndcg = 26.45 
2025-04-09 10:48:58.818817: Steps 54/90: batch_recall = 29.91, batch_ndcg = 19.97 
2025-04-09 10:48:59.522890: Steps 55/90: batch_recall = 36.32, batch_ndcg = 23.42 
2025-04-09 10:49:00.235956: Steps 56/90: batch_recall = 33.47, batch_ndcg = 19.92 
2025-04-09 10:49:00.966632: Steps 57/90: batch_recall = 36.54, batch_ndcg = 24.38 
2025-04-09 10:49:01.673436: Steps 58/90: batch_recall = 39.04, batch_ndcg = 23.57 
2025-04-09 10:49:02.414373: Steps 59/90: batch_recall = 36.35, batch_ndcg = 23.96 
2025-04-09 10:49:03.141488: Steps 60/90: batch_recall = 35.57, batch_ndcg = 23.06 
2025-04-09 10:49:03.850699: Steps 61/90: batch_recall = 40.03, batch_ndcg = 27.59 
2025-04-09 10:49:04.543561: Steps 62/90: batch_recall = 38.89, batch_ndcg = 24.92 
2025-04-09 10:49:05.259252: Steps 63/90: batch_recall = 33.52, batch_ndcg = 22.80 
2025-04-09 10:49:05.962303: Steps 64/90: batch_recall = 35.10, batch_ndcg = 22.77 
2025-04-09 10:49:06.673803: Steps 65/90: batch_recall = 33.96, batch_ndcg = 22.40 
2025-04-09 10:49:07.378492: Steps 66/90: batch_recall = 36.87, batch_ndcg = 22.82 
2025-04-09 10:49:08.076509: Steps 67/90: batch_recall = 32.50, batch_ndcg = 20.67 
2025-04-09 10:49:08.766913: Steps 68/90: batch_recall = 36.30, batch_ndcg = 22.39 
2025-04-09 10:49:09.460199: Steps 69/90: batch_recall = 40.49, batch_ndcg = 24.75 
2025-04-09 10:49:10.176243: Steps 70/90: batch_recall = 39.43, batch_ndcg = 25.12 
2025-04-09 10:49:10.885778: Steps 71/90: batch_recall = 40.26, batch_ndcg = 24.85 
2025-04-09 10:49:11.591525: Steps 72/90: batch_recall = 34.42, batch_ndcg = 21.78 
2025-04-09 10:49:12.298782: Steps 73/90: batch_recall = 35.41, batch_ndcg = 22.26 
2025-04-09 10:49:13.008960: Steps 74/90: batch_recall = 39.81, batch_ndcg = 25.57 
2025-04-09 10:49:13.727880: Steps 75/90: batch_recall = 38.26, batch_ndcg = 23.65 
2025-04-09 10:49:14.442648: Steps 76/90: batch_recall = 39.35, batch_ndcg = 25.80 
2025-04-09 10:49:15.152431: Steps 77/90: batch_recall = 45.03, batch_ndcg = 27.85 
2025-04-09 10:49:15.893451: Steps 78/90: batch_recall = 34.82, batch_ndcg = 21.22 
2025-04-09 10:49:16.616328: Steps 79/90: batch_recall = 39.97, batch_ndcg = 26.41 
2025-04-09 10:49:17.324139: Steps 80/90: batch_recall = 44.53, batch_ndcg = 29.30 
2025-04-09 10:49:18.023442: Steps 81/90: batch_recall = 47.15, batch_ndcg = 29.02 
2025-04-09 10:49:18.743834: Steps 82/90: batch_recall = 44.36, batch_ndcg = 28.02 
2025-04-09 10:49:19.443140: Steps 83/90: batch_recall = 44.48, batch_ndcg = 27.01 
2025-04-09 10:49:20.149018: Steps 84/90: batch_recall = 35.71, batch_ndcg = 22.41 
2025-04-09 10:49:20.844833: Steps 85/90: batch_recall = 41.72, batch_ndcg = 24.63 
2025-04-09 10:49:21.550411: Steps 86/90: batch_recall = 42.89, batch_ndcg = 29.06 
2025-04-09 10:49:22.247263: Steps 87/90: batch_recall = 44.16, batch_ndcg = 29.16 
2025-04-09 10:49:22.945556: Steps 88/90: batch_recall = 50.74, batch_ndcg = 30.30 
2025-04-09 10:49:23.483368: Steps 89/90: batch_recall = 41.27, batch_ndcg = 26.38 
2025-04-09 10:49:23.484265: Epoch 37/1000, Test: Recall = 0.0763, NDCG = 0.0511  

2025-04-09 10:49:24.515527: Training Step 0/115: batchLoss = 4.1302, diffLoss = 6.7689, kgLoss = 0.1722
2025-04-09 10:49:25.245427: Training Step 1/115: batchLoss = 4.1659, diffLoss = 6.8258, kgLoss = 0.1761
2025-04-09 10:49:25.959361: Training Step 2/115: batchLoss = 4.2476, diffLoss = 6.9615, kgLoss = 0.1768
2025-04-09 10:49:26.679730: Training Step 3/115: batchLoss = 4.4126, diffLoss = 7.2372, kgLoss = 0.1757
2025-04-09 10:49:27.412493: Training Step 4/115: batchLoss = 3.9139, diffLoss = 6.4156, kgLoss = 0.1613
2025-04-09 10:49:28.142361: Training Step 5/115: batchLoss = 4.5151, diffLoss = 7.4013, kgLoss = 0.1858
2025-04-09 10:49:28.866351: Training Step 6/115: batchLoss = 4.5949, diffLoss = 7.5355, kgLoss = 0.1840
2025-04-09 10:49:29.595174: Training Step 7/115: batchLoss = 3.9061, diffLoss = 6.3961, kgLoss = 0.1711
2025-04-09 10:49:30.325393: Training Step 8/115: batchLoss = 4.2827, diffLoss = 7.0254, kgLoss = 0.1686
2025-04-09 10:49:31.050308: Training Step 9/115: batchLoss = 3.8950, diffLoss = 6.3780, kgLoss = 0.1704
2025-04-09 10:49:31.789994: Training Step 10/115: batchLoss = 4.2430, diffLoss = 6.9525, kgLoss = 0.1787
2025-04-09 10:49:32.526219: Training Step 11/115: batchLoss = 4.3332, diffLoss = 7.1001, kgLoss = 0.1828
2025-04-09 10:49:33.255829: Training Step 12/115: batchLoss = 4.3170, diffLoss = 7.0771, kgLoss = 0.1769
2025-04-09 10:49:33.989755: Training Step 13/115: batchLoss = 4.0844, diffLoss = 6.6881, kgLoss = 0.1790
2025-04-09 10:49:34.729321: Training Step 14/115: batchLoss = 4.4019, diffLoss = 7.2171, kgLoss = 0.1791
2025-04-09 10:49:35.464642: Training Step 15/115: batchLoss = 4.1753, diffLoss = 6.8480, kgLoss = 0.1663
2025-04-09 10:49:36.200341: Training Step 16/115: batchLoss = 4.1410, diffLoss = 6.7890, kgLoss = 0.1689
2025-04-09 10:49:36.930755: Training Step 17/115: batchLoss = 4.1435, diffLoss = 6.7916, kgLoss = 0.1712
2025-04-09 10:49:37.663265: Training Step 18/115: batchLoss = 4.0441, diffLoss = 6.6282, kgLoss = 0.1679
2025-04-09 10:49:38.388962: Training Step 19/115: batchLoss = 4.2909, diffLoss = 7.0295, kgLoss = 0.1831
2025-04-09 10:49:39.110727: Training Step 20/115: batchLoss = 3.8510, diffLoss = 6.3108, kgLoss = 0.1613
2025-04-09 10:49:39.836925: Training Step 21/115: batchLoss = 4.2328, diffLoss = 6.9349, kgLoss = 0.1796
2025-04-09 10:49:40.560395: Training Step 22/115: batchLoss = 4.4111, diffLoss = 7.2251, kgLoss = 0.1900
2025-04-09 10:49:41.300626: Training Step 23/115: batchLoss = 3.8487, diffLoss = 6.3037, kgLoss = 0.1662
2025-04-09 10:49:42.036481: Training Step 24/115: batchLoss = 4.7896, diffLoss = 7.8513, kgLoss = 0.1972
2025-04-09 10:49:42.770316: Training Step 25/115: batchLoss = 4.0303, diffLoss = 6.6063, kgLoss = 0.1663
2025-04-09 10:49:43.507890: Training Step 26/115: batchLoss = 4.2641, diffLoss = 6.9917, kgLoss = 0.1726
2025-04-09 10:49:44.240848: Training Step 27/115: batchLoss = 3.8331, diffLoss = 6.2798, kgLoss = 0.1630
2025-04-09 10:49:44.975535: Training Step 28/115: batchLoss = 5.0521, diffLoss = 8.2854, kgLoss = 0.2023
2025-04-09 10:49:45.696449: Training Step 29/115: batchLoss = 4.0954, diffLoss = 6.7124, kgLoss = 0.1698
2025-04-09 10:49:46.434560: Training Step 30/115: batchLoss = 4.0624, diffLoss = 6.6518, kgLoss = 0.1783
2025-04-09 10:49:47.176906: Training Step 31/115: batchLoss = 4.3981, diffLoss = 7.2048, kgLoss = 0.1880
2025-04-09 10:49:47.916494: Training Step 32/115: batchLoss = 4.5342, diffLoss = 7.4368, kgLoss = 0.1802
2025-04-09 10:49:48.657165: Training Step 33/115: batchLoss = 4.4093, diffLoss = 7.2366, kgLoss = 0.1682
2025-04-09 10:49:49.393191: Training Step 34/115: batchLoss = 3.7750, diffLoss = 6.1868, kgLoss = 0.1572
2025-04-09 10:49:50.122027: Training Step 35/115: batchLoss = 4.0924, diffLoss = 6.7079, kgLoss = 0.1692
2025-04-09 10:49:50.851573: Training Step 36/115: batchLoss = 3.8878, diffLoss = 6.3730, kgLoss = 0.1601
2025-04-09 10:49:51.581043: Training Step 37/115: batchLoss = 3.6286, diffLoss = 5.9507, kgLoss = 0.1454
2025-04-09 10:49:52.333115: Training Step 38/115: batchLoss = 4.6944, diffLoss = 7.6873, kgLoss = 0.2050
2025-04-09 10:49:53.076998: Training Step 39/115: batchLoss = 3.6746, diffLoss = 6.0186, kgLoss = 0.1586
2025-04-09 10:49:53.827998: Training Step 40/115: batchLoss = 4.5316, diffLoss = 7.4316, kgLoss = 0.1817
2025-04-09 10:49:54.581291: Training Step 41/115: batchLoss = 4.0793, diffLoss = 6.6838, kgLoss = 0.1725
2025-04-09 10:49:55.319451: Training Step 42/115: batchLoss = 4.4933, diffLoss = 7.3672, kgLoss = 0.1824
2025-04-09 10:49:56.068012: Training Step 43/115: batchLoss = 4.2680, diffLoss = 6.9977, kgLoss = 0.1733
2025-04-09 10:49:56.811821: Training Step 44/115: batchLoss = 4.2594, diffLoss = 6.9807, kgLoss = 0.1774
2025-04-09 10:49:57.555475: Training Step 45/115: batchLoss = 3.9343, diffLoss = 6.4545, kgLoss = 0.1541
2025-04-09 10:49:58.284861: Training Step 46/115: batchLoss = 4.7580, diffLoss = 7.7963, kgLoss = 0.2005
2025-04-09 10:49:59.024423: Training Step 47/115: batchLoss = 3.9408, diffLoss = 6.4575, kgLoss = 0.1656
2025-04-09 10:49:59.756404: Training Step 48/115: batchLoss = 4.7319, diffLoss = 7.7414, kgLoss = 0.2176
2025-04-09 10:50:00.492703: Training Step 49/115: batchLoss = 4.0831, diffLoss = 6.6883, kgLoss = 0.1754
2025-04-09 10:50:01.224820: Training Step 50/115: batchLoss = 4.3792, diffLoss = 7.1800, kgLoss = 0.1780
2025-04-09 10:50:01.968364: Training Step 51/115: batchLoss = 4.1131, diffLoss = 6.7427, kgLoss = 0.1686
2025-04-09 10:50:02.722447: Training Step 52/115: batchLoss = 3.7761, diffLoss = 6.1834, kgLoss = 0.1653
2025-04-09 10:50:03.478829: Training Step 53/115: batchLoss = 4.6561, diffLoss = 7.6344, kgLoss = 0.1887
2025-04-09 10:50:04.222812: Training Step 54/115: batchLoss = 4.3669, diffLoss = 7.1568, kgLoss = 0.1820
2025-04-09 10:50:04.957354: Training Step 55/115: batchLoss = 4.4939, diffLoss = 7.3645, kgLoss = 0.1881
2025-04-09 10:50:05.700701: Training Step 56/115: batchLoss = 4.3240, diffLoss = 7.0857, kgLoss = 0.1815
2025-04-09 10:50:06.436586: Training Step 57/115: batchLoss = 3.7932, diffLoss = 6.2134, kgLoss = 0.1630
2025-04-09 10:50:07.176378: Training Step 58/115: batchLoss = 3.8575, diffLoss = 6.3147, kgLoss = 0.1718
2025-04-09 10:50:07.909859: Training Step 59/115: batchLoss = 4.1007, diffLoss = 6.7242, kgLoss = 0.1653
2025-04-09 10:50:08.647719: Training Step 60/115: batchLoss = 3.9458, diffLoss = 6.4714, kgLoss = 0.1574
2025-04-09 10:50:09.396269: Training Step 61/115: batchLoss = 3.8790, diffLoss = 6.3521, kgLoss = 0.1694
2025-04-09 10:50:10.121264: Training Step 62/115: batchLoss = 5.1269, diffLoss = 8.4003, kgLoss = 0.2167
2025-04-09 10:50:10.858164: Training Step 63/115: batchLoss = 3.8859, diffLoss = 6.3637, kgLoss = 0.1692
2025-04-09 10:50:11.590946: Training Step 64/115: batchLoss = 3.8267, diffLoss = 6.2718, kgLoss = 0.1591
2025-04-09 10:50:12.331039: Training Step 65/115: batchLoss = 3.8744, diffLoss = 6.3405, kgLoss = 0.1753
2025-04-09 10:50:13.069223: Training Step 66/115: batchLoss = 4.3242, diffLoss = 7.0863, kgLoss = 0.1811
2025-04-09 10:50:13.813316: Training Step 67/115: batchLoss = 4.2047, diffLoss = 6.8964, kgLoss = 0.1672
2025-04-09 10:50:14.557309: Training Step 68/115: batchLoss = 4.0257, diffLoss = 6.6010, kgLoss = 0.1628
2025-04-09 10:50:15.311168: Training Step 69/115: batchLoss = 4.6929, diffLoss = 7.6941, kgLoss = 0.1911
2025-04-09 10:50:16.057789: Training Step 70/115: batchLoss = 4.3932, diffLoss = 7.1969, kgLoss = 0.1878
2025-04-09 10:50:16.812143: Training Step 71/115: batchLoss = 4.1384, diffLoss = 6.7780, kgLoss = 0.1790
2025-04-09 10:50:17.554821: Training Step 72/115: batchLoss = 3.6898, diffLoss = 6.0453, kgLoss = 0.1564
2025-04-09 10:50:18.289629: Training Step 73/115: batchLoss = 4.0015, diffLoss = 6.5542, kgLoss = 0.1725
2025-04-09 10:50:19.036181: Training Step 74/115: batchLoss = 4.2213, diffLoss = 6.9195, kgLoss = 0.1741
2025-04-09 10:50:19.776927: Training Step 75/115: batchLoss = 4.0047, diffLoss = 6.5666, kgLoss = 0.1617
2025-04-09 10:50:20.512935: Training Step 76/115: batchLoss = 3.9883, diffLoss = 6.5431, kgLoss = 0.1562
2025-04-09 10:50:21.252559: Training Step 77/115: batchLoss = 4.2589, diffLoss = 6.9865, kgLoss = 0.1676
2025-04-09 10:50:21.996949: Training Step 78/115: batchLoss = 3.7695, diffLoss = 6.1739, kgLoss = 0.1630
2025-04-09 10:50:22.731055: Training Step 79/115: batchLoss = 4.0178, diffLoss = 6.5777, kgLoss = 0.1778
2025-04-09 10:50:23.470412: Training Step 80/115: batchLoss = 4.1932, diffLoss = 6.8793, kgLoss = 0.1642
2025-04-09 10:50:24.238954: Training Step 81/115: batchLoss = 3.9519, diffLoss = 6.4733, kgLoss = 0.1697
2025-04-09 10:50:24.981034: Training Step 82/115: batchLoss = 4.2530, diffLoss = 6.9743, kgLoss = 0.1711
2025-04-09 10:50:25.716302: Training Step 83/115: batchLoss = 4.2437, diffLoss = 6.9589, kgLoss = 0.1709
2025-04-09 10:50:26.454047: Training Step 84/115: batchLoss = 4.4502, diffLoss = 7.2960, kgLoss = 0.1814
2025-04-09 10:50:27.191815: Training Step 85/115: batchLoss = 4.2039, diffLoss = 6.8884, kgLoss = 0.1771
2025-04-09 10:50:27.937187: Training Step 86/115: batchLoss = 4.0331, diffLoss = 6.6079, kgLoss = 0.1708
2025-04-09 10:50:28.686952: Training Step 87/115: batchLoss = 4.2126, diffLoss = 6.9105, kgLoss = 0.1659
2025-04-09 10:50:29.425236: Training Step 88/115: batchLoss = 4.4804, diffLoss = 7.3442, kgLoss = 0.1846
2025-04-09 10:50:30.165819: Training Step 89/115: batchLoss = 4.2691, diffLoss = 6.9987, kgLoss = 0.1746
2025-04-09 10:50:30.907600: Training Step 90/115: batchLoss = 4.0154, diffLoss = 6.5852, kgLoss = 0.1608
2025-04-09 10:50:31.666157: Training Step 91/115: batchLoss = 4.2389, diffLoss = 6.9534, kgLoss = 0.1672
2025-04-09 10:50:32.411447: Training Step 92/115: batchLoss = 4.0040, diffLoss = 6.5621, kgLoss = 0.1668
2025-04-09 10:50:33.140810: Training Step 93/115: batchLoss = 4.2911, diffLoss = 7.0354, kgLoss = 0.1746
2025-04-09 10:50:33.883275: Training Step 94/115: batchLoss = 4.2047, diffLoss = 6.9031, kgLoss = 0.1570
2025-04-09 10:50:34.636030: Training Step 95/115: batchLoss = 4.6067, diffLoss = 7.5555, kgLoss = 0.1835
2025-04-09 10:50:35.388689: Training Step 96/115: batchLoss = 4.4914, diffLoss = 7.3662, kgLoss = 0.1792
2025-04-09 10:50:36.128585: Training Step 97/115: batchLoss = 4.0827, diffLoss = 6.6986, kgLoss = 0.1589
2025-04-09 10:50:36.867423: Training Step 98/115: batchLoss = 4.1845, diffLoss = 6.8604, kgLoss = 0.1707
2025-04-09 10:50:37.603482: Training Step 99/115: batchLoss = 3.9898, diffLoss = 6.5362, kgLoss = 0.1702
2025-04-09 10:50:38.343354: Training Step 100/115: batchLoss = 4.2646, diffLoss = 6.9850, kgLoss = 0.1841
2025-04-09 10:50:39.083985: Training Step 101/115: batchLoss = 3.9439, diffLoss = 6.4615, kgLoss = 0.1674
2025-04-09 10:50:39.812519: Training Step 102/115: batchLoss = 3.8872, diffLoss = 6.3749, kgLoss = 0.1556
2025-04-09 10:50:40.558651: Training Step 103/115: batchLoss = 4.4126, diffLoss = 7.2375, kgLoss = 0.1752
2025-04-09 10:50:41.302217: Training Step 104/115: batchLoss = 4.3196, diffLoss = 7.0864, kgLoss = 0.1695
2025-04-09 10:50:42.049759: Training Step 105/115: batchLoss = 4.3182, diffLoss = 7.0777, kgLoss = 0.1789
2025-04-09 10:50:42.817495: Training Step 106/115: batchLoss = 4.3043, diffLoss = 7.0529, kgLoss = 0.1814
2025-04-09 10:50:43.566756: Training Step 107/115: batchLoss = 3.9697, diffLoss = 6.5072, kgLoss = 0.1633
2025-04-09 10:50:44.315548: Training Step 108/115: batchLoss = 4.2175, diffLoss = 6.9134, kgLoss = 0.1735
2025-04-09 10:50:45.054688: Training Step 109/115: batchLoss = 4.0598, diffLoss = 6.6519, kgLoss = 0.1717
2025-04-09 10:50:45.797122: Training Step 110/115: batchLoss = 3.9398, diffLoss = 6.4544, kgLoss = 0.1679
2025-04-09 10:50:46.541525: Training Step 111/115: batchLoss = 4.1799, diffLoss = 6.8506, kgLoss = 0.1739
2025-04-09 10:50:47.274242: Training Step 112/115: batchLoss = 4.2263, diffLoss = 6.9261, kgLoss = 0.1765
2025-04-09 10:50:47.928002: Training Step 113/115: batchLoss = 4.3409, diffLoss = 7.1199, kgLoss = 0.1723
2025-04-09 10:50:48.571174: Training Step 114/115: batchLoss = 4.2979, diffLoss = 7.0482, kgLoss = 0.1725
2025-04-09 10:50:48.696717: 
2025-04-09 10:50:48.697737: Epoch 38/1000, Train: epLoss = 1.2052, epDfLoss = 1.9754, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 10:50:49.479944: Steps 0/90: batch_recall = 56.54, batch_ndcg = 43.37 
2025-04-09 10:50:50.226278: Steps 1/90: batch_recall = 57.58, batch_ndcg = 40.80 
2025-04-09 10:50:50.978423: Steps 2/90: batch_recall = 50.62, batch_ndcg = 38.08 
2025-04-09 10:50:51.721586: Steps 3/90: batch_recall = 44.89, batch_ndcg = 32.40 
2025-04-09 10:50:52.467071: Steps 4/90: batch_recall = 47.72, batch_ndcg = 35.68 
2025-04-09 10:50:53.208410: Steps 5/90: batch_recall = 37.94, batch_ndcg = 28.27 
2025-04-09 10:50:53.945467: Steps 6/90: batch_recall = 46.87, batch_ndcg = 32.89 
2025-04-09 10:50:54.697592: Steps 7/90: batch_recall = 43.54, batch_ndcg = 31.66 
2025-04-09 10:50:55.439600: Steps 8/90: batch_recall = 47.86, batch_ndcg = 34.53 
2025-04-09 10:50:56.205195: Steps 9/90: batch_recall = 43.91, batch_ndcg = 33.39 
2025-04-09 10:50:56.952862: Steps 10/90: batch_recall = 39.91, batch_ndcg = 29.67 
2025-04-09 10:50:57.697625: Steps 11/90: batch_recall = 43.69, batch_ndcg = 29.38 
2025-04-09 10:50:58.435267: Steps 12/90: batch_recall = 37.29, batch_ndcg = 26.96 
2025-04-09 10:50:59.180698: Steps 13/90: batch_recall = 35.53, batch_ndcg = 26.03 
2025-04-09 10:50:59.929775: Steps 14/90: batch_recall = 35.56, batch_ndcg = 24.98 
2025-04-09 10:51:00.668479: Steps 15/90: batch_recall = 45.26, batch_ndcg = 29.93 
2025-04-09 10:51:01.427831: Steps 16/90: batch_recall = 35.83, batch_ndcg = 25.82 
2025-04-09 10:51:02.182293: Steps 17/90: batch_recall = 32.30, batch_ndcg = 22.21 
2025-04-09 10:51:02.917911: Steps 18/90: batch_recall = 40.68, batch_ndcg = 27.46 
2025-04-09 10:51:03.679203: Steps 19/90: batch_recall = 33.47, batch_ndcg = 23.75 
2025-04-09 10:51:04.420981: Steps 20/90: batch_recall = 41.03, batch_ndcg = 26.95 
2025-04-09 10:51:05.161358: Steps 21/90: batch_recall = 42.36, batch_ndcg = 30.63 
2025-04-09 10:51:05.886247: Steps 22/90: batch_recall = 38.95, batch_ndcg = 28.03 
2025-04-09 10:51:06.609969: Steps 23/90: batch_recall = 40.77, batch_ndcg = 27.16 
2025-04-09 10:51:07.324121: Steps 24/90: batch_recall = 37.40, batch_ndcg = 27.16 
2025-04-09 10:51:08.037361: Steps 25/90: batch_recall = 38.17, batch_ndcg = 25.45 
2025-04-09 10:51:08.752373: Steps 26/90: batch_recall = 37.78, batch_ndcg = 27.61 
2025-04-09 10:51:09.454961: Steps 27/90: batch_recall = 34.26, batch_ndcg = 23.70 
2025-04-09 10:51:10.187213: Steps 28/90: batch_recall = 39.01, batch_ndcg = 26.31 
2025-04-09 10:51:10.915171: Steps 29/90: batch_recall = 34.11, batch_ndcg = 24.03 
2025-04-09 10:51:11.645759: Steps 30/90: batch_recall = 33.75, batch_ndcg = 22.89 
2025-04-09 10:51:12.409949: Steps 31/90: batch_recall = 33.48, batch_ndcg = 24.09 
2025-04-09 10:51:13.158951: Steps 32/90: batch_recall = 35.45, batch_ndcg = 23.98 
2025-04-09 10:51:13.904332: Steps 33/90: batch_recall = 35.44, batch_ndcg = 22.66 
2025-04-09 10:51:14.623258: Steps 34/90: batch_recall = 35.30, batch_ndcg = 24.23 
2025-04-09 10:51:15.365118: Steps 35/90: batch_recall = 39.14, batch_ndcg = 27.76 
2025-04-09 10:51:16.100226: Steps 36/90: batch_recall = 36.78, batch_ndcg = 25.01 
2025-04-09 10:51:16.835639: Steps 37/90: batch_recall = 32.84, batch_ndcg = 21.40 
2025-04-09 10:51:17.580606: Steps 38/90: batch_recall = 36.58, batch_ndcg = 23.93 
2025-04-09 10:51:18.300595: Steps 39/90: batch_recall = 38.02, batch_ndcg = 24.14 
2025-04-09 10:51:19.026487: Steps 40/90: batch_recall = 31.76, batch_ndcg = 22.36 
2025-04-09 10:51:19.739960: Steps 41/90: batch_recall = 34.70, batch_ndcg = 21.75 
2025-04-09 10:51:20.445105: Steps 42/90: batch_recall = 44.43, batch_ndcg = 29.50 
2025-04-09 10:51:21.160402: Steps 43/90: batch_recall = 35.15, batch_ndcg = 22.46 
2025-04-09 10:51:21.874350: Steps 44/90: batch_recall = 33.36, batch_ndcg = 22.94 
2025-04-09 10:51:22.570907: Steps 45/90: batch_recall = 32.84, batch_ndcg = 22.42 
2025-04-09 10:51:23.290810: Steps 46/90: batch_recall = 34.83, batch_ndcg = 22.85 
2025-04-09 10:51:24.008136: Steps 47/90: batch_recall = 33.11, batch_ndcg = 20.76 
2025-04-09 10:51:24.725877: Steps 48/90: batch_recall = 36.23, batch_ndcg = 23.64 
2025-04-09 10:51:25.436783: Steps 49/90: batch_recall = 31.69, batch_ndcg = 19.55 
2025-04-09 10:51:26.143894: Steps 50/90: batch_recall = 41.07, batch_ndcg = 26.67 
2025-04-09 10:51:26.848975: Steps 51/90: batch_recall = 39.02, batch_ndcg = 26.09 
2025-04-09 10:51:27.553151: Steps 52/90: batch_recall = 38.23, batch_ndcg = 25.08 
2025-04-09 10:51:28.270616: Steps 53/90: batch_recall = 42.05, batch_ndcg = 26.97 
2025-04-09 10:51:28.991342: Steps 54/90: batch_recall = 30.59, batch_ndcg = 20.34 
2025-04-09 10:51:29.727663: Steps 55/90: batch_recall = 35.65, batch_ndcg = 23.16 
2025-04-09 10:51:30.476944: Steps 56/90: batch_recall = 33.19, batch_ndcg = 19.79 
2025-04-09 10:51:31.203958: Steps 57/90: batch_recall = 35.62, batch_ndcg = 24.28 
2025-04-09 10:51:31.913644: Steps 58/90: batch_recall = 39.32, batch_ndcg = 23.87 
2025-04-09 10:51:32.627202: Steps 59/90: batch_recall = 36.06, batch_ndcg = 23.72 
2025-04-09 10:51:33.354581: Steps 60/90: batch_recall = 34.91, batch_ndcg = 22.53 
2025-04-09 10:51:34.062418: Steps 61/90: batch_recall = 40.22, batch_ndcg = 27.67 
2025-04-09 10:51:34.771821: Steps 62/90: batch_recall = 38.55, batch_ndcg = 24.97 
2025-04-09 10:51:35.468709: Steps 63/90: batch_recall = 33.09, batch_ndcg = 22.36 
2025-04-09 10:51:36.170224: Steps 64/90: batch_recall = 35.78, batch_ndcg = 23.32 
2025-04-09 10:51:36.862004: Steps 65/90: batch_recall = 33.78, batch_ndcg = 22.39 
2025-04-09 10:51:37.558218: Steps 66/90: batch_recall = 36.45, batch_ndcg = 22.59 
2025-04-09 10:51:38.280221: Steps 67/90: batch_recall = 32.18, batch_ndcg = 20.66 
2025-04-09 10:51:38.988502: Steps 68/90: batch_recall = 36.88, batch_ndcg = 22.60 
2025-04-09 10:51:39.697633: Steps 69/90: batch_recall = 39.94, batch_ndcg = 24.19 
2025-04-09 10:51:40.413559: Steps 70/90: batch_recall = 38.99, batch_ndcg = 25.13 
2025-04-09 10:51:41.126916: Steps 71/90: batch_recall = 40.42, batch_ndcg = 24.72 
2025-04-09 10:51:41.834839: Steps 72/90: batch_recall = 35.05, batch_ndcg = 21.91 
2025-04-09 10:51:42.557064: Steps 73/90: batch_recall = 34.64, batch_ndcg = 21.81 
2025-04-09 10:51:43.267272: Steps 74/90: batch_recall = 39.97, batch_ndcg = 25.47 
2025-04-09 10:51:43.988962: Steps 75/90: batch_recall = 37.35, batch_ndcg = 23.26 
2025-04-09 10:51:44.701105: Steps 76/90: batch_recall = 40.87, batch_ndcg = 25.98 
2025-04-09 10:51:45.410217: Steps 77/90: batch_recall = 44.08, batch_ndcg = 27.28 
2025-04-09 10:51:46.119847: Steps 78/90: batch_recall = 34.02, batch_ndcg = 20.92 
2025-04-09 10:51:46.841375: Steps 79/90: batch_recall = 39.00, batch_ndcg = 25.87 
2025-04-09 10:51:47.543905: Steps 80/90: batch_recall = 45.53, batch_ndcg = 29.66 
2025-04-09 10:51:48.253048: Steps 81/90: batch_recall = 45.73, batch_ndcg = 28.61 
2025-04-09 10:51:48.948007: Steps 82/90: batch_recall = 45.72, batch_ndcg = 28.42 
2025-04-09 10:51:49.645425: Steps 83/90: batch_recall = 45.07, batch_ndcg = 27.24 
2025-04-09 10:51:50.351237: Steps 84/90: batch_recall = 36.06, batch_ndcg = 22.60 
2025-04-09 10:51:51.056060: Steps 85/90: batch_recall = 41.24, batch_ndcg = 24.48 
2025-04-09 10:51:51.771280: Steps 86/90: batch_recall = 43.66, batch_ndcg = 29.26 
2025-04-09 10:51:52.468839: Steps 87/90: batch_recall = 43.16, batch_ndcg = 28.93 
2025-04-09 10:51:53.163151: Steps 88/90: batch_recall = 49.51, batch_ndcg = 29.73 
2025-04-09 10:51:53.709531: Steps 89/90: batch_recall = 41.12, batch_ndcg = 25.94 
2025-04-09 10:51:53.710244: Epoch 38/1000, Test: Recall = 0.0762, NDCG = 0.0510  

2025-04-09 10:51:54.730785: Training Step 0/115: batchLoss = 3.9008, diffLoss = 6.3897, kgLoss = 0.1674
2025-04-09 10:51:55.462885: Training Step 1/115: batchLoss = 3.9380, diffLoss = 6.4559, kgLoss = 0.1612
2025-04-09 10:51:56.197761: Training Step 2/115: batchLoss = 4.2101, diffLoss = 6.8957, kgLoss = 0.1815
2025-04-09 10:51:56.938561: Training Step 3/115: batchLoss = 3.7800, diffLoss = 6.1978, kgLoss = 0.1533
2025-04-09 10:51:57.669568: Training Step 4/115: batchLoss = 4.0910, diffLoss = 6.7016, kgLoss = 0.1752
2025-04-09 10:51:58.403595: Training Step 5/115: batchLoss = 4.3481, diffLoss = 7.1299, kgLoss = 0.1755
2025-04-09 10:51:59.127681: Training Step 6/115: batchLoss = 4.3752, diffLoss = 7.1738, kgLoss = 0.1772
2025-04-09 10:51:59.848526: Training Step 7/115: batchLoss = 4.3272, diffLoss = 7.0975, kgLoss = 0.1716
2025-04-09 10:52:00.580822: Training Step 8/115: batchLoss = 4.4506, diffLoss = 7.2990, kgLoss = 0.1779
2025-04-09 10:52:01.310258: Training Step 9/115: batchLoss = 3.8930, diffLoss = 6.3822, kgLoss = 0.1591
2025-04-09 10:52:02.048936: Training Step 10/115: batchLoss = 4.0702, diffLoss = 6.6730, kgLoss = 0.1659
2025-04-09 10:52:02.795749: Training Step 11/115: batchLoss = 4.1110, diffLoss = 6.7346, kgLoss = 0.1757
2025-04-09 10:52:03.541723: Training Step 12/115: batchLoss = 4.1265, diffLoss = 6.7550, kgLoss = 0.1838
2025-04-09 10:52:04.286997: Training Step 13/115: batchLoss = 4.1381, diffLoss = 6.7851, kgLoss = 0.1677
2025-04-09 10:52:05.019060: Training Step 14/115: batchLoss = 4.1586, diffLoss = 6.8118, kgLoss = 0.1789
2025-04-09 10:52:05.742935: Training Step 15/115: batchLoss = 4.1333, diffLoss = 6.7732, kgLoss = 0.1733
2025-04-09 10:52:06.480068: Training Step 16/115: batchLoss = 5.0330, diffLoss = 8.2561, kgLoss = 0.1985
2025-04-09 10:52:07.210980: Training Step 17/115: batchLoss = 3.9599, diffLoss = 6.4925, kgLoss = 0.1610
2025-04-09 10:52:07.937218: Training Step 18/115: batchLoss = 4.3763, diffLoss = 7.1696, kgLoss = 0.1862
2025-04-09 10:52:08.665764: Training Step 19/115: batchLoss = 4.1500, diffLoss = 6.8013, kgLoss = 0.1730
2025-04-09 10:52:09.396337: Training Step 20/115: batchLoss = 3.8046, diffLoss = 6.2355, kgLoss = 0.1582
2025-04-09 10:52:10.141563: Training Step 21/115: batchLoss = 3.9024, diffLoss = 6.3954, kgLoss = 0.1628
2025-04-09 10:52:10.893270: Training Step 22/115: batchLoss = 4.1735, diffLoss = 6.8457, kgLoss = 0.1652
2025-04-09 10:52:11.637603: Training Step 23/115: batchLoss = 4.0946, diffLoss = 6.7028, kgLoss = 0.1822
2025-04-09 10:52:12.386457: Training Step 24/115: batchLoss = 5.2257, diffLoss = 8.5583, kgLoss = 0.2268
2025-04-09 10:52:13.123705: Training Step 25/115: batchLoss = 4.7235, diffLoss = 7.7489, kgLoss = 0.1854
2025-04-09 10:52:13.860361: Training Step 26/115: batchLoss = 4.5045, diffLoss = 7.3859, kgLoss = 0.1824
2025-04-09 10:52:14.605555: Training Step 27/115: batchLoss = 4.1187, diffLoss = 6.7447, kgLoss = 0.1796
2025-04-09 10:52:15.360306: Training Step 28/115: batchLoss = 4.4166, diffLoss = 7.2368, kgLoss = 0.1863
2025-04-09 10:52:16.100661: Training Step 29/115: batchLoss = 3.8441, diffLoss = 6.2962, kgLoss = 0.1660
2025-04-09 10:52:16.836659: Training Step 30/115: batchLoss = 4.6297, diffLoss = 7.5866, kgLoss = 0.1943
2025-04-09 10:52:17.595979: Training Step 31/115: batchLoss = 4.3834, diffLoss = 7.1874, kgLoss = 0.1774
2025-04-09 10:52:18.332080: Training Step 32/115: batchLoss = 3.8071, diffLoss = 6.2327, kgLoss = 0.1687
2025-04-09 10:52:19.067344: Training Step 33/115: batchLoss = 3.8117, diffLoss = 6.2402, kgLoss = 0.1688
2025-04-09 10:52:19.803280: Training Step 34/115: batchLoss = 3.9436, diffLoss = 6.4593, kgLoss = 0.1701
2025-04-09 10:52:20.541998: Training Step 35/115: batchLoss = 3.7655, diffLoss = 6.1659, kgLoss = 0.1649
2025-04-09 10:52:21.279053: Training Step 36/115: batchLoss = 4.3769, diffLoss = 7.1823, kgLoss = 0.1688
2025-04-09 10:52:22.014885: Training Step 37/115: batchLoss = 4.6538, diffLoss = 7.6316, kgLoss = 0.1871
2025-04-09 10:52:22.750662: Training Step 38/115: batchLoss = 4.4724, diffLoss = 7.3372, kgLoss = 0.1753
2025-04-09 10:52:23.497485: Training Step 39/115: batchLoss = 3.7283, diffLoss = 6.1061, kgLoss = 0.1616
2025-04-09 10:52:24.242960: Training Step 40/115: batchLoss = 4.4501, diffLoss = 7.2977, kgLoss = 0.1786
2025-04-09 10:52:24.986312: Training Step 41/115: batchLoss = 4.0780, diffLoss = 6.6833, kgLoss = 0.1701
2025-04-09 10:52:25.723630: Training Step 42/115: batchLoss = 3.8306, diffLoss = 6.2736, kgLoss = 0.1661
2025-04-09 10:52:26.463703: Training Step 43/115: batchLoss = 4.1429, diffLoss = 6.7951, kgLoss = 0.1647
2025-04-09 10:52:27.203473: Training Step 44/115: batchLoss = 4.0819, diffLoss = 6.6943, kgLoss = 0.1632
2025-04-09 10:52:27.943450: Training Step 45/115: batchLoss = 4.4065, diffLoss = 7.2279, kgLoss = 0.1744
2025-04-09 10:52:28.683402: Training Step 46/115: batchLoss = 4.2560, diffLoss = 6.9721, kgLoss = 0.1818
2025-04-09 10:52:29.437204: Training Step 47/115: batchLoss = 4.4457, diffLoss = 7.2903, kgLoss = 0.1787
2025-04-09 10:52:30.185248: Training Step 48/115: batchLoss = 4.1996, diffLoss = 6.8871, kgLoss = 0.1683
2025-04-09 10:52:30.931284: Training Step 49/115: batchLoss = 4.5345, diffLoss = 7.4321, kgLoss = 0.1882
2025-04-09 10:52:31.669213: Training Step 50/115: batchLoss = 4.2724, diffLoss = 7.0017, kgLoss = 0.1784
2025-04-09 10:52:32.411908: Training Step 51/115: batchLoss = 4.5482, diffLoss = 7.4532, kgLoss = 0.1907
2025-04-09 10:52:33.170902: Training Step 52/115: batchLoss = 4.0237, diffLoss = 6.5949, kgLoss = 0.1670
2025-04-09 10:52:33.923773: Training Step 53/115: batchLoss = 4.4770, diffLoss = 7.3391, kgLoss = 0.1839
2025-04-09 10:52:34.659053: Training Step 54/115: batchLoss = 3.7997, diffLoss = 6.2255, kgLoss = 0.1610
2025-04-09 10:52:35.394970: Training Step 55/115: batchLoss = 4.4301, diffLoss = 7.2652, kgLoss = 0.1774
2025-04-09 10:52:36.145985: Training Step 56/115: batchLoss = 3.8241, diffLoss = 6.2627, kgLoss = 0.1661
2025-04-09 10:52:36.892835: Training Step 57/115: batchLoss = 4.0939, diffLoss = 6.7128, kgLoss = 0.1655
2025-04-09 10:52:37.638624: Training Step 58/115: batchLoss = 4.2887, diffLoss = 7.0322, kgLoss = 0.1734
2025-04-09 10:52:38.381731: Training Step 59/115: batchLoss = 4.2387, diffLoss = 6.9532, kgLoss = 0.1668
2025-04-09 10:52:39.122985: Training Step 60/115: batchLoss = 3.7327, diffLoss = 6.1169, kgLoss = 0.1564
2025-04-09 10:52:39.890735: Training Step 61/115: batchLoss = 3.8637, diffLoss = 6.3334, kgLoss = 0.1592
2025-04-09 10:52:40.632963: Training Step 62/115: batchLoss = 3.6216, diffLoss = 5.9344, kgLoss = 0.1524
2025-04-09 10:52:41.364167: Training Step 63/115: batchLoss = 4.8082, diffLoss = 7.8881, kgLoss = 0.1884
2025-04-09 10:52:42.120925: Training Step 64/115: batchLoss = 4.3833, diffLoss = 7.1857, kgLoss = 0.1798
2025-04-09 10:52:42.859439: Training Step 65/115: batchLoss = 3.6560, diffLoss = 5.9930, kgLoss = 0.1506
2025-04-09 10:52:43.591918: Training Step 66/115: batchLoss = 4.2083, diffLoss = 6.8985, kgLoss = 0.1728
2025-04-09 10:52:44.319084: Training Step 67/115: batchLoss = 4.1557, diffLoss = 6.8132, kgLoss = 0.1694
2025-04-09 10:52:45.054681: Training Step 68/115: batchLoss = 3.7500, diffLoss = 6.1498, kgLoss = 0.1504
2025-04-09 10:52:45.794446: Training Step 69/115: batchLoss = 3.5630, diffLoss = 5.8326, kgLoss = 0.1586
2025-04-09 10:52:46.543136: Training Step 70/115: batchLoss = 4.1171, diffLoss = 6.7468, kgLoss = 0.1725
2025-04-09 10:52:47.285748: Training Step 71/115: batchLoss = 4.1492, diffLoss = 6.7910, kgLoss = 0.1867
2025-04-09 10:52:48.024441: Training Step 72/115: batchLoss = 4.5315, diffLoss = 7.4361, kgLoss = 0.1747
2025-04-09 10:52:48.750184: Training Step 73/115: batchLoss = 4.0226, diffLoss = 6.5849, kgLoss = 0.1792
2025-04-09 10:52:49.475480: Training Step 74/115: batchLoss = 4.4171, diffLoss = 7.2428, kgLoss = 0.1786
2025-04-09 10:52:50.209113: Training Step 75/115: batchLoss = 4.3682, diffLoss = 7.1626, kgLoss = 0.1767
2025-04-09 10:52:50.957134: Training Step 76/115: batchLoss = 4.8488, diffLoss = 7.9447, kgLoss = 0.2049
2025-04-09 10:52:51.712517: Training Step 77/115: batchLoss = 3.8977, diffLoss = 6.3871, kgLoss = 0.1635
2025-04-09 10:52:52.462274: Training Step 78/115: batchLoss = 4.1963, diffLoss = 6.8840, kgLoss = 0.1648
2025-04-09 10:52:53.198236: Training Step 79/115: batchLoss = 4.2138, diffLoss = 6.9092, kgLoss = 0.1706
2025-04-09 10:52:53.939426: Training Step 80/115: batchLoss = 4.1127, diffLoss = 6.7346, kgLoss = 0.1798
2025-04-09 10:52:54.677379: Training Step 81/115: batchLoss = 4.4914, diffLoss = 7.3634, kgLoss = 0.1834
2025-04-09 10:52:55.410720: Training Step 82/115: batchLoss = 3.7184, diffLoss = 6.0866, kgLoss = 0.1662
2025-04-09 10:52:56.151540: Training Step 83/115: batchLoss = 3.6467, diffLoss = 5.9729, kgLoss = 0.1574
2025-04-09 10:52:56.895526: Training Step 84/115: batchLoss = 4.0002, diffLoss = 6.5582, kgLoss = 0.1633
2025-04-09 10:52:57.629735: Training Step 85/115: batchLoss = 3.9663, diffLoss = 6.5008, kgLoss = 0.1644
2025-04-09 10:52:58.376391: Training Step 86/115: batchLoss = 4.6234, diffLoss = 7.5783, kgLoss = 0.1911
2025-04-09 10:52:59.131917: Training Step 87/115: batchLoss = 3.8439, diffLoss = 6.3027, kgLoss = 0.1556
2025-04-09 10:52:59.888916: Training Step 88/115: batchLoss = 4.1974, diffLoss = 6.8809, kgLoss = 0.1721
2025-04-09 10:53:00.630316: Training Step 89/115: batchLoss = 4.2594, diffLoss = 6.9814, kgLoss = 0.1763
2025-04-09 10:53:01.377531: Training Step 90/115: batchLoss = 4.2313, diffLoss = 6.9348, kgLoss = 0.1762
2025-04-09 10:53:02.117398: Training Step 91/115: batchLoss = 4.0388, diffLoss = 6.6166, kgLoss = 0.1721
2025-04-09 10:53:02.852336: Training Step 92/115: batchLoss = 4.2419, diffLoss = 6.9485, kgLoss = 0.1820
2025-04-09 10:53:03.589417: Training Step 93/115: batchLoss = 4.3259, diffLoss = 7.0879, kgLoss = 0.1828
2025-04-09 10:53:04.338580: Training Step 94/115: batchLoss = 3.7977, diffLoss = 6.2263, kgLoss = 0.1546
2025-04-09 10:53:05.064243: Training Step 95/115: batchLoss = 4.6638, diffLoss = 7.6475, kgLoss = 0.1881
2025-04-09 10:53:05.795709: Training Step 96/115: batchLoss = 4.6634, diffLoss = 7.6417, kgLoss = 0.1960
2025-04-09 10:53:06.526816: Training Step 97/115: batchLoss = 4.2979, diffLoss = 7.0374, kgLoss = 0.1885
2025-04-09 10:53:07.254514: Training Step 98/115: batchLoss = 3.8946, diffLoss = 6.3854, kgLoss = 0.1583
2025-04-09 10:53:07.985864: Training Step 99/115: batchLoss = 4.0512, diffLoss = 6.6437, kgLoss = 0.1625
2025-04-09 10:53:08.727666: Training Step 100/115: batchLoss = 4.1192, diffLoss = 6.7535, kgLoss = 0.1677
2025-04-09 10:53:09.470728: Training Step 101/115: batchLoss = 4.6918, diffLoss = 7.6939, kgLoss = 0.1886
2025-04-09 10:53:10.219026: Training Step 102/115: batchLoss = 4.0583, diffLoss = 6.6474, kgLoss = 0.1746
2025-04-09 10:53:10.971383: Training Step 103/115: batchLoss = 4.0190, diffLoss = 6.5904, kgLoss = 0.1620
2025-04-09 10:53:11.711763: Training Step 104/115: batchLoss = 4.2498, diffLoss = 6.9615, kgLoss = 0.1822
2025-04-09 10:53:12.453180: Training Step 105/115: batchLoss = 4.4994, diffLoss = 7.3777, kgLoss = 0.1820
2025-04-09 10:53:13.200465: Training Step 106/115: batchLoss = 4.3734, diffLoss = 7.1668, kgLoss = 0.1833
2025-04-09 10:53:13.935821: Training Step 107/115: batchLoss = 4.0096, diffLoss = 6.5666, kgLoss = 0.1741
2025-04-09 10:53:14.676512: Training Step 108/115: batchLoss = 4.0940, diffLoss = 6.7080, kgLoss = 0.1729
2025-04-09 10:53:15.423069: Training Step 109/115: batchLoss = 4.2779, diffLoss = 7.0114, kgLoss = 0.1776
2025-04-09 10:53:16.160696: Training Step 110/115: batchLoss = 4.4491, diffLoss = 7.2968, kgLoss = 0.1775
2025-04-09 10:53:16.933955: Training Step 111/115: batchLoss = 4.2813, diffLoss = 7.0171, kgLoss = 0.1775
2025-04-09 10:53:17.681725: Training Step 112/115: batchLoss = 4.1789, diffLoss = 6.8516, kgLoss = 0.1699
2025-04-09 10:53:18.350296: Training Step 113/115: batchLoss = 4.2139, diffLoss = 6.9067, kgLoss = 0.1747
2025-04-09 10:53:18.997687: Training Step 114/115: batchLoss = 3.8462, diffLoss = 6.2998, kgLoss = 0.1657
2025-04-09 10:53:19.139135: 
2025-04-09 10:53:19.141206: Epoch 39/1000, Train: epLoss = 1.2033, epDfLoss = 1.9722, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 10:53:19.897852: Steps 0/90: batch_recall = 56.48, batch_ndcg = 43.24 
2025-04-09 10:53:20.638363: Steps 1/90: batch_recall = 57.90, batch_ndcg = 41.10 
2025-04-09 10:53:21.417927: Steps 2/90: batch_recall = 50.90, batch_ndcg = 38.37 
2025-04-09 10:53:22.163678: Steps 3/90: batch_recall = 44.38, batch_ndcg = 32.25 
2025-04-09 10:53:22.904798: Steps 4/90: batch_recall = 48.40, batch_ndcg = 35.27 
2025-04-09 10:53:23.642280: Steps 5/90: batch_recall = 38.28, batch_ndcg = 28.45 
2025-04-09 10:53:24.379428: Steps 6/90: batch_recall = 46.50, batch_ndcg = 32.65 
2025-04-09 10:53:25.130640: Steps 7/90: batch_recall = 43.67, batch_ndcg = 31.69 
2025-04-09 10:53:25.874007: Steps 8/90: batch_recall = 47.30, batch_ndcg = 34.33 
2025-04-09 10:53:26.636686: Steps 9/90: batch_recall = 44.45, batch_ndcg = 33.38 
2025-04-09 10:53:27.377463: Steps 10/90: batch_recall = 40.36, batch_ndcg = 29.94 
2025-04-09 10:53:28.108496: Steps 11/90: batch_recall = 42.40, batch_ndcg = 28.83 
2025-04-09 10:53:28.839461: Steps 12/90: batch_recall = 36.60, batch_ndcg = 26.48 
2025-04-09 10:53:29.580664: Steps 13/90: batch_recall = 36.22, batch_ndcg = 26.29 
2025-04-09 10:53:30.325598: Steps 14/90: batch_recall = 35.50, batch_ndcg = 24.81 
2025-04-09 10:53:31.066455: Steps 15/90: batch_recall = 44.76, batch_ndcg = 29.41 
2025-04-09 10:53:31.793530: Steps 16/90: batch_recall = 35.52, batch_ndcg = 25.59 
2025-04-09 10:53:32.520614: Steps 17/90: batch_recall = 32.00, batch_ndcg = 21.80 
2025-04-09 10:53:33.235272: Steps 18/90: batch_recall = 38.73, batch_ndcg = 26.80 
2025-04-09 10:53:33.971910: Steps 19/90: batch_recall = 32.94, batch_ndcg = 23.67 
2025-04-09 10:53:34.694429: Steps 20/90: batch_recall = 40.87, batch_ndcg = 27.17 
2025-04-09 10:53:35.429695: Steps 21/90: batch_recall = 42.86, batch_ndcg = 30.60 
2025-04-09 10:53:36.146536: Steps 22/90: batch_recall = 37.59, batch_ndcg = 27.53 
2025-04-09 10:53:36.870726: Steps 23/90: batch_recall = 39.44, batch_ndcg = 26.89 
2025-04-09 10:53:37.600404: Steps 24/90: batch_recall = 36.45, batch_ndcg = 27.02 
2025-04-09 10:53:38.347334: Steps 25/90: batch_recall = 36.50, batch_ndcg = 24.88 
2025-04-09 10:53:39.076830: Steps 26/90: batch_recall = 37.06, batch_ndcg = 27.29 
2025-04-09 10:53:39.794069: Steps 27/90: batch_recall = 33.54, batch_ndcg = 23.35 
2025-04-09 10:53:40.514334: Steps 28/90: batch_recall = 38.00, batch_ndcg = 25.89 
2025-04-09 10:53:41.238647: Steps 29/90: batch_recall = 32.35, batch_ndcg = 23.37 
2025-04-09 10:53:41.957922: Steps 30/90: batch_recall = 33.69, batch_ndcg = 23.04 
2025-04-09 10:53:42.689998: Steps 31/90: batch_recall = 33.41, batch_ndcg = 23.63 
2025-04-09 10:53:43.402557: Steps 32/90: batch_recall = 34.66, batch_ndcg = 23.98 
2025-04-09 10:53:44.132033: Steps 33/90: batch_recall = 36.41, batch_ndcg = 22.89 
2025-04-09 10:53:44.867046: Steps 34/90: batch_recall = 35.64, batch_ndcg = 24.09 
2025-04-09 10:53:45.575008: Steps 35/90: batch_recall = 37.49, batch_ndcg = 27.17 
2025-04-09 10:53:46.310468: Steps 36/90: batch_recall = 36.84, batch_ndcg = 25.12 
2025-04-09 10:53:47.028335: Steps 37/90: batch_recall = 33.59, batch_ndcg = 21.60 
2025-04-09 10:53:47.746000: Steps 38/90: batch_recall = 37.28, batch_ndcg = 24.50 
2025-04-09 10:53:48.442489: Steps 39/90: batch_recall = 38.31, batch_ndcg = 24.08 
2025-04-09 10:53:49.164940: Steps 40/90: batch_recall = 31.17, batch_ndcg = 22.04 
2025-04-09 10:53:49.880514: Steps 41/90: batch_recall = 33.99, batch_ndcg = 21.78 
2025-04-09 10:53:50.597249: Steps 42/90: batch_recall = 44.73, batch_ndcg = 29.69 
2025-04-09 10:53:51.316019: Steps 43/90: batch_recall = 35.39, batch_ndcg = 22.36 
2025-04-09 10:53:52.022356: Steps 44/90: batch_recall = 32.69, batch_ndcg = 22.41 
2025-04-09 10:53:52.733932: Steps 45/90: batch_recall = 31.84, batch_ndcg = 22.15 
2025-04-09 10:53:53.436011: Steps 46/90: batch_recall = 34.20, batch_ndcg = 22.57 
2025-04-09 10:53:54.150179: Steps 47/90: batch_recall = 32.60, batch_ndcg = 20.80 
2025-04-09 10:53:54.870555: Steps 48/90: batch_recall = 36.26, batch_ndcg = 23.91 
2025-04-09 10:53:55.605838: Steps 49/90: batch_recall = 31.07, batch_ndcg = 19.24 
2025-04-09 10:53:56.323217: Steps 50/90: batch_recall = 40.44, batch_ndcg = 26.74 
2025-04-09 10:53:57.047015: Steps 51/90: batch_recall = 39.60, batch_ndcg = 25.98 
2025-04-09 10:53:57.765465: Steps 52/90: batch_recall = 39.67, batch_ndcg = 25.72 
2025-04-09 10:53:58.489604: Steps 53/90: batch_recall = 41.18, batch_ndcg = 26.44 
2025-04-09 10:53:59.199008: Steps 54/90: batch_recall = 30.29, batch_ndcg = 20.49 
2025-04-09 10:53:59.918525: Steps 55/90: batch_recall = 36.31, batch_ndcg = 23.12 
2025-04-09 10:54:00.618982: Steps 56/90: batch_recall = 33.40, batch_ndcg = 20.06 
2025-04-09 10:54:01.338913: Steps 57/90: batch_recall = 35.84, batch_ndcg = 24.32 
2025-04-09 10:54:02.045221: Steps 58/90: batch_recall = 38.42, batch_ndcg = 23.51 
2025-04-09 10:54:02.763103: Steps 59/90: batch_recall = 35.61, batch_ndcg = 23.82 
2025-04-09 10:54:03.471422: Steps 60/90: batch_recall = 34.36, batch_ndcg = 22.46 
2025-04-09 10:54:04.162366: Steps 61/90: batch_recall = 39.46, batch_ndcg = 27.53 
2025-04-09 10:54:04.870508: Steps 62/90: batch_recall = 38.30, batch_ndcg = 25.03 
2025-04-09 10:54:05.582229: Steps 63/90: batch_recall = 33.61, batch_ndcg = 22.77 
2025-04-09 10:54:06.301335: Steps 64/90: batch_recall = 35.93, batch_ndcg = 23.14 
2025-04-09 10:54:06.999129: Steps 65/90: batch_recall = 34.48, batch_ndcg = 22.61 
2025-04-09 10:54:07.702150: Steps 66/90: batch_recall = 36.39, batch_ndcg = 22.63 
2025-04-09 10:54:08.408771: Steps 67/90: batch_recall = 31.35, batch_ndcg = 20.20 
2025-04-09 10:54:09.110195: Steps 68/90: batch_recall = 36.83, batch_ndcg = 22.46 
2025-04-09 10:54:09.817280: Steps 69/90: batch_recall = 40.31, batch_ndcg = 24.79 
2025-04-09 10:54:10.528823: Steps 70/90: batch_recall = 39.33, batch_ndcg = 25.26 
2025-04-09 10:54:11.243492: Steps 71/90: batch_recall = 40.99, batch_ndcg = 24.87 
2025-04-09 10:54:11.949073: Steps 72/90: batch_recall = 34.36, batch_ndcg = 21.66 
2025-04-09 10:54:12.652602: Steps 73/90: batch_recall = 34.07, batch_ndcg = 21.58 
2025-04-09 10:54:13.362387: Steps 74/90: batch_recall = 39.70, batch_ndcg = 25.36 
2025-04-09 10:54:14.061491: Steps 75/90: batch_recall = 38.22, batch_ndcg = 23.47 
2025-04-09 10:54:14.768046: Steps 76/90: batch_recall = 41.47, batch_ndcg = 26.30 
2025-04-09 10:54:15.461047: Steps 77/90: batch_recall = 44.72, batch_ndcg = 27.39 
2025-04-09 10:54:16.175617: Steps 78/90: batch_recall = 35.35, batch_ndcg = 21.55 
2025-04-09 10:54:16.873048: Steps 79/90: batch_recall = 39.32, batch_ndcg = 26.38 
2025-04-09 10:54:17.580744: Steps 80/90: batch_recall = 45.49, batch_ndcg = 29.78 
2025-04-09 10:54:18.298295: Steps 81/90: batch_recall = 45.02, batch_ndcg = 28.65 
2025-04-09 10:54:19.016219: Steps 82/90: batch_recall = 45.35, batch_ndcg = 28.16 
2025-04-09 10:54:19.710370: Steps 83/90: batch_recall = 45.57, batch_ndcg = 27.60 
2025-04-09 10:54:20.407817: Steps 84/90: batch_recall = 36.07, batch_ndcg = 22.81 
2025-04-09 10:54:21.103758: Steps 85/90: batch_recall = 41.70, batch_ndcg = 24.48 
2025-04-09 10:54:21.807738: Steps 86/90: batch_recall = 42.53, batch_ndcg = 29.32 
2025-04-09 10:54:22.503024: Steps 87/90: batch_recall = 45.14, batch_ndcg = 29.54 
2025-04-09 10:54:23.220403: Steps 88/90: batch_recall = 50.60, batch_ndcg = 30.03 
2025-04-09 10:54:23.782359: Steps 89/90: batch_recall = 41.02, batch_ndcg = 26.19 
2025-04-09 10:54:23.783079: Epoch 39/1000, Test: Recall = 0.0759, NDCG = 0.0510  

2025-04-09 10:54:24.817235: Training Step 0/115: batchLoss = 3.8363, diffLoss = 6.2846, kgLoss = 0.1637
2025-04-09 10:54:25.559955: Training Step 1/115: batchLoss = 4.2452, diffLoss = 6.9551, kgLoss = 0.1803
2025-04-09 10:54:26.298136: Training Step 2/115: batchLoss = 4.2448, diffLoss = 6.9554, kgLoss = 0.1789
2025-04-09 10:54:27.036490: Training Step 3/115: batchLoss = 3.8973, diffLoss = 6.3833, kgLoss = 0.1684
2025-04-09 10:54:27.770867: Training Step 4/115: batchLoss = 3.7470, diffLoss = 6.1385, kgLoss = 0.1599
2025-04-09 10:54:28.514479: Training Step 5/115: batchLoss = 4.0571, diffLoss = 6.6473, kgLoss = 0.1717
2025-04-09 10:54:29.254061: Training Step 6/115: batchLoss = 4.0840, diffLoss = 6.6859, kgLoss = 0.1812
2025-04-09 10:54:29.986105: Training Step 7/115: batchLoss = 3.8436, diffLoss = 6.3025, kgLoss = 0.1553
2025-04-09 10:54:30.720029: Training Step 8/115: batchLoss = 4.1173, diffLoss = 6.7443, kgLoss = 0.1769
2025-04-09 10:54:31.457084: Training Step 9/115: batchLoss = 3.9596, diffLoss = 6.4795, kgLoss = 0.1797
2025-04-09 10:54:32.199314: Training Step 10/115: batchLoss = 4.1421, diffLoss = 6.7893, kgLoss = 0.1712
2025-04-09 10:54:32.932545: Training Step 11/115: batchLoss = 4.5150, diffLoss = 7.4019, kgLoss = 0.1846
2025-04-09 10:54:33.657135: Training Step 12/115: batchLoss = 4.2739, diffLoss = 7.0058, kgLoss = 0.1761
2025-04-09 10:54:34.395593: Training Step 13/115: batchLoss = 4.4021, diffLoss = 7.2200, kgLoss = 0.1753
2025-04-09 10:54:35.134561: Training Step 14/115: batchLoss = 4.2629, diffLoss = 6.9874, kgLoss = 0.1760
2025-04-09 10:54:35.858227: Training Step 15/115: batchLoss = 3.7553, diffLoss = 6.1591, kgLoss = 0.1495
2025-04-09 10:54:36.587281: Training Step 16/115: batchLoss = 4.1209, diffLoss = 6.7561, kgLoss = 0.1680
2025-04-09 10:54:37.311880: Training Step 17/115: batchLoss = 4.4529, diffLoss = 7.3075, kgLoss = 0.1710
2025-04-09 10:54:38.052854: Training Step 18/115: batchLoss = 4.2751, diffLoss = 7.0111, kgLoss = 0.1711
2025-04-09 10:54:38.786872: Training Step 19/115: batchLoss = 4.1549, diffLoss = 6.8045, kgLoss = 0.1805
2025-04-09 10:54:39.518634: Training Step 20/115: batchLoss = 4.3771, diffLoss = 7.1830, kgLoss = 0.1681
2025-04-09 10:54:40.249019: Training Step 21/115: batchLoss = 4.0946, diffLoss = 6.7132, kgLoss = 0.1668
2025-04-09 10:54:40.980904: Training Step 22/115: batchLoss = 4.2768, diffLoss = 7.0110, kgLoss = 0.1756
2025-04-09 10:54:41.708735: Training Step 23/115: batchLoss = 3.7685, diffLoss = 6.1743, kgLoss = 0.1596
2025-04-09 10:54:42.437519: Training Step 24/115: batchLoss = 4.0935, diffLoss = 6.7092, kgLoss = 0.1699
2025-04-09 10:54:43.179521: Training Step 25/115: batchLoss = 4.1971, diffLoss = 6.8683, kgLoss = 0.1902
2025-04-09 10:54:43.915748: Training Step 26/115: batchLoss = 3.8856, diffLoss = 6.3665, kgLoss = 0.1643
2025-04-09 10:54:44.664201: Training Step 27/115: batchLoss = 4.4722, diffLoss = 7.3265, kgLoss = 0.1908
2025-04-09 10:54:45.414551: Training Step 28/115: batchLoss = 4.1675, diffLoss = 6.8295, kgLoss = 0.1745
2025-04-09 10:54:46.155647: Training Step 29/115: batchLoss = 3.7375, diffLoss = 6.1279, kgLoss = 0.1519
2025-04-09 10:54:46.887986: Training Step 30/115: batchLoss = 4.8014, diffLoss = 7.8718, kgLoss = 0.1959
2025-04-09 10:54:47.614629: Training Step 31/115: batchLoss = 4.2636, diffLoss = 6.9861, kgLoss = 0.1799
2025-04-09 10:54:48.340052: Training Step 32/115: batchLoss = 4.0483, diffLoss = 6.6325, kgLoss = 0.1721
2025-04-09 10:54:49.072082: Training Step 33/115: batchLoss = 4.5370, diffLoss = 7.4318, kgLoss = 0.1949
2025-04-09 10:54:49.806454: Training Step 34/115: batchLoss = 4.0320, diffLoss = 6.6138, kgLoss = 0.1594
2025-04-09 10:54:50.555791: Training Step 35/115: batchLoss = 4.0077, diffLoss = 6.5729, kgLoss = 0.1600
2025-04-09 10:54:51.284668: Training Step 36/115: batchLoss = 4.2406, diffLoss = 6.9526, kgLoss = 0.1725
2025-04-09 10:54:52.020870: Training Step 37/115: batchLoss = 4.2180, diffLoss = 6.9176, kgLoss = 0.1685
2025-04-09 10:54:52.767604: Training Step 38/115: batchLoss = 4.4499, diffLoss = 7.2921, kgLoss = 0.1866
2025-04-09 10:54:53.504003: Training Step 39/115: batchLoss = 4.3657, diffLoss = 7.1522, kgLoss = 0.1860
2025-04-09 10:54:54.241419: Training Step 40/115: batchLoss = 4.7685, diffLoss = 7.8209, kgLoss = 0.1898
2025-04-09 10:54:54.977156: Training Step 41/115: batchLoss = 3.7311, diffLoss = 6.1124, kgLoss = 0.1591
2025-04-09 10:54:55.715137: Training Step 42/115: batchLoss = 4.7362, diffLoss = 7.7669, kgLoss = 0.1900
2025-04-09 10:54:56.450077: Training Step 43/115: batchLoss = 4.0594, diffLoss = 6.6554, kgLoss = 0.1655
2025-04-09 10:54:57.188010: Training Step 44/115: batchLoss = 4.6852, diffLoss = 7.6828, kgLoss = 0.1890
2025-04-09 10:54:57.918087: Training Step 45/115: batchLoss = 3.8133, diffLoss = 6.2431, kgLoss = 0.1686
2025-04-09 10:54:58.666291: Training Step 46/115: batchLoss = 4.1139, diffLoss = 6.7405, kgLoss = 0.1740
2025-04-09 10:54:59.401758: Training Step 47/115: batchLoss = 3.7317, diffLoss = 6.1159, kgLoss = 0.1553
2025-04-09 10:55:00.138486: Training Step 48/115: batchLoss = 4.0885, diffLoss = 6.7002, kgLoss = 0.1710
2025-04-09 10:55:00.878475: Training Step 49/115: batchLoss = 4.5696, diffLoss = 7.4944, kgLoss = 0.1826
2025-04-09 10:55:01.613687: Training Step 50/115: batchLoss = 3.7105, diffLoss = 6.0811, kgLoss = 0.1545
2025-04-09 10:55:02.350466: Training Step 51/115: batchLoss = 4.7645, diffLoss = 7.8173, kgLoss = 0.1853
2025-04-09 10:55:03.082333: Training Step 52/115: batchLoss = 3.7618, diffLoss = 6.1667, kgLoss = 0.1545
2025-04-09 10:55:03.821723: Training Step 53/115: batchLoss = 4.1151, diffLoss = 6.7439, kgLoss = 0.1720
2025-04-09 10:55:04.555308: Training Step 54/115: batchLoss = 4.7644, diffLoss = 7.8119, kgLoss = 0.1932
2025-04-09 10:55:05.304196: Training Step 55/115: batchLoss = 4.3800, diffLoss = 7.1854, kgLoss = 0.1720
2025-04-09 10:55:06.035964: Training Step 56/115: batchLoss = 4.2415, diffLoss = 6.9513, kgLoss = 0.1767
2025-04-09 10:55:06.772240: Training Step 57/115: batchLoss = 4.2124, diffLoss = 6.9019, kgLoss = 0.1782
2025-04-09 10:55:07.513989: Training Step 58/115: batchLoss = 4.4944, diffLoss = 7.3696, kgLoss = 0.1816
2025-04-09 10:55:08.258025: Training Step 59/115: batchLoss = 3.8998, diffLoss = 6.3788, kgLoss = 0.1812
2025-04-09 10:55:09.005734: Training Step 60/115: batchLoss = 4.3163, diffLoss = 7.0769, kgLoss = 0.1755
2025-04-09 10:55:09.756150: Training Step 61/115: batchLoss = 3.9871, diffLoss = 6.5358, kgLoss = 0.1641
2025-04-09 10:55:10.491878: Training Step 62/115: batchLoss = 4.0362, diffLoss = 6.6136, kgLoss = 0.1700
2025-04-09 10:55:11.340323: Training Step 63/115: batchLoss = 4.4339, diffLoss = 7.2751, kgLoss = 0.1720
2025-04-09 10:55:12.100113: Training Step 64/115: batchLoss = 3.9605, diffLoss = 6.4947, kgLoss = 0.1592
2025-04-09 10:55:12.852515: Training Step 65/115: batchLoss = 4.6248, diffLoss = 7.5786, kgLoss = 0.1942
2025-04-09 10:55:13.586217: Training Step 66/115: batchLoss = 4.7127, diffLoss = 7.7314, kgLoss = 0.1848
2025-04-09 10:55:14.324411: Training Step 67/115: batchLoss = 3.7508, diffLoss = 6.1249, kgLoss = 0.1897
2025-04-09 10:55:15.061973: Training Step 68/115: batchLoss = 4.3639, diffLoss = 7.1567, kgLoss = 0.1747
2025-04-09 10:55:15.811795: Training Step 69/115: batchLoss = 4.4457, diffLoss = 7.2892, kgLoss = 0.1804
2025-04-09 10:55:16.550857: Training Step 70/115: batchLoss = 4.1715, diffLoss = 6.8379, kgLoss = 0.1719
2025-04-09 10:55:17.281800: Training Step 71/115: batchLoss = 4.4284, diffLoss = 7.2567, kgLoss = 0.1858
2025-04-09 10:55:18.012089: Training Step 72/115: batchLoss = 3.6949, diffLoss = 6.0564, kgLoss = 0.1528
2025-04-09 10:55:18.754344: Training Step 73/115: batchLoss = 4.0869, diffLoss = 6.6844, kgLoss = 0.1907
2025-04-09 10:55:19.487408: Training Step 74/115: batchLoss = 4.2164, diffLoss = 6.9087, kgLoss = 0.1780
2025-04-09 10:55:20.226587: Training Step 75/115: batchLoss = 4.6287, diffLoss = 7.5924, kgLoss = 0.1832
2025-04-09 10:55:20.960522: Training Step 76/115: batchLoss = 4.1294, diffLoss = 6.7623, kgLoss = 0.1799
2025-04-09 10:55:21.688422: Training Step 77/115: batchLoss = 3.5926, diffLoss = 5.8861, kgLoss = 0.1524
2025-04-09 10:55:22.425015: Training Step 78/115: batchLoss = 3.9553, diffLoss = 6.4814, kgLoss = 0.1662
2025-04-09 10:55:23.168251: Training Step 79/115: batchLoss = 4.3185, diffLoss = 7.0802, kgLoss = 0.1760
2025-04-09 10:55:23.916300: Training Step 80/115: batchLoss = 4.0653, diffLoss = 6.6593, kgLoss = 0.1743
2025-04-09 10:55:24.673922: Training Step 81/115: batchLoss = 4.5408, diffLoss = 7.4461, kgLoss = 0.1829
2025-04-09 10:55:25.427423: Training Step 82/115: batchLoss = 3.9439, diffLoss = 6.4646, kgLoss = 0.1628
2025-04-09 10:55:26.163309: Training Step 83/115: batchLoss = 4.4978, diffLoss = 7.3747, kgLoss = 0.1824
2025-04-09 10:55:26.902582: Training Step 84/115: batchLoss = 4.1361, diffLoss = 6.7794, kgLoss = 0.1711
2025-04-09 10:55:27.640724: Training Step 85/115: batchLoss = 4.3743, diffLoss = 7.1733, kgLoss = 0.1758
2025-04-09 10:55:28.377166: Training Step 86/115: batchLoss = 3.8875, diffLoss = 6.3705, kgLoss = 0.1629
2025-04-09 10:55:29.105923: Training Step 87/115: batchLoss = 4.2574, diffLoss = 6.9752, kgLoss = 0.1806
2025-04-09 10:55:29.840029: Training Step 88/115: batchLoss = 4.0422, diffLoss = 6.6237, kgLoss = 0.1700
2025-04-09 10:55:30.587608: Training Step 89/115: batchLoss = 3.8568, diffLoss = 6.3159, kgLoss = 0.1682
2025-04-09 10:55:31.328484: Training Step 90/115: batchLoss = 4.0684, diffLoss = 6.6633, kgLoss = 0.1760
2025-04-09 10:55:32.147171: Training Step 91/115: batchLoss = 4.7621, diffLoss = 7.7993, kgLoss = 0.2063
2025-04-09 10:55:32.895461: Training Step 92/115: batchLoss = 4.0816, diffLoss = 6.6883, kgLoss = 0.1715
2025-04-09 10:55:33.623574: Training Step 93/115: batchLoss = 3.6604, diffLoss = 5.9899, kgLoss = 0.1661
2025-04-09 10:55:34.372326: Training Step 94/115: batchLoss = 4.0319, diffLoss = 6.6059, kgLoss = 0.1708
2025-04-09 10:55:35.119933: Training Step 95/115: batchLoss = 3.9762, diffLoss = 6.5163, kgLoss = 0.1660
2025-04-09 10:55:35.841624: Training Step 96/115: batchLoss = 3.9797, diffLoss = 6.5217, kgLoss = 0.1668
2025-04-09 10:55:36.574110: Training Step 97/115: batchLoss = 4.1997, diffLoss = 6.8813, kgLoss = 0.1773
2025-04-09 10:55:37.312745: Training Step 98/115: batchLoss = 4.1806, diffLoss = 6.8469, kgLoss = 0.1810
2025-04-09 10:55:38.054602: Training Step 99/115: batchLoss = 3.9918, diffLoss = 6.5412, kgLoss = 0.1678
2025-04-09 10:55:38.793492: Training Step 100/115: batchLoss = 4.2731, diffLoss = 7.0005, kgLoss = 0.1820
2025-04-09 10:55:39.534175: Training Step 101/115: batchLoss = 4.2580, diffLoss = 6.9801, kgLoss = 0.1749
2025-04-09 10:55:40.268946: Training Step 102/115: batchLoss = 4.3420, diffLoss = 7.1138, kgLoss = 0.1842
2025-04-09 10:55:41.013435: Training Step 103/115: batchLoss = 4.0234, diffLoss = 6.5960, kgLoss = 0.1646
2025-04-09 10:55:41.782997: Training Step 104/115: batchLoss = 3.7599, diffLoss = 6.1564, kgLoss = 0.1652
2025-04-09 10:55:42.527827: Training Step 105/115: batchLoss = 4.0356, diffLoss = 6.6147, kgLoss = 0.1669
2025-04-09 10:55:43.271371: Training Step 106/115: batchLoss = 3.9534, diffLoss = 6.4799, kgLoss = 0.1638
2025-04-09 10:55:44.006774: Training Step 107/115: batchLoss = 3.5794, diffLoss = 5.8584, kgLoss = 0.1610
2025-04-09 10:55:44.742300: Training Step 108/115: batchLoss = 4.4908, diffLoss = 7.3692, kgLoss = 0.1732
2025-04-09 10:55:45.482884: Training Step 109/115: batchLoss = 4.2781, diffLoss = 7.0107, kgLoss = 0.1791
2025-04-09 10:55:46.226556: Training Step 110/115: batchLoss = 3.7414, diffLoss = 6.1304, kgLoss = 0.1578
2025-04-09 10:55:46.963008: Training Step 111/115: batchLoss = 3.9081, diffLoss = 6.4044, kgLoss = 0.1638
2025-04-09 10:55:47.694572: Training Step 112/115: batchLoss = 4.4899, diffLoss = 7.3644, kgLoss = 0.1782
2025-04-09 10:55:48.358104: Training Step 113/115: batchLoss = 4.0779, diffLoss = 6.6822, kgLoss = 0.1716
2025-04-09 10:55:48.974796: Training Step 114/115: batchLoss = 4.4169, diffLoss = 7.2384, kgLoss = 0.1846
2025-04-09 10:55:49.096845: 
2025-04-09 10:55:49.098137: Epoch 40/1000, Train: epLoss = 1.1977, epDfLoss = 1.9629, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 10:55:49.872815: Steps 0/90: batch_recall = 56.39, batch_ndcg = 43.40 
2025-04-09 10:55:50.626075: Steps 1/90: batch_recall = 57.96, batch_ndcg = 40.87 
2025-04-09 10:55:51.375352: Steps 2/90: batch_recall = 50.72, batch_ndcg = 38.29 
2025-04-09 10:55:52.137381: Steps 3/90: batch_recall = 44.21, batch_ndcg = 32.07 
2025-04-09 10:55:52.893104: Steps 4/90: batch_recall = 48.89, batch_ndcg = 35.58 
2025-04-09 10:55:53.637291: Steps 5/90: batch_recall = 38.52, batch_ndcg = 28.45 
2025-04-09 10:55:54.361924: Steps 6/90: batch_recall = 45.85, batch_ndcg = 32.40 
2025-04-09 10:55:55.107268: Steps 7/90: batch_recall = 43.79, batch_ndcg = 31.86 
2025-04-09 10:55:55.838627: Steps 8/90: batch_recall = 47.63, batch_ndcg = 34.61 
2025-04-09 10:55:56.591946: Steps 9/90: batch_recall = 44.21, batch_ndcg = 33.51 
2025-04-09 10:55:57.333281: Steps 10/90: batch_recall = 40.03, batch_ndcg = 29.71 
2025-04-09 10:55:58.056520: Steps 11/90: batch_recall = 42.08, batch_ndcg = 28.83 
2025-04-09 10:55:58.785868: Steps 12/90: batch_recall = 36.30, batch_ndcg = 26.32 
2025-04-09 10:55:59.516902: Steps 13/90: batch_recall = 36.49, batch_ndcg = 26.31 
2025-04-09 10:56:00.253730: Steps 14/90: batch_recall = 35.00, batch_ndcg = 24.68 
2025-04-09 10:56:00.982758: Steps 15/90: batch_recall = 45.36, batch_ndcg = 30.26 
2025-04-09 10:56:01.725439: Steps 16/90: batch_recall = 35.51, batch_ndcg = 26.18 
2025-04-09 10:56:02.459031: Steps 17/90: batch_recall = 32.25, batch_ndcg = 22.05 
2025-04-09 10:56:03.175070: Steps 18/90: batch_recall = 39.33, batch_ndcg = 27.07 
2025-04-09 10:56:03.923711: Steps 19/90: batch_recall = 32.56, batch_ndcg = 23.39 
2025-04-09 10:56:04.647228: Steps 20/90: batch_recall = 39.52, batch_ndcg = 26.81 
2025-04-09 10:56:05.381233: Steps 21/90: batch_recall = 42.40, batch_ndcg = 30.54 
2025-04-09 10:56:06.127441: Steps 22/90: batch_recall = 38.65, batch_ndcg = 27.87 
2025-04-09 10:56:06.874426: Steps 23/90: batch_recall = 39.05, batch_ndcg = 26.71 
2025-04-09 10:56:07.599249: Steps 24/90: batch_recall = 37.84, batch_ndcg = 27.62 
2025-04-09 10:56:08.324071: Steps 25/90: batch_recall = 36.32, batch_ndcg = 24.75 
2025-04-09 10:56:09.059408: Steps 26/90: batch_recall = 38.09, batch_ndcg = 27.54 
2025-04-09 10:56:09.766767: Steps 27/90: batch_recall = 34.43, batch_ndcg = 23.59 
2025-04-09 10:56:10.495882: Steps 28/90: batch_recall = 38.05, batch_ndcg = 25.87 
2025-04-09 10:56:11.224340: Steps 29/90: batch_recall = 32.96, batch_ndcg = 22.98 
2025-04-09 10:56:11.941264: Steps 30/90: batch_recall = 34.52, batch_ndcg = 23.37 
2025-04-09 10:56:12.667327: Steps 31/90: batch_recall = 32.24, batch_ndcg = 23.53 
2025-04-09 10:56:13.387457: Steps 32/90: batch_recall = 35.24, batch_ndcg = 23.94 
2025-04-09 10:56:14.098470: Steps 33/90: batch_recall = 36.87, batch_ndcg = 23.00 
2025-04-09 10:56:14.819687: Steps 34/90: batch_recall = 35.93, batch_ndcg = 24.54 
2025-04-09 10:56:15.524285: Steps 35/90: batch_recall = 37.75, batch_ndcg = 27.18 
2025-04-09 10:56:16.233292: Steps 36/90: batch_recall = 36.90, batch_ndcg = 25.45 
2025-04-09 10:56:16.942508: Steps 37/90: batch_recall = 32.38, batch_ndcg = 21.39 
2025-04-09 10:56:17.664888: Steps 38/90: batch_recall = 38.07, batch_ndcg = 24.29 
2025-04-09 10:56:18.394178: Steps 39/90: batch_recall = 37.61, batch_ndcg = 23.56 
2025-04-09 10:56:19.113857: Steps 40/90: batch_recall = 31.60, batch_ndcg = 22.44 
2025-04-09 10:56:19.821804: Steps 41/90: batch_recall = 33.80, batch_ndcg = 21.65 
2025-04-09 10:56:20.528221: Steps 42/90: batch_recall = 44.39, batch_ndcg = 29.44 
2025-04-09 10:56:21.237600: Steps 43/90: batch_recall = 36.06, batch_ndcg = 22.60 
2025-04-09 10:56:21.957668: Steps 44/90: batch_recall = 33.49, batch_ndcg = 23.14 
2025-04-09 10:56:22.703824: Steps 45/90: batch_recall = 33.71, batch_ndcg = 22.80 
2025-04-09 10:56:23.419639: Steps 46/90: batch_recall = 34.39, batch_ndcg = 22.55 
2025-04-09 10:56:24.131242: Steps 47/90: batch_recall = 32.76, batch_ndcg = 20.44 
2025-04-09 10:56:24.836503: Steps 48/90: batch_recall = 36.65, batch_ndcg = 23.99 
2025-04-09 10:56:25.540746: Steps 49/90: batch_recall = 31.16, batch_ndcg = 19.12 
2025-04-09 10:56:26.242582: Steps 50/90: batch_recall = 40.33, batch_ndcg = 26.32 
2025-04-09 10:56:26.948870: Steps 51/90: batch_recall = 39.32, batch_ndcg = 25.86 
2025-04-09 10:56:27.663011: Steps 52/90: batch_recall = 38.73, batch_ndcg = 25.40 
2025-04-09 10:56:28.373795: Steps 53/90: batch_recall = 41.03, batch_ndcg = 26.37 
2025-04-09 10:56:29.071367: Steps 54/90: batch_recall = 29.71, batch_ndcg = 20.61 
2025-04-09 10:56:29.790647: Steps 55/90: batch_recall = 35.76, batch_ndcg = 23.17 
2025-04-09 10:56:30.522110: Steps 56/90: batch_recall = 34.31, batch_ndcg = 20.26 
2025-04-09 10:56:31.251376: Steps 57/90: batch_recall = 35.93, batch_ndcg = 24.31 
2025-04-09 10:56:31.958767: Steps 58/90: batch_recall = 38.21, batch_ndcg = 23.61 
2025-04-09 10:56:32.664357: Steps 59/90: batch_recall = 35.43, batch_ndcg = 23.31 
2025-04-09 10:56:33.361453: Steps 60/90: batch_recall = 34.69, batch_ndcg = 22.60 
2025-04-09 10:56:34.066589: Steps 61/90: batch_recall = 39.62, batch_ndcg = 28.05 
2025-04-09 10:56:34.776985: Steps 62/90: batch_recall = 39.35, batch_ndcg = 25.52 
2025-04-09 10:56:35.482702: Steps 63/90: batch_recall = 33.58, batch_ndcg = 23.02 
2025-04-09 10:56:36.217520: Steps 64/90: batch_recall = 36.13, batch_ndcg = 23.24 
2025-04-09 10:56:36.929996: Steps 65/90: batch_recall = 35.07, batch_ndcg = 22.75 
2025-04-09 10:56:37.650016: Steps 66/90: batch_recall = 36.59, batch_ndcg = 22.40 
2025-04-09 10:56:38.354804: Steps 67/90: batch_recall = 32.55, batch_ndcg = 20.79 
2025-04-09 10:56:39.071952: Steps 68/90: batch_recall = 36.92, batch_ndcg = 22.59 
2025-04-09 10:56:39.781455: Steps 69/90: batch_recall = 40.76, batch_ndcg = 24.61 
2025-04-09 10:56:40.505077: Steps 70/90: batch_recall = 39.56, batch_ndcg = 25.48 
2025-04-09 10:56:41.207679: Steps 71/90: batch_recall = 39.86, batch_ndcg = 24.62 
2025-04-09 10:56:41.916001: Steps 72/90: batch_recall = 35.83, batch_ndcg = 21.84 
2025-04-09 10:56:42.630239: Steps 73/90: batch_recall = 34.03, batch_ndcg = 21.44 
2025-04-09 10:56:43.353080: Steps 74/90: batch_recall = 40.99, batch_ndcg = 25.98 
2025-04-09 10:56:44.057237: Steps 75/90: batch_recall = 38.59, batch_ndcg = 23.54 
2025-04-09 10:56:44.759498: Steps 76/90: batch_recall = 40.91, batch_ndcg = 25.86 
2025-04-09 10:56:45.455841: Steps 77/90: batch_recall = 44.10, batch_ndcg = 27.71 
2025-04-09 10:56:46.161340: Steps 78/90: batch_recall = 34.05, batch_ndcg = 21.15 
2025-04-09 10:56:46.857643: Steps 79/90: batch_recall = 39.65, batch_ndcg = 26.20 
2025-04-09 10:56:47.567641: Steps 80/90: batch_recall = 45.49, batch_ndcg = 29.60 
2025-04-09 10:56:48.275671: Steps 81/90: batch_recall = 44.84, batch_ndcg = 28.87 
2025-04-09 10:56:48.979605: Steps 82/90: batch_recall = 44.70, batch_ndcg = 27.88 
2025-04-09 10:56:49.667327: Steps 83/90: batch_recall = 44.70, batch_ndcg = 27.29 
2025-04-09 10:56:50.375344: Steps 84/90: batch_recall = 35.91, batch_ndcg = 22.52 
2025-04-09 10:56:51.072313: Steps 85/90: batch_recall = 41.74, batch_ndcg = 24.55 
2025-04-09 10:56:51.780410: Steps 86/90: batch_recall = 43.71, batch_ndcg = 29.62 
2025-04-09 10:56:52.483694: Steps 87/90: batch_recall = 45.80, batch_ndcg = 29.86 
2025-04-09 10:56:53.174642: Steps 88/90: batch_recall = 51.12, batch_ndcg = 30.12 
2025-04-09 10:56:53.724904: Steps 89/90: batch_recall = 40.70, batch_ndcg = 26.34 
2025-04-09 10:56:53.725626: Epoch 40/1000, Test: Recall = 0.0761, NDCG = 0.0510  

2025-04-09 10:56:54.758163: Training Step 0/115: batchLoss = 4.5503, diffLoss = 7.4584, kgLoss = 0.1880
2025-04-09 10:56:55.484671: Training Step 1/115: batchLoss = 4.5465, diffLoss = 7.4525, kgLoss = 0.1874
2025-04-09 10:56:56.207706: Training Step 2/115: batchLoss = 4.6376, diffLoss = 7.6023, kgLoss = 0.1906
2025-04-09 10:56:56.930731: Training Step 3/115: batchLoss = 4.0265, diffLoss = 6.6030, kgLoss = 0.1619
2025-04-09 10:56:57.658332: Training Step 4/115: batchLoss = 3.7032, diffLoss = 6.0598, kgLoss = 0.1682
2025-04-09 10:56:58.382475: Training Step 5/115: batchLoss = 3.9374, diffLoss = 6.4463, kgLoss = 0.1741
2025-04-09 10:56:59.105999: Training Step 6/115: batchLoss = 4.0658, diffLoss = 6.6686, kgLoss = 0.1615
2025-04-09 10:56:59.839950: Training Step 7/115: batchLoss = 4.4816, diffLoss = 7.3491, kgLoss = 0.1804
2025-04-09 10:57:00.562187: Training Step 8/115: batchLoss = 3.7179, diffLoss = 6.0902, kgLoss = 0.1594
2025-04-09 10:57:01.284657: Training Step 9/115: batchLoss = 3.9478, diffLoss = 6.4710, kgLoss = 0.1630
2025-04-09 10:57:02.004370: Training Step 10/115: batchLoss = 4.2446, diffLoss = 6.9599, kgLoss = 0.1716
2025-04-09 10:57:02.715910: Training Step 11/115: batchLoss = 4.1692, diffLoss = 6.8347, kgLoss = 0.1710
2025-04-09 10:57:03.440740: Training Step 12/115: batchLoss = 4.0320, diffLoss = 6.6108, kgLoss = 0.1638
2025-04-09 10:57:04.157165: Training Step 13/115: batchLoss = 4.1438, diffLoss = 6.7926, kgLoss = 0.1707
2025-04-09 10:57:04.879219: Training Step 14/115: batchLoss = 4.3774, diffLoss = 7.1807, kgLoss = 0.1725
2025-04-09 10:57:05.604303: Training Step 15/115: batchLoss = 4.2016, diffLoss = 6.8835, kgLoss = 0.1788
2025-04-09 10:57:06.339801: Training Step 16/115: batchLoss = 4.0474, diffLoss = 6.6391, kgLoss = 0.1600
2025-04-09 10:57:07.066565: Training Step 17/115: batchLoss = 4.2969, diffLoss = 7.0441, kgLoss = 0.1761
2025-04-09 10:57:07.792298: Training Step 18/115: batchLoss = 4.3026, diffLoss = 7.0550, kgLoss = 0.1741
2025-04-09 10:57:08.529463: Training Step 19/115: batchLoss = 4.3694, diffLoss = 7.1630, kgLoss = 0.1791
2025-04-09 10:57:09.260588: Training Step 20/115: batchLoss = 4.1607, diffLoss = 6.8024, kgLoss = 0.1982
2025-04-09 10:57:09.994151: Training Step 21/115: batchLoss = 3.8015, diffLoss = 6.2338, kgLoss = 0.1530
2025-04-09 10:57:10.717416: Training Step 22/115: batchLoss = 4.4398, diffLoss = 7.2773, kgLoss = 0.1834
2025-04-09 10:57:11.437338: Training Step 23/115: batchLoss = 4.1393, diffLoss = 6.7812, kgLoss = 0.1765
2025-04-09 10:57:12.171964: Training Step 24/115: batchLoss = 4.0847, diffLoss = 6.6916, kgLoss = 0.1743
2025-04-09 10:57:12.912469: Training Step 25/115: batchLoss = 4.2525, diffLoss = 6.9600, kgLoss = 0.1913
2025-04-09 10:57:13.639084: Training Step 26/115: batchLoss = 4.4181, diffLoss = 7.2427, kgLoss = 0.1813
2025-04-09 10:57:14.362005: Training Step 27/115: batchLoss = 4.2989, diffLoss = 7.0496, kgLoss = 0.1729
2025-04-09 10:57:15.087432: Training Step 28/115: batchLoss = 3.9870, diffLoss = 6.5281, kgLoss = 0.1754
2025-04-09 10:57:15.803964: Training Step 29/115: batchLoss = 4.2431, diffLoss = 6.9593, kgLoss = 0.1689
2025-04-09 10:57:16.523317: Training Step 30/115: batchLoss = 3.8801, diffLoss = 6.3577, kgLoss = 0.1635
2025-04-09 10:57:17.253008: Training Step 31/115: batchLoss = 4.1302, diffLoss = 6.7685, kgLoss = 0.1726
2025-04-09 10:57:17.968559: Training Step 32/115: batchLoss = 3.6929, diffLoss = 6.0460, kgLoss = 0.1634
2025-04-09 10:57:18.707436: Training Step 33/115: batchLoss = 4.2228, diffLoss = 6.9216, kgLoss = 0.1746
2025-04-09 10:57:19.443556: Training Step 34/115: batchLoss = 4.2045, diffLoss = 6.8958, kgLoss = 0.1675
2025-04-09 10:57:20.180874: Training Step 35/115: batchLoss = 4.4886, diffLoss = 7.3599, kgLoss = 0.1816
2025-04-09 10:57:20.920193: Training Step 36/115: batchLoss = 4.2903, diffLoss = 7.0195, kgLoss = 0.1964
2025-04-09 10:57:21.652784: Training Step 37/115: batchLoss = 4.1517, diffLoss = 6.8095, kgLoss = 0.1649
2025-04-09 10:57:22.386607: Training Step 38/115: batchLoss = 4.1376, diffLoss = 6.7791, kgLoss = 0.1754
2025-04-09 10:57:23.132903: Training Step 39/115: batchLoss = 4.2415, diffLoss = 6.9514, kgLoss = 0.1766
2025-04-09 10:57:23.871266: Training Step 40/115: batchLoss = 3.7436, diffLoss = 6.1309, kgLoss = 0.1628
2025-04-09 10:57:24.619904: Training Step 41/115: batchLoss = 4.2989, diffLoss = 7.0464, kgLoss = 0.1777
2025-04-09 10:57:25.351755: Training Step 42/115: batchLoss = 4.2733, diffLoss = 7.0041, kgLoss = 0.1772
2025-04-09 10:57:26.083943: Training Step 43/115: batchLoss = 3.9528, diffLoss = 6.4705, kgLoss = 0.1763
2025-04-09 10:57:26.820084: Training Step 44/115: batchLoss = 3.9572, diffLoss = 6.4808, kgLoss = 0.1718
2025-04-09 10:57:27.574758: Training Step 45/115: batchLoss = 3.7899, diffLoss = 6.2142, kgLoss = 0.1535
2025-04-09 10:57:28.324534: Training Step 46/115: batchLoss = 3.7453, diffLoss = 6.1393, kgLoss = 0.1543
2025-04-09 10:57:29.072425: Training Step 47/115: batchLoss = 4.0168, diffLoss = 6.5776, kgLoss = 0.1756
2025-04-09 10:57:29.817833: Training Step 48/115: batchLoss = 4.1823, diffLoss = 6.8531, kgLoss = 0.1761
2025-04-09 10:57:30.549855: Training Step 49/115: batchLoss = 4.2769, diffLoss = 7.0110, kgLoss = 0.1758
2025-04-09 10:57:31.277678: Training Step 50/115: batchLoss = 4.0999, diffLoss = 6.7131, kgLoss = 0.1801
2025-04-09 10:57:32.005503: Training Step 51/115: batchLoss = 4.0503, diffLoss = 6.6424, kgLoss = 0.1622
2025-04-09 10:57:32.736652: Training Step 52/115: batchLoss = 3.9972, diffLoss = 6.5404, kgLoss = 0.1823
2025-04-09 10:57:33.470238: Training Step 53/115: batchLoss = 4.0997, diffLoss = 6.7210, kgLoss = 0.1678
2025-04-09 10:57:34.202955: Training Step 54/115: batchLoss = 4.2738, diffLoss = 7.0063, kgLoss = 0.1751
2025-04-09 10:57:34.941601: Training Step 55/115: batchLoss = 3.9394, diffLoss = 6.4550, kgLoss = 0.1660
2025-04-09 10:57:35.682863: Training Step 56/115: batchLoss = 3.7437, diffLoss = 6.1321, kgLoss = 0.1611
2025-04-09 10:57:36.427043: Training Step 57/115: batchLoss = 4.1231, diffLoss = 6.7592, kgLoss = 0.1691
2025-04-09 10:57:37.176773: Training Step 58/115: batchLoss = 4.5277, diffLoss = 7.4238, kgLoss = 0.1836
2025-04-09 10:57:37.911379: Training Step 59/115: batchLoss = 4.2376, diffLoss = 6.9464, kgLoss = 0.1744
2025-04-09 10:57:38.647823: Training Step 60/115: batchLoss = 3.9237, diffLoss = 6.4315, kgLoss = 0.1620
2025-04-09 10:57:39.384750: Training Step 61/115: batchLoss = 4.0097, diffLoss = 6.5735, kgLoss = 0.1639
2025-04-09 10:57:40.125618: Training Step 62/115: batchLoss = 4.1014, diffLoss = 6.7186, kgLoss = 0.1757
2025-04-09 10:57:40.871238: Training Step 63/115: batchLoss = 3.9875, diffLoss = 6.5397, kgLoss = 0.1592
2025-04-09 10:57:41.613652: Training Step 64/115: batchLoss = 3.9679, diffLoss = 6.5005, kgLoss = 0.1689
2025-04-09 10:57:42.341993: Training Step 65/115: batchLoss = 4.2130, diffLoss = 6.9023, kgLoss = 0.1790
2025-04-09 10:57:43.072105: Training Step 66/115: batchLoss = 4.6060, diffLoss = 7.5510, kgLoss = 0.1886
2025-04-09 10:57:43.810371: Training Step 67/115: batchLoss = 4.0127, diffLoss = 6.5743, kgLoss = 0.1704
2025-04-09 10:57:44.548742: Training Step 68/115: batchLoss = 4.0303, diffLoss = 6.6072, kgLoss = 0.1651
2025-04-09 10:57:45.292606: Training Step 69/115: batchLoss = 4.8152, diffLoss = 7.8943, kgLoss = 0.1965
2025-04-09 10:57:46.036068: Training Step 70/115: batchLoss = 4.5590, diffLoss = 7.4747, kgLoss = 0.1854
2025-04-09 10:57:46.792487: Training Step 71/115: batchLoss = 4.1910, diffLoss = 6.8697, kgLoss = 0.1731
2025-04-09 10:57:47.539290: Training Step 72/115: batchLoss = 3.8452, diffLoss = 6.2966, kgLoss = 0.1680
2025-04-09 10:57:48.284061: Training Step 73/115: batchLoss = 4.0990, diffLoss = 6.7211, kgLoss = 0.1659
2025-04-09 10:57:49.020199: Training Step 74/115: batchLoss = 3.7491, diffLoss = 6.1361, kgLoss = 0.1684
2025-04-09 10:57:49.751687: Training Step 75/115: batchLoss = 4.0180, diffLoss = 6.5857, kgLoss = 0.1663
2025-04-09 10:57:50.496640: Training Step 76/115: batchLoss = 4.4008, diffLoss = 7.2099, kgLoss = 0.1870
2025-04-09 10:57:51.237706: Training Step 77/115: batchLoss = 4.4092, diffLoss = 7.2284, kgLoss = 0.1803
2025-04-09 10:57:51.971963: Training Step 78/115: batchLoss = 3.8141, diffLoss = 6.2476, kgLoss = 0.1638
2025-04-09 10:57:52.714885: Training Step 79/115: batchLoss = 4.2058, diffLoss = 6.8951, kgLoss = 0.1719
2025-04-09 10:57:53.465462: Training Step 80/115: batchLoss = 4.6813, diffLoss = 7.6748, kgLoss = 0.1912
2025-04-09 10:57:54.206101: Training Step 81/115: batchLoss = 4.3310, diffLoss = 7.0983, kgLoss = 0.1801
2025-04-09 10:57:54.956350: Training Step 82/115: batchLoss = 4.3938, diffLoss = 7.1982, kgLoss = 0.1873
2025-04-09 10:57:55.720348: Training Step 83/115: batchLoss = 4.6541, diffLoss = 7.6326, kgLoss = 0.1865
2025-04-09 10:57:56.473873: Training Step 84/115: batchLoss = 3.9400, diffLoss = 6.4550, kgLoss = 0.1673
2025-04-09 10:57:57.219639: Training Step 85/115: batchLoss = 3.7729, diffLoss = 6.1773, kgLoss = 0.1662
2025-04-09 10:57:57.956152: Training Step 86/115: batchLoss = 4.1743, diffLoss = 6.8451, kgLoss = 0.1680
2025-04-09 10:57:58.687189: Training Step 87/115: batchLoss = 3.9944, diffLoss = 6.5364, kgLoss = 0.1812
2025-04-09 10:57:59.434890: Training Step 88/115: batchLoss = 4.0996, diffLoss = 6.7216, kgLoss = 0.1667
2025-04-09 10:58:00.183589: Training Step 89/115: batchLoss = 4.2180, diffLoss = 6.9083, kgLoss = 0.1826
2025-04-09 10:58:00.917064: Training Step 90/115: batchLoss = 4.1761, diffLoss = 6.8408, kgLoss = 0.1789
2025-04-09 10:58:01.653974: Training Step 91/115: batchLoss = 4.2777, diffLoss = 7.0071, kgLoss = 0.1835
2025-04-09 10:58:02.389808: Training Step 92/115: batchLoss = 4.6028, diffLoss = 7.5491, kgLoss = 0.1834
2025-04-09 10:58:03.115242: Training Step 93/115: batchLoss = 4.3223, diffLoss = 7.0797, kgLoss = 0.1861
2025-04-09 10:58:03.856143: Training Step 94/115: batchLoss = 4.2968, diffLoss = 7.0474, kgLoss = 0.1710
2025-04-09 10:58:04.588997: Training Step 95/115: batchLoss = 4.0403, diffLoss = 6.6203, kgLoss = 0.1702
2025-04-09 10:58:05.327115: Training Step 96/115: batchLoss = 4.2275, diffLoss = 6.9268, kgLoss = 0.1785
2025-04-09 10:58:06.071848: Training Step 97/115: batchLoss = 4.0252, diffLoss = 6.5945, kgLoss = 0.1714
2025-04-09 10:58:06.829151: Training Step 98/115: batchLoss = 4.0208, diffLoss = 6.5917, kgLoss = 0.1646
2025-04-09 10:58:07.588583: Training Step 99/115: batchLoss = 3.8466, diffLoss = 6.3042, kgLoss = 0.1602
2025-04-09 10:58:08.335803: Training Step 100/115: batchLoss = 3.6536, diffLoss = 5.9833, kgLoss = 0.1591
2025-04-09 10:58:09.074965: Training Step 101/115: batchLoss = 3.9116, diffLoss = 6.4053, kgLoss = 0.1711
2025-04-09 10:58:09.810248: Training Step 102/115: batchLoss = 3.9976, diffLoss = 6.5497, kgLoss = 0.1695
2025-04-09 10:58:10.563513: Training Step 103/115: batchLoss = 4.0290, diffLoss = 6.6043, kgLoss = 0.1661
2025-04-09 10:58:11.305507: Training Step 104/115: batchLoss = 4.4133, diffLoss = 7.2316, kgLoss = 0.1859
2025-04-09 10:58:12.056088: Training Step 105/115: batchLoss = 4.5391, diffLoss = 7.4456, kgLoss = 0.1794
2025-04-09 10:58:12.788222: Training Step 106/115: batchLoss = 4.5336, diffLoss = 7.4235, kgLoss = 0.1988
2025-04-09 10:58:13.539203: Training Step 107/115: batchLoss = 4.4891, diffLoss = 7.3602, kgLoss = 0.1824
2025-04-09 10:58:14.299538: Training Step 108/115: batchLoss = 4.1496, diffLoss = 6.7978, kgLoss = 0.1774
2025-04-09 10:58:15.043054: Training Step 109/115: batchLoss = 4.0394, diffLoss = 6.6225, kgLoss = 0.1649
2025-04-09 10:58:15.782327: Training Step 110/115: batchLoss = 4.1909, diffLoss = 6.8727, kgLoss = 0.1681
2025-04-09 10:58:16.510135: Training Step 111/115: batchLoss = 3.8985, diffLoss = 6.3797, kgLoss = 0.1766
2025-04-09 10:58:17.260112: Training Step 112/115: batchLoss = 3.9057, diffLoss = 6.3965, kgLoss = 0.1696
2025-04-09 10:58:17.897275: Training Step 113/115: batchLoss = 3.4903, diffLoss = 5.7136, kgLoss = 0.1553
2025-04-09 10:58:18.543726: Training Step 114/115: batchLoss = 4.3017, diffLoss = 7.0537, kgLoss = 0.1738
2025-04-09 10:58:18.672655: 
2025-04-09 10:58:18.674133: Epoch 41/1000, Train: epLoss = 1.1925, epDfLoss = 1.9542, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 10:58:19.459801: Steps 0/90: batch_recall = 56.13, batch_ndcg = 43.32 
2025-04-09 10:58:20.198527: Steps 1/90: batch_recall = 57.47, batch_ndcg = 40.80 
2025-04-09 10:58:20.955736: Steps 2/90: batch_recall = 50.44, batch_ndcg = 37.76 
2025-04-09 10:58:21.713270: Steps 3/90: batch_recall = 44.68, batch_ndcg = 32.38 
2025-04-09 10:58:22.477931: Steps 4/90: batch_recall = 48.53, batch_ndcg = 35.08 
2025-04-09 10:58:23.243558: Steps 5/90: batch_recall = 38.05, batch_ndcg = 28.33 
2025-04-09 10:58:23.997141: Steps 6/90: batch_recall = 47.19, batch_ndcg = 32.85 
2025-04-09 10:58:24.748549: Steps 7/90: batch_recall = 43.87, batch_ndcg = 31.35 
2025-04-09 10:58:25.499857: Steps 8/90: batch_recall = 48.01, batch_ndcg = 34.99 
2025-04-09 10:58:26.256817: Steps 9/90: batch_recall = 43.81, batch_ndcg = 33.45 
2025-04-09 10:58:26.999920: Steps 10/90: batch_recall = 39.75, batch_ndcg = 29.76 
2025-04-09 10:58:27.743285: Steps 11/90: batch_recall = 41.65, batch_ndcg = 28.73 
2025-04-09 10:58:28.480148: Steps 12/90: batch_recall = 35.50, batch_ndcg = 26.19 
2025-04-09 10:58:29.213295: Steps 13/90: batch_recall = 36.12, batch_ndcg = 26.27 
2025-04-09 10:58:29.949528: Steps 14/90: batch_recall = 34.55, batch_ndcg = 24.19 
2025-04-09 10:58:30.674813: Steps 15/90: batch_recall = 45.22, batch_ndcg = 30.22 
2025-04-09 10:58:31.392414: Steps 16/90: batch_recall = 35.10, batch_ndcg = 25.96 
2025-04-09 10:58:32.119157: Steps 17/90: batch_recall = 32.20, batch_ndcg = 21.69 
2025-04-09 10:58:32.834842: Steps 18/90: batch_recall = 38.95, batch_ndcg = 26.69 
2025-04-09 10:58:33.564613: Steps 19/90: batch_recall = 32.34, batch_ndcg = 23.27 
2025-04-09 10:58:34.296220: Steps 20/90: batch_recall = 40.49, batch_ndcg = 27.31 
2025-04-09 10:58:35.042140: Steps 21/90: batch_recall = 42.45, batch_ndcg = 30.06 
2025-04-09 10:58:35.777426: Steps 22/90: batch_recall = 38.25, batch_ndcg = 27.63 
2025-04-09 10:58:36.516760: Steps 23/90: batch_recall = 39.95, batch_ndcg = 26.92 
2025-04-09 10:58:37.254783: Steps 24/90: batch_recall = 38.18, batch_ndcg = 27.91 
2025-04-09 10:58:37.992928: Steps 25/90: batch_recall = 37.06, batch_ndcg = 25.18 
2025-04-09 10:58:38.720929: Steps 26/90: batch_recall = 37.53, batch_ndcg = 27.27 
2025-04-09 10:58:39.438979: Steps 27/90: batch_recall = 34.05, batch_ndcg = 23.53 
2025-04-09 10:58:40.162897: Steps 28/90: batch_recall = 38.40, batch_ndcg = 26.03 
2025-04-09 10:58:40.888110: Steps 29/90: batch_recall = 33.18, batch_ndcg = 23.17 
2025-04-09 10:58:41.603833: Steps 30/90: batch_recall = 33.24, batch_ndcg = 23.12 
2025-04-09 10:58:42.330243: Steps 31/90: batch_recall = 32.02, batch_ndcg = 23.41 
2025-04-09 10:58:43.043080: Steps 32/90: batch_recall = 35.63, batch_ndcg = 23.81 
2025-04-09 10:58:43.761961: Steps 33/90: batch_recall = 35.33, batch_ndcg = 22.31 
2025-04-09 10:58:44.495006: Steps 34/90: batch_recall = 35.69, batch_ndcg = 24.29 
2025-04-09 10:58:45.212060: Steps 35/90: batch_recall = 37.78, batch_ndcg = 27.00 
2025-04-09 10:58:45.938491: Steps 36/90: batch_recall = 37.07, batch_ndcg = 25.50 
2025-04-09 10:58:46.645301: Steps 37/90: batch_recall = 31.93, batch_ndcg = 21.05 
2025-04-09 10:58:47.359436: Steps 38/90: batch_recall = 37.44, batch_ndcg = 24.63 
2025-04-09 10:58:48.079658: Steps 39/90: batch_recall = 36.49, batch_ndcg = 23.30 
2025-04-09 10:58:48.785622: Steps 40/90: batch_recall = 31.58, batch_ndcg = 22.18 
2025-04-09 10:58:49.498936: Steps 41/90: batch_recall = 34.68, batch_ndcg = 21.91 
2025-04-09 10:58:50.209704: Steps 42/90: batch_recall = 44.56, batch_ndcg = 29.12 
2025-04-09 10:58:50.927691: Steps 43/90: batch_recall = 36.29, batch_ndcg = 22.59 
2025-04-09 10:58:51.638876: Steps 44/90: batch_recall = 33.42, batch_ndcg = 22.77 
2025-04-09 10:58:52.343721: Steps 45/90: batch_recall = 32.85, batch_ndcg = 22.43 
2025-04-09 10:58:53.065581: Steps 46/90: batch_recall = 33.97, batch_ndcg = 22.43 
2025-04-09 10:58:53.790134: Steps 47/90: batch_recall = 32.40, batch_ndcg = 20.09 
2025-04-09 10:58:54.517455: Steps 48/90: batch_recall = 36.93, batch_ndcg = 24.17 
2025-04-09 10:58:55.227543: Steps 49/90: batch_recall = 31.07, batch_ndcg = 19.04 
2025-04-09 10:58:55.939116: Steps 50/90: batch_recall = 41.42, batch_ndcg = 26.87 
2025-04-09 10:58:56.645972: Steps 51/90: batch_recall = 38.55, batch_ndcg = 25.22 
2025-04-09 10:58:57.353968: Steps 52/90: batch_recall = 37.95, batch_ndcg = 24.87 
2025-04-09 10:58:58.076574: Steps 53/90: batch_recall = 41.90, batch_ndcg = 26.99 
2025-04-09 10:58:58.784711: Steps 54/90: batch_recall = 29.82, batch_ndcg = 20.36 
2025-04-09 10:58:59.507919: Steps 55/90: batch_recall = 34.95, batch_ndcg = 22.79 
2025-04-09 10:59:00.231154: Steps 56/90: batch_recall = 33.38, batch_ndcg = 20.04 
2025-04-09 10:59:00.949861: Steps 57/90: batch_recall = 37.45, batch_ndcg = 24.62 
2025-04-09 10:59:01.651334: Steps 58/90: batch_recall = 38.10, batch_ndcg = 23.23 
2025-04-09 10:59:02.359055: Steps 59/90: batch_recall = 34.98, batch_ndcg = 23.18 
2025-04-09 10:59:03.072675: Steps 60/90: batch_recall = 34.92, batch_ndcg = 22.86 
2025-04-09 10:59:03.765585: Steps 61/90: batch_recall = 39.80, batch_ndcg = 27.92 
2025-04-09 10:59:04.463551: Steps 62/90: batch_recall = 39.34, batch_ndcg = 25.56 
2025-04-09 10:59:05.161217: Steps 63/90: batch_recall = 33.63, batch_ndcg = 23.04 
2025-04-09 10:59:05.866973: Steps 64/90: batch_recall = 35.29, batch_ndcg = 22.93 
2025-04-09 10:59:06.562807: Steps 65/90: batch_recall = 33.39, batch_ndcg = 22.35 
2025-04-09 10:59:07.276449: Steps 66/90: batch_recall = 36.63, batch_ndcg = 22.57 
2025-04-09 10:59:07.989735: Steps 67/90: batch_recall = 32.72, batch_ndcg = 20.79 
2025-04-09 10:59:08.715976: Steps 68/90: batch_recall = 36.15, batch_ndcg = 22.54 
2025-04-09 10:59:09.434690: Steps 69/90: batch_recall = 40.45, batch_ndcg = 24.36 
2025-04-09 10:59:10.141940: Steps 70/90: batch_recall = 39.18, batch_ndcg = 25.45 
2025-04-09 10:59:10.854654: Steps 71/90: batch_recall = 39.81, batch_ndcg = 24.59 
2025-04-09 10:59:11.561645: Steps 72/90: batch_recall = 35.22, batch_ndcg = 21.81 
2025-04-09 10:59:12.279108: Steps 73/90: batch_recall = 34.56, batch_ndcg = 21.53 
2025-04-09 10:59:12.983369: Steps 74/90: batch_recall = 41.32, batch_ndcg = 26.18 
2025-04-09 10:59:13.719733: Steps 75/90: batch_recall = 38.81, batch_ndcg = 23.38 
2025-04-09 10:59:14.416529: Steps 76/90: batch_recall = 41.41, batch_ndcg = 25.76 
2025-04-09 10:59:15.119038: Steps 77/90: batch_recall = 45.49, batch_ndcg = 28.07 
2025-04-09 10:59:15.810753: Steps 78/90: batch_recall = 34.00, batch_ndcg = 21.13 
2025-04-09 10:59:16.522076: Steps 79/90: batch_recall = 40.49, batch_ndcg = 26.70 
2025-04-09 10:59:17.227652: Steps 80/90: batch_recall = 43.95, batch_ndcg = 29.27 
2025-04-09 10:59:17.927173: Steps 81/90: batch_recall = 44.75, batch_ndcg = 29.03 
2025-04-09 10:59:18.614101: Steps 82/90: batch_recall = 45.03, batch_ndcg = 28.16 
2025-04-09 10:59:19.309111: Steps 83/90: batch_recall = 44.70, batch_ndcg = 27.32 
2025-04-09 10:59:20.012224: Steps 84/90: batch_recall = 35.14, batch_ndcg = 22.21 
2025-04-09 10:59:20.717133: Steps 85/90: batch_recall = 40.96, batch_ndcg = 24.44 
2025-04-09 10:59:21.429989: Steps 86/90: batch_recall = 44.21, batch_ndcg = 29.68 
2025-04-09 10:59:22.156280: Steps 87/90: batch_recall = 45.22, batch_ndcg = 29.55 
2025-04-09 10:59:22.869431: Steps 88/90: batch_recall = 50.33, batch_ndcg = 29.86 
2025-04-09 10:59:23.415549: Steps 89/90: batch_recall = 40.91, batch_ndcg = 26.54 
2025-04-09 10:59:23.416429: Epoch 41/1000, Test: Recall = 0.0759, NDCG = 0.0509  

2025-04-09 10:59:24.460708: Training Step 0/115: batchLoss = 4.3651, diffLoss = 7.1580, kgLoss = 0.1757
2025-04-09 10:59:25.191966: Training Step 1/115: batchLoss = 4.2079, diffLoss = 6.8993, kgLoss = 0.1707
2025-04-09 10:59:25.919503: Training Step 2/115: batchLoss = 4.1965, diffLoss = 6.8764, kgLoss = 0.1767
2025-04-09 10:59:26.646377: Training Step 3/115: batchLoss = 3.8550, diffLoss = 6.3248, kgLoss = 0.1503
2025-04-09 10:59:27.375842: Training Step 4/115: batchLoss = 4.3379, diffLoss = 7.1119, kgLoss = 0.1770
2025-04-09 10:59:28.106183: Training Step 5/115: batchLoss = 4.0029, diffLoss = 6.5559, kgLoss = 0.1734
2025-04-09 10:59:28.831836: Training Step 6/115: batchLoss = 4.0135, diffLoss = 6.5752, kgLoss = 0.1709
2025-04-09 10:59:29.551214: Training Step 7/115: batchLoss = 3.9945, diffLoss = 6.5413, kgLoss = 0.1742
2025-04-09 10:59:30.280185: Training Step 8/115: batchLoss = 4.3337, diffLoss = 7.1089, kgLoss = 0.1708
2025-04-09 10:59:30.999956: Training Step 9/115: batchLoss = 4.4701, diffLoss = 7.3187, kgLoss = 0.1972
2025-04-09 10:59:31.722255: Training Step 10/115: batchLoss = 4.4726, diffLoss = 7.3287, kgLoss = 0.1885
2025-04-09 10:59:32.441152: Training Step 11/115: batchLoss = 4.2908, diffLoss = 7.0373, kgLoss = 0.1711
2025-04-09 10:59:33.171445: Training Step 12/115: batchLoss = 4.3949, diffLoss = 7.2037, kgLoss = 0.1816
2025-04-09 10:59:33.899936: Training Step 13/115: batchLoss = 4.1743, diffLoss = 6.8410, kgLoss = 0.1741
2025-04-09 10:59:34.628466: Training Step 14/115: batchLoss = 4.2120, diffLoss = 6.8884, kgLoss = 0.1974
2025-04-09 10:59:35.359589: Training Step 15/115: batchLoss = 3.8612, diffLoss = 6.3304, kgLoss = 0.1575
2025-04-09 10:59:36.099843: Training Step 16/115: batchLoss = 3.8335, diffLoss = 6.2867, kgLoss = 0.1537
2025-04-09 10:59:36.828907: Training Step 17/115: batchLoss = 4.3770, diffLoss = 7.1675, kgLoss = 0.1914
2025-04-09 10:59:37.557611: Training Step 18/115: batchLoss = 3.8786, diffLoss = 6.3519, kgLoss = 0.1687
2025-04-09 10:59:38.288960: Training Step 19/115: batchLoss = 3.7132, diffLoss = 6.0812, kgLoss = 0.1611
2025-04-09 10:59:39.013386: Training Step 20/115: batchLoss = 3.7949, diffLoss = 6.2171, kgLoss = 0.1616
2025-04-09 10:59:39.738496: Training Step 21/115: batchLoss = 4.0940, diffLoss = 6.7103, kgLoss = 0.1696
2025-04-09 10:59:40.468655: Training Step 22/115: batchLoss = 4.1722, diffLoss = 6.8396, kgLoss = 0.1712
2025-04-09 10:59:41.202474: Training Step 23/115: batchLoss = 3.9895, diffLoss = 6.5349, kgLoss = 0.1714
2025-04-09 10:59:41.936904: Training Step 24/115: batchLoss = 3.6454, diffLoss = 5.9699, kgLoss = 0.1586
2025-04-09 10:59:42.663138: Training Step 25/115: batchLoss = 3.9188, diffLoss = 6.4169, kgLoss = 0.1717
2025-04-09 10:59:43.382911: Training Step 26/115: batchLoss = 4.0660, diffLoss = 6.6587, kgLoss = 0.1768
2025-04-09 10:59:44.123023: Training Step 27/115: batchLoss = 3.9361, diffLoss = 6.4498, kgLoss = 0.1657
2025-04-09 10:59:44.857803: Training Step 28/115: batchLoss = 3.8568, diffLoss = 6.3186, kgLoss = 0.1640
2025-04-09 10:59:45.597123: Training Step 29/115: batchLoss = 4.1180, diffLoss = 6.7499, kgLoss = 0.1702
2025-04-09 10:59:46.327163: Training Step 30/115: batchLoss = 4.1780, diffLoss = 6.8473, kgLoss = 0.1739
2025-04-09 10:59:47.067288: Training Step 31/115: batchLoss = 4.1580, diffLoss = 6.8182, kgLoss = 0.1677
2025-04-09 10:59:47.800269: Training Step 32/115: batchLoss = 3.8518, diffLoss = 6.3049, kgLoss = 0.1721
2025-04-09 10:59:48.517057: Training Step 33/115: batchLoss = 4.3219, diffLoss = 7.0853, kgLoss = 0.1768
2025-04-09 10:59:49.242758: Training Step 34/115: batchLoss = 4.1979, diffLoss = 6.8793, kgLoss = 0.1758
2025-04-09 10:59:49.978276: Training Step 35/115: batchLoss = 4.0612, diffLoss = 6.6536, kgLoss = 0.1726
2025-04-09 10:59:50.743516: Training Step 36/115: batchLoss = 4.3527, diffLoss = 7.1376, kgLoss = 0.1754
2025-04-09 10:59:51.484728: Training Step 37/115: batchLoss = 4.3963, diffLoss = 7.2040, kgLoss = 0.1848
2025-04-09 10:59:52.235336: Training Step 38/115: batchLoss = 4.2415, diffLoss = 6.9517, kgLoss = 0.1762
2025-04-09 10:59:52.986759: Training Step 39/115: batchLoss = 3.6874, diffLoss = 6.0441, kgLoss = 0.1525
2025-04-09 10:59:53.736593: Training Step 40/115: batchLoss = 4.4631, diffLoss = 7.3084, kgLoss = 0.1952
2025-04-09 10:59:54.484186: Training Step 41/115: batchLoss = 3.6484, diffLoss = 5.9839, kgLoss = 0.1451
2025-04-09 10:59:55.222210: Training Step 42/115: batchLoss = 3.6149, diffLoss = 5.9183, kgLoss = 0.1598
2025-04-09 10:59:55.964873: Training Step 43/115: batchLoss = 4.1005, diffLoss = 6.7231, kgLoss = 0.1667
2025-04-09 10:59:56.719713: Training Step 44/115: batchLoss = 4.3719, diffLoss = 7.1621, kgLoss = 0.1865
2025-04-09 10:59:57.479317: Training Step 45/115: batchLoss = 3.8448, diffLoss = 6.3012, kgLoss = 0.1601
2025-04-09 10:59:58.223141: Training Step 46/115: batchLoss = 3.8571, diffLoss = 6.3227, kgLoss = 0.1589
2025-04-09 10:59:58.990181: Training Step 47/115: batchLoss = 4.2593, diffLoss = 6.9795, kgLoss = 0.1791
2025-04-09 10:59:59.728282: Training Step 48/115: batchLoss = 4.0523, diffLoss = 6.6406, kgLoss = 0.1698
2025-04-09 11:00:00.468416: Training Step 49/115: batchLoss = 4.0082, diffLoss = 6.5662, kgLoss = 0.1713
2025-04-09 11:00:01.201486: Training Step 50/115: batchLoss = 4.1124, diffLoss = 6.7417, kgLoss = 0.1684
2025-04-09 11:00:01.922388: Training Step 51/115: batchLoss = 4.1083, diffLoss = 6.7279, kgLoss = 0.1791
2025-04-09 11:00:02.648847: Training Step 52/115: batchLoss = 3.4525, diffLoss = 5.6516, kgLoss = 0.1539
2025-04-09 11:00:03.371601: Training Step 53/115: batchLoss = 3.9447, diffLoss = 6.4617, kgLoss = 0.1691
2025-04-09 11:00:04.098314: Training Step 54/115: batchLoss = 4.2336, diffLoss = 6.9389, kgLoss = 0.1755
2025-04-09 11:00:04.820863: Training Step 55/115: batchLoss = 4.4337, diffLoss = 7.2718, kgLoss = 0.1767
2025-04-09 11:00:05.557715: Training Step 56/115: batchLoss = 4.1312, diffLoss = 6.7624, kgLoss = 0.1845
2025-04-09 11:00:06.296779: Training Step 57/115: batchLoss = 4.1642, diffLoss = 6.8254, kgLoss = 0.1723
2025-04-09 11:00:07.043186: Training Step 58/115: batchLoss = 3.9177, diffLoss = 6.4168, kgLoss = 0.1691
2025-04-09 11:00:07.802117: Training Step 59/115: batchLoss = 4.0832, diffLoss = 6.6902, kgLoss = 0.1725
2025-04-09 11:00:08.548301: Training Step 60/115: batchLoss = 4.0355, diffLoss = 6.6187, kgLoss = 0.1607
2025-04-09 11:00:09.281572: Training Step 61/115: batchLoss = 4.1156, diffLoss = 6.7451, kgLoss = 0.1714
2025-04-09 11:00:10.013927: Training Step 62/115: batchLoss = 4.0453, diffLoss = 6.6207, kgLoss = 0.1823
2025-04-09 11:00:10.745614: Training Step 63/115: batchLoss = 3.9523, diffLoss = 6.4783, kgLoss = 0.1632
2025-04-09 11:00:11.484098: Training Step 64/115: batchLoss = 4.7627, diffLoss = 7.8148, kgLoss = 0.1844
2025-04-09 11:00:12.220917: Training Step 65/115: batchLoss = 4.2216, diffLoss = 6.9183, kgLoss = 0.1767
2025-04-09 11:00:12.953548: Training Step 66/115: batchLoss = 3.7194, diffLoss = 6.0924, kgLoss = 0.1599
2025-04-09 11:00:13.682970: Training Step 67/115: batchLoss = 4.4771, diffLoss = 7.3431, kgLoss = 0.1780
2025-04-09 11:00:14.416035: Training Step 68/115: batchLoss = 4.6453, diffLoss = 7.6117, kgLoss = 0.1957
2025-04-09 11:00:15.161940: Training Step 69/115: batchLoss = 4.8206, diffLoss = 7.9068, kgLoss = 0.1914
2025-04-09 11:00:15.886251: Training Step 70/115: batchLoss = 4.1772, diffLoss = 6.8413, kgLoss = 0.1810
2025-04-09 11:00:16.627368: Training Step 71/115: batchLoss = 4.5137, diffLoss = 7.3989, kgLoss = 0.1859
2025-04-09 11:00:17.358999: Training Step 72/115: batchLoss = 5.0456, diffLoss = 8.2701, kgLoss = 0.2089
2025-04-09 11:00:18.099763: Training Step 73/115: batchLoss = 4.1994, diffLoss = 6.8794, kgLoss = 0.1794
2025-04-09 11:00:18.839257: Training Step 74/115: batchLoss = 3.8591, diffLoss = 6.3242, kgLoss = 0.1613
2025-04-09 11:00:19.593667: Training Step 75/115: batchLoss = 3.9133, diffLoss = 6.4179, kgLoss = 0.1563
2025-04-09 11:00:20.346260: Training Step 76/115: batchLoss = 3.9335, diffLoss = 6.4499, kgLoss = 0.1589
2025-04-09 11:00:21.085909: Training Step 77/115: batchLoss = 4.2987, diffLoss = 7.0461, kgLoss = 0.1777
2025-04-09 11:00:21.846589: Training Step 78/115: batchLoss = 3.7660, diffLoss = 6.1741, kgLoss = 0.1539
2025-04-09 11:00:22.583257: Training Step 79/115: batchLoss = 4.3219, diffLoss = 7.0856, kgLoss = 0.1763
2025-04-09 11:00:23.324056: Training Step 80/115: batchLoss = 4.4666, diffLoss = 7.3162, kgLoss = 0.1922
2025-04-09 11:00:24.069317: Training Step 81/115: batchLoss = 4.1401, diffLoss = 6.7861, kgLoss = 0.1712
2025-04-09 11:00:24.816077: Training Step 82/115: batchLoss = 4.1377, diffLoss = 6.7825, kgLoss = 0.1703
2025-04-09 11:00:25.558571: Training Step 83/115: batchLoss = 4.4146, diffLoss = 7.2385, kgLoss = 0.1787
2025-04-09 11:00:26.309911: Training Step 84/115: batchLoss = 4.0867, diffLoss = 6.7015, kgLoss = 0.1645
2025-04-09 11:00:27.080135: Training Step 85/115: batchLoss = 4.3576, diffLoss = 7.1347, kgLoss = 0.1920
2025-04-09 11:00:27.826374: Training Step 86/115: batchLoss = 4.0639, diffLoss = 6.6502, kgLoss = 0.1844
2025-04-09 11:00:28.575558: Training Step 87/115: batchLoss = 3.9930, diffLoss = 6.5478, kgLoss = 0.1608
2025-04-09 11:00:29.335944: Training Step 88/115: batchLoss = 4.3768, diffLoss = 7.1801, kgLoss = 0.1720
2025-04-09 11:00:30.059603: Training Step 89/115: batchLoss = 4.2087, diffLoss = 6.9045, kgLoss = 0.1651
2025-04-09 11:00:30.800166: Training Step 90/115: batchLoss = 3.8367, diffLoss = 6.2827, kgLoss = 0.1675
2025-04-09 11:00:31.533170: Training Step 91/115: batchLoss = 3.9969, diffLoss = 6.5501, kgLoss = 0.1670
2025-04-09 11:00:32.257204: Training Step 92/115: batchLoss = 3.9575, diffLoss = 6.4817, kgLoss = 0.1712
2025-04-09 11:00:32.991677: Training Step 93/115: batchLoss = 3.9192, diffLoss = 6.4200, kgLoss = 0.1681
2025-04-09 11:00:33.717428: Training Step 94/115: batchLoss = 3.9025, diffLoss = 6.3919, kgLoss = 0.1684
2025-04-09 11:00:34.450900: Training Step 95/115: batchLoss = 4.5991, diffLoss = 7.5323, kgLoss = 0.1993
2025-04-09 11:00:35.202300: Training Step 96/115: batchLoss = 4.6643, diffLoss = 7.6483, kgLoss = 0.1882
2025-04-09 11:00:35.939669: Training Step 97/115: batchLoss = 4.5249, diffLoss = 7.4209, kgLoss = 0.1808
2025-04-09 11:00:36.669021: Training Step 98/115: batchLoss = 4.2114, diffLoss = 6.9061, kgLoss = 0.1694
2025-04-09 11:00:37.416944: Training Step 99/115: batchLoss = 4.1931, diffLoss = 6.8706, kgLoss = 0.1768
2025-04-09 11:00:38.171410: Training Step 100/115: batchLoss = 4.2325, diffLoss = 6.9426, kgLoss = 0.1674
2025-04-09 11:00:38.922461: Training Step 101/115: batchLoss = 3.9100, diffLoss = 6.4088, kgLoss = 0.1618
2025-04-09 11:00:39.666740: Training Step 102/115: batchLoss = 4.5537, diffLoss = 7.4655, kgLoss = 0.1861
2025-04-09 11:00:40.410010: Training Step 103/115: batchLoss = 4.0481, diffLoss = 6.6365, kgLoss = 0.1654
2025-04-09 11:00:41.153197: Training Step 104/115: batchLoss = 4.7480, diffLoss = 7.7806, kgLoss = 0.1990
2025-04-09 11:00:41.892817: Training Step 105/115: batchLoss = 4.4198, diffLoss = 7.2378, kgLoss = 0.1928
2025-04-09 11:00:42.633512: Training Step 106/115: batchLoss = 3.7595, diffLoss = 6.1582, kgLoss = 0.1616
2025-04-09 11:00:43.377140: Training Step 107/115: batchLoss = 4.1673, diffLoss = 6.8314, kgLoss = 0.1712
2025-04-09 11:00:44.125342: Training Step 108/115: batchLoss = 4.8801, diffLoss = 7.9970, kgLoss = 0.2047
2025-04-09 11:00:44.863115: Training Step 109/115: batchLoss = 4.3203, diffLoss = 7.0837, kgLoss = 0.1753
2025-04-09 11:00:45.625785: Training Step 110/115: batchLoss = 3.8861, diffLoss = 6.3684, kgLoss = 0.1628
2025-04-09 11:00:46.374484: Training Step 111/115: batchLoss = 4.2904, diffLoss = 7.0311, kgLoss = 0.1793
2025-04-09 11:00:47.105647: Training Step 112/115: batchLoss = 4.3160, diffLoss = 7.0759, kgLoss = 0.1761
2025-04-09 11:00:47.749517: Training Step 113/115: batchLoss = 4.4281, diffLoss = 7.2536, kgLoss = 0.1899
2025-04-09 11:00:48.383104: Training Step 114/115: batchLoss = 3.5969, diffLoss = 5.8917, kgLoss = 0.1546
2025-04-09 11:00:48.524561: 
2025-04-09 11:00:48.525484: Epoch 42/1000, Train: epLoss = 1.1931, epDfLoss = 1.9553, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:00:49.314189: Steps 0/90: batch_recall = 56.95, batch_ndcg = 43.62 
2025-04-09 11:00:50.078167: Steps 1/90: batch_recall = 57.37, batch_ndcg = 40.93 
2025-04-09 11:00:50.876128: Steps 2/90: batch_recall = 50.55, batch_ndcg = 38.44 
2025-04-09 11:00:51.632944: Steps 3/90: batch_recall = 44.13, batch_ndcg = 32.12 
2025-04-09 11:00:52.391328: Steps 4/90: batch_recall = 49.12, batch_ndcg = 35.88 
2025-04-09 11:00:53.155853: Steps 5/90: batch_recall = 38.32, batch_ndcg = 28.37 
2025-04-09 11:00:53.917015: Steps 6/90: batch_recall = 47.45, batch_ndcg = 33.01 
2025-04-09 11:00:54.687332: Steps 7/90: batch_recall = 44.76, batch_ndcg = 31.91 
2025-04-09 11:00:55.418571: Steps 8/90: batch_recall = 46.75, batch_ndcg = 34.41 
2025-04-09 11:00:56.165334: Steps 9/90: batch_recall = 44.52, batch_ndcg = 33.75 
2025-04-09 11:00:56.901990: Steps 10/90: batch_recall = 39.90, batch_ndcg = 29.88 
2025-04-09 11:00:57.636879: Steps 11/90: batch_recall = 42.03, batch_ndcg = 28.84 
2025-04-09 11:00:58.363542: Steps 12/90: batch_recall = 35.56, batch_ndcg = 26.55 
2025-04-09 11:00:59.093020: Steps 13/90: batch_recall = 35.64, batch_ndcg = 25.98 
2025-04-09 11:00:59.833428: Steps 14/90: batch_recall = 34.06, batch_ndcg = 24.31 
2025-04-09 11:01:00.561394: Steps 15/90: batch_recall = 45.22, batch_ndcg = 30.08 
2025-04-09 11:01:01.297421: Steps 16/90: batch_recall = 35.19, batch_ndcg = 25.96 
2025-04-09 11:01:02.031098: Steps 17/90: batch_recall = 32.24, batch_ndcg = 21.47 
2025-04-09 11:01:02.756873: Steps 18/90: batch_recall = 38.64, batch_ndcg = 26.60 
2025-04-09 11:01:03.499335: Steps 19/90: batch_recall = 32.09, batch_ndcg = 23.07 
2025-04-09 11:01:04.234308: Steps 20/90: batch_recall = 40.60, batch_ndcg = 27.34 
2025-04-09 11:01:04.976451: Steps 21/90: batch_recall = 42.12, batch_ndcg = 29.89 
2025-04-09 11:01:05.717055: Steps 22/90: batch_recall = 38.85, batch_ndcg = 27.65 
2025-04-09 11:01:06.438849: Steps 23/90: batch_recall = 40.40, batch_ndcg = 27.12 
2025-04-09 11:01:07.162394: Steps 24/90: batch_recall = 37.88, batch_ndcg = 27.78 
2025-04-09 11:01:07.887660: Steps 25/90: batch_recall = 37.64, batch_ndcg = 25.42 
2025-04-09 11:01:08.611074: Steps 26/90: batch_recall = 38.37, batch_ndcg = 27.80 
2025-04-09 11:01:09.326357: Steps 27/90: batch_recall = 34.89, batch_ndcg = 23.65 
2025-04-09 11:01:10.052388: Steps 28/90: batch_recall = 37.62, batch_ndcg = 25.93 
2025-04-09 11:01:10.778617: Steps 29/90: batch_recall = 32.77, batch_ndcg = 22.87 
2025-04-09 11:01:11.516378: Steps 30/90: batch_recall = 34.71, batch_ndcg = 23.66 
2025-04-09 11:01:12.254899: Steps 31/90: batch_recall = 32.21, batch_ndcg = 23.33 
2025-04-09 11:01:13.001371: Steps 32/90: batch_recall = 35.76, batch_ndcg = 23.85 
2025-04-09 11:01:13.741468: Steps 33/90: batch_recall = 35.15, batch_ndcg = 21.85 
2025-04-09 11:01:14.449472: Steps 34/90: batch_recall = 34.67, batch_ndcg = 24.11 
2025-04-09 11:01:15.151713: Steps 35/90: batch_recall = 36.97, batch_ndcg = 26.82 
2025-04-09 11:01:15.881930: Steps 36/90: batch_recall = 36.44, batch_ndcg = 25.06 
2025-04-09 11:01:16.585409: Steps 37/90: batch_recall = 33.94, batch_ndcg = 21.47 
2025-04-09 11:01:17.300034: Steps 38/90: batch_recall = 35.88, batch_ndcg = 23.72 
2025-04-09 11:01:18.016392: Steps 39/90: batch_recall = 38.52, batch_ndcg = 23.86 
2025-04-09 11:01:18.714761: Steps 40/90: batch_recall = 31.12, batch_ndcg = 22.16 
2025-04-09 11:01:19.422700: Steps 41/90: batch_recall = 33.57, batch_ndcg = 21.57 
2025-04-09 11:01:20.137814: Steps 42/90: batch_recall = 45.13, batch_ndcg = 29.27 
2025-04-09 11:01:20.838373: Steps 43/90: batch_recall = 36.71, batch_ndcg = 22.79 
2025-04-09 11:01:21.550574: Steps 44/90: batch_recall = 34.41, batch_ndcg = 23.46 
2025-04-09 11:01:22.269361: Steps 45/90: batch_recall = 32.97, batch_ndcg = 22.51 
2025-04-09 11:01:22.995066: Steps 46/90: batch_recall = 35.00, batch_ndcg = 22.89 
2025-04-09 11:01:23.709347: Steps 47/90: batch_recall = 32.83, batch_ndcg = 20.62 
2025-04-09 11:01:24.427622: Steps 48/90: batch_recall = 37.04, batch_ndcg = 24.22 
2025-04-09 11:01:25.142382: Steps 49/90: batch_recall = 30.52, batch_ndcg = 18.94 
2025-04-09 11:01:25.848807: Steps 50/90: batch_recall = 41.01, batch_ndcg = 26.55 
2025-04-09 11:01:26.572052: Steps 51/90: batch_recall = 40.38, batch_ndcg = 25.79 
2025-04-09 11:01:27.284390: Steps 52/90: batch_recall = 38.93, batch_ndcg = 25.36 
2025-04-09 11:01:28.001690: Steps 53/90: batch_recall = 42.48, batch_ndcg = 27.09 
2025-04-09 11:01:28.739268: Steps 54/90: batch_recall = 30.44, batch_ndcg = 20.07 
2025-04-09 11:01:29.450088: Steps 55/90: batch_recall = 35.40, batch_ndcg = 23.05 
2025-04-09 11:01:30.162652: Steps 56/90: batch_recall = 33.96, batch_ndcg = 20.30 
2025-04-09 11:01:30.868463: Steps 57/90: batch_recall = 36.56, batch_ndcg = 24.37 
2025-04-09 11:01:31.572327: Steps 58/90: batch_recall = 37.01, batch_ndcg = 22.85 
2025-04-09 11:01:32.281253: Steps 59/90: batch_recall = 35.46, batch_ndcg = 23.13 
2025-04-09 11:01:32.978289: Steps 60/90: batch_recall = 34.27, batch_ndcg = 22.65 
2025-04-09 11:01:33.684568: Steps 61/90: batch_recall = 40.13, batch_ndcg = 28.17 
2025-04-09 11:01:34.388040: Steps 62/90: batch_recall = 39.66, batch_ndcg = 25.74 
2025-04-09 11:01:35.089889: Steps 63/90: batch_recall = 32.94, batch_ndcg = 22.94 
2025-04-09 11:01:35.791333: Steps 64/90: batch_recall = 35.29, batch_ndcg = 23.12 
2025-04-09 11:01:36.499758: Steps 65/90: batch_recall = 33.33, batch_ndcg = 22.27 
2025-04-09 11:01:37.236184: Steps 66/90: batch_recall = 37.87, batch_ndcg = 23.01 
2025-04-09 11:01:37.960335: Steps 67/90: batch_recall = 31.94, batch_ndcg = 20.15 
2025-04-09 11:01:38.669522: Steps 68/90: batch_recall = 37.09, batch_ndcg = 22.69 
2025-04-09 11:01:39.373183: Steps 69/90: batch_recall = 39.25, batch_ndcg = 24.01 
2025-04-09 11:01:40.073785: Steps 70/90: batch_recall = 38.71, batch_ndcg = 25.05 
2025-04-09 11:01:40.784138: Steps 71/90: batch_recall = 39.12, batch_ndcg = 24.62 
2025-04-09 11:01:41.501089: Steps 72/90: batch_recall = 34.38, batch_ndcg = 21.60 
2025-04-09 11:01:42.222675: Steps 73/90: batch_recall = 34.51, batch_ndcg = 21.79 
2025-04-09 11:01:42.942678: Steps 74/90: batch_recall = 41.59, batch_ndcg = 26.34 
2025-04-09 11:01:43.661061: Steps 75/90: batch_recall = 37.85, batch_ndcg = 22.94 
2025-04-09 11:01:44.362341: Steps 76/90: batch_recall = 40.38, batch_ndcg = 25.30 
2025-04-09 11:01:45.063966: Steps 77/90: batch_recall = 44.43, batch_ndcg = 28.12 
2025-04-09 11:01:45.769064: Steps 78/90: batch_recall = 33.85, batch_ndcg = 21.04 
2025-04-09 11:01:46.466799: Steps 79/90: batch_recall = 39.41, batch_ndcg = 26.28 
2025-04-09 11:01:47.162563: Steps 80/90: batch_recall = 45.30, batch_ndcg = 29.48 
2025-04-09 11:01:47.858540: Steps 81/90: batch_recall = 44.94, batch_ndcg = 28.57 
2025-04-09 11:01:48.561197: Steps 82/90: batch_recall = 45.11, batch_ndcg = 28.26 
2025-04-09 11:01:49.242409: Steps 83/90: batch_recall = 44.57, batch_ndcg = 27.38 
2025-04-09 11:01:49.947801: Steps 84/90: batch_recall = 34.91, batch_ndcg = 22.13 
2025-04-09 11:01:50.666388: Steps 85/90: batch_recall = 42.69, batch_ndcg = 25.04 
2025-04-09 11:01:51.392468: Steps 86/90: batch_recall = 44.13, batch_ndcg = 29.70 
2025-04-09 11:01:52.096085: Steps 87/90: batch_recall = 45.90, batch_ndcg = 29.91 
2025-04-09 11:01:52.791159: Steps 88/90: batch_recall = 50.80, batch_ndcg = 30.09 
2025-04-09 11:01:53.325712: Steps 89/90: batch_recall = 40.91, batch_ndcg = 26.24 
2025-04-09 11:01:53.326577: Epoch 42/1000, Test: Recall = 0.0760, NDCG = 0.0510  

2025-04-09 11:01:54.361005: Training Step 0/115: batchLoss = 4.1676, diffLoss = 6.8313, kgLoss = 0.1721
2025-04-09 11:01:55.094982: Training Step 1/115: batchLoss = 4.1116, diffLoss = 6.7376, kgLoss = 0.1725
2025-04-09 11:01:55.826572: Training Step 2/115: batchLoss = 3.6042, diffLoss = 5.9084, kgLoss = 0.1478
2025-04-09 11:01:56.558290: Training Step 3/115: batchLoss = 4.1259, diffLoss = 6.7539, kgLoss = 0.1840
2025-04-09 11:01:57.289387: Training Step 4/115: batchLoss = 3.8682, diffLoss = 6.3429, kgLoss = 0.1560
2025-04-09 11:01:58.027140: Training Step 5/115: batchLoss = 3.8529, diffLoss = 6.3138, kgLoss = 0.1617
2025-04-09 11:01:58.769364: Training Step 6/115: batchLoss = 4.0328, diffLoss = 6.6062, kgLoss = 0.1727
2025-04-09 11:01:59.496553: Training Step 7/115: batchLoss = 3.4837, diffLoss = 5.7009, kgLoss = 0.1579
2025-04-09 11:02:00.220584: Training Step 8/115: batchLoss = 4.0456, diffLoss = 6.6185, kgLoss = 0.1863
2025-04-09 11:02:00.948383: Training Step 9/115: batchLoss = 4.1172, diffLoss = 6.7441, kgLoss = 0.1768
2025-04-09 11:02:01.669288: Training Step 10/115: batchLoss = 4.8758, diffLoss = 7.9853, kgLoss = 0.2115
2025-04-09 11:02:02.398004: Training Step 11/115: batchLoss = 3.7680, diffLoss = 6.1751, kgLoss = 0.1574
2025-04-09 11:02:03.129184: Training Step 12/115: batchLoss = 4.3214, diffLoss = 7.0792, kgLoss = 0.1846
2025-04-09 11:02:03.853771: Training Step 13/115: batchLoss = 4.1191, diffLoss = 6.7491, kgLoss = 0.1741
2025-04-09 11:02:04.584206: Training Step 14/115: batchLoss = 4.6640, diffLoss = 7.6449, kgLoss = 0.1925
2025-04-09 11:02:05.314859: Training Step 15/115: batchLoss = 3.9392, diffLoss = 6.4541, kgLoss = 0.1670
2025-04-09 11:02:06.042264: Training Step 16/115: batchLoss = 4.3402, diffLoss = 7.1102, kgLoss = 0.1851
2025-04-09 11:02:06.780570: Training Step 17/115: batchLoss = 3.7993, diffLoss = 6.2280, kgLoss = 0.1561
2025-04-09 11:02:07.510728: Training Step 18/115: batchLoss = 4.1821, diffLoss = 6.8447, kgLoss = 0.1882
2025-04-09 11:02:08.235267: Training Step 19/115: batchLoss = 3.7459, diffLoss = 6.1338, kgLoss = 0.1639
2025-04-09 11:02:08.965828: Training Step 20/115: batchLoss = 4.7299, diffLoss = 7.7612, kgLoss = 0.1831
2025-04-09 11:02:09.710639: Training Step 21/115: batchLoss = 3.9143, diffLoss = 6.4200, kgLoss = 0.1556
2025-04-09 11:02:10.451814: Training Step 22/115: batchLoss = 3.7743, diffLoss = 6.1864, kgLoss = 0.1560
2025-04-09 11:02:11.196946: Training Step 23/115: batchLoss = 4.1835, diffLoss = 6.8582, kgLoss = 0.1713
2025-04-09 11:02:11.935125: Training Step 24/115: batchLoss = 3.7973, diffLoss = 6.2181, kgLoss = 0.1662
2025-04-09 11:02:12.681887: Training Step 25/115: batchLoss = 4.3161, diffLoss = 7.0797, kgLoss = 0.1707
2025-04-09 11:02:13.437411: Training Step 26/115: batchLoss = 4.4193, diffLoss = 7.2421, kgLoss = 0.1851
2025-04-09 11:02:14.188178: Training Step 27/115: batchLoss = 3.8761, diffLoss = 6.3408, kgLoss = 0.1789
2025-04-09 11:02:14.933403: Training Step 28/115: batchLoss = 4.1299, diffLoss = 6.7681, kgLoss = 0.1726
2025-04-09 11:02:15.672864: Training Step 29/115: batchLoss = 4.3207, diffLoss = 7.0825, kgLoss = 0.1779
2025-04-09 11:02:16.415662: Training Step 30/115: batchLoss = 3.7901, diffLoss = 6.2086, kgLoss = 0.1623
2025-04-09 11:02:17.146078: Training Step 31/115: batchLoss = 4.0592, diffLoss = 6.6565, kgLoss = 0.1632
2025-04-09 11:02:17.882018: Training Step 32/115: batchLoss = 4.5640, diffLoss = 7.4840, kgLoss = 0.1839
2025-04-09 11:02:18.619012: Training Step 33/115: batchLoss = 4.3677, diffLoss = 7.1542, kgLoss = 0.1879
2025-04-09 11:02:19.353265: Training Step 34/115: batchLoss = 3.9680, diffLoss = 6.4962, kgLoss = 0.1757
2025-04-09 11:02:20.089103: Training Step 35/115: batchLoss = 3.9757, diffLoss = 6.5071, kgLoss = 0.1786
2025-04-09 11:02:20.838574: Training Step 36/115: batchLoss = 3.9843, diffLoss = 6.5304, kgLoss = 0.1651
2025-04-09 11:02:21.584498: Training Step 37/115: batchLoss = 4.5349, diffLoss = 7.4342, kgLoss = 0.1860
2025-04-09 11:02:22.331395: Training Step 38/115: batchLoss = 3.6152, diffLoss = 5.9126, kgLoss = 0.1691
2025-04-09 11:02:23.064551: Training Step 39/115: batchLoss = 4.2841, diffLoss = 7.0149, kgLoss = 0.1879
2025-04-09 11:02:23.807863: Training Step 40/115: batchLoss = 3.9384, diffLoss = 6.4554, kgLoss = 0.1630
2025-04-09 11:02:24.546339: Training Step 41/115: batchLoss = 4.2839, diffLoss = 7.0270, kgLoss = 0.1694
2025-04-09 11:02:25.301895: Training Step 42/115: batchLoss = 4.3077, diffLoss = 7.0654, kgLoss = 0.1711
2025-04-09 11:02:26.051473: Training Step 43/115: batchLoss = 4.1347, diffLoss = 6.7819, kgLoss = 0.1638
2025-04-09 11:02:26.798959: Training Step 44/115: batchLoss = 4.5058, diffLoss = 7.3873, kgLoss = 0.1836
2025-04-09 11:02:27.544305: Training Step 45/115: batchLoss = 4.2321, diffLoss = 6.9388, kgLoss = 0.1721
2025-04-09 11:02:28.278451: Training Step 46/115: batchLoss = 3.9556, diffLoss = 6.4786, kgLoss = 0.1711
2025-04-09 11:02:29.004803: Training Step 47/115: batchLoss = 4.0900, diffLoss = 6.7005, kgLoss = 0.1741
2025-04-09 11:02:29.745757: Training Step 48/115: batchLoss = 4.3156, diffLoss = 7.0699, kgLoss = 0.1840
2025-04-09 11:02:30.480289: Training Step 49/115: batchLoss = 4.0159, diffLoss = 6.5794, kgLoss = 0.1707
2025-04-09 11:02:31.210362: Training Step 50/115: batchLoss = 4.3079, diffLoss = 7.0607, kgLoss = 0.1788
2025-04-09 11:02:31.940913: Training Step 51/115: batchLoss = 4.4047, diffLoss = 7.2189, kgLoss = 0.1834
2025-04-09 11:02:32.676691: Training Step 52/115: batchLoss = 3.6762, diffLoss = 6.0235, kgLoss = 0.1552
2025-04-09 11:02:33.411458: Training Step 53/115: batchLoss = 3.7884, diffLoss = 6.2096, kgLoss = 0.1567
2025-04-09 11:02:34.151329: Training Step 54/115: batchLoss = 4.0701, diffLoss = 6.6772, kgLoss = 0.1594
2025-04-09 11:02:34.902173: Training Step 55/115: batchLoss = 4.6082, diffLoss = 7.5493, kgLoss = 0.1964
2025-04-09 11:02:35.648578: Training Step 56/115: batchLoss = 3.8392, diffLoss = 6.2768, kgLoss = 0.1829
2025-04-09 11:02:36.394362: Training Step 57/115: batchLoss = 4.0142, diffLoss = 6.5782, kgLoss = 0.1681
2025-04-09 11:02:37.130528: Training Step 58/115: batchLoss = 4.4153, diffLoss = 7.2375, kgLoss = 0.1820
2025-04-09 11:02:37.874231: Training Step 59/115: batchLoss = 3.8976, diffLoss = 6.3774, kgLoss = 0.1781
2025-04-09 11:02:38.619740: Training Step 60/115: batchLoss = 4.0255, diffLoss = 6.5963, kgLoss = 0.1693
2025-04-09 11:02:39.368897: Training Step 61/115: batchLoss = 4.1508, diffLoss = 6.8061, kgLoss = 0.1680
2025-04-09 11:02:40.132735: Training Step 62/115: batchLoss = 4.2638, diffLoss = 6.9895, kgLoss = 0.1752
2025-04-09 11:02:40.902175: Training Step 63/115: batchLoss = 4.4042, diffLoss = 7.2191, kgLoss = 0.1818
2025-04-09 11:02:41.647895: Training Step 64/115: batchLoss = 3.9711, diffLoss = 6.5065, kgLoss = 0.1679
2025-04-09 11:02:42.398244: Training Step 65/115: batchLoss = 3.9686, diffLoss = 6.4987, kgLoss = 0.1734
2025-04-09 11:02:43.143795: Training Step 66/115: batchLoss = 4.3167, diffLoss = 7.0791, kgLoss = 0.1731
2025-04-09 11:02:43.891825: Training Step 67/115: batchLoss = 4.0907, diffLoss = 6.6950, kgLoss = 0.1842
2025-04-09 11:02:44.619465: Training Step 68/115: batchLoss = 4.3754, diffLoss = 7.1709, kgLoss = 0.1820
2025-04-09 11:02:45.356105: Training Step 69/115: batchLoss = 4.0774, diffLoss = 6.6851, kgLoss = 0.1659
2025-04-09 11:02:46.091817: Training Step 70/115: batchLoss = 4.3135, diffLoss = 7.0721, kgLoss = 0.1755
2025-04-09 11:02:46.823350: Training Step 71/115: batchLoss = 4.1364, diffLoss = 6.7754, kgLoss = 0.1780
2025-04-09 11:02:47.555829: Training Step 72/115: batchLoss = 4.3351, diffLoss = 7.0998, kgLoss = 0.1880
2025-04-09 11:02:48.286465: Training Step 73/115: batchLoss = 4.1858, diffLoss = 6.8572, kgLoss = 0.1788
2025-04-09 11:02:49.037766: Training Step 74/115: batchLoss = 4.2568, diffLoss = 6.9779, kgLoss = 0.1753
2025-04-09 11:02:49.754852: Training Step 75/115: batchLoss = 4.7069, diffLoss = 7.7079, kgLoss = 0.2054
2025-04-09 11:02:50.494253: Training Step 76/115: batchLoss = 4.7052, diffLoss = 7.7071, kgLoss = 0.2022
2025-04-09 11:02:51.242181: Training Step 77/115: batchLoss = 4.5553, diffLoss = 7.4681, kgLoss = 0.1862
2025-04-09 11:02:51.970519: Training Step 78/115: batchLoss = 4.2227, diffLoss = 6.9214, kgLoss = 0.1746
2025-04-09 11:02:52.711663: Training Step 79/115: batchLoss = 4.0675, diffLoss = 6.6709, kgLoss = 0.1624
2025-04-09 11:02:53.454618: Training Step 80/115: batchLoss = 3.9993, diffLoss = 6.5585, kgLoss = 0.1607
2025-04-09 11:02:54.202407: Training Step 81/115: batchLoss = 3.8866, diffLoss = 6.3656, kgLoss = 0.1682
2025-04-09 11:02:54.960314: Training Step 82/115: batchLoss = 3.8785, diffLoss = 6.3517, kgLoss = 0.1685
2025-04-09 11:02:55.715738: Training Step 83/115: batchLoss = 4.4550, diffLoss = 7.3061, kgLoss = 0.1783
2025-04-09 11:02:56.454013: Training Step 84/115: batchLoss = 4.1054, diffLoss = 6.7303, kgLoss = 0.1680
2025-04-09 11:02:57.204865: Training Step 85/115: batchLoss = 4.0856, diffLoss = 6.6981, kgLoss = 0.1668
2025-04-09 11:02:57.938221: Training Step 86/115: batchLoss = 3.7081, diffLoss = 6.0779, kgLoss = 0.1535
2025-04-09 11:02:58.673307: Training Step 87/115: batchLoss = 3.9397, diffLoss = 6.4553, kgLoss = 0.1662
2025-04-09 11:02:59.415403: Training Step 88/115: batchLoss = 4.2222, diffLoss = 6.9191, kgLoss = 0.1768
2025-04-09 11:03:00.157379: Training Step 89/115: batchLoss = 4.0428, diffLoss = 6.6164, kgLoss = 0.1824
2025-04-09 11:03:00.888162: Training Step 90/115: batchLoss = 3.9969, diffLoss = 6.5448, kgLoss = 0.1751
2025-04-09 11:03:01.638455: Training Step 91/115: batchLoss = 4.2359, diffLoss = 6.9394, kgLoss = 0.1806
2025-04-09 11:03:02.403032: Training Step 92/115: batchLoss = 3.8468, diffLoss = 6.3055, kgLoss = 0.1587
2025-04-09 11:03:03.148639: Training Step 93/115: batchLoss = 4.4616, diffLoss = 7.3162, kgLoss = 0.1798
2025-04-09 11:03:03.889076: Training Step 94/115: batchLoss = 4.2467, diffLoss = 6.9577, kgLoss = 0.1801
2025-04-09 11:03:04.635527: Training Step 95/115: batchLoss = 4.0919, diffLoss = 6.7127, kgLoss = 0.1606
2025-04-09 11:03:05.355363: Training Step 96/115: batchLoss = 4.4467, diffLoss = 7.2980, kgLoss = 0.1698
2025-04-09 11:03:06.094978: Training Step 97/115: batchLoss = 4.5031, diffLoss = 7.3839, kgLoss = 0.1818
2025-04-09 11:03:06.833078: Training Step 98/115: batchLoss = 3.7822, diffLoss = 6.1975, kgLoss = 0.1592
2025-04-09 11:03:07.574933: Training Step 99/115: batchLoss = 4.2517, diffLoss = 6.9737, kgLoss = 0.1687
2025-04-09 11:03:08.320054: Training Step 100/115: batchLoss = 4.2269, diffLoss = 6.9267, kgLoss = 0.1772
2025-04-09 11:03:09.069044: Training Step 101/115: batchLoss = 4.1950, diffLoss = 6.8825, kgLoss = 0.1638
2025-04-09 11:03:09.812262: Training Step 102/115: batchLoss = 4.4983, diffLoss = 7.3737, kgLoss = 0.1851
2025-04-09 11:03:10.545860: Training Step 103/115: batchLoss = 4.0597, diffLoss = 6.6525, kgLoss = 0.1706
2025-04-09 11:03:11.276917: Training Step 104/115: batchLoss = 4.1924, diffLoss = 6.8730, kgLoss = 0.1715
2025-04-09 11:03:12.018382: Training Step 105/115: batchLoss = 3.7740, diffLoss = 6.1920, kgLoss = 0.1469
2025-04-09 11:03:12.771154: Training Step 106/115: batchLoss = 4.3236, diffLoss = 7.0889, kgLoss = 0.1758
2025-04-09 11:03:13.519388: Training Step 107/115: batchLoss = 3.9624, diffLoss = 6.4943, kgLoss = 0.1645
2025-04-09 11:03:14.263203: Training Step 108/115: batchLoss = 4.7124, diffLoss = 7.7224, kgLoss = 0.1973
2025-04-09 11:03:15.004542: Training Step 109/115: batchLoss = 4.1535, diffLoss = 6.8056, kgLoss = 0.1754
2025-04-09 11:03:15.743235: Training Step 110/115: batchLoss = 4.1505, diffLoss = 6.8049, kgLoss = 0.1690
2025-04-09 11:03:16.478978: Training Step 111/115: batchLoss = 3.9371, diffLoss = 6.4495, kgLoss = 0.1685
2025-04-09 11:03:17.200428: Training Step 112/115: batchLoss = 4.5632, diffLoss = 7.4754, kgLoss = 0.1950
2025-04-09 11:03:17.855356: Training Step 113/115: batchLoss = 3.8929, diffLoss = 6.3819, kgLoss = 0.1594
2025-04-09 11:03:18.499121: Training Step 114/115: batchLoss = 3.8347, diffLoss = 6.2835, kgLoss = 0.1616
2025-04-09 11:03:18.623191: 
2025-04-09 11:03:18.624348: Epoch 43/1000, Train: epLoss = 1.1909, epDfLoss = 1.9516, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:03:19.398361: Steps 0/90: batch_recall = 57.79, batch_ndcg = 44.22 
2025-04-09 11:03:20.178078: Steps 1/90: batch_recall = 57.39, batch_ndcg = 40.99 
2025-04-09 11:03:20.947524: Steps 2/90: batch_recall = 49.99, batch_ndcg = 37.84 
2025-04-09 11:03:21.706325: Steps 3/90: batch_recall = 44.25, batch_ndcg = 32.27 
2025-04-09 11:03:22.456962: Steps 4/90: batch_recall = 48.95, batch_ndcg = 35.96 
2025-04-09 11:03:23.216924: Steps 5/90: batch_recall = 38.74, batch_ndcg = 28.04 
2025-04-09 11:03:23.982407: Steps 6/90: batch_recall = 46.27, batch_ndcg = 32.19 
2025-04-09 11:03:24.747293: Steps 7/90: batch_recall = 44.41, batch_ndcg = 32.28 
2025-04-09 11:03:25.508509: Steps 8/90: batch_recall = 47.17, batch_ndcg = 34.46 
2025-04-09 11:03:26.259538: Steps 9/90: batch_recall = 44.64, batch_ndcg = 33.79 
2025-04-09 11:03:27.024875: Steps 10/90: batch_recall = 39.06, batch_ndcg = 30.07 
2025-04-09 11:03:27.775730: Steps 11/90: batch_recall = 41.99, batch_ndcg = 28.97 
2025-04-09 11:03:28.517817: Steps 12/90: batch_recall = 36.36, batch_ndcg = 26.96 
2025-04-09 11:03:29.252765: Steps 13/90: batch_recall = 36.09, batch_ndcg = 26.17 
2025-04-09 11:03:29.994399: Steps 14/90: batch_recall = 34.42, batch_ndcg = 24.60 
2025-04-09 11:03:30.726195: Steps 15/90: batch_recall = 46.25, batch_ndcg = 30.45 
2025-04-09 11:03:31.459638: Steps 16/90: batch_recall = 36.17, batch_ndcg = 26.15 
2025-04-09 11:03:32.187757: Steps 17/90: batch_recall = 31.89, batch_ndcg = 21.58 
2025-04-09 11:03:32.905802: Steps 18/90: batch_recall = 36.97, batch_ndcg = 25.93 
2025-04-09 11:03:33.640069: Steps 19/90: batch_recall = 32.51, batch_ndcg = 23.30 
2025-04-09 11:03:34.374115: Steps 20/90: batch_recall = 40.93, batch_ndcg = 27.42 
2025-04-09 11:03:35.094123: Steps 21/90: batch_recall = 42.16, batch_ndcg = 30.24 
2025-04-09 11:03:35.812277: Steps 22/90: batch_recall = 39.50, batch_ndcg = 28.01 
2025-04-09 11:03:36.541026: Steps 23/90: batch_recall = 40.20, batch_ndcg = 27.33 
2025-04-09 11:03:37.273420: Steps 24/90: batch_recall = 38.04, batch_ndcg = 28.03 
2025-04-09 11:03:38.005377: Steps 25/90: batch_recall = 37.62, batch_ndcg = 25.07 
2025-04-09 11:03:38.728786: Steps 26/90: batch_recall = 38.07, batch_ndcg = 27.60 
2025-04-09 11:03:39.454902: Steps 27/90: batch_recall = 34.82, batch_ndcg = 23.67 
2025-04-09 11:03:40.204709: Steps 28/90: batch_recall = 38.62, batch_ndcg = 26.26 
2025-04-09 11:03:40.934397: Steps 29/90: batch_recall = 33.20, batch_ndcg = 23.05 
2025-04-09 11:03:41.657476: Steps 30/90: batch_recall = 35.04, batch_ndcg = 23.82 
2025-04-09 11:03:42.398176: Steps 31/90: batch_recall = 32.39, batch_ndcg = 23.68 
2025-04-09 11:03:43.109603: Steps 32/90: batch_recall = 35.34, batch_ndcg = 23.57 
2025-04-09 11:03:43.832117: Steps 33/90: batch_recall = 35.59, batch_ndcg = 21.92 
2025-04-09 11:03:44.565385: Steps 34/90: batch_recall = 34.99, batch_ndcg = 24.12 
2025-04-09 11:03:45.273265: Steps 35/90: batch_recall = 37.74, batch_ndcg = 26.50 
2025-04-09 11:03:45.992285: Steps 36/90: batch_recall = 36.90, batch_ndcg = 24.97 
2025-04-09 11:03:46.710319: Steps 37/90: batch_recall = 33.36, batch_ndcg = 21.14 
2025-04-09 11:03:47.410959: Steps 38/90: batch_recall = 37.19, batch_ndcg = 24.20 
2025-04-09 11:03:48.124299: Steps 39/90: batch_recall = 38.82, batch_ndcg = 24.02 
2025-04-09 11:03:48.836267: Steps 40/90: batch_recall = 31.89, batch_ndcg = 22.28 
2025-04-09 11:03:49.531507: Steps 41/90: batch_recall = 33.81, batch_ndcg = 21.61 
2025-04-09 11:03:50.238540: Steps 42/90: batch_recall = 43.78, batch_ndcg = 28.82 
2025-04-09 11:03:50.955960: Steps 43/90: batch_recall = 36.87, batch_ndcg = 22.83 
2025-04-09 11:03:51.680786: Steps 44/90: batch_recall = 34.62, batch_ndcg = 23.31 
2025-04-09 11:03:52.431118: Steps 45/90: batch_recall = 32.75, batch_ndcg = 22.38 
2025-04-09 11:03:53.141888: Steps 46/90: batch_recall = 34.83, batch_ndcg = 22.84 
2025-04-09 11:03:53.851855: Steps 47/90: batch_recall = 33.85, batch_ndcg = 20.85 
2025-04-09 11:03:54.574013: Steps 48/90: batch_recall = 37.15, batch_ndcg = 24.05 
2025-04-09 11:03:55.280709: Steps 49/90: batch_recall = 30.98, batch_ndcg = 19.05 
2025-04-09 11:03:55.983872: Steps 50/90: batch_recall = 41.46, batch_ndcg = 26.73 
2025-04-09 11:03:56.694971: Steps 51/90: batch_recall = 40.82, batch_ndcg = 25.92 
2025-04-09 11:03:57.421131: Steps 52/90: batch_recall = 38.98, batch_ndcg = 25.60 
2025-04-09 11:03:58.149782: Steps 53/90: batch_recall = 42.11, batch_ndcg = 27.01 
2025-04-09 11:03:58.869373: Steps 54/90: batch_recall = 32.08, batch_ndcg = 21.08 
2025-04-09 11:03:59.580216: Steps 55/90: batch_recall = 36.33, batch_ndcg = 23.43 
2025-04-09 11:04:00.290646: Steps 56/90: batch_recall = 33.11, batch_ndcg = 20.20 
2025-04-09 11:04:01.001860: Steps 57/90: batch_recall = 36.13, batch_ndcg = 24.37 
2025-04-09 11:04:01.720843: Steps 58/90: batch_recall = 37.54, batch_ndcg = 23.03 
2025-04-09 11:04:02.422688: Steps 59/90: batch_recall = 35.67, batch_ndcg = 23.52 
2025-04-09 11:04:03.129665: Steps 60/90: batch_recall = 35.02, batch_ndcg = 22.84 
2025-04-09 11:04:03.839080: Steps 61/90: batch_recall = 40.39, batch_ndcg = 27.85 
2025-04-09 11:04:04.536512: Steps 62/90: batch_recall = 40.13, batch_ndcg = 26.08 
2025-04-09 11:04:05.228729: Steps 63/90: batch_recall = 32.94, batch_ndcg = 22.85 
2025-04-09 11:04:05.923093: Steps 64/90: batch_recall = 35.68, batch_ndcg = 23.29 
2025-04-09 11:04:06.639442: Steps 65/90: batch_recall = 35.19, batch_ndcg = 22.74 
2025-04-09 11:04:07.359661: Steps 66/90: batch_recall = 36.93, batch_ndcg = 22.55 
2025-04-09 11:04:08.089326: Steps 67/90: batch_recall = 32.93, batch_ndcg = 20.41 
2025-04-09 11:04:08.802844: Steps 68/90: batch_recall = 37.22, batch_ndcg = 22.97 
2025-04-09 11:04:09.513277: Steps 69/90: batch_recall = 39.71, batch_ndcg = 24.37 
2025-04-09 11:04:10.220879: Steps 70/90: batch_recall = 38.48, batch_ndcg = 24.87 
2025-04-09 11:04:10.932803: Steps 71/90: batch_recall = 39.51, batch_ndcg = 24.23 
2025-04-09 11:04:11.648124: Steps 72/90: batch_recall = 34.25, batch_ndcg = 21.62 
2025-04-09 11:04:12.362840: Steps 73/90: batch_recall = 33.49, batch_ndcg = 21.24 
2025-04-09 11:04:13.091974: Steps 74/90: batch_recall = 40.33, batch_ndcg = 25.80 
2025-04-09 11:04:13.797722: Steps 75/90: batch_recall = 37.57, batch_ndcg = 22.81 
2025-04-09 11:04:14.496280: Steps 76/90: batch_recall = 40.69, batch_ndcg = 25.30 
2025-04-09 11:04:15.197288: Steps 77/90: batch_recall = 44.66, batch_ndcg = 28.33 
2025-04-09 11:04:15.896723: Steps 78/90: batch_recall = 34.28, batch_ndcg = 21.20 
2025-04-09 11:04:16.602195: Steps 79/90: batch_recall = 39.30, batch_ndcg = 25.76 
2025-04-09 11:04:17.285330: Steps 80/90: batch_recall = 45.34, batch_ndcg = 29.62 
2025-04-09 11:04:17.982049: Steps 81/90: batch_recall = 45.19, batch_ndcg = 28.64 
2025-04-09 11:04:18.673611: Steps 82/90: batch_recall = 43.65, batch_ndcg = 27.79 
2025-04-09 11:04:19.375181: Steps 83/90: batch_recall = 45.39, batch_ndcg = 27.62 
2025-04-09 11:04:20.085456: Steps 84/90: batch_recall = 34.42, batch_ndcg = 22.16 
2025-04-09 11:04:20.785162: Steps 85/90: batch_recall = 42.15, batch_ndcg = 24.79 
2025-04-09 11:04:21.482984: Steps 86/90: batch_recall = 44.61, batch_ndcg = 30.26 
2025-04-09 11:04:22.185276: Steps 87/90: batch_recall = 45.03, batch_ndcg = 29.62 
2025-04-09 11:04:22.881616: Steps 88/90: batch_recall = 50.60, batch_ndcg = 30.18 
2025-04-09 11:04:23.449958: Steps 89/90: batch_recall = 38.84, batch_ndcg = 25.64 
2025-04-09 11:04:23.450668: Epoch 43/1000, Test: Recall = 0.0762, NDCG = 0.0510  

2025-04-09 11:04:24.472765: Training Step 0/115: batchLoss = 4.0380, diffLoss = 6.6145, kgLoss = 0.1734
2025-04-09 11:04:25.204628: Training Step 1/115: batchLoss = 4.1490, diffLoss = 6.8027, kgLoss = 0.1684
2025-04-09 11:04:25.952565: Training Step 2/115: batchLoss = 4.1360, diffLoss = 6.7803, kgLoss = 0.1697
2025-04-09 11:04:26.697800: Training Step 3/115: batchLoss = 4.2282, diffLoss = 6.9273, kgLoss = 0.1796
2025-04-09 11:04:27.423034: Training Step 4/115: batchLoss = 4.3350, diffLoss = 7.1050, kgLoss = 0.1800
2025-04-09 11:04:28.156852: Training Step 5/115: batchLoss = 4.1492, diffLoss = 6.7937, kgLoss = 0.1824
2025-04-09 11:04:28.893552: Training Step 6/115: batchLoss = 4.2214, diffLoss = 6.9184, kgLoss = 0.1759
2025-04-09 11:04:29.635243: Training Step 7/115: batchLoss = 4.1076, diffLoss = 6.7383, kgLoss = 0.1614
2025-04-09 11:04:30.370272: Training Step 8/115: batchLoss = 3.9016, diffLoss = 6.3750, kgLoss = 0.1916
2025-04-09 11:04:31.103718: Training Step 9/115: batchLoss = 4.1582, diffLoss = 6.8119, kgLoss = 0.1776
2025-04-09 11:04:31.839312: Training Step 10/115: batchLoss = 3.9826, diffLoss = 6.5262, kgLoss = 0.1672
2025-04-09 11:04:32.571961: Training Step 11/115: batchLoss = 3.8048, diffLoss = 6.2310, kgLoss = 0.1656
2025-04-09 11:04:33.314531: Training Step 12/115: batchLoss = 4.3772, diffLoss = 7.1661, kgLoss = 0.1938
2025-04-09 11:04:34.049597: Training Step 13/115: batchLoss = 4.1212, diffLoss = 6.7582, kgLoss = 0.1658
2025-04-09 11:04:34.787880: Training Step 14/115: batchLoss = 4.7638, diffLoss = 7.8077, kgLoss = 0.1979
2025-04-09 11:04:35.522701: Training Step 15/115: batchLoss = 4.2056, diffLoss = 6.8908, kgLoss = 0.1778
2025-04-09 11:04:36.270139: Training Step 16/115: batchLoss = 3.8109, diffLoss = 6.2452, kgLoss = 0.1596
2025-04-09 11:04:37.016551: Training Step 17/115: batchLoss = 4.4610, diffLoss = 7.3157, kgLoss = 0.1789
2025-04-09 11:04:37.759927: Training Step 18/115: batchLoss = 4.1422, diffLoss = 6.7852, kgLoss = 0.1776
2025-04-09 11:04:38.493834: Training Step 19/115: batchLoss = 4.1173, diffLoss = 6.7441, kgLoss = 0.1771
2025-04-09 11:04:39.231666: Training Step 20/115: batchLoss = 4.1144, diffLoss = 6.7471, kgLoss = 0.1654
2025-04-09 11:04:39.968132: Training Step 21/115: batchLoss = 4.3222, diffLoss = 7.0692, kgLoss = 0.2018
2025-04-09 11:04:40.707991: Training Step 22/115: batchLoss = 3.9344, diffLoss = 6.4465, kgLoss = 0.1663
2025-04-09 11:04:41.440363: Training Step 23/115: batchLoss = 4.4204, diffLoss = 7.2483, kgLoss = 0.1786
2025-04-09 11:04:42.174962: Training Step 24/115: batchLoss = 3.9637, diffLoss = 6.4876, kgLoss = 0.1777
2025-04-09 11:04:42.912169: Training Step 25/115: batchLoss = 3.9811, diffLoss = 6.5219, kgLoss = 0.1699
2025-04-09 11:04:43.647813: Training Step 26/115: batchLoss = 4.0242, diffLoss = 6.5922, kgLoss = 0.1722
2025-04-09 11:04:44.380800: Training Step 27/115: batchLoss = 4.3979, diffLoss = 7.2068, kgLoss = 0.1845
2025-04-09 11:04:45.117721: Training Step 28/115: batchLoss = 4.2203, diffLoss = 6.9151, kgLoss = 0.1781
2025-04-09 11:04:45.853998: Training Step 29/115: batchLoss = 4.0499, diffLoss = 6.6387, kgLoss = 0.1668
2025-04-09 11:04:46.588792: Training Step 30/115: batchLoss = 3.6809, diffLoss = 6.0258, kgLoss = 0.1635
2025-04-09 11:04:47.324770: Training Step 31/115: batchLoss = 4.1715, diffLoss = 6.8381, kgLoss = 0.1716
2025-04-09 11:04:48.052411: Training Step 32/115: batchLoss = 3.6882, diffLoss = 6.0414, kgLoss = 0.1583
2025-04-09 11:04:48.784087: Training Step 33/115: batchLoss = 3.6022, diffLoss = 5.8978, kgLoss = 0.1588
2025-04-09 11:04:49.510553: Training Step 34/115: batchLoss = 4.0512, diffLoss = 6.6360, kgLoss = 0.1741
2025-04-09 11:04:50.245344: Training Step 35/115: batchLoss = 4.0828, diffLoss = 6.6911, kgLoss = 0.1702
2025-04-09 11:04:50.982710: Training Step 36/115: batchLoss = 3.8982, diffLoss = 6.3820, kgLoss = 0.1725
2025-04-09 11:04:51.718291: Training Step 37/115: batchLoss = 4.6642, diffLoss = 7.6477, kgLoss = 0.1890
2025-04-09 11:04:52.463019: Training Step 38/115: batchLoss = 3.7144, diffLoss = 6.0820, kgLoss = 0.1629
2025-04-09 11:04:53.210760: Training Step 39/115: batchLoss = 4.0755, diffLoss = 6.6768, kgLoss = 0.1735
2025-04-09 11:04:53.956155: Training Step 40/115: batchLoss = 3.9481, diffLoss = 6.4596, kgLoss = 0.1808
2025-04-09 11:04:54.702559: Training Step 41/115: batchLoss = 3.9586, diffLoss = 6.4919, kgLoss = 0.1586
2025-04-09 11:04:55.450546: Training Step 42/115: batchLoss = 3.9594, diffLoss = 6.4877, kgLoss = 0.1670
2025-04-09 11:04:56.185662: Training Step 43/115: batchLoss = 3.9901, diffLoss = 6.5322, kgLoss = 0.1768
2025-04-09 11:04:56.938804: Training Step 44/115: batchLoss = 4.2458, diffLoss = 6.9615, kgLoss = 0.1723
2025-04-09 11:04:57.683938: Training Step 45/115: batchLoss = 4.1564, diffLoss = 6.8127, kgLoss = 0.1719
2025-04-09 11:04:58.435203: Training Step 46/115: batchLoss = 4.0250, diffLoss = 6.5929, kgLoss = 0.1731
2025-04-09 11:04:59.166699: Training Step 47/115: batchLoss = 3.7691, diffLoss = 6.1733, kgLoss = 0.1629
2025-04-09 11:04:59.904033: Training Step 48/115: batchLoss = 3.4884, diffLoss = 5.7074, kgLoss = 0.1599
2025-04-09 11:05:00.645567: Training Step 49/115: batchLoss = 4.2556, diffLoss = 6.9780, kgLoss = 0.1719
2025-04-09 11:05:01.382957: Training Step 50/115: batchLoss = 3.9521, diffLoss = 6.4703, kgLoss = 0.1748
2025-04-09 11:05:02.113828: Training Step 51/115: batchLoss = 3.8932, diffLoss = 6.3767, kgLoss = 0.1680
2025-04-09 11:05:02.844556: Training Step 52/115: batchLoss = 4.0866, diffLoss = 6.6980, kgLoss = 0.1697
2025-04-09 11:05:03.596192: Training Step 53/115: batchLoss = 4.3588, diffLoss = 7.1384, kgLoss = 0.1894
2025-04-09 11:05:04.328611: Training Step 54/115: batchLoss = 3.6876, diffLoss = 6.0378, kgLoss = 0.1625
2025-04-09 11:05:05.062593: Training Step 55/115: batchLoss = 3.8739, diffLoss = 6.3436, kgLoss = 0.1694
2025-04-09 11:05:05.806043: Training Step 56/115: batchLoss = 4.3556, diffLoss = 7.1497, kgLoss = 0.1643
2025-04-09 11:05:06.557949: Training Step 57/115: batchLoss = 3.6945, diffLoss = 6.0471, kgLoss = 0.1656
2025-04-09 11:05:07.295257: Training Step 58/115: batchLoss = 4.0881, diffLoss = 6.7003, kgLoss = 0.1698
2025-04-09 11:05:08.035995: Training Step 59/115: batchLoss = 4.3625, diffLoss = 7.1504, kgLoss = 0.1805
2025-04-09 11:05:08.773762: Training Step 60/115: batchLoss = 4.1365, diffLoss = 6.7829, kgLoss = 0.1668
2025-04-09 11:05:09.530282: Training Step 61/115: batchLoss = 3.7573, diffLoss = 6.1528, kgLoss = 0.1640
2025-04-09 11:05:10.289794: Training Step 62/115: batchLoss = 4.3279, diffLoss = 7.0887, kgLoss = 0.1866
2025-04-09 11:05:11.148492: Training Step 63/115: batchLoss = 4.2176, diffLoss = 6.9130, kgLoss = 0.1745
2025-04-09 11:05:11.881279: Training Step 64/115: batchLoss = 3.8627, diffLoss = 6.3266, kgLoss = 0.1668
2025-04-09 11:05:12.627058: Training Step 65/115: batchLoss = 4.1477, diffLoss = 6.7999, kgLoss = 0.1693
2025-04-09 11:05:13.374839: Training Step 66/115: batchLoss = 4.9003, diffLoss = 8.0338, kgLoss = 0.1999
2025-04-09 11:05:14.118933: Training Step 67/115: batchLoss = 4.4366, diffLoss = 7.2725, kgLoss = 0.1826
2025-04-09 11:05:14.855044: Training Step 68/115: batchLoss = 4.3723, diffLoss = 7.1652, kgLoss = 0.1830
2025-04-09 11:05:15.585111: Training Step 69/115: batchLoss = 4.2411, diffLoss = 6.9559, kgLoss = 0.1689
2025-04-09 11:05:16.319417: Training Step 70/115: batchLoss = 3.6695, diffLoss = 6.0149, kgLoss = 0.1514
2025-04-09 11:05:17.050740: Training Step 71/115: batchLoss = 4.3248, diffLoss = 7.0846, kgLoss = 0.1852
2025-04-09 11:05:17.781323: Training Step 72/115: batchLoss = 4.3514, diffLoss = 7.1388, kgLoss = 0.1703
2025-04-09 11:05:18.504959: Training Step 73/115: batchLoss = 4.2948, diffLoss = 7.0372, kgLoss = 0.1812
2025-04-09 11:05:19.236490: Training Step 74/115: batchLoss = 3.4516, diffLoss = 5.6500, kgLoss = 0.1540
2025-04-09 11:05:19.973224: Training Step 75/115: batchLoss = 4.7644, diffLoss = 7.8141, kgLoss = 0.1898
2025-04-09 11:05:20.711722: Training Step 76/115: batchLoss = 4.1505, diffLoss = 6.7955, kgLoss = 0.1830
2025-04-09 11:05:21.436335: Training Step 77/115: batchLoss = 4.4776, diffLoss = 7.3396, kgLoss = 0.1847
2025-04-09 11:05:22.167411: Training Step 78/115: batchLoss = 3.6718, diffLoss = 6.0148, kgLoss = 0.1573
2025-04-09 11:05:22.902314: Training Step 79/115: batchLoss = 4.2045, diffLoss = 6.8934, kgLoss = 0.1712
2025-04-09 11:05:23.650117: Training Step 80/115: batchLoss = 4.0978, diffLoss = 6.7143, kgLoss = 0.1730
2025-04-09 11:05:24.399496: Training Step 81/115: batchLoss = 4.1132, diffLoss = 6.7432, kgLoss = 0.1683
2025-04-09 11:05:25.156917: Training Step 82/115: batchLoss = 4.3265, diffLoss = 7.0873, kgLoss = 0.1853
2025-04-09 11:05:25.896915: Training Step 83/115: batchLoss = 3.6965, diffLoss = 6.0552, kgLoss = 0.1584
2025-04-09 11:05:26.637398: Training Step 84/115: batchLoss = 4.2262, diffLoss = 6.9196, kgLoss = 0.1861
2025-04-09 11:05:27.377838: Training Step 85/115: batchLoss = 3.8597, diffLoss = 6.3217, kgLoss = 0.1667
2025-04-09 11:05:28.116813: Training Step 86/115: batchLoss = 4.1044, diffLoss = 6.7291, kgLoss = 0.1673
2025-04-09 11:05:28.852419: Training Step 87/115: batchLoss = 4.3121, diffLoss = 7.0663, kgLoss = 0.1807
2025-04-09 11:05:29.588869: Training Step 88/115: batchLoss = 4.4098, diffLoss = 7.2274, kgLoss = 0.1833
2025-04-09 11:05:30.337513: Training Step 89/115: batchLoss = 4.2387, diffLoss = 6.9483, kgLoss = 0.1741
2025-04-09 11:05:31.171800: Training Step 90/115: batchLoss = 3.9319, diffLoss = 6.4451, kgLoss = 0.1622
2025-04-09 11:05:31.913704: Training Step 91/115: batchLoss = 4.5806, diffLoss = 7.5115, kgLoss = 0.1841
2025-04-09 11:05:32.660994: Training Step 92/115: batchLoss = 4.2807, diffLoss = 7.0171, kgLoss = 0.1763
2025-04-09 11:05:33.382083: Training Step 93/115: batchLoss = 4.1872, diffLoss = 6.8516, kgLoss = 0.1905
2025-04-09 11:05:34.088170: Training Step 94/115: batchLoss = 4.1247, diffLoss = 6.7603, kgLoss = 0.1712
2025-04-09 11:05:34.829253: Training Step 95/115: batchLoss = 4.0183, diffLoss = 6.5891, kgLoss = 0.1620
2025-04-09 11:05:35.562525: Training Step 96/115: batchLoss = 4.5038, diffLoss = 7.3744, kgLoss = 0.1980
2025-04-09 11:05:36.297905: Training Step 97/115: batchLoss = 4.4031, diffLoss = 7.2210, kgLoss = 0.1762
2025-04-09 11:05:37.028771: Training Step 98/115: batchLoss = 3.8040, diffLoss = 6.2348, kgLoss = 0.1578
2025-04-09 11:05:37.762539: Training Step 99/115: batchLoss = 4.2232, diffLoss = 6.9214, kgLoss = 0.1759
2025-04-09 11:05:38.518921: Training Step 100/115: batchLoss = 3.8760, diffLoss = 6.3449, kgLoss = 0.1725
2025-04-09 11:05:39.255880: Training Step 101/115: batchLoss = 4.0688, diffLoss = 6.6730, kgLoss = 0.1626
2025-04-09 11:05:39.983603: Training Step 102/115: batchLoss = 4.3092, diffLoss = 7.0679, kgLoss = 0.1712
2025-04-09 11:05:40.717878: Training Step 103/115: batchLoss = 3.8861, diffLoss = 6.3675, kgLoss = 0.1640
2025-04-09 11:05:41.466026: Training Step 104/115: batchLoss = 3.9809, diffLoss = 6.5209, kgLoss = 0.1709
2025-04-09 11:05:42.213090: Training Step 105/115: batchLoss = 4.2113, diffLoss = 6.8921, kgLoss = 0.1900
2025-04-09 11:05:42.966231: Training Step 106/115: batchLoss = 4.7162, diffLoss = 7.7288, kgLoss = 0.1972
2025-04-09 11:05:43.709577: Training Step 107/115: batchLoss = 3.9409, diffLoss = 6.4590, kgLoss = 0.1636
2025-04-09 11:05:44.448012: Training Step 108/115: batchLoss = 3.8422, diffLoss = 6.2947, kgLoss = 0.1633
2025-04-09 11:05:45.206564: Training Step 109/115: batchLoss = 3.9540, diffLoss = 6.4795, kgLoss = 0.1657
2025-04-09 11:05:45.938328: Training Step 110/115: batchLoss = 4.3606, diffLoss = 7.1435, kgLoss = 0.1862
2025-04-09 11:05:46.670578: Training Step 111/115: batchLoss = 4.2001, diffLoss = 6.8824, kgLoss = 0.1768
2025-04-09 11:05:47.395455: Training Step 112/115: batchLoss = 3.7869, diffLoss = 6.2053, kgLoss = 0.1593
2025-04-09 11:05:48.049102: Training Step 113/115: batchLoss = 3.5451, diffLoss = 5.8084, kgLoss = 0.1501
2025-04-09 11:05:48.666184: Training Step 114/115: batchLoss = 4.7213, diffLoss = 7.7333, kgLoss = 0.2033
2025-04-09 11:05:48.786133: 
2025-04-09 11:05:48.787240: Epoch 44/1000, Train: epLoss = 1.1826, epDfLoss = 1.9376, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:05:49.587077: Steps 0/90: batch_recall = 57.14, batch_ndcg = 43.98 
2025-04-09 11:05:50.350471: Steps 1/90: batch_recall = 57.45, batch_ndcg = 40.92 
2025-04-09 11:05:51.131539: Steps 2/90: batch_recall = 49.66, batch_ndcg = 37.41 
2025-04-09 11:05:51.885101: Steps 3/90: batch_recall = 44.08, batch_ndcg = 32.13 
2025-04-09 11:05:52.654589: Steps 4/90: batch_recall = 49.05, batch_ndcg = 35.79 
2025-04-09 11:05:53.424198: Steps 5/90: batch_recall = 39.14, batch_ndcg = 28.15 
2025-04-09 11:05:54.207938: Steps 6/90: batch_recall = 46.49, batch_ndcg = 32.25 
2025-04-09 11:05:54.989159: Steps 7/90: batch_recall = 44.48, batch_ndcg = 31.93 
2025-04-09 11:05:55.738561: Steps 8/90: batch_recall = 46.73, batch_ndcg = 34.50 
2025-04-09 11:05:56.513152: Steps 9/90: batch_recall = 44.00, batch_ndcg = 33.40 
2025-04-09 11:05:57.261237: Steps 10/90: batch_recall = 39.35, batch_ndcg = 30.09 
2025-04-09 11:05:57.993543: Steps 11/90: batch_recall = 42.37, batch_ndcg = 29.15 
2025-04-09 11:05:58.730597: Steps 12/90: batch_recall = 36.46, batch_ndcg = 26.65 
2025-04-09 11:05:59.482057: Steps 13/90: batch_recall = 36.28, batch_ndcg = 26.05 
2025-04-09 11:06:00.231338: Steps 14/90: batch_recall = 33.24, batch_ndcg = 24.27 
2025-04-09 11:06:00.973851: Steps 15/90: batch_recall = 45.54, batch_ndcg = 30.57 
2025-04-09 11:06:01.710252: Steps 16/90: batch_recall = 36.16, batch_ndcg = 25.95 
2025-04-09 11:06:02.450718: Steps 17/90: batch_recall = 31.64, batch_ndcg = 21.25 
2025-04-09 11:06:03.183517: Steps 18/90: batch_recall = 37.56, batch_ndcg = 26.17 
2025-04-09 11:06:03.928424: Steps 19/90: batch_recall = 32.21, batch_ndcg = 23.12 
2025-04-09 11:06:04.668990: Steps 20/90: batch_recall = 40.11, batch_ndcg = 27.32 
2025-04-09 11:06:05.401435: Steps 21/90: batch_recall = 42.97, batch_ndcg = 30.47 
2025-04-09 11:06:06.116597: Steps 22/90: batch_recall = 38.84, batch_ndcg = 27.44 
2025-04-09 11:06:06.833262: Steps 23/90: batch_recall = 39.89, batch_ndcg = 27.37 
2025-04-09 11:06:07.546585: Steps 24/90: batch_recall = 39.08, batch_ndcg = 28.08 
2025-04-09 11:06:08.266495: Steps 25/90: batch_recall = 37.84, batch_ndcg = 24.95 
2025-04-09 11:06:08.997218: Steps 26/90: batch_recall = 38.76, batch_ndcg = 27.80 
2025-04-09 11:06:09.715059: Steps 27/90: batch_recall = 33.60, batch_ndcg = 23.34 
2025-04-09 11:06:10.443613: Steps 28/90: batch_recall = 39.54, batch_ndcg = 26.30 
2025-04-09 11:06:11.182472: Steps 29/90: batch_recall = 33.15, batch_ndcg = 23.18 
2025-04-09 11:06:11.906091: Steps 30/90: batch_recall = 34.72, batch_ndcg = 23.95 
2025-04-09 11:06:12.645377: Steps 31/90: batch_recall = 31.70, batch_ndcg = 23.32 
2025-04-09 11:06:13.374112: Steps 32/90: batch_recall = 35.00, batch_ndcg = 23.88 
2025-04-09 11:06:14.106151: Steps 33/90: batch_recall = 35.02, batch_ndcg = 21.60 
2025-04-09 11:06:14.825230: Steps 34/90: batch_recall = 34.85, batch_ndcg = 23.64 
2025-04-09 11:06:15.544544: Steps 35/90: batch_recall = 38.02, batch_ndcg = 27.00 
2025-04-09 11:06:16.261528: Steps 36/90: batch_recall = 36.98, batch_ndcg = 24.83 
2025-04-09 11:06:16.984870: Steps 37/90: batch_recall = 33.16, batch_ndcg = 21.10 
2025-04-09 11:06:17.698131: Steps 38/90: batch_recall = 36.61, batch_ndcg = 23.60 
2025-04-09 11:06:18.407615: Steps 39/90: batch_recall = 38.21, batch_ndcg = 23.81 
2025-04-09 11:06:19.111001: Steps 40/90: batch_recall = 31.43, batch_ndcg = 22.18 
2025-04-09 11:06:19.811114: Steps 41/90: batch_recall = 33.08, batch_ndcg = 21.05 
2025-04-09 11:06:20.529985: Steps 42/90: batch_recall = 43.95, batch_ndcg = 28.88 
2025-04-09 11:06:21.235034: Steps 43/90: batch_recall = 36.36, batch_ndcg = 22.71 
2025-04-09 11:06:21.939003: Steps 44/90: batch_recall = 34.39, batch_ndcg = 23.22 
2025-04-09 11:06:22.644941: Steps 45/90: batch_recall = 33.25, batch_ndcg = 22.56 
2025-04-09 11:06:23.368671: Steps 46/90: batch_recall = 34.74, batch_ndcg = 22.98 
2025-04-09 11:06:24.077482: Steps 47/90: batch_recall = 32.92, batch_ndcg = 20.55 
2025-04-09 11:06:24.781248: Steps 48/90: batch_recall = 38.32, batch_ndcg = 24.61 
2025-04-09 11:06:25.498807: Steps 49/90: batch_recall = 30.35, batch_ndcg = 18.91 
2025-04-09 11:06:26.207073: Steps 50/90: batch_recall = 41.82, batch_ndcg = 26.97 
2025-04-09 11:06:26.924355: Steps 51/90: batch_recall = 39.66, batch_ndcg = 25.57 
2025-04-09 11:06:27.667913: Steps 52/90: batch_recall = 39.16, batch_ndcg = 25.20 
2025-04-09 11:06:28.379663: Steps 53/90: batch_recall = 41.71, batch_ndcg = 26.53 
2025-04-09 11:06:29.101412: Steps 54/90: batch_recall = 32.60, batch_ndcg = 21.14 
2025-04-09 11:06:29.804983: Steps 55/90: batch_recall = 36.74, batch_ndcg = 23.43 
2025-04-09 11:06:30.529891: Steps 56/90: batch_recall = 33.93, batch_ndcg = 20.58 
2025-04-09 11:06:31.250070: Steps 57/90: batch_recall = 36.19, batch_ndcg = 24.35 
2025-04-09 11:06:31.965886: Steps 58/90: batch_recall = 37.21, batch_ndcg = 22.93 
2025-04-09 11:06:32.663290: Steps 59/90: batch_recall = 36.16, batch_ndcg = 23.80 
2025-04-09 11:06:33.367022: Steps 60/90: batch_recall = 34.40, batch_ndcg = 22.50 
2025-04-09 11:06:34.063772: Steps 61/90: batch_recall = 40.03, batch_ndcg = 27.86 
2025-04-09 11:06:34.758115: Steps 62/90: batch_recall = 38.39, batch_ndcg = 25.16 
2025-04-09 11:06:35.463876: Steps 63/90: batch_recall = 32.81, batch_ndcg = 23.30 
2025-04-09 11:06:36.160991: Steps 64/90: batch_recall = 35.06, batch_ndcg = 23.13 
2025-04-09 11:06:36.858697: Steps 65/90: batch_recall = 33.93, batch_ndcg = 22.47 
2025-04-09 11:06:37.565510: Steps 66/90: batch_recall = 37.50, batch_ndcg = 22.75 
2025-04-09 11:06:38.283680: Steps 67/90: batch_recall = 32.67, batch_ndcg = 20.10 
2025-04-09 11:06:39.008327: Steps 68/90: batch_recall = 37.04, batch_ndcg = 23.28 
2025-04-09 11:06:39.726562: Steps 69/90: batch_recall = 40.91, batch_ndcg = 25.26 
2025-04-09 11:06:40.435825: Steps 70/90: batch_recall = 38.86, batch_ndcg = 24.87 
2025-04-09 11:06:41.134918: Steps 71/90: batch_recall = 39.11, batch_ndcg = 24.19 
2025-04-09 11:06:41.846032: Steps 72/90: batch_recall = 33.05, batch_ndcg = 21.09 
2025-04-09 11:06:42.547431: Steps 73/90: batch_recall = 34.25, batch_ndcg = 21.70 
2025-04-09 11:06:43.245023: Steps 74/90: batch_recall = 40.62, batch_ndcg = 25.67 
2025-04-09 11:06:43.951914: Steps 75/90: batch_recall = 37.09, batch_ndcg = 22.79 
2025-04-09 11:06:44.663892: Steps 76/90: batch_recall = 40.62, batch_ndcg = 25.49 
2025-04-09 11:06:45.357464: Steps 77/90: batch_recall = 44.23, batch_ndcg = 27.85 
2025-04-09 11:06:46.078896: Steps 78/90: batch_recall = 34.35, batch_ndcg = 21.12 
2025-04-09 11:06:46.794277: Steps 79/90: batch_recall = 40.11, batch_ndcg = 26.12 
2025-04-09 11:06:47.505104: Steps 80/90: batch_recall = 46.09, batch_ndcg = 29.73 
2025-04-09 11:06:48.200093: Steps 81/90: batch_recall = 43.93, batch_ndcg = 27.98 
2025-04-09 11:06:48.908959: Steps 82/90: batch_recall = 44.49, batch_ndcg = 28.05 
2025-04-09 11:06:49.597922: Steps 83/90: batch_recall = 45.75, batch_ndcg = 27.73 
2025-04-09 11:06:50.308219: Steps 84/90: batch_recall = 35.25, batch_ndcg = 22.25 
2025-04-09 11:06:51.011288: Steps 85/90: batch_recall = 42.53, batch_ndcg = 24.99 
2025-04-09 11:06:51.721797: Steps 86/90: batch_recall = 45.28, batch_ndcg = 30.57 
2025-04-09 11:06:52.410936: Steps 87/90: batch_recall = 44.45, batch_ndcg = 29.13 
2025-04-09 11:06:53.104083: Steps 88/90: batch_recall = 50.77, batch_ndcg = 30.53 
2025-04-09 11:06:53.651951: Steps 89/90: batch_recall = 39.75, batch_ndcg = 26.20 
2025-04-09 11:06:53.652708: Epoch 44/1000, Test: Recall = 0.0761, NDCG = 0.0509  

2025-04-09 11:06:54.670022: Training Step 0/115: batchLoss = 3.7104, diffLoss = 6.0819, kgLoss = 0.1533
2025-04-09 11:06:55.404016: Training Step 1/115: batchLoss = 3.6583, diffLoss = 5.9848, kgLoss = 0.1685
2025-04-09 11:06:56.132978: Training Step 2/115: batchLoss = 4.3667, diffLoss = 7.1629, kgLoss = 0.1724
2025-04-09 11:06:56.871172: Training Step 3/115: batchLoss = 4.1589, diffLoss = 6.8194, kgLoss = 0.1681
2025-04-09 11:06:57.604471: Training Step 4/115: batchLoss = 4.4219, diffLoss = 7.2429, kgLoss = 0.1904
2025-04-09 11:06:58.333342: Training Step 5/115: batchLoss = 4.3050, diffLoss = 7.0596, kgLoss = 0.1731
2025-04-09 11:06:59.075361: Training Step 6/115: batchLoss = 4.4380, diffLoss = 7.2754, kgLoss = 0.1820
2025-04-09 11:06:59.804764: Training Step 7/115: batchLoss = 4.4272, diffLoss = 7.2625, kgLoss = 0.1742
2025-04-09 11:07:00.535920: Training Step 8/115: batchLoss = 4.3563, diffLoss = 7.1449, kgLoss = 0.1733
2025-04-09 11:07:01.267347: Training Step 9/115: batchLoss = 3.9106, diffLoss = 6.4161, kgLoss = 0.1523
2025-04-09 11:07:01.994510: Training Step 10/115: batchLoss = 4.0590, diffLoss = 6.6514, kgLoss = 0.1705
2025-04-09 11:07:02.713411: Training Step 11/115: batchLoss = 4.1099, diffLoss = 6.7384, kgLoss = 0.1672
2025-04-09 11:07:03.437447: Training Step 12/115: batchLoss = 3.6261, diffLoss = 5.9389, kgLoss = 0.1569
2025-04-09 11:07:04.165975: Training Step 13/115: batchLoss = 4.4153, diffLoss = 7.2334, kgLoss = 0.1881
2025-04-09 11:07:04.889416: Training Step 14/115: batchLoss = 4.0230, diffLoss = 6.5925, kgLoss = 0.1688
2025-04-09 11:07:05.613583: Training Step 15/115: batchLoss = 4.0822, diffLoss = 6.6899, kgLoss = 0.1705
2025-04-09 11:07:06.337799: Training Step 16/115: batchLoss = 4.1922, diffLoss = 6.8714, kgLoss = 0.1734
2025-04-09 11:07:07.067384: Training Step 17/115: batchLoss = 3.8992, diffLoss = 6.3893, kgLoss = 0.1642
2025-04-09 11:07:07.789332: Training Step 18/115: batchLoss = 4.3716, diffLoss = 7.1621, kgLoss = 0.1857
2025-04-09 11:07:08.515834: Training Step 19/115: batchLoss = 4.5345, diffLoss = 7.4248, kgLoss = 0.1989
2025-04-09 11:07:09.244383: Training Step 20/115: batchLoss = 4.4280, diffLoss = 7.2558, kgLoss = 0.1865
2025-04-09 11:07:09.973202: Training Step 21/115: batchLoss = 4.0679, diffLoss = 6.6499, kgLoss = 0.1949
2025-04-09 11:07:10.703297: Training Step 22/115: batchLoss = 4.2759, diffLoss = 7.0094, kgLoss = 0.1756
2025-04-09 11:07:11.433101: Training Step 23/115: batchLoss = 4.3506, diffLoss = 7.1349, kgLoss = 0.1742
2025-04-09 11:07:12.152715: Training Step 24/115: batchLoss = 4.0378, diffLoss = 6.6148, kgLoss = 0.1725
2025-04-09 11:07:12.875523: Training Step 25/115: batchLoss = 4.2567, diffLoss = 6.9730, kgLoss = 0.1823
2025-04-09 11:07:13.601728: Training Step 26/115: batchLoss = 3.9691, diffLoss = 6.4995, kgLoss = 0.1734
2025-04-09 11:07:14.337005: Training Step 27/115: batchLoss = 3.5913, diffLoss = 5.8784, kgLoss = 0.1606
2025-04-09 11:07:15.073411: Training Step 28/115: batchLoss = 4.0356, diffLoss = 6.6200, kgLoss = 0.1590
2025-04-09 11:07:15.800153: Training Step 29/115: batchLoss = 3.9094, diffLoss = 6.4082, kgLoss = 0.1611
2025-04-09 11:07:16.522079: Training Step 30/115: batchLoss = 3.7388, diffLoss = 6.1197, kgLoss = 0.1674
2025-04-09 11:07:17.245040: Training Step 31/115: batchLoss = 4.4227, diffLoss = 7.2497, kgLoss = 0.1822
2025-04-09 11:07:17.966168: Training Step 32/115: batchLoss = 3.8357, diffLoss = 6.2873, kgLoss = 0.1583
2025-04-09 11:07:18.678579: Training Step 33/115: batchLoss = 4.0437, diffLoss = 6.6257, kgLoss = 0.1706
2025-04-09 11:07:19.401858: Training Step 34/115: batchLoss = 4.0212, diffLoss = 6.5880, kgLoss = 0.1710
2025-04-09 11:07:20.114690: Training Step 35/115: batchLoss = 3.9776, diffLoss = 6.5140, kgLoss = 0.1729
2025-04-09 11:07:20.841765: Training Step 36/115: batchLoss = 4.3735, diffLoss = 7.1647, kgLoss = 0.1867
2025-04-09 11:07:21.560928: Training Step 37/115: batchLoss = 4.0496, diffLoss = 6.6375, kgLoss = 0.1678
2025-04-09 11:07:22.284693: Training Step 38/115: batchLoss = 4.5135, diffLoss = 7.3961, kgLoss = 0.1897
2025-04-09 11:07:23.018229: Training Step 39/115: batchLoss = 4.2587, diffLoss = 6.9720, kgLoss = 0.1888
2025-04-09 11:07:23.750755: Training Step 40/115: batchLoss = 4.4600, diffLoss = 7.3159, kgLoss = 0.1762
2025-04-09 11:07:24.496170: Training Step 41/115: batchLoss = 3.9865, diffLoss = 6.5331, kgLoss = 0.1667
2025-04-09 11:07:25.232551: Training Step 42/115: batchLoss = 4.2522, diffLoss = 6.9733, kgLoss = 0.1706
2025-04-09 11:07:25.974554: Training Step 43/115: batchLoss = 4.3292, diffLoss = 7.0891, kgLoss = 0.1892
2025-04-09 11:07:26.714969: Training Step 44/115: batchLoss = 3.9885, diffLoss = 6.5367, kgLoss = 0.1662
2025-04-09 11:07:27.462691: Training Step 45/115: batchLoss = 3.8560, diffLoss = 6.3221, kgLoss = 0.1567
2025-04-09 11:07:28.201862: Training Step 46/115: batchLoss = 4.2718, diffLoss = 6.9913, kgLoss = 0.1926
2025-04-09 11:07:28.941898: Training Step 47/115: batchLoss = 3.5964, diffLoss = 5.8928, kgLoss = 0.1518
2025-04-09 11:07:29.678169: Training Step 48/115: batchLoss = 3.5669, diffLoss = 5.8334, kgLoss = 0.1670
2025-04-09 11:07:30.415869: Training Step 49/115: batchLoss = 4.0582, diffLoss = 6.6526, kgLoss = 0.1667
2025-04-09 11:07:31.145546: Training Step 50/115: batchLoss = 4.1777, diffLoss = 6.8442, kgLoss = 0.1778
2025-04-09 11:07:31.880356: Training Step 51/115: batchLoss = 4.2664, diffLoss = 6.9936, kgLoss = 0.1756
2025-04-09 11:07:32.611073: Training Step 52/115: batchLoss = 4.0226, diffLoss = 6.5882, kgLoss = 0.1743
2025-04-09 11:07:33.356813: Training Step 53/115: batchLoss = 4.1170, diffLoss = 6.7467, kgLoss = 0.1723
2025-04-09 11:07:34.078865: Training Step 54/115: batchLoss = 4.2017, diffLoss = 6.8888, kgLoss = 0.1712
2025-04-09 11:07:34.807600: Training Step 55/115: batchLoss = 4.2944, diffLoss = 7.0391, kgLoss = 0.1774
2025-04-09 11:07:35.544664: Training Step 56/115: batchLoss = 4.4842, diffLoss = 7.3485, kgLoss = 0.1877
2025-04-09 11:07:36.280357: Training Step 57/115: batchLoss = 4.2292, diffLoss = 6.9306, kgLoss = 0.1771
2025-04-09 11:07:37.010876: Training Step 58/115: batchLoss = 4.0945, diffLoss = 6.7034, kgLoss = 0.1811
2025-04-09 11:07:37.752726: Training Step 59/115: batchLoss = 3.7094, diffLoss = 6.0749, kgLoss = 0.1610
2025-04-09 11:07:38.495000: Training Step 60/115: batchLoss = 4.1381, diffLoss = 6.7777, kgLoss = 0.1788
2025-04-09 11:07:39.231794: Training Step 61/115: batchLoss = 3.8479, diffLoss = 6.3029, kgLoss = 0.1654
2025-04-09 11:07:39.968446: Training Step 62/115: batchLoss = 3.9289, diffLoss = 6.4353, kgLoss = 0.1694
2025-04-09 11:07:40.704729: Training Step 63/115: batchLoss = 3.8216, diffLoss = 6.2610, kgLoss = 0.1624
2025-04-09 11:07:41.446976: Training Step 64/115: batchLoss = 4.1788, diffLoss = 6.8527, kgLoss = 0.1678
2025-04-09 11:07:42.190066: Training Step 65/115: batchLoss = 4.1893, diffLoss = 6.8676, kgLoss = 0.1718
2025-04-09 11:07:42.935545: Training Step 66/115: batchLoss = 3.9913, diffLoss = 6.5332, kgLoss = 0.1785
2025-04-09 11:07:43.681353: Training Step 67/115: batchLoss = 4.0782, diffLoss = 6.6860, kgLoss = 0.1665
2025-04-09 11:07:44.430182: Training Step 68/115: batchLoss = 4.1772, diffLoss = 6.8459, kgLoss = 0.1741
2025-04-09 11:07:45.178444: Training Step 69/115: batchLoss = 3.9873, diffLoss = 6.5321, kgLoss = 0.1701
2025-04-09 11:07:45.931981: Training Step 70/115: batchLoss = 4.0252, diffLoss = 6.5928, kgLoss = 0.1739
2025-04-09 11:07:46.685878: Training Step 71/115: batchLoss = 4.4275, diffLoss = 7.2430, kgLoss = 0.2042
2025-04-09 11:07:47.430058: Training Step 72/115: batchLoss = 3.8814, diffLoss = 6.3665, kgLoss = 0.1538
2025-04-09 11:07:48.166541: Training Step 73/115: batchLoss = 3.7584, diffLoss = 6.1548, kgLoss = 0.1639
2025-04-09 11:07:48.906254: Training Step 74/115: batchLoss = 3.5787, diffLoss = 5.8589, kgLoss = 0.1584
2025-04-09 11:07:49.650243: Training Step 75/115: batchLoss = 4.3369, diffLoss = 7.1114, kgLoss = 0.1753
2025-04-09 11:07:50.382271: Training Step 76/115: batchLoss = 4.3251, diffLoss = 7.0923, kgLoss = 0.1741
2025-04-09 11:07:51.111937: Training Step 77/115: batchLoss = 4.5567, diffLoss = 7.4724, kgLoss = 0.1831
2025-04-09 11:07:51.844290: Training Step 78/115: batchLoss = 3.9066, diffLoss = 6.4062, kgLoss = 0.1572
2025-04-09 11:07:52.586113: Training Step 79/115: batchLoss = 4.1905, diffLoss = 6.8702, kgLoss = 0.1709
2025-04-09 11:07:53.338867: Training Step 80/115: batchLoss = 4.0611, diffLoss = 6.6572, kgLoss = 0.1669
2025-04-09 11:07:54.079397: Training Step 81/115: batchLoss = 4.3404, diffLoss = 7.1153, kgLoss = 0.1780
2025-04-09 11:07:54.826307: Training Step 82/115: batchLoss = 3.9616, diffLoss = 6.4849, kgLoss = 0.1767
2025-04-09 11:07:55.585253: Training Step 83/115: batchLoss = 4.3410, diffLoss = 7.1147, kgLoss = 0.1803
2025-04-09 11:07:56.337504: Training Step 84/115: batchLoss = 4.5971, diffLoss = 7.5437, kgLoss = 0.1773
2025-04-09 11:07:57.082001: Training Step 85/115: batchLoss = 4.5940, diffLoss = 7.5369, kgLoss = 0.1796
2025-04-09 11:07:57.844697: Training Step 86/115: batchLoss = 4.2438, diffLoss = 6.9558, kgLoss = 0.1757
2025-04-09 11:07:58.578343: Training Step 87/115: batchLoss = 3.7652, diffLoss = 6.1695, kgLoss = 0.1587
2025-04-09 11:07:59.312306: Training Step 88/115: batchLoss = 4.5156, diffLoss = 7.3968, kgLoss = 0.1937
2025-04-09 11:08:00.068477: Training Step 89/115: batchLoss = 3.6846, diffLoss = 6.0362, kgLoss = 0.1571
2025-04-09 11:08:00.814148: Training Step 90/115: batchLoss = 4.4426, diffLoss = 7.2810, kgLoss = 0.1850
2025-04-09 11:08:01.547629: Training Step 91/115: batchLoss = 4.2463, diffLoss = 6.9523, kgLoss = 0.1872
2025-04-09 11:08:02.301124: Training Step 92/115: batchLoss = 4.0768, diffLoss = 6.6786, kgLoss = 0.1740
2025-04-09 11:08:03.030263: Training Step 93/115: batchLoss = 3.8938, diffLoss = 6.3801, kgLoss = 0.1644
2025-04-09 11:08:03.758861: Training Step 94/115: batchLoss = 3.9854, diffLoss = 6.5321, kgLoss = 0.1655
2025-04-09 11:08:04.497488: Training Step 95/115: batchLoss = 4.1199, diffLoss = 6.7492, kgLoss = 0.1759
2025-04-09 11:08:05.231513: Training Step 96/115: batchLoss = 4.1380, diffLoss = 6.7841, kgLoss = 0.1689
2025-04-09 11:08:05.969458: Training Step 97/115: batchLoss = 4.2658, diffLoss = 6.9911, kgLoss = 0.1779
2025-04-09 11:08:06.704969: Training Step 98/115: batchLoss = 4.3222, diffLoss = 7.0846, kgLoss = 0.1785
2025-04-09 11:08:07.437148: Training Step 99/115: batchLoss = 5.0539, diffLoss = 8.2829, kgLoss = 0.2103
2025-04-09 11:08:08.179261: Training Step 100/115: batchLoss = 4.0364, diffLoss = 6.6168, kgLoss = 0.1658
2025-04-09 11:08:08.944799: Training Step 101/115: batchLoss = 3.9539, diffLoss = 6.4720, kgLoss = 0.1769
2025-04-09 11:08:09.716081: Training Step 102/115: batchLoss = 4.1536, diffLoss = 6.8022, kgLoss = 0.1806
2025-04-09 11:08:10.475553: Training Step 103/115: batchLoss = 4.7874, diffLoss = 7.8454, kgLoss = 0.2003
2025-04-09 11:08:11.226142: Training Step 104/115: batchLoss = 3.7817, diffLoss = 6.1938, kgLoss = 0.1637
2025-04-09 11:08:11.964256: Training Step 105/115: batchLoss = 3.9984, diffLoss = 6.5535, kgLoss = 0.1657
2025-04-09 11:08:12.709950: Training Step 106/115: batchLoss = 3.7901, diffLoss = 6.2003, kgLoss = 0.1748
2025-04-09 11:08:13.445994: Training Step 107/115: batchLoss = 3.9455, diffLoss = 6.4627, kgLoss = 0.1698
2025-04-09 11:08:14.186925: Training Step 108/115: batchLoss = 4.0433, diffLoss = 6.6186, kgLoss = 0.1804
2025-04-09 11:08:14.932500: Training Step 109/115: batchLoss = 3.9590, diffLoss = 6.4876, kgLoss = 0.1661
2025-04-09 11:08:15.700720: Training Step 110/115: batchLoss = 3.8474, diffLoss = 6.2987, kgLoss = 0.1704
2025-04-09 11:08:16.443874: Training Step 111/115: batchLoss = 3.6847, diffLoss = 6.0378, kgLoss = 0.1551
2025-04-09 11:08:17.183373: Training Step 112/115: batchLoss = 4.3825, diffLoss = 7.1831, kgLoss = 0.1816
2025-04-09 11:08:17.843211: Training Step 113/115: batchLoss = 4.6209, diffLoss = 7.5767, kgLoss = 0.1872
2025-04-09 11:08:18.508916: Training Step 114/115: batchLoss = 3.7883, diffLoss = 6.2042, kgLoss = 0.1644
2025-04-09 11:08:18.645603: 
2025-04-09 11:08:18.646710: Epoch 45/1000, Train: epLoss = 1.1848, epDfLoss = 1.9415, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:08:19.419026: Steps 0/90: batch_recall = 57.02, batch_ndcg = 43.92 
2025-04-09 11:08:20.188122: Steps 1/90: batch_recall = 56.26, batch_ndcg = 40.18 
2025-04-09 11:08:20.965648: Steps 2/90: batch_recall = 50.70, batch_ndcg = 38.11 
2025-04-09 11:08:21.710481: Steps 3/90: batch_recall = 43.25, batch_ndcg = 31.66 
2025-04-09 11:08:22.466581: Steps 4/90: batch_recall = 48.91, batch_ndcg = 35.38 
2025-04-09 11:08:23.233211: Steps 5/90: batch_recall = 37.59, batch_ndcg = 27.27 
2025-04-09 11:08:24.002964: Steps 6/90: batch_recall = 45.35, batch_ndcg = 31.82 
2025-04-09 11:08:24.788608: Steps 7/90: batch_recall = 43.60, batch_ndcg = 31.61 
2025-04-09 11:08:25.549167: Steps 8/90: batch_recall = 46.12, batch_ndcg = 34.19 
2025-04-09 11:08:26.321226: Steps 9/90: batch_recall = 43.78, batch_ndcg = 33.63 
2025-04-09 11:08:27.087261: Steps 10/90: batch_recall = 38.95, batch_ndcg = 29.76 
2025-04-09 11:08:27.836980: Steps 11/90: batch_recall = 42.28, batch_ndcg = 29.29 
2025-04-09 11:08:28.585797: Steps 12/90: batch_recall = 36.90, batch_ndcg = 26.67 
2025-04-09 11:08:29.313164: Steps 13/90: batch_recall = 37.10, batch_ndcg = 26.44 
2025-04-09 11:08:30.044324: Steps 14/90: batch_recall = 33.67, batch_ndcg = 23.91 
2025-04-09 11:08:30.772798: Steps 15/90: batch_recall = 45.12, batch_ndcg = 30.35 
2025-04-09 11:08:31.501963: Steps 16/90: batch_recall = 36.18, batch_ndcg = 26.23 
2025-04-09 11:08:32.241919: Steps 17/90: batch_recall = 31.13, batch_ndcg = 21.20 
2025-04-09 11:08:32.956527: Steps 18/90: batch_recall = 37.17, batch_ndcg = 25.87 
2025-04-09 11:08:33.691891: Steps 19/90: batch_recall = 31.80, batch_ndcg = 22.82 
2025-04-09 11:08:34.412482: Steps 20/90: batch_recall = 39.92, batch_ndcg = 27.14 
2025-04-09 11:08:35.145999: Steps 21/90: batch_recall = 41.06, batch_ndcg = 29.65 
2025-04-09 11:08:35.861591: Steps 22/90: batch_recall = 40.21, batch_ndcg = 28.17 
2025-04-09 11:08:36.582966: Steps 23/90: batch_recall = 40.05, batch_ndcg = 27.37 
2025-04-09 11:08:37.292423: Steps 24/90: batch_recall = 39.60, batch_ndcg = 28.27 
2025-04-09 11:08:38.017128: Steps 25/90: batch_recall = 39.03, batch_ndcg = 25.23 
2025-04-09 11:08:38.753468: Steps 26/90: batch_recall = 38.48, batch_ndcg = 27.52 
2025-04-09 11:08:39.487657: Steps 27/90: batch_recall = 34.79, batch_ndcg = 23.88 
2025-04-09 11:08:40.230069: Steps 28/90: batch_recall = 39.70, batch_ndcg = 26.76 
2025-04-09 11:08:40.966557: Steps 29/90: batch_recall = 33.17, batch_ndcg = 23.29 
2025-04-09 11:08:41.678740: Steps 30/90: batch_recall = 36.23, batch_ndcg = 24.57 
2025-04-09 11:08:42.406249: Steps 31/90: batch_recall = 32.36, batch_ndcg = 23.33 
2025-04-09 11:08:43.115269: Steps 32/90: batch_recall = 35.48, batch_ndcg = 23.72 
2025-04-09 11:08:43.839239: Steps 33/90: batch_recall = 35.16, batch_ndcg = 21.82 
2025-04-09 11:08:44.546732: Steps 34/90: batch_recall = 34.67, batch_ndcg = 23.37 
2025-04-09 11:08:45.251926: Steps 35/90: batch_recall = 37.88, batch_ndcg = 26.55 
2025-04-09 11:08:45.980520: Steps 36/90: batch_recall = 37.93, batch_ndcg = 24.78 
2025-04-09 11:08:46.697260: Steps 37/90: batch_recall = 33.79, batch_ndcg = 21.20 
2025-04-09 11:08:47.416795: Steps 38/90: batch_recall = 36.09, batch_ndcg = 23.50 
2025-04-09 11:08:48.143005: Steps 39/90: batch_recall = 37.30, batch_ndcg = 23.50 
2025-04-09 11:08:48.858186: Steps 40/90: batch_recall = 31.99, batch_ndcg = 22.24 
2025-04-09 11:08:49.558414: Steps 41/90: batch_recall = 34.06, batch_ndcg = 21.31 
2025-04-09 11:08:50.262223: Steps 42/90: batch_recall = 43.46, batch_ndcg = 28.82 
2025-04-09 11:08:50.966519: Steps 43/90: batch_recall = 35.79, batch_ndcg = 22.15 
2025-04-09 11:08:51.676625: Steps 44/90: batch_recall = 33.89, batch_ndcg = 23.32 
2025-04-09 11:08:52.372436: Steps 45/90: batch_recall = 33.59, batch_ndcg = 22.83 
2025-04-09 11:08:53.081490: Steps 46/90: batch_recall = 36.01, batch_ndcg = 23.32 
2025-04-09 11:08:53.783091: Steps 47/90: batch_recall = 33.00, batch_ndcg = 20.66 
2025-04-09 11:08:54.491305: Steps 48/90: batch_recall = 36.86, batch_ndcg = 24.02 
2025-04-09 11:08:55.196742: Steps 49/90: batch_recall = 30.48, batch_ndcg = 18.86 
2025-04-09 11:08:55.902908: Steps 50/90: batch_recall = 41.40, batch_ndcg = 26.61 
2025-04-09 11:08:56.625917: Steps 51/90: batch_recall = 40.08, batch_ndcg = 26.01 
2025-04-09 11:08:57.359024: Steps 52/90: batch_recall = 38.84, batch_ndcg = 25.31 
2025-04-09 11:08:58.076578: Steps 53/90: batch_recall = 41.42, batch_ndcg = 26.48 
2025-04-09 11:08:58.782322: Steps 54/90: batch_recall = 31.55, batch_ndcg = 20.52 
2025-04-09 11:08:59.491033: Steps 55/90: batch_recall = 37.08, batch_ndcg = 23.60 
2025-04-09 11:09:00.205006: Steps 56/90: batch_recall = 33.32, batch_ndcg = 20.47 
2025-04-09 11:09:00.931524: Steps 57/90: batch_recall = 37.73, batch_ndcg = 24.95 
2025-04-09 11:09:01.652640: Steps 58/90: batch_recall = 36.48, batch_ndcg = 22.88 
2025-04-09 11:09:02.388887: Steps 59/90: batch_recall = 36.10, batch_ndcg = 23.86 
2025-04-09 11:09:03.108404: Steps 60/90: batch_recall = 34.60, batch_ndcg = 22.59 
2025-04-09 11:09:03.813051: Steps 61/90: batch_recall = 40.48, batch_ndcg = 27.68 
2025-04-09 11:09:04.496481: Steps 62/90: batch_recall = 38.81, batch_ndcg = 25.29 
2025-04-09 11:09:05.211614: Steps 63/90: batch_recall = 32.06, batch_ndcg = 22.78 
2025-04-09 11:09:05.904680: Steps 64/90: batch_recall = 34.95, batch_ndcg = 23.16 
2025-04-09 11:09:06.615757: Steps 65/90: batch_recall = 35.30, batch_ndcg = 22.83 
2025-04-09 11:09:07.309238: Steps 66/90: batch_recall = 37.38, batch_ndcg = 22.35 
2025-04-09 11:09:08.013741: Steps 67/90: batch_recall = 32.79, batch_ndcg = 20.17 
2025-04-09 11:09:08.707642: Steps 68/90: batch_recall = 36.21, batch_ndcg = 23.08 
2025-04-09 11:09:09.423560: Steps 69/90: batch_recall = 41.57, batch_ndcg = 24.85 
2025-04-09 11:09:10.140438: Steps 70/90: batch_recall = 38.24, batch_ndcg = 24.57 
2025-04-09 11:09:10.871450: Steps 71/90: batch_recall = 40.08, batch_ndcg = 24.28 
2025-04-09 11:09:11.583233: Steps 72/90: batch_recall = 34.24, batch_ndcg = 21.85 
2025-04-09 11:09:12.288691: Steps 73/90: batch_recall = 34.19, batch_ndcg = 21.60 
2025-04-09 11:09:13.002597: Steps 74/90: batch_recall = 40.37, batch_ndcg = 25.24 
2025-04-09 11:09:13.713377: Steps 75/90: batch_recall = 37.70, batch_ndcg = 22.85 
2025-04-09 11:09:14.430189: Steps 76/90: batch_recall = 41.46, batch_ndcg = 25.64 
2025-04-09 11:09:15.142079: Steps 77/90: batch_recall = 44.12, batch_ndcg = 28.11 
2025-04-09 11:09:15.878329: Steps 78/90: batch_recall = 34.53, batch_ndcg = 21.24 
2025-04-09 11:09:16.590696: Steps 79/90: batch_recall = 40.24, batch_ndcg = 26.12 
2025-04-09 11:09:17.301883: Steps 80/90: batch_recall = 46.24, batch_ndcg = 29.94 
2025-04-09 11:09:18.007986: Steps 81/90: batch_recall = 44.48, batch_ndcg = 28.11 
2025-04-09 11:09:18.709552: Steps 82/90: batch_recall = 44.57, batch_ndcg = 28.04 
2025-04-09 11:09:19.407718: Steps 83/90: batch_recall = 46.01, batch_ndcg = 27.61 
2025-04-09 11:09:20.096783: Steps 84/90: batch_recall = 33.87, batch_ndcg = 21.77 
2025-04-09 11:09:20.788774: Steps 85/90: batch_recall = 42.24, batch_ndcg = 24.83 
2025-04-09 11:09:21.485440: Steps 86/90: batch_recall = 44.32, batch_ndcg = 30.28 
2025-04-09 11:09:22.202292: Steps 87/90: batch_recall = 45.60, batch_ndcg = 29.78 
2025-04-09 11:09:22.907774: Steps 88/90: batch_recall = 50.47, batch_ndcg = 30.39 
2025-04-09 11:09:23.448301: Steps 89/90: batch_recall = 40.00, batch_ndcg = 26.07 
2025-04-09 11:09:23.449080: Epoch 45/1000, Test: Recall = 0.0761, NDCG = 0.0509  

2025-04-09 11:09:24.492118: Training Step 0/115: batchLoss = 4.0787, diffLoss = 6.6900, kgLoss = 0.1618
2025-04-09 11:09:25.222357: Training Step 1/115: batchLoss = 4.0176, diffLoss = 6.5692, kgLoss = 0.1902
2025-04-09 11:09:25.951909: Training Step 2/115: batchLoss = 4.0354, diffLoss = 6.6158, kgLoss = 0.1648
2025-04-09 11:09:26.683649: Training Step 3/115: batchLoss = 4.2130, diffLoss = 6.9067, kgLoss = 0.1725
2025-04-09 11:09:27.410024: Training Step 4/115: batchLoss = 4.3917, diffLoss = 7.2019, kgLoss = 0.1764
2025-04-09 11:09:28.140079: Training Step 5/115: batchLoss = 4.4062, diffLoss = 7.2229, kgLoss = 0.1811
2025-04-09 11:09:28.870037: Training Step 6/115: batchLoss = 3.8064, diffLoss = 6.2366, kgLoss = 0.1613
2025-04-09 11:09:29.608036: Training Step 7/115: batchLoss = 4.2314, diffLoss = 6.9368, kgLoss = 0.1734
2025-04-09 11:09:30.331499: Training Step 8/115: batchLoss = 4.3932, diffLoss = 7.2084, kgLoss = 0.1704
2025-04-09 11:09:31.059361: Training Step 9/115: batchLoss = 4.3418, diffLoss = 7.1190, kgLoss = 0.1760
2025-04-09 11:09:31.787140: Training Step 10/115: batchLoss = 3.8097, diffLoss = 6.2456, kgLoss = 0.1559
2025-04-09 11:09:32.512641: Training Step 11/115: batchLoss = 4.3454, diffLoss = 7.1187, kgLoss = 0.1856
2025-04-09 11:09:33.241750: Training Step 12/115: batchLoss = 3.8136, diffLoss = 6.2515, kgLoss = 0.1568
2025-04-09 11:09:33.977032: Training Step 13/115: batchLoss = 4.0515, diffLoss = 6.6466, kgLoss = 0.1587
2025-04-09 11:09:34.708615: Training Step 14/115: batchLoss = 4.2688, diffLoss = 6.9931, kgLoss = 0.1825
2025-04-09 11:09:35.432110: Training Step 15/115: batchLoss = 3.8204, diffLoss = 6.2569, kgLoss = 0.1655
2025-04-09 11:09:36.166103: Training Step 16/115: batchLoss = 4.7412, diffLoss = 7.7721, kgLoss = 0.1947
2025-04-09 11:09:36.893778: Training Step 17/115: batchLoss = 4.0215, diffLoss = 6.5904, kgLoss = 0.1683
2025-04-09 11:09:37.615959: Training Step 18/115: batchLoss = 4.4269, diffLoss = 7.2511, kgLoss = 0.1905
2025-04-09 11:09:38.341009: Training Step 19/115: batchLoss = 3.9093, diffLoss = 6.4012, kgLoss = 0.1715
2025-04-09 11:09:39.066617: Training Step 20/115: batchLoss = 4.1789, diffLoss = 6.8474, kgLoss = 0.1761
2025-04-09 11:09:39.802296: Training Step 21/115: batchLoss = 4.0170, diffLoss = 6.5844, kgLoss = 0.1660
2025-04-09 11:09:40.519193: Training Step 22/115: batchLoss = 4.1058, diffLoss = 6.7175, kgLoss = 0.1881
2025-04-09 11:09:41.244237: Training Step 23/115: batchLoss = 4.2288, diffLoss = 6.9366, kgLoss = 0.1670
2025-04-09 11:09:41.970871: Training Step 24/115: batchLoss = 4.1310, diffLoss = 6.7698, kgLoss = 0.1728
2025-04-09 11:09:42.714233: Training Step 25/115: batchLoss = 4.1984, diffLoss = 6.8820, kgLoss = 0.1729
2025-04-09 11:09:43.449735: Training Step 26/115: batchLoss = 4.1352, diffLoss = 6.7756, kgLoss = 0.1746
2025-04-09 11:09:44.184606: Training Step 27/115: batchLoss = 4.1924, diffLoss = 6.8591, kgLoss = 0.1925
2025-04-09 11:09:44.925947: Training Step 28/115: batchLoss = 4.3025, diffLoss = 7.0436, kgLoss = 0.1909
2025-04-09 11:09:45.653803: Training Step 29/115: batchLoss = 4.6506, diffLoss = 7.6295, kgLoss = 0.1823
2025-04-09 11:09:46.391145: Training Step 30/115: batchLoss = 4.2951, diffLoss = 7.0378, kgLoss = 0.1812
2025-04-09 11:09:47.126282: Training Step 31/115: batchLoss = 3.9713, diffLoss = 6.5080, kgLoss = 0.1662
2025-04-09 11:09:47.861241: Training Step 32/115: batchLoss = 4.2165, diffLoss = 6.9053, kgLoss = 0.1834
2025-04-09 11:09:48.590223: Training Step 33/115: batchLoss = 4.4612, diffLoss = 7.3164, kgLoss = 0.1784
2025-04-09 11:09:49.317211: Training Step 34/115: batchLoss = 4.5890, diffLoss = 7.5248, kgLoss = 0.1852
2025-04-09 11:09:50.040494: Training Step 35/115: batchLoss = 3.8623, diffLoss = 6.3216, kgLoss = 0.1733
2025-04-09 11:09:50.772247: Training Step 36/115: batchLoss = 3.9599, diffLoss = 6.4859, kgLoss = 0.1708
2025-04-09 11:09:51.495047: Training Step 37/115: batchLoss = 4.1629, diffLoss = 6.8214, kgLoss = 0.1751
2025-04-09 11:09:52.219936: Training Step 38/115: batchLoss = 3.9796, diffLoss = 6.5176, kgLoss = 0.1725
2025-04-09 11:09:52.936057: Training Step 39/115: batchLoss = 4.2607, diffLoss = 6.9829, kgLoss = 0.1776
2025-04-09 11:09:53.665207: Training Step 40/115: batchLoss = 3.9862, diffLoss = 6.5312, kgLoss = 0.1687
2025-04-09 11:09:54.415560: Training Step 41/115: batchLoss = 4.0490, diffLoss = 6.6317, kgLoss = 0.1750
2025-04-09 11:09:55.167899: Training Step 42/115: batchLoss = 4.3959, diffLoss = 7.2023, kgLoss = 0.1863
2025-04-09 11:09:55.907595: Training Step 43/115: batchLoss = 3.7448, diffLoss = 6.1398, kgLoss = 0.1523
2025-04-09 11:09:56.671426: Training Step 44/115: batchLoss = 3.8866, diffLoss = 6.3673, kgLoss = 0.1656
2025-04-09 11:09:57.409123: Training Step 45/115: batchLoss = 3.9448, diffLoss = 6.4630, kgLoss = 0.1675
2025-04-09 11:09:58.140474: Training Step 46/115: batchLoss = 4.2360, diffLoss = 6.9387, kgLoss = 0.1821
2025-04-09 11:09:58.872244: Training Step 47/115: batchLoss = 3.8525, diffLoss = 6.3062, kgLoss = 0.1720
2025-04-09 11:09:59.600865: Training Step 48/115: batchLoss = 4.2906, diffLoss = 7.0353, kgLoss = 0.1735
2025-04-09 11:10:00.340202: Training Step 49/115: batchLoss = 4.1241, diffLoss = 6.7637, kgLoss = 0.1649
2025-04-09 11:10:01.086334: Training Step 50/115: batchLoss = 4.2340, diffLoss = 6.9322, kgLoss = 0.1867
2025-04-09 11:10:01.836777: Training Step 51/115: batchLoss = 4.2932, diffLoss = 7.0380, kgLoss = 0.1760
2025-04-09 11:10:02.576451: Training Step 52/115: batchLoss = 3.6637, diffLoss = 6.0051, kgLoss = 0.1515
2025-04-09 11:10:03.317582: Training Step 53/115: batchLoss = 3.6111, diffLoss = 5.9164, kgLoss = 0.1532
2025-04-09 11:10:04.039564: Training Step 54/115: batchLoss = 3.5038, diffLoss = 5.7363, kgLoss = 0.1551
2025-04-09 11:10:04.759917: Training Step 55/115: batchLoss = 4.1997, diffLoss = 6.8903, kgLoss = 0.1639
2025-04-09 11:10:05.492764: Training Step 56/115: batchLoss = 3.9974, diffLoss = 6.5470, kgLoss = 0.1730
2025-04-09 11:10:06.214320: Training Step 57/115: batchLoss = 4.1294, diffLoss = 6.7704, kgLoss = 0.1679
2025-04-09 11:10:06.943724: Training Step 58/115: batchLoss = 4.1620, diffLoss = 6.8157, kgLoss = 0.1814
2025-04-09 11:10:07.683671: Training Step 59/115: batchLoss = 4.3052, diffLoss = 7.0538, kgLoss = 0.1824
2025-04-09 11:10:08.420859: Training Step 60/115: batchLoss = 3.9245, diffLoss = 6.4265, kgLoss = 0.1716
2025-04-09 11:10:09.161543: Training Step 61/115: batchLoss = 3.6960, diffLoss = 6.0473, kgLoss = 0.1690
2025-04-09 11:10:09.906021: Training Step 62/115: batchLoss = 3.7699, diffLoss = 6.1694, kgLoss = 0.1707
2025-04-09 11:10:10.654662: Training Step 63/115: batchLoss = 3.9784, diffLoss = 6.5155, kgLoss = 0.1726
2025-04-09 11:10:11.388233: Training Step 64/115: batchLoss = 4.8071, diffLoss = 7.8776, kgLoss = 0.2013
2025-04-09 11:10:12.129134: Training Step 65/115: batchLoss = 3.6424, diffLoss = 5.9647, kgLoss = 0.1589
2025-04-09 11:10:12.869291: Training Step 66/115: batchLoss = 3.8107, diffLoss = 6.2441, kgLoss = 0.1605
2025-04-09 11:10:13.606919: Training Step 67/115: batchLoss = 3.9683, diffLoss = 6.5005, kgLoss = 0.1700
2025-04-09 11:10:14.344724: Training Step 68/115: batchLoss = 3.8738, diffLoss = 6.3395, kgLoss = 0.1753
2025-04-09 11:10:15.077919: Training Step 69/115: batchLoss = 4.3880, diffLoss = 7.1938, kgLoss = 0.1792
2025-04-09 11:10:15.821002: Training Step 70/115: batchLoss = 4.4293, diffLoss = 7.2547, kgLoss = 0.1913
2025-04-09 11:10:16.559783: Training Step 71/115: batchLoss = 4.4064, diffLoss = 7.2235, kgLoss = 0.1807
2025-04-09 11:10:17.297040: Training Step 72/115: batchLoss = 4.2952, diffLoss = 7.0365, kgLoss = 0.1832
2025-04-09 11:10:18.037455: Training Step 73/115: batchLoss = 4.4069, diffLoss = 7.2238, kgLoss = 0.1816
2025-04-09 11:10:18.784709: Training Step 74/115: batchLoss = 4.0547, diffLoss = 6.6523, kgLoss = 0.1583
2025-04-09 11:10:19.516443: Training Step 75/115: batchLoss = 4.1497, diffLoss = 6.8020, kgLoss = 0.1712
2025-04-09 11:10:20.251292: Training Step 76/115: batchLoss = 3.5630, diffLoss = 5.8410, kgLoss = 0.1461
2025-04-09 11:10:20.990973: Training Step 77/115: batchLoss = 4.7189, diffLoss = 7.7300, kgLoss = 0.2024
2025-04-09 11:10:21.738916: Training Step 78/115: batchLoss = 4.3916, diffLoss = 7.2014, kgLoss = 0.1769
2025-04-09 11:10:22.493036: Training Step 79/115: batchLoss = 3.8642, diffLoss = 6.3374, kgLoss = 0.1544
2025-04-09 11:10:23.235625: Training Step 80/115: batchLoss = 3.6759, diffLoss = 6.0185, kgLoss = 0.1621
2025-04-09 11:10:23.969078: Training Step 81/115: batchLoss = 4.4390, diffLoss = 7.2692, kgLoss = 0.1938
2025-04-09 11:10:24.712492: Training Step 82/115: batchLoss = 3.5144, diffLoss = 5.7553, kgLoss = 0.1529
2025-04-09 11:10:25.479259: Training Step 83/115: batchLoss = 4.3631, diffLoss = 7.1534, kgLoss = 0.1776
2025-04-09 11:10:26.217379: Training Step 84/115: batchLoss = 4.2685, diffLoss = 7.0001, kgLoss = 0.1710
2025-04-09 11:10:26.950068: Training Step 85/115: batchLoss = 3.9978, diffLoss = 6.5516, kgLoss = 0.1670
2025-04-09 11:10:27.690763: Training Step 86/115: batchLoss = 4.0039, diffLoss = 6.5634, kgLoss = 0.1645
2025-04-09 11:10:28.441870: Training Step 87/115: batchLoss = 3.8676, diffLoss = 6.3404, kgLoss = 0.1584
2025-04-09 11:10:29.175172: Training Step 88/115: batchLoss = 3.9529, diffLoss = 6.4774, kgLoss = 0.1662
2025-04-09 11:10:29.918366: Training Step 89/115: batchLoss = 4.3652, diffLoss = 7.1547, kgLoss = 0.1810
2025-04-09 11:10:30.679136: Training Step 90/115: batchLoss = 3.6928, diffLoss = 6.0520, kgLoss = 0.1539
2025-04-09 11:10:31.438262: Training Step 91/115: batchLoss = 4.3181, diffLoss = 7.0694, kgLoss = 0.1911
2025-04-09 11:10:32.198196: Training Step 92/115: batchLoss = 4.0849, diffLoss = 6.6910, kgLoss = 0.1757
2025-04-09 11:10:32.952800: Training Step 93/115: batchLoss = 4.5475, diffLoss = 7.4551, kgLoss = 0.1861
2025-04-09 11:10:33.685370: Training Step 94/115: batchLoss = 3.5180, diffLoss = 5.7625, kgLoss = 0.1512
2025-04-09 11:10:34.426246: Training Step 95/115: batchLoss = 4.2669, diffLoss = 6.9934, kgLoss = 0.1772
2025-04-09 11:10:35.172933: Training Step 96/115: batchLoss = 4.5071, diffLoss = 7.3915, kgLoss = 0.1804
2025-04-09 11:10:35.898607: Training Step 97/115: batchLoss = 4.0638, diffLoss = 6.6595, kgLoss = 0.1704
2025-04-09 11:10:36.632315: Training Step 98/115: batchLoss = 4.1344, diffLoss = 6.7782, kgLoss = 0.1688
2025-04-09 11:10:37.365767: Training Step 99/115: batchLoss = 3.7801, diffLoss = 6.1918, kgLoss = 0.1626
2025-04-09 11:10:38.105348: Training Step 100/115: batchLoss = 4.0518, diffLoss = 6.6396, kgLoss = 0.1702
2025-04-09 11:10:38.835781: Training Step 101/115: batchLoss = 3.8671, diffLoss = 6.3370, kgLoss = 0.1623
2025-04-09 11:10:39.581198: Training Step 102/115: batchLoss = 4.6154, diffLoss = 7.5526, kgLoss = 0.2096
2025-04-09 11:10:40.317323: Training Step 103/115: batchLoss = 4.3782, diffLoss = 7.1654, kgLoss = 0.1973
2025-04-09 11:10:41.055313: Training Step 104/115: batchLoss = 4.2764, diffLoss = 7.0016, kgLoss = 0.1888
2025-04-09 11:10:41.813306: Training Step 105/115: batchLoss = 4.1021, diffLoss = 6.7186, kgLoss = 0.1774
2025-04-09 11:10:42.559523: Training Step 106/115: batchLoss = 4.3785, diffLoss = 7.1715, kgLoss = 0.1889
2025-04-09 11:10:43.304670: Training Step 107/115: batchLoss = 4.3657, diffLoss = 7.1559, kgLoss = 0.1804
2025-04-09 11:10:44.056062: Training Step 108/115: batchLoss = 3.9874, diffLoss = 6.5402, kgLoss = 0.1583
2025-04-09 11:10:44.795200: Training Step 109/115: batchLoss = 4.2900, diffLoss = 7.0266, kgLoss = 0.1851
2025-04-09 11:10:45.542557: Training Step 110/115: batchLoss = 4.2136, diffLoss = 6.9036, kgLoss = 0.1786
2025-04-09 11:10:46.286361: Training Step 111/115: batchLoss = 4.2269, diffLoss = 6.9236, kgLoss = 0.1819
2025-04-09 11:10:47.034345: Training Step 112/115: batchLoss = 3.9560, diffLoss = 6.4888, kgLoss = 0.1568
2025-04-09 11:10:47.686405: Training Step 113/115: batchLoss = 4.3593, diffLoss = 7.1473, kgLoss = 0.1772
2025-04-09 11:10:48.314149: Training Step 114/115: batchLoss = 3.8351, diffLoss = 6.2908, kgLoss = 0.1517
2025-04-09 11:10:48.430139: 
2025-04-09 11:10:48.431101: Epoch 46/1000, Train: epLoss = 1.1851, epDfLoss = 1.9419, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:10:49.239513: Steps 0/90: batch_recall = 57.62, batch_ndcg = 44.06 
2025-04-09 11:10:49.982382: Steps 1/90: batch_recall = 55.97, batch_ndcg = 39.77 
2025-04-09 11:10:50.755868: Steps 2/90: batch_recall = 50.08, batch_ndcg = 38.01 
2025-04-09 11:10:51.496648: Steps 3/90: batch_recall = 43.25, batch_ndcg = 31.56 
2025-04-09 11:10:52.255259: Steps 4/90: batch_recall = 48.67, batch_ndcg = 35.30 
2025-04-09 11:10:53.041649: Steps 5/90: batch_recall = 37.62, batch_ndcg = 27.64 
2025-04-09 11:10:53.812608: Steps 6/90: batch_recall = 45.90, batch_ndcg = 31.78 
2025-04-09 11:10:54.577577: Steps 7/90: batch_recall = 43.95, batch_ndcg = 31.79 
2025-04-09 11:10:55.333484: Steps 8/90: batch_recall = 46.49, batch_ndcg = 34.21 
2025-04-09 11:10:56.112891: Steps 9/90: batch_recall = 42.23, batch_ndcg = 32.97 
2025-04-09 11:10:56.898506: Steps 10/90: batch_recall = 38.13, batch_ndcg = 29.50 
2025-04-09 11:10:57.649215: Steps 11/90: batch_recall = 42.10, batch_ndcg = 28.97 
2025-04-09 11:10:58.381744: Steps 12/90: batch_recall = 37.74, batch_ndcg = 26.80 
2025-04-09 11:10:59.107601: Steps 13/90: batch_recall = 37.17, batch_ndcg = 26.26 
2025-04-09 11:10:59.815210: Steps 14/90: batch_recall = 32.47, batch_ndcg = 23.74 
2025-04-09 11:11:00.522745: Steps 15/90: batch_recall = 44.24, batch_ndcg = 30.08 
2025-04-09 11:11:01.225354: Steps 16/90: batch_recall = 36.77, batch_ndcg = 26.08 
2025-04-09 11:11:01.936985: Steps 17/90: batch_recall = 31.07, batch_ndcg = 21.04 
2025-04-09 11:11:02.636989: Steps 18/90: batch_recall = 36.95, batch_ndcg = 25.96 
2025-04-09 11:11:03.345994: Steps 19/90: batch_recall = 31.98, batch_ndcg = 22.99 
2025-04-09 11:11:04.045826: Steps 20/90: batch_recall = 40.61, batch_ndcg = 27.56 
2025-04-09 11:11:04.748255: Steps 21/90: batch_recall = 41.43, batch_ndcg = 30.06 
2025-04-09 11:11:05.447632: Steps 22/90: batch_recall = 39.87, batch_ndcg = 27.95 
2025-04-09 11:11:06.149565: Steps 23/90: batch_recall = 39.43, batch_ndcg = 27.10 
2025-04-09 11:11:06.843671: Steps 24/90: batch_recall = 38.18, batch_ndcg = 27.64 
2025-04-09 11:11:07.538583: Steps 25/90: batch_recall = 38.51, batch_ndcg = 24.91 
2025-04-09 11:11:08.240293: Steps 26/90: batch_recall = 38.31, batch_ndcg = 27.30 
2025-04-09 11:11:08.930082: Steps 27/90: batch_recall = 34.25, batch_ndcg = 23.72 
2025-04-09 11:11:09.633134: Steps 28/90: batch_recall = 39.27, batch_ndcg = 26.51 
2025-04-09 11:11:10.329218: Steps 29/90: batch_recall = 33.07, batch_ndcg = 23.14 
2025-04-09 11:11:11.033704: Steps 30/90: batch_recall = 35.02, batch_ndcg = 24.17 
2025-04-09 11:11:11.738105: Steps 31/90: batch_recall = 32.93, batch_ndcg = 23.62 
2025-04-09 11:11:12.429031: Steps 32/90: batch_recall = 34.42, batch_ndcg = 23.34 
2025-04-09 11:11:13.113815: Steps 33/90: batch_recall = 33.18, batch_ndcg = 21.08 
2025-04-09 11:11:13.803465: Steps 34/90: batch_recall = 35.39, batch_ndcg = 23.73 
2025-04-09 11:11:14.484587: Steps 35/90: batch_recall = 38.40, batch_ndcg = 27.20 
2025-04-09 11:11:15.177565: Steps 36/90: batch_recall = 36.52, batch_ndcg = 24.83 
2025-04-09 11:11:15.865446: Steps 37/90: batch_recall = 33.46, batch_ndcg = 21.35 
2025-04-09 11:11:16.563326: Steps 38/90: batch_recall = 36.71, batch_ndcg = 23.82 
2025-04-09 11:11:17.255851: Steps 39/90: batch_recall = 36.46, batch_ndcg = 23.30 
2025-04-09 11:11:17.954916: Steps 40/90: batch_recall = 32.44, batch_ndcg = 22.11 
2025-04-09 11:11:18.646735: Steps 41/90: batch_recall = 34.35, batch_ndcg = 21.46 
2025-04-09 11:11:19.343206: Steps 42/90: batch_recall = 43.13, batch_ndcg = 28.45 
2025-04-09 11:11:20.029135: Steps 43/90: batch_recall = 37.50, batch_ndcg = 22.39 
2025-04-09 11:11:20.721523: Steps 44/90: batch_recall = 33.61, batch_ndcg = 22.90 
2025-04-09 11:11:21.405524: Steps 45/90: batch_recall = 33.39, batch_ndcg = 22.65 
2025-04-09 11:11:22.095796: Steps 46/90: batch_recall = 35.79, batch_ndcg = 23.33 
2025-04-09 11:11:22.783738: Steps 47/90: batch_recall = 32.70, batch_ndcg = 20.46 
2025-04-09 11:11:23.472053: Steps 48/90: batch_recall = 35.94, batch_ndcg = 23.89 
2025-04-09 11:11:24.160187: Steps 49/90: batch_recall = 31.22, batch_ndcg = 19.32 
2025-04-09 11:11:24.857839: Steps 50/90: batch_recall = 41.39, batch_ndcg = 26.89 
2025-04-09 11:11:25.549595: Steps 51/90: batch_recall = 40.47, batch_ndcg = 26.24 
2025-04-09 11:11:26.237065: Steps 52/90: batch_recall = 39.05, batch_ndcg = 25.16 
2025-04-09 11:11:26.920099: Steps 53/90: batch_recall = 41.07, batch_ndcg = 26.27 
2025-04-09 11:11:27.597221: Steps 54/90: batch_recall = 32.08, batch_ndcg = 20.81 
2025-04-09 11:11:28.279614: Steps 55/90: batch_recall = 36.38, batch_ndcg = 23.38 
2025-04-09 11:11:28.968480: Steps 56/90: batch_recall = 33.89, batch_ndcg = 20.62 
2025-04-09 11:11:29.651938: Steps 57/90: batch_recall = 37.17, batch_ndcg = 24.62 
2025-04-09 11:11:30.332988: Steps 58/90: batch_recall = 35.72, batch_ndcg = 22.62 
2025-04-09 11:11:31.019009: Steps 59/90: batch_recall = 35.08, batch_ndcg = 23.38 
2025-04-09 11:11:31.703005: Steps 60/90: batch_recall = 34.46, batch_ndcg = 22.65 
2025-04-09 11:11:32.392238: Steps 61/90: batch_recall = 39.81, batch_ndcg = 27.33 
2025-04-09 11:11:33.076986: Steps 62/90: batch_recall = 38.51, batch_ndcg = 25.25 
2025-04-09 11:11:33.758517: Steps 63/90: batch_recall = 32.19, batch_ndcg = 22.59 
2025-04-09 11:11:34.439919: Steps 64/90: batch_recall = 34.82, batch_ndcg = 23.24 
2025-04-09 11:11:35.127711: Steps 65/90: batch_recall = 34.14, batch_ndcg = 22.44 
2025-04-09 11:11:35.809439: Steps 66/90: batch_recall = 38.71, batch_ndcg = 22.60 
2025-04-09 11:11:36.493475: Steps 67/90: batch_recall = 31.97, batch_ndcg = 19.94 
2025-04-09 11:11:37.178210: Steps 68/90: batch_recall = 35.96, batch_ndcg = 22.73 
2025-04-09 11:11:37.867422: Steps 69/90: batch_recall = 41.78, batch_ndcg = 25.03 
2025-04-09 11:11:38.554474: Steps 70/90: batch_recall = 37.73, batch_ndcg = 24.91 
2025-04-09 11:11:39.235502: Steps 71/90: batch_recall = 39.17, batch_ndcg = 24.10 
2025-04-09 11:11:39.918980: Steps 72/90: batch_recall = 32.28, batch_ndcg = 21.18 
2025-04-09 11:11:40.596963: Steps 73/90: batch_recall = 35.23, batch_ndcg = 21.82 
2025-04-09 11:11:41.274353: Steps 74/90: batch_recall = 40.03, batch_ndcg = 25.41 
2025-04-09 11:11:41.964962: Steps 75/90: batch_recall = 37.54, batch_ndcg = 22.83 
2025-04-09 11:11:42.641330: Steps 76/90: batch_recall = 40.45, batch_ndcg = 25.46 
2025-04-09 11:11:43.319368: Steps 77/90: batch_recall = 44.71, batch_ndcg = 28.21 
2025-04-09 11:11:43.993804: Steps 78/90: batch_recall = 33.88, batch_ndcg = 21.17 
2025-04-09 11:11:44.677371: Steps 79/90: batch_recall = 39.27, batch_ndcg = 25.72 
2025-04-09 11:11:45.353801: Steps 80/90: batch_recall = 46.31, batch_ndcg = 29.54 
2025-04-09 11:11:46.037679: Steps 81/90: batch_recall = 44.35, batch_ndcg = 27.72 
2025-04-09 11:11:46.728216: Steps 82/90: batch_recall = 44.29, batch_ndcg = 27.84 
2025-04-09 11:11:47.408440: Steps 83/90: batch_recall = 45.24, batch_ndcg = 27.33 
2025-04-09 11:11:48.090930: Steps 84/90: batch_recall = 34.48, batch_ndcg = 22.09 
2025-04-09 11:11:48.768559: Steps 85/90: batch_recall = 41.39, batch_ndcg = 24.51 
2025-04-09 11:11:49.454996: Steps 86/90: batch_recall = 45.03, batch_ndcg = 30.37 
2025-04-09 11:11:50.138795: Steps 87/90: batch_recall = 44.07, batch_ndcg = 29.25 
2025-04-09 11:11:50.818474: Steps 88/90: batch_recall = 50.42, batch_ndcg = 30.17 
2025-04-09 11:11:51.349888: Steps 89/90: batch_recall = 40.54, batch_ndcg = 26.19 
2025-04-09 11:11:51.350608: Epoch 46/1000, Test: Recall = 0.0757, NDCG = 0.0507  

2025-04-09 11:11:52.349179: Training Step 0/115: batchLoss = 4.5071, diffLoss = 7.3884, kgLoss = 0.1850
2025-04-09 11:11:53.062374: Training Step 1/115: batchLoss = 3.6178, diffLoss = 5.9282, kgLoss = 0.1522
2025-04-09 11:11:53.781657: Training Step 2/115: batchLoss = 3.9645, diffLoss = 6.4941, kgLoss = 0.1701
2025-04-09 11:11:54.492049: Training Step 3/115: batchLoss = 4.1965, diffLoss = 6.8766, kgLoss = 0.1764
2025-04-09 11:11:55.205062: Training Step 4/115: batchLoss = 3.6129, diffLoss = 5.9203, kgLoss = 0.1519
2025-04-09 11:11:55.919641: Training Step 5/115: batchLoss = 3.9546, diffLoss = 6.4833, kgLoss = 0.1616
2025-04-09 11:11:56.630568: Training Step 6/115: batchLoss = 4.6063, diffLoss = 7.5491, kgLoss = 0.1921
2025-04-09 11:11:57.347166: Training Step 7/115: batchLoss = 4.8108, diffLoss = 7.8815, kgLoss = 0.2048
2025-04-09 11:11:58.062593: Training Step 8/115: batchLoss = 4.5866, diffLoss = 7.5149, kgLoss = 0.1941
2025-04-09 11:11:58.782495: Training Step 9/115: batchLoss = 4.3592, diffLoss = 7.1415, kgLoss = 0.1857
2025-04-09 11:11:59.507296: Training Step 10/115: batchLoss = 3.8180, diffLoss = 6.2587, kgLoss = 0.1570
2025-04-09 11:12:00.223269: Training Step 11/115: batchLoss = 3.8357, diffLoss = 6.2900, kgLoss = 0.1542
2025-04-09 11:12:00.947444: Training Step 12/115: batchLoss = 4.0313, diffLoss = 6.6063, kgLoss = 0.1689
2025-04-09 11:12:01.664920: Training Step 13/115: batchLoss = 4.1206, diffLoss = 6.7495, kgLoss = 0.1774
2025-04-09 11:12:02.385432: Training Step 14/115: batchLoss = 4.0480, diffLoss = 6.6340, kgLoss = 0.1690
2025-04-09 11:12:03.102705: Training Step 15/115: batchLoss = 4.2454, diffLoss = 6.9543, kgLoss = 0.1821
2025-04-09 11:12:03.826075: Training Step 16/115: batchLoss = 3.9876, diffLoss = 6.5354, kgLoss = 0.1658
2025-04-09 11:12:04.546247: Training Step 17/115: batchLoss = 3.4866, diffLoss = 5.7038, kgLoss = 0.1610
2025-04-09 11:12:05.264611: Training Step 18/115: batchLoss = 3.7841, diffLoss = 6.1937, kgLoss = 0.1696
2025-04-09 11:12:05.981380: Training Step 19/115: batchLoss = 3.3623, diffLoss = 5.5094, kgLoss = 0.1417
2025-04-09 11:12:06.693911: Training Step 20/115: batchLoss = 4.0173, diffLoss = 6.5856, kgLoss = 0.1649
2025-04-09 11:12:07.412009: Training Step 21/115: batchLoss = 4.0624, diffLoss = 6.6497, kgLoss = 0.1814
2025-04-09 11:12:08.125032: Training Step 22/115: batchLoss = 3.8372, diffLoss = 6.2923, kgLoss = 0.1546
2025-04-09 11:12:08.838769: Training Step 23/115: batchLoss = 4.1748, diffLoss = 6.8392, kgLoss = 0.1783
2025-04-09 11:12:09.566939: Training Step 24/115: batchLoss = 4.3868, diffLoss = 7.1919, kgLoss = 0.1793
2025-04-09 11:12:10.277032: Training Step 25/115: batchLoss = 4.1623, diffLoss = 6.8215, kgLoss = 0.1735
2025-04-09 11:12:10.993140: Training Step 26/115: batchLoss = 4.0160, diffLoss = 6.5850, kgLoss = 0.1626
2025-04-09 11:12:11.706499: Training Step 27/115: batchLoss = 3.8829, diffLoss = 6.3627, kgLoss = 0.1631
2025-04-09 11:12:12.429891: Training Step 28/115: batchLoss = 4.2390, diffLoss = 6.9486, kgLoss = 0.1746
2025-04-09 11:12:13.149359: Training Step 29/115: batchLoss = 4.2485, diffLoss = 6.9547, kgLoss = 0.1892
2025-04-09 11:12:13.869777: Training Step 30/115: batchLoss = 4.1627, diffLoss = 6.8224, kgLoss = 0.1731
2025-04-09 11:12:14.582849: Training Step 31/115: batchLoss = 4.4161, diffLoss = 7.2305, kgLoss = 0.1946
2025-04-09 11:12:15.313219: Training Step 32/115: batchLoss = 3.8026, diffLoss = 6.2308, kgLoss = 0.1604
2025-04-09 11:12:16.035153: Training Step 33/115: batchLoss = 4.2906, diffLoss = 7.0223, kgLoss = 0.1930
2025-04-09 11:12:16.748977: Training Step 34/115: batchLoss = 4.0751, diffLoss = 6.6684, kgLoss = 0.1852
2025-04-09 11:12:17.468803: Training Step 35/115: batchLoss = 4.0786, diffLoss = 6.6875, kgLoss = 0.1653
2025-04-09 11:12:18.183597: Training Step 36/115: batchLoss = 4.0479, diffLoss = 6.6335, kgLoss = 0.1694
2025-04-09 11:12:18.902067: Training Step 37/115: batchLoss = 4.1056, diffLoss = 6.7272, kgLoss = 0.1733
2025-04-09 11:12:19.626788: Training Step 38/115: batchLoss = 3.8008, diffLoss = 6.2330, kgLoss = 0.1524
2025-04-09 11:12:20.339986: Training Step 39/115: batchLoss = 3.9412, diffLoss = 6.4480, kgLoss = 0.1811
2025-04-09 11:12:21.055272: Training Step 40/115: batchLoss = 4.2709, diffLoss = 7.0006, kgLoss = 0.1764
2025-04-09 11:12:21.765007: Training Step 41/115: batchLoss = 3.5239, diffLoss = 5.7728, kgLoss = 0.1506
2025-04-09 11:12:22.460331: Training Step 42/115: batchLoss = 4.7056, diffLoss = 7.7127, kgLoss = 0.1948
2025-04-09 11:12:23.158518: Training Step 43/115: batchLoss = 4.0655, diffLoss = 6.6552, kgLoss = 0.1811
2025-04-09 11:12:23.866786: Training Step 44/115: batchLoss = 3.8037, diffLoss = 6.2237, kgLoss = 0.1736
2025-04-09 11:12:24.578886: Training Step 45/115: batchLoss = 3.6484, diffLoss = 5.9714, kgLoss = 0.1639
2025-04-09 11:12:25.295019: Training Step 46/115: batchLoss = 3.7544, diffLoss = 6.1491, kgLoss = 0.1624
2025-04-09 11:12:26.006637: Training Step 47/115: batchLoss = 3.5723, diffLoss = 5.8505, kgLoss = 0.1549
2025-04-09 11:12:26.724745: Training Step 48/115: batchLoss = 4.3365, diffLoss = 7.1078, kgLoss = 0.1796
2025-04-09 11:12:27.438925: Training Step 49/115: batchLoss = 4.2209, diffLoss = 6.9136, kgLoss = 0.1819
2025-04-09 11:12:28.156768: Training Step 50/115: batchLoss = 4.4995, diffLoss = 7.3803, kgLoss = 0.1782
2025-04-09 11:12:28.872159: Training Step 51/115: batchLoss = 3.7863, diffLoss = 6.2083, kgLoss = 0.1533
2025-04-09 11:12:29.589014: Training Step 52/115: batchLoss = 3.9969, diffLoss = 6.5468, kgLoss = 0.1722
2025-04-09 11:12:30.307638: Training Step 53/115: batchLoss = 4.2443, diffLoss = 6.9421, kgLoss = 0.1976
2025-04-09 11:12:31.030120: Training Step 54/115: batchLoss = 4.1525, diffLoss = 6.8093, kgLoss = 0.1674
2025-04-09 11:12:31.757773: Training Step 55/115: batchLoss = 4.4395, diffLoss = 7.2821, kgLoss = 0.1756
2025-04-09 11:12:32.477922: Training Step 56/115: batchLoss = 4.0877, diffLoss = 6.6983, kgLoss = 0.1717
2025-04-09 11:12:33.200525: Training Step 57/115: batchLoss = 4.9427, diffLoss = 8.1022, kgLoss = 0.2035
2025-04-09 11:12:33.919383: Training Step 58/115: batchLoss = 3.8378, diffLoss = 6.2841, kgLoss = 0.1684
2025-04-09 11:12:34.636927: Training Step 59/115: batchLoss = 4.0117, diffLoss = 6.5693, kgLoss = 0.1753
2025-04-09 11:12:35.346038: Training Step 60/115: batchLoss = 3.4490, diffLoss = 5.6465, kgLoss = 0.1526
2025-04-09 11:12:36.057559: Training Step 61/115: batchLoss = 4.4230, diffLoss = 7.2422, kgLoss = 0.1941
2025-04-09 11:12:36.768106: Training Step 62/115: batchLoss = 4.1512, diffLoss = 6.8035, kgLoss = 0.1729
2025-04-09 11:12:37.481723: Training Step 63/115: batchLoss = 4.2799, diffLoss = 7.0118, kgLoss = 0.1820
2025-04-09 11:12:38.193385: Training Step 64/115: batchLoss = 4.1844, diffLoss = 6.8610, kgLoss = 0.1696
2025-04-09 11:12:38.911788: Training Step 65/115: batchLoss = 4.3986, diffLoss = 7.2072, kgLoss = 0.1857
2025-04-09 11:12:39.627354: Training Step 66/115: batchLoss = 3.9865, diffLoss = 6.5284, kgLoss = 0.1737
2025-04-09 11:12:40.357347: Training Step 67/115: batchLoss = 4.4283, diffLoss = 7.2596, kgLoss = 0.1814
2025-04-09 11:12:41.081434: Training Step 68/115: batchLoss = 4.1248, diffLoss = 6.7666, kgLoss = 0.1620
2025-04-09 11:12:41.814120: Training Step 69/115: batchLoss = 4.2652, diffLoss = 6.9907, kgLoss = 0.1771
2025-04-09 11:12:42.528655: Training Step 70/115: batchLoss = 4.4293, diffLoss = 7.2599, kgLoss = 0.1835
2025-04-09 11:12:43.241443: Training Step 71/115: batchLoss = 4.0344, diffLoss = 6.6110, kgLoss = 0.1696
2025-04-09 11:12:43.957980: Training Step 72/115: batchLoss = 3.8746, diffLoss = 6.3526, kgLoss = 0.1575
2025-04-09 11:12:44.688162: Training Step 73/115: batchLoss = 3.9368, diffLoss = 6.4502, kgLoss = 0.1666
2025-04-09 11:12:45.406400: Training Step 74/115: batchLoss = 3.9709, diffLoss = 6.5088, kgLoss = 0.1642
2025-04-09 11:12:46.126563: Training Step 75/115: batchLoss = 3.8616, diffLoss = 6.3221, kgLoss = 0.1708
2025-04-09 11:12:46.842976: Training Step 76/115: batchLoss = 4.2606, diffLoss = 6.9823, kgLoss = 0.1782
2025-04-09 11:12:47.561162: Training Step 77/115: batchLoss = 4.5954, diffLoss = 7.5323, kgLoss = 0.1899
2025-04-09 11:12:48.278791: Training Step 78/115: batchLoss = 4.1999, diffLoss = 6.8826, kgLoss = 0.1758
2025-04-09 11:12:48.992179: Training Step 79/115: batchLoss = 4.2774, diffLoss = 7.0111, kgLoss = 0.1769
2025-04-09 11:12:49.701627: Training Step 80/115: batchLoss = 3.8330, diffLoss = 6.2851, kgLoss = 0.1549
2025-04-09 11:12:50.416978: Training Step 81/115: batchLoss = 3.8546, diffLoss = 6.3162, kgLoss = 0.1622
2025-04-09 11:12:51.126718: Training Step 82/115: batchLoss = 4.2602, diffLoss = 6.9827, kgLoss = 0.1765
2025-04-09 11:12:51.847373: Training Step 83/115: batchLoss = 4.1074, diffLoss = 6.7387, kgLoss = 0.1604
2025-04-09 11:12:52.559074: Training Step 84/115: batchLoss = 4.0643, diffLoss = 6.6628, kgLoss = 0.1667
2025-04-09 11:12:53.272792: Training Step 85/115: batchLoss = 4.1780, diffLoss = 6.8448, kgLoss = 0.1779
2025-04-09 11:12:53.989485: Training Step 86/115: batchLoss = 4.4635, diffLoss = 7.3165, kgLoss = 0.1840
2025-04-09 11:12:54.703963: Training Step 87/115: batchLoss = 4.0196, diffLoss = 6.5830, kgLoss = 0.1745
2025-04-09 11:12:55.421755: Training Step 88/115: batchLoss = 4.4225, diffLoss = 7.2455, kgLoss = 0.1880
2025-04-09 11:12:56.141217: Training Step 89/115: batchLoss = 4.1005, diffLoss = 6.7221, kgLoss = 0.1681
2025-04-09 11:12:56.859819: Training Step 90/115: batchLoss = 4.0958, diffLoss = 6.7122, kgLoss = 0.1711
2025-04-09 11:12:57.576648: Training Step 91/115: batchLoss = 3.9198, diffLoss = 6.4264, kgLoss = 0.1599
2025-04-09 11:12:58.301512: Training Step 92/115: batchLoss = 4.4075, diffLoss = 7.2144, kgLoss = 0.1972
2025-04-09 11:12:59.028460: Training Step 93/115: batchLoss = 4.5628, diffLoss = 7.4764, kgLoss = 0.1925
2025-04-09 11:12:59.747473: Training Step 94/115: batchLoss = 4.0870, diffLoss = 6.7008, kgLoss = 0.1663
2025-04-09 11:13:00.466947: Training Step 95/115: batchLoss = 4.0660, diffLoss = 6.6643, kgLoss = 0.1684
2025-04-09 11:13:01.184968: Training Step 96/115: batchLoss = 4.2446, diffLoss = 6.9545, kgLoss = 0.1798
2025-04-09 11:13:01.901161: Training Step 97/115: batchLoss = 4.0191, diffLoss = 6.5895, kgLoss = 0.1637
2025-04-09 11:13:02.615651: Training Step 98/115: batchLoss = 3.9092, diffLoss = 6.4046, kgLoss = 0.1659
2025-04-09 11:13:03.323668: Training Step 99/115: batchLoss = 3.8814, diffLoss = 6.3593, kgLoss = 0.1645
2025-04-09 11:13:04.034904: Training Step 100/115: batchLoss = 4.2652, diffLoss = 6.9890, kgLoss = 0.1795
2025-04-09 11:13:04.748730: Training Step 101/115: batchLoss = 3.7013, diffLoss = 6.0707, kgLoss = 0.1472
2025-04-09 11:13:05.459572: Training Step 102/115: batchLoss = 4.1001, diffLoss = 6.7194, kgLoss = 0.1713
2025-04-09 11:13:06.198176: Training Step 103/115: batchLoss = 3.9228, diffLoss = 6.4174, kgLoss = 0.1808
2025-04-09 11:13:06.920634: Training Step 104/115: batchLoss = 4.2685, diffLoss = 7.0054, kgLoss = 0.1632
2025-04-09 11:13:07.642263: Training Step 105/115: batchLoss = 4.0398, diffLoss = 6.6136, kgLoss = 0.1790
2025-04-09 11:13:08.358987: Training Step 106/115: batchLoss = 3.9068, diffLoss = 6.3889, kgLoss = 0.1837
2025-04-09 11:13:09.075569: Training Step 107/115: batchLoss = 3.9308, diffLoss = 6.4311, kgLoss = 0.1804
2025-04-09 11:13:09.794170: Training Step 108/115: batchLoss = 4.5680, diffLoss = 7.4827, kgLoss = 0.1958
2025-04-09 11:13:10.513956: Training Step 109/115: batchLoss = 4.0249, diffLoss = 6.5978, kgLoss = 0.1655
2025-04-09 11:13:11.232532: Training Step 110/115: batchLoss = 4.0643, diffLoss = 6.6614, kgLoss = 0.1687
2025-04-09 11:13:11.947865: Training Step 111/115: batchLoss = 4.6724, diffLoss = 7.6607, kgLoss = 0.1901
2025-04-09 11:13:12.659349: Training Step 112/115: batchLoss = 3.7507, diffLoss = 6.1347, kgLoss = 0.1746
2025-04-09 11:13:13.293728: Training Step 113/115: batchLoss = 4.9285, diffLoss = 8.0807, kgLoss = 0.2002
2025-04-09 11:13:13.924382: Training Step 114/115: batchLoss = 4.2509, diffLoss = 6.9625, kgLoss = 0.1836
2025-04-09 11:13:14.044827: 
2025-04-09 11:13:14.045778: Epoch 47/1000, Train: epLoss = 1.1815, epDfLoss = 1.9359, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:13:14.783704: Steps 0/90: batch_recall = 57.11, batch_ndcg = 43.88 
2025-04-09 11:13:15.494195: Steps 1/90: batch_recall = 55.98, batch_ndcg = 40.06 
2025-04-09 11:13:16.218754: Steps 2/90: batch_recall = 51.20, batch_ndcg = 38.65 
2025-04-09 11:13:16.931846: Steps 3/90: batch_recall = 43.46, batch_ndcg = 31.65 
2025-04-09 11:13:17.653428: Steps 4/90: batch_recall = 47.83, batch_ndcg = 35.07 
2025-04-09 11:13:18.369980: Steps 5/90: batch_recall = 37.04, batch_ndcg = 27.28 
2025-04-09 11:13:19.075075: Steps 6/90: batch_recall = 45.70, batch_ndcg = 32.45 
2025-04-09 11:13:19.803454: Steps 7/90: batch_recall = 43.48, batch_ndcg = 31.49 
2025-04-09 11:13:20.523278: Steps 8/90: batch_recall = 46.89, batch_ndcg = 34.52 
2025-04-09 11:13:21.266713: Steps 9/90: batch_recall = 43.01, batch_ndcg = 33.24 
2025-04-09 11:13:21.986037: Steps 10/90: batch_recall = 37.65, batch_ndcg = 29.12 
2025-04-09 11:13:22.701857: Steps 11/90: batch_recall = 42.00, batch_ndcg = 29.02 
2025-04-09 11:13:23.409312: Steps 12/90: batch_recall = 36.64, batch_ndcg = 26.40 
2025-04-09 11:13:24.130879: Steps 13/90: batch_recall = 35.98, batch_ndcg = 25.67 
2025-04-09 11:13:24.858630: Steps 14/90: batch_recall = 32.93, batch_ndcg = 23.78 
2025-04-09 11:13:25.568056: Steps 15/90: batch_recall = 43.33, batch_ndcg = 29.28 
2025-04-09 11:13:26.278851: Steps 16/90: batch_recall = 36.57, batch_ndcg = 26.17 
2025-04-09 11:13:26.985442: Steps 17/90: batch_recall = 31.25, batch_ndcg = 21.16 
2025-04-09 11:13:27.685289: Steps 18/90: batch_recall = 36.80, batch_ndcg = 25.57 
2025-04-09 11:13:28.396946: Steps 19/90: batch_recall = 32.71, batch_ndcg = 23.37 
2025-04-09 11:13:29.090376: Steps 20/90: batch_recall = 40.97, batch_ndcg = 27.58 
2025-04-09 11:13:29.794543: Steps 21/90: batch_recall = 40.81, batch_ndcg = 29.71 
2025-04-09 11:13:30.484326: Steps 22/90: batch_recall = 38.96, batch_ndcg = 27.80 
2025-04-09 11:13:31.182463: Steps 23/90: batch_recall = 40.29, batch_ndcg = 27.01 
2025-04-09 11:13:31.866039: Steps 24/90: batch_recall = 38.17, batch_ndcg = 27.82 
2025-04-09 11:13:32.564560: Steps 25/90: batch_recall = 38.91, batch_ndcg = 25.07 
2025-04-09 11:13:33.278235: Steps 26/90: batch_recall = 37.49, batch_ndcg = 27.27 
2025-04-09 11:13:33.976360: Steps 27/90: batch_recall = 34.06, batch_ndcg = 23.43 
2025-04-09 11:13:34.681221: Steps 28/90: batch_recall = 38.49, batch_ndcg = 26.34 
2025-04-09 11:13:35.377261: Steps 29/90: batch_recall = 33.51, batch_ndcg = 23.07 
2025-04-09 11:13:36.071228: Steps 30/90: batch_recall = 35.39, batch_ndcg = 24.16 
2025-04-09 11:13:36.772892: Steps 31/90: batch_recall = 32.56, batch_ndcg = 23.39 
2025-04-09 11:13:37.464340: Steps 32/90: batch_recall = 35.27, batch_ndcg = 23.65 
2025-04-09 11:13:38.161996: Steps 33/90: batch_recall = 34.79, batch_ndcg = 21.73 
2025-04-09 11:13:38.853009: Steps 34/90: batch_recall = 35.20, batch_ndcg = 23.76 
2025-04-09 11:13:39.538189: Steps 35/90: batch_recall = 37.62, batch_ndcg = 27.08 
2025-04-09 11:13:40.234428: Steps 36/90: batch_recall = 36.49, batch_ndcg = 24.80 
2025-04-09 11:13:40.924451: Steps 37/90: batch_recall = 33.42, batch_ndcg = 21.39 
2025-04-09 11:13:41.619813: Steps 38/90: batch_recall = 35.13, batch_ndcg = 23.37 
2025-04-09 11:13:42.311218: Steps 39/90: batch_recall = 35.56, batch_ndcg = 23.01 
2025-04-09 11:13:43.006440: Steps 40/90: batch_recall = 32.89, batch_ndcg = 22.66 
2025-04-09 11:13:43.686777: Steps 41/90: batch_recall = 33.61, batch_ndcg = 21.19 
2025-04-09 11:13:44.374359: Steps 42/90: batch_recall = 44.17, batch_ndcg = 28.77 
2025-04-09 11:13:45.046680: Steps 43/90: batch_recall = 36.18, batch_ndcg = 21.93 
2025-04-09 11:13:45.737341: Steps 44/90: batch_recall = 34.90, batch_ndcg = 23.30 
2025-04-09 11:13:46.425335: Steps 45/90: batch_recall = 32.72, batch_ndcg = 22.47 
2025-04-09 11:13:47.115678: Steps 46/90: batch_recall = 35.47, batch_ndcg = 22.94 
2025-04-09 11:13:47.810952: Steps 47/90: batch_recall = 32.01, batch_ndcg = 20.25 
2025-04-09 11:13:48.496316: Steps 48/90: batch_recall = 37.31, batch_ndcg = 24.34 
2025-04-09 11:13:49.183407: Steps 49/90: batch_recall = 31.37, batch_ndcg = 19.39 
2025-04-09 11:13:49.869229: Steps 50/90: batch_recall = 40.76, batch_ndcg = 26.92 
2025-04-09 11:13:50.555716: Steps 51/90: batch_recall = 40.06, batch_ndcg = 25.69 
2025-04-09 11:13:51.246304: Steps 52/90: batch_recall = 38.17, batch_ndcg = 24.77 
2025-04-09 11:13:51.948965: Steps 53/90: batch_recall = 42.15, batch_ndcg = 26.54 
2025-04-09 11:13:52.636377: Steps 54/90: batch_recall = 31.00, batch_ndcg = 20.46 
2025-04-09 11:13:53.316302: Steps 55/90: batch_recall = 36.33, batch_ndcg = 23.32 
2025-04-09 11:13:54.008906: Steps 56/90: batch_recall = 33.17, batch_ndcg = 20.38 
2025-04-09 11:13:54.704397: Steps 57/90: batch_recall = 37.25, batch_ndcg = 24.29 
2025-04-09 11:13:55.387116: Steps 58/90: batch_recall = 36.37, batch_ndcg = 22.72 
2025-04-09 11:13:56.069949: Steps 59/90: batch_recall = 36.10, batch_ndcg = 23.65 
2025-04-09 11:13:56.751583: Steps 60/90: batch_recall = 34.84, batch_ndcg = 22.77 
2025-04-09 11:13:57.426163: Steps 61/90: batch_recall = 39.50, batch_ndcg = 27.24 
2025-04-09 11:13:58.093870: Steps 62/90: batch_recall = 38.08, batch_ndcg = 25.32 
2025-04-09 11:13:58.774943: Steps 63/90: batch_recall = 31.94, batch_ndcg = 22.87 
2025-04-09 11:13:59.472711: Steps 64/90: batch_recall = 34.56, batch_ndcg = 22.80 
2025-04-09 11:14:00.158150: Steps 65/90: batch_recall = 34.21, batch_ndcg = 22.57 
2025-04-09 11:14:00.836363: Steps 66/90: batch_recall = 38.91, batch_ndcg = 22.60 
2025-04-09 11:14:01.523341: Steps 67/90: batch_recall = 32.14, batch_ndcg = 20.05 
2025-04-09 11:14:02.203595: Steps 68/90: batch_recall = 36.25, batch_ndcg = 22.55 
2025-04-09 11:14:02.890244: Steps 69/90: batch_recall = 41.64, batch_ndcg = 24.86 
2025-04-09 11:14:03.579438: Steps 70/90: batch_recall = 37.91, batch_ndcg = 24.64 
2025-04-09 11:14:04.272017: Steps 71/90: batch_recall = 38.85, batch_ndcg = 23.92 
2025-04-09 11:14:04.951876: Steps 72/90: batch_recall = 32.67, batch_ndcg = 21.24 
2025-04-09 11:14:05.646356: Steps 73/90: batch_recall = 34.80, batch_ndcg = 21.57 
2025-04-09 11:14:06.328995: Steps 74/90: batch_recall = 40.03, batch_ndcg = 24.89 
2025-04-09 11:14:07.018685: Steps 75/90: batch_recall = 38.05, batch_ndcg = 23.07 
2025-04-09 11:14:07.698159: Steps 76/90: batch_recall = 40.19, batch_ndcg = 25.24 
2025-04-09 11:14:08.379558: Steps 77/90: batch_recall = 44.26, batch_ndcg = 28.48 
2025-04-09 11:14:09.052696: Steps 78/90: batch_recall = 33.83, batch_ndcg = 21.17 
2025-04-09 11:14:09.740377: Steps 79/90: batch_recall = 38.88, batch_ndcg = 25.64 
2025-04-09 11:14:10.410588: Steps 80/90: batch_recall = 46.54, batch_ndcg = 29.65 
2025-04-09 11:14:11.083542: Steps 81/90: batch_recall = 44.35, batch_ndcg = 27.77 
2025-04-09 11:14:11.752731: Steps 82/90: batch_recall = 44.28, batch_ndcg = 27.98 
2025-04-09 11:14:12.435090: Steps 83/90: batch_recall = 45.49, batch_ndcg = 27.46 
2025-04-09 11:14:13.109749: Steps 84/90: batch_recall = 35.05, batch_ndcg = 22.26 
2025-04-09 11:14:13.792113: Steps 85/90: batch_recall = 42.28, batch_ndcg = 24.78 
2025-04-09 11:14:14.470986: Steps 86/90: batch_recall = 46.52, batch_ndcg = 30.71 
2025-04-09 11:14:15.147505: Steps 87/90: batch_recall = 42.87, batch_ndcg = 28.60 
2025-04-09 11:14:15.822705: Steps 88/90: batch_recall = 48.82, batch_ndcg = 29.71 
2025-04-09 11:14:16.355869: Steps 89/90: batch_recall = 39.62, batch_ndcg = 25.84 
2025-04-09 11:14:16.356583: Epoch 47/1000, Test: Recall = 0.0756, NDCG = 0.0506  

2025-04-09 11:14:17.367507: Training Step 0/115: batchLoss = 4.1842, diffLoss = 6.8559, kgLoss = 0.1767
2025-04-09 11:14:18.088660: Training Step 1/115: batchLoss = 4.2972, diffLoss = 7.0333, kgLoss = 0.1931
2025-04-09 11:14:18.806794: Training Step 2/115: batchLoss = 3.9919, diffLoss = 6.5424, kgLoss = 0.1662
2025-04-09 11:14:19.525876: Training Step 3/115: batchLoss = 4.2711, diffLoss = 6.9999, kgLoss = 0.1779
2025-04-09 11:14:20.242398: Training Step 4/115: batchLoss = 3.6855, diffLoss = 6.0409, kgLoss = 0.1523
2025-04-09 11:14:20.966205: Training Step 5/115: batchLoss = 4.1286, diffLoss = 6.7545, kgLoss = 0.1898
2025-04-09 11:14:21.678902: Training Step 6/115: batchLoss = 4.1634, diffLoss = 6.8200, kgLoss = 0.1784
2025-04-09 11:14:22.399915: Training Step 7/115: batchLoss = 4.3012, diffLoss = 7.0441, kgLoss = 0.1869
2025-04-09 11:14:23.115452: Training Step 8/115: batchLoss = 4.4790, diffLoss = 7.3382, kgLoss = 0.1902
2025-04-09 11:14:23.828498: Training Step 9/115: batchLoss = 3.8776, diffLoss = 6.3426, kgLoss = 0.1800
2025-04-09 11:14:24.538964: Training Step 10/115: batchLoss = 3.9805, diffLoss = 6.5266, kgLoss = 0.1615
2025-04-09 11:14:25.253127: Training Step 11/115: batchLoss = 4.1375, diffLoss = 6.7808, kgLoss = 0.1725
2025-04-09 11:14:25.961481: Training Step 12/115: batchLoss = 4.1324, diffLoss = 6.7745, kgLoss = 0.1693
2025-04-09 11:14:26.677208: Training Step 13/115: batchLoss = 3.4098, diffLoss = 5.5834, kgLoss = 0.1494
2025-04-09 11:14:27.395944: Training Step 14/115: batchLoss = 3.9630, diffLoss = 6.4943, kgLoss = 0.1659
2025-04-09 11:14:28.109165: Training Step 15/115: batchLoss = 4.1389, diffLoss = 6.7783, kgLoss = 0.1798
2025-04-09 11:14:28.822934: Training Step 16/115: batchLoss = 4.3515, diffLoss = 7.1283, kgLoss = 0.1863
2025-04-09 11:14:29.544037: Training Step 17/115: batchLoss = 4.1133, diffLoss = 6.7366, kgLoss = 0.1783
2025-04-09 11:14:30.260532: Training Step 18/115: batchLoss = 3.4548, diffLoss = 5.6574, kgLoss = 0.1510
2025-04-09 11:14:30.978005: Training Step 19/115: batchLoss = 4.1401, diffLoss = 6.7829, kgLoss = 0.1759
2025-04-09 11:14:31.695085: Training Step 20/115: batchLoss = 3.7490, diffLoss = 6.1390, kgLoss = 0.1640
2025-04-09 11:14:32.411741: Training Step 21/115: batchLoss = 4.2256, diffLoss = 6.9209, kgLoss = 0.1827
2025-04-09 11:14:33.130755: Training Step 22/115: batchLoss = 3.9979, diffLoss = 6.5463, kgLoss = 0.1752
2025-04-09 11:14:33.840485: Training Step 23/115: batchLoss = 3.8449, diffLoss = 6.3042, kgLoss = 0.1560
2025-04-09 11:14:34.559733: Training Step 24/115: batchLoss = 3.7519, diffLoss = 6.1412, kgLoss = 0.1680
2025-04-09 11:14:35.277375: Training Step 25/115: batchLoss = 4.1181, diffLoss = 6.7428, kgLoss = 0.1812
2025-04-09 11:14:35.991754: Training Step 26/115: batchLoss = 4.1143, diffLoss = 6.7450, kgLoss = 0.1682
2025-04-09 11:14:36.704282: Training Step 27/115: batchLoss = 3.9459, diffLoss = 6.4626, kgLoss = 0.1708
2025-04-09 11:14:37.415964: Training Step 28/115: batchLoss = 3.8397, diffLoss = 6.2833, kgLoss = 0.1744
2025-04-09 11:14:38.131574: Training Step 29/115: batchLoss = 4.4494, diffLoss = 7.2888, kgLoss = 0.1903
2025-04-09 11:14:38.846159: Training Step 30/115: batchLoss = 3.8384, diffLoss = 6.2932, kgLoss = 0.1563
2025-04-09 11:14:39.549192: Training Step 31/115: batchLoss = 4.0882, diffLoss = 6.7048, kgLoss = 0.1633
2025-04-09 11:14:40.257312: Training Step 32/115: batchLoss = 4.1887, diffLoss = 6.8697, kgLoss = 0.1672
2025-04-09 11:14:40.974308: Training Step 33/115: batchLoss = 4.3732, diffLoss = 7.1681, kgLoss = 0.1809
2025-04-09 11:14:41.685261: Training Step 34/115: batchLoss = 4.1795, diffLoss = 6.8434, kgLoss = 0.1836
2025-04-09 11:14:42.404161: Training Step 35/115: batchLoss = 4.2042, diffLoss = 6.8877, kgLoss = 0.1790
2025-04-09 11:14:43.118270: Training Step 36/115: batchLoss = 3.9855, diffLoss = 6.5328, kgLoss = 0.1645
2025-04-09 11:14:43.834414: Training Step 37/115: batchLoss = 3.9872, diffLoss = 6.5368, kgLoss = 0.1627
2025-04-09 11:14:44.560021: Training Step 38/115: batchLoss = 3.8656, diffLoss = 6.3363, kgLoss = 0.1596
2025-04-09 11:14:45.280577: Training Step 39/115: batchLoss = 4.2756, diffLoss = 7.0043, kgLoss = 0.1825
2025-04-09 11:14:45.994323: Training Step 40/115: batchLoss = 4.1461, diffLoss = 6.7927, kgLoss = 0.1761
2025-04-09 11:14:46.714636: Training Step 41/115: batchLoss = 4.2838, diffLoss = 7.0233, kgLoss = 0.1746
2025-04-09 11:14:47.432801: Training Step 42/115: batchLoss = 3.9115, diffLoss = 6.4074, kgLoss = 0.1677
2025-04-09 11:14:48.157676: Training Step 43/115: batchLoss = 4.6058, diffLoss = 7.5526, kgLoss = 0.1856
2025-04-09 11:14:48.872889: Training Step 44/115: batchLoss = 4.3728, diffLoss = 7.1630, kgLoss = 0.1874
2025-04-09 11:14:49.592848: Training Step 45/115: batchLoss = 3.4461, diffLoss = 5.6366, kgLoss = 0.1604
2025-04-09 11:14:50.304743: Training Step 46/115: batchLoss = 4.1386, diffLoss = 6.7843, kgLoss = 0.1699
2025-04-09 11:14:51.029894: Training Step 47/115: batchLoss = 3.8118, diffLoss = 6.2502, kgLoss = 0.1544
2025-04-09 11:14:51.738504: Training Step 48/115: batchLoss = 4.2946, diffLoss = 7.0343, kgLoss = 0.1851
2025-04-09 11:14:52.451062: Training Step 49/115: batchLoss = 4.1066, diffLoss = 6.7252, kgLoss = 0.1788
2025-04-09 11:14:53.256447: Training Step 50/115: batchLoss = 3.8005, diffLoss = 6.2301, kgLoss = 0.1561
2025-04-09 11:14:53.974681: Training Step 51/115: batchLoss = 3.7343, diffLoss = 6.1155, kgLoss = 0.1625
2025-04-09 11:14:54.691203: Training Step 52/115: batchLoss = 3.9006, diffLoss = 6.3846, kgLoss = 0.1747
2025-04-09 11:14:55.411492: Training Step 53/115: batchLoss = 4.6421, diffLoss = 7.5957, kgLoss = 0.2118
2025-04-09 11:14:56.139139: Training Step 54/115: batchLoss = 4.6707, diffLoss = 7.6442, kgLoss = 0.2103
2025-04-09 11:14:56.854063: Training Step 55/115: batchLoss = 4.5807, diffLoss = 7.5096, kgLoss = 0.1874
2025-04-09 11:14:57.571281: Training Step 56/115: batchLoss = 4.4050, diffLoss = 7.2225, kgLoss = 0.1787
2025-04-09 11:14:58.289933: Training Step 57/115: batchLoss = 3.9511, diffLoss = 6.4665, kgLoss = 0.1780
2025-04-09 11:14:59.000377: Training Step 58/115: batchLoss = 3.9444, diffLoss = 6.4645, kgLoss = 0.1643
2025-04-09 11:14:59.715331: Training Step 59/115: batchLoss = 4.0343, diffLoss = 6.6140, kgLoss = 0.1647
2025-04-09 11:15:00.431330: Training Step 60/115: batchLoss = 3.7379, diffLoss = 6.1218, kgLoss = 0.1621
2025-04-09 11:15:01.154032: Training Step 61/115: batchLoss = 3.8671, diffLoss = 6.3421, kgLoss = 0.1545
2025-04-09 11:15:01.884064: Training Step 62/115: batchLoss = 3.8128, diffLoss = 6.2540, kgLoss = 0.1512
2025-04-09 11:15:02.611838: Training Step 63/115: batchLoss = 4.2509, diffLoss = 6.9660, kgLoss = 0.1782
2025-04-09 11:15:03.327311: Training Step 64/115: batchLoss = 4.5316, diffLoss = 7.4282, kgLoss = 0.1868
2025-04-09 11:15:04.040327: Training Step 65/115: batchLoss = 3.9590, diffLoss = 6.4878, kgLoss = 0.1660
2025-04-09 11:15:04.748356: Training Step 66/115: batchLoss = 4.3023, diffLoss = 7.0522, kgLoss = 0.1775
2025-04-09 11:15:05.459130: Training Step 67/115: batchLoss = 3.9015, diffLoss = 6.3916, kgLoss = 0.1662
2025-04-09 11:15:06.171949: Training Step 68/115: batchLoss = 4.5172, diffLoss = 7.4060, kgLoss = 0.1840
2025-04-09 11:15:06.881934: Training Step 69/115: batchLoss = 3.8865, diffLoss = 6.3694, kgLoss = 0.1622
2025-04-09 11:15:07.596642: Training Step 70/115: batchLoss = 3.8027, diffLoss = 6.2288, kgLoss = 0.1635
2025-04-09 11:15:08.322895: Training Step 71/115: batchLoss = 3.9036, diffLoss = 6.4002, kgLoss = 0.1586
2025-04-09 11:15:09.044958: Training Step 72/115: batchLoss = 4.2551, diffLoss = 6.9712, kgLoss = 0.1809
2025-04-09 11:15:09.770229: Training Step 73/115: batchLoss = 4.0998, diffLoss = 6.7081, kgLoss = 0.1875
2025-04-09 11:15:10.495681: Training Step 74/115: batchLoss = 3.6893, diffLoss = 6.0504, kgLoss = 0.1476
2025-04-09 11:15:11.213640: Training Step 75/115: batchLoss = 4.2666, diffLoss = 6.9876, kgLoss = 0.1851
2025-04-09 11:15:11.931734: Training Step 76/115: batchLoss = 4.2755, diffLoss = 7.0008, kgLoss = 0.1877
2025-04-09 11:15:12.732937: Training Step 77/115: batchLoss = 4.1036, diffLoss = 6.7208, kgLoss = 0.1778
2025-04-09 11:15:13.455105: Training Step 78/115: batchLoss = 3.7726, diffLoss = 6.1846, kgLoss = 0.1546
2025-04-09 11:15:14.178347: Training Step 79/115: batchLoss = 3.9758, diffLoss = 6.5140, kgLoss = 0.1685
2025-04-09 11:15:14.900304: Training Step 80/115: batchLoss = 3.8783, diffLoss = 6.3536, kgLoss = 0.1653
2025-04-09 11:15:15.616753: Training Step 81/115: batchLoss = 4.0748, diffLoss = 6.6732, kgLoss = 0.1772
2025-04-09 11:15:16.335784: Training Step 82/115: batchLoss = 3.8053, diffLoss = 6.2379, kgLoss = 0.1564
2025-04-09 11:15:17.053740: Training Step 83/115: batchLoss = 3.7700, diffLoss = 6.1694, kgLoss = 0.1709
2025-04-09 11:15:17.766435: Training Step 84/115: batchLoss = 3.7327, diffLoss = 6.1112, kgLoss = 0.1651
2025-04-09 11:15:18.475961: Training Step 85/115: batchLoss = 3.8522, diffLoss = 6.3067, kgLoss = 0.1703
2025-04-09 11:15:19.191822: Training Step 86/115: batchLoss = 4.8944, diffLoss = 8.0215, kgLoss = 0.2039
2025-04-09 11:15:19.907064: Training Step 87/115: batchLoss = 4.3740, diffLoss = 7.1610, kgLoss = 0.1935
2025-04-09 11:15:20.620287: Training Step 88/115: batchLoss = 4.3805, diffLoss = 7.1835, kgLoss = 0.1760
2025-04-09 11:15:21.334761: Training Step 89/115: batchLoss = 4.1234, diffLoss = 6.7500, kgLoss = 0.1834
2025-04-09 11:15:22.056258: Training Step 90/115: batchLoss = 4.2569, diffLoss = 6.9785, kgLoss = 0.1746
2025-04-09 11:15:22.775396: Training Step 91/115: batchLoss = 4.2966, diffLoss = 7.0370, kgLoss = 0.1860
2025-04-09 11:15:23.489964: Training Step 92/115: batchLoss = 3.6141, diffLoss = 5.9076, kgLoss = 0.1737
2025-04-09 11:15:24.209487: Training Step 93/115: batchLoss = 4.1675, diffLoss = 6.8302, kgLoss = 0.1735
2025-04-09 11:15:24.921756: Training Step 94/115: batchLoss = 4.0360, diffLoss = 6.6120, kgLoss = 0.1721
2025-04-09 11:15:25.637570: Training Step 95/115: batchLoss = 4.3712, diffLoss = 7.1611, kgLoss = 0.1862
2025-04-09 11:15:26.354173: Training Step 96/115: batchLoss = 3.8038, diffLoss = 6.2266, kgLoss = 0.1697
2025-04-09 11:15:27.080364: Training Step 97/115: batchLoss = 4.0721, diffLoss = 6.6767, kgLoss = 0.1653
2025-04-09 11:15:27.791734: Training Step 98/115: batchLoss = 4.5274, diffLoss = 7.4238, kgLoss = 0.1828
2025-04-09 11:15:28.513025: Training Step 99/115: batchLoss = 3.8109, diffLoss = 6.2414, kgLoss = 0.1650
2025-04-09 11:15:29.233854: Training Step 100/115: batchLoss = 3.9907, diffLoss = 6.5319, kgLoss = 0.1788
2025-04-09 11:15:29.954759: Training Step 101/115: batchLoss = 4.2085, diffLoss = 6.8961, kgLoss = 0.1772
2025-04-09 11:15:30.677804: Training Step 102/115: batchLoss = 4.0687, diffLoss = 6.6679, kgLoss = 0.1698
2025-04-09 11:15:31.391018: Training Step 103/115: batchLoss = 3.9421, diffLoss = 6.4621, kgLoss = 0.1622
2025-04-09 11:15:32.102787: Training Step 104/115: batchLoss = 4.2373, diffLoss = 6.9432, kgLoss = 0.1784
2025-04-09 11:15:32.818554: Training Step 105/115: batchLoss = 4.4413, diffLoss = 7.2772, kgLoss = 0.1876
2025-04-09 11:15:33.528570: Training Step 106/115: batchLoss = 4.1264, diffLoss = 6.7596, kgLoss = 0.1767
2025-04-09 11:15:34.240441: Training Step 107/115: batchLoss = 4.1078, diffLoss = 6.7333, kgLoss = 0.1694
2025-04-09 11:15:34.957833: Training Step 108/115: batchLoss = 4.0775, diffLoss = 6.6849, kgLoss = 0.1665
2025-04-09 11:15:35.677044: Training Step 109/115: batchLoss = 4.1860, diffLoss = 6.8648, kgLoss = 0.1677
2025-04-09 11:15:36.389973: Training Step 110/115: batchLoss = 4.2591, diffLoss = 6.9802, kgLoss = 0.1775
2025-04-09 11:15:37.119808: Training Step 111/115: batchLoss = 4.2920, diffLoss = 7.0316, kgLoss = 0.1825
2025-04-09 11:15:37.821608: Training Step 112/115: batchLoss = 4.2854, diffLoss = 7.0295, kgLoss = 0.1692
2025-04-09 11:15:38.459575: Training Step 113/115: batchLoss = 4.2066, diffLoss = 6.8939, kgLoss = 0.1757
2025-04-09 11:15:39.066987: Training Step 114/115: batchLoss = 3.9632, diffLoss = 6.4938, kgLoss = 0.1672
2025-04-09 11:15:39.184422: 
2025-04-09 11:15:39.185379: Epoch 48/1000, Train: epLoss = 1.1754, epDfLoss = 1.9257, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:15:39.926337: Steps 0/90: batch_recall = 56.46, batch_ndcg = 43.78 
2025-04-09 11:15:40.635271: Steps 1/90: batch_recall = 56.90, batch_ndcg = 40.14 
2025-04-09 11:15:41.358218: Steps 2/90: batch_recall = 51.14, batch_ndcg = 38.70 
2025-04-09 11:15:42.072895: Steps 3/90: batch_recall = 44.26, batch_ndcg = 32.23 
2025-04-09 11:15:42.789076: Steps 4/90: batch_recall = 48.11, batch_ndcg = 35.20 
2025-04-09 11:15:43.510757: Steps 5/90: batch_recall = 37.50, batch_ndcg = 27.60 
2025-04-09 11:15:44.217441: Steps 6/90: batch_recall = 45.79, batch_ndcg = 31.90 
2025-04-09 11:15:44.940533: Steps 7/90: batch_recall = 43.58, batch_ndcg = 31.71 
2025-04-09 11:15:45.657591: Steps 8/90: batch_recall = 46.81, batch_ndcg = 34.54 
2025-04-09 11:15:46.376726: Steps 9/90: batch_recall = 43.27, batch_ndcg = 33.30 
2025-04-09 11:15:47.091959: Steps 10/90: batch_recall = 37.21, batch_ndcg = 28.95 
2025-04-09 11:15:47.803206: Steps 11/90: batch_recall = 41.87, batch_ndcg = 29.30 
2025-04-09 11:15:48.510147: Steps 12/90: batch_recall = 38.02, batch_ndcg = 27.05 
2025-04-09 11:15:49.225175: Steps 13/90: batch_recall = 35.20, batch_ndcg = 25.68 
2025-04-09 11:15:49.943692: Steps 14/90: batch_recall = 33.04, batch_ndcg = 23.67 
2025-04-09 11:15:50.654788: Steps 15/90: batch_recall = 45.67, batch_ndcg = 30.33 
2025-04-09 11:15:51.356644: Steps 16/90: batch_recall = 36.86, batch_ndcg = 26.09 
2025-04-09 11:15:52.055794: Steps 17/90: batch_recall = 32.15, batch_ndcg = 21.61 
2025-04-09 11:15:52.751429: Steps 18/90: batch_recall = 37.95, batch_ndcg = 26.07 
2025-04-09 11:15:53.467437: Steps 19/90: batch_recall = 31.82, batch_ndcg = 23.14 
2025-04-09 11:15:54.158343: Steps 20/90: batch_recall = 39.96, batch_ndcg = 27.45 
2025-04-09 11:15:54.876706: Steps 21/90: batch_recall = 41.92, batch_ndcg = 30.34 
2025-04-09 11:15:55.578571: Steps 22/90: batch_recall = 39.61, batch_ndcg = 28.17 
2025-04-09 11:15:56.278104: Steps 23/90: batch_recall = 39.36, batch_ndcg = 26.72 
2025-04-09 11:15:56.977325: Steps 24/90: batch_recall = 37.39, batch_ndcg = 27.33 
2025-04-09 11:15:57.662241: Steps 25/90: batch_recall = 38.53, batch_ndcg = 25.06 
2025-04-09 11:15:58.354223: Steps 26/90: batch_recall = 38.53, batch_ndcg = 27.61 
2025-04-09 11:15:59.039961: Steps 27/90: batch_recall = 35.47, batch_ndcg = 24.32 
2025-04-09 11:15:59.732551: Steps 28/90: batch_recall = 37.99, batch_ndcg = 26.01 
2025-04-09 11:16:00.421600: Steps 29/90: batch_recall = 33.24, batch_ndcg = 23.30 
2025-04-09 11:16:01.108708: Steps 30/90: batch_recall = 36.34, batch_ndcg = 24.72 
2025-04-09 11:16:01.810847: Steps 31/90: batch_recall = 32.53, batch_ndcg = 23.52 
2025-04-09 11:16:02.516437: Steps 32/90: batch_recall = 33.57, batch_ndcg = 23.01 
2025-04-09 11:16:03.208847: Steps 33/90: batch_recall = 34.46, batch_ndcg = 21.40 
2025-04-09 11:16:03.904258: Steps 34/90: batch_recall = 34.37, batch_ndcg = 23.36 
2025-04-09 11:16:04.593493: Steps 35/90: batch_recall = 37.07, batch_ndcg = 26.21 
2025-04-09 11:16:05.294934: Steps 36/90: batch_recall = 36.75, batch_ndcg = 24.63 
2025-04-09 11:16:05.992149: Steps 37/90: batch_recall = 32.96, batch_ndcg = 21.10 
2025-04-09 11:16:06.684339: Steps 38/90: batch_recall = 36.07, batch_ndcg = 23.89 
2025-04-09 11:16:07.380696: Steps 39/90: batch_recall = 36.44, batch_ndcg = 23.15 
2025-04-09 11:16:08.072950: Steps 40/90: batch_recall = 33.26, batch_ndcg = 22.36 
2025-04-09 11:16:08.761154: Steps 41/90: batch_recall = 33.23, batch_ndcg = 21.04 
2025-04-09 11:16:09.449312: Steps 42/90: batch_recall = 43.90, batch_ndcg = 28.83 
2025-04-09 11:16:10.134274: Steps 43/90: batch_recall = 36.30, batch_ndcg = 22.33 
2025-04-09 11:16:10.818822: Steps 44/90: batch_recall = 34.45, batch_ndcg = 23.21 
2025-04-09 11:16:11.503021: Steps 45/90: batch_recall = 32.06, batch_ndcg = 22.26 
2025-04-09 11:16:12.185327: Steps 46/90: batch_recall = 37.04, batch_ndcg = 23.42 
2025-04-09 11:16:12.863354: Steps 47/90: batch_recall = 32.47, batch_ndcg = 20.36 
2025-04-09 11:16:13.547676: Steps 48/90: batch_recall = 37.21, batch_ndcg = 24.44 
2025-04-09 11:16:14.241508: Steps 49/90: batch_recall = 30.93, batch_ndcg = 19.27 
2025-04-09 11:16:14.921196: Steps 50/90: batch_recall = 41.50, batch_ndcg = 26.71 
2025-04-09 11:16:15.706066: Steps 51/90: batch_recall = 40.51, batch_ndcg = 25.90 
2025-04-09 11:16:16.394090: Steps 52/90: batch_recall = 37.92, batch_ndcg = 25.16 
2025-04-09 11:16:17.089457: Steps 53/90: batch_recall = 41.32, batch_ndcg = 26.51 
2025-04-09 11:16:17.775061: Steps 54/90: batch_recall = 30.75, batch_ndcg = 20.61 
2025-04-09 11:16:18.460884: Steps 55/90: batch_recall = 37.03, batch_ndcg = 23.48 
2025-04-09 11:16:19.148945: Steps 56/90: batch_recall = 33.00, batch_ndcg = 20.33 
2025-04-09 11:16:19.849688: Steps 57/90: batch_recall = 38.62, batch_ndcg = 24.95 
2025-04-09 11:16:20.540381: Steps 58/90: batch_recall = 36.25, batch_ndcg = 22.89 
2025-04-09 11:16:21.234178: Steps 59/90: batch_recall = 35.93, batch_ndcg = 23.73 
2025-04-09 11:16:21.917746: Steps 60/90: batch_recall = 34.32, batch_ndcg = 22.40 
2025-04-09 11:16:22.607103: Steps 61/90: batch_recall = 39.78, batch_ndcg = 27.05 
2025-04-09 11:16:23.297223: Steps 62/90: batch_recall = 38.53, batch_ndcg = 25.51 
2025-04-09 11:16:23.978065: Steps 63/90: batch_recall = 31.91, batch_ndcg = 23.14 
2025-04-09 11:16:24.657187: Steps 64/90: batch_recall = 35.92, batch_ndcg = 23.54 
2025-04-09 11:16:25.326393: Steps 65/90: batch_recall = 35.86, batch_ndcg = 22.96 
2025-04-09 11:16:26.007151: Steps 66/90: batch_recall = 37.98, batch_ndcg = 22.45 
2025-04-09 11:16:26.679593: Steps 67/90: batch_recall = 30.88, batch_ndcg = 19.61 
2025-04-09 11:16:27.355107: Steps 68/90: batch_recall = 37.25, batch_ndcg = 22.98 
2025-04-09 11:16:28.039283: Steps 69/90: batch_recall = 41.81, batch_ndcg = 24.98 
2025-04-09 11:16:28.722671: Steps 70/90: batch_recall = 38.56, batch_ndcg = 24.58 
2025-04-09 11:16:29.421129: Steps 71/90: batch_recall = 38.27, batch_ndcg = 23.87 
2025-04-09 11:16:30.102132: Steps 72/90: batch_recall = 33.52, batch_ndcg = 21.44 
2025-04-09 11:16:30.790572: Steps 73/90: batch_recall = 36.19, batch_ndcg = 22.12 
2025-04-09 11:16:31.467402: Steps 74/90: batch_recall = 40.58, batch_ndcg = 25.15 
2025-04-09 11:16:32.155086: Steps 75/90: batch_recall = 38.71, batch_ndcg = 23.12 
2025-04-09 11:16:32.839946: Steps 76/90: batch_recall = 39.88, batch_ndcg = 25.14 
2025-04-09 11:16:33.518646: Steps 77/90: batch_recall = 44.43, batch_ndcg = 28.48 
2025-04-09 11:16:34.201934: Steps 78/90: batch_recall = 34.49, batch_ndcg = 21.16 
2025-04-09 11:16:34.882732: Steps 79/90: batch_recall = 39.02, batch_ndcg = 25.62 
2025-04-09 11:16:35.560177: Steps 80/90: batch_recall = 45.58, batch_ndcg = 29.06 
2025-04-09 11:16:36.239612: Steps 81/90: batch_recall = 44.56, batch_ndcg = 27.48 
2025-04-09 11:16:36.916692: Steps 82/90: batch_recall = 44.05, batch_ndcg = 27.88 
2025-04-09 11:16:37.592523: Steps 83/90: batch_recall = 45.71, batch_ndcg = 27.20 
2025-04-09 11:16:38.273752: Steps 84/90: batch_recall = 35.09, batch_ndcg = 22.00 
2025-04-09 11:16:38.942522: Steps 85/90: batch_recall = 42.45, batch_ndcg = 25.20 
2025-04-09 11:16:39.616148: Steps 86/90: batch_recall = 45.60, batch_ndcg = 30.66 
2025-04-09 11:16:40.283842: Steps 87/90: batch_recall = 43.33, batch_ndcg = 29.16 
2025-04-09 11:16:40.954947: Steps 88/90: batch_recall = 48.54, batch_ndcg = 29.52 
2025-04-09 11:16:41.488194: Steps 89/90: batch_recall = 38.84, batch_ndcg = 25.44 
2025-04-09 11:16:41.488867: Epoch 48/1000, Test: Recall = 0.0758, NDCG = 0.0507  

2025-04-09 11:16:42.492135: Training Step 0/115: batchLoss = 3.8532, diffLoss = 6.3099, kgLoss = 0.1682
2025-04-09 11:16:43.202632: Training Step 1/115: batchLoss = 4.0088, diffLoss = 6.5736, kgLoss = 0.1616
2025-04-09 11:16:43.909531: Training Step 2/115: batchLoss = 3.8043, diffLoss = 6.2322, kgLoss = 0.1624
2025-04-09 11:16:44.632980: Training Step 3/115: batchLoss = 3.6054, diffLoss = 5.8982, kgLoss = 0.1663
2025-04-09 11:16:45.350596: Training Step 4/115: batchLoss = 4.1785, diffLoss = 6.8484, kgLoss = 0.1736
2025-04-09 11:16:46.073701: Training Step 5/115: batchLoss = 4.2516, diffLoss = 6.9668, kgLoss = 0.1787
2025-04-09 11:16:46.792577: Training Step 6/115: batchLoss = 4.0236, diffLoss = 6.5923, kgLoss = 0.1706
2025-04-09 11:16:47.516829: Training Step 7/115: batchLoss = 4.5784, diffLoss = 7.5001, kgLoss = 0.1958
2025-04-09 11:16:48.234946: Training Step 8/115: batchLoss = 3.9750, diffLoss = 6.5104, kgLoss = 0.1718
2025-04-09 11:16:48.947418: Training Step 9/115: batchLoss = 4.3537, diffLoss = 7.1308, kgLoss = 0.1881
2025-04-09 11:16:49.668440: Training Step 10/115: batchLoss = 4.2398, diffLoss = 6.9527, kgLoss = 0.1706
2025-04-09 11:16:50.380367: Training Step 11/115: batchLoss = 4.2436, diffLoss = 6.9540, kgLoss = 0.1780
2025-04-09 11:16:51.094086: Training Step 12/115: batchLoss = 4.4562, diffLoss = 7.3109, kgLoss = 0.1742
2025-04-09 11:16:51.800348: Training Step 13/115: batchLoss = 4.2353, diffLoss = 6.9455, kgLoss = 0.1700
2025-04-09 11:16:52.511592: Training Step 14/115: batchLoss = 3.9930, diffLoss = 6.5397, kgLoss = 0.1730
2025-04-09 11:16:53.223822: Training Step 15/115: batchLoss = 3.7377, diffLoss = 6.1252, kgLoss = 0.1565
2025-04-09 11:16:53.936378: Training Step 16/115: batchLoss = 4.0840, diffLoss = 6.6969, kgLoss = 0.1646
2025-04-09 11:16:54.649874: Training Step 17/115: batchLoss = 3.6499, diffLoss = 5.9742, kgLoss = 0.1633
2025-04-09 11:16:55.377940: Training Step 18/115: batchLoss = 3.8233, diffLoss = 6.2648, kgLoss = 0.1612
2025-04-09 11:16:56.089301: Training Step 19/115: batchLoss = 4.1533, diffLoss = 6.8046, kgLoss = 0.1762
2025-04-09 11:16:56.803742: Training Step 20/115: batchLoss = 4.3478, diffLoss = 7.1202, kgLoss = 0.1893
2025-04-09 11:16:57.512916: Training Step 21/115: batchLoss = 3.9510, diffLoss = 6.4710, kgLoss = 0.1708
2025-04-09 11:16:58.233185: Training Step 22/115: batchLoss = 3.7628, diffLoss = 6.1655, kgLoss = 0.1589
2025-04-09 11:16:58.950631: Training Step 23/115: batchLoss = 4.1428, diffLoss = 6.7862, kgLoss = 0.1776
2025-04-09 11:16:59.665108: Training Step 24/115: batchLoss = 3.8385, diffLoss = 6.2931, kgLoss = 0.1567
2025-04-09 11:17:00.380404: Training Step 25/115: batchLoss = 3.9672, diffLoss = 6.4970, kgLoss = 0.1726
2025-04-09 11:17:01.098945: Training Step 26/115: batchLoss = 3.9994, diffLoss = 6.5537, kgLoss = 0.1680
2025-04-09 11:17:01.822470: Training Step 27/115: batchLoss = 3.9616, diffLoss = 6.4934, kgLoss = 0.1638
2025-04-09 11:17:02.540750: Training Step 28/115: batchLoss = 3.5441, diffLoss = 5.8096, kgLoss = 0.1458
2025-04-09 11:17:03.254976: Training Step 29/115: batchLoss = 4.3925, diffLoss = 7.1940, kgLoss = 0.1904
2025-04-09 11:17:03.976653: Training Step 30/115: batchLoss = 4.3555, diffLoss = 7.1379, kgLoss = 0.1817
2025-04-09 11:17:04.686468: Training Step 31/115: batchLoss = 4.5671, diffLoss = 7.4909, kgLoss = 0.1813
2025-04-09 11:17:05.401137: Training Step 32/115: batchLoss = 4.1123, diffLoss = 6.7444, kgLoss = 0.1640
2025-04-09 11:17:06.109001: Training Step 33/115: batchLoss = 4.2057, diffLoss = 6.8919, kgLoss = 0.1765
2025-04-09 11:17:06.819638: Training Step 34/115: batchLoss = 3.8591, diffLoss = 6.3243, kgLoss = 0.1613
2025-04-09 11:17:07.527071: Training Step 35/115: batchLoss = 5.1027, diffLoss = 8.3613, kgLoss = 0.2148
2025-04-09 11:17:08.240302: Training Step 36/115: batchLoss = 4.2639, diffLoss = 6.9883, kgLoss = 0.1773
2025-04-09 11:17:08.957755: Training Step 37/115: batchLoss = 3.8913, diffLoss = 6.3738, kgLoss = 0.1676
2025-04-09 11:17:09.676026: Training Step 38/115: batchLoss = 4.0223, diffLoss = 6.5890, kgLoss = 0.1723
2025-04-09 11:17:10.393950: Training Step 39/115: batchLoss = 4.0147, diffLoss = 6.5756, kgLoss = 0.1733
2025-04-09 11:17:11.110852: Training Step 40/115: batchLoss = 4.2142, diffLoss = 6.9044, kgLoss = 0.1790
2025-04-09 11:17:11.832541: Training Step 41/115: batchLoss = 4.5351, diffLoss = 7.4329, kgLoss = 0.1884
2025-04-09 11:17:12.551086: Training Step 42/115: batchLoss = 3.9505, diffLoss = 6.4720, kgLoss = 0.1683
2025-04-09 11:17:13.282489: Training Step 43/115: batchLoss = 4.1481, diffLoss = 6.7969, kgLoss = 0.1750
2025-04-09 11:17:14.006670: Training Step 44/115: batchLoss = 4.5536, diffLoss = 7.4691, kgLoss = 0.1804
2025-04-09 11:17:14.728880: Training Step 45/115: batchLoss = 4.3444, diffLoss = 7.1186, kgLoss = 0.1830
2025-04-09 11:17:15.449505: Training Step 46/115: batchLoss = 3.6973, diffLoss = 6.0549, kgLoss = 0.1609
2025-04-09 11:17:16.167680: Training Step 47/115: batchLoss = 4.0971, diffLoss = 6.7118, kgLoss = 0.1751
2025-04-09 11:17:16.891104: Training Step 48/115: batchLoss = 4.2687, diffLoss = 6.9999, kgLoss = 0.1720
2025-04-09 11:17:17.609175: Training Step 49/115: batchLoss = 3.8598, diffLoss = 6.3218, kgLoss = 0.1670
2025-04-09 11:17:18.326168: Training Step 50/115: batchLoss = 3.9190, diffLoss = 6.4268, kgLoss = 0.1573
2025-04-09 11:17:19.052161: Training Step 51/115: batchLoss = 3.7229, diffLoss = 6.1005, kgLoss = 0.1566
2025-04-09 11:17:19.760835: Training Step 52/115: batchLoss = 4.7034, diffLoss = 7.7135, kgLoss = 0.1884
2025-04-09 11:17:20.473329: Training Step 53/115: batchLoss = 4.1752, diffLoss = 6.8382, kgLoss = 0.1809
2025-04-09 11:17:21.182159: Training Step 54/115: batchLoss = 4.3259, diffLoss = 7.0914, kgLoss = 0.1777
2025-04-09 11:17:21.897442: Training Step 55/115: batchLoss = 4.3941, diffLoss = 7.1995, kgLoss = 0.1859
2025-04-09 11:17:22.617723: Training Step 56/115: batchLoss = 4.2103, diffLoss = 6.8880, kgLoss = 0.1937
2025-04-09 11:17:23.332423: Training Step 57/115: batchLoss = 4.1763, diffLoss = 6.8439, kgLoss = 0.1749
2025-04-09 11:17:24.049249: Training Step 58/115: batchLoss = 3.7099, diffLoss = 6.0778, kgLoss = 0.1581
2025-04-09 11:17:24.771476: Training Step 59/115: batchLoss = 4.0924, diffLoss = 6.7044, kgLoss = 0.1744
2025-04-09 11:17:25.499178: Training Step 60/115: batchLoss = 3.7568, diffLoss = 6.1529, kgLoss = 0.1627
2025-04-09 11:17:26.219181: Training Step 61/115: batchLoss = 4.2061, diffLoss = 6.8981, kgLoss = 0.1679
2025-04-09 11:17:26.939342: Training Step 62/115: batchLoss = 3.8908, diffLoss = 6.3725, kgLoss = 0.1681
2025-04-09 11:17:27.660010: Training Step 63/115: batchLoss = 4.1158, diffLoss = 6.7445, kgLoss = 0.1727
2025-04-09 11:17:28.372154: Training Step 64/115: batchLoss = 4.4991, diffLoss = 7.3788, kgLoss = 0.1795
2025-04-09 11:17:29.091593: Training Step 65/115: batchLoss = 3.8797, diffLoss = 6.3513, kgLoss = 0.1723
2025-04-09 11:17:29.802628: Training Step 66/115: batchLoss = 4.1350, diffLoss = 6.7761, kgLoss = 0.1733
2025-04-09 11:17:30.522223: Training Step 67/115: batchLoss = 3.6653, diffLoss = 6.0056, kgLoss = 0.1550
2025-04-09 11:17:31.240532: Training Step 68/115: batchLoss = 3.9497, diffLoss = 6.4675, kgLoss = 0.1730
2025-04-09 11:17:31.966728: Training Step 69/115: batchLoss = 3.8469, diffLoss = 6.3000, kgLoss = 0.1672
2025-04-09 11:17:32.685285: Training Step 70/115: batchLoss = 4.3424, diffLoss = 7.1173, kgLoss = 0.1801
2025-04-09 11:17:33.400575: Training Step 71/115: batchLoss = 3.7032, diffLoss = 6.0688, kgLoss = 0.1549
2025-04-09 11:17:34.105168: Training Step 72/115: batchLoss = 3.7930, diffLoss = 6.2107, kgLoss = 0.1665
2025-04-09 11:17:34.820395: Training Step 73/115: batchLoss = 4.8756, diffLoss = 7.9899, kgLoss = 0.2042
2025-04-09 11:17:35.532478: Training Step 74/115: batchLoss = 4.3054, diffLoss = 7.0405, kgLoss = 0.2029
2025-04-09 11:17:36.246208: Training Step 75/115: batchLoss = 4.1396, diffLoss = 6.7743, kgLoss = 0.1876
2025-04-09 11:17:36.962006: Training Step 76/115: batchLoss = 3.8187, diffLoss = 6.2608, kgLoss = 0.1555
2025-04-09 11:17:37.676603: Training Step 77/115: batchLoss = 4.0239, diffLoss = 6.5930, kgLoss = 0.1702
2025-04-09 11:17:38.380975: Training Step 78/115: batchLoss = 3.9914, diffLoss = 6.5235, kgLoss = 0.1933
2025-04-09 11:17:39.104600: Training Step 79/115: batchLoss = 3.8725, diffLoss = 6.3442, kgLoss = 0.1650
2025-04-09 11:17:39.825376: Training Step 80/115: batchLoss = 3.2924, diffLoss = 5.3880, kgLoss = 0.1489
2025-04-09 11:17:40.555962: Training Step 81/115: batchLoss = 3.7283, diffLoss = 6.1018, kgLoss = 0.1681
2025-04-09 11:17:41.268943: Training Step 82/115: batchLoss = 4.5094, diffLoss = 7.3899, kgLoss = 0.1888
2025-04-09 11:17:41.987366: Training Step 83/115: batchLoss = 4.0144, diffLoss = 6.5799, kgLoss = 0.1660
2025-04-09 11:17:42.705778: Training Step 84/115: batchLoss = 4.6998, diffLoss = 7.7015, kgLoss = 0.1972
2025-04-09 11:17:43.431284: Training Step 85/115: batchLoss = 4.3179, diffLoss = 7.0764, kgLoss = 0.1802
2025-04-09 11:17:44.147375: Training Step 86/115: batchLoss = 3.7323, diffLoss = 6.1065, kgLoss = 0.1709
2025-04-09 11:17:44.863137: Training Step 87/115: batchLoss = 4.2046, diffLoss = 6.8961, kgLoss = 0.1674
2025-04-09 11:17:45.573041: Training Step 88/115: batchLoss = 3.9474, diffLoss = 6.4719, kgLoss = 0.1607
2025-04-09 11:17:46.286172: Training Step 89/115: batchLoss = 4.2929, diffLoss = 7.0375, kgLoss = 0.1761
2025-04-09 11:17:46.998759: Training Step 90/115: batchLoss = 3.7925, diffLoss = 6.2101, kgLoss = 0.1663
2025-04-09 11:17:47.714216: Training Step 91/115: batchLoss = 4.0011, diffLoss = 6.5618, kgLoss = 0.1599
2025-04-09 11:17:48.435110: Training Step 92/115: batchLoss = 3.5130, diffLoss = 5.7442, kgLoss = 0.1662
2025-04-09 11:17:49.151392: Training Step 93/115: batchLoss = 4.6112, diffLoss = 7.5561, kgLoss = 0.1940
2025-04-09 11:17:49.869400: Training Step 94/115: batchLoss = 4.2384, diffLoss = 6.9458, kgLoss = 0.1774
2025-04-09 11:17:50.587272: Training Step 95/115: batchLoss = 4.3664, diffLoss = 7.1463, kgLoss = 0.1965
2025-04-09 11:17:51.299918: Training Step 96/115: batchLoss = 3.6140, diffLoss = 5.9179, kgLoss = 0.1582
2025-04-09 11:17:52.018022: Training Step 97/115: batchLoss = 3.9117, diffLoss = 6.4098, kgLoss = 0.1646
2025-04-09 11:17:52.735774: Training Step 98/115: batchLoss = 4.1461, diffLoss = 6.7941, kgLoss = 0.1740
2025-04-09 11:17:53.456883: Training Step 99/115: batchLoss = 4.4529, diffLoss = 7.3035, kgLoss = 0.1771
2025-04-09 11:17:54.167848: Training Step 100/115: batchLoss = 4.3644, diffLoss = 7.1553, kgLoss = 0.1781
2025-04-09 11:17:54.885992: Training Step 101/115: batchLoss = 4.5488, diffLoss = 7.4526, kgLoss = 0.1931
2025-04-09 11:17:55.601230: Training Step 102/115: batchLoss = 4.1404, diffLoss = 6.7815, kgLoss = 0.1787
2025-04-09 11:17:56.325227: Training Step 103/115: batchLoss = 4.3654, diffLoss = 7.1539, kgLoss = 0.1828
2025-04-09 11:17:57.037225: Training Step 104/115: batchLoss = 4.2192, diffLoss = 6.9178, kgLoss = 0.1713
2025-04-09 11:17:57.757939: Training Step 105/115: batchLoss = 3.7103, diffLoss = 6.0806, kgLoss = 0.1548
2025-04-09 11:17:58.476382: Training Step 106/115: batchLoss = 3.9824, diffLoss = 6.5269, kgLoss = 0.1656
2025-04-09 11:17:59.190657: Training Step 107/115: batchLoss = 3.9861, diffLoss = 6.5329, kgLoss = 0.1659
2025-04-09 11:17:59.901495: Training Step 108/115: batchLoss = 4.0792, diffLoss = 6.6867, kgLoss = 0.1678
2025-04-09 11:18:00.619221: Training Step 109/115: batchLoss = 4.0973, diffLoss = 6.7089, kgLoss = 0.1800
2025-04-09 11:18:01.333792: Training Step 110/115: batchLoss = 4.4902, diffLoss = 7.3490, kgLoss = 0.2019
2025-04-09 11:18:02.052626: Training Step 111/115: batchLoss = 4.0349, diffLoss = 6.6090, kgLoss = 0.1736
2025-04-09 11:18:02.773541: Training Step 112/115: batchLoss = 4.3338, diffLoss = 7.0983, kgLoss = 0.1871
2025-04-09 11:18:03.406677: Training Step 113/115: batchLoss = 3.8449, diffLoss = 6.3050, kgLoss = 0.1547
2025-04-09 11:18:04.039656: Training Step 114/115: batchLoss = 4.5609, diffLoss = 7.4762, kgLoss = 0.1878
2025-04-09 11:18:04.157343: 
2025-04-09 11:18:04.158295: Epoch 49/1000, Train: epLoss = 1.1790, epDfLoss = 1.9318, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:18:04.887254: Steps 0/90: batch_recall = 57.58, batch_ndcg = 44.17 
2025-04-09 11:18:05.601751: Steps 1/90: batch_recall = 55.79, batch_ndcg = 39.96 
2025-04-09 11:18:06.334927: Steps 2/90: batch_recall = 50.84, batch_ndcg = 38.47 
2025-04-09 11:18:07.050720: Steps 3/90: batch_recall = 42.51, batch_ndcg = 31.31 
2025-04-09 11:18:07.768902: Steps 4/90: batch_recall = 47.96, batch_ndcg = 35.19 
2025-04-09 11:18:08.488994: Steps 5/90: batch_recall = 38.41, batch_ndcg = 27.71 
2025-04-09 11:18:09.202248: Steps 6/90: batch_recall = 45.04, batch_ndcg = 31.64 
2025-04-09 11:18:09.930880: Steps 7/90: batch_recall = 43.40, batch_ndcg = 31.58 
2025-04-09 11:18:10.652857: Steps 8/90: batch_recall = 46.86, batch_ndcg = 34.62 
2025-04-09 11:18:11.382376: Steps 9/90: batch_recall = 42.79, batch_ndcg = 32.97 
2025-04-09 11:18:12.094512: Steps 10/90: batch_recall = 37.80, batch_ndcg = 29.31 
2025-04-09 11:18:12.809847: Steps 11/90: batch_recall = 40.74, batch_ndcg = 28.90 
2025-04-09 11:18:13.520034: Steps 12/90: batch_recall = 37.03, batch_ndcg = 26.80 
2025-04-09 11:18:14.232513: Steps 13/90: batch_recall = 36.12, batch_ndcg = 25.99 
2025-04-09 11:18:14.940568: Steps 14/90: batch_recall = 33.28, batch_ndcg = 23.64 
2025-04-09 11:18:15.643149: Steps 15/90: batch_recall = 44.57, batch_ndcg = 29.93 
2025-04-09 11:18:16.356846: Steps 16/90: batch_recall = 36.79, batch_ndcg = 26.35 
2025-04-09 11:18:17.067412: Steps 17/90: batch_recall = 32.81, batch_ndcg = 21.84 
2025-04-09 11:18:17.760406: Steps 18/90: batch_recall = 36.63, batch_ndcg = 25.69 
2025-04-09 11:18:18.475842: Steps 19/90: batch_recall = 31.32, batch_ndcg = 22.84 
2025-04-09 11:18:19.172378: Steps 20/90: batch_recall = 41.03, batch_ndcg = 27.66 
2025-04-09 11:18:19.876184: Steps 21/90: batch_recall = 41.35, batch_ndcg = 30.10 
2025-04-09 11:18:20.571343: Steps 22/90: batch_recall = 39.04, batch_ndcg = 28.04 
2025-04-09 11:18:21.267049: Steps 23/90: batch_recall = 38.72, batch_ndcg = 26.58 
2025-04-09 11:18:21.954639: Steps 24/90: batch_recall = 39.50, batch_ndcg = 28.15 
2025-04-09 11:18:22.649727: Steps 25/90: batch_recall = 38.56, batch_ndcg = 25.30 
2025-04-09 11:18:23.341798: Steps 26/90: batch_recall = 38.80, batch_ndcg = 27.80 
2025-04-09 11:18:24.038121: Steps 27/90: batch_recall = 34.72, batch_ndcg = 23.53 
2025-04-09 11:18:24.732212: Steps 28/90: batch_recall = 38.43, batch_ndcg = 26.42 
2025-04-09 11:18:25.427949: Steps 29/90: batch_recall = 33.38, batch_ndcg = 23.22 
2025-04-09 11:18:26.123938: Steps 30/90: batch_recall = 36.71, batch_ndcg = 24.99 
2025-04-09 11:18:26.813050: Steps 31/90: batch_recall = 32.80, batch_ndcg = 23.65 
2025-04-09 11:18:27.498750: Steps 32/90: batch_recall = 33.13, batch_ndcg = 23.23 
2025-04-09 11:18:28.181985: Steps 33/90: batch_recall = 33.60, batch_ndcg = 21.10 
2025-04-09 11:18:28.876947: Steps 34/90: batch_recall = 34.63, batch_ndcg = 23.45 
2025-04-09 11:18:29.567676: Steps 35/90: batch_recall = 37.57, batch_ndcg = 26.45 
2025-04-09 11:18:30.262114: Steps 36/90: batch_recall = 35.63, batch_ndcg = 24.19 
2025-04-09 11:18:30.965839: Steps 37/90: batch_recall = 32.82, batch_ndcg = 21.44 
2025-04-09 11:18:31.661446: Steps 38/90: batch_recall = 36.93, batch_ndcg = 23.82 
2025-04-09 11:18:32.353492: Steps 39/90: batch_recall = 36.81, batch_ndcg = 23.13 
2025-04-09 11:18:33.059999: Steps 40/90: batch_recall = 32.83, batch_ndcg = 22.14 
2025-04-09 11:18:33.752661: Steps 41/90: batch_recall = 32.35, batch_ndcg = 20.83 
2025-04-09 11:18:34.445412: Steps 42/90: batch_recall = 43.28, batch_ndcg = 28.54 
2025-04-09 11:18:35.136825: Steps 43/90: batch_recall = 36.22, batch_ndcg = 22.19 
2025-04-09 11:18:35.829063: Steps 44/90: batch_recall = 34.37, batch_ndcg = 22.98 
2025-04-09 11:18:36.520466: Steps 45/90: batch_recall = 32.91, batch_ndcg = 22.51 
2025-04-09 11:18:37.206554: Steps 46/90: batch_recall = 35.65, batch_ndcg = 22.99 
2025-04-09 11:18:37.896831: Steps 47/90: batch_recall = 32.69, batch_ndcg = 20.42 
2025-04-09 11:18:38.578069: Steps 48/90: batch_recall = 36.95, batch_ndcg = 24.43 
2025-04-09 11:18:39.267654: Steps 49/90: batch_recall = 30.46, batch_ndcg = 19.17 
2025-04-09 11:18:39.944865: Steps 50/90: batch_recall = 41.22, batch_ndcg = 26.79 
2025-04-09 11:18:40.626728: Steps 51/90: batch_recall = 40.62, batch_ndcg = 26.36 
2025-04-09 11:18:41.308386: Steps 52/90: batch_recall = 39.28, batch_ndcg = 25.59 
2025-04-09 11:18:41.990731: Steps 53/90: batch_recall = 40.43, batch_ndcg = 26.51 
2025-04-09 11:18:42.677008: Steps 54/90: batch_recall = 30.26, batch_ndcg = 20.62 
2025-04-09 11:18:43.363180: Steps 55/90: batch_recall = 36.88, batch_ndcg = 23.39 
2025-04-09 11:18:44.051648: Steps 56/90: batch_recall = 32.19, batch_ndcg = 19.97 
2025-04-09 11:18:44.741238: Steps 57/90: batch_recall = 37.99, batch_ndcg = 25.07 
2025-04-09 11:18:45.425941: Steps 58/90: batch_recall = 36.72, batch_ndcg = 22.80 
2025-04-09 11:18:46.119592: Steps 59/90: batch_recall = 36.61, batch_ndcg = 23.97 
2025-04-09 11:18:46.795981: Steps 60/90: batch_recall = 34.06, batch_ndcg = 22.80 
2025-04-09 11:18:47.479052: Steps 61/90: batch_recall = 39.24, batch_ndcg = 27.01 
2025-04-09 11:18:48.159888: Steps 62/90: batch_recall = 37.19, batch_ndcg = 24.80 
2025-04-09 11:18:48.834517: Steps 63/90: batch_recall = 31.94, batch_ndcg = 22.65 
2025-04-09 11:18:49.512653: Steps 64/90: batch_recall = 35.56, batch_ndcg = 23.06 
2025-04-09 11:18:50.192469: Steps 65/90: batch_recall = 36.01, batch_ndcg = 23.15 
2025-04-09 11:18:50.870520: Steps 66/90: batch_recall = 38.58, batch_ndcg = 22.66 
2025-04-09 11:18:51.563413: Steps 67/90: batch_recall = 32.24, batch_ndcg = 20.13 
2025-04-09 11:18:52.242329: Steps 68/90: batch_recall = 36.77, batch_ndcg = 22.87 
2025-04-09 11:18:52.921239: Steps 69/90: batch_recall = 42.18, batch_ndcg = 25.01 
2025-04-09 11:18:53.597183: Steps 70/90: batch_recall = 38.83, batch_ndcg = 24.69 
2025-04-09 11:18:54.270873: Steps 71/90: batch_recall = 39.35, batch_ndcg = 24.71 
2025-04-09 11:18:54.942059: Steps 72/90: batch_recall = 33.33, batch_ndcg = 21.68 
2025-04-09 11:18:55.621946: Steps 73/90: batch_recall = 34.45, batch_ndcg = 21.42 
2025-04-09 11:18:56.298504: Steps 74/90: batch_recall = 40.53, batch_ndcg = 25.05 
2025-04-09 11:18:56.982649: Steps 75/90: batch_recall = 38.15, batch_ndcg = 22.89 
2025-04-09 11:18:57.662292: Steps 76/90: batch_recall = 40.42, batch_ndcg = 25.40 
2025-04-09 11:18:58.333680: Steps 77/90: batch_recall = 44.15, batch_ndcg = 28.42 
2025-04-09 11:18:59.029203: Steps 78/90: batch_recall = 34.38, batch_ndcg = 21.35 
2025-04-09 11:18:59.716180: Steps 79/90: batch_recall = 38.40, batch_ndcg = 25.41 
2025-04-09 11:19:00.393318: Steps 80/90: batch_recall = 46.45, batch_ndcg = 29.39 
2025-04-09 11:19:01.069746: Steps 81/90: batch_recall = 42.70, batch_ndcg = 27.14 
2025-04-09 11:19:01.749124: Steps 82/90: batch_recall = 44.08, batch_ndcg = 27.76 
2025-04-09 11:19:02.424153: Steps 83/90: batch_recall = 46.03, batch_ndcg = 27.54 
2025-04-09 11:19:03.102368: Steps 84/90: batch_recall = 34.96, batch_ndcg = 21.96 
2025-04-09 11:19:03.780967: Steps 85/90: batch_recall = 44.32, batch_ndcg = 25.63 
2025-04-09 11:19:04.460265: Steps 86/90: batch_recall = 46.54, batch_ndcg = 31.21 
2025-04-09 11:19:05.155266: Steps 87/90: batch_recall = 43.20, batch_ndcg = 28.52 
2025-04-09 11:19:05.823504: Steps 88/90: batch_recall = 49.14, batch_ndcg = 29.70 
2025-04-09 11:19:06.356751: Steps 89/90: batch_recall = 40.00, batch_ndcg = 25.80 
2025-04-09 11:19:06.357449: Epoch 49/1000, Test: Recall = 0.0757, NDCG = 0.0507  

2025-04-09 11:19:07.349066: Training Step 0/115: batchLoss = 3.6965, diffLoss = 6.0555, kgLoss = 0.1580
2025-04-09 11:19:08.062046: Training Step 1/115: batchLoss = 3.9749, diffLoss = 6.5147, kgLoss = 0.1651
2025-04-09 11:19:08.778386: Training Step 2/115: batchLoss = 3.7334, diffLoss = 6.1110, kgLoss = 0.1669
2025-04-09 11:19:09.491547: Training Step 3/115: batchLoss = 4.3971, diffLoss = 7.2103, kgLoss = 0.1773
2025-04-09 11:19:10.213500: Training Step 4/115: batchLoss = 4.0517, diffLoss = 6.6419, kgLoss = 0.1664
2025-04-09 11:19:10.935842: Training Step 5/115: batchLoss = 3.8875, diffLoss = 6.3699, kgLoss = 0.1639
2025-04-09 11:19:11.657053: Training Step 6/115: batchLoss = 4.3477, diffLoss = 7.1219, kgLoss = 0.1864
2025-04-09 11:19:12.375993: Training Step 7/115: batchLoss = 3.9925, diffLoss = 6.5423, kgLoss = 0.1679
2025-04-09 11:19:13.092005: Training Step 8/115: batchLoss = 3.8153, diffLoss = 6.2514, kgLoss = 0.1612
2025-04-09 11:19:13.808675: Training Step 9/115: batchLoss = 4.8106, diffLoss = 7.8859, kgLoss = 0.1976
2025-04-09 11:19:14.532370: Training Step 10/115: batchLoss = 3.8228, diffLoss = 6.2595, kgLoss = 0.1678
2025-04-09 11:19:15.252320: Training Step 11/115: batchLoss = 3.6574, diffLoss = 5.9908, kgLoss = 0.1572
2025-04-09 11:19:15.970803: Training Step 12/115: batchLoss = 3.9266, diffLoss = 6.4334, kgLoss = 0.1665
2025-04-09 11:19:16.690218: Training Step 13/115: batchLoss = 4.3199, diffLoss = 7.0814, kgLoss = 0.1777
2025-04-09 11:19:17.423483: Training Step 14/115: batchLoss = 4.2629, diffLoss = 6.9868, kgLoss = 0.1772
2025-04-09 11:19:18.147317: Training Step 15/115: batchLoss = 3.9020, diffLoss = 6.3946, kgLoss = 0.1630
2025-04-09 11:19:18.863135: Training Step 16/115: batchLoss = 4.0411, diffLoss = 6.6194, kgLoss = 0.1735
2025-04-09 11:19:19.573884: Training Step 17/115: batchLoss = 4.5454, diffLoss = 7.4576, kgLoss = 0.1771
2025-04-09 11:19:20.284480: Training Step 18/115: batchLoss = 3.9897, diffLoss = 6.5277, kgLoss = 0.1828
2025-04-09 11:19:20.992370: Training Step 19/115: batchLoss = 4.2649, diffLoss = 6.9842, kgLoss = 0.1859
2025-04-09 11:19:21.704058: Training Step 20/115: batchLoss = 4.3874, diffLoss = 7.1946, kgLoss = 0.1766
2025-04-09 11:19:22.416056: Training Step 21/115: batchLoss = 4.0638, diffLoss = 6.6563, kgLoss = 0.1751
2025-04-09 11:19:23.125856: Training Step 22/115: batchLoss = 4.2397, diffLoss = 6.9556, kgLoss = 0.1658
2025-04-09 11:19:23.845474: Training Step 23/115: batchLoss = 4.2654, diffLoss = 6.9943, kgLoss = 0.1720
2025-04-09 11:19:24.560873: Training Step 24/115: batchLoss = 3.6455, diffLoss = 5.9658, kgLoss = 0.1651
2025-04-09 11:19:25.278213: Training Step 25/115: batchLoss = 3.9313, diffLoss = 6.4380, kgLoss = 0.1712
2025-04-09 11:19:25.998885: Training Step 26/115: batchLoss = 4.7517, diffLoss = 7.7779, kgLoss = 0.2123
2025-04-09 11:19:26.720955: Training Step 27/115: batchLoss = 3.5421, diffLoss = 5.7968, kgLoss = 0.1601
2025-04-09 11:19:27.443805: Training Step 28/115: batchLoss = 4.8903, diffLoss = 8.0156, kgLoss = 0.2022
2025-04-09 11:19:28.160881: Training Step 29/115: batchLoss = 4.4910, diffLoss = 7.3636, kgLoss = 0.1822
2025-04-09 11:19:28.885008: Training Step 30/115: batchLoss = 3.8866, diffLoss = 6.3671, kgLoss = 0.1659
2025-04-09 11:19:29.600881: Training Step 31/115: batchLoss = 3.6226, diffLoss = 5.9365, kgLoss = 0.1517
2025-04-09 11:19:30.317261: Training Step 32/115: batchLoss = 3.9003, diffLoss = 6.3922, kgLoss = 0.1625
2025-04-09 11:19:31.039787: Training Step 33/115: batchLoss = 4.1557, diffLoss = 6.8081, kgLoss = 0.1771
2025-04-09 11:19:31.762691: Training Step 34/115: batchLoss = 4.0365, diffLoss = 6.6126, kgLoss = 0.1723
2025-04-09 11:19:32.492005: Training Step 35/115: batchLoss = 4.0482, diffLoss = 6.6356, kgLoss = 0.1671
2025-04-09 11:19:33.205803: Training Step 36/115: batchLoss = 3.7306, diffLoss = 6.1134, kgLoss = 0.1564
2025-04-09 11:19:33.920906: Training Step 37/115: batchLoss = 4.1355, diffLoss = 6.7763, kgLoss = 0.1744
2025-04-09 11:19:34.635419: Training Step 38/115: batchLoss = 4.0451, diffLoss = 6.6325, kgLoss = 0.1638
2025-04-09 11:19:35.350458: Training Step 39/115: batchLoss = 3.9734, diffLoss = 6.5085, kgLoss = 0.1709
2025-04-09 11:19:36.068660: Training Step 40/115: batchLoss = 4.4387, diffLoss = 7.2742, kgLoss = 0.1854
2025-04-09 11:19:36.785771: Training Step 41/115: batchLoss = 4.4732, diffLoss = 7.3303, kgLoss = 0.1876
2025-04-09 11:19:37.502637: Training Step 42/115: batchLoss = 4.0749, diffLoss = 6.6830, kgLoss = 0.1627
2025-04-09 11:19:38.230669: Training Step 43/115: batchLoss = 4.4831, diffLoss = 7.3458, kgLoss = 0.1891
2025-04-09 11:19:38.952057: Training Step 44/115: batchLoss = 4.0082, diffLoss = 6.5698, kgLoss = 0.1657
2025-04-09 11:19:39.669289: Training Step 45/115: batchLoss = 4.0563, diffLoss = 6.6512, kgLoss = 0.1638
2025-04-09 11:19:40.387750: Training Step 46/115: batchLoss = 4.3656, diffLoss = 7.1556, kgLoss = 0.1806
2025-04-09 11:19:41.109912: Training Step 47/115: batchLoss = 4.1732, diffLoss = 6.8399, kgLoss = 0.1731
2025-04-09 11:19:41.822544: Training Step 48/115: batchLoss = 3.8121, diffLoss = 6.2461, kgLoss = 0.1612
2025-04-09 11:19:42.546429: Training Step 49/115: batchLoss = 4.2697, diffLoss = 6.9873, kgLoss = 0.1934
2025-04-09 11:19:43.274702: Training Step 50/115: batchLoss = 3.6120, diffLoss = 5.9155, kgLoss = 0.1567
2025-04-09 11:19:43.989711: Training Step 51/115: batchLoss = 3.9312, diffLoss = 6.4311, kgLoss = 0.1815
2025-04-09 11:19:44.712521: Training Step 52/115: batchLoss = 4.4129, diffLoss = 7.2312, kgLoss = 0.1856
2025-04-09 11:19:45.429467: Training Step 53/115: batchLoss = 4.7484, diffLoss = 7.7744, kgLoss = 0.2093
2025-04-09 11:19:46.158236: Training Step 54/115: batchLoss = 3.8214, diffLoss = 6.2603, kgLoss = 0.1630
2025-04-09 11:19:46.871737: Training Step 55/115: batchLoss = 4.0393, diffLoss = 6.6216, kgLoss = 0.1659
2025-04-09 11:19:47.579921: Training Step 56/115: batchLoss = 4.2297, diffLoss = 6.9322, kgLoss = 0.1758
2025-04-09 11:19:48.292357: Training Step 57/115: batchLoss = 4.3478, diffLoss = 7.1258, kgLoss = 0.1807
2025-04-09 11:19:48.999857: Training Step 58/115: batchLoss = 3.7515, diffLoss = 6.1484, kgLoss = 0.1561
2025-04-09 11:19:49.724207: Training Step 59/115: batchLoss = 4.2136, diffLoss = 6.9093, kgLoss = 0.1702
2025-04-09 11:19:50.445799: Training Step 60/115: batchLoss = 4.2351, diffLoss = 6.9421, kgLoss = 0.1747
2025-04-09 11:19:51.166892: Training Step 61/115: batchLoss = 3.8895, diffLoss = 6.3714, kgLoss = 0.1667
2025-04-09 11:19:51.887820: Training Step 62/115: batchLoss = 3.6577, diffLoss = 5.9886, kgLoss = 0.1613
2025-04-09 11:19:52.612080: Training Step 63/115: batchLoss = 4.3026, diffLoss = 7.0519, kgLoss = 0.1786
2025-04-09 11:19:53.334669: Training Step 64/115: batchLoss = 4.4519, diffLoss = 7.2976, kgLoss = 0.1834
2025-04-09 11:19:54.063046: Training Step 65/115: batchLoss = 4.1368, diffLoss = 6.7743, kgLoss = 0.1807
2025-04-09 11:19:54.780280: Training Step 66/115: batchLoss = 3.9393, diffLoss = 6.4464, kgLoss = 0.1786
2025-04-09 11:19:55.498304: Training Step 67/115: batchLoss = 3.9530, diffLoss = 6.4724, kgLoss = 0.1739
2025-04-09 11:19:56.220629: Training Step 68/115: batchLoss = 4.7526, diffLoss = 7.7807, kgLoss = 0.2105
2025-04-09 11:19:56.935813: Training Step 69/115: batchLoss = 4.0227, diffLoss = 6.5873, kgLoss = 0.1757
2025-04-09 11:19:57.652017: Training Step 70/115: batchLoss = 3.7048, diffLoss = 6.0632, kgLoss = 0.1673
2025-04-09 11:19:58.374172: Training Step 71/115: batchLoss = 3.8713, diffLoss = 6.3440, kgLoss = 0.1621
2025-04-09 11:19:59.089144: Training Step 72/115: batchLoss = 3.8505, diffLoss = 6.3095, kgLoss = 0.1620
2025-04-09 11:19:59.802048: Training Step 73/115: batchLoss = 4.3886, diffLoss = 7.1926, kgLoss = 0.1826
2025-04-09 11:20:00.512181: Training Step 74/115: batchLoss = 4.2610, diffLoss = 6.9849, kgLoss = 0.1753
2025-04-09 11:20:01.221983: Training Step 75/115: batchLoss = 4.2286, diffLoss = 6.9369, kgLoss = 0.1662
2025-04-09 11:20:01.917422: Training Step 76/115: batchLoss = 4.0075, diffLoss = 6.5521, kgLoss = 0.1906
2025-04-09 11:20:02.625583: Training Step 77/115: batchLoss = 4.0134, diffLoss = 6.5764, kgLoss = 0.1689
2025-04-09 11:20:03.317057: Training Step 78/115: batchLoss = 3.9088, diffLoss = 6.3994, kgLoss = 0.1729
2025-04-09 11:20:04.029396: Training Step 79/115: batchLoss = 3.9933, diffLoss = 6.5371, kgLoss = 0.1776
2025-04-09 11:20:04.746527: Training Step 80/115: batchLoss = 4.1965, diffLoss = 6.8853, kgLoss = 0.1632
2025-04-09 11:20:05.466841: Training Step 81/115: batchLoss = 3.7350, diffLoss = 6.1113, kgLoss = 0.1706
2025-04-09 11:20:06.184956: Training Step 82/115: batchLoss = 4.3077, diffLoss = 7.0588, kgLoss = 0.1810
2025-04-09 11:20:06.900246: Training Step 83/115: batchLoss = 4.3694, diffLoss = 7.1604, kgLoss = 0.1828
2025-04-09 11:20:07.616170: Training Step 84/115: batchLoss = 4.0028, diffLoss = 6.5435, kgLoss = 0.1916
2025-04-09 11:20:08.329887: Training Step 85/115: batchLoss = 4.0080, diffLoss = 6.5701, kgLoss = 0.1649
2025-04-09 11:20:09.046194: Training Step 86/115: batchLoss = 3.6979, diffLoss = 6.0591, kgLoss = 0.1562
2025-04-09 11:20:09.767578: Training Step 87/115: batchLoss = 3.9561, diffLoss = 6.4839, kgLoss = 0.1645
2025-04-09 11:20:10.496161: Training Step 88/115: batchLoss = 4.4434, diffLoss = 7.2800, kgLoss = 0.1886
2025-04-09 11:20:11.204968: Training Step 89/115: batchLoss = 4.1049, diffLoss = 6.7316, kgLoss = 0.1649
2025-04-09 11:20:11.923416: Training Step 90/115: batchLoss = 4.0188, diffLoss = 6.5813, kgLoss = 0.1751
2025-04-09 11:20:12.645863: Training Step 91/115: batchLoss = 4.7039, diffLoss = 7.7104, kgLoss = 0.1941
2025-04-09 11:20:13.363770: Training Step 92/115: batchLoss = 3.9649, diffLoss = 6.4907, kgLoss = 0.1761
2025-04-09 11:20:14.078188: Training Step 93/115: batchLoss = 4.4701, diffLoss = 7.3274, kgLoss = 0.1843
2025-04-09 11:20:14.796337: Training Step 94/115: batchLoss = 4.5399, diffLoss = 7.4419, kgLoss = 0.1868
2025-04-09 11:20:15.508452: Training Step 95/115: batchLoss = 3.8348, diffLoss = 6.2846, kgLoss = 0.1601
2025-04-09 11:20:16.219150: Training Step 96/115: batchLoss = 3.7120, diffLoss = 6.0819, kgLoss = 0.1570
2025-04-09 11:20:16.936724: Training Step 97/115: batchLoss = 4.3647, diffLoss = 7.1566, kgLoss = 0.1768
2025-04-09 11:20:17.650173: Training Step 98/115: batchLoss = 4.3358, diffLoss = 7.1013, kgLoss = 0.1876
2025-04-09 11:20:18.371169: Training Step 99/115: batchLoss = 3.7704, diffLoss = 6.1746, kgLoss = 0.1641
2025-04-09 11:20:19.106432: Training Step 100/115: batchLoss = 3.7458, diffLoss = 6.1303, kgLoss = 0.1691
2025-04-09 11:20:19.825430: Training Step 101/115: batchLoss = 4.1297, diffLoss = 6.7705, kgLoss = 0.1684
2025-04-09 11:20:20.546694: Training Step 102/115: batchLoss = 4.3282, diffLoss = 7.0964, kgLoss = 0.1758
2025-04-09 11:20:21.260604: Training Step 103/115: batchLoss = 4.0925, diffLoss = 6.7120, kgLoss = 0.1634
2025-04-09 11:20:21.984978: Training Step 104/115: batchLoss = 3.9531, diffLoss = 6.4711, kgLoss = 0.1761
2025-04-09 11:20:22.704080: Training Step 105/115: batchLoss = 3.6361, diffLoss = 5.9518, kgLoss = 0.1627
2025-04-09 11:20:23.421599: Training Step 106/115: batchLoss = 4.2702, diffLoss = 6.9968, kgLoss = 0.1804
2025-04-09 11:20:24.143764: Training Step 107/115: batchLoss = 4.5319, diffLoss = 7.4286, kgLoss = 0.1868
2025-04-09 11:20:24.856596: Training Step 108/115: batchLoss = 3.9775, diffLoss = 6.5169, kgLoss = 0.1683
2025-04-09 11:20:25.573006: Training Step 109/115: batchLoss = 4.2240, diffLoss = 6.9215, kgLoss = 0.1778
2025-04-09 11:20:26.309200: Training Step 110/115: batchLoss = 4.0349, diffLoss = 6.6063, kgLoss = 0.1778
2025-04-09 11:20:27.026902: Training Step 111/115: batchLoss = 4.1534, diffLoss = 6.8095, kgLoss = 0.1693
2025-04-09 11:20:27.729109: Training Step 112/115: batchLoss = 4.1003, diffLoss = 6.7207, kgLoss = 0.1698
2025-04-09 11:20:28.363981: Training Step 113/115: batchLoss = 3.8414, diffLoss = 6.2987, kgLoss = 0.1553
2025-04-09 11:20:28.962330: Training Step 114/115: batchLoss = 3.7833, diffLoss = 6.1980, kgLoss = 0.1613
2025-04-09 11:20:29.076849: 
2025-04-09 11:20:29.077812: Epoch 50/1000, Train: epLoss = 1.1784, epDfLoss = 1.9307, epfTransLoss = 0.0000, epKgLoss = 0.0499  
2025-04-09 11:20:29.818065: Steps 0/90: batch_recall = 56.87, batch_ndcg = 43.64 
2025-04-09 11:20:30.524576: Steps 1/90: batch_recall = 56.48, batch_ndcg = 40.53 
2025-04-09 11:20:31.248160: Steps 2/90: batch_recall = 51.06, batch_ndcg = 38.47 
2025-04-09 11:20:31.969882: Steps 3/90: batch_recall = 44.27, batch_ndcg = 31.87 
2025-04-09 11:20:32.692969: Steps 4/90: batch_recall = 47.07, batch_ndcg = 34.97 
2025-04-09 11:20:33.412257: Steps 5/90: batch_recall = 37.95, batch_ndcg = 27.54 
2025-04-09 11:20:34.120313: Steps 6/90: batch_recall = 45.91, batch_ndcg = 31.90 
2025-04-09 11:20:34.851358: Steps 7/90: batch_recall = 42.95, batch_ndcg = 31.27 
2025-04-09 11:20:35.569779: Steps 8/90: batch_recall = 47.48, batch_ndcg = 34.95 
2025-04-09 11:20:36.304376: Steps 9/90: batch_recall = 42.60, batch_ndcg = 33.25 
2025-04-09 11:20:37.034255: Steps 10/90: batch_recall = 38.52, batch_ndcg = 29.46 
2025-04-09 11:20:37.752674: Steps 11/90: batch_recall = 41.18, batch_ndcg = 29.12 
2025-04-09 11:20:38.459156: Steps 12/90: batch_recall = 37.09, batch_ndcg = 26.89 
2025-04-09 11:20:39.176946: Steps 13/90: batch_recall = 35.25, batch_ndcg = 26.15 
2025-04-09 11:20:39.897710: Steps 14/90: batch_recall = 33.05, batch_ndcg = 23.64 
2025-04-09 11:20:40.603435: Steps 15/90: batch_recall = 45.17, batch_ndcg = 30.41 
2025-04-09 11:20:41.310641: Steps 16/90: batch_recall = 37.27, batch_ndcg = 26.33 
2025-04-09 11:20:42.013571: Steps 17/90: batch_recall = 33.46, batch_ndcg = 22.42 
2025-04-09 11:20:42.715035: Steps 18/90: batch_recall = 36.56, batch_ndcg = 25.61 
2025-04-09 11:20:43.419857: Steps 19/90: batch_recall = 31.65, batch_ndcg = 22.85 
2025-04-09 11:20:44.115299: Steps 20/90: batch_recall = 41.64, batch_ndcg = 27.95 
2025-04-09 11:20:44.825252: Steps 21/90: batch_recall = 40.96, batch_ndcg = 29.80 
2025-04-09 11:20:45.526674: Steps 22/90: batch_recall = 40.09, batch_ndcg = 28.08 
2025-04-09 11:20:46.233392: Steps 23/90: batch_recall = 38.91, batch_ndcg = 26.97 
2025-04-09 11:20:46.940838: Steps 24/90: batch_recall = 37.60, batch_ndcg = 27.69 
2025-04-09 11:20:47.652962: Steps 25/90: batch_recall = 38.10, batch_ndcg = 24.86 
2025-04-09 11:20:48.346775: Steps 26/90: batch_recall = 38.05, batch_ndcg = 27.20 
2025-04-09 11:20:49.043423: Steps 27/90: batch_recall = 36.12, batch_ndcg = 24.46 
2025-04-09 11:20:49.749846: Steps 28/90: batch_recall = 38.34, batch_ndcg = 26.35 
2025-04-09 11:20:50.448115: Steps 29/90: batch_recall = 33.05, batch_ndcg = 23.16 
2025-04-09 11:20:51.147918: Steps 30/90: batch_recall = 37.27, batch_ndcg = 24.83 
2025-04-09 11:20:51.854944: Steps 31/90: batch_recall = 33.58, batch_ndcg = 24.07 
2025-04-09 11:20:52.565894: Steps 32/90: batch_recall = 33.75, batch_ndcg = 23.29 
2025-04-09 11:20:53.260658: Steps 33/90: batch_recall = 34.33, batch_ndcg = 21.28 
2025-04-09 11:20:53.951838: Steps 34/90: batch_recall = 34.57, batch_ndcg = 23.37 
2025-04-09 11:20:54.629983: Steps 35/90: batch_recall = 38.03, batch_ndcg = 26.51 
2025-04-09 11:20:55.333142: Steps 36/90: batch_recall = 36.28, batch_ndcg = 24.48 
2025-04-09 11:20:56.021306: Steps 37/90: batch_recall = 33.01, batch_ndcg = 21.51 
2025-04-09 11:20:56.711244: Steps 38/90: batch_recall = 35.12, batch_ndcg = 23.33 
2025-04-09 11:20:57.405621: Steps 39/90: batch_recall = 36.37, batch_ndcg = 23.19 
2025-04-09 11:20:58.099285: Steps 40/90: batch_recall = 32.97, batch_ndcg = 22.09 
2025-04-09 11:20:58.799337: Steps 41/90: batch_recall = 32.13, batch_ndcg = 20.60 
2025-04-09 11:20:59.486392: Steps 42/90: batch_recall = 43.28, batch_ndcg = 28.39 
2025-04-09 11:21:00.179906: Steps 43/90: batch_recall = 35.64, batch_ndcg = 22.21 
2025-04-09 11:21:00.885891: Steps 44/90: batch_recall = 34.50, batch_ndcg = 23.04 
2025-04-09 11:21:01.578195: Steps 45/90: batch_recall = 32.23, batch_ndcg = 22.32 
2025-04-09 11:21:02.260265: Steps 46/90: batch_recall = 35.48, batch_ndcg = 22.69 
2025-04-09 11:21:02.951803: Steps 47/90: batch_recall = 32.40, batch_ndcg = 20.23 
2025-04-09 11:21:03.651754: Steps 48/90: batch_recall = 37.42, batch_ndcg = 24.49 
2025-04-09 11:21:04.352644: Steps 49/90: batch_recall = 31.40, batch_ndcg = 18.95 
2025-04-09 11:21:05.045461: Steps 50/90: batch_recall = 40.62, batch_ndcg = 26.23 
2025-04-09 11:21:05.742714: Steps 51/90: batch_recall = 40.49, batch_ndcg = 26.12 
2025-04-09 11:21:06.440835: Steps 52/90: batch_recall = 39.63, batch_ndcg = 25.68 
2025-04-09 11:21:07.136767: Steps 53/90: batch_recall = 40.37, batch_ndcg = 26.46 
2025-04-09 11:21:07.823287: Steps 54/90: batch_recall = 30.91, batch_ndcg = 20.43 
2025-04-09 11:21:08.512315: Steps 55/90: batch_recall = 36.21, batch_ndcg = 23.24 
2025-04-09 11:21:09.188121: Steps 56/90: batch_recall = 32.98, batch_ndcg = 20.31 
2025-04-09 11:21:09.882964: Steps 57/90: batch_recall = 37.62, batch_ndcg = 24.61 
2025-04-09 11:21:10.562899: Steps 58/90: batch_recall = 35.95, batch_ndcg = 22.64 
2025-04-09 11:21:11.248659: Steps 59/90: batch_recall = 37.00, batch_ndcg = 24.08 
2025-04-09 11:21:11.933480: Steps 60/90: batch_recall = 34.40, batch_ndcg = 22.63 
2025-04-09 11:21:12.633583: Steps 61/90: batch_recall = 39.38, batch_ndcg = 27.29 
2025-04-09 11:21:13.317909: Steps 62/90: batch_recall = 37.22, batch_ndcg = 24.89 
2025-04-09 11:21:14.003750: Steps 63/90: batch_recall = 33.32, batch_ndcg = 23.22 
2025-04-09 11:21:14.691288: Steps 64/90: batch_recall = 35.33, batch_ndcg = 23.20 
2025-04-09 11:21:15.372721: Steps 65/90: batch_recall = 35.14, batch_ndcg = 22.56 
2025-04-09 11:21:16.059861: Steps 66/90: batch_recall = 37.96, batch_ndcg = 22.49 
2025-04-09 11:21:16.746072: Steps 67/90: batch_recall = 32.64, batch_ndcg = 20.16 
2025-04-09 11:21:17.435035: Steps 68/90: batch_recall = 36.45, batch_ndcg = 22.74 
2025-04-09 11:21:18.119032: Steps 69/90: batch_recall = 43.29, batch_ndcg = 25.50 
2025-04-09 11:21:18.809115: Steps 70/90: batch_recall = 38.59, batch_ndcg = 24.83 
2025-04-09 11:21:19.495366: Steps 71/90: batch_recall = 38.99, batch_ndcg = 24.60 
2025-04-09 11:21:20.172526: Steps 72/90: batch_recall = 33.59, batch_ndcg = 21.94 
2025-04-09 11:21:20.860550: Steps 73/90: batch_recall = 34.91, batch_ndcg = 21.58 
2025-04-09 11:21:21.541651: Steps 74/90: batch_recall = 40.09, batch_ndcg = 24.84 
2025-04-09 11:21:22.221399: Steps 75/90: batch_recall = 37.65, batch_ndcg = 22.65 
2025-04-09 11:21:22.896996: Steps 76/90: batch_recall = 39.91, batch_ndcg = 25.26 
2025-04-09 11:21:23.574501: Steps 77/90: batch_recall = 43.91, batch_ndcg = 28.09 
2025-04-09 11:21:24.256092: Steps 78/90: batch_recall = 34.76, batch_ndcg = 21.81 
2025-04-09 11:21:24.936211: Steps 79/90: batch_recall = 37.93, batch_ndcg = 25.07 
2025-04-09 11:21:25.611153: Steps 80/90: batch_recall = 46.22, batch_ndcg = 29.21 
2025-04-09 11:21:26.291611: Steps 81/90: batch_recall = 42.50, batch_ndcg = 27.07 
2025-04-09 11:21:26.972574: Steps 82/90: batch_recall = 44.35, batch_ndcg = 27.88 
2025-04-09 11:21:27.647945: Steps 83/90: batch_recall = 46.44, batch_ndcg = 27.84 
2025-04-09 11:21:28.328386: Steps 84/90: batch_recall = 35.63, batch_ndcg = 22.17 
2025-04-09 11:21:29.008844: Steps 85/90: batch_recall = 43.37, batch_ndcg = 25.31 
2025-04-09 11:21:29.688476: Steps 86/90: batch_recall = 47.38, batch_ndcg = 31.41 
2025-04-09 11:21:30.373082: Steps 87/90: batch_recall = 42.58, batch_ndcg = 28.39 
2025-04-09 11:21:31.054689: Steps 88/90: batch_recall = 48.43, batch_ndcg = 29.87 
2025-04-09 11:21:31.594207: Steps 89/90: batch_recall = 40.11, batch_ndcg = 26.14 
2025-04-09 11:21:31.594923: Epoch 50/1000, Test: Recall = 0.0758, NDCG = 0.0507  

------------------
Exiting from training early
Best epoch :  31  , Recall :  0.07724350320278021  , NDCG :  0.0510165951188568

------------------------------------------------------------
Sender: LSF System <XXX>
Subject: Job 8136073: <RecommenderSys> in cluster <XXX> Done

Job <RecommenderSys> was submitted from host <XXX> by user <XX> in cluster <XXX> at Wed Apr  9 08:58:51 2025
Job was executed on host(s) <XXX>, in queue <batch_a100>, as user <XXX> in cluster <XXX> at Wed Apr  9 09:14:05 2025
</home/XXX/> was used as the home directory.
</home/XXX/recommenderSys/XXX/DiKGRec> was used as the working directory.
Started at Wed Apr  9 09:14:05 2025
Terminated at Wed Apr  9 11:21:35 2025
Results reported at Wed Apr  9 11:21:35 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python Main.py --data yelp2018 --lr2 1e-3 --kg_loss_ratio 0.4 --updateW 2 --oriW 0 --layer 2
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   8769.00 sec.
    Max Memory :                                 4908 MB
    Average Memory :                             4558.30 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               5092.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   7647 sec.
    Turnaround time :                            8564 sec.

The output (if any) is above this job summary.



PS:

Read file <recommander.8136073.stderr> for stderr output of this job.


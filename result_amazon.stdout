2025-04-08 12:34:12.769491: Start
Begin to load knowledge graph triples ...
In KG: 78 relations, 113487 entities, 5115492 triples
2025-04-08 12:34:49.883275: Load Data
2025-04-08 12:34:49.883295: Namespace(batch=200, cold_start_num=0, d_emb_size=10, data='amazon-book', diff_type=0, dims='[1000]', entity=113487, epoch=1000, gpu='0', head=1, head_diff_ratio=0, item=24915, kg_loss_ratio=0.8, kg_norm=2, latdim=64, layer=4, load_model=None, lr=5e-05, lr2=0.005, noise_max=0.005, noise_min=0.0005, noise_ratio=0.2, noise_scale=0.0005, norm=True, oriW=1.0, relation=78, res_lambda=0.5, sampling_N=20, sampling_steps=0, save_path='tem', seed=421, steps=5, topk=20, trans_ratio=0, tstBat=512, tstEpoch=1, updateW=1.0, user=70679)
USER 70679 ITEM 24915
NUM OF INTERACTIONS 652514
Number of all parameters: 49866103
2025-04-08 12:34:50.359384: Model Prepared
2025-04-08 12:34:50.359396: Model Initialized
2025-04-08 12:34:56.503031: Training Step 0/354: batchLoss = 0.9176, diffLoss = 4.5195, kgLoss = 0.0171
2025-04-08 12:34:58.110486: Training Step 1/354: batchLoss = 0.6580, diffLoss = 3.2314, kgLoss = 0.0147
2025-04-08 12:34:59.720638: Training Step 2/354: batchLoss = 0.7437, diffLoss = 3.6613, kgLoss = 0.0142
2025-04-08 12:35:01.336773: Training Step 3/354: batchLoss = 0.6854, diffLoss = 3.3793, kgLoss = 0.0120
2025-04-08 12:35:02.956854: Training Step 4/354: batchLoss = 0.7702, diffLoss = 3.7913, kgLoss = 0.0149
2025-04-08 12:35:04.581725: Training Step 5/354: batchLoss = 0.5438, diffLoss = 2.6732, kgLoss = 0.0115
2025-04-08 12:35:06.204054: Training Step 6/354: batchLoss = 0.6985, diffLoss = 3.4321, kgLoss = 0.0151
2025-04-08 12:35:07.825815: Training Step 7/354: batchLoss = 0.7674, diffLoss = 3.7742, kgLoss = 0.0158
2025-04-08 12:35:09.454043: Training Step 8/354: batchLoss = 0.8735, diffLoss = 4.2976, kgLoss = 0.0175
2025-04-08 12:35:11.071857: Training Step 9/354: batchLoss = 0.6644, diffLoss = 3.2701, kgLoss = 0.0130
2025-04-08 12:35:12.679110: Training Step 10/354: batchLoss = 0.7350, diffLoss = 3.6199, kgLoss = 0.0138
2025-04-08 12:35:14.298965: Training Step 11/354: batchLoss = 0.7539, diffLoss = 3.7051, kgLoss = 0.0161
2025-04-08 12:35:15.919951: Training Step 12/354: batchLoss = 0.6091, diffLoss = 2.9922, kgLoss = 0.0134
2025-04-08 12:35:17.537792: Training Step 13/354: batchLoss = 0.6923, diffLoss = 3.4032, kgLoss = 0.0145
2025-04-08 12:35:19.160331: Training Step 14/354: batchLoss = 0.6644, diffLoss = 3.2685, kgLoss = 0.0133
2025-04-08 12:35:20.779193: Training Step 15/354: batchLoss = 0.6723, diffLoss = 3.3082, kgLoss = 0.0133
2025-04-08 12:35:22.411187: Training Step 16/354: batchLoss = 0.6127, diffLoss = 3.0174, kgLoss = 0.0115
2025-04-08 12:35:24.033599: Training Step 17/354: batchLoss = 0.6662, diffLoss = 3.2750, kgLoss = 0.0140
2025-04-08 12:35:25.642526: Training Step 18/354: batchLoss = 0.6462, diffLoss = 3.1619, kgLoss = 0.0173
2025-04-08 12:35:27.259783: Training Step 19/354: batchLoss = 0.5975, diffLoss = 2.9313, kgLoss = 0.0141
2025-04-08 12:35:28.873289: Training Step 20/354: batchLoss = 0.7414, diffLoss = 3.6483, kgLoss = 0.0147
2025-04-08 12:35:30.491808: Training Step 21/354: batchLoss = 0.5377, diffLoss = 2.6435, kgLoss = 0.0113
2025-04-08 12:35:32.108963: Training Step 22/354: batchLoss = 0.6544, diffLoss = 3.2116, kgLoss = 0.0151
2025-04-08 12:35:33.723084: Training Step 23/354: batchLoss = 0.6742, diffLoss = 3.2727, kgLoss = 0.0246
2025-04-08 12:35:35.340487: Training Step 24/354: batchLoss = 0.6054, diffLoss = 2.9733, kgLoss = 0.0134
2025-04-08 12:35:36.963892: Training Step 25/354: batchLoss = 0.7073, diffLoss = 3.4714, kgLoss = 0.0163
2025-04-08 12:35:38.585874: Training Step 26/354: batchLoss = 0.6326, diffLoss = 3.1101, kgLoss = 0.0132
2025-04-08 12:35:40.202091: Training Step 27/354: batchLoss = 0.5757, diffLoss = 2.8313, kgLoss = 0.0118
2025-04-08 12:35:41.813729: Training Step 28/354: batchLoss = 0.6894, diffLoss = 3.3924, kgLoss = 0.0137
2025-04-08 12:35:43.429430: Training Step 29/354: batchLoss = 0.5749, diffLoss = 2.8176, kgLoss = 0.0143
2025-04-08 12:35:45.043869: Training Step 30/354: batchLoss = 0.6235, diffLoss = 3.0620, kgLoss = 0.0139
2025-04-08 12:35:46.668659: Training Step 31/354: batchLoss = 0.7191, diffLoss = 3.5308, kgLoss = 0.0162
2025-04-08 12:35:48.301177: Training Step 32/354: batchLoss = 0.6608, diffLoss = 3.2494, kgLoss = 0.0136
2025-04-08 12:35:49.933630: Training Step 33/354: batchLoss = 0.6763, diffLoss = 3.3216, kgLoss = 0.0150
2025-04-08 12:35:51.551661: Training Step 34/354: batchLoss = 0.6415, diffLoss = 3.1547, kgLoss = 0.0132
2025-04-08 12:35:53.182957: Training Step 35/354: batchLoss = 0.6226, diffLoss = 3.0425, kgLoss = 0.0176
2025-04-08 12:35:54.793997: Training Step 36/354: batchLoss = 0.6744, diffLoss = 3.3088, kgLoss = 0.0158
2025-04-08 12:35:56.403430: Training Step 37/354: batchLoss = 0.5588, diffLoss = 2.7378, kgLoss = 0.0140
2025-04-08 12:35:58.028268: Training Step 38/354: batchLoss = 0.7100, diffLoss = 3.4877, kgLoss = 0.0155
2025-04-08 12:35:59.644295: Training Step 39/354: batchLoss = 0.9190, diffLoss = 4.5242, kgLoss = 0.0177
2025-04-08 12:36:01.277904: Training Step 40/354: batchLoss = 0.6502, diffLoss = 3.1978, kgLoss = 0.0133
2025-04-08 12:36:02.898246: Training Step 41/354: batchLoss = 0.7876, diffLoss = 3.8723, kgLoss = 0.0164
2025-04-08 12:36:04.523746: Training Step 42/354: batchLoss = 0.6765, diffLoss = 3.3337, kgLoss = 0.0122
2025-04-08 12:36:06.166944: Training Step 43/354: batchLoss = 0.7004, diffLoss = 3.4440, kgLoss = 0.0145
2025-04-08 12:36:07.793879: Training Step 44/354: batchLoss = 0.7320, diffLoss = 3.6073, kgLoss = 0.0132
2025-04-08 12:36:09.406516: Training Step 45/354: batchLoss = 0.6427, diffLoss = 3.1614, kgLoss = 0.0131
2025-04-08 12:36:11.016453: Training Step 46/354: batchLoss = 0.6552, diffLoss = 3.2205, kgLoss = 0.0138
2025-04-08 12:36:12.629838: Training Step 47/354: batchLoss = 0.5444, diffLoss = 2.6776, kgLoss = 0.0111
2025-04-08 12:36:14.253264: Training Step 48/354: batchLoss = 0.5079, diffLoss = 2.4942, kgLoss = 0.0113
2025-04-08 12:36:15.874740: Training Step 49/354: batchLoss = 0.6609, diffLoss = 3.2509, kgLoss = 0.0134
2025-04-08 12:36:17.498454: Training Step 50/354: batchLoss = 0.7954, diffLoss = 3.9074, kgLoss = 0.0174
2025-04-08 12:36:19.115895: Training Step 51/354: batchLoss = 0.6595, diffLoss = 3.2434, kgLoss = 0.0136
2025-04-08 12:36:20.736679: Training Step 52/354: batchLoss = 0.6370, diffLoss = 3.1306, kgLoss = 0.0136
2025-04-08 12:36:22.365386: Training Step 53/354: batchLoss = 0.6374, diffLoss = 3.1323, kgLoss = 0.0136
2025-04-08 12:36:23.981451: Training Step 54/354: batchLoss = 0.8006, diffLoss = 3.9281, kgLoss = 0.0187
2025-04-08 12:36:25.586449: Training Step 55/354: batchLoss = 0.8887, diffLoss = 4.3721, kgLoss = 0.0178
2025-04-08 12:36:27.193672: Training Step 56/354: batchLoss = 0.7467, diffLoss = 3.6708, kgLoss = 0.0157
2025-04-08 12:36:28.807254: Training Step 57/354: batchLoss = 0.6844, diffLoss = 3.3488, kgLoss = 0.0183
2025-04-08 12:36:30.428717: Training Step 58/354: batchLoss = 0.7570, diffLoss = 3.7166, kgLoss = 0.0171
2025-04-08 12:36:32.048917: Training Step 59/354: batchLoss = 0.6220, diffLoss = 3.0558, kgLoss = 0.0135
2025-04-08 12:36:33.674059: Training Step 60/354: batchLoss = 0.7640, diffLoss = 3.7528, kgLoss = 0.0168
2025-04-08 12:36:35.287337: Training Step 61/354: batchLoss = 0.7130, diffLoss = 3.5041, kgLoss = 0.0152
2025-04-08 12:36:36.906681: Training Step 62/354: batchLoss = 0.8629, diffLoss = 4.2460, kgLoss = 0.0171
2025-04-08 12:36:38.516564: Training Step 63/354: batchLoss = 0.4967, diffLoss = 2.4349, kgLoss = 0.0122
2025-04-08 12:36:40.125695: Training Step 64/354: batchLoss = 0.6294, diffLoss = 3.0909, kgLoss = 0.0141
2025-04-08 12:36:41.730119: Training Step 65/354: batchLoss = 0.5875, diffLoss = 2.8779, kgLoss = 0.0149
2025-04-08 12:36:43.348305: Training Step 66/354: batchLoss = 0.6083, diffLoss = 2.9911, kgLoss = 0.0126
2025-04-08 12:36:44.968857: Training Step 67/354: batchLoss = 0.6844, diffLoss = 3.3657, kgLoss = 0.0141
2025-04-08 12:36:46.585638: Training Step 68/354: batchLoss = 0.6070, diffLoss = 2.9766, kgLoss = 0.0146
2025-04-08 12:36:48.203584: Training Step 69/354: batchLoss = 0.6777, diffLoss = 3.3356, kgLoss = 0.0132
2025-04-08 12:36:49.827183: Training Step 70/354: batchLoss = 0.6620, diffLoss = 3.2518, kgLoss = 0.0145
2025-04-08 12:36:51.444680: Training Step 71/354: batchLoss = 0.5900, diffLoss = 2.8966, kgLoss = 0.0134
2025-04-08 12:36:53.068174: Training Step 72/354: batchLoss = 0.7380, diffLoss = 3.6288, kgLoss = 0.0153
2025-04-08 12:36:54.675413: Training Step 73/354: batchLoss = 0.6255, diffLoss = 3.0770, kgLoss = 0.0127
2025-04-08 12:36:56.298477: Training Step 74/354: batchLoss = 0.6587, diffLoss = 3.2405, kgLoss = 0.0132
2025-04-08 12:36:57.913709: Training Step 75/354: batchLoss = 0.6095, diffLoss = 2.9933, kgLoss = 0.0135
2025-04-08 12:36:59.539643: Training Step 76/354: batchLoss = 0.6106, diffLoss = 2.9995, kgLoss = 0.0134
2025-04-08 12:37:01.156511: Training Step 77/354: batchLoss = 0.6761, diffLoss = 3.3259, kgLoss = 0.0136
2025-04-08 12:37:02.768938: Training Step 78/354: batchLoss = 0.5347, diffLoss = 2.6271, kgLoss = 0.0116
2025-04-08 12:37:04.390686: Training Step 79/354: batchLoss = 0.6900, diffLoss = 3.3915, kgLoss = 0.0146
2025-04-08 12:37:06.011320: Training Step 80/354: batchLoss = 0.6719, diffLoss = 3.3066, kgLoss = 0.0133
2025-04-08 12:37:07.625587: Training Step 81/354: batchLoss = 0.6210, diffLoss = 3.0505, kgLoss = 0.0137
2025-04-08 12:37:09.233144: Training Step 82/354: batchLoss = 0.6874, diffLoss = 3.3811, kgLoss = 0.0140
2025-04-08 12:37:10.834776: Training Step 83/354: batchLoss = 0.6040, diffLoss = 2.9668, kgLoss = 0.0133
2025-04-08 12:37:12.455975: Training Step 84/354: batchLoss = 0.6015, diffLoss = 2.9591, kgLoss = 0.0121
2025-04-08 12:37:14.077885: Training Step 85/354: batchLoss = 0.7922, diffLoss = 3.9001, kgLoss = 0.0153
2025-04-08 12:37:15.714256: Training Step 86/354: batchLoss = 0.6529, diffLoss = 3.1994, kgLoss = 0.0162
2025-04-08 12:37:17.360733: Training Step 87/354: batchLoss = 0.8441, diffLoss = 4.1579, kgLoss = 0.0156
2025-04-08 12:37:19.002293: Training Step 88/354: batchLoss = 0.8575, diffLoss = 4.2187, kgLoss = 0.0173
2025-04-08 12:37:20.645762: Training Step 89/354: batchLoss = 0.7326, diffLoss = 3.5980, kgLoss = 0.0163
2025-04-08 12:37:22.268604: Training Step 90/354: batchLoss = 0.6013, diffLoss = 2.9512, kgLoss = 0.0139
2025-04-08 12:37:23.885846: Training Step 91/354: batchLoss = 0.6849, diffLoss = 3.3645, kgLoss = 0.0150
2025-04-08 12:37:25.512905: Training Step 92/354: batchLoss = 0.8252, diffLoss = 4.0591, kgLoss = 0.0167
2025-04-08 12:37:27.151416: Training Step 93/354: batchLoss = 0.6945, diffLoss = 3.4066, kgLoss = 0.0165
2025-04-08 12:37:28.803376: Training Step 94/354: batchLoss = 0.6799, diffLoss = 3.3404, kgLoss = 0.0147
2025-04-08 12:37:30.418778: Training Step 95/354: batchLoss = 0.6982, diffLoss = 3.4330, kgLoss = 0.0145
2025-04-08 12:37:32.035179: Training Step 96/354: batchLoss = 0.5988, diffLoss = 2.9453, kgLoss = 0.0122
2025-04-08 12:37:33.662879: Training Step 97/354: batchLoss = 0.7512, diffLoss = 3.6956, kgLoss = 0.0151
2025-04-08 12:37:35.291692: Training Step 98/354: batchLoss = 0.8415, diffLoss = 4.1430, kgLoss = 0.0161
2025-04-08 12:37:36.901385: Training Step 99/354: batchLoss = 1.4207, diffLoss = 6.9951, kgLoss = 0.0270
2025-04-08 12:37:38.514549: Training Step 100/354: batchLoss = 0.5732, diffLoss = 2.8193, kgLoss = 0.0117
2025-04-08 12:37:40.133497: Training Step 101/354: batchLoss = 0.6106, diffLoss = 3.0039, kgLoss = 0.0123
2025-04-08 12:37:41.758650: Training Step 102/354: batchLoss = 0.7188, diffLoss = 3.5347, kgLoss = 0.0149
2025-04-08 12:37:43.384915: Training Step 103/354: batchLoss = 0.6850, diffLoss = 3.3690, kgLoss = 0.0140
2025-04-08 12:37:45.002294: Training Step 104/354: batchLoss = 0.7552, diffLoss = 3.7179, kgLoss = 0.0145
2025-04-08 12:37:46.630046: Training Step 105/354: batchLoss = 0.8167, diffLoss = 4.0113, kgLoss = 0.0181
2025-04-08 12:37:48.255363: Training Step 106/354: batchLoss = 0.7963, diffLoss = 3.9196, kgLoss = 0.0155
2025-04-08 12:37:49.883548: Training Step 107/354: batchLoss = 0.7636, diffLoss = 3.7577, kgLoss = 0.0151
2025-04-08 12:37:51.503830: Training Step 108/354: batchLoss = 0.7083, diffLoss = 3.4784, kgLoss = 0.0157
2025-04-08 12:37:53.107784: Training Step 109/354: batchLoss = 0.6826, diffLoss = 3.3549, kgLoss = 0.0145
2025-04-08 12:37:54.717067: Training Step 110/354: batchLoss = 0.6824, diffLoss = 3.3552, kgLoss = 0.0143
2025-04-08 12:37:56.339237: Training Step 111/354: batchLoss = 0.6243, diffLoss = 3.0618, kgLoss = 0.0149
2025-04-08 12:37:57.975831: Training Step 112/354: batchLoss = 0.7269, diffLoss = 3.5736, kgLoss = 0.0152
2025-04-08 12:37:59.601775: Training Step 113/354: batchLoss = 0.7375, diffLoss = 3.6266, kgLoss = 0.0152
2025-04-08 12:38:01.222155: Training Step 114/354: batchLoss = 0.5459, diffLoss = 2.6747, kgLoss = 0.0137
2025-04-08 12:38:02.871622: Training Step 115/354: batchLoss = 0.7348, diffLoss = 3.6124, kgLoss = 0.0154
2025-04-08 12:38:04.490647: Training Step 116/354: batchLoss = 0.8795, diffLoss = 4.3260, kgLoss = 0.0178
2025-04-08 12:38:06.110172: Training Step 117/354: batchLoss = 0.6399, diffLoss = 3.1435, kgLoss = 0.0139
2025-04-08 12:38:07.718325: Training Step 118/354: batchLoss = 0.6538, diffLoss = 3.2095, kgLoss = 0.0148
2025-04-08 12:38:09.331294: Training Step 119/354: batchLoss = 0.5963, diffLoss = 2.9270, kgLoss = 0.0137
2025-04-08 12:38:10.954806: Training Step 120/354: batchLoss = 0.5546, diffLoss = 2.7225, kgLoss = 0.0126
2025-04-08 12:38:12.582307: Training Step 121/354: batchLoss = 0.7694, diffLoss = 3.7731, kgLoss = 0.0185
2025-04-08 12:38:14.207721: Training Step 122/354: batchLoss = 0.9280, diffLoss = 4.5622, kgLoss = 0.0195
2025-04-08 12:38:15.827432: Training Step 123/354: batchLoss = 0.6558, diffLoss = 3.2245, kgLoss = 0.0137
2025-04-08 12:38:17.452402: Training Step 124/354: batchLoss = 0.6222, diffLoss = 3.0590, kgLoss = 0.0130
2025-04-08 12:38:19.066037: Training Step 125/354: batchLoss = 0.6510, diffLoss = 3.2028, kgLoss = 0.0131
2025-04-08 12:38:20.676357: Training Step 126/354: batchLoss = 0.6383, diffLoss = 3.1365, kgLoss = 0.0137
2025-04-08 12:38:22.284036: Training Step 127/354: batchLoss = 2.3499, diffLoss = 11.5741, kgLoss = 0.0439
2025-04-08 12:38:23.889755: Training Step 128/354: batchLoss = 0.6663, diffLoss = 3.2775, kgLoss = 0.0135
2025-04-08 12:38:25.510716: Training Step 129/354: batchLoss = 0.6685, diffLoss = 3.2847, kgLoss = 0.0144
2025-04-08 12:38:27.126620: Training Step 130/354: batchLoss = 0.6264, diffLoss = 3.0785, kgLoss = 0.0134
2025-04-08 12:38:28.742918: Training Step 131/354: batchLoss = 0.6808, diffLoss = 3.3419, kgLoss = 0.0156
2025-04-08 12:38:30.363681: Training Step 132/354: batchLoss = 0.6897, diffLoss = 3.3964, kgLoss = 0.0130
2025-04-08 12:38:31.980584: Training Step 133/354: batchLoss = 0.6540, diffLoss = 3.2162, kgLoss = 0.0134
2025-04-08 12:38:33.590172: Training Step 134/354: batchLoss = 0.6573, diffLoss = 3.2291, kgLoss = 0.0143
2025-04-08 12:38:35.200586: Training Step 135/354: batchLoss = 0.6013, diffLoss = 2.9540, kgLoss = 0.0131
2025-04-08 12:38:36.823630: Training Step 136/354: batchLoss = 0.7106, diffLoss = 3.4863, kgLoss = 0.0167
2025-04-08 12:38:38.433130: Training Step 137/354: batchLoss = 0.6857, diffLoss = 3.3740, kgLoss = 0.0136
2025-04-08 12:38:40.047002: Training Step 138/354: batchLoss = 0.6345, diffLoss = 3.1108, kgLoss = 0.0155
2025-04-08 12:38:41.664354: Training Step 139/354: batchLoss = 0.6820, diffLoss = 3.3499, kgLoss = 0.0150
2025-04-08 12:38:43.284522: Training Step 140/354: batchLoss = 0.6285, diffLoss = 3.0922, kgLoss = 0.0125
2025-04-08 12:38:44.904494: Training Step 141/354: batchLoss = 0.5595, diffLoss = 2.7442, kgLoss = 0.0134
2025-04-08 12:38:46.540795: Training Step 142/354: batchLoss = 0.9130, diffLoss = 4.4947, kgLoss = 0.0176
2025-04-08 12:38:48.172967: Training Step 143/354: batchLoss = 0.6084, diffLoss = 2.9879, kgLoss = 0.0136
2025-04-08 12:38:49.795947: Training Step 144/354: batchLoss = 0.6892, diffLoss = 3.3909, kgLoss = 0.0138
2025-04-08 12:38:51.410134: Training Step 145/354: batchLoss = 0.7320, diffLoss = 3.6010, kgLoss = 0.0147
2025-04-08 12:38:53.017175: Training Step 146/354: batchLoss = 0.6525, diffLoss = 3.2076, kgLoss = 0.0137
2025-04-08 12:38:54.634667: Training Step 147/354: batchLoss = 0.5966, diffLoss = 2.9310, kgLoss = 0.0130
2025-04-08 12:38:56.273317: Training Step 148/354: batchLoss = 0.4930, diffLoss = 2.4199, kgLoss = 0.0112
2025-04-08 12:38:57.896458: Training Step 149/354: batchLoss = 0.7265, diffLoss = 3.5679, kgLoss = 0.0162
2025-04-08 12:38:59.521584: Training Step 150/354: batchLoss = 0.6641, diffLoss = 3.2568, kgLoss = 0.0159
2025-04-08 12:39:01.141921: Training Step 151/354: batchLoss = 0.6714, diffLoss = 3.3045, kgLoss = 0.0131
2025-04-08 12:39:02.769181: Training Step 152/354: batchLoss = 0.8609, diffLoss = 4.2345, kgLoss = 0.0175
2025-04-08 12:39:04.381398: Training Step 153/354: batchLoss = 0.6819, diffLoss = 3.3516, kgLoss = 0.0145
2025-04-08 12:39:05.999154: Training Step 154/354: batchLoss = 0.7790, diffLoss = 3.8321, kgLoss = 0.0157
2025-04-08 12:39:07.607514: Training Step 155/354: batchLoss = 0.7191, diffLoss = 3.5385, kgLoss = 0.0143
2025-04-08 12:39:09.225492: Training Step 156/354: batchLoss = 0.7057, diffLoss = 3.4651, kgLoss = 0.0159
2025-04-08 12:39:10.853360: Training Step 157/354: batchLoss = 0.6275, diffLoss = 3.0837, kgLoss = 0.0135
2025-04-08 12:39:12.482298: Training Step 158/354: batchLoss = 0.6510, diffLoss = 3.2016, kgLoss = 0.0134
2025-04-08 12:39:14.104390: Training Step 159/354: batchLoss = 0.6392, diffLoss = 3.1222, kgLoss = 0.0184
2025-04-08 12:39:15.731192: Training Step 160/354: batchLoss = 0.6534, diffLoss = 3.2121, kgLoss = 0.0137
2025-04-08 12:39:17.347250: Training Step 161/354: batchLoss = 0.6740, diffLoss = 3.3153, kgLoss = 0.0137
2025-04-08 12:39:18.968589: Training Step 162/354: batchLoss = 0.9170, diffLoss = 4.5138, kgLoss = 0.0178
2025-04-08 12:39:20.572462: Training Step 163/354: batchLoss = 0.6422, diffLoss = 3.1528, kgLoss = 0.0145
2025-04-08 12:39:22.195893: Training Step 164/354: batchLoss = 0.6178, diffLoss = 3.0363, kgLoss = 0.0132
2025-04-08 12:39:23.819335: Training Step 165/354: batchLoss = 0.6761, diffLoss = 3.3183, kgLoss = 0.0155
2025-04-08 12:39:25.443939: Training Step 166/354: batchLoss = 0.6853, diffLoss = 3.3693, kgLoss = 0.0143
2025-04-08 12:39:27.073319: Training Step 167/354: batchLoss = 0.6413, diffLoss = 3.1545, kgLoss = 0.0130
2025-04-08 12:39:28.694307: Training Step 168/354: batchLoss = 0.6925, diffLoss = 3.4080, kgLoss = 0.0137
2025-04-08 12:39:30.314565: Training Step 169/354: batchLoss = 0.6753, diffLoss = 3.3198, kgLoss = 0.0142
2025-04-08 12:39:31.932976: Training Step 170/354: batchLoss = 0.6478, diffLoss = 3.1804, kgLoss = 0.0147
2025-04-08 12:39:33.555624: Training Step 171/354: batchLoss = 0.9349, diffLoss = 4.5979, kgLoss = 0.0192
2025-04-08 12:39:35.155336: Training Step 172/354: batchLoss = 0.7089, diffLoss = 3.4804, kgLoss = 0.0160
2025-04-08 12:39:36.747361: Training Step 173/354: batchLoss = 0.5942, diffLoss = 2.9217, kgLoss = 0.0123
2025-04-08 12:39:38.355783: Training Step 174/354: batchLoss = 0.5972, diffLoss = 2.9355, kgLoss = 0.0127
2025-04-08 12:39:39.981797: Training Step 175/354: batchLoss = 0.5639, diffLoss = 2.7721, kgLoss = 0.0119
2025-04-08 12:39:41.607951: Training Step 176/354: batchLoss = 0.7105, diffLoss = 3.4856, kgLoss = 0.0167
2025-04-08 12:39:43.227865: Training Step 177/354: batchLoss = 0.6236, diffLoss = 3.0643, kgLoss = 0.0134
2025-04-08 12:39:44.852946: Training Step 178/354: batchLoss = 0.5990, diffLoss = 2.9347, kgLoss = 0.0150
2025-04-08 12:39:46.478776: Training Step 179/354: batchLoss = 0.6278, diffLoss = 3.0808, kgLoss = 0.0145
2025-04-08 12:39:48.100724: Training Step 180/354: batchLoss = 0.6925, diffLoss = 3.4002, kgLoss = 0.0156
2025-04-08 12:39:49.717688: Training Step 181/354: batchLoss = 0.6283, diffLoss = 3.0853, kgLoss = 0.0141
2025-04-08 12:39:51.322276: Training Step 182/354: batchLoss = 0.6473, diffLoss = 3.1855, kgLoss = 0.0127
2025-04-08 12:39:52.930708: Training Step 183/354: batchLoss = 0.7370, diffLoss = 3.6212, kgLoss = 0.0160
2025-04-08 12:39:54.548609: Training Step 184/354: batchLoss = 0.6480, diffLoss = 3.1854, kgLoss = 0.0137
2025-04-08 12:39:56.172556: Training Step 185/354: batchLoss = 0.6172, diffLoss = 3.0275, kgLoss = 0.0146
2025-04-08 12:39:57.796646: Training Step 186/354: batchLoss = 0.6412, diffLoss = 3.1496, kgLoss = 0.0141
2025-04-08 12:39:59.422913: Training Step 187/354: batchLoss = 0.6986, diffLoss = 3.4161, kgLoss = 0.0192
2025-04-08 12:40:01.040886: Training Step 188/354: batchLoss = 0.7050, diffLoss = 3.4695, kgLoss = 0.0139
2025-04-08 12:40:02.660229: Training Step 189/354: batchLoss = 0.6547, diffLoss = 3.2143, kgLoss = 0.0148
2025-04-08 12:40:04.271511: Training Step 190/354: batchLoss = 0.6374, diffLoss = 3.1308, kgLoss = 0.0140
2025-04-08 12:40:05.877920: Training Step 191/354: batchLoss = 0.6911, diffLoss = 3.3986, kgLoss = 0.0143
2025-04-08 12:40:07.491941: Training Step 192/354: batchLoss = 0.6972, diffLoss = 3.4244, kgLoss = 0.0153
2025-04-08 12:40:09.107778: Training Step 193/354: batchLoss = 0.5628, diffLoss = 2.7656, kgLoss = 0.0121
2025-04-08 12:40:10.727785: Training Step 194/354: batchLoss = 0.6177, diffLoss = 3.0375, kgLoss = 0.0127
2025-04-08 12:40:12.358488: Training Step 195/354: batchLoss = 0.6469, diffLoss = 3.1726, kgLoss = 0.0154
2025-04-08 12:40:13.984823: Training Step 196/354: batchLoss = 0.8586, diffLoss = 4.2224, kgLoss = 0.0177
2025-04-08 12:40:15.601602: Training Step 197/354: batchLoss = 0.5668, diffLoss = 2.7818, kgLoss = 0.0131
2025-04-08 12:40:17.217048: Training Step 198/354: batchLoss = 0.9049, diffLoss = 4.4506, kgLoss = 0.0184
2025-04-08 12:40:18.829597: Training Step 199/354: batchLoss = 0.6158, diffLoss = 3.0308, kgLoss = 0.0120
2025-04-08 12:40:20.444199: Training Step 200/354: batchLoss = 0.5544, diffLoss = 2.7237, kgLoss = 0.0121
2025-04-08 12:40:22.061193: Training Step 201/354: batchLoss = 0.7139, diffLoss = 3.5034, kgLoss = 0.0166
2025-04-08 12:40:23.675161: Training Step 202/354: batchLoss = 0.5788, diffLoss = 2.8323, kgLoss = 0.0154
2025-04-08 12:40:25.299098: Training Step 203/354: batchLoss = 0.9125, diffLoss = 4.4956, kgLoss = 0.0168
2025-04-08 12:40:26.921815: Training Step 204/354: batchLoss = 0.6722, diffLoss = 3.3121, kgLoss = 0.0122
2025-04-08 12:40:28.536973: Training Step 205/354: batchLoss = 0.4882, diffLoss = 2.3828, kgLoss = 0.0145
2025-04-08 12:40:30.161124: Training Step 206/354: batchLoss = 0.6681, diffLoss = 3.2778, kgLoss = 0.0157
2025-04-08 12:40:31.777404: Training Step 207/354: batchLoss = 0.6804, diffLoss = 3.3448, kgLoss = 0.0144
2025-04-08 12:40:33.407582: Training Step 208/354: batchLoss = 0.5856, diffLoss = 2.8685, kgLoss = 0.0148
2025-04-08 12:40:35.023208: Training Step 209/354: batchLoss = 0.7145, diffLoss = 3.5114, kgLoss = 0.0153
2025-04-08 12:40:36.635658: Training Step 210/354: batchLoss = 0.5758, diffLoss = 2.8238, kgLoss = 0.0137
2025-04-08 12:40:38.250464: Training Step 211/354: batchLoss = 0.7673, diffLoss = 3.7719, kgLoss = 0.0162
2025-04-08 12:40:39.872195: Training Step 212/354: batchLoss = 0.6926, diffLoss = 3.4045, kgLoss = 0.0146
2025-04-08 12:40:41.489897: Training Step 213/354: batchLoss = 0.6768, diffLoss = 3.3251, kgLoss = 0.0147
2025-04-08 12:40:43.105258: Training Step 214/354: batchLoss = 0.6213, diffLoss = 3.0508, kgLoss = 0.0139
2025-04-08 12:40:44.722889: Training Step 215/354: batchLoss = 0.6539, diffLoss = 3.2157, kgLoss = 0.0134
2025-04-08 12:40:46.345947: Training Step 216/354: batchLoss = 0.5999, diffLoss = 2.9408, kgLoss = 0.0147
2025-04-08 12:40:47.964673: Training Step 217/354: batchLoss = 0.5707, diffLoss = 2.8011, kgLoss = 0.0131
2025-04-08 12:40:49.568654: Training Step 218/354: batchLoss = 0.6166, diffLoss = 3.0280, kgLoss = 0.0137
2025-04-08 12:40:51.179796: Training Step 219/354: batchLoss = 0.7039, diffLoss = 3.4586, kgLoss = 0.0152
2025-04-08 12:40:52.798127: Training Step 220/354: batchLoss = 0.6942, diffLoss = 3.4124, kgLoss = 0.0147
2025-04-08 12:40:54.421459: Training Step 221/354: batchLoss = 0.6275, diffLoss = 3.0762, kgLoss = 0.0153
2025-04-08 12:40:56.044267: Training Step 222/354: batchLoss = 0.7996, diffLoss = 3.9293, kgLoss = 0.0172
2025-04-08 12:40:57.667727: Training Step 223/354: batchLoss = 0.6188, diffLoss = 3.0344, kgLoss = 0.0148
2025-04-08 12:40:59.287464: Training Step 224/354: batchLoss = 0.7847, diffLoss = 3.8630, kgLoss = 0.0151
2025-04-08 12:41:00.905461: Training Step 225/354: batchLoss = 0.5616, diffLoss = 2.7439, kgLoss = 0.0160
2025-04-08 12:41:02.525836: Training Step 226/354: batchLoss = 0.7319, diffLoss = 3.6024, kgLoss = 0.0143
2025-04-08 12:41:04.137747: Training Step 227/354: batchLoss = 0.5298, diffLoss = 2.5944, kgLoss = 0.0137
2025-04-08 12:41:05.763392: Training Step 228/354: batchLoss = 0.5411, diffLoss = 2.6600, kgLoss = 0.0114
2025-04-08 12:41:07.385447: Training Step 229/354: batchLoss = 0.7516, diffLoss = 3.6929, kgLoss = 0.0163
2025-04-08 12:41:09.003273: Training Step 230/354: batchLoss = 0.6761, diffLoss = 3.3237, kgLoss = 0.0142
2025-04-08 12:41:10.631370: Training Step 231/354: batchLoss = 0.9717, diffLoss = 4.7704, kgLoss = 0.0220
2025-04-08 12:41:12.261617: Training Step 232/354: batchLoss = 0.9777, diffLoss = 4.8048, kgLoss = 0.0209
2025-04-08 12:41:13.883797: Training Step 233/354: batchLoss = 0.8626, diffLoss = 4.2366, kgLoss = 0.0191
2025-04-08 12:41:15.502297: Training Step 234/354: batchLoss = 0.8708, diffLoss = 4.2866, kgLoss = 0.0169
2025-04-08 12:41:17.114498: Training Step 235/354: batchLoss = 0.6458, diffLoss = 3.1725, kgLoss = 0.0141
2025-04-08 12:41:18.720175: Training Step 236/354: batchLoss = 0.5670, diffLoss = 2.7832, kgLoss = 0.0130
2025-04-08 12:41:20.338213: Training Step 237/354: batchLoss = 0.5980, diffLoss = 2.9387, kgLoss = 0.0129
2025-04-08 12:41:21.946780: Training Step 238/354: batchLoss = 0.7259, diffLoss = 3.5688, kgLoss = 0.0152
2025-04-08 12:41:23.568420: Training Step 239/354: batchLoss = 0.6980, diffLoss = 3.4266, kgLoss = 0.0158
2025-04-08 12:41:25.188815: Training Step 240/354: batchLoss = 0.6510, diffLoss = 3.2018, kgLoss = 0.0133
2025-04-08 12:41:26.813395: Training Step 241/354: batchLoss = 0.6417, diffLoss = 3.1469, kgLoss = 0.0154
2025-04-08 12:41:28.441256: Training Step 242/354: batchLoss = 0.5148, diffLoss = 2.5265, kgLoss = 0.0119
2025-04-08 12:41:30.059829: Training Step 243/354: batchLoss = 0.6379, diffLoss = 3.1369, kgLoss = 0.0131
2025-04-08 12:41:31.676611: Training Step 244/354: batchLoss = 0.6234, diffLoss = 3.0627, kgLoss = 0.0135
2025-04-08 12:41:33.285664: Training Step 245/354: batchLoss = 0.6405, diffLoss = 3.1528, kgLoss = 0.0124
2025-04-08 12:41:34.900244: Training Step 246/354: batchLoss = 0.7514, diffLoss = 3.6914, kgLoss = 0.0164
2025-04-08 12:41:36.513455: Training Step 247/354: batchLoss = 0.7309, diffLoss = 3.5920, kgLoss = 0.0156
2025-04-08 12:41:38.140867: Training Step 248/354: batchLoss = 0.7580, diffLoss = 3.7278, kgLoss = 0.0156
2025-04-08 12:41:39.763836: Training Step 249/354: batchLoss = 0.6864, diffLoss = 3.3706, kgLoss = 0.0153
2025-04-08 12:41:41.387786: Training Step 250/354: batchLoss = 0.6220, diffLoss = 3.0570, kgLoss = 0.0132
2025-04-08 12:41:43.002052: Training Step 251/354: batchLoss = 0.5878, diffLoss = 2.8786, kgLoss = 0.0151
2025-04-08 12:41:44.622525: Training Step 252/354: batchLoss = 0.6797, diffLoss = 3.3388, kgLoss = 0.0150
2025-04-08 12:41:46.242028: Training Step 253/354: batchLoss = 0.6995, diffLoss = 3.4424, kgLoss = 0.0138
2025-04-08 12:41:47.851387: Training Step 254/354: batchLoss = 0.4816, diffLoss = 2.3659, kgLoss = 0.0105
2025-04-08 12:41:49.460270: Training Step 255/354: batchLoss = 0.7153, diffLoss = 3.5176, kgLoss = 0.0147
2025-04-08 12:41:51.083494: Training Step 256/354: batchLoss = 0.7248, diffLoss = 3.5644, kgLoss = 0.0148
2025-04-08 12:41:52.716795: Training Step 257/354: batchLoss = 0.9007, diffLoss = 4.4300, kgLoss = 0.0184
2025-04-08 12:41:54.358336: Training Step 258/354: batchLoss = 0.5199, diffLoss = 2.5555, kgLoss = 0.0110
2025-04-08 12:41:56.006572: Training Step 259/354: batchLoss = 0.6282, diffLoss = 3.0904, kgLoss = 0.0127
2025-04-08 12:41:57.636108: Training Step 260/354: batchLoss = 0.6391, diffLoss = 3.1441, kgLoss = 0.0128
2025-04-08 12:41:59.268418: Training Step 261/354: batchLoss = 0.5937, diffLoss = 2.9183, kgLoss = 0.0126
2025-04-08 12:42:00.898186: Training Step 262/354: batchLoss = 0.5414, diffLoss = 2.6599, kgLoss = 0.0118
2025-04-08 12:42:02.528163: Training Step 263/354: batchLoss = 0.5848, diffLoss = 2.8732, kgLoss = 0.0127
2025-04-08 12:42:04.183725: Training Step 264/354: batchLoss = 0.5276, diffLoss = 2.5789, kgLoss = 0.0147
2025-04-08 12:42:05.805994: Training Step 265/354: batchLoss = 0.7081, diffLoss = 3.4789, kgLoss = 0.0155
2025-04-08 12:42:07.428978: Training Step 266/354: batchLoss = 0.6848, diffLoss = 3.3688, kgLoss = 0.0138
2025-04-08 12:42:09.051655: Training Step 267/354: batchLoss = 0.7129, diffLoss = 3.4991, kgLoss = 0.0163
2025-04-08 12:42:10.671415: Training Step 268/354: batchLoss = 0.6587, diffLoss = 3.2355, kgLoss = 0.0145
2025-04-08 12:42:12.291470: Training Step 269/354: batchLoss = 0.5562, diffLoss = 2.7248, kgLoss = 0.0141
2025-04-08 12:42:13.908055: Training Step 270/354: batchLoss = 0.8446, diffLoss = 4.1516, kgLoss = 0.0178
2025-04-08 12:42:15.526609: Training Step 271/354: batchLoss = 0.5882, diffLoss = 2.8864, kgLoss = 0.0136
2025-04-08 12:42:17.138793: Training Step 272/354: batchLoss = 0.7163, diffLoss = 3.5235, kgLoss = 0.0145
2025-04-08 12:42:18.756399: Training Step 273/354: batchLoss = 0.6391, diffLoss = 3.1426, kgLoss = 0.0132
2025-04-08 12:42:20.371714: Training Step 274/354: batchLoss = 0.7541, diffLoss = 3.7121, kgLoss = 0.0146
2025-04-08 12:42:21.990881: Training Step 275/354: batchLoss = 0.6379, diffLoss = 3.1405, kgLoss = 0.0122
2025-04-08 12:42:23.624509: Training Step 276/354: batchLoss = 0.5188, diffLoss = 2.5476, kgLoss = 0.0116
2025-04-08 12:42:25.254718: Training Step 277/354: batchLoss = 0.7004, diffLoss = 3.4362, kgLoss = 0.0164
2025-04-08 12:42:26.876078: Training Step 278/354: batchLoss = 0.5860, diffLoss = 2.8858, kgLoss = 0.0110
2025-04-08 12:42:28.496935: Training Step 279/354: batchLoss = 0.7720, diffLoss = 3.7898, kgLoss = 0.0176
2025-04-08 12:42:30.115138: Training Step 280/354: batchLoss = 0.6185, diffLoss = 3.0327, kgLoss = 0.0150
2025-04-08 12:42:31.729789: Training Step 281/354: batchLoss = 0.6203, diffLoss = 3.0480, kgLoss = 0.0134
2025-04-08 12:42:33.340233: Training Step 282/354: batchLoss = 0.7544, diffLoss = 3.7144, kgLoss = 0.0144
2025-04-08 12:42:34.955668: Training Step 283/354: batchLoss = 0.7861, diffLoss = 3.8720, kgLoss = 0.0146
2025-04-08 12:42:36.582111: Training Step 284/354: batchLoss = 0.5479, diffLoss = 2.6899, kgLoss = 0.0124
2025-04-08 12:42:38.206239: Training Step 285/354: batchLoss = 0.6528, diffLoss = 3.2082, kgLoss = 0.0139
2025-04-08 12:42:39.823269: Training Step 286/354: batchLoss = 0.8077, diffLoss = 3.9710, kgLoss = 0.0169
2025-04-08 12:42:41.447853: Training Step 287/354: batchLoss = 0.9245, diffLoss = 4.5400, kgLoss = 0.0207
2025-04-08 12:42:43.074025: Training Step 288/354: batchLoss = 0.6323, diffLoss = 3.1069, kgLoss = 0.0137
2025-04-08 12:42:44.699908: Training Step 289/354: batchLoss = 0.6251, diffLoss = 3.0734, kgLoss = 0.0131
2025-04-08 12:42:46.316693: Training Step 290/354: batchLoss = 0.7404, diffLoss = 3.6432, kgLoss = 0.0147
2025-04-08 12:42:47.931494: Training Step 291/354: batchLoss = 0.7754, diffLoss = 3.8136, kgLoss = 0.0159
2025-04-08 12:42:49.542405: Training Step 292/354: batchLoss = 0.5766, diffLoss = 2.8278, kgLoss = 0.0138
2025-04-08 12:42:51.156290: Training Step 293/354: batchLoss = 0.7235, diffLoss = 3.5592, kgLoss = 0.0146
2025-04-08 12:42:52.775375: Training Step 294/354: batchLoss = 0.6152, diffLoss = 3.0161, kgLoss = 0.0150
2025-04-08 12:42:54.391406: Training Step 295/354: batchLoss = 0.8379, diffLoss = 4.0985, kgLoss = 0.0227
2025-04-08 12:42:56.010699: Training Step 296/354: batchLoss = 0.7375, diffLoss = 3.6264, kgLoss = 0.0153
2025-04-08 12:42:57.649407: Training Step 297/354: batchLoss = 0.6868, diffLoss = 3.3725, kgLoss = 0.0154
2025-04-08 12:42:59.275685: Training Step 298/354: batchLoss = 0.5771, diffLoss = 2.8399, kgLoss = 0.0115
2025-04-08 12:43:00.908016: Training Step 299/354: batchLoss = 0.7143, diffLoss = 3.5094, kgLoss = 0.0155
2025-04-08 12:43:02.516439: Training Step 300/354: batchLoss = 0.7484, diffLoss = 3.6816, kgLoss = 0.0150
2025-04-08 12:43:04.127150: Training Step 301/354: batchLoss = 0.5696, diffLoss = 2.7998, kgLoss = 0.0120
2025-04-08 12:43:05.746949: Training Step 302/354: batchLoss = 0.6311, diffLoss = 3.0874, kgLoss = 0.0171
2025-04-08 12:43:07.382030: Training Step 303/354: batchLoss = 0.7308, diffLoss = 3.5940, kgLoss = 0.0151
2025-04-08 12:43:09.003614: Training Step 304/354: batchLoss = 0.7525, diffLoss = 3.6913, kgLoss = 0.0178
2025-04-08 12:43:10.616137: Training Step 305/354: batchLoss = 0.7093, diffLoss = 3.4833, kgLoss = 0.0158
2025-04-08 12:43:12.234923: Training Step 306/354: batchLoss = 0.7999, diffLoss = 3.9261, kgLoss = 0.0184
2025-04-08 12:43:13.855297: Training Step 307/354: batchLoss = 0.5189, diffLoss = 2.5490, kgLoss = 0.0113
2025-04-08 12:43:15.465145: Training Step 308/354: batchLoss = 0.5509, diffLoss = 2.7020, kgLoss = 0.0131
2025-04-08 12:43:17.078011: Training Step 309/354: batchLoss = 0.5508, diffLoss = 2.7083, kgLoss = 0.0114
2025-04-08 12:43:18.698628: Training Step 310/354: batchLoss = 0.6953, diffLoss = 3.4188, kgLoss = 0.0144
2025-04-08 12:43:20.323791: Training Step 311/354: batchLoss = 0.7117, diffLoss = 3.4940, kgLoss = 0.0161
2025-04-08 12:43:21.945848: Training Step 312/354: batchLoss = 0.8696, diffLoss = 4.2776, kgLoss = 0.0176
2025-04-08 12:43:23.564724: Training Step 313/354: batchLoss = 0.5586, diffLoss = 2.7479, kgLoss = 0.0113
2025-04-08 12:43:25.192289: Training Step 314/354: batchLoss = 0.7682, diffLoss = 3.7757, kgLoss = 0.0164
2025-04-08 12:43:26.813430: Training Step 315/354: batchLoss = 0.7677, diffLoss = 3.7752, kgLoss = 0.0158
2025-04-08 12:43:28.430751: Training Step 316/354: batchLoss = 0.5739, diffLoss = 2.8129, kgLoss = 0.0141
2025-04-08 12:43:30.040574: Training Step 317/354: batchLoss = 0.6423, diffLoss = 3.1508, kgLoss = 0.0152
2025-04-08 12:43:31.667109: Training Step 318/354: batchLoss = 0.5812, diffLoss = 2.8545, kgLoss = 0.0128
2025-04-08 12:43:33.280976: Training Step 319/354: batchLoss = 0.5137, diffLoss = 2.5228, kgLoss = 0.0114
2025-04-08 12:43:34.905568: Training Step 320/354: batchLoss = 0.6722, diffLoss = 3.2950, kgLoss = 0.0165
2025-04-08 12:43:36.523673: Training Step 321/354: batchLoss = 0.7974, diffLoss = 3.9227, kgLoss = 0.0161
2025-04-08 12:43:38.139228: Training Step 322/354: batchLoss = 0.5916, diffLoss = 2.8988, kgLoss = 0.0148
2025-04-08 12:43:39.759006: Training Step 323/354: batchLoss = 0.8311, diffLoss = 4.0784, kgLoss = 0.0193
2025-04-08 12:43:41.379048: Training Step 324/354: batchLoss = 0.6545, diffLoss = 3.2107, kgLoss = 0.0155
2025-04-08 12:43:42.993699: Training Step 325/354: batchLoss = 0.7674, diffLoss = 3.7729, kgLoss = 0.0160
2025-04-08 12:43:44.609961: Training Step 326/354: batchLoss = 0.6897, diffLoss = 3.3925, kgLoss = 0.0140
2025-04-08 12:43:46.216611: Training Step 327/354: batchLoss = 0.7059, diffLoss = 3.4732, kgLoss = 0.0140
2025-04-08 12:43:47.827626: Training Step 328/354: batchLoss = 0.8228, diffLoss = 4.0484, kgLoss = 0.0164
2025-04-08 12:43:49.444790: Training Step 329/354: batchLoss = 0.5522, diffLoss = 2.7036, kgLoss = 0.0144
2025-04-08 12:43:51.063388: Training Step 330/354: batchLoss = 0.7792, diffLoss = 3.8269, kgLoss = 0.0173
2025-04-08 12:43:52.681365: Training Step 331/354: batchLoss = 0.5855, diffLoss = 2.8807, kgLoss = 0.0117
2025-04-08 12:43:54.311771: Training Step 332/354: batchLoss = 0.8368, diffLoss = 4.1110, kgLoss = 0.0183
2025-04-08 12:43:55.937636: Training Step 333/354: batchLoss = 0.6457, diffLoss = 3.1675, kgLoss = 0.0152
2025-04-08 12:43:57.559148: Training Step 334/354: batchLoss = 0.6077, diffLoss = 2.9844, kgLoss = 0.0135
2025-04-08 12:43:59.170532: Training Step 335/354: batchLoss = 0.6590, diffLoss = 3.2422, kgLoss = 0.0132
2025-04-08 12:44:00.797082: Training Step 336/354: batchLoss = 0.6245, diffLoss = 3.0639, kgLoss = 0.0146
2025-04-08 12:44:02.418898: Training Step 337/354: batchLoss = 0.7748, diffLoss = 3.8077, kgLoss = 0.0165
2025-04-08 12:44:04.045024: Training Step 338/354: batchLoss = 0.7500, diffLoss = 3.6842, kgLoss = 0.0164
2025-04-08 12:44:05.674923: Training Step 339/354: batchLoss = 0.8457, diffLoss = 4.1649, kgLoss = 0.0159
2025-04-08 12:44:07.301138: Training Step 340/354: batchLoss = 0.6726, diffLoss = 3.3112, kgLoss = 0.0130
2025-04-08 12:44:08.918555: Training Step 341/354: batchLoss = 0.8517, diffLoss = 4.1875, kgLoss = 0.0178
2025-04-08 12:44:10.546090: Training Step 342/354: batchLoss = 0.6447, diffLoss = 3.1704, kgLoss = 0.0133
2025-04-08 12:44:12.164970: Training Step 343/354: batchLoss = 0.7256, diffLoss = 3.5641, kgLoss = 0.0160
2025-04-08 12:44:13.782700: Training Step 344/354: batchLoss = 0.7429, diffLoss = 3.6471, kgLoss = 0.0168
2025-04-08 12:44:15.397147: Training Step 345/354: batchLoss = 0.6403, diffLoss = 3.1506, kgLoss = 0.0128
2025-04-08 12:44:17.011971: Training Step 346/354: batchLoss = 0.6144, diffLoss = 3.0179, kgLoss = 0.0135
2025-04-08 12:44:18.629168: Training Step 347/354: batchLoss = 0.7538, diffLoss = 3.7080, kgLoss = 0.0152
2025-04-08 12:44:20.259462: Training Step 348/354: batchLoss = 0.5957, diffLoss = 2.9264, kgLoss = 0.0130
2025-04-08 12:44:21.878607: Training Step 349/354: batchLoss = 0.4885, diffLoss = 2.3949, kgLoss = 0.0119
2025-04-08 12:44:23.506387: Training Step 350/354: batchLoss = 0.6096, diffLoss = 3.0001, kgLoss = 0.0119
2025-04-08 12:44:25.131291: Training Step 351/354: batchLoss = 0.6561, diffLoss = 3.2236, kgLoss = 0.0142
2025-04-08 12:44:26.740880: Training Step 352/354: batchLoss = 0.5770, diffLoss = 2.8353, kgLoss = 0.0124
2025-04-08 12:44:28.144853: Training Step 353/354: batchLoss = 0.5670, diffLoss = 2.7807, kgLoss = 0.0136
2025-04-08 12:44:28.223852: 
2025-04-08 12:44:28.224468: Epoch 0/1000, Train: epLoss = 1.2121, epDfLoss = 5.9562, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 12:44:29.529702: Steps 0/138: batch_recall = 10.86, batch_ndcg = 9.35 
2025-04-08 12:44:30.826567: Steps 1/138: batch_recall = 15.26, batch_ndcg = 10.12 
2025-04-08 12:44:32.130984: Steps 2/138: batch_recall = 14.07, batch_ndcg = 12.18 
2025-04-08 12:44:33.466709: Steps 3/138: batch_recall = 21.98, batch_ndcg = 14.59 
2025-04-08 12:44:34.788448: Steps 4/138: batch_recall = 18.78, batch_ndcg = 16.98 
2025-04-08 12:44:36.111127: Steps 5/138: batch_recall = 13.91, batch_ndcg = 9.73 
2025-04-08 12:44:37.432047: Steps 6/138: batch_recall = 14.24, batch_ndcg = 11.04 
2025-04-08 12:44:38.741868: Steps 7/138: batch_recall = 18.20, batch_ndcg = 13.33 
2025-04-08 12:44:40.055628: Steps 8/138: batch_recall = 15.34, batch_ndcg = 12.32 
2025-04-08 12:44:41.365869: Steps 9/138: batch_recall = 14.36, batch_ndcg = 11.74 
2025-04-08 12:44:42.664966: Steps 10/138: batch_recall = 14.56, batch_ndcg = 9.73 
2025-04-08 12:44:43.958308: Steps 11/138: batch_recall = 22.51, batch_ndcg = 15.74 
2025-04-08 12:44:45.240453: Steps 12/138: batch_recall = 16.31, batch_ndcg = 10.88 
2025-04-08 12:44:46.529732: Steps 13/138: batch_recall = 16.57, batch_ndcg = 11.69 
2025-04-08 12:44:47.828449: Steps 14/138: batch_recall = 13.11, batch_ndcg = 9.23 
2025-04-08 12:44:49.133653: Steps 15/138: batch_recall = 16.47, batch_ndcg = 13.34 
2025-04-08 12:44:50.442846: Steps 16/138: batch_recall = 13.52, batch_ndcg = 9.23 
2025-04-08 12:44:51.733891: Steps 17/138: batch_recall = 13.26, batch_ndcg = 9.29 
2025-04-08 12:44:53.038595: Steps 18/138: batch_recall = 19.71, batch_ndcg = 14.80 
2025-04-08 12:44:54.334886: Steps 19/138: batch_recall = 16.21, batch_ndcg = 12.60 
2025-04-08 12:44:55.661666: Steps 20/138: batch_recall = 18.47, batch_ndcg = 13.13 
2025-04-08 12:44:56.957408: Steps 21/138: batch_recall = 18.04, batch_ndcg = 12.33 
2025-04-08 12:44:58.250066: Steps 22/138: batch_recall = 12.13, batch_ndcg = 9.94 
2025-04-08 12:44:59.545874: Steps 23/138: batch_recall = 13.65, batch_ndcg = 8.74 
2025-04-08 12:45:00.836602: Steps 24/138: batch_recall = 15.98, batch_ndcg = 11.11 
2025-04-08 12:45:02.150345: Steps 25/138: batch_recall = 17.29, batch_ndcg = 10.69 
2025-04-08 12:45:03.455599: Steps 26/138: batch_recall = 16.64, batch_ndcg = 12.58 
2025-04-08 12:45:04.735269: Steps 27/138: batch_recall = 18.52, batch_ndcg = 12.68 
2025-04-08 12:45:06.030790: Steps 28/138: batch_recall = 21.50, batch_ndcg = 12.93 
2025-04-08 12:45:07.321753: Steps 29/138: batch_recall = 14.76, batch_ndcg = 9.59 
2025-04-08 12:45:08.618388: Steps 30/138: batch_recall = 18.52, batch_ndcg = 13.50 
2025-04-08 12:45:09.919896: Steps 31/138: batch_recall = 12.07, batch_ndcg = 8.80 
2025-04-08 12:45:11.195922: Steps 32/138: batch_recall = 16.17, batch_ndcg = 12.24 
2025-04-08 12:45:12.478205: Steps 33/138: batch_recall = 20.18, batch_ndcg = 13.70 
2025-04-08 12:45:13.750896: Steps 34/138: batch_recall = 17.76, batch_ndcg = 11.48 
2025-04-08 12:45:15.030258: Steps 35/138: batch_recall = 18.00, batch_ndcg = 12.64 
2025-04-08 12:45:16.342987: Steps 36/138: batch_recall = 15.13, batch_ndcg = 10.08 
2025-04-08 12:45:17.627016: Steps 37/138: batch_recall = 21.84, batch_ndcg = 13.91 
2025-04-08 12:45:18.915285: Steps 38/138: batch_recall = 20.33, batch_ndcg = 13.36 
2025-04-08 12:45:20.200418: Steps 39/138: batch_recall = 20.93, batch_ndcg = 15.70 
2025-04-08 12:45:21.489933: Steps 40/138: batch_recall = 19.50, batch_ndcg = 10.66 
2025-04-08 12:45:22.785694: Steps 41/138: batch_recall = 16.02, batch_ndcg = 11.76 
2025-04-08 12:45:24.093351: Steps 42/138: batch_recall = 20.19, batch_ndcg = 13.26 
2025-04-08 12:45:25.377338: Steps 43/138: batch_recall = 24.66, batch_ndcg = 16.54 
2025-04-08 12:45:26.667319: Steps 44/138: batch_recall = 17.52, batch_ndcg = 10.91 
2025-04-08 12:45:27.936589: Steps 45/138: batch_recall = 21.00, batch_ndcg = 14.33 
2025-04-08 12:45:29.211390: Steps 46/138: batch_recall = 21.40, batch_ndcg = 13.77 
2025-04-08 12:45:30.506247: Steps 47/138: batch_recall = 15.08, batch_ndcg = 10.66 
2025-04-08 12:45:31.790172: Steps 48/138: batch_recall = 21.31, batch_ndcg = 14.07 
2025-04-08 12:45:33.077566: Steps 49/138: batch_recall = 22.00, batch_ndcg = 13.85 
2025-04-08 12:45:34.365817: Steps 50/138: batch_recall = 16.41, batch_ndcg = 11.92 
2025-04-08 12:45:35.654180: Steps 51/138: batch_recall = 21.89, batch_ndcg = 14.68 
2025-04-08 12:45:36.976535: Steps 52/138: batch_recall = 24.98, batch_ndcg = 18.51 
2025-04-08 12:45:38.263212: Steps 53/138: batch_recall = 20.28, batch_ndcg = 12.42 
2025-04-08 12:45:39.551819: Steps 54/138: batch_recall = 19.60, batch_ndcg = 13.18 
2025-04-08 12:45:40.845303: Steps 55/138: batch_recall = 13.28, batch_ndcg = 8.95 
2025-04-08 12:45:42.127803: Steps 56/138: batch_recall = 19.09, batch_ndcg = 12.89 
2025-04-08 12:45:43.409945: Steps 57/138: batch_recall = 18.41, batch_ndcg = 12.63 
2025-04-08 12:45:44.704877: Steps 58/138: batch_recall = 23.62, batch_ndcg = 13.49 
2025-04-08 12:45:45.975852: Steps 59/138: batch_recall = 24.93, batch_ndcg = 15.60 
2025-04-08 12:45:47.257609: Steps 60/138: batch_recall = 26.52, batch_ndcg = 16.51 
2025-04-08 12:45:48.545213: Steps 61/138: batch_recall = 18.66, batch_ndcg = 11.10 
2025-04-08 12:45:49.833506: Steps 62/138: batch_recall = 24.57, batch_ndcg = 16.43 
2025-04-08 12:45:51.123953: Steps 63/138: batch_recall = 22.15, batch_ndcg = 15.68 
2025-04-08 12:45:52.398760: Steps 64/138: batch_recall = 21.06, batch_ndcg = 11.58 
2025-04-08 12:45:53.678970: Steps 65/138: batch_recall = 29.14, batch_ndcg = 18.07 
2025-04-08 12:45:54.951333: Steps 66/138: batch_recall = 27.78, batch_ndcg = 17.19 
2025-04-08 12:45:56.225178: Steps 67/138: batch_recall = 34.85, batch_ndcg = 22.33 
2025-04-08 12:45:57.495090: Steps 68/138: batch_recall = 27.82, batch_ndcg = 15.84 
2025-04-08 12:45:58.756614: Steps 69/138: batch_recall = 39.69, batch_ndcg = 24.82 
2025-04-08 12:46:00.010982: Steps 70/138: batch_recall = 25.62, batch_ndcg = 17.60 
2025-04-08 12:46:01.282993: Steps 71/138: batch_recall = 34.13, batch_ndcg = 22.57 
2025-04-08 12:46:02.563602: Steps 72/138: batch_recall = 35.65, batch_ndcg = 22.68 
2025-04-08 12:46:03.846469: Steps 73/138: batch_recall = 33.61, batch_ndcg = 19.67 
2025-04-08 12:46:05.136985: Steps 74/138: batch_recall = 23.94, batch_ndcg = 16.60 
2025-04-08 12:46:06.416927: Steps 75/138: batch_recall = 32.29, batch_ndcg = 22.97 
2025-04-08 12:46:07.697297: Steps 76/138: batch_recall = 35.94, batch_ndcg = 22.87 
2025-04-08 12:46:08.974988: Steps 77/138: batch_recall = 31.42, batch_ndcg = 21.44 
2025-04-08 12:46:10.242671: Steps 78/138: batch_recall = 37.37, batch_ndcg = 22.91 
2025-04-08 12:46:11.502610: Steps 79/138: batch_recall = 31.05, batch_ndcg = 19.76 
2025-04-08 12:46:12.775983: Steps 80/138: batch_recall = 32.52, batch_ndcg = 19.72 
2025-04-08 12:46:14.042928: Steps 81/138: batch_recall = 43.47, batch_ndcg = 28.25 
2025-04-08 12:46:15.300787: Steps 82/138: batch_recall = 31.99, batch_ndcg = 21.61 
2025-04-08 12:46:16.589909: Steps 83/138: batch_recall = 35.31, batch_ndcg = 21.50 
2025-04-08 12:46:17.868607: Steps 84/138: batch_recall = 34.47, batch_ndcg = 23.03 
2025-04-08 12:46:19.145911: Steps 85/138: batch_recall = 36.10, batch_ndcg = 22.78 
2025-04-08 12:46:20.424426: Steps 86/138: batch_recall = 55.08, batch_ndcg = 34.78 
2025-04-08 12:46:21.698920: Steps 87/138: batch_recall = 46.34, batch_ndcg = 30.93 
2025-04-08 12:46:22.983734: Steps 88/138: batch_recall = 42.98, batch_ndcg = 27.43 
2025-04-08 12:46:24.254685: Steps 89/138: batch_recall = 52.90, batch_ndcg = 31.09 
2025-04-08 12:46:25.517197: Steps 90/138: batch_recall = 35.74, batch_ndcg = 22.00 
2025-04-08 12:46:26.805454: Steps 91/138: batch_recall = 58.06, batch_ndcg = 35.16 
2025-04-08 12:46:28.116548: Steps 92/138: batch_recall = 52.23, batch_ndcg = 32.04 
2025-04-08 12:46:29.426347: Steps 93/138: batch_recall = 52.36, batch_ndcg = 29.78 
2025-04-08 12:46:30.754280: Steps 94/138: batch_recall = 53.23, batch_ndcg = 29.33 
2025-04-08 12:46:32.070453: Steps 95/138: batch_recall = 56.48, batch_ndcg = 33.08 
2025-04-08 12:46:33.395411: Steps 96/138: batch_recall = 57.19, batch_ndcg = 36.01 
2025-04-08 12:46:34.690866: Steps 97/138: batch_recall = 49.40, batch_ndcg = 31.42 
2025-04-08 12:46:35.977439: Steps 98/138: batch_recall = 38.86, batch_ndcg = 24.10 
2025-04-08 12:46:37.270784: Steps 99/138: batch_recall = 44.33, batch_ndcg = 26.93 
2025-04-08 12:46:38.579952: Steps 100/138: batch_recall = 60.26, batch_ndcg = 35.47 
2025-04-08 12:46:39.874961: Steps 101/138: batch_recall = 54.66, batch_ndcg = 31.65 
2025-04-08 12:46:41.209502: Steps 102/138: batch_recall = 57.30, batch_ndcg = 34.56 
2025-04-08 12:46:42.524104: Steps 103/138: batch_recall = 63.48, batch_ndcg = 33.59 
2025-04-08 12:46:43.788181: Steps 104/138: batch_recall = 49.55, batch_ndcg = 25.33 
2025-04-08 12:46:45.063382: Steps 105/138: batch_recall = 57.06, batch_ndcg = 35.68 
2025-04-08 12:46:46.349013: Steps 106/138: batch_recall = 44.40, batch_ndcg = 25.46 
2025-04-08 12:46:47.667462: Steps 107/138: batch_recall = 47.06, batch_ndcg = 24.72 
2025-04-08 12:46:48.948019: Steps 108/138: batch_recall = 50.31, batch_ndcg = 32.03 
2025-04-08 12:46:50.226003: Steps 109/138: batch_recall = 57.58, batch_ndcg = 29.96 
2025-04-08 12:46:51.492706: Steps 110/138: batch_recall = 47.72, batch_ndcg = 27.23 
2025-04-08 12:46:52.795360: Steps 111/138: batch_recall = 60.72, batch_ndcg = 36.18 
2025-04-08 12:46:54.076245: Steps 112/138: batch_recall = 49.61, batch_ndcg = 31.12 
2025-04-08 12:46:55.370123: Steps 113/138: batch_recall = 40.85, batch_ndcg = 26.28 
2025-04-08 12:46:56.631680: Steps 114/138: batch_recall = 50.90, batch_ndcg = 32.00 
2025-04-08 12:46:57.888707: Steps 115/138: batch_recall = 44.54, batch_ndcg = 23.16 
2025-04-08 12:46:59.150019: Steps 116/138: batch_recall = 50.63, batch_ndcg = 24.46 
2025-04-08 12:47:00.418565: Steps 117/138: batch_recall = 53.05, batch_ndcg = 29.92 
2025-04-08 12:47:01.691431: Steps 118/138: batch_recall = 51.66, batch_ndcg = 28.36 
2025-04-08 12:47:02.995567: Steps 119/138: batch_recall = 47.47, batch_ndcg = 27.57 
2025-04-08 12:47:04.280731: Steps 120/138: batch_recall = 49.33, batch_ndcg = 27.68 
2025-04-08 12:47:05.560326: Steps 121/138: batch_recall = 47.10, batch_ndcg = 27.60 
2025-04-08 12:47:06.824480: Steps 122/138: batch_recall = 35.23, batch_ndcg = 21.17 
2025-04-08 12:47:08.108191: Steps 123/138: batch_recall = 42.51, batch_ndcg = 28.78 
2025-04-08 12:47:09.377833: Steps 124/138: batch_recall = 59.02, batch_ndcg = 36.07 
2025-04-08 12:47:10.630931: Steps 125/138: batch_recall = 39.73, batch_ndcg = 25.77 
2025-04-08 12:47:11.889950: Steps 126/138: batch_recall = 54.66, batch_ndcg = 30.44 
2025-04-08 12:47:13.150924: Steps 127/138: batch_recall = 42.10, batch_ndcg = 27.12 
2025-04-08 12:47:14.407946: Steps 128/138: batch_recall = 41.18, batch_ndcg = 25.98 
2025-04-08 12:47:15.691695: Steps 129/138: batch_recall = 54.45, batch_ndcg = 32.78 
2025-04-08 12:47:16.966686: Steps 130/138: batch_recall = 39.05, batch_ndcg = 26.55 
2025-04-08 12:47:18.252559: Steps 131/138: batch_recall = 50.09, batch_ndcg = 33.97 
2025-04-08 12:47:19.529661: Steps 132/138: batch_recall = 44.64, batch_ndcg = 30.32 
2025-04-08 12:47:20.796142: Steps 133/138: batch_recall = 41.81, batch_ndcg = 28.07 
2025-04-08 12:47:22.062370: Steps 134/138: batch_recall = 49.03, batch_ndcg = 30.15 
2025-04-08 12:47:23.326035: Steps 135/138: batch_recall = 53.96, batch_ndcg = 34.63 
2025-04-08 12:47:24.593356: Steps 136/138: batch_recall = 46.22, batch_ndcg = 27.31 
2025-04-08 12:47:25.857059: Steps 137/138: batch_recall = 51.80, batch_ndcg = 37.93 
2025-04-08 12:47:25.857583: Epoch 0/1000, Test: Recall = 0.0631, NDCG = 0.0398  

2025-04-08 12:47:27.612417: Training Step 0/354: batchLoss = 0.6136, diffLoss = 3.0194, kgLoss = 0.0122
2025-04-08 12:47:29.229105: Training Step 1/354: batchLoss = 0.5925, diffLoss = 2.9057, kgLoss = 0.0142
2025-04-08 12:47:30.844329: Training Step 2/354: batchLoss = 0.6302, diffLoss = 3.0906, kgLoss = 0.0151
2025-04-08 12:47:32.460701: Training Step 3/354: batchLoss = 0.7366, diffLoss = 3.6273, kgLoss = 0.0140
2025-04-08 12:47:34.092048: Training Step 4/354: batchLoss = 0.6849, diffLoss = 3.3618, kgLoss = 0.0157
2025-04-08 12:47:35.719044: Training Step 5/354: batchLoss = 0.6791, diffLoss = 3.3343, kgLoss = 0.0152
2025-04-08 12:47:37.350355: Training Step 6/354: batchLoss = 0.6426, diffLoss = 3.1568, kgLoss = 0.0140
2025-04-08 12:47:38.969832: Training Step 7/354: batchLoss = 0.7212, diffLoss = 3.5346, kgLoss = 0.0178
2025-04-08 12:47:40.580338: Training Step 8/354: batchLoss = 0.8509, diffLoss = 4.1833, kgLoss = 0.0178
2025-04-08 12:47:42.192782: Training Step 9/354: batchLoss = 0.4555, diffLoss = 2.2323, kgLoss = 0.0113
2025-04-08 12:47:43.808299: Training Step 10/354: batchLoss = 0.6020, diffLoss = 2.9531, kgLoss = 0.0142
2025-04-08 12:47:45.428535: Training Step 11/354: batchLoss = 0.6801, diffLoss = 3.3437, kgLoss = 0.0142
2025-04-08 12:47:47.061210: Training Step 12/354: batchLoss = 0.5722, diffLoss = 2.8038, kgLoss = 0.0143
2025-04-08 12:47:48.687267: Training Step 13/354: batchLoss = 0.4671, diffLoss = 2.2932, kgLoss = 0.0105
2025-04-08 12:47:50.317996: Training Step 14/354: batchLoss = 0.6303, diffLoss = 3.0981, kgLoss = 0.0134
2025-04-08 12:47:51.947314: Training Step 15/354: batchLoss = 0.6252, diffLoss = 3.0730, kgLoss = 0.0132
2025-04-08 12:47:53.567640: Training Step 16/354: batchLoss = 0.6112, diffLoss = 2.9896, kgLoss = 0.0166
2025-04-08 12:47:55.179635: Training Step 17/354: batchLoss = 0.6383, diffLoss = 3.1330, kgLoss = 0.0146
2025-04-08 12:47:56.793754: Training Step 18/354: batchLoss = 0.6774, diffLoss = 3.3257, kgLoss = 0.0153
2025-04-08 12:47:58.403376: Training Step 19/354: batchLoss = 0.6217, diffLoss = 3.0548, kgLoss = 0.0134
2025-04-08 12:48:00.019070: Training Step 20/354: batchLoss = 0.6692, diffLoss = 3.2827, kgLoss = 0.0158
2025-04-08 12:48:01.639896: Training Step 21/354: batchLoss = 0.6335, diffLoss = 3.1108, kgLoss = 0.0142
2025-04-08 12:48:03.256952: Training Step 22/354: batchLoss = 0.6468, diffLoss = 3.1744, kgLoss = 0.0150
2025-04-08 12:48:04.870253: Training Step 23/354: batchLoss = 0.5936, diffLoss = 2.9139, kgLoss = 0.0136
2025-04-08 12:48:06.493348: Training Step 24/354: batchLoss = 0.5811, diffLoss = 2.8591, kgLoss = 0.0117
2025-04-08 12:48:08.110315: Training Step 25/354: batchLoss = 0.6553, diffLoss = 3.2241, kgLoss = 0.0130
2025-04-08 12:48:09.733057: Training Step 26/354: batchLoss = 0.6124, diffLoss = 3.0125, kgLoss = 0.0124
2025-04-08 12:48:11.338894: Training Step 27/354: batchLoss = 0.8441, diffLoss = 4.1397, kgLoss = 0.0202
2025-04-08 12:48:12.952659: Training Step 28/354: batchLoss = 0.6344, diffLoss = 3.1083, kgLoss = 0.0160
2025-04-08 12:48:14.578542: Training Step 29/354: batchLoss = 0.6991, diffLoss = 3.4408, kgLoss = 0.0136
2025-04-08 12:48:16.202254: Training Step 30/354: batchLoss = 0.6587, diffLoss = 3.2343, kgLoss = 0.0148
2025-04-08 12:48:17.836779: Training Step 31/354: batchLoss = 0.7078, diffLoss = 3.4781, kgLoss = 0.0152
2025-04-08 12:48:19.459092: Training Step 32/354: batchLoss = 0.5906, diffLoss = 2.9053, kgLoss = 0.0120
2025-04-08 12:48:21.081984: Training Step 33/354: batchLoss = 0.7306, diffLoss = 3.5898, kgLoss = 0.0158
2025-04-08 12:48:22.698707: Training Step 34/354: batchLoss = 0.6134, diffLoss = 3.0154, kgLoss = 0.0130
2025-04-08 12:48:24.318912: Training Step 35/354: batchLoss = 0.5838, diffLoss = 2.8670, kgLoss = 0.0130
2025-04-08 12:48:25.925469: Training Step 36/354: batchLoss = 0.6454, diffLoss = 3.1697, kgLoss = 0.0144
2025-04-08 12:48:27.535052: Training Step 37/354: batchLoss = 0.5767, diffLoss = 2.8325, kgLoss = 0.0128
2025-04-08 12:48:29.153257: Training Step 38/354: batchLoss = 0.6446, diffLoss = 3.1684, kgLoss = 0.0136
2025-04-08 12:48:30.769418: Training Step 39/354: batchLoss = 0.6056, diffLoss = 2.9785, kgLoss = 0.0124
2025-04-08 12:48:32.396571: Training Step 40/354: batchLoss = 0.7479, diffLoss = 3.6784, kgLoss = 0.0152
2025-04-08 12:48:34.013948: Training Step 41/354: batchLoss = 0.8137, diffLoss = 3.9974, kgLoss = 0.0178
2025-04-08 12:48:35.631582: Training Step 42/354: batchLoss = 0.6040, diffLoss = 2.9630, kgLoss = 0.0142
2025-04-08 12:48:37.254365: Training Step 43/354: batchLoss = 0.7181, diffLoss = 3.5273, kgLoss = 0.0158
2025-04-08 12:48:38.876758: Training Step 44/354: batchLoss = 0.5887, diffLoss = 2.8838, kgLoss = 0.0150
2025-04-08 12:48:40.488732: Training Step 45/354: batchLoss = 0.7900, diffLoss = 3.8763, kgLoss = 0.0185
2025-04-08 12:48:42.099073: Training Step 46/354: batchLoss = 0.6040, diffLoss = 2.9656, kgLoss = 0.0136
2025-04-08 12:48:43.723211: Training Step 47/354: batchLoss = 0.6402, diffLoss = 3.1387, kgLoss = 0.0155
2025-04-08 12:48:45.342570: Training Step 48/354: batchLoss = 0.5342, diffLoss = 2.6186, kgLoss = 0.0132
2025-04-08 12:48:46.965405: Training Step 49/354: batchLoss = 0.6353, diffLoss = 3.1148, kgLoss = 0.0154
2025-04-08 12:48:48.588431: Training Step 50/354: batchLoss = 0.5877, diffLoss = 2.8851, kgLoss = 0.0134
2025-04-08 12:48:50.208508: Training Step 51/354: batchLoss = 0.9762, diffLoss = 4.8000, kgLoss = 0.0203
2025-04-08 12:48:51.833989: Training Step 52/354: batchLoss = 0.9166, diffLoss = 4.5061, kgLoss = 0.0192
2025-04-08 12:48:53.452261: Training Step 53/354: batchLoss = 0.6314, diffLoss = 3.1064, kgLoss = 0.0127
2025-04-08 12:48:55.071818: Training Step 54/354: batchLoss = 0.6698, diffLoss = 3.2964, kgLoss = 0.0131
2025-04-08 12:48:56.684110: Training Step 55/354: batchLoss = 0.6836, diffLoss = 3.3595, kgLoss = 0.0146
2025-04-08 12:48:58.301234: Training Step 56/354: batchLoss = 0.7009, diffLoss = 3.4451, kgLoss = 0.0148
2025-04-08 12:48:59.917144: Training Step 57/354: batchLoss = 0.5751, diffLoss = 2.8250, kgLoss = 0.0126
2025-04-08 12:49:01.540254: Training Step 58/354: batchLoss = 0.7007, diffLoss = 3.4481, kgLoss = 0.0139
2025-04-08 12:49:03.159994: Training Step 59/354: batchLoss = 0.7461, diffLoss = 3.6700, kgLoss = 0.0151
2025-04-08 12:49:04.780552: Training Step 60/354: batchLoss = 0.6680, diffLoss = 3.2791, kgLoss = 0.0152
2025-04-08 12:49:06.402307: Training Step 61/354: batchLoss = 0.5691, diffLoss = 2.7929, kgLoss = 0.0131
2025-04-08 12:49:08.012548: Training Step 62/354: batchLoss = 0.7271, diffLoss = 3.5745, kgLoss = 0.0153
2025-04-08 12:49:09.624079: Training Step 63/354: batchLoss = 0.6432, diffLoss = 3.1614, kgLoss = 0.0136
2025-04-08 12:49:11.238609: Training Step 64/354: batchLoss = 0.7361, diffLoss = 3.6184, kgLoss = 0.0155
2025-04-08 12:49:12.854395: Training Step 65/354: batchLoss = 0.7321, diffLoss = 3.5985, kgLoss = 0.0155
2025-04-08 12:49:14.470365: Training Step 66/354: batchLoss = 0.6218, diffLoss = 3.0532, kgLoss = 0.0139
2025-04-08 12:49:16.086565: Training Step 67/354: batchLoss = 0.5608, diffLoss = 2.7515, kgLoss = 0.0132
2025-04-08 12:49:17.708868: Training Step 68/354: batchLoss = 0.8097, diffLoss = 3.9847, kgLoss = 0.0160
2025-04-08 12:49:19.328344: Training Step 69/354: batchLoss = 0.7916, diffLoss = 3.8971, kgLoss = 0.0152
2025-04-08 12:49:20.945323: Training Step 70/354: batchLoss = 0.6260, diffLoss = 3.0772, kgLoss = 0.0132
2025-04-08 12:49:22.560859: Training Step 71/354: batchLoss = 0.7327, diffLoss = 3.5962, kgLoss = 0.0169
2025-04-08 12:49:24.173026: Training Step 72/354: batchLoss = 0.6846, diffLoss = 3.3647, kgLoss = 0.0145
2025-04-08 12:49:25.789351: Training Step 73/354: batchLoss = 0.6972, diffLoss = 3.4238, kgLoss = 0.0155
2025-04-08 12:49:27.414325: Training Step 74/354: batchLoss = 0.5992, diffLoss = 2.9393, kgLoss = 0.0142
2025-04-08 12:49:29.039459: Training Step 75/354: batchLoss = 0.7429, diffLoss = 3.6574, kgLoss = 0.0143
2025-04-08 12:49:30.666449: Training Step 76/354: batchLoss = 0.7422, diffLoss = 3.6516, kgLoss = 0.0149
2025-04-08 12:49:32.293088: Training Step 77/354: batchLoss = 0.6110, diffLoss = 3.0003, kgLoss = 0.0137
2025-04-08 12:49:33.913358: Training Step 78/354: batchLoss = 0.7179, diffLoss = 3.5285, kgLoss = 0.0152
2025-04-08 12:49:35.527601: Training Step 79/354: batchLoss = 0.8021, diffLoss = 3.9381, kgLoss = 0.0181
2025-04-08 12:49:37.139332: Training Step 80/354: batchLoss = 0.7035, diffLoss = 3.4547, kgLoss = 0.0157
2025-04-08 12:49:38.749784: Training Step 81/354: batchLoss = 0.6917, diffLoss = 3.4018, kgLoss = 0.0142
2025-04-08 12:49:40.360172: Training Step 82/354: batchLoss = 0.5365, diffLoss = 2.6373, kgLoss = 0.0112
2025-04-08 12:49:41.977215: Training Step 83/354: batchLoss = 0.8135, diffLoss = 4.0012, kgLoss = 0.0165
2025-04-08 12:49:43.606564: Training Step 84/354: batchLoss = 0.6866, diffLoss = 3.3707, kgLoss = 0.0156
2025-04-08 12:49:45.231348: Training Step 85/354: batchLoss = 0.6348, diffLoss = 3.1237, kgLoss = 0.0125
2025-04-08 12:49:46.862601: Training Step 86/354: batchLoss = 0.6215, diffLoss = 3.0584, kgLoss = 0.0122
2025-04-08 12:49:48.482626: Training Step 87/354: batchLoss = 0.5234, diffLoss = 2.5711, kgLoss = 0.0115
2025-04-08 12:49:50.111324: Training Step 88/354: batchLoss = 0.6444, diffLoss = 3.1681, kgLoss = 0.0135
2025-04-08 12:49:51.722826: Training Step 89/354: batchLoss = 0.6326, diffLoss = 3.1067, kgLoss = 0.0140
2025-04-08 12:49:53.331886: Training Step 90/354: batchLoss = 0.6366, diffLoss = 3.1247, kgLoss = 0.0146
2025-04-08 12:49:54.951160: Training Step 91/354: batchLoss = 0.5823, diffLoss = 2.8585, kgLoss = 0.0132
2025-04-08 12:49:56.577019: Training Step 92/354: batchLoss = 0.5734, diffLoss = 2.8133, kgLoss = 0.0135
2025-04-08 12:49:58.200950: Training Step 93/354: batchLoss = 0.6288, diffLoss = 3.0916, kgLoss = 0.0131
2025-04-08 12:49:59.819502: Training Step 94/354: batchLoss = 0.6735, diffLoss = 3.2970, kgLoss = 0.0177
2025-04-08 12:50:01.448267: Training Step 95/354: batchLoss = 0.6739, diffLoss = 3.3042, kgLoss = 0.0164
2025-04-08 12:50:03.071637: Training Step 96/354: batchLoss = 0.8189, diffLoss = 4.0305, kgLoss = 0.0160
2025-04-08 12:50:04.693131: Training Step 97/354: batchLoss = 0.8031, diffLoss = 3.9478, kgLoss = 0.0169
2025-04-08 12:50:06.321635: Training Step 98/354: batchLoss = 0.8496, diffLoss = 4.1781, kgLoss = 0.0174
2025-04-08 12:50:07.919726: Training Step 99/354: batchLoss = 0.5170, diffLoss = 2.5308, kgLoss = 0.0136
2025-04-08 12:50:09.548594: Training Step 100/354: batchLoss = 0.5883, diffLoss = 2.8774, kgLoss = 0.0161
2025-04-08 12:50:11.166013: Training Step 101/354: batchLoss = 0.6349, diffLoss = 3.1227, kgLoss = 0.0130
2025-04-08 12:50:12.785556: Training Step 102/354: batchLoss = 0.6039, diffLoss = 2.9681, kgLoss = 0.0128
2025-04-08 12:50:14.407551: Training Step 103/354: batchLoss = 0.6629, diffLoss = 3.2581, kgLoss = 0.0141
2025-04-08 12:50:16.025964: Training Step 104/354: batchLoss = 0.6920, diffLoss = 3.4005, kgLoss = 0.0148
2025-04-08 12:50:17.650835: Training Step 105/354: batchLoss = 0.7501, diffLoss = 3.6929, kgLoss = 0.0144
2025-04-08 12:50:19.271714: Training Step 106/354: batchLoss = 0.7415, diffLoss = 3.6398, kgLoss = 0.0169
2025-04-08 12:50:20.904033: Training Step 107/354: batchLoss = 0.5802, diffLoss = 2.8404, kgLoss = 0.0152
2025-04-08 12:50:22.525825: Training Step 108/354: batchLoss = 0.7193, diffLoss = 3.5414, kgLoss = 0.0138
2025-04-08 12:50:24.148820: Training Step 109/354: batchLoss = 0.6829, diffLoss = 3.3482, kgLoss = 0.0166
2025-04-08 12:50:25.769590: Training Step 110/354: batchLoss = 0.6340, diffLoss = 3.1144, kgLoss = 0.0139
2025-04-08 12:50:27.389861: Training Step 111/354: batchLoss = 0.6173, diffLoss = 3.0287, kgLoss = 0.0145
2025-04-08 12:50:29.011275: Training Step 112/354: batchLoss = 0.6192, diffLoss = 3.0410, kgLoss = 0.0137
2025-04-08 12:50:30.630435: Training Step 113/354: batchLoss = 0.6609, diffLoss = 3.2369, kgLoss = 0.0169
2025-04-08 12:50:32.251783: Training Step 114/354: batchLoss = 0.6612, diffLoss = 3.2530, kgLoss = 0.0133
2025-04-08 12:50:33.874525: Training Step 115/354: batchLoss = 0.7205, diffLoss = 3.5406, kgLoss = 0.0154
2025-04-08 12:50:35.493628: Training Step 116/354: batchLoss = 0.6986, diffLoss = 3.4369, kgLoss = 0.0140
2025-04-08 12:50:37.100230: Training Step 117/354: batchLoss = 0.6300, diffLoss = 3.0936, kgLoss = 0.0141
2025-04-08 12:50:38.708323: Training Step 118/354: batchLoss = 0.5488, diffLoss = 2.6960, kgLoss = 0.0121
2025-04-08 12:50:40.317492: Training Step 119/354: batchLoss = 0.5628, diffLoss = 2.7680, kgLoss = 0.0115
2025-04-08 12:50:41.937528: Training Step 120/354: batchLoss = 0.6127, diffLoss = 3.0123, kgLoss = 0.0128
2025-04-08 12:50:43.566563: Training Step 121/354: batchLoss = 0.6643, diffLoss = 3.2684, kgLoss = 0.0133
2025-04-08 12:50:45.185533: Training Step 122/354: batchLoss = 0.6039, diffLoss = 2.9610, kgLoss = 0.0146
2025-04-08 12:50:46.804678: Training Step 123/354: batchLoss = 0.7690, diffLoss = 3.7808, kgLoss = 0.0160
2025-04-08 12:50:48.420664: Training Step 124/354: batchLoss = 0.6836, diffLoss = 3.3606, kgLoss = 0.0144
2025-04-08 12:50:50.039119: Training Step 125/354: batchLoss = 0.7873, diffLoss = 3.8740, kgLoss = 0.0157
2025-04-08 12:50:51.643873: Training Step 126/354: batchLoss = 0.6959, diffLoss = 3.4217, kgLoss = 0.0144
2025-04-08 12:50:53.256747: Training Step 127/354: batchLoss = 0.9912, diffLoss = 4.8790, kgLoss = 0.0193
2025-04-08 12:50:54.871128: Training Step 128/354: batchLoss = 0.5779, diffLoss = 2.8444, kgLoss = 0.0113
2025-04-08 12:50:56.491934: Training Step 129/354: batchLoss = 0.7157, diffLoss = 3.5134, kgLoss = 0.0162
2025-04-08 12:50:58.107361: Training Step 130/354: batchLoss = 0.6168, diffLoss = 3.0268, kgLoss = 0.0142
2025-04-08 12:50:59.726532: Training Step 131/354: batchLoss = 0.5640, diffLoss = 2.7731, kgLoss = 0.0118
2025-04-08 12:51:01.356743: Training Step 132/354: batchLoss = 0.7543, diffLoss = 3.7093, kgLoss = 0.0155
2025-04-08 12:51:02.977297: Training Step 133/354: batchLoss = 0.5719, diffLoss = 2.8069, kgLoss = 0.0132
2025-04-08 12:51:04.607340: Training Step 134/354: batchLoss = 0.6929, diffLoss = 3.4031, kgLoss = 0.0153
2025-04-08 12:51:06.237069: Training Step 135/354: batchLoss = 0.6805, diffLoss = 3.3382, kgLoss = 0.0161
2025-04-08 12:51:07.861859: Training Step 136/354: batchLoss = 0.8485, diffLoss = 4.1696, kgLoss = 0.0182
2025-04-08 12:51:09.494462: Training Step 137/354: batchLoss = 0.6578, diffLoss = 3.2345, kgLoss = 0.0137
2025-04-08 12:51:11.120000: Training Step 138/354: batchLoss = 0.6053, diffLoss = 2.9655, kgLoss = 0.0152
2025-04-08 12:51:12.744849: Training Step 139/354: batchLoss = 0.6841, diffLoss = 3.3602, kgLoss = 0.0151
2025-04-08 12:51:14.371748: Training Step 140/354: batchLoss = 0.6174, diffLoss = 3.0327, kgLoss = 0.0136
2025-04-08 12:51:16.003062: Training Step 141/354: batchLoss = 0.6402, diffLoss = 3.1490, kgLoss = 0.0130
2025-04-08 12:51:17.632863: Training Step 142/354: batchLoss = 0.6353, diffLoss = 3.1242, kgLoss = 0.0131
2025-04-08 12:51:19.268362: Training Step 143/354: batchLoss = 0.6379, diffLoss = 3.1351, kgLoss = 0.0137
2025-04-08 12:51:20.914046: Training Step 144/354: batchLoss = 0.6576, diffLoss = 3.2319, kgLoss = 0.0140
2025-04-08 12:51:22.551951: Training Step 145/354: batchLoss = 0.5956, diffLoss = 2.9185, kgLoss = 0.0148
2025-04-08 12:51:24.162199: Training Step 146/354: batchLoss = 0.6549, diffLoss = 3.2170, kgLoss = 0.0143
2025-04-08 12:51:25.785366: Training Step 147/354: batchLoss = 0.6326, diffLoss = 3.1142, kgLoss = 0.0122
2025-04-08 12:51:27.418052: Training Step 148/354: batchLoss = 0.7188, diffLoss = 3.5281, kgLoss = 0.0165
2025-04-08 12:51:29.033629: Training Step 149/354: batchLoss = 0.6214, diffLoss = 3.0438, kgLoss = 0.0158
2025-04-08 12:51:30.674948: Training Step 150/354: batchLoss = 0.8503, diffLoss = 4.1837, kgLoss = 0.0169
2025-04-08 12:51:32.293224: Training Step 151/354: batchLoss = 0.6326, diffLoss = 3.1061, kgLoss = 0.0143
2025-04-08 12:51:33.913480: Training Step 152/354: batchLoss = 0.5946, diffLoss = 2.9246, kgLoss = 0.0121
2025-04-08 12:51:35.525081: Training Step 153/354: batchLoss = 0.5654, diffLoss = 2.7772, kgLoss = 0.0125
2025-04-08 12:51:37.135611: Training Step 154/354: batchLoss = 0.4804, diffLoss = 2.3551, kgLoss = 0.0118
2025-04-08 12:51:38.752942: Training Step 155/354: batchLoss = 0.5402, diffLoss = 2.6344, kgLoss = 0.0166
2025-04-08 12:51:40.368043: Training Step 156/354: batchLoss = 0.7086, diffLoss = 3.4844, kgLoss = 0.0147
2025-04-08 12:51:42.000487: Training Step 157/354: batchLoss = 0.6780, diffLoss = 3.3392, kgLoss = 0.0127
2025-04-08 12:51:43.626248: Training Step 158/354: batchLoss = 0.5114, diffLoss = 2.5107, kgLoss = 0.0116
2025-04-08 12:51:45.247719: Training Step 159/354: batchLoss = 0.8177, diffLoss = 4.0209, kgLoss = 0.0169
2025-04-08 12:51:46.874291: Training Step 160/354: batchLoss = 0.6437, diffLoss = 3.1676, kgLoss = 0.0127
2025-04-08 12:51:48.501233: Training Step 161/354: batchLoss = 0.7078, diffLoss = 3.4802, kgLoss = 0.0148
2025-04-08 12:51:50.123967: Training Step 162/354: batchLoss = 0.8344, diffLoss = 4.1022, kgLoss = 0.0174
2025-04-08 12:51:51.739323: Training Step 163/354: batchLoss = 0.8167, diffLoss = 4.0137, kgLoss = 0.0175
2025-04-08 12:51:53.355402: Training Step 164/354: batchLoss = 0.6171, diffLoss = 3.0375, kgLoss = 0.0120
2025-04-08 12:51:54.973161: Training Step 165/354: batchLoss = 0.7046, diffLoss = 3.4632, kgLoss = 0.0150
2025-04-08 12:51:56.607621: Training Step 166/354: batchLoss = 0.7743, diffLoss = 3.8100, kgLoss = 0.0154
2025-04-08 12:51:58.226895: Training Step 167/354: batchLoss = 0.6277, diffLoss = 3.0791, kgLoss = 0.0148
2025-04-08 12:51:59.841648: Training Step 168/354: batchLoss = 0.8863, diffLoss = 4.3620, kgLoss = 0.0173
2025-04-08 12:52:01.462207: Training Step 169/354: batchLoss = 0.7991, diffLoss = 3.9324, kgLoss = 0.0158
2025-04-08 12:52:03.083231: Training Step 170/354: batchLoss = 0.5210, diffLoss = 2.5545, kgLoss = 0.0126
2025-04-08 12:52:04.696586: Training Step 171/354: batchLoss = 0.5952, diffLoss = 2.9254, kgLoss = 0.0126
2025-04-08 12:52:06.307214: Training Step 172/354: batchLoss = 0.4927, diffLoss = 2.4144, kgLoss = 0.0122
2025-04-08 12:52:07.919028: Training Step 173/354: batchLoss = 0.6258, diffLoss = 3.0772, kgLoss = 0.0129
2025-04-08 12:52:09.535454: Training Step 174/354: batchLoss = 0.8474, diffLoss = 4.1637, kgLoss = 0.0184
2025-04-08 12:52:11.165680: Training Step 175/354: batchLoss = 0.6114, diffLoss = 2.9948, kgLoss = 0.0156
2025-04-08 12:52:12.787260: Training Step 176/354: batchLoss = 0.6400, diffLoss = 3.1475, kgLoss = 0.0131
2025-04-08 12:52:14.412514: Training Step 177/354: batchLoss = 0.5350, diffLoss = 2.6255, kgLoss = 0.0124
2025-04-08 12:52:16.034701: Training Step 178/354: batchLoss = 0.7012, diffLoss = 3.4446, kgLoss = 0.0154
2025-04-08 12:52:17.667724: Training Step 179/354: batchLoss = 0.5514, diffLoss = 2.7085, kgLoss = 0.0121
2025-04-08 12:52:19.285656: Training Step 180/354: batchLoss = 0.5314, diffLoss = 2.6130, kgLoss = 0.0110
2025-04-08 12:52:20.904446: Training Step 181/354: batchLoss = 0.7420, diffLoss = 3.6518, kgLoss = 0.0146
2025-04-08 12:52:22.525610: Training Step 182/354: batchLoss = 0.5725, diffLoss = 2.8133, kgLoss = 0.0123
2025-04-08 12:52:24.151556: Training Step 183/354: batchLoss = 0.7453, diffLoss = 3.6543, kgLoss = 0.0181
2025-04-08 12:52:25.781955: Training Step 184/354: batchLoss = 0.6307, diffLoss = 3.1033, kgLoss = 0.0125
2025-04-08 12:52:27.404460: Training Step 185/354: batchLoss = 0.6657, diffLoss = 3.2725, kgLoss = 0.0140
2025-04-08 12:52:29.019533: Training Step 186/354: batchLoss = 0.7391, diffLoss = 3.6267, kgLoss = 0.0172
2025-04-08 12:52:30.641627: Training Step 187/354: batchLoss = 0.6694, diffLoss = 3.2843, kgLoss = 0.0157
2025-04-08 12:52:32.269907: Training Step 188/354: batchLoss = 0.6425, diffLoss = 3.1528, kgLoss = 0.0149
2025-04-08 12:52:33.882571: Training Step 189/354: batchLoss = 0.6311, diffLoss = 3.0961, kgLoss = 0.0149
2025-04-08 12:52:35.491348: Training Step 190/354: batchLoss = 0.8416, diffLoss = 4.1326, kgLoss = 0.0188
2025-04-08 12:52:37.106277: Training Step 191/354: batchLoss = 0.6709, diffLoss = 3.2995, kgLoss = 0.0137
2025-04-08 12:52:38.736241: Training Step 192/354: batchLoss = 0.7591, diffLoss = 3.7293, kgLoss = 0.0165
2025-04-08 12:52:40.357464: Training Step 193/354: batchLoss = 0.6428, diffLoss = 3.1621, kgLoss = 0.0130
2025-04-08 12:52:41.987773: Training Step 194/354: batchLoss = 0.6489, diffLoss = 3.1905, kgLoss = 0.0136
2025-04-08 12:52:43.605354: Training Step 195/354: batchLoss = 0.6723, diffLoss = 3.3058, kgLoss = 0.0140
2025-04-08 12:52:45.229651: Training Step 196/354: batchLoss = 0.5444, diffLoss = 2.6729, kgLoss = 0.0122
2025-04-08 12:52:46.851392: Training Step 197/354: batchLoss = 0.6307, diffLoss = 3.1043, kgLoss = 0.0123
2025-04-08 12:52:48.458810: Training Step 198/354: batchLoss = 0.7484, diffLoss = 3.6770, kgLoss = 0.0163
2025-04-08 12:52:50.066203: Training Step 199/354: batchLoss = 0.6166, diffLoss = 3.0245, kgLoss = 0.0147
2025-04-08 12:52:51.669391: Training Step 200/354: batchLoss = 0.5531, diffLoss = 2.7038, kgLoss = 0.0154
2025-04-08 12:52:53.274136: Training Step 201/354: batchLoss = 0.5918, diffLoss = 2.9101, kgLoss = 0.0122
2025-04-08 12:52:54.891091: Training Step 202/354: batchLoss = 0.6781, diffLoss = 3.3233, kgLoss = 0.0168
2025-04-08 12:52:56.512868: Training Step 203/354: batchLoss = 0.7028, diffLoss = 3.4528, kgLoss = 0.0153
2025-04-08 12:52:58.149745: Training Step 204/354: batchLoss = 0.5624, diffLoss = 2.7601, kgLoss = 0.0130
2025-04-08 12:52:59.766168: Training Step 205/354: batchLoss = 0.6244, diffLoss = 3.0668, kgLoss = 0.0138
2025-04-08 12:53:01.382814: Training Step 206/354: batchLoss = 0.6114, diffLoss = 2.9961, kgLoss = 0.0152
2025-04-08 12:53:02.994534: Training Step 207/354: batchLoss = 0.5246, diffLoss = 2.5745, kgLoss = 0.0122
2025-04-08 12:53:04.610574: Training Step 208/354: batchLoss = 0.6538, diffLoss = 3.2142, kgLoss = 0.0138
2025-04-08 12:53:06.217149: Training Step 209/354: batchLoss = 0.5128, diffLoss = 2.5195, kgLoss = 0.0112
2025-04-08 12:53:07.839713: Training Step 210/354: batchLoss = 0.7393, diffLoss = 3.6311, kgLoss = 0.0163
2025-04-08 12:53:09.455653: Training Step 211/354: batchLoss = 0.5659, diffLoss = 2.7806, kgLoss = 0.0122
2025-04-08 12:53:11.078576: Training Step 212/354: batchLoss = 0.8149, diffLoss = 4.0073, kgLoss = 0.0168
2025-04-08 12:53:12.696039: Training Step 213/354: batchLoss = 0.5914, diffLoss = 2.9041, kgLoss = 0.0132
2025-04-08 12:53:14.312822: Training Step 214/354: batchLoss = 0.6578, diffLoss = 3.2334, kgLoss = 0.0139
2025-04-08 12:53:15.938014: Training Step 215/354: batchLoss = 0.5560, diffLoss = 2.7306, kgLoss = 0.0124
2025-04-08 12:53:17.553902: Training Step 216/354: batchLoss = 0.6904, diffLoss = 3.3922, kgLoss = 0.0150
2025-04-08 12:53:19.163556: Training Step 217/354: batchLoss = 0.6345, diffLoss = 3.1130, kgLoss = 0.0149
2025-04-08 12:53:20.788041: Training Step 218/354: batchLoss = 0.6908, diffLoss = 3.3914, kgLoss = 0.0157
2025-04-08 12:53:22.399842: Training Step 219/354: batchLoss = 0.8999, diffLoss = 4.4227, kgLoss = 0.0192
2025-04-08 12:53:24.020534: Training Step 220/354: batchLoss = 0.6355, diffLoss = 3.1260, kgLoss = 0.0129
2025-04-08 12:53:25.646375: Training Step 221/354: batchLoss = 0.6496, diffLoss = 3.1928, kgLoss = 0.0138
2025-04-08 12:53:27.271591: Training Step 222/354: batchLoss = 0.5618, diffLoss = 2.7476, kgLoss = 0.0153
2025-04-08 12:53:28.895550: Training Step 223/354: batchLoss = 0.7605, diffLoss = 3.7435, kgLoss = 0.0148
2025-04-08 12:53:30.518983: Training Step 224/354: batchLoss = 0.6440, diffLoss = 3.1559, kgLoss = 0.0160
2025-04-08 12:53:32.144990: Training Step 225/354: batchLoss = 0.6060, diffLoss = 2.9723, kgLoss = 0.0144
2025-04-08 12:53:33.761485: Training Step 226/354: batchLoss = 0.8766, diffLoss = 4.3090, kgLoss = 0.0184
2025-04-08 12:53:35.382502: Training Step 227/354: batchLoss = 0.6033, diffLoss = 2.9684, kgLoss = 0.0121
2025-04-08 12:53:36.999162: Training Step 228/354: batchLoss = 0.7460, diffLoss = 3.6682, kgLoss = 0.0154
2025-04-08 12:53:38.624284: Training Step 229/354: batchLoss = 1.5246, diffLoss = 7.5031, kgLoss = 0.0300
2025-04-08 12:53:40.252308: Training Step 230/354: batchLoss = 0.7032, diffLoss = 3.4570, kgLoss = 0.0148
2025-04-08 12:53:41.877046: Training Step 231/354: batchLoss = 0.8404, diffLoss = 4.1339, kgLoss = 0.0171
2025-04-08 12:53:43.501988: Training Step 232/354: batchLoss = 0.6919, diffLoss = 3.3994, kgLoss = 0.0150
2025-04-08 12:53:45.135605: Training Step 233/354: batchLoss = 0.7375, diffLoss = 3.6278, kgLoss = 0.0149
2025-04-08 12:53:46.755688: Training Step 234/354: batchLoss = 0.5278, diffLoss = 2.5832, kgLoss = 0.0140
2025-04-08 12:53:48.379597: Training Step 235/354: batchLoss = 0.6184, diffLoss = 3.0398, kgLoss = 0.0131
2025-04-08 12:53:49.989550: Training Step 236/354: batchLoss = 0.5991, diffLoss = 2.9355, kgLoss = 0.0150
2025-04-08 12:53:51.589677: Training Step 237/354: batchLoss = 0.5498, diffLoss = 2.6925, kgLoss = 0.0141
2025-04-08 12:53:53.210040: Training Step 238/354: batchLoss = 0.7751, diffLoss = 3.8065, kgLoss = 0.0173
2025-04-08 12:53:54.827555: Training Step 239/354: batchLoss = 0.6893, diffLoss = 3.3859, kgLoss = 0.0152
2025-04-08 12:53:56.447665: Training Step 240/354: batchLoss = 0.8201, diffLoss = 4.0277, kgLoss = 0.0182
2025-04-08 12:53:58.075387: Training Step 241/354: batchLoss = 0.5897, diffLoss = 2.9007, kgLoss = 0.0120
2025-04-08 12:53:59.695602: Training Step 242/354: batchLoss = 0.6752, diffLoss = 3.3195, kgLoss = 0.0142
2025-04-08 12:54:01.310689: Training Step 243/354: batchLoss = 0.6120, diffLoss = 3.0062, kgLoss = 0.0135
2025-04-08 12:54:02.917726: Training Step 244/354: batchLoss = 0.6078, diffLoss = 2.9896, kgLoss = 0.0124
2025-04-08 12:54:04.528877: Training Step 245/354: batchLoss = 0.5668, diffLoss = 2.7805, kgLoss = 0.0134
2025-04-08 12:54:06.144891: Training Step 246/354: batchLoss = 0.6760, diffLoss = 3.3243, kgLoss = 0.0140
2025-04-08 12:54:07.765704: Training Step 247/354: batchLoss = 0.7449, diffLoss = 3.6624, kgLoss = 0.0155
2025-04-08 12:54:09.390486: Training Step 248/354: batchLoss = 0.7433, diffLoss = 3.6501, kgLoss = 0.0166
2025-04-08 12:54:11.007511: Training Step 249/354: batchLoss = 0.8181, diffLoss = 4.0172, kgLoss = 0.0183
2025-04-08 12:54:12.639649: Training Step 250/354: batchLoss = 0.6234, diffLoss = 3.0606, kgLoss = 0.0141
2025-04-08 12:54:14.268896: Training Step 251/354: batchLoss = 0.6636, diffLoss = 3.2550, kgLoss = 0.0158
2025-04-08 12:54:15.898890: Training Step 252/354: batchLoss = 0.8257, diffLoss = 4.0601, kgLoss = 0.0171
2025-04-08 12:54:17.508820: Training Step 253/354: batchLoss = 0.5826, diffLoss = 2.8651, kgLoss = 0.0120
2025-04-08 12:54:19.120194: Training Step 254/354: batchLoss = 0.8390, diffLoss = 4.1289, kgLoss = 0.0165
2025-04-08 12:54:20.732937: Training Step 255/354: batchLoss = 0.6095, diffLoss = 2.9969, kgLoss = 0.0127
2025-04-08 12:54:22.350751: Training Step 256/354: batchLoss = 0.6448, diffLoss = 3.1640, kgLoss = 0.0150
2025-04-08 12:54:23.977090: Training Step 257/354: batchLoss = 0.6734, diffLoss = 3.3091, kgLoss = 0.0144
2025-04-08 12:54:25.600607: Training Step 258/354: batchLoss = 0.5171, diffLoss = 2.5361, kgLoss = 0.0124
2025-04-08 12:54:27.221285: Training Step 259/354: batchLoss = 0.8031, diffLoss = 3.9465, kgLoss = 0.0172
2025-04-08 12:54:28.843102: Training Step 260/354: batchLoss = 0.5943, diffLoss = 2.9219, kgLoss = 0.0124
2025-04-08 12:54:30.465148: Training Step 261/354: batchLoss = 0.5832, diffLoss = 2.8635, kgLoss = 0.0131
2025-04-08 12:54:32.075831: Training Step 262/354: batchLoss = 0.6678, diffLoss = 3.2817, kgLoss = 0.0143
2025-04-08 12:54:33.680944: Training Step 263/354: batchLoss = 0.6333, diffLoss = 3.1160, kgLoss = 0.0126
2025-04-08 12:54:35.289581: Training Step 264/354: batchLoss = 0.6467, diffLoss = 3.1760, kgLoss = 0.0144
2025-04-08 12:54:36.911145: Training Step 265/354: batchLoss = 0.7179, diffLoss = 3.5255, kgLoss = 0.0160
2025-04-08 12:54:38.529939: Training Step 266/354: batchLoss = 0.6031, diffLoss = 2.9635, kgLoss = 0.0130
2025-04-08 12:54:40.150621: Training Step 267/354: batchLoss = 0.6106, diffLoss = 2.9900, kgLoss = 0.0157
2025-04-08 12:54:41.768287: Training Step 268/354: batchLoss = 0.5931, diffLoss = 2.9090, kgLoss = 0.0142
2025-04-08 12:54:43.389410: Training Step 269/354: batchLoss = 0.7214, diffLoss = 3.5530, kgLoss = 0.0135
2025-04-08 12:54:45.017022: Training Step 270/354: batchLoss = 0.5644, diffLoss = 2.7697, kgLoss = 0.0131
2025-04-08 12:54:46.625850: Training Step 271/354: batchLoss = 0.6219, diffLoss = 3.0549, kgLoss = 0.0136
2025-04-08 12:54:48.238594: Training Step 272/354: batchLoss = 0.7632, diffLoss = 3.7519, kgLoss = 0.0160
2025-04-08 12:54:49.855832: Training Step 273/354: batchLoss = 0.6252, diffLoss = 3.0723, kgLoss = 0.0135
2025-04-08 12:54:51.480171: Training Step 274/354: batchLoss = 2.3778, diffLoss = 11.7045, kgLoss = 0.0461
2025-04-08 12:54:53.103421: Training Step 275/354: batchLoss = 0.9336, diffLoss = 4.5919, kgLoss = 0.0190
2025-04-08 12:54:54.728961: Training Step 276/354: batchLoss = 0.7213, diffLoss = 3.5506, kgLoss = 0.0140
2025-04-08 12:54:56.349586: Training Step 277/354: batchLoss = 0.5463, diffLoss = 2.6795, kgLoss = 0.0130
2025-04-08 12:54:57.976737: Training Step 278/354: batchLoss = 0.7828, diffLoss = 3.8438, kgLoss = 0.0175
2025-04-08 12:54:59.598951: Training Step 279/354: batchLoss = 0.6531, diffLoss = 3.1957, kgLoss = 0.0175
2025-04-08 12:55:01.211658: Training Step 280/354: batchLoss = 0.6352, diffLoss = 3.1203, kgLoss = 0.0139
2025-04-08 12:55:02.807805: Training Step 281/354: batchLoss = 0.6270, diffLoss = 3.0792, kgLoss = 0.0139
2025-04-08 12:55:04.417172: Training Step 282/354: batchLoss = 0.5503, diffLoss = 2.7061, kgLoss = 0.0114
2025-04-08 12:55:06.038502: Training Step 283/354: batchLoss = 0.5418, diffLoss = 2.6562, kgLoss = 0.0132
2025-04-08 12:55:07.659479: Training Step 284/354: batchLoss = 0.6477, diffLoss = 3.1860, kgLoss = 0.0132
2025-04-08 12:55:09.284359: Training Step 285/354: batchLoss = 0.6677, diffLoss = 3.2784, kgLoss = 0.0150
2025-04-08 12:55:10.901369: Training Step 286/354: batchLoss = 0.8061, diffLoss = 3.9621, kgLoss = 0.0171
2025-04-08 12:55:12.518475: Training Step 287/354: batchLoss = 0.6158, diffLoss = 3.0208, kgLoss = 0.0146
2025-04-08 12:55:14.145126: Training Step 288/354: batchLoss = 0.6653, diffLoss = 3.2578, kgLoss = 0.0172
2025-04-08 12:55:15.752437: Training Step 289/354: batchLoss = 0.5790, diffLoss = 2.8434, kgLoss = 0.0129
2025-04-08 12:55:17.368502: Training Step 290/354: batchLoss = 0.7131, diffLoss = 3.5099, kgLoss = 0.0140
2025-04-08 12:55:18.976421: Training Step 291/354: batchLoss = 0.5983, diffLoss = 2.9377, kgLoss = 0.0135
2025-04-08 12:55:20.592784: Training Step 292/354: batchLoss = 0.5878, diffLoss = 2.8846, kgLoss = 0.0136
2025-04-08 12:55:22.211915: Training Step 293/354: batchLoss = 0.6210, diffLoss = 3.0441, kgLoss = 0.0152
2025-04-08 12:55:23.829465: Training Step 294/354: batchLoss = 0.6505, diffLoss = 3.1937, kgLoss = 0.0147
2025-04-08 12:55:25.457623: Training Step 295/354: batchLoss = 0.6257, diffLoss = 3.0737, kgLoss = 0.0137
2025-04-08 12:55:27.077686: Training Step 296/354: batchLoss = 0.6961, diffLoss = 3.4232, kgLoss = 0.0144
2025-04-08 12:55:28.696586: Training Step 297/354: batchLoss = 0.7198, diffLoss = 3.5325, kgLoss = 0.0166
2025-04-08 12:55:30.306459: Training Step 298/354: batchLoss = 0.5866, diffLoss = 2.8835, kgLoss = 0.0124
2025-04-08 12:55:31.924342: Training Step 299/354: batchLoss = 0.6370, diffLoss = 3.1298, kgLoss = 0.0138
2025-04-08 12:55:33.528584: Training Step 300/354: batchLoss = 0.6402, diffLoss = 3.1395, kgLoss = 0.0154
2025-04-08 12:55:35.144101: Training Step 301/354: batchLoss = 0.6896, diffLoss = 3.3858, kgLoss = 0.0155
2025-04-08 12:55:36.764317: Training Step 302/354: batchLoss = 0.6424, diffLoss = 3.1533, kgLoss = 0.0146
2025-04-08 12:55:38.386943: Training Step 303/354: batchLoss = 0.6499, diffLoss = 3.1972, kgLoss = 0.0130
2025-04-08 12:55:40.019611: Training Step 304/354: batchLoss = 0.7653, diffLoss = 3.7610, kgLoss = 0.0164
2025-04-08 12:55:41.656267: Training Step 305/354: batchLoss = 0.5555, diffLoss = 2.7291, kgLoss = 0.0121
2025-04-08 12:55:43.292421: Training Step 306/354: batchLoss = 0.5913, diffLoss = 2.9063, kgLoss = 0.0125
2025-04-08 12:55:44.931137: Training Step 307/354: batchLoss = 0.6528, diffLoss = 3.2025, kgLoss = 0.0153
2025-04-08 12:55:46.566781: Training Step 308/354: batchLoss = 0.7097, diffLoss = 3.4958, kgLoss = 0.0132
2025-04-08 12:55:48.187707: Training Step 309/354: batchLoss = 0.6511, diffLoss = 3.1971, kgLoss = 0.0146
2025-04-08 12:55:49.813376: Training Step 310/354: batchLoss = 0.6842, diffLoss = 3.3591, kgLoss = 0.0155
2025-04-08 12:55:51.467891: Training Step 311/354: batchLoss = 0.6031, diffLoss = 2.9626, kgLoss = 0.0133
2025-04-08 12:55:53.117359: Training Step 312/354: batchLoss = 0.6484, diffLoss = 3.1868, kgLoss = 0.0138
2025-04-08 12:55:54.760090: Training Step 313/354: batchLoss = 0.7150, diffLoss = 3.5129, kgLoss = 0.0155
2025-04-08 12:55:56.379066: Training Step 314/354: batchLoss = 0.6924, diffLoss = 3.4005, kgLoss = 0.0154
2025-04-08 12:55:57.999931: Training Step 315/354: batchLoss = 0.5486, diffLoss = 2.6888, kgLoss = 0.0135
2025-04-08 12:55:59.615626: Training Step 316/354: batchLoss = 0.8807, diffLoss = 4.3307, kgLoss = 0.0181
2025-04-08 12:56:01.228379: Training Step 317/354: batchLoss = 0.7310, diffLoss = 3.5878, kgLoss = 0.0168
2025-04-08 12:56:02.840434: Training Step 318/354: batchLoss = 0.5766, diffLoss = 2.8339, kgLoss = 0.0123
2025-04-08 12:56:04.460136: Training Step 319/354: batchLoss = 0.7800, diffLoss = 3.8310, kgLoss = 0.0173
2025-04-08 12:56:06.086387: Training Step 320/354: batchLoss = 0.6833, diffLoss = 3.3586, kgLoss = 0.0145
2025-04-08 12:56:07.714610: Training Step 321/354: batchLoss = 0.7293, diffLoss = 3.5824, kgLoss = 0.0161
2025-04-08 12:56:09.334609: Training Step 322/354: batchLoss = 0.5256, diffLoss = 2.5816, kgLoss = 0.0116
2025-04-08 12:56:10.963214: Training Step 323/354: batchLoss = 0.6077, diffLoss = 2.9885, kgLoss = 0.0125
2025-04-08 12:56:12.591133: Training Step 324/354: batchLoss = 0.6819, diffLoss = 3.3423, kgLoss = 0.0168
2025-04-08 12:56:14.219590: Training Step 325/354: batchLoss = 0.7303, diffLoss = 3.5907, kgLoss = 0.0153
2025-04-08 12:56:15.837607: Training Step 326/354: batchLoss = 0.5984, diffLoss = 2.9404, kgLoss = 0.0129
2025-04-08 12:56:17.451921: Training Step 327/354: batchLoss = 0.6424, diffLoss = 3.1475, kgLoss = 0.0161
2025-04-08 12:56:19.071850: Training Step 328/354: batchLoss = 0.6708, diffLoss = 3.2977, kgLoss = 0.0140
2025-04-08 12:56:20.695682: Training Step 329/354: batchLoss = 0.6775, diffLoss = 3.3239, kgLoss = 0.0159
2025-04-08 12:56:22.335910: Training Step 330/354: batchLoss = 0.7090, diffLoss = 3.4781, kgLoss = 0.0167
2025-04-08 12:56:23.965166: Training Step 331/354: batchLoss = 0.5355, diffLoss = 2.6274, kgLoss = 0.0125
2025-04-08 12:56:25.596875: Training Step 332/354: batchLoss = 0.4775, diffLoss = 2.3415, kgLoss = 0.0115
2025-04-08 12:56:27.220078: Training Step 333/354: batchLoss = 0.6855, diffLoss = 3.3650, kgLoss = 0.0156
2025-04-08 12:56:28.829605: Training Step 334/354: batchLoss = 1.1587, diffLoss = 5.6967, kgLoss = 0.0242
2025-04-08 12:56:30.447817: Training Step 335/354: batchLoss = 0.5781, diffLoss = 2.8304, kgLoss = 0.0151
2025-04-08 12:56:32.069422: Training Step 336/354: batchLoss = 0.6323, diffLoss = 3.1057, kgLoss = 0.0140
2025-04-08 12:56:33.695155: Training Step 337/354: batchLoss = 0.5467, diffLoss = 2.6802, kgLoss = 0.0134
2025-04-08 12:56:35.316461: Training Step 338/354: batchLoss = 0.5649, diffLoss = 2.7708, kgLoss = 0.0135
2025-04-08 12:56:36.938319: Training Step 339/354: batchLoss = 0.6428, diffLoss = 3.1548, kgLoss = 0.0148
2025-04-08 12:56:38.569546: Training Step 340/354: batchLoss = 0.7785, diffLoss = 3.8217, kgLoss = 0.0177
2025-04-08 12:56:40.215892: Training Step 341/354: batchLoss = 0.6906, diffLoss = 3.3920, kgLoss = 0.0152
2025-04-08 12:56:41.835535: Training Step 342/354: batchLoss = 0.6586, diffLoss = 3.2337, kgLoss = 0.0148
2025-04-08 12:56:43.452692: Training Step 343/354: batchLoss = 0.8391, diffLoss = 4.1253, kgLoss = 0.0175
2025-04-08 12:56:45.061360: Training Step 344/354: batchLoss = 0.6037, diffLoss = 2.9661, kgLoss = 0.0131
2025-04-08 12:56:46.683609: Training Step 345/354: batchLoss = 0.7986, diffLoss = 3.9253, kgLoss = 0.0169
2025-04-08 12:56:48.302099: Training Step 346/354: batchLoss = 0.5511, diffLoss = 2.7049, kgLoss = 0.0126
2025-04-08 12:56:49.934547: Training Step 347/354: batchLoss = 0.6340, diffLoss = 3.1059, kgLoss = 0.0160
2025-04-08 12:56:51.555044: Training Step 348/354: batchLoss = 0.5886, diffLoss = 2.8819, kgLoss = 0.0153
2025-04-08 12:56:53.176369: Training Step 349/354: batchLoss = 0.5502, diffLoss = 2.6978, kgLoss = 0.0133
2025-04-08 12:56:54.800010: Training Step 350/354: batchLoss = 0.6070, diffLoss = 2.9780, kgLoss = 0.0142
2025-04-08 12:56:56.414647: Training Step 351/354: batchLoss = 0.8410, diffLoss = 4.1446, kgLoss = 0.0151
2025-04-08 12:56:58.006110: Training Step 352/354: batchLoss = 0.6862, diffLoss = 3.3694, kgLoss = 0.0154
2025-04-08 12:56:59.408392: Training Step 353/354: batchLoss = 0.5735, diffLoss = 2.8126, kgLoss = 0.0137
2025-04-08 12:56:59.492546: 
2025-04-08 12:56:59.493174: Epoch 1/1000, Train: epLoss = 1.1875, epDfLoss = 5.8331, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 12:57:00.791265: Steps 0/138: batch_recall = 26.72, batch_ndcg = 17.80 
2025-04-08 12:57:02.131547: Steps 1/138: batch_recall = 29.65, batch_ndcg = 17.95 
2025-04-08 12:57:03.451321: Steps 2/138: batch_recall = 34.70, batch_ndcg = 23.36 
2025-04-08 12:57:04.784519: Steps 3/138: batch_recall = 37.88, batch_ndcg = 22.76 
2025-04-08 12:57:06.086193: Steps 4/138: batch_recall = 40.44, batch_ndcg = 28.76 
2025-04-08 12:57:07.416167: Steps 5/138: batch_recall = 34.94, batch_ndcg = 19.01 
2025-04-08 12:57:08.733421: Steps 6/138: batch_recall = 28.57, batch_ndcg = 18.34 
2025-04-08 12:57:10.045315: Steps 7/138: batch_recall = 43.42, batch_ndcg = 26.68 
2025-04-08 12:57:11.351081: Steps 8/138: batch_recall = 37.90, batch_ndcg = 23.59 
2025-04-08 12:57:12.650051: Steps 9/138: batch_recall = 27.03, batch_ndcg = 18.02 
2025-04-08 12:57:13.944287: Steps 10/138: batch_recall = 33.22, batch_ndcg = 18.32 
2025-04-08 12:57:15.238067: Steps 11/138: batch_recall = 45.25, batch_ndcg = 26.33 
2025-04-08 12:57:16.538282: Steps 12/138: batch_recall = 31.77, batch_ndcg = 20.06 
2025-04-08 12:57:17.850344: Steps 13/138: batch_recall = 30.12, batch_ndcg = 18.81 
2025-04-08 12:57:19.142996: Steps 14/138: batch_recall = 33.18, batch_ndcg = 20.07 
2025-04-08 12:57:20.464311: Steps 15/138: batch_recall = 33.51, batch_ndcg = 23.30 
2025-04-08 12:57:21.772441: Steps 16/138: batch_recall = 34.15, batch_ndcg = 18.55 
2025-04-08 12:57:23.080775: Steps 17/138: batch_recall = 35.69, batch_ndcg = 20.02 
2025-04-08 12:57:24.398493: Steps 18/138: batch_recall = 36.11, batch_ndcg = 23.55 
2025-04-08 12:57:25.699306: Steps 19/138: batch_recall = 35.93, batch_ndcg = 22.24 
2025-04-08 12:57:26.982371: Steps 20/138: batch_recall = 39.72, batch_ndcg = 22.46 
2025-04-08 12:57:28.267611: Steps 21/138: batch_recall = 42.30, batch_ndcg = 28.36 
2025-04-08 12:57:29.541252: Steps 22/138: batch_recall = 35.46, batch_ndcg = 21.56 
2025-04-08 12:57:30.816924: Steps 23/138: batch_recall = 31.96, batch_ndcg = 18.08 
2025-04-08 12:57:32.135473: Steps 24/138: batch_recall = 33.83, batch_ndcg = 20.73 
2025-04-08 12:57:33.436414: Steps 25/138: batch_recall = 36.91, batch_ndcg = 22.30 
2025-04-08 12:57:34.736115: Steps 26/138: batch_recall = 37.81, batch_ndcg = 21.48 
2025-04-08 12:57:36.026393: Steps 27/138: batch_recall = 41.12, batch_ndcg = 25.20 
2025-04-08 12:57:37.322875: Steps 28/138: batch_recall = 41.71, batch_ndcg = 22.57 
2025-04-08 12:57:38.604831: Steps 29/138: batch_recall = 36.28, batch_ndcg = 19.69 
2025-04-08 12:57:39.925397: Steps 30/138: batch_recall = 40.77, batch_ndcg = 25.55 
2025-04-08 12:57:41.211991: Steps 31/138: batch_recall = 27.14, batch_ndcg = 16.77 
2025-04-08 12:57:42.478469: Steps 32/138: batch_recall = 31.45, batch_ndcg = 20.84 
2025-04-08 12:57:43.744991: Steps 33/138: batch_recall = 36.86, batch_ndcg = 22.05 
2025-04-08 12:57:45.025154: Steps 34/138: batch_recall = 31.99, batch_ndcg = 19.05 
2025-04-08 12:57:46.338236: Steps 35/138: batch_recall = 34.63, batch_ndcg = 20.18 
2025-04-08 12:57:47.631090: Steps 36/138: batch_recall = 35.11, batch_ndcg = 20.81 
2025-04-08 12:57:48.930036: Steps 37/138: batch_recall = 42.32, batch_ndcg = 25.48 
2025-04-08 12:57:50.226515: Steps 38/138: batch_recall = 37.57, batch_ndcg = 24.40 
2025-04-08 12:57:51.511550: Steps 39/138: batch_recall = 44.38, batch_ndcg = 27.36 
2025-04-08 12:57:52.814815: Steps 40/138: batch_recall = 38.77, batch_ndcg = 20.81 
2025-04-08 12:57:54.106089: Steps 41/138: batch_recall = 34.51, batch_ndcg = 20.90 
2025-04-08 12:57:55.384892: Steps 42/138: batch_recall = 36.70, batch_ndcg = 22.03 
2025-04-08 12:57:56.665684: Steps 43/138: batch_recall = 42.37, batch_ndcg = 27.82 
2025-04-08 12:57:57.946271: Steps 44/138: batch_recall = 31.33, batch_ndcg = 18.68 
2025-04-08 12:57:59.223836: Steps 45/138: batch_recall = 36.48, batch_ndcg = 22.59 
2025-04-08 12:58:00.498443: Steps 46/138: batch_recall = 46.52, batch_ndcg = 27.17 
2025-04-08 12:58:01.788433: Steps 47/138: batch_recall = 33.10, batch_ndcg = 18.98 
2025-04-08 12:58:03.066982: Steps 48/138: batch_recall = 37.40, batch_ndcg = 22.32 
2025-04-08 12:58:04.364905: Steps 49/138: batch_recall = 45.46, batch_ndcg = 27.94 
2025-04-08 12:58:05.653070: Steps 50/138: batch_recall = 36.71, batch_ndcg = 21.64 
2025-04-08 12:58:06.928240: Steps 51/138: batch_recall = 39.50, batch_ndcg = 23.92 
2025-04-08 12:58:08.257648: Steps 52/138: batch_recall = 46.40, batch_ndcg = 29.48 
2025-04-08 12:58:09.561350: Steps 53/138: batch_recall = 39.47, batch_ndcg = 22.36 
2025-04-08 12:58:10.843777: Steps 54/138: batch_recall = 41.42, batch_ndcg = 22.76 
2025-04-08 12:58:12.109957: Steps 55/138: batch_recall = 39.89, batch_ndcg = 21.25 
2025-04-08 12:58:13.407298: Steps 56/138: batch_recall = 39.90, batch_ndcg = 22.46 
2025-04-08 12:58:14.706875: Steps 57/138: batch_recall = 33.97, batch_ndcg = 19.98 
2025-04-08 12:58:16.015881: Steps 58/138: batch_recall = 48.61, batch_ndcg = 26.44 
2025-04-08 12:58:17.305345: Steps 59/138: batch_recall = 50.70, batch_ndcg = 27.98 
2025-04-08 12:58:18.590664: Steps 60/138: batch_recall = 50.77, batch_ndcg = 28.72 
2025-04-08 12:58:19.889532: Steps 61/138: batch_recall = 43.70, batch_ndcg = 23.44 
2025-04-08 12:58:21.179521: Steps 62/138: batch_recall = 53.72, batch_ndcg = 31.61 
2025-04-08 12:58:22.473365: Steps 63/138: batch_recall = 51.89, batch_ndcg = 29.37 
2025-04-08 12:58:23.765362: Steps 64/138: batch_recall = 42.96, batch_ndcg = 20.84 
2025-04-08 12:58:25.061737: Steps 65/138: batch_recall = 56.75, batch_ndcg = 31.84 
2025-04-08 12:58:26.336076: Steps 66/138: batch_recall = 47.53, batch_ndcg = 28.92 
2025-04-08 12:58:27.621658: Steps 67/138: batch_recall = 60.98, batch_ndcg = 37.77 
2025-04-08 12:58:28.883535: Steps 68/138: batch_recall = 44.98, batch_ndcg = 24.51 
2025-04-08 12:58:30.164219: Steps 69/138: batch_recall = 68.35, batch_ndcg = 37.59 
2025-04-08 12:58:31.439216: Steps 70/138: batch_recall = 49.99, batch_ndcg = 31.13 
2025-04-08 12:58:32.732337: Steps 71/138: batch_recall = 59.22, batch_ndcg = 35.91 
2025-04-08 12:58:34.026593: Steps 72/138: batch_recall = 67.41, batch_ndcg = 37.78 
2025-04-08 12:58:35.317269: Steps 73/138: batch_recall = 67.65, batch_ndcg = 35.49 
2025-04-08 12:58:36.615765: Steps 74/138: batch_recall = 52.82, batch_ndcg = 31.12 
2025-04-08 12:58:37.895766: Steps 75/138: batch_recall = 58.54, batch_ndcg = 35.83 
2025-04-08 12:58:39.177420: Steps 76/138: batch_recall = 65.99, batch_ndcg = 38.91 
2025-04-08 12:58:40.454622: Steps 77/138: batch_recall = 60.87, batch_ndcg = 32.96 
2025-04-08 12:58:41.728496: Steps 78/138: batch_recall = 62.27, batch_ndcg = 35.09 
2025-04-08 12:58:43.000285: Steps 79/138: batch_recall = 60.37, batch_ndcg = 36.01 
2025-04-08 12:58:44.282457: Steps 80/138: batch_recall = 52.98, batch_ndcg = 29.61 
2025-04-08 12:58:45.572080: Steps 81/138: batch_recall = 61.42, batch_ndcg = 35.98 
2025-04-08 12:58:46.851316: Steps 82/138: batch_recall = 58.70, batch_ndcg = 35.77 
2025-04-08 12:58:48.142017: Steps 83/138: batch_recall = 60.03, batch_ndcg = 33.28 
2025-04-08 12:58:49.439467: Steps 84/138: batch_recall = 71.23, batch_ndcg = 39.64 
2025-04-08 12:58:50.725393: Steps 85/138: batch_recall = 59.09, batch_ndcg = 35.21 
2025-04-08 12:58:52.000368: Steps 86/138: batch_recall = 91.24, batch_ndcg = 52.60 
2025-04-08 12:58:53.286483: Steps 87/138: batch_recall = 69.38, batch_ndcg = 41.31 
2025-04-08 12:58:54.567390: Steps 88/138: batch_recall = 71.76, batch_ndcg = 40.91 
2025-04-08 12:58:55.830897: Steps 89/138: batch_recall = 90.51, batch_ndcg = 49.08 
2025-04-08 12:58:57.088812: Steps 90/138: batch_recall = 62.50, batch_ndcg = 36.08 
2025-04-08 12:58:58.343812: Steps 91/138: batch_recall = 84.96, batch_ndcg = 47.96 
2025-04-08 12:58:59.620174: Steps 92/138: batch_recall = 76.73, batch_ndcg = 44.64 
2025-04-08 12:59:00.915933: Steps 93/138: batch_recall = 82.77, batch_ndcg = 47.20 
2025-04-08 12:59:02.202176: Steps 94/138: batch_recall = 80.13, batch_ndcg = 42.96 
2025-04-08 12:59:03.491761: Steps 95/138: batch_recall = 90.77, batch_ndcg = 50.85 
2025-04-08 12:59:04.774372: Steps 96/138: batch_recall = 93.77, batch_ndcg = 57.24 
2025-04-08 12:59:06.066085: Steps 97/138: batch_recall = 98.75, batch_ndcg = 56.15 
2025-04-08 12:59:07.360638: Steps 98/138: batch_recall = 72.79, batch_ndcg = 41.41 
2025-04-08 12:59:08.647940: Steps 99/138: batch_recall = 76.73, batch_ndcg = 44.67 
2025-04-08 12:59:09.923411: Steps 100/138: batch_recall = 88.15, batch_ndcg = 49.84 
2025-04-08 12:59:11.185584: Steps 101/138: batch_recall = 91.48, batch_ndcg = 53.69 
2025-04-08 12:59:12.480284: Steps 102/138: batch_recall = 85.30, batch_ndcg = 49.38 
2025-04-08 12:59:13.738505: Steps 103/138: batch_recall = 99.00, batch_ndcg = 51.83 
2025-04-08 12:59:15.011124: Steps 104/138: batch_recall = 84.47, batch_ndcg = 45.05 
2025-04-08 12:59:16.300535: Steps 105/138: batch_recall = 83.83, batch_ndcg = 50.44 
2025-04-08 12:59:17.572353: Steps 106/138: batch_recall = 78.54, batch_ndcg = 42.94 
2025-04-08 12:59:18.867291: Steps 107/138: batch_recall = 78.15, batch_ndcg = 41.59 
2025-04-08 12:59:20.139340: Steps 108/138: batch_recall = 80.42, batch_ndcg = 50.59 
2025-04-08 12:59:21.428170: Steps 109/138: batch_recall = 89.40, batch_ndcg = 48.37 
2025-04-08 12:59:22.697038: Steps 110/138: batch_recall = 79.73, batch_ndcg = 42.70 
2025-04-08 12:59:23.972551: Steps 111/138: batch_recall = 93.73, batch_ndcg = 55.10 
2025-04-08 12:59:25.233635: Steps 112/138: batch_recall = 102.44, batch_ndcg = 56.44 
2025-04-08 12:59:26.506788: Steps 113/138: batch_recall = 84.16, batch_ndcg = 47.68 
2025-04-08 12:59:27.769217: Steps 114/138: batch_recall = 81.13, batch_ndcg = 46.81 
2025-04-08 12:59:29.034389: Steps 115/138: batch_recall = 69.89, batch_ndcg = 37.26 
2025-04-08 12:59:30.307107: Steps 116/138: batch_recall = 71.36, batch_ndcg = 37.34 
2025-04-08 12:59:31.584123: Steps 117/138: batch_recall = 78.10, batch_ndcg = 46.25 
2025-04-08 12:59:32.854794: Steps 118/138: batch_recall = 91.50, batch_ndcg = 49.31 
2025-04-08 12:59:34.141976: Steps 119/138: batch_recall = 82.72, batch_ndcg = 43.15 
2025-04-08 12:59:35.428878: Steps 120/138: batch_recall = 77.79, batch_ndcg = 43.39 
2025-04-08 12:59:36.695336: Steps 121/138: batch_recall = 81.09, batch_ndcg = 45.21 
2025-04-08 12:59:37.964893: Steps 122/138: batch_recall = 85.30, batch_ndcg = 43.95 
2025-04-08 12:59:39.241893: Steps 123/138: batch_recall = 79.08, batch_ndcg = 45.54 
2025-04-08 12:59:40.495827: Steps 124/138: batch_recall = 97.53, batch_ndcg = 59.66 
2025-04-08 12:59:41.759280: Steps 125/138: batch_recall = 75.30, batch_ndcg = 42.75 
2025-04-08 12:59:43.022651: Steps 126/138: batch_recall = 110.79, batch_ndcg = 61.74 
2025-04-08 12:59:44.297465: Steps 127/138: batch_recall = 83.57, batch_ndcg = 51.15 
2025-04-08 12:59:45.580157: Steps 128/138: batch_recall = 80.05, batch_ndcg = 43.58 
2025-04-08 12:59:46.848212: Steps 129/138: batch_recall = 93.40, batch_ndcg = 57.05 
2025-04-08 12:59:48.133182: Steps 130/138: batch_recall = 82.30, batch_ndcg = 46.62 
2025-04-08 12:59:49.407861: Steps 131/138: batch_recall = 79.98, batch_ndcg = 50.36 
2025-04-08 12:59:50.680793: Steps 132/138: batch_recall = 82.64, batch_ndcg = 48.62 
2025-04-08 12:59:51.948475: Steps 133/138: batch_recall = 86.98, batch_ndcg = 53.37 
2025-04-08 12:59:53.220113: Steps 134/138: batch_recall = 86.56, batch_ndcg = 50.71 
2025-04-08 12:59:54.485625: Steps 135/138: batch_recall = 92.81, batch_ndcg = 57.34 
2025-04-08 12:59:55.741950: Steps 136/138: batch_recall = 101.47, batch_ndcg = 56.16 
2025-04-08 12:59:56.991262: Steps 137/138: batch_recall = 94.45, batch_ndcg = 62.64 
2025-04-08 12:59:56.991807: Epoch 1/1000, Test: Recall = 0.1151, NDCG = 0.0668  

2025-04-08 12:59:58.762107: Training Step 0/354: batchLoss = 0.5341, diffLoss = 2.6219, kgLoss = 0.0121
2025-04-08 13:00:00.391367: Training Step 1/354: batchLoss = 0.5897, diffLoss = 2.9008, kgLoss = 0.0119
2025-04-08 13:00:02.014626: Training Step 2/354: batchLoss = 0.5197, diffLoss = 2.5506, kgLoss = 0.0120
2025-04-08 13:00:03.638440: Training Step 3/354: batchLoss = 0.5924, diffLoss = 2.9072, kgLoss = 0.0136
2025-04-08 13:00:05.257001: Training Step 4/354: batchLoss = 0.7050, diffLoss = 3.4598, kgLoss = 0.0163
2025-04-08 13:00:06.876061: Training Step 5/354: batchLoss = 0.5382, diffLoss = 2.6419, kgLoss = 0.0122
2025-04-08 13:00:08.495999: Training Step 6/354: batchLoss = 0.5704, diffLoss = 2.7845, kgLoss = 0.0169
2025-04-08 13:00:10.103079: Training Step 7/354: batchLoss = 0.5153, diffLoss = 2.5314, kgLoss = 0.0112
2025-04-08 13:00:11.710070: Training Step 8/354: batchLoss = 0.6102, diffLoss = 2.9883, kgLoss = 0.0157
2025-04-08 13:00:13.322035: Training Step 9/354: batchLoss = 0.6025, diffLoss = 2.9616, kgLoss = 0.0127
2025-04-08 13:00:14.960412: Training Step 10/354: batchLoss = 0.7132, diffLoss = 3.5027, kgLoss = 0.0158
2025-04-08 13:00:16.608212: Training Step 11/354: batchLoss = 0.6320, diffLoss = 3.1034, kgLoss = 0.0141
2025-04-08 13:00:18.244321: Training Step 12/354: batchLoss = 0.7206, diffLoss = 3.5435, kgLoss = 0.0149
2025-04-08 13:00:19.883502: Training Step 13/354: batchLoss = 0.5482, diffLoss = 2.6873, kgLoss = 0.0134
2025-04-08 13:00:21.525499: Training Step 14/354: batchLoss = 0.5156, diffLoss = 2.5320, kgLoss = 0.0114
2025-04-08 13:00:23.157151: Training Step 15/354: batchLoss = 0.6593, diffLoss = 3.2304, kgLoss = 0.0165
2025-04-08 13:00:24.782692: Training Step 16/354: batchLoss = 0.6431, diffLoss = 3.1582, kgLoss = 0.0143
2025-04-08 13:00:26.406375: Training Step 17/354: batchLoss = 0.6890, diffLoss = 3.3849, kgLoss = 0.0150
2025-04-08 13:00:28.071811: Training Step 18/354: batchLoss = 0.8077, diffLoss = 3.9663, kgLoss = 0.0180
2025-04-08 13:00:29.710292: Training Step 19/354: batchLoss = 0.7145, diffLoss = 3.5120, kgLoss = 0.0151
2025-04-08 13:00:31.325821: Training Step 20/354: batchLoss = 0.6975, diffLoss = 3.4329, kgLoss = 0.0137
2025-04-08 13:00:32.954427: Training Step 21/354: batchLoss = 0.6614, diffLoss = 3.2458, kgLoss = 0.0153
2025-04-08 13:00:34.575429: Training Step 22/354: batchLoss = 0.6494, diffLoss = 3.1928, kgLoss = 0.0135
2025-04-08 13:00:36.205693: Training Step 23/354: batchLoss = 0.6882, diffLoss = 3.3871, kgLoss = 0.0135
2025-04-08 13:00:37.830120: Training Step 24/354: batchLoss = 0.5839, diffLoss = 2.8589, kgLoss = 0.0151
2025-04-08 13:00:39.437770: Training Step 25/354: batchLoss = 0.6229, diffLoss = 3.0587, kgLoss = 0.0139
2025-04-08 13:00:41.044572: Training Step 26/354: batchLoss = 0.5963, diffLoss = 2.9332, kgLoss = 0.0121
2025-04-08 13:00:42.659645: Training Step 27/354: batchLoss = 0.7040, diffLoss = 3.4541, kgLoss = 0.0165
2025-04-08 13:00:44.284967: Training Step 28/354: batchLoss = 0.6470, diffLoss = 3.1774, kgLoss = 0.0144
2025-04-08 13:00:45.912901: Training Step 29/354: batchLoss = 0.7967, diffLoss = 3.9190, kgLoss = 0.0161
2025-04-08 13:00:47.535573: Training Step 30/354: batchLoss = 0.5225, diffLoss = 2.5637, kgLoss = 0.0123
2025-04-08 13:00:49.168406: Training Step 31/354: batchLoss = 0.5616, diffLoss = 2.7560, kgLoss = 0.0130
2025-04-08 13:00:50.791296: Training Step 32/354: batchLoss = 0.5834, diffLoss = 2.8694, kgLoss = 0.0119
2025-04-08 13:00:52.407219: Training Step 33/354: batchLoss = 0.6856, diffLoss = 3.3651, kgLoss = 0.0157
2025-04-08 13:00:54.013883: Training Step 34/354: batchLoss = 0.6475, diffLoss = 3.1794, kgLoss = 0.0146
2025-04-08 13:00:55.629710: Training Step 35/354: batchLoss = 0.5862, diffLoss = 2.8693, kgLoss = 0.0155
2025-04-08 13:00:57.250302: Training Step 36/354: batchLoss = 0.6083, diffLoss = 2.9841, kgLoss = 0.0144
2025-04-08 13:00:58.875344: Training Step 37/354: batchLoss = 0.7903, diffLoss = 3.8829, kgLoss = 0.0171
2025-04-08 13:01:00.492358: Training Step 38/354: batchLoss = 0.5828, diffLoss = 2.8579, kgLoss = 0.0141
2025-04-08 13:01:02.131762: Training Step 39/354: batchLoss = 0.5812, diffLoss = 2.8480, kgLoss = 0.0145
2025-04-08 13:01:03.752813: Training Step 40/354: batchLoss = 0.7638, diffLoss = 3.7527, kgLoss = 0.0165
2025-04-08 13:01:05.376269: Training Step 41/354: batchLoss = 0.6644, diffLoss = 3.2676, kgLoss = 0.0136
2025-04-08 13:01:06.994988: Training Step 42/354: batchLoss = 0.5890, diffLoss = 2.8965, kgLoss = 0.0121
2025-04-08 13:01:08.604376: Training Step 43/354: batchLoss = 0.5930, diffLoss = 2.9139, kgLoss = 0.0128
2025-04-08 13:01:10.215123: Training Step 44/354: batchLoss = 0.5720, diffLoss = 2.8071, kgLoss = 0.0132
2025-04-08 13:01:11.827010: Training Step 45/354: batchLoss = 0.8017, diffLoss = 3.9370, kgLoss = 0.0178
2025-04-08 13:01:13.448244: Training Step 46/354: batchLoss = 0.6046, diffLoss = 2.9679, kgLoss = 0.0137
2025-04-08 13:01:15.071502: Training Step 47/354: batchLoss = 0.5646, diffLoss = 2.7704, kgLoss = 0.0132
2025-04-08 13:01:16.697362: Training Step 48/354: batchLoss = 0.5623, diffLoss = 2.7532, kgLoss = 0.0146
2025-04-08 13:01:18.318722: Training Step 49/354: batchLoss = 0.8268, diffLoss = 4.0613, kgLoss = 0.0182
2025-04-08 13:01:19.939183: Training Step 50/354: batchLoss = 0.5822, diffLoss = 2.8560, kgLoss = 0.0137
2025-04-08 13:01:21.557231: Training Step 51/354: batchLoss = 0.7865, diffLoss = 3.8625, kgLoss = 0.0176
2025-04-08 13:01:23.171022: Training Step 52/354: batchLoss = 0.6185, diffLoss = 3.0407, kgLoss = 0.0129
2025-04-08 13:01:24.786469: Training Step 53/354: batchLoss = 0.6809, diffLoss = 3.3463, kgLoss = 0.0145
2025-04-08 13:01:26.398465: Training Step 54/354: batchLoss = 0.5925, diffLoss = 2.9115, kgLoss = 0.0127
2025-04-08 13:01:28.025936: Training Step 55/354: batchLoss = 0.6815, diffLoss = 3.3300, kgLoss = 0.0193
2025-04-08 13:01:29.646305: Training Step 56/354: batchLoss = 0.5504, diffLoss = 2.6948, kgLoss = 0.0143
2025-04-08 13:01:31.259863: Training Step 57/354: batchLoss = 0.7017, diffLoss = 3.4474, kgLoss = 0.0153
2025-04-08 13:01:32.880944: Training Step 58/354: batchLoss = 0.5301, diffLoss = 2.5937, kgLoss = 0.0142
2025-04-08 13:01:34.498523: Training Step 59/354: batchLoss = 0.6045, diffLoss = 2.9656, kgLoss = 0.0142
2025-04-08 13:01:36.115392: Training Step 60/354: batchLoss = 0.6669, diffLoss = 3.2735, kgLoss = 0.0153
2025-04-08 13:01:37.729593: Training Step 61/354: batchLoss = 0.7817, diffLoss = 3.8434, kgLoss = 0.0163
2025-04-08 13:01:39.342338: Training Step 62/354: batchLoss = 0.6933, diffLoss = 3.4108, kgLoss = 0.0140
2025-04-08 13:01:40.959222: Training Step 63/354: batchLoss = 0.7586, diffLoss = 3.7302, kgLoss = 0.0157
2025-04-08 13:01:42.583435: Training Step 64/354: batchLoss = 2.2225, diffLoss = 10.9375, kgLoss = 0.0438
2025-04-08 13:01:44.210710: Training Step 65/354: batchLoss = 0.6670, diffLoss = 3.2774, kgLoss = 0.0144
2025-04-08 13:01:45.825378: Training Step 66/354: batchLoss = 0.7489, diffLoss = 3.6663, kgLoss = 0.0195
2025-04-08 13:01:47.450463: Training Step 67/354: batchLoss = 0.7796, diffLoss = 3.8338, kgLoss = 0.0160
2025-04-08 13:01:49.068604: Training Step 68/354: batchLoss = 0.6748, diffLoss = 3.3166, kgLoss = 0.0143
2025-04-08 13:01:50.689308: Training Step 69/354: batchLoss = 0.6568, diffLoss = 3.2270, kgLoss = 0.0142
2025-04-08 13:01:52.297475: Training Step 70/354: batchLoss = 0.6225, diffLoss = 3.0552, kgLoss = 0.0143
2025-04-08 13:01:53.925959: Training Step 71/354: batchLoss = 0.7794, diffLoss = 3.8336, kgLoss = 0.0158
2025-04-08 13:01:55.566390: Training Step 72/354: batchLoss = 0.6283, diffLoss = 3.0874, kgLoss = 0.0136
2025-04-08 13:01:57.188858: Training Step 73/354: batchLoss = 0.7346, diffLoss = 3.6111, kgLoss = 0.0154
2025-04-08 13:01:58.817655: Training Step 74/354: batchLoss = 0.6848, diffLoss = 3.3680, kgLoss = 0.0140
2025-04-08 13:02:00.446475: Training Step 75/354: batchLoss = 0.5826, diffLoss = 2.8620, kgLoss = 0.0128
2025-04-08 13:02:02.075468: Training Step 76/354: batchLoss = 0.5451, diffLoss = 2.6734, kgLoss = 0.0130
2025-04-08 13:02:03.706591: Training Step 77/354: batchLoss = 0.6092, diffLoss = 2.9920, kgLoss = 0.0135
2025-04-08 13:02:05.328531: Training Step 78/354: batchLoss = 0.6151, diffLoss = 3.0214, kgLoss = 0.0135
2025-04-08 13:02:06.939915: Training Step 79/354: batchLoss = 0.6886, diffLoss = 3.3819, kgLoss = 0.0153
2025-04-08 13:02:08.548166: Training Step 80/354: batchLoss = 0.6242, diffLoss = 3.0608, kgLoss = 0.0150
2025-04-08 13:02:10.166322: Training Step 81/354: batchLoss = 0.6581, diffLoss = 3.2321, kgLoss = 0.0146
2025-04-08 13:02:11.789107: Training Step 82/354: batchLoss = 0.7972, diffLoss = 3.9110, kgLoss = 0.0188
2025-04-08 13:02:13.408979: Training Step 83/354: batchLoss = 0.4951, diffLoss = 2.4348, kgLoss = 0.0102
2025-04-08 13:02:15.034306: Training Step 84/354: batchLoss = 0.5384, diffLoss = 2.6401, kgLoss = 0.0130
2025-04-08 13:02:16.653526: Training Step 85/354: batchLoss = 0.6720, diffLoss = 3.3038, kgLoss = 0.0141
2025-04-08 13:02:18.278866: Training Step 86/354: batchLoss = 0.6960, diffLoss = 3.4222, kgLoss = 0.0145
2025-04-08 13:02:19.889755: Training Step 87/354: batchLoss = 0.6290, diffLoss = 3.0852, kgLoss = 0.0149
2025-04-08 13:02:21.514115: Training Step 88/354: batchLoss = 0.7468, diffLoss = 3.6749, kgLoss = 0.0148
2025-04-08 13:02:23.137117: Training Step 89/354: batchLoss = 0.6130, diffLoss = 3.0171, kgLoss = 0.0120
2025-04-08 13:02:24.769266: Training Step 90/354: batchLoss = 0.6807, diffLoss = 3.3422, kgLoss = 0.0153
2025-04-08 13:02:26.392984: Training Step 91/354: batchLoss = 0.7306, diffLoss = 3.5906, kgLoss = 0.0155
2025-04-08 13:02:28.017176: Training Step 92/354: batchLoss = 0.7512, diffLoss = 3.6923, kgLoss = 0.0159
2025-04-08 13:02:29.636945: Training Step 93/354: batchLoss = 0.8006, diffLoss = 3.9357, kgLoss = 0.0168
2025-04-08 13:02:31.257272: Training Step 94/354: batchLoss = 0.6385, diffLoss = 3.1380, kgLoss = 0.0136
2025-04-08 13:02:32.875557: Training Step 95/354: batchLoss = 0.6351, diffLoss = 3.1180, kgLoss = 0.0144
2025-04-08 13:02:34.483631: Training Step 96/354: batchLoss = 0.5456, diffLoss = 2.6736, kgLoss = 0.0137
2025-04-08 13:02:36.090813: Training Step 97/354: batchLoss = 0.5532, diffLoss = 2.6993, kgLoss = 0.0167
2025-04-08 13:02:37.713044: Training Step 98/354: batchLoss = 0.4882, diffLoss = 2.3902, kgLoss = 0.0127
2025-04-08 13:02:39.330094: Training Step 99/354: batchLoss = 0.8272, diffLoss = 4.0626, kgLoss = 0.0183
2025-04-08 13:02:40.956860: Training Step 100/354: batchLoss = 0.6567, diffLoss = 3.2291, kgLoss = 0.0136
2025-04-08 13:02:42.578242: Training Step 101/354: batchLoss = 0.6094, diffLoss = 2.9875, kgLoss = 0.0149
2025-04-08 13:02:44.206172: Training Step 102/354: batchLoss = 0.7113, diffLoss = 3.4929, kgLoss = 0.0159
2025-04-08 13:02:45.838773: Training Step 103/354: batchLoss = 0.5373, diffLoss = 2.6358, kgLoss = 0.0126
2025-04-08 13:02:47.463108: Training Step 104/354: batchLoss = 0.6512, diffLoss = 3.1924, kgLoss = 0.0159
2025-04-08 13:02:49.080988: Training Step 105/354: batchLoss = 0.5957, diffLoss = 2.9234, kgLoss = 0.0138
2025-04-08 13:02:50.691644: Training Step 106/354: batchLoss = 0.6426, diffLoss = 3.1596, kgLoss = 0.0134
2025-04-08 13:02:52.298316: Training Step 107/354: batchLoss = 0.6018, diffLoss = 2.9572, kgLoss = 0.0130
2025-04-08 13:02:53.921562: Training Step 108/354: batchLoss = 0.6759, diffLoss = 3.3218, kgLoss = 0.0145
2025-04-08 13:02:55.543999: Training Step 109/354: batchLoss = 0.5895, diffLoss = 2.8934, kgLoss = 0.0135
2025-04-08 13:02:57.168783: Training Step 110/354: batchLoss = 0.6863, diffLoss = 3.3666, kgLoss = 0.0162
2025-04-08 13:02:58.793267: Training Step 111/354: batchLoss = 0.7266, diffLoss = 3.5662, kgLoss = 0.0167
2025-04-08 13:03:00.411915: Training Step 112/354: batchLoss = 0.7596, diffLoss = 3.7279, kgLoss = 0.0175
2025-04-08 13:03:02.035675: Training Step 113/354: batchLoss = 0.5843, diffLoss = 2.8643, kgLoss = 0.0143
2025-04-08 13:03:03.647837: Training Step 114/354: batchLoss = 0.6461, diffLoss = 3.1694, kgLoss = 0.0153
2025-04-08 13:03:05.257943: Training Step 115/354: batchLoss = 0.5379, diffLoss = 2.6292, kgLoss = 0.0150
2025-04-08 13:03:06.861378: Training Step 116/354: batchLoss = 0.6361, diffLoss = 3.1135, kgLoss = 0.0167
2025-04-08 13:03:08.481316: Training Step 117/354: batchLoss = 0.7237, diffLoss = 3.5538, kgLoss = 0.0162
2025-04-08 13:03:10.098492: Training Step 118/354: batchLoss = 0.6466, diffLoss = 3.1732, kgLoss = 0.0149
2025-04-08 13:03:11.720149: Training Step 119/354: batchLoss = 0.5842, diffLoss = 2.8665, kgLoss = 0.0137
2025-04-08 13:03:13.343688: Training Step 120/354: batchLoss = 0.5904, diffLoss = 2.9033, kgLoss = 0.0122
2025-04-08 13:03:14.966083: Training Step 121/354: batchLoss = 0.7251, diffLoss = 3.5615, kgLoss = 0.0160
2025-04-08 13:03:16.588804: Training Step 122/354: batchLoss = 0.6784, diffLoss = 3.3368, kgLoss = 0.0138
2025-04-08 13:03:18.210802: Training Step 123/354: batchLoss = 0.6105, diffLoss = 3.0029, kgLoss = 0.0124
2025-04-08 13:03:19.825845: Training Step 124/354: batchLoss = 0.7518, diffLoss = 3.6964, kgLoss = 0.0157
2025-04-08 13:03:21.434585: Training Step 125/354: batchLoss = 0.5725, diffLoss = 2.8109, kgLoss = 0.0129
2025-04-08 13:03:23.050460: Training Step 126/354: batchLoss = 0.7787, diffLoss = 3.8222, kgLoss = 0.0179
2025-04-08 13:03:24.670861: Training Step 127/354: batchLoss = 0.6724, diffLoss = 3.3059, kgLoss = 0.0140
2025-04-08 13:03:26.299816: Training Step 128/354: batchLoss = 0.6781, diffLoss = 3.3358, kgLoss = 0.0136
2025-04-08 13:03:27.924911: Training Step 129/354: batchLoss = 0.7871, diffLoss = 3.8682, kgLoss = 0.0169
2025-04-08 13:03:29.559552: Training Step 130/354: batchLoss = 0.7129, diffLoss = 3.4968, kgLoss = 0.0169
2025-04-08 13:03:31.186664: Training Step 131/354: batchLoss = 0.6008, diffLoss = 2.9500, kgLoss = 0.0135
2025-04-08 13:03:32.801535: Training Step 132/354: batchLoss = 0.7338, diffLoss = 3.6084, kgLoss = 0.0151
2025-04-08 13:03:34.410444: Training Step 133/354: batchLoss = 0.8421, diffLoss = 4.1381, kgLoss = 0.0181
2025-04-08 13:03:36.019076: Training Step 134/354: batchLoss = 0.7273, diffLoss = 3.5772, kgLoss = 0.0148
2025-04-08 13:03:37.630132: Training Step 135/354: batchLoss = 0.7402, diffLoss = 3.6388, kgLoss = 0.0156
2025-04-08 13:03:39.248353: Training Step 136/354: batchLoss = 0.6809, diffLoss = 3.3472, kgLoss = 0.0143
2025-04-08 13:03:40.862776: Training Step 137/354: batchLoss = 0.6274, diffLoss = 3.0838, kgLoss = 0.0133
2025-04-08 13:03:42.488075: Training Step 138/354: batchLoss = 0.7159, diffLoss = 3.5206, kgLoss = 0.0148
2025-04-08 13:03:44.111819: Training Step 139/354: batchLoss = 0.5214, diffLoss = 2.5572, kgLoss = 0.0124
2025-04-08 13:03:45.745709: Training Step 140/354: batchLoss = 0.7825, diffLoss = 3.8406, kgLoss = 0.0180
2025-04-08 13:03:47.374819: Training Step 141/354: batchLoss = 0.5145, diffLoss = 2.5291, kgLoss = 0.0108
2025-04-08 13:03:48.983984: Training Step 142/354: batchLoss = 0.5793, diffLoss = 2.8466, kgLoss = 0.0124
2025-04-08 13:03:50.599509: Training Step 143/354: batchLoss = 0.6200, diffLoss = 3.0454, kgLoss = 0.0137
2025-04-08 13:03:52.216481: Training Step 144/354: batchLoss = 0.7457, diffLoss = 3.6638, kgLoss = 0.0162
2025-04-08 13:03:53.851248: Training Step 145/354: batchLoss = 0.7093, diffLoss = 3.4862, kgLoss = 0.0151
2025-04-08 13:03:55.469250: Training Step 146/354: batchLoss = 0.5653, diffLoss = 2.7795, kgLoss = 0.0118
2025-04-08 13:03:57.089315: Training Step 147/354: batchLoss = 0.5835, diffLoss = 2.8646, kgLoss = 0.0132
2025-04-08 13:03:58.717190: Training Step 148/354: batchLoss = 0.6087, diffLoss = 2.9876, kgLoss = 0.0140
2025-04-08 13:04:00.336125: Training Step 149/354: batchLoss = 0.7922, diffLoss = 3.8943, kgLoss = 0.0167
2025-04-08 13:04:01.955083: Training Step 150/354: batchLoss = 0.6786, diffLoss = 3.3349, kgLoss = 0.0146
2025-04-08 13:04:03.565496: Training Step 151/354: batchLoss = 0.8727, diffLoss = 4.2899, kgLoss = 0.0184
2025-04-08 13:04:05.173340: Training Step 152/354: batchLoss = 0.5314, diffLoss = 2.6131, kgLoss = 0.0110
2025-04-08 13:04:06.792284: Training Step 153/354: batchLoss = 0.7207, diffLoss = 3.5426, kgLoss = 0.0152
2025-04-08 13:04:08.409518: Training Step 154/354: batchLoss = 0.6768, diffLoss = 3.3239, kgLoss = 0.0151
2025-04-08 13:04:10.027740: Training Step 155/354: batchLoss = 0.6145, diffLoss = 3.0186, kgLoss = 0.0134
2025-04-08 13:04:11.644697: Training Step 156/354: batchLoss = 0.5855, diffLoss = 2.8774, kgLoss = 0.0125
2025-04-08 13:04:13.263929: Training Step 157/354: batchLoss = 0.7548, diffLoss = 3.7113, kgLoss = 0.0156
2025-04-08 13:04:14.887654: Training Step 158/354: batchLoss = 0.8064, diffLoss = 3.9656, kgLoss = 0.0166
2025-04-08 13:04:16.501300: Training Step 159/354: batchLoss = 0.6403, diffLoss = 3.1443, kgLoss = 0.0143
2025-04-08 13:04:18.115969: Training Step 160/354: batchLoss = 0.7659, diffLoss = 3.7672, kgLoss = 0.0156
2025-04-08 13:04:19.723780: Training Step 161/354: batchLoss = 0.6203, diffLoss = 3.0450, kgLoss = 0.0141
2025-04-08 13:04:21.335501: Training Step 162/354: batchLoss = 0.5893, diffLoss = 2.8950, kgLoss = 0.0129
2025-04-08 13:04:22.954474: Training Step 163/354: batchLoss = 0.8779, diffLoss = 4.3109, kgLoss = 0.0196
2025-04-08 13:04:24.563585: Training Step 164/354: batchLoss = 0.5790, diffLoss = 2.8230, kgLoss = 0.0180
2025-04-08 13:04:26.172739: Training Step 165/354: batchLoss = 0.7073, diffLoss = 3.4756, kgLoss = 0.0152
2025-04-08 13:04:27.783662: Training Step 166/354: batchLoss = 0.6225, diffLoss = 3.0585, kgLoss = 0.0135
2025-04-08 13:04:29.398522: Training Step 167/354: batchLoss = 0.6026, diffLoss = 2.9600, kgLoss = 0.0132
2025-04-08 13:04:31.011188: Training Step 168/354: batchLoss = 0.5139, diffLoss = 2.5263, kgLoss = 0.0108
2025-04-08 13:04:32.623165: Training Step 169/354: batchLoss = 0.6369, diffLoss = 3.1283, kgLoss = 0.0140
2025-04-08 13:04:34.230646: Training Step 170/354: batchLoss = 0.5526, diffLoss = 2.7135, kgLoss = 0.0124
2025-04-08 13:04:35.842795: Training Step 171/354: batchLoss = 0.5096, diffLoss = 2.5004, kgLoss = 0.0119
2025-04-08 13:04:37.460761: Training Step 172/354: batchLoss = 0.5202, diffLoss = 2.5556, kgLoss = 0.0113
2025-04-08 13:04:39.087340: Training Step 173/354: batchLoss = 0.6606, diffLoss = 3.2443, kgLoss = 0.0146
2025-04-08 13:04:40.706571: Training Step 174/354: batchLoss = 0.5401, diffLoss = 2.6533, kgLoss = 0.0118
2025-04-08 13:04:42.327674: Training Step 175/354: batchLoss = 0.7107, diffLoss = 3.4864, kgLoss = 0.0168
2025-04-08 13:04:43.947678: Training Step 176/354: batchLoss = 0.5800, diffLoss = 2.8543, kgLoss = 0.0115
2025-04-08 13:04:45.566499: Training Step 177/354: batchLoss = 0.6723, diffLoss = 3.3098, kgLoss = 0.0130
2025-04-08 13:04:47.175404: Training Step 178/354: batchLoss = 0.5873, diffLoss = 2.8833, kgLoss = 0.0133
2025-04-08 13:04:48.791637: Training Step 179/354: batchLoss = 0.9786, diffLoss = 4.8141, kgLoss = 0.0197
2025-04-08 13:04:50.409071: Training Step 180/354: batchLoss = 0.7538, diffLoss = 3.7052, kgLoss = 0.0159
2025-04-08 13:04:52.052136: Training Step 181/354: batchLoss = 0.6044, diffLoss = 2.9641, kgLoss = 0.0145
2025-04-08 13:04:53.688159: Training Step 182/354: batchLoss = 0.5782, diffLoss = 2.8382, kgLoss = 0.0131
2025-04-08 13:04:55.327792: Training Step 183/354: batchLoss = 0.7556, diffLoss = 3.7205, kgLoss = 0.0143
2025-04-08 13:04:56.975017: Training Step 184/354: batchLoss = 0.6749, diffLoss = 3.3176, kgLoss = 0.0142
2025-04-08 13:04:58.611037: Training Step 185/354: batchLoss = 0.7189, diffLoss = 3.5304, kgLoss = 0.0160
2025-04-08 13:05:00.233853: Training Step 186/354: batchLoss = 0.8114, diffLoss = 3.9941, kgLoss = 0.0157
2025-04-08 13:05:01.862391: Training Step 187/354: batchLoss = 0.5765, diffLoss = 2.8314, kgLoss = 0.0128
2025-04-08 13:05:03.507810: Training Step 188/354: batchLoss = 0.7817, diffLoss = 3.8408, kgLoss = 0.0169
2025-04-08 13:05:05.155601: Training Step 189/354: batchLoss = 0.6687, diffLoss = 3.2347, kgLoss = 0.0272
2025-04-08 13:05:06.775348: Training Step 190/354: batchLoss = 0.6689, diffLoss = 3.2895, kgLoss = 0.0138
2025-04-08 13:05:08.398786: Training Step 191/354: batchLoss = 0.9600, diffLoss = 4.7147, kgLoss = 0.0213
2025-04-08 13:05:10.021362: Training Step 192/354: batchLoss = 0.7370, diffLoss = 3.6226, kgLoss = 0.0155
2025-04-08 13:05:11.657449: Training Step 193/354: batchLoss = 0.6672, diffLoss = 3.2758, kgLoss = 0.0150
2025-04-08 13:05:13.288954: Training Step 194/354: batchLoss = 0.5773, diffLoss = 2.8251, kgLoss = 0.0153
2025-04-08 13:05:14.916497: Training Step 195/354: batchLoss = 0.6454, diffLoss = 3.1663, kgLoss = 0.0152
2025-04-08 13:05:16.534369: Training Step 196/354: batchLoss = 0.7975, diffLoss = 3.9223, kgLoss = 0.0163
2025-04-08 13:05:18.156901: Training Step 197/354: batchLoss = 0.5940, diffLoss = 2.9194, kgLoss = 0.0126
2025-04-08 13:05:19.767828: Training Step 198/354: batchLoss = 0.8115, diffLoss = 3.9858, kgLoss = 0.0179
2025-04-08 13:05:21.393768: Training Step 199/354: batchLoss = 0.5947, diffLoss = 2.9227, kgLoss = 0.0127
2025-04-08 13:05:23.014143: Training Step 200/354: batchLoss = 0.5042, diffLoss = 2.4735, kgLoss = 0.0118
2025-04-08 13:05:24.643634: Training Step 201/354: batchLoss = 0.6473, diffLoss = 3.1814, kgLoss = 0.0138
2025-04-08 13:05:26.260804: Training Step 202/354: batchLoss = 0.6124, diffLoss = 3.0016, kgLoss = 0.0151
2025-04-08 13:05:27.882303: Training Step 203/354: batchLoss = 0.6536, diffLoss = 3.1960, kgLoss = 0.0180
2025-04-08 13:05:29.497007: Training Step 204/354: batchLoss = 0.6913, diffLoss = 3.3922, kgLoss = 0.0161
2025-04-08 13:05:31.112188: Training Step 205/354: batchLoss = 0.7349, diffLoss = 3.6124, kgLoss = 0.0156
2025-04-08 13:05:32.723699: Training Step 206/354: batchLoss = 0.5837, diffLoss = 2.8650, kgLoss = 0.0134
2025-04-08 13:05:34.341195: Training Step 207/354: batchLoss = 0.7687, diffLoss = 3.7791, kgLoss = 0.0161
2025-04-08 13:05:35.966479: Training Step 208/354: batchLoss = 0.6351, diffLoss = 3.1110, kgLoss = 0.0161
2025-04-08 13:05:37.582936: Training Step 209/354: batchLoss = 0.7157, diffLoss = 3.5217, kgLoss = 0.0142
2025-04-08 13:05:39.197302: Training Step 210/354: batchLoss = 0.5854, diffLoss = 2.8743, kgLoss = 0.0132
2025-04-08 13:05:40.831088: Training Step 211/354: batchLoss = 0.6268, diffLoss = 3.0815, kgLoss = 0.0132
2025-04-08 13:05:42.460079: Training Step 212/354: batchLoss = 0.5825, diffLoss = 2.8559, kgLoss = 0.0142
2025-04-08 13:05:44.083056: Training Step 213/354: batchLoss = 0.7368, diffLoss = 3.6180, kgLoss = 0.0165
2025-04-08 13:05:45.696171: Training Step 214/354: batchLoss = 0.6296, diffLoss = 3.0909, kgLoss = 0.0142
2025-04-08 13:05:47.300307: Training Step 215/354: batchLoss = 0.6438, diffLoss = 3.1577, kgLoss = 0.0154
2025-04-08 13:05:48.909611: Training Step 216/354: batchLoss = 0.6721, diffLoss = 3.2950, kgLoss = 0.0163
2025-04-08 13:05:50.532007: Training Step 217/354: batchLoss = 0.6729, diffLoss = 3.3076, kgLoss = 0.0142
2025-04-08 13:05:52.153595: Training Step 218/354: batchLoss = 0.5419, diffLoss = 2.6586, kgLoss = 0.0128
2025-04-08 13:05:53.772268: Training Step 219/354: batchLoss = 0.6399, diffLoss = 3.1420, kgLoss = 0.0144
2025-04-08 13:05:55.396652: Training Step 220/354: batchLoss = 0.7077, diffLoss = 3.4727, kgLoss = 0.0164
2025-04-08 13:05:57.011700: Training Step 221/354: batchLoss = 0.8224, diffLoss = 4.0403, kgLoss = 0.0180
2025-04-08 13:05:58.641559: Training Step 222/354: batchLoss = 0.7141, diffLoss = 3.5070, kgLoss = 0.0159
2025-04-08 13:06:00.254732: Training Step 223/354: batchLoss = 0.5776, diffLoss = 2.8324, kgLoss = 0.0139
2025-04-08 13:06:01.866335: Training Step 224/354: batchLoss = 0.7753, diffLoss = 3.8114, kgLoss = 0.0163
2025-04-08 13:06:03.474548: Training Step 225/354: batchLoss = 0.7391, diffLoss = 3.6225, kgLoss = 0.0182
2025-04-08 13:06:05.099482: Training Step 226/354: batchLoss = 0.6548, diffLoss = 3.2154, kgLoss = 0.0147
2025-04-08 13:06:06.722286: Training Step 227/354: batchLoss = 0.6189, diffLoss = 3.0403, kgLoss = 0.0136
2025-04-08 13:06:08.339784: Training Step 228/354: batchLoss = 0.9771, diffLoss = 4.8064, kgLoss = 0.0197
2025-04-08 13:06:09.960005: Training Step 229/354: batchLoss = 0.5524, diffLoss = 2.7148, kgLoss = 0.0118
2025-04-08 13:06:11.582064: Training Step 230/354: batchLoss = 0.8574, diffLoss = 4.2169, kgLoss = 0.0176
2025-04-08 13:06:13.208132: Training Step 231/354: batchLoss = 0.6505, diffLoss = 3.1974, kgLoss = 0.0137
2025-04-08 13:06:14.822943: Training Step 232/354: batchLoss = 0.6946, diffLoss = 3.4110, kgLoss = 0.0155
2025-04-08 13:06:16.434557: Training Step 233/354: batchLoss = 0.5774, diffLoss = 2.8296, kgLoss = 0.0143
2025-04-08 13:06:18.052567: Training Step 234/354: batchLoss = 0.6075, diffLoss = 2.9843, kgLoss = 0.0133
2025-04-08 13:06:19.665107: Training Step 235/354: batchLoss = 0.5405, diffLoss = 2.6449, kgLoss = 0.0144
2025-04-08 13:06:21.291152: Training Step 236/354: batchLoss = 0.6264, diffLoss = 3.0764, kgLoss = 0.0139
2025-04-08 13:06:22.927741: Training Step 237/354: batchLoss = 0.7953, diffLoss = 3.9095, kgLoss = 0.0167
2025-04-08 13:06:24.552546: Training Step 238/354: batchLoss = 0.5524, diffLoss = 2.7088, kgLoss = 0.0133
2025-04-08 13:06:26.177878: Training Step 239/354: batchLoss = 0.6374, diffLoss = 3.1276, kgLoss = 0.0149
2025-04-08 13:06:27.792934: Training Step 240/354: batchLoss = 0.7827, diffLoss = 3.8485, kgLoss = 0.0163
2025-04-08 13:06:29.409482: Training Step 241/354: batchLoss = 0.6125, diffLoss = 3.0061, kgLoss = 0.0141
2025-04-08 13:06:31.019870: Training Step 242/354: batchLoss = 0.9067, diffLoss = 4.4632, kgLoss = 0.0176
2025-04-08 13:06:32.625378: Training Step 243/354: batchLoss = 0.5516, diffLoss = 2.7082, kgLoss = 0.0124
2025-04-08 13:06:34.250952: Training Step 244/354: batchLoss = 0.6251, diffLoss = 3.0685, kgLoss = 0.0142
2025-04-08 13:06:35.875236: Training Step 245/354: batchLoss = 0.7513, diffLoss = 3.6872, kgLoss = 0.0174
2025-04-08 13:06:37.493279: Training Step 246/354: batchLoss = 0.6167, diffLoss = 3.0342, kgLoss = 0.0123
2025-04-08 13:06:39.114543: Training Step 247/354: batchLoss = 0.5737, diffLoss = 2.8190, kgLoss = 0.0124
2025-04-08 13:06:40.734389: Training Step 248/354: batchLoss = 0.7872, diffLoss = 3.8679, kgLoss = 0.0170
2025-04-08 13:06:42.355363: Training Step 249/354: batchLoss = 0.5830, diffLoss = 2.8630, kgLoss = 0.0130
2025-04-08 13:06:43.966579: Training Step 250/354: batchLoss = 0.6196, diffLoss = 3.0403, kgLoss = 0.0144
2025-04-08 13:06:45.578240: Training Step 251/354: batchLoss = 0.5835, diffLoss = 2.8712, kgLoss = 0.0116
2025-04-08 13:06:47.183916: Training Step 252/354: batchLoss = 0.6533, diffLoss = 3.2113, kgLoss = 0.0138
2025-04-08 13:06:48.803763: Training Step 253/354: batchLoss = 0.6248, diffLoss = 3.0675, kgLoss = 0.0141
2025-04-08 13:06:50.419255: Training Step 254/354: batchLoss = 0.6149, diffLoss = 3.0224, kgLoss = 0.0130
2025-04-08 13:06:52.040179: Training Step 255/354: batchLoss = 0.6446, diffLoss = 3.1653, kgLoss = 0.0144
2025-04-08 13:06:53.670382: Training Step 256/354: batchLoss = 0.6840, diffLoss = 3.3652, kgLoss = 0.0138
2025-04-08 13:06:55.295747: Training Step 257/354: batchLoss = 0.5745, diffLoss = 2.8194, kgLoss = 0.0133
2025-04-08 13:06:56.915463: Training Step 258/354: batchLoss = 0.6414, diffLoss = 3.1533, kgLoss = 0.0134
2025-04-08 13:06:58.526664: Training Step 259/354: batchLoss = 0.7233, diffLoss = 3.5549, kgLoss = 0.0154
2025-04-08 13:07:00.132843: Training Step 260/354: batchLoss = 0.5510, diffLoss = 2.7014, kgLoss = 0.0134
2025-04-08 13:07:01.738508: Training Step 261/354: batchLoss = 0.5745, diffLoss = 2.8194, kgLoss = 0.0133
2025-04-08 13:07:03.363454: Training Step 262/354: batchLoss = 0.6374, diffLoss = 3.1275, kgLoss = 0.0149
2025-04-08 13:07:04.991698: Training Step 263/354: batchLoss = 0.7092, diffLoss = 3.4816, kgLoss = 0.0161
2025-04-08 13:07:06.625224: Training Step 264/354: batchLoss = 0.6139, diffLoss = 3.0162, kgLoss = 0.0133
2025-04-08 13:07:08.243940: Training Step 265/354: batchLoss = 0.5594, diffLoss = 2.7421, kgLoss = 0.0137
2025-04-08 13:07:09.868473: Training Step 266/354: batchLoss = 0.6531, diffLoss = 3.2097, kgLoss = 0.0139
2025-04-08 13:07:11.485693: Training Step 267/354: batchLoss = 0.7942, diffLoss = 3.9065, kgLoss = 0.0161
2025-04-08 13:07:13.098098: Training Step 268/354: batchLoss = 0.6287, diffLoss = 3.0929, kgLoss = 0.0126
2025-04-08 13:07:14.707396: Training Step 269/354: batchLoss = 0.5792, diffLoss = 2.8384, kgLoss = 0.0144
2025-04-08 13:07:16.319568: Training Step 270/354: batchLoss = 0.6245, diffLoss = 3.0674, kgLoss = 0.0137
2025-04-08 13:07:17.944231: Training Step 271/354: batchLoss = 0.6547, diffLoss = 3.2178, kgLoss = 0.0139
2025-04-08 13:07:19.573245: Training Step 272/354: batchLoss = 0.7921, diffLoss = 3.8960, kgLoss = 0.0162
2025-04-08 13:07:21.193030: Training Step 273/354: batchLoss = 0.7132, diffLoss = 3.5007, kgLoss = 0.0163
2025-04-08 13:07:22.824449: Training Step 274/354: batchLoss = 0.5052, diffLoss = 2.4786, kgLoss = 0.0118
2025-04-08 13:07:24.443850: Training Step 275/354: batchLoss = 0.7083, diffLoss = 3.4810, kgLoss = 0.0152
2025-04-08 13:07:26.065121: Training Step 276/354: batchLoss = 0.5828, diffLoss = 2.8619, kgLoss = 0.0131
2025-04-08 13:07:27.674670: Training Step 277/354: batchLoss = 0.5748, diffLoss = 2.8200, kgLoss = 0.0135
2025-04-08 13:07:29.298711: Training Step 278/354: batchLoss = 0.6160, diffLoss = 3.0258, kgLoss = 0.0136
2025-04-08 13:07:30.922846: Training Step 279/354: batchLoss = 0.8217, diffLoss = 4.0412, kgLoss = 0.0168
2025-04-08 13:07:32.548678: Training Step 280/354: batchLoss = 0.6482, diffLoss = 3.1862, kgLoss = 0.0137
2025-04-08 13:07:34.179070: Training Step 281/354: batchLoss = 0.5788, diffLoss = 2.8381, kgLoss = 0.0140
2025-04-08 13:07:35.798728: Training Step 282/354: batchLoss = 0.6801, diffLoss = 3.3439, kgLoss = 0.0141
2025-04-08 13:07:37.416944: Training Step 283/354: batchLoss = 0.5541, diffLoss = 2.7138, kgLoss = 0.0142
2025-04-08 13:07:39.035319: Training Step 284/354: batchLoss = 0.5158, diffLoss = 2.5324, kgLoss = 0.0117
2025-04-08 13:07:40.648753: Training Step 285/354: batchLoss = 0.6375, diffLoss = 3.1324, kgLoss = 0.0138
2025-04-08 13:07:42.246132: Training Step 286/354: batchLoss = 0.4738, diffLoss = 2.3276, kgLoss = 0.0103
2025-04-08 13:07:43.846728: Training Step 287/354: batchLoss = 0.8207, diffLoss = 4.0317, kgLoss = 0.0180
2025-04-08 13:07:45.459111: Training Step 288/354: batchLoss = 0.7579, diffLoss = 3.7254, kgLoss = 0.0160
2025-04-08 13:07:47.072481: Training Step 289/354: batchLoss = 0.7231, diffLoss = 3.5485, kgLoss = 0.0167
2025-04-08 13:07:48.706977: Training Step 290/354: batchLoss = 0.7073, diffLoss = 3.4784, kgLoss = 0.0145
2025-04-08 13:07:50.342775: Training Step 291/354: batchLoss = 0.5055, diffLoss = 2.4817, kgLoss = 0.0115
2025-04-08 13:07:51.960611: Training Step 292/354: batchLoss = 0.6127, diffLoss = 3.0084, kgLoss = 0.0137
2025-04-08 13:07:53.576646: Training Step 293/354: batchLoss = 0.6410, diffLoss = 3.1468, kgLoss = 0.0145
2025-04-08 13:07:55.202849: Training Step 294/354: batchLoss = 0.5664, diffLoss = 2.7832, kgLoss = 0.0122
2025-04-08 13:07:56.821959: Training Step 295/354: batchLoss = 0.5957, diffLoss = 2.9214, kgLoss = 0.0143
2025-04-08 13:07:58.427700: Training Step 296/354: batchLoss = 0.6546, diffLoss = 3.2161, kgLoss = 0.0142
2025-04-08 13:08:00.038403: Training Step 297/354: batchLoss = 0.5734, diffLoss = 2.8169, kgLoss = 0.0126
2025-04-08 13:08:01.660824: Training Step 298/354: batchLoss = 0.6819, diffLoss = 3.3532, kgLoss = 0.0141
2025-04-08 13:08:03.288393: Training Step 299/354: batchLoss = 0.8359, diffLoss = 4.1075, kgLoss = 0.0180
2025-04-08 13:08:04.907848: Training Step 300/354: batchLoss = 0.5695, diffLoss = 2.7888, kgLoss = 0.0147
2025-04-08 13:08:06.538495: Training Step 301/354: batchLoss = 0.6604, diffLoss = 3.2462, kgLoss = 0.0140
2025-04-08 13:08:08.167259: Training Step 302/354: batchLoss = 0.4912, diffLoss = 2.4030, kgLoss = 0.0132
2025-04-08 13:08:09.785714: Training Step 303/354: batchLoss = 0.7254, diffLoss = 3.5677, kgLoss = 0.0148
2025-04-08 13:08:11.407043: Training Step 304/354: batchLoss = 0.7778, diffLoss = 3.8225, kgLoss = 0.0166
2025-04-08 13:08:13.013478: Training Step 305/354: batchLoss = 0.7225, diffLoss = 3.5509, kgLoss = 0.0154
2025-04-08 13:08:14.624169: Training Step 306/354: batchLoss = 0.5634, diffLoss = 2.7708, kgLoss = 0.0116
2025-04-08 13:08:16.225677: Training Step 307/354: batchLoss = 0.6865, diffLoss = 3.3693, kgLoss = 0.0159
2025-04-08 13:08:17.847694: Training Step 308/354: batchLoss = 0.5345, diffLoss = 2.6159, kgLoss = 0.0142
2025-04-08 13:08:19.478135: Training Step 309/354: batchLoss = 0.6415, diffLoss = 3.1529, kgLoss = 0.0137
2025-04-08 13:08:21.106144: Training Step 310/354: batchLoss = 0.6568, diffLoss = 3.2317, kgLoss = 0.0130
2025-04-08 13:08:22.734904: Training Step 311/354: batchLoss = 0.8714, diffLoss = 4.2808, kgLoss = 0.0190
2025-04-08 13:08:24.364022: Training Step 312/354: batchLoss = 0.6239, diffLoss = 3.0540, kgLoss = 0.0164
2025-04-08 13:08:25.982751: Training Step 313/354: batchLoss = 0.6096, diffLoss = 2.9939, kgLoss = 0.0135
2025-04-08 13:08:27.588643: Training Step 314/354: batchLoss = 0.7508, diffLoss = 3.6910, kgLoss = 0.0158
2025-04-08 13:08:29.195608: Training Step 315/354: batchLoss = 0.6281, diffLoss = 3.0897, kgLoss = 0.0127
2025-04-08 13:08:30.809036: Training Step 316/354: batchLoss = 0.5728, diffLoss = 2.8110, kgLoss = 0.0132
2025-04-08 13:08:32.425607: Training Step 317/354: batchLoss = 0.6033, diffLoss = 2.9621, kgLoss = 0.0136
2025-04-08 13:08:34.049354: Training Step 318/354: batchLoss = 0.5747, diffLoss = 2.8196, kgLoss = 0.0134
2025-04-08 13:08:35.667229: Training Step 319/354: batchLoss = 0.5551, diffLoss = 2.7284, kgLoss = 0.0118
2025-04-08 13:08:37.294866: Training Step 320/354: batchLoss = 0.6419, diffLoss = 3.1536, kgLoss = 0.0139
2025-04-08 13:08:38.919892: Training Step 321/354: batchLoss = 0.7269, diffLoss = 3.5711, kgLoss = 0.0159
2025-04-08 13:08:40.546029: Training Step 322/354: batchLoss = 0.6917, diffLoss = 3.3993, kgLoss = 0.0148
2025-04-08 13:08:42.160973: Training Step 323/354: batchLoss = 0.8699, diffLoss = 4.2779, kgLoss = 0.0179
2025-04-08 13:08:43.770861: Training Step 324/354: batchLoss = 0.6789, diffLoss = 3.3376, kgLoss = 0.0143
2025-04-08 13:08:45.388580: Training Step 325/354: batchLoss = 0.5851, diffLoss = 2.8729, kgLoss = 0.0132
2025-04-08 13:08:47.011196: Training Step 326/354: batchLoss = 0.5717, diffLoss = 2.8048, kgLoss = 0.0134
2025-04-08 13:08:48.635578: Training Step 327/354: batchLoss = 0.5188, diffLoss = 2.5421, kgLoss = 0.0130
2025-04-08 13:08:50.255193: Training Step 328/354: batchLoss = 0.6759, diffLoss = 3.3190, kgLoss = 0.0151
2025-04-08 13:08:51.872523: Training Step 329/354: batchLoss = 0.5486, diffLoss = 2.6946, kgLoss = 0.0121
2025-04-08 13:08:53.490985: Training Step 330/354: batchLoss = 0.8274, diffLoss = 4.0675, kgLoss = 0.0174
2025-04-08 13:08:55.110264: Training Step 331/354: batchLoss = 0.7747, diffLoss = 3.7984, kgLoss = 0.0188
2025-04-08 13:08:56.727264: Training Step 332/354: batchLoss = 0.8110, diffLoss = 3.9855, kgLoss = 0.0174
2025-04-08 13:08:58.337400: Training Step 333/354: batchLoss = 0.5759, diffLoss = 2.8266, kgLoss = 0.0132
2025-04-08 13:08:59.945267: Training Step 334/354: batchLoss = 0.5719, diffLoss = 2.8053, kgLoss = 0.0136
2025-04-08 13:09:01.564175: Training Step 335/354: batchLoss = 0.6369, diffLoss = 3.1327, kgLoss = 0.0130
2025-04-08 13:09:03.187984: Training Step 336/354: batchLoss = 0.6856, diffLoss = 3.3713, kgLoss = 0.0142
2025-04-08 13:09:04.806519: Training Step 337/354: batchLoss = 1.2225, diffLoss = 6.0186, kgLoss = 0.0235
2025-04-08 13:09:06.420922: Training Step 338/354: batchLoss = 0.8182, diffLoss = 4.0198, kgLoss = 0.0178
2025-04-08 13:09:08.048489: Training Step 339/354: batchLoss = 0.6269, diffLoss = 3.0787, kgLoss = 0.0140
2025-04-08 13:09:09.672208: Training Step 340/354: batchLoss = 0.5593, diffLoss = 2.7418, kgLoss = 0.0136
2025-04-08 13:09:11.278156: Training Step 341/354: batchLoss = 0.5975, diffLoss = 2.9359, kgLoss = 0.0129
2025-04-08 13:09:12.887354: Training Step 342/354: batchLoss = 0.6910, diffLoss = 3.3935, kgLoss = 0.0153
2025-04-08 13:09:14.514061: Training Step 343/354: batchLoss = 0.4909, diffLoss = 2.4082, kgLoss = 0.0116
2025-04-08 13:09:16.132341: Training Step 344/354: batchLoss = 0.5555, diffLoss = 2.7280, kgLoss = 0.0124
2025-04-08 13:09:17.746154: Training Step 345/354: batchLoss = 0.7045, diffLoss = 3.4610, kgLoss = 0.0153
2025-04-08 13:09:19.367203: Training Step 346/354: batchLoss = 0.5291, diffLoss = 2.5904, kgLoss = 0.0138
2025-04-08 13:09:20.983461: Training Step 347/354: batchLoss = 0.5342, diffLoss = 2.6205, kgLoss = 0.0127
2025-04-08 13:09:22.601053: Training Step 348/354: batchLoss = 0.6178, diffLoss = 3.0387, kgLoss = 0.0126
2025-04-08 13:09:24.223736: Training Step 349/354: batchLoss = 0.5693, diffLoss = 2.7972, kgLoss = 0.0123
2025-04-08 13:09:25.829854: Training Step 350/354: batchLoss = 0.6535, diffLoss = 3.2083, kgLoss = 0.0148
2025-04-08 13:09:27.456928: Training Step 351/354: batchLoss = 0.7274, diffLoss = 3.5755, kgLoss = 0.0154
2025-04-08 13:09:29.056764: Training Step 352/354: batchLoss = 0.6549, diffLoss = 3.2156, kgLoss = 0.0147
2025-04-08 13:09:30.470266: Training Step 353/354: batchLoss = 0.4805, diffLoss = 2.3637, kgLoss = 0.0097
2025-04-08 13:09:30.562307: 
2025-04-08 13:09:30.563108: Epoch 2/1000, Train: epLoss = 1.1685, epDfLoss = 5.7386, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 13:09:31.925101: Steps 0/138: batch_recall = 41.46, batch_ndcg = 23.41 
2025-04-08 13:09:33.250973: Steps 1/138: batch_recall = 38.92, batch_ndcg = 23.22 
2025-04-08 13:09:34.583617: Steps 2/138: batch_recall = 49.04, batch_ndcg = 30.48 
2025-04-08 13:09:35.925087: Steps 3/138: batch_recall = 48.21, batch_ndcg = 26.66 
2025-04-08 13:09:37.257369: Steps 4/138: batch_recall = 53.06, batch_ndcg = 35.09 
2025-04-08 13:09:38.597061: Steps 5/138: batch_recall = 43.45, batch_ndcg = 24.10 
2025-04-08 13:09:39.959825: Steps 6/138: batch_recall = 41.74, batch_ndcg = 24.15 
2025-04-08 13:09:41.276524: Steps 7/138: batch_recall = 54.33, batch_ndcg = 34.38 
2025-04-08 13:09:42.578034: Steps 8/138: batch_recall = 51.79, batch_ndcg = 32.48 
2025-04-08 13:09:43.859495: Steps 9/138: batch_recall = 43.69, batch_ndcg = 26.16 
2025-04-08 13:09:45.142615: Steps 10/138: batch_recall = 41.58, batch_ndcg = 23.84 
2025-04-08 13:09:46.442631: Steps 11/138: batch_recall = 55.18, batch_ndcg = 31.82 
2025-04-08 13:09:47.740679: Steps 12/138: batch_recall = 45.31, batch_ndcg = 25.83 
2025-04-08 13:09:49.027285: Steps 13/138: batch_recall = 39.33, batch_ndcg = 23.82 
2025-04-08 13:09:50.315642: Steps 14/138: batch_recall = 40.76, batch_ndcg = 23.92 
2025-04-08 13:09:51.603596: Steps 15/138: batch_recall = 46.68, batch_ndcg = 27.74 
2025-04-08 13:09:52.908750: Steps 16/138: batch_recall = 46.93, batch_ndcg = 26.74 
2025-04-08 13:09:54.199002: Steps 17/138: batch_recall = 50.23, batch_ndcg = 26.87 
2025-04-08 13:09:55.496024: Steps 18/138: batch_recall = 40.93, batch_ndcg = 27.01 
2025-04-08 13:09:56.774263: Steps 19/138: batch_recall = 47.07, batch_ndcg = 27.19 
2025-04-08 13:09:58.061281: Steps 20/138: batch_recall = 52.05, batch_ndcg = 29.92 
2025-04-08 13:09:59.346598: Steps 21/138: batch_recall = 56.59, batch_ndcg = 35.86 
2025-04-08 13:10:00.652428: Steps 22/138: batch_recall = 48.51, batch_ndcg = 28.60 
2025-04-08 13:10:01.944160: Steps 23/138: batch_recall = 41.49, batch_ndcg = 23.70 
2025-04-08 13:10:03.245110: Steps 24/138: batch_recall = 41.72, batch_ndcg = 25.77 
2025-04-08 13:10:04.546677: Steps 25/138: batch_recall = 46.18, batch_ndcg = 26.86 
2025-04-08 13:10:05.851730: Steps 26/138: batch_recall = 48.69, batch_ndcg = 27.45 
2025-04-08 13:10:07.145848: Steps 27/138: batch_recall = 52.92, batch_ndcg = 31.95 
2025-04-08 13:10:08.438925: Steps 28/138: batch_recall = 50.96, batch_ndcg = 27.10 
2025-04-08 13:10:09.741871: Steps 29/138: batch_recall = 49.86, batch_ndcg = 26.43 
2025-04-08 13:10:11.031996: Steps 30/138: batch_recall = 48.99, batch_ndcg = 30.60 
2025-04-08 13:10:12.298680: Steps 31/138: batch_recall = 34.85, batch_ndcg = 20.78 
2025-04-08 13:10:13.583360: Steps 32/138: batch_recall = 46.40, batch_ndcg = 27.30 
2025-04-08 13:10:14.849663: Steps 33/138: batch_recall = 50.00, batch_ndcg = 28.60 
2025-04-08 13:10:16.138271: Steps 34/138: batch_recall = 43.89, batch_ndcg = 23.42 
2025-04-08 13:10:17.455004: Steps 35/138: batch_recall = 41.12, batch_ndcg = 24.72 
2025-04-08 13:10:18.734731: Steps 36/138: batch_recall = 42.76, batch_ndcg = 24.31 
2025-04-08 13:10:20.026303: Steps 37/138: batch_recall = 57.13, batch_ndcg = 30.28 
2025-04-08 13:10:21.317236: Steps 38/138: batch_recall = 48.77, batch_ndcg = 30.19 
2025-04-08 13:10:22.600972: Steps 39/138: batch_recall = 65.25, batch_ndcg = 35.62 
2025-04-08 13:10:23.889313: Steps 40/138: batch_recall = 48.22, batch_ndcg = 24.50 
2025-04-08 13:10:25.164827: Steps 41/138: batch_recall = 50.42, batch_ndcg = 29.20 
2025-04-08 13:10:26.423711: Steps 42/138: batch_recall = 50.26, batch_ndcg = 27.42 
2025-04-08 13:10:27.708708: Steps 43/138: batch_recall = 52.68, batch_ndcg = 32.97 
2025-04-08 13:10:28.987633: Steps 44/138: batch_recall = 45.64, batch_ndcg = 25.33 
2025-04-08 13:10:30.250982: Steps 45/138: batch_recall = 54.79, batch_ndcg = 29.19 
2025-04-08 13:10:31.519776: Steps 46/138: batch_recall = 52.08, batch_ndcg = 30.11 
2025-04-08 13:10:32.801511: Steps 47/138: batch_recall = 50.20, batch_ndcg = 26.81 
2025-04-08 13:10:34.074055: Steps 48/138: batch_recall = 46.99, batch_ndcg = 26.80 
2025-04-08 13:10:35.362045: Steps 49/138: batch_recall = 51.83, batch_ndcg = 32.53 
2025-04-08 13:10:36.642104: Steps 50/138: batch_recall = 50.49, batch_ndcg = 27.43 
2025-04-08 13:10:37.930983: Steps 51/138: batch_recall = 47.28, batch_ndcg = 28.25 
2025-04-08 13:10:39.210774: Steps 52/138: batch_recall = 62.21, batch_ndcg = 36.25 
2025-04-08 13:10:40.488787: Steps 53/138: batch_recall = 47.64, batch_ndcg = 24.70 
2025-04-08 13:10:41.740251: Steps 54/138: batch_recall = 48.82, batch_ndcg = 27.27 
2025-04-08 13:10:43.004551: Steps 55/138: batch_recall = 50.29, batch_ndcg = 26.74 
2025-04-08 13:10:44.267071: Steps 56/138: batch_recall = 53.48, batch_ndcg = 30.48 
2025-04-08 13:10:45.559449: Steps 57/138: batch_recall = 49.90, batch_ndcg = 27.51 
2025-04-08 13:10:46.855417: Steps 58/138: batch_recall = 60.50, batch_ndcg = 31.61 
2025-04-08 13:10:48.148751: Steps 59/138: batch_recall = 64.53, batch_ndcg = 33.65 
2025-04-08 13:10:49.421478: Steps 60/138: batch_recall = 66.81, batch_ndcg = 34.03 
2025-04-08 13:10:50.707607: Steps 61/138: batch_recall = 53.97, batch_ndcg = 28.73 
2025-04-08 13:10:51.982917: Steps 62/138: batch_recall = 69.22, batch_ndcg = 38.39 
2025-04-08 13:10:53.273004: Steps 63/138: batch_recall = 64.73, batch_ndcg = 37.05 
2025-04-08 13:10:54.562909: Steps 64/138: batch_recall = 51.62, batch_ndcg = 27.41 
2025-04-08 13:10:55.834478: Steps 65/138: batch_recall = 75.55, batch_ndcg = 40.90 
2025-04-08 13:10:57.085050: Steps 66/138: batch_recall = 58.02, batch_ndcg = 34.44 
2025-04-08 13:10:58.337839: Steps 67/138: batch_recall = 77.87, batch_ndcg = 44.95 
2025-04-08 13:10:59.599485: Steps 68/138: batch_recall = 54.53, batch_ndcg = 29.63 
2025-04-08 13:11:00.878864: Steps 69/138: batch_recall = 82.31, batch_ndcg = 46.11 
2025-04-08 13:11:02.151636: Steps 70/138: batch_recall = 64.78, batch_ndcg = 36.47 
2025-04-08 13:11:03.447485: Steps 71/138: batch_recall = 73.70, batch_ndcg = 41.89 
2025-04-08 13:11:04.737254: Steps 72/138: batch_recall = 79.77, batch_ndcg = 45.43 
2025-04-08 13:11:06.009859: Steps 73/138: batch_recall = 75.23, batch_ndcg = 40.20 
2025-04-08 13:11:07.316594: Steps 74/138: batch_recall = 70.57, batch_ndcg = 42.06 
2025-04-08 13:11:08.613998: Steps 75/138: batch_recall = 75.05, batch_ndcg = 43.10 
2025-04-08 13:11:09.889157: Steps 76/138: batch_recall = 87.11, batch_ndcg = 50.84 
2025-04-08 13:11:11.151309: Steps 77/138: batch_recall = 74.17, batch_ndcg = 41.97 
2025-04-08 13:11:12.422462: Steps 78/138: batch_recall = 83.41, batch_ndcg = 42.34 
2025-04-08 13:11:13.688502: Steps 79/138: batch_recall = 76.64, batch_ndcg = 42.96 
2025-04-08 13:11:14.949923: Steps 80/138: batch_recall = 72.59, batch_ndcg = 37.72 
2025-04-08 13:11:16.229010: Steps 81/138: batch_recall = 79.70, batch_ndcg = 44.61 
2025-04-08 13:11:17.504072: Steps 82/138: batch_recall = 76.86, batch_ndcg = 43.70 
2025-04-08 13:11:18.780402: Steps 83/138: batch_recall = 70.38, batch_ndcg = 39.62 
2025-04-08 13:11:20.066744: Steps 84/138: batch_recall = 99.08, batch_ndcg = 51.80 
2025-04-08 13:11:21.331724: Steps 85/138: batch_recall = 86.34, batch_ndcg = 47.91 
2025-04-08 13:11:22.610753: Steps 86/138: batch_recall = 101.81, batch_ndcg = 59.94 
2025-04-08 13:11:23.890928: Steps 87/138: batch_recall = 90.01, batch_ndcg = 49.67 
2025-04-08 13:11:25.173154: Steps 88/138: batch_recall = 98.92, batch_ndcg = 52.74 
2025-04-08 13:11:26.432282: Steps 89/138: batch_recall = 107.62, batch_ndcg = 58.76 
2025-04-08 13:11:27.700310: Steps 90/138: batch_recall = 94.71, batch_ndcg = 50.53 
2025-04-08 13:11:28.949785: Steps 91/138: batch_recall = 114.14, batch_ndcg = 59.97 
2025-04-08 13:11:30.225237: Steps 92/138: batch_recall = 94.58, batch_ndcg = 52.80 
2025-04-08 13:11:31.495307: Steps 93/138: batch_recall = 108.61, batch_ndcg = 61.09 
2025-04-08 13:11:32.766591: Steps 94/138: batch_recall = 106.31, batch_ndcg = 52.55 
2025-04-08 13:11:34.040602: Steps 95/138: batch_recall = 108.58, batch_ndcg = 61.32 
2025-04-08 13:11:35.325686: Steps 96/138: batch_recall = 127.45, batch_ndcg = 68.11 
2025-04-08 13:11:36.593656: Steps 97/138: batch_recall = 140.65, batch_ndcg = 72.68 
2025-04-08 13:11:37.878489: Steps 98/138: batch_recall = 95.22, batch_ndcg = 51.93 
2025-04-08 13:11:39.146327: Steps 99/138: batch_recall = 105.82, batch_ndcg = 59.33 
2025-04-08 13:11:40.420076: Steps 100/138: batch_recall = 105.79, batch_ndcg = 60.84 
2025-04-08 13:11:41.706944: Steps 101/138: batch_recall = 122.18, batch_ndcg = 64.80 
2025-04-08 13:11:42.986895: Steps 102/138: batch_recall = 112.04, batch_ndcg = 61.97 
2025-04-08 13:11:44.265208: Steps 103/138: batch_recall = 119.92, batch_ndcg = 66.76 
2025-04-08 13:11:45.533968: Steps 104/138: batch_recall = 118.42, batch_ndcg = 62.25 
2025-04-08 13:11:46.800850: Steps 105/138: batch_recall = 112.33, batch_ndcg = 61.46 
2025-04-08 13:11:48.057613: Steps 106/138: batch_recall = 97.28, batch_ndcg = 52.11 
2025-04-08 13:11:49.315682: Steps 107/138: batch_recall = 106.62, batch_ndcg = 54.17 
2025-04-08 13:11:50.586991: Steps 108/138: batch_recall = 106.67, batch_ndcg = 61.61 
2025-04-08 13:11:51.861078: Steps 109/138: batch_recall = 119.73, batch_ndcg = 62.77 
2025-04-08 13:11:53.143034: Steps 110/138: batch_recall = 109.99, batch_ndcg = 55.66 
2025-04-08 13:11:54.417039: Steps 111/138: batch_recall = 121.24, batch_ndcg = 68.29 
2025-04-08 13:11:55.697531: Steps 112/138: batch_recall = 157.03, batch_ndcg = 77.86 
2025-04-08 13:11:56.987201: Steps 113/138: batch_recall = 120.54, batch_ndcg = 64.08 
2025-04-08 13:11:58.262245: Steps 114/138: batch_recall = 106.06, batch_ndcg = 59.36 
2025-04-08 13:12:01.024444: Steps 115/138: batch_recall = 97.86, batch_ndcg = 50.03 
2025-04-08 13:12:04.969982: Steps 116/138: batch_recall = 106.48, batch_ndcg = 53.47 
2025-04-08 13:12:06.247693: Steps 117/138: batch_recall = 105.84, batch_ndcg = 58.78 
2025-04-08 13:12:07.524393: Steps 118/138: batch_recall = 118.46, batch_ndcg = 65.30 
2025-04-08 13:12:08.964139: Steps 119/138: batch_recall = 119.38, batch_ndcg = 63.71 
2025-04-08 13:12:10.235814: Steps 120/138: batch_recall = 110.90, batch_ndcg = 59.81 
2025-04-08 13:12:11.516949: Steps 121/138: batch_recall = 128.33, batch_ndcg = 65.52 
2025-04-08 13:12:12.798002: Steps 122/138: batch_recall = 120.19, batch_ndcg = 64.34 
2025-04-08 13:12:14.080519: Steps 123/138: batch_recall = 109.92, batch_ndcg = 60.73 
2025-04-08 13:12:15.331919: Steps 124/138: batch_recall = 134.44, batch_ndcg = 83.85 
2025-04-08 13:12:16.613134: Steps 125/138: batch_recall = 115.00, batch_ndcg = 58.32 
2025-04-08 13:12:17.885825: Steps 126/138: batch_recall = 145.88, batch_ndcg = 78.72 
2025-04-08 13:12:19.143997: Steps 127/138: batch_recall = 118.27, batch_ndcg = 69.38 
2025-04-08 13:12:20.411466: Steps 128/138: batch_recall = 113.72, batch_ndcg = 56.71 
2025-04-08 13:12:21.691243: Steps 129/138: batch_recall = 131.04, batch_ndcg = 77.96 
2025-04-08 13:12:22.975472: Steps 130/138: batch_recall = 112.88, batch_ndcg = 59.09 
2025-04-08 13:12:24.265359: Steps 131/138: batch_recall = 120.08, batch_ndcg = 70.65 
2025-04-08 13:12:25.555075: Steps 132/138: batch_recall = 130.47, batch_ndcg = 72.41 
2025-04-08 13:12:26.840322: Steps 133/138: batch_recall = 131.56, batch_ndcg = 73.23 
2025-04-08 13:12:28.115620: Steps 134/138: batch_recall = 115.51, batch_ndcg = 64.47 
2025-04-08 13:12:29.397307: Steps 135/138: batch_recall = 154.23, batch_ndcg = 83.75 
2025-04-08 13:12:30.656453: Steps 136/138: batch_recall = 138.50, batch_ndcg = 71.55 
2025-04-08 13:12:31.923234: Steps 137/138: batch_recall = 125.17, batch_ndcg = 77.26 
2025-04-08 13:12:31.923750: Epoch 2/1000, Test: Recall = 0.1539, NDCG = 0.0853  

2025-04-08 13:12:33.679104: Training Step 0/354: batchLoss = 0.5978, diffLoss = 2.9351, kgLoss = 0.0135
2025-04-08 13:12:35.297007: Training Step 1/354: batchLoss = 0.5504, diffLoss = 2.6992, kgLoss = 0.0132
2025-04-08 13:12:36.913412: Training Step 2/354: batchLoss = 0.6967, diffLoss = 3.4297, kgLoss = 0.0134
2025-04-08 13:12:38.544992: Training Step 3/354: batchLoss = 0.7163, diffLoss = 3.5196, kgLoss = 0.0154
2025-04-08 13:12:40.170730: Training Step 4/354: batchLoss = 0.5739, diffLoss = 2.8204, kgLoss = 0.0122
2025-04-08 13:12:41.802475: Training Step 5/354: batchLoss = 0.5507, diffLoss = 2.6981, kgLoss = 0.0138
2025-04-08 13:12:43.423761: Training Step 6/354: batchLoss = 0.6657, diffLoss = 3.2698, kgLoss = 0.0146
2025-04-08 13:12:45.037172: Training Step 7/354: batchLoss = 0.4602, diffLoss = 2.2557, kgLoss = 0.0113
2025-04-08 13:12:46.654212: Training Step 8/354: batchLoss = 1.1769, diffLoss = 5.7881, kgLoss = 0.0241
2025-04-08 13:12:48.267689: Training Step 9/354: batchLoss = 0.6698, diffLoss = 3.2858, kgLoss = 0.0158
2025-04-08 13:12:49.883516: Training Step 10/354: batchLoss = 0.6456, diffLoss = 3.1727, kgLoss = 0.0138
2025-04-08 13:12:51.503393: Training Step 11/354: batchLoss = 0.7257, diffLoss = 3.5697, kgLoss = 0.0147
2025-04-08 13:12:53.133691: Training Step 12/354: batchLoss = 0.7687, diffLoss = 3.7819, kgLoss = 0.0154
2025-04-08 13:12:54.756608: Training Step 13/354: batchLoss = 0.5657, diffLoss = 2.7767, kgLoss = 0.0130
2025-04-08 13:12:56.380316: Training Step 14/354: batchLoss = 0.6011, diffLoss = 2.9515, kgLoss = 0.0136
2025-04-08 13:12:58.008454: Training Step 15/354: batchLoss = 0.8071, diffLoss = 3.9676, kgLoss = 0.0170
2025-04-08 13:12:59.626378: Training Step 16/354: batchLoss = 0.5361, diffLoss = 2.6289, kgLoss = 0.0128
2025-04-08 13:13:01.237391: Training Step 17/354: batchLoss = 0.5837, diffLoss = 2.8636, kgLoss = 0.0137
2025-04-08 13:13:02.853278: Training Step 18/354: batchLoss = 0.6314, diffLoss = 3.0991, kgLoss = 0.0144
2025-04-08 13:13:04.467546: Training Step 19/354: batchLoss = 0.6580, diffLoss = 3.2242, kgLoss = 0.0165
2025-04-08 13:13:06.090818: Training Step 20/354: batchLoss = 0.5324, diffLoss = 2.6138, kgLoss = 0.0121
2025-04-08 13:13:07.706474: Training Step 21/354: batchLoss = 0.7499, diffLoss = 3.6875, kgLoss = 0.0155
2025-04-08 13:13:09.326997: Training Step 22/354: batchLoss = 0.5605, diffLoss = 2.7495, kgLoss = 0.0133
2025-04-08 13:13:10.946640: Training Step 23/354: batchLoss = 0.7159, diffLoss = 3.5091, kgLoss = 0.0177
2025-04-08 13:13:12.569741: Training Step 24/354: batchLoss = 0.5041, diffLoss = 2.4750, kgLoss = 0.0113
2025-04-08 13:13:14.187316: Training Step 25/354: batchLoss = 0.5633, diffLoss = 2.7629, kgLoss = 0.0134
2025-04-08 13:13:15.794852: Training Step 26/354: batchLoss = 0.8132, diffLoss = 4.0023, kgLoss = 0.0159
2025-04-08 13:13:17.414128: Training Step 27/354: batchLoss = 0.6587, diffLoss = 3.2410, kgLoss = 0.0131
2025-04-08 13:13:19.028091: Training Step 28/354: batchLoss = 0.6463, diffLoss = 3.1772, kgLoss = 0.0136
2025-04-08 13:13:20.655526: Training Step 29/354: batchLoss = 0.5634, diffLoss = 2.7568, kgLoss = 0.0151
2025-04-08 13:13:22.285656: Training Step 30/354: batchLoss = 0.5840, diffLoss = 2.8593, kgLoss = 0.0151
2025-04-08 13:13:23.915987: Training Step 31/354: batchLoss = 0.5686, diffLoss = 2.7874, kgLoss = 0.0139
2025-04-08 13:13:25.547013: Training Step 32/354: batchLoss = 0.6680, diffLoss = 3.2736, kgLoss = 0.0166
2025-04-08 13:13:27.165632: Training Step 33/354: batchLoss = 0.7374, diffLoss = 3.6239, kgLoss = 0.0158
2025-04-08 13:13:28.788523: Training Step 34/354: batchLoss = 0.9854, diffLoss = 4.8464, kgLoss = 0.0201
2025-04-08 13:13:30.398001: Training Step 35/354: batchLoss = 0.5789, diffLoss = 2.8425, kgLoss = 0.0130
2025-04-08 13:13:32.017284: Training Step 36/354: batchLoss = 0.7308, diffLoss = 3.5959, kgLoss = 0.0145
2025-04-08 13:13:33.627164: Training Step 37/354: batchLoss = 0.6022, diffLoss = 2.9566, kgLoss = 0.0136
2025-04-08 13:13:35.242846: Training Step 38/354: batchLoss = 0.5953, diffLoss = 2.9162, kgLoss = 0.0151
2025-04-08 13:13:36.861432: Training Step 39/354: batchLoss = 0.7219, diffLoss = 3.5450, kgLoss = 0.0161
2025-04-08 13:13:38.480297: Training Step 40/354: batchLoss = 0.6454, diffLoss = 3.1727, kgLoss = 0.0136
2025-04-08 13:13:40.102837: Training Step 41/354: batchLoss = 0.6514, diffLoss = 3.1887, kgLoss = 0.0171
2025-04-08 13:13:41.727249: Training Step 42/354: batchLoss = 0.5671, diffLoss = 2.7861, kgLoss = 0.0124
2025-04-08 13:13:43.340407: Training Step 43/354: batchLoss = 0.7954, diffLoss = 3.9056, kgLoss = 0.0179
2025-04-08 13:13:44.950230: Training Step 44/354: batchLoss = 0.5404, diffLoss = 2.6509, kgLoss = 0.0128
2025-04-08 13:13:46.559408: Training Step 45/354: batchLoss = 0.5634, diffLoss = 2.7637, kgLoss = 0.0133
2025-04-08 13:13:48.167571: Training Step 46/354: batchLoss = 0.6712, diffLoss = 3.3014, kgLoss = 0.0136
2025-04-08 13:13:49.789821: Training Step 47/354: batchLoss = 0.5347, diffLoss = 2.6268, kgLoss = 0.0117
2025-04-08 13:13:51.406233: Training Step 48/354: batchLoss = 0.4926, diffLoss = 2.4186, kgLoss = 0.0111
2025-04-08 13:13:53.027994: Training Step 49/354: batchLoss = 0.6958, diffLoss = 3.4122, kgLoss = 0.0166
2025-04-08 13:13:54.645483: Training Step 50/354: batchLoss = 0.6593, diffLoss = 3.2446, kgLoss = 0.0129
2025-04-08 13:13:56.267155: Training Step 51/354: batchLoss = 0.6631, diffLoss = 3.2570, kgLoss = 0.0147
2025-04-08 13:13:57.882498: Training Step 52/354: batchLoss = 0.6259, diffLoss = 3.0795, kgLoss = 0.0125
2025-04-08 13:13:59.491887: Training Step 53/354: batchLoss = 0.6839, diffLoss = 3.3629, kgLoss = 0.0141
2025-04-08 13:14:01.098331: Training Step 54/354: batchLoss = 0.6143, diffLoss = 3.0126, kgLoss = 0.0147
2025-04-08 13:14:02.708765: Training Step 55/354: batchLoss = 0.6632, diffLoss = 3.2599, kgLoss = 0.0141
2025-04-08 13:14:04.329091: Training Step 56/354: batchLoss = 0.7149, diffLoss = 3.5042, kgLoss = 0.0176
2025-04-08 13:14:05.956633: Training Step 57/354: batchLoss = 0.6799, diffLoss = 3.3369, kgLoss = 0.0156
2025-04-08 13:14:07.605330: Training Step 58/354: batchLoss = 0.6365, diffLoss = 3.1218, kgLoss = 0.0152
2025-04-08 13:14:09.247707: Training Step 59/354: batchLoss = 0.6872, diffLoss = 3.3760, kgLoss = 0.0150
2025-04-08 13:14:10.893951: Training Step 60/354: batchLoss = 0.6344, diffLoss = 3.1107, kgLoss = 0.0153
2025-04-08 13:14:12.534008: Training Step 61/354: batchLoss = 0.8102, diffLoss = 3.9747, kgLoss = 0.0191
2025-04-08 13:14:14.159551: Training Step 62/354: batchLoss = 0.7608, diffLoss = 3.7333, kgLoss = 0.0177
2025-04-08 13:14:15.784558: Training Step 63/354: batchLoss = 0.7671, diffLoss = 3.7751, kgLoss = 0.0151
2025-04-08 13:14:17.414096: Training Step 64/354: batchLoss = 0.6403, diffLoss = 3.1415, kgLoss = 0.0150
2025-04-08 13:14:19.082793: Training Step 65/354: batchLoss = 0.5722, diffLoss = 2.8147, kgLoss = 0.0116
2025-04-08 13:14:20.735319: Training Step 66/354: batchLoss = 0.6536, diffLoss = 3.2102, kgLoss = 0.0144
2025-04-08 13:14:22.362777: Training Step 67/354: batchLoss = 0.5592, diffLoss = 2.7450, kgLoss = 0.0128
2025-04-08 13:14:23.985025: Training Step 68/354: batchLoss = 0.5825, diffLoss = 2.8641, kgLoss = 0.0121
2025-04-08 13:14:25.608641: Training Step 69/354: batchLoss = 0.4996, diffLoss = 2.4484, kgLoss = 0.0125
2025-04-08 13:14:27.228140: Training Step 70/354: batchLoss = 0.5772, diffLoss = 2.8366, kgLoss = 0.0123
2025-04-08 13:14:28.837687: Training Step 71/354: batchLoss = 0.5944, diffLoss = 2.9119, kgLoss = 0.0150
2025-04-08 13:14:30.450004: Training Step 72/354: batchLoss = 0.5141, diffLoss = 2.5267, kgLoss = 0.0109
2025-04-08 13:14:32.058490: Training Step 73/354: batchLoss = 0.5615, diffLoss = 2.7546, kgLoss = 0.0132
2025-04-08 13:14:33.692107: Training Step 74/354: batchLoss = 0.9446, diffLoss = 4.6491, kgLoss = 0.0184
2025-04-08 13:14:35.314199: Training Step 75/354: batchLoss = 0.6270, diffLoss = 3.0776, kgLoss = 0.0143
2025-04-08 13:14:36.931565: Training Step 76/354: batchLoss = 0.5530, diffLoss = 2.7154, kgLoss = 0.0124
2025-04-08 13:14:38.568572: Training Step 77/354: batchLoss = 0.7451, diffLoss = 3.6656, kgLoss = 0.0150
2025-04-08 13:14:40.203620: Training Step 78/354: batchLoss = 0.6049, diffLoss = 2.9728, kgLoss = 0.0129
2025-04-08 13:14:41.820077: Training Step 79/354: batchLoss = 0.5708, diffLoss = 2.7984, kgLoss = 0.0139
2025-04-08 13:14:43.429528: Training Step 80/354: batchLoss = 0.7623, diffLoss = 3.7441, kgLoss = 0.0169
2025-04-08 13:14:45.038802: Training Step 81/354: batchLoss = 0.5985, diffLoss = 2.9409, kgLoss = 0.0129
2025-04-08 13:14:46.654233: Training Step 82/354: batchLoss = 0.7298, diffLoss = 3.5874, kgLoss = 0.0154
2025-04-08 13:14:48.277356: Training Step 83/354: batchLoss = 0.7083, diffLoss = 3.4752, kgLoss = 0.0166
2025-04-08 13:14:49.904493: Training Step 84/354: batchLoss = 0.8248, diffLoss = 4.0570, kgLoss = 0.0168
2025-04-08 13:14:51.538299: Training Step 85/354: batchLoss = 0.6755, diffLoss = 3.3163, kgLoss = 0.0153
2025-04-08 13:14:53.157964: Training Step 86/354: batchLoss = 0.6790, diffLoss = 3.3371, kgLoss = 0.0145
2025-04-08 13:14:54.790792: Training Step 87/354: batchLoss = 0.6336, diffLoss = 3.1167, kgLoss = 0.0128
2025-04-08 13:14:56.422759: Training Step 88/354: batchLoss = 0.5822, diffLoss = 2.8513, kgLoss = 0.0150
2025-04-08 13:14:58.032142: Training Step 89/354: batchLoss = 0.5679, diffLoss = 2.7755, kgLoss = 0.0160
2025-04-08 13:14:59.644948: Training Step 90/354: batchLoss = 0.5828, diffLoss = 2.8638, kgLoss = 0.0125
2025-04-08 13:15:01.257408: Training Step 91/354: batchLoss = 0.7685, diffLoss = 3.7706, kgLoss = 0.0180
2025-04-08 13:15:02.883323: Training Step 92/354: batchLoss = 0.5850, diffLoss = 2.8706, kgLoss = 0.0135
2025-04-08 13:15:04.508468: Training Step 93/354: batchLoss = 0.5741, diffLoss = 2.8177, kgLoss = 0.0131
2025-04-08 13:15:06.136446: Training Step 94/354: batchLoss = 0.5762, diffLoss = 2.8263, kgLoss = 0.0136
2025-04-08 13:15:07.756232: Training Step 95/354: batchLoss = 0.5736, diffLoss = 2.8044, kgLoss = 0.0160
2025-04-08 13:15:09.384222: Training Step 96/354: batchLoss = 0.5511, diffLoss = 2.7003, kgLoss = 0.0138
2025-04-08 13:15:11.012115: Training Step 97/354: batchLoss = 0.7610, diffLoss = 3.7404, kgLoss = 0.0161
2025-04-08 13:15:12.639076: Training Step 98/354: batchLoss = 0.6110, diffLoss = 3.0027, kgLoss = 0.0131
2025-04-08 13:15:14.257004: Training Step 99/354: batchLoss = 0.7162, diffLoss = 3.5152, kgLoss = 0.0165
2025-04-08 13:15:15.882042: Training Step 100/354: batchLoss = 0.6559, diffLoss = 3.2203, kgLoss = 0.0148
2025-04-08 13:15:17.508191: Training Step 101/354: batchLoss = 0.7054, diffLoss = 3.4574, kgLoss = 0.0174
2025-04-08 13:15:19.138831: Training Step 102/354: batchLoss = 0.6124, diffLoss = 3.0043, kgLoss = 0.0144
2025-04-08 13:15:20.775270: Training Step 103/354: batchLoss = 0.5337, diffLoss = 2.6146, kgLoss = 0.0135
2025-04-08 13:15:22.404924: Training Step 104/354: batchLoss = 0.6363, diffLoss = 3.1275, kgLoss = 0.0136
2025-04-08 13:15:24.031990: Training Step 105/354: batchLoss = 0.8039, diffLoss = 3.9529, kgLoss = 0.0167
2025-04-08 13:15:25.649979: Training Step 106/354: batchLoss = 0.6737, diffLoss = 3.3039, kgLoss = 0.0162
2025-04-08 13:15:27.264558: Training Step 107/354: batchLoss = 0.5012, diffLoss = 2.4576, kgLoss = 0.0121
2025-04-08 13:15:28.875512: Training Step 108/354: batchLoss = 0.6225, diffLoss = 3.0516, kgLoss = 0.0152
2025-04-08 13:15:30.488142: Training Step 109/354: batchLoss = 0.5544, diffLoss = 2.7209, kgLoss = 0.0128
2025-04-08 13:15:32.112981: Training Step 110/354: batchLoss = 0.6280, diffLoss = 3.0816, kgLoss = 0.0145
2025-04-08 13:15:33.736278: Training Step 111/354: batchLoss = 0.6787, diffLoss = 3.3299, kgLoss = 0.0159
2025-04-08 13:15:35.354085: Training Step 112/354: batchLoss = 0.8571, diffLoss = 4.2156, kgLoss = 0.0175
2025-04-08 13:15:36.977377: Training Step 113/354: batchLoss = 0.6183, diffLoss = 3.0340, kgLoss = 0.0144
2025-04-08 13:15:38.605791: Training Step 114/354: batchLoss = 0.5683, diffLoss = 2.7911, kgLoss = 0.0126
2025-04-08 13:15:40.236037: Training Step 115/354: batchLoss = 0.6677, diffLoss = 3.2758, kgLoss = 0.0157
2025-04-08 13:15:41.853421: Training Step 116/354: batchLoss = 0.6633, diffLoss = 3.2585, kgLoss = 0.0145
2025-04-08 13:15:43.469685: Training Step 117/354: batchLoss = 0.6367, diffLoss = 3.1286, kgLoss = 0.0137
2025-04-08 13:15:45.081182: Training Step 118/354: batchLoss = 0.6567, diffLoss = 3.2327, kgLoss = 0.0127
2025-04-08 13:15:46.716478: Training Step 119/354: batchLoss = 0.6881, diffLoss = 3.3777, kgLoss = 0.0157
2025-04-08 13:15:48.353916: Training Step 120/354: batchLoss = 0.6339, diffLoss = 3.1142, kgLoss = 0.0138
2025-04-08 13:15:49.970357: Training Step 121/354: batchLoss = 0.5999, diffLoss = 2.9420, kgLoss = 0.0144
2025-04-08 13:15:51.600523: Training Step 122/354: batchLoss = 0.6160, diffLoss = 3.0209, kgLoss = 0.0148
2025-04-08 13:15:53.223163: Training Step 123/354: batchLoss = 0.5031, diffLoss = 2.4595, kgLoss = 0.0140
2025-04-08 13:15:54.842259: Training Step 124/354: batchLoss = 0.5710, diffLoss = 2.8043, kgLoss = 0.0127
2025-04-08 13:15:56.457092: Training Step 125/354: batchLoss = 0.6108, diffLoss = 2.9899, kgLoss = 0.0160
2025-04-08 13:15:58.069019: Training Step 126/354: batchLoss = 0.5818, diffLoss = 2.8548, kgLoss = 0.0135
2025-04-08 13:15:59.675578: Training Step 127/354: batchLoss = 0.6006, diffLoss = 2.9530, kgLoss = 0.0125
2025-04-08 13:16:01.302131: Training Step 128/354: batchLoss = 0.6469, diffLoss = 3.1721, kgLoss = 0.0157
2025-04-08 13:16:02.918462: Training Step 129/354: batchLoss = 0.5545, diffLoss = 2.7224, kgLoss = 0.0125
2025-04-08 13:16:04.542995: Training Step 130/354: batchLoss = 0.6724, diffLoss = 3.3014, kgLoss = 0.0152
2025-04-08 13:16:06.165042: Training Step 131/354: batchLoss = 0.6621, diffLoss = 3.2533, kgLoss = 0.0143
2025-04-08 13:16:07.784898: Training Step 132/354: batchLoss = 0.6612, diffLoss = 3.2509, kgLoss = 0.0137
2025-04-08 13:16:09.408823: Training Step 133/354: batchLoss = 0.6032, diffLoss = 2.9618, kgLoss = 0.0136
2025-04-08 13:16:11.031807: Training Step 134/354: batchLoss = 0.5874, diffLoss = 2.8828, kgLoss = 0.0136
2025-04-08 13:16:12.647784: Training Step 135/354: batchLoss = 0.6571, diffLoss = 3.2241, kgLoss = 0.0153
2025-04-08 13:16:14.254632: Training Step 136/354: batchLoss = 0.6554, diffLoss = 3.2143, kgLoss = 0.0157
2025-04-08 13:16:15.869905: Training Step 137/354: batchLoss = 2.2298, diffLoss = 10.9807, kgLoss = 0.0420
2025-04-08 13:16:17.487911: Training Step 138/354: batchLoss = 0.7427, diffLoss = 3.6423, kgLoss = 0.0178
2025-04-08 13:16:19.106108: Training Step 139/354: batchLoss = 0.6559, diffLoss = 3.2142, kgLoss = 0.0163
2025-04-08 13:16:20.730608: Training Step 140/354: batchLoss = 0.6352, diffLoss = 3.1225, kgLoss = 0.0134
2025-04-08 13:16:22.361990: Training Step 141/354: batchLoss = 0.5578, diffLoss = 2.7363, kgLoss = 0.0132
2025-04-08 13:16:23.980124: Training Step 142/354: batchLoss = 0.7529, diffLoss = 3.6979, kgLoss = 0.0167
2025-04-08 13:16:25.603022: Training Step 143/354: batchLoss = 0.7196, diffLoss = 3.5384, kgLoss = 0.0148
2025-04-08 13:16:27.220653: Training Step 144/354: batchLoss = 0.5650, diffLoss = 2.7725, kgLoss = 0.0132
2025-04-08 13:16:28.824552: Training Step 145/354: batchLoss = 0.5213, diffLoss = 2.5558, kgLoss = 0.0127
2025-04-08 13:16:30.437769: Training Step 146/354: batchLoss = 0.6745, diffLoss = 3.3101, kgLoss = 0.0156
2025-04-08 13:16:32.059056: Training Step 147/354: batchLoss = 0.7964, diffLoss = 3.9098, kgLoss = 0.0180
2025-04-08 13:16:33.683006: Training Step 148/354: batchLoss = 0.6197, diffLoss = 3.0395, kgLoss = 0.0148
2025-04-08 13:16:35.317585: Training Step 149/354: batchLoss = 0.5564, diffLoss = 2.7237, kgLoss = 0.0146
2025-04-08 13:16:36.940856: Training Step 150/354: batchLoss = 0.5928, diffLoss = 2.9164, kgLoss = 0.0118
2025-04-08 13:16:38.568894: Training Step 151/354: batchLoss = 0.6674, diffLoss = 3.2801, kgLoss = 0.0142
2025-04-08 13:16:40.198012: Training Step 152/354: batchLoss = 0.5443, diffLoss = 2.6729, kgLoss = 0.0121
2025-04-08 13:16:41.816432: Training Step 153/354: batchLoss = 0.8829, diffLoss = 4.3391, kgLoss = 0.0188
2025-04-08 13:16:43.423245: Training Step 154/354: batchLoss = 0.5269, diffLoss = 2.5822, kgLoss = 0.0130
2025-04-08 13:16:45.040152: Training Step 155/354: batchLoss = 0.7764, diffLoss = 3.8175, kgLoss = 0.0162
2025-04-08 13:16:46.670146: Training Step 156/354: batchLoss = 0.6273, diffLoss = 3.0836, kgLoss = 0.0132
2025-04-08 13:16:48.293508: Training Step 157/354: batchLoss = 0.7169, diffLoss = 3.5238, kgLoss = 0.0152
2025-04-08 13:16:49.916123: Training Step 158/354: batchLoss = 0.6080, diffLoss = 2.9885, kgLoss = 0.0129
2025-04-08 13:16:51.533973: Training Step 159/354: batchLoss = 0.5932, diffLoss = 2.9138, kgLoss = 0.0130
2025-04-08 13:16:53.167887: Training Step 160/354: batchLoss = 0.6359, diffLoss = 3.1205, kgLoss = 0.0147
2025-04-08 13:16:54.783056: Training Step 161/354: batchLoss = 0.5273, diffLoss = 2.5872, kgLoss = 0.0123
2025-04-08 13:16:56.403603: Training Step 162/354: batchLoss = 0.7328, diffLoss = 3.6012, kgLoss = 0.0157
2025-04-08 13:16:58.027114: Training Step 163/354: batchLoss = 0.6088, diffLoss = 2.9904, kgLoss = 0.0134
2025-04-08 13:16:59.646553: Training Step 164/354: batchLoss = 0.6606, diffLoss = 3.2436, kgLoss = 0.0148
2025-04-08 13:17:01.275168: Training Step 165/354: batchLoss = 0.6015, diffLoss = 2.9600, kgLoss = 0.0118
2025-04-08 13:17:02.906979: Training Step 166/354: batchLoss = 0.6658, diffLoss = 3.2698, kgLoss = 0.0148
2025-04-08 13:17:04.532466: Training Step 167/354: batchLoss = 0.5565, diffLoss = 2.7327, kgLoss = 0.0124
2025-04-08 13:17:06.159051: Training Step 168/354: batchLoss = 0.7622, diffLoss = 3.7488, kgLoss = 0.0156
2025-04-08 13:17:07.781737: Training Step 169/354: batchLoss = 0.7353, diffLoss = 3.6103, kgLoss = 0.0166
2025-04-08 13:17:09.409465: Training Step 170/354: batchLoss = 0.7045, diffLoss = 3.4652, kgLoss = 0.0143
2025-04-08 13:17:11.031665: Training Step 171/354: batchLoss = 0.6821, diffLoss = 3.3461, kgLoss = 0.0162
2025-04-08 13:17:12.644824: Training Step 172/354: batchLoss = 0.5818, diffLoss = 2.8537, kgLoss = 0.0138
2025-04-08 13:17:14.263952: Training Step 173/354: batchLoss = 0.6615, diffLoss = 3.2438, kgLoss = 0.0159
2025-04-08 13:17:15.910292: Training Step 174/354: batchLoss = 1.1369, diffLoss = 5.5949, kgLoss = 0.0224
2025-04-08 13:17:17.541390: Training Step 175/354: batchLoss = 0.6285, diffLoss = 3.0882, kgLoss = 0.0135
2025-04-08 13:17:19.172095: Training Step 176/354: batchLoss = 0.9693, diffLoss = 4.7668, kgLoss = 0.0200
2025-04-08 13:17:20.792091: Training Step 177/354: batchLoss = 0.6461, diffLoss = 3.1715, kgLoss = 0.0147
2025-04-08 13:17:22.413764: Training Step 178/354: batchLoss = 0.6403, diffLoss = 3.1451, kgLoss = 0.0141
2025-04-08 13:17:24.035745: Training Step 179/354: batchLoss = 0.6780, diffLoss = 3.3296, kgLoss = 0.0151
2025-04-08 13:17:25.648602: Training Step 180/354: batchLoss = 0.6639, diffLoss = 3.2562, kgLoss = 0.0158
2025-04-08 13:17:27.259164: Training Step 181/354: batchLoss = 0.6046, diffLoss = 2.9719, kgLoss = 0.0127
2025-04-08 13:17:28.877044: Training Step 182/354: batchLoss = 0.5793, diffLoss = 2.8456, kgLoss = 0.0128
2025-04-08 13:17:30.490163: Training Step 183/354: batchLoss = 0.5794, diffLoss = 2.8380, kgLoss = 0.0147
2025-04-08 13:17:32.117710: Training Step 184/354: batchLoss = 0.6330, diffLoss = 3.1086, kgLoss = 0.0141
2025-04-08 13:17:33.746857: Training Step 185/354: batchLoss = 0.6239, diffLoss = 3.0617, kgLoss = 0.0145
2025-04-08 13:17:35.370245: Training Step 186/354: batchLoss = 0.6875, diffLoss = 3.3718, kgLoss = 0.0165
2025-04-08 13:17:36.995727: Training Step 187/354: batchLoss = 0.5761, diffLoss = 2.8237, kgLoss = 0.0142
2025-04-08 13:17:38.606818: Training Step 188/354: batchLoss = 0.7686, diffLoss = 3.7733, kgLoss = 0.0174
2025-04-08 13:17:40.211913: Training Step 189/354: batchLoss = 0.6136, diffLoss = 3.0110, kgLoss = 0.0142
2025-04-08 13:17:41.820915: Training Step 190/354: batchLoss = 0.5848, diffLoss = 2.8712, kgLoss = 0.0132
2025-04-08 13:17:43.432052: Training Step 191/354: batchLoss = 0.6951, diffLoss = 3.4146, kgLoss = 0.0152
2025-04-08 13:17:45.049370: Training Step 192/354: batchLoss = 0.5766, diffLoss = 2.8371, kgLoss = 0.0115
2025-04-08 13:17:46.659392: Training Step 193/354: batchLoss = 0.5846, diffLoss = 2.8765, kgLoss = 0.0116
2025-04-08 13:17:48.268594: Training Step 194/354: batchLoss = 0.8657, diffLoss = 4.2530, kgLoss = 0.0189
2025-04-08 13:17:49.890847: Training Step 195/354: batchLoss = 0.5440, diffLoss = 2.6634, kgLoss = 0.0142
2025-04-08 13:17:51.515150: Training Step 196/354: batchLoss = 0.4881, diffLoss = 2.3955, kgLoss = 0.0113
2025-04-08 13:17:53.132678: Training Step 197/354: batchLoss = 0.6847, diffLoss = 3.3626, kgLoss = 0.0152
2025-04-08 13:17:54.737557: Training Step 198/354: batchLoss = 0.6552, diffLoss = 3.2227, kgLoss = 0.0133
2025-04-08 13:17:56.344773: Training Step 199/354: batchLoss = 0.6325, diffLoss = 3.1060, kgLoss = 0.0142
2025-04-08 13:17:57.969877: Training Step 200/354: batchLoss = 0.5846, diffLoss = 2.8591, kgLoss = 0.0159
2025-04-08 13:17:59.594829: Training Step 201/354: batchLoss = 0.7505, diffLoss = 3.6840, kgLoss = 0.0171
2025-04-08 13:18:01.203770: Training Step 202/354: batchLoss = 0.6830, diffLoss = 3.3522, kgLoss = 0.0158
2025-04-08 13:18:02.815927: Training Step 203/354: batchLoss = 0.7192, diffLoss = 3.5310, kgLoss = 0.0163
2025-04-08 13:18:04.428721: Training Step 204/354: batchLoss = 0.5518, diffLoss = 2.7077, kgLoss = 0.0128
2025-04-08 13:18:06.051899: Training Step 205/354: batchLoss = 0.6432, diffLoss = 3.1603, kgLoss = 0.0140
2025-04-08 13:18:07.667059: Training Step 206/354: batchLoss = 0.5286, diffLoss = 2.5975, kgLoss = 0.0114
2025-04-08 13:18:09.274004: Training Step 207/354: batchLoss = 0.8400, diffLoss = 4.1319, kgLoss = 0.0171
2025-04-08 13:18:10.885630: Training Step 208/354: batchLoss = 0.8095, diffLoss = 3.9755, kgLoss = 0.0180
2025-04-08 13:18:12.507510: Training Step 209/354: batchLoss = 0.7027, diffLoss = 3.4499, kgLoss = 0.0159
2025-04-08 13:18:14.136980: Training Step 210/354: batchLoss = 0.6602, diffLoss = 3.2477, kgLoss = 0.0133
2025-04-08 13:18:15.762688: Training Step 211/354: batchLoss = 0.8346, diffLoss = 4.1024, kgLoss = 0.0177
2025-04-08 13:18:17.393246: Training Step 212/354: batchLoss = 0.9250, diffLoss = 4.5431, kgLoss = 0.0205
2025-04-08 13:18:19.015803: Training Step 213/354: batchLoss = 0.7033, diffLoss = 3.4575, kgLoss = 0.0147
2025-04-08 13:18:20.634033: Training Step 214/354: batchLoss = 0.7603, diffLoss = 3.7404, kgLoss = 0.0152
2025-04-08 13:18:22.256510: Training Step 215/354: batchLoss = 0.9527, diffLoss = 4.6775, kgLoss = 0.0216
2025-04-08 13:18:23.875130: Training Step 216/354: batchLoss = 0.6797, diffLoss = 3.3444, kgLoss = 0.0136
2025-04-08 13:18:25.489197: Training Step 217/354: batchLoss = 0.5755, diffLoss = 2.8283, kgLoss = 0.0123
2025-04-08 13:18:27.117337: Training Step 218/354: batchLoss = 0.8496, diffLoss = 4.1747, kgLoss = 0.0184
2025-04-08 13:18:28.730748: Training Step 219/354: batchLoss = 0.5821, diffLoss = 2.8596, kgLoss = 0.0127
2025-04-08 13:18:30.349705: Training Step 220/354: batchLoss = 0.5934, diffLoss = 2.9116, kgLoss = 0.0138
2025-04-08 13:18:31.975038: Training Step 221/354: batchLoss = 0.6273, diffLoss = 3.0784, kgLoss = 0.0146
2025-04-08 13:18:33.595803: Training Step 222/354: batchLoss = 0.7328, diffLoss = 3.5915, kgLoss = 0.0181
2025-04-08 13:18:35.227512: Training Step 223/354: batchLoss = 0.5804, diffLoss = 2.8516, kgLoss = 0.0126
2025-04-08 13:18:36.853345: Training Step 224/354: batchLoss = 0.7174, diffLoss = 3.5220, kgLoss = 0.0162
2025-04-08 13:18:38.465902: Training Step 225/354: batchLoss = 0.6766, diffLoss = 3.3255, kgLoss = 0.0144
2025-04-08 13:18:40.087327: Training Step 226/354: batchLoss = 0.6100, diffLoss = 3.0018, kgLoss = 0.0121
2025-04-08 13:18:41.712563: Training Step 227/354: batchLoss = 0.5979, diffLoss = 2.9326, kgLoss = 0.0142
2025-04-08 13:18:43.362057: Training Step 228/354: batchLoss = 0.6074, diffLoss = 2.9817, kgLoss = 0.0138
2025-04-08 13:18:45.015501: Training Step 229/354: batchLoss = 0.4941, diffLoss = 2.4201, kgLoss = 0.0126
2025-04-08 13:18:46.658909: Training Step 230/354: batchLoss = 0.5802, diffLoss = 2.8511, kgLoss = 0.0125
2025-04-08 13:18:48.302816: Training Step 231/354: batchLoss = 0.5899, diffLoss = 2.8968, kgLoss = 0.0132
2025-04-08 13:18:49.938232: Training Step 232/354: batchLoss = 0.7693, diffLoss = 3.7783, kgLoss = 0.0170
2025-04-08 13:18:51.584304: Training Step 233/354: batchLoss = 0.6375, diffLoss = 3.1263, kgLoss = 0.0153
2025-04-08 13:18:53.227252: Training Step 234/354: batchLoss = 0.6665, diffLoss = 3.2716, kgLoss = 0.0153
2025-04-08 13:18:54.870984: Training Step 235/354: batchLoss = 0.7751, diffLoss = 3.8062, kgLoss = 0.0174
2025-04-08 13:18:56.478145: Training Step 236/354: batchLoss = 0.5631, diffLoss = 2.7668, kgLoss = 0.0121
2025-04-08 13:18:58.095777: Training Step 237/354: batchLoss = 0.5967, diffLoss = 2.9249, kgLoss = 0.0147
2025-04-08 13:18:59.722485: Training Step 238/354: batchLoss = 0.7325, diffLoss = 3.5975, kgLoss = 0.0162
2025-04-08 13:19:01.353994: Training Step 239/354: batchLoss = 0.5353, diffLoss = 2.6264, kgLoss = 0.0125
2025-04-08 13:19:02.982826: Training Step 240/354: batchLoss = 0.6262, diffLoss = 3.0763, kgLoss = 0.0137
2025-04-08 13:19:04.613765: Training Step 241/354: batchLoss = 0.5993, diffLoss = 2.9407, kgLoss = 0.0140
2025-04-08 13:19:06.235161: Training Step 242/354: batchLoss = 0.6052, diffLoss = 2.9637, kgLoss = 0.0155
2025-04-08 13:19:07.844410: Training Step 243/354: batchLoss = 0.8605, diffLoss = 4.2251, kgLoss = 0.0194
2025-04-08 13:19:09.449785: Training Step 244/354: batchLoss = 0.6121, diffLoss = 2.9973, kgLoss = 0.0158
2025-04-08 13:19:11.064328: Training Step 245/354: batchLoss = 0.6188, diffLoss = 3.0415, kgLoss = 0.0132
2025-04-08 13:19:12.700552: Training Step 246/354: batchLoss = 0.4921, diffLoss = 2.4131, kgLoss = 0.0118
2025-04-08 13:19:14.326150: Training Step 247/354: batchLoss = 0.5888, diffLoss = 2.8923, kgLoss = 0.0129
2025-04-08 13:19:15.944220: Training Step 248/354: batchLoss = 0.6588, diffLoss = 3.2362, kgLoss = 0.0144
2025-04-08 13:19:17.558154: Training Step 249/354: batchLoss = 0.5767, diffLoss = 2.8302, kgLoss = 0.0133
2025-04-08 13:19:19.182572: Training Step 250/354: batchLoss = 0.7181, diffLoss = 3.5298, kgLoss = 0.0152
2025-04-08 13:19:20.798790: Training Step 251/354: batchLoss = 0.6489, diffLoss = 3.1895, kgLoss = 0.0138
2025-04-08 13:19:22.407901: Training Step 252/354: batchLoss = 0.6368, diffLoss = 3.1199, kgLoss = 0.0160
2025-04-08 13:19:24.013269: Training Step 253/354: batchLoss = 0.7839, diffLoss = 3.8516, kgLoss = 0.0169
2025-04-08 13:19:25.619563: Training Step 254/354: batchLoss = 0.5565, diffLoss = 2.7389, kgLoss = 0.0109
2025-04-08 13:19:27.237426: Training Step 255/354: batchLoss = 0.6331, diffLoss = 3.1122, kgLoss = 0.0133
2025-04-08 13:19:28.858059: Training Step 256/354: batchLoss = 0.5897, diffLoss = 2.8944, kgLoss = 0.0135
2025-04-08 13:19:30.480700: Training Step 257/354: batchLoss = 0.7112, diffLoss = 3.4888, kgLoss = 0.0168
2025-04-08 13:19:32.109896: Training Step 258/354: batchLoss = 0.5300, diffLoss = 2.5976, kgLoss = 0.0131
2025-04-08 13:19:33.737514: Training Step 259/354: batchLoss = 0.7752, diffLoss = 3.8123, kgLoss = 0.0160
2025-04-08 13:19:35.362254: Training Step 260/354: batchLoss = 0.6650, diffLoss = 3.2658, kgLoss = 0.0148
2025-04-08 13:19:36.979401: Training Step 261/354: batchLoss = 0.6000, diffLoss = 2.9355, kgLoss = 0.0161
2025-04-08 13:19:38.594563: Training Step 262/354: batchLoss = 0.6168, diffLoss = 3.0151, kgLoss = 0.0172
2025-04-08 13:19:40.205683: Training Step 263/354: batchLoss = 0.6271, diffLoss = 3.0767, kgLoss = 0.0147
2025-04-08 13:19:41.823339: Training Step 264/354: batchLoss = 0.7109, diffLoss = 3.4863, kgLoss = 0.0170
2025-04-08 13:19:43.442544: Training Step 265/354: batchLoss = 0.7363, diffLoss = 3.6182, kgLoss = 0.0159
2025-04-08 13:19:45.062164: Training Step 266/354: batchLoss = 0.4754, diffLoss = 2.3341, kgLoss = 0.0108
2025-04-08 13:19:46.683242: Training Step 267/354: batchLoss = 0.5966, diffLoss = 2.9282, kgLoss = 0.0137
2025-04-08 13:19:48.312648: Training Step 268/354: batchLoss = 0.5555, diffLoss = 2.7269, kgLoss = 0.0127
2025-04-08 13:19:49.935678: Training Step 269/354: batchLoss = 0.6394, diffLoss = 3.1331, kgLoss = 0.0160
2025-04-08 13:19:51.537331: Training Step 270/354: batchLoss = 0.5634, diffLoss = 2.7710, kgLoss = 0.0115
2025-04-08 13:19:53.145565: Training Step 271/354: batchLoss = 0.6032, diffLoss = 2.9558, kgLoss = 0.0150
2025-04-08 13:19:54.756905: Training Step 272/354: batchLoss = 0.6123, diffLoss = 3.0052, kgLoss = 0.0141
2025-04-08 13:19:56.370916: Training Step 273/354: batchLoss = 0.5752, diffLoss = 2.8266, kgLoss = 0.0123
2025-04-08 13:19:57.983420: Training Step 274/354: batchLoss = 0.6356, diffLoss = 3.1179, kgLoss = 0.0150
2025-04-08 13:19:59.599852: Training Step 275/354: batchLoss = 0.5170, diffLoss = 2.5373, kgLoss = 0.0120
2025-04-08 13:20:01.223798: Training Step 276/354: batchLoss = 0.5630, diffLoss = 2.7608, kgLoss = 0.0135
2025-04-08 13:20:02.851979: Training Step 277/354: batchLoss = 0.5899, diffLoss = 2.8871, kgLoss = 0.0156
2025-04-08 13:20:04.472495: Training Step 278/354: batchLoss = 0.4722, diffLoss = 2.3078, kgLoss = 0.0132
2025-04-08 13:20:06.086843: Training Step 279/354: batchLoss = 0.6195, diffLoss = 3.0373, kgLoss = 0.0151
2025-04-08 13:20:07.706280: Training Step 280/354: batchLoss = 0.5767, diffLoss = 2.8225, kgLoss = 0.0152
2025-04-08 13:20:09.319446: Training Step 281/354: batchLoss = 0.6344, diffLoss = 3.1072, kgLoss = 0.0162
2025-04-08 13:20:10.933630: Training Step 282/354: batchLoss = 0.4859, diffLoss = 2.3814, kgLoss = 0.0120
2025-04-08 13:20:12.552466: Training Step 283/354: batchLoss = 0.6855, diffLoss = 3.3645, kgLoss = 0.0158
2025-04-08 13:20:14.181112: Training Step 284/354: batchLoss = 0.6722, diffLoss = 3.2946, kgLoss = 0.0165
2025-04-08 13:20:15.804991: Training Step 285/354: batchLoss = 0.5634, diffLoss = 2.7594, kgLoss = 0.0143
2025-04-08 13:20:17.427911: Training Step 286/354: batchLoss = 0.5871, diffLoss = 2.8850, kgLoss = 0.0126
2025-04-08 13:20:19.050952: Training Step 287/354: batchLoss = 0.6556, diffLoss = 3.2201, kgLoss = 0.0145
2025-04-08 13:20:20.665038: Training Step 288/354: batchLoss = 0.5082, diffLoss = 2.4958, kgLoss = 0.0113
2025-04-08 13:20:22.276554: Training Step 289/354: batchLoss = 0.6993, diffLoss = 3.4306, kgLoss = 0.0164
2025-04-08 13:20:23.887102: Training Step 290/354: batchLoss = 0.5491, diffLoss = 2.6949, kgLoss = 0.0127
2025-04-08 13:20:25.503415: Training Step 291/354: batchLoss = 0.5736, diffLoss = 2.8106, kgLoss = 0.0144
2025-04-08 13:20:27.123935: Training Step 292/354: batchLoss = 0.6184, diffLoss = 3.0356, kgLoss = 0.0141
2025-04-08 13:20:28.743959: Training Step 293/354: batchLoss = 0.6749, diffLoss = 3.3090, kgLoss = 0.0164
2025-04-08 13:20:30.366496: Training Step 294/354: batchLoss = 0.6005, diffLoss = 2.9535, kgLoss = 0.0123
2025-04-08 13:20:31.991168: Training Step 295/354: batchLoss = 0.6107, diffLoss = 2.9970, kgLoss = 0.0141
2025-04-08 13:20:33.619903: Training Step 296/354: batchLoss = 0.6557, diffLoss = 3.2221, kgLoss = 0.0141
2025-04-08 13:20:35.229443: Training Step 297/354: batchLoss = 0.8218, diffLoss = 4.0445, kgLoss = 0.0161
2025-04-08 13:20:36.838943: Training Step 298/354: batchLoss = 0.6510, diffLoss = 3.1962, kgLoss = 0.0148
2025-04-08 13:20:38.451187: Training Step 299/354: batchLoss = 0.7450, diffLoss = 3.6602, kgLoss = 0.0162
2025-04-08 13:20:40.071157: Training Step 300/354: batchLoss = 0.6191, diffLoss = 3.0391, kgLoss = 0.0142
2025-04-08 13:20:41.685962: Training Step 301/354: batchLoss = 0.6146, diffLoss = 3.0166, kgLoss = 0.0141
2025-04-08 13:20:43.311562: Training Step 302/354: batchLoss = 0.7969, diffLoss = 3.9114, kgLoss = 0.0183
2025-04-08 13:20:44.932606: Training Step 303/354: batchLoss = 0.7150, diffLoss = 3.5171, kgLoss = 0.0145
2025-04-08 13:20:46.553469: Training Step 304/354: batchLoss = 0.6865, diffLoss = 3.3758, kgLoss = 0.0142
2025-04-08 13:20:48.173718: Training Step 305/354: batchLoss = 0.5565, diffLoss = 2.7183, kgLoss = 0.0160
2025-04-08 13:20:49.790844: Training Step 306/354: batchLoss = 0.5571, diffLoss = 2.7327, kgLoss = 0.0132
2025-04-08 13:20:51.397885: Training Step 307/354: batchLoss = 0.6098, diffLoss = 2.9930, kgLoss = 0.0141
2025-04-08 13:20:53.021945: Training Step 308/354: batchLoss = 0.5495, diffLoss = 2.6924, kgLoss = 0.0137
2025-04-08 13:20:54.641714: Training Step 309/354: batchLoss = 0.6598, diffLoss = 3.2397, kgLoss = 0.0148
2025-04-08 13:20:56.263580: Training Step 310/354: batchLoss = 0.7695, diffLoss = 3.7820, kgLoss = 0.0163
2025-04-08 13:20:57.877856: Training Step 311/354: batchLoss = 0.6877, diffLoss = 3.3744, kgLoss = 0.0160
2025-04-08 13:20:59.494162: Training Step 312/354: batchLoss = 0.6086, diffLoss = 2.9830, kgLoss = 0.0150
2025-04-08 13:21:01.106054: Training Step 313/354: batchLoss = 0.5898, diffLoss = 2.8998, kgLoss = 0.0123
2025-04-08 13:21:02.735207: Training Step 314/354: batchLoss = 0.9405, diffLoss = 4.6231, kgLoss = 0.0199
2025-04-08 13:21:04.360999: Training Step 315/354: batchLoss = 0.6147, diffLoss = 3.0197, kgLoss = 0.0134
2025-04-08 13:21:05.977063: Training Step 316/354: batchLoss = 0.6821, diffLoss = 3.3440, kgLoss = 0.0166
2025-04-08 13:21:07.583642: Training Step 317/354: batchLoss = 0.6548, diffLoss = 3.2069, kgLoss = 0.0168
2025-04-08 13:21:09.199545: Training Step 318/354: batchLoss = 0.6018, diffLoss = 2.9562, kgLoss = 0.0132
2025-04-08 13:21:10.820533: Training Step 319/354: batchLoss = 0.6197, diffLoss = 3.0432, kgLoss = 0.0139
2025-04-08 13:21:12.439925: Training Step 320/354: batchLoss = 0.6297, diffLoss = 3.0906, kgLoss = 0.0145
2025-04-08 13:21:14.053120: Training Step 321/354: batchLoss = 0.6036, diffLoss = 2.9664, kgLoss = 0.0129
2025-04-08 13:21:15.690200: Training Step 322/354: batchLoss = 0.5500, diffLoss = 2.7010, kgLoss = 0.0122
2025-04-08 13:21:17.317276: Training Step 323/354: batchLoss = 0.6556, diffLoss = 3.2218, kgLoss = 0.0141
2025-04-08 13:21:18.951467: Training Step 324/354: batchLoss = 0.6268, diffLoss = 3.0796, kgLoss = 0.0136
2025-04-08 13:21:20.577628: Training Step 325/354: batchLoss = 0.7046, diffLoss = 3.4577, kgLoss = 0.0163
2025-04-08 13:21:22.188450: Training Step 326/354: batchLoss = 0.6184, diffLoss = 3.0347, kgLoss = 0.0143
2025-04-08 13:21:23.803486: Training Step 327/354: batchLoss = 0.6883, diffLoss = 3.3795, kgLoss = 0.0155
2025-04-08 13:21:25.414890: Training Step 328/354: batchLoss = 0.5492, diffLoss = 2.6947, kgLoss = 0.0128
2025-04-08 13:21:27.031902: Training Step 329/354: batchLoss = 0.5594, diffLoss = 2.7435, kgLoss = 0.0134
2025-04-08 13:21:28.653466: Training Step 330/354: batchLoss = 0.6112, diffLoss = 2.9982, kgLoss = 0.0144
2025-04-08 13:21:30.276163: Training Step 331/354: batchLoss = 0.5888, diffLoss = 2.8843, kgLoss = 0.0149
2025-04-08 13:21:31.892791: Training Step 332/354: batchLoss = 0.5832, diffLoss = 2.8565, kgLoss = 0.0149
2025-04-08 13:21:33.514913: Training Step 333/354: batchLoss = 0.5520, diffLoss = 2.7102, kgLoss = 0.0125
2025-04-08 13:21:35.124359: Training Step 334/354: batchLoss = 0.6027, diffLoss = 2.9418, kgLoss = 0.0179
2025-04-08 13:21:36.729973: Training Step 335/354: batchLoss = 0.5544, diffLoss = 2.7234, kgLoss = 0.0121
2025-04-08 13:21:38.344227: Training Step 336/354: batchLoss = 0.4919, diffLoss = 2.4036, kgLoss = 0.0140
2025-04-08 13:21:39.977175: Training Step 337/354: batchLoss = 0.8339, diffLoss = 4.0934, kgLoss = 0.0190
2025-04-08 13:21:41.598900: Training Step 338/354: batchLoss = 0.7309, diffLoss = 3.5905, kgLoss = 0.0160
2025-04-08 13:21:43.214781: Training Step 339/354: batchLoss = 0.6360, diffLoss = 3.1214, kgLoss = 0.0147
2025-04-08 13:21:44.836254: Training Step 340/354: batchLoss = 0.5844, diffLoss = 2.8681, kgLoss = 0.0135
2025-04-08 13:21:46.451776: Training Step 341/354: batchLoss = 0.6489, diffLoss = 3.1904, kgLoss = 0.0135
2025-04-08 13:21:48.068407: Training Step 342/354: batchLoss = 0.6080, diffLoss = 2.9772, kgLoss = 0.0157
2025-04-08 13:21:49.677948: Training Step 343/354: batchLoss = 0.4949, diffLoss = 2.4272, kgLoss = 0.0119
2025-04-08 13:21:51.293655: Training Step 344/354: batchLoss = 0.5766, diffLoss = 2.8256, kgLoss = 0.0144
2025-04-08 13:21:52.904923: Training Step 345/354: batchLoss = 0.9368, diffLoss = 4.6141, kgLoss = 0.0175
2025-04-08 13:21:54.528556: Training Step 346/354: batchLoss = 0.7559, diffLoss = 3.7156, kgLoss = 0.0160
2025-04-08 13:21:56.149432: Training Step 347/354: batchLoss = 0.6369, diffLoss = 3.1314, kgLoss = 0.0132
2025-04-08 13:21:57.769437: Training Step 348/354: batchLoss = 0.5715, diffLoss = 2.8107, kgLoss = 0.0117
2025-04-08 13:21:59.396448: Training Step 349/354: batchLoss = 0.7692, diffLoss = 3.7821, kgLoss = 0.0160
2025-04-08 13:22:01.021365: Training Step 350/354: batchLoss = 0.6113, diffLoss = 2.9898, kgLoss = 0.0167
2025-04-08 13:22:02.637103: Training Step 351/354: batchLoss = 0.7967, diffLoss = 3.9085, kgLoss = 0.0187
2025-04-08 13:22:04.237083: Training Step 352/354: batchLoss = 0.5783, diffLoss = 2.8348, kgLoss = 0.0142
2025-04-08 13:22:05.636816: Training Step 353/354: batchLoss = 0.8309, diffLoss = 4.0873, kgLoss = 0.0168
2025-04-08 13:22:05.730209: 
2025-04-08 13:22:05.731058: Epoch 3/1000, Train: epLoss = 1.1529, epDfLoss = 5.6600, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 13:22:07.034936: Steps 0/138: batch_recall = 43.32, batch_ndcg = 24.00 
2025-04-08 13:22:08.346983: Steps 1/138: batch_recall = 40.81, batch_ndcg = 24.51 
2025-04-08 13:22:09.669922: Steps 2/138: batch_recall = 49.54, batch_ndcg = 31.17 
2025-04-08 13:22:10.984749: Steps 3/138: batch_recall = 51.09, batch_ndcg = 28.75 
2025-04-08 13:22:12.298787: Steps 4/138: batch_recall = 56.36, batch_ndcg = 36.50 
2025-04-08 13:22:13.622831: Steps 5/138: batch_recall = 47.27, batch_ndcg = 25.79 
2025-04-08 13:22:14.928710: Steps 6/138: batch_recall = 44.92, batch_ndcg = 26.77 
2025-04-08 13:22:16.240889: Steps 7/138: batch_recall = 57.91, batch_ndcg = 36.14 
2025-04-08 13:22:17.565369: Steps 8/138: batch_recall = 53.84, batch_ndcg = 34.14 
2025-04-08 13:22:18.864159: Steps 9/138: batch_recall = 47.77, batch_ndcg = 29.10 
2025-04-08 13:22:20.144913: Steps 10/138: batch_recall = 45.49, batch_ndcg = 26.41 
2025-04-08 13:22:21.436560: Steps 11/138: batch_recall = 58.68, batch_ndcg = 33.42 
2025-04-08 13:22:22.731046: Steps 12/138: batch_recall = 46.24, batch_ndcg = 27.12 
2025-04-08 13:22:24.040015: Steps 13/138: batch_recall = 42.37, batch_ndcg = 25.32 
2025-04-08 13:22:25.332467: Steps 14/138: batch_recall = 41.58, batch_ndcg = 25.31 
2025-04-08 13:22:26.626892: Steps 15/138: batch_recall = 47.86, batch_ndcg = 29.32 
2025-04-08 13:22:27.945552: Steps 16/138: batch_recall = 49.98, batch_ndcg = 28.14 
2025-04-08 13:22:29.235504: Steps 17/138: batch_recall = 51.63, batch_ndcg = 28.29 
2025-04-08 13:22:30.525634: Steps 18/138: batch_recall = 44.33, batch_ndcg = 28.48 
2025-04-08 13:22:31.826809: Steps 19/138: batch_recall = 48.12, batch_ndcg = 28.52 
2025-04-08 13:22:33.125225: Steps 20/138: batch_recall = 55.35, batch_ndcg = 30.66 
2025-04-08 13:22:34.408055: Steps 21/138: batch_recall = 58.41, batch_ndcg = 36.86 
2025-04-08 13:22:35.698090: Steps 22/138: batch_recall = 51.23, batch_ndcg = 29.83 
2025-04-08 13:22:36.976590: Steps 23/138: batch_recall = 43.09, batch_ndcg = 25.34 
2025-04-08 13:22:38.276392: Steps 24/138: batch_recall = 41.28, batch_ndcg = 26.28 
2025-04-08 13:22:39.576430: Steps 25/138: batch_recall = 50.81, batch_ndcg = 29.58 
2025-04-08 13:22:40.868459: Steps 26/138: batch_recall = 53.13, batch_ndcg = 29.45 
2025-04-08 13:22:42.157739: Steps 27/138: batch_recall = 56.30, batch_ndcg = 33.15 
2025-04-08 13:22:43.446638: Steps 28/138: batch_recall = 49.93, batch_ndcg = 28.05 
2025-04-08 13:22:44.731636: Steps 29/138: batch_recall = 54.52, batch_ndcg = 28.92 
2025-04-08 13:22:46.031381: Steps 30/138: batch_recall = 49.34, batch_ndcg = 31.72 
2025-04-08 13:22:47.331348: Steps 31/138: batch_recall = 35.35, batch_ndcg = 21.19 
2025-04-08 13:22:48.605858: Steps 32/138: batch_recall = 45.08, batch_ndcg = 27.08 
2025-04-08 13:22:49.890553: Steps 33/138: batch_recall = 48.78, batch_ndcg = 28.73 
2025-04-08 13:22:51.169726: Steps 34/138: batch_recall = 48.30, batch_ndcg = 25.55 
2025-04-08 13:22:52.455668: Steps 35/138: batch_recall = 42.87, batch_ndcg = 26.39 
2025-04-08 13:22:53.755297: Steps 36/138: batch_recall = 44.99, batch_ndcg = 25.85 
2025-04-08 13:22:55.038370: Steps 37/138: batch_recall = 61.77, batch_ndcg = 32.18 
2025-04-08 13:22:56.329518: Steps 38/138: batch_recall = 52.18, batch_ndcg = 31.45 
2025-04-08 13:22:57.629661: Steps 39/138: batch_recall = 67.63, batch_ndcg = 36.05 
2025-04-08 13:22:58.907809: Steps 40/138: batch_recall = 47.67, batch_ndcg = 25.09 
2025-04-08 13:23:00.194756: Steps 41/138: batch_recall = 53.85, batch_ndcg = 30.80 
2025-04-08 13:23:01.467994: Steps 42/138: batch_recall = 50.93, batch_ndcg = 28.43 
2025-04-08 13:23:02.747880: Steps 43/138: batch_recall = 51.72, batch_ndcg = 32.60 
2025-04-08 13:23:04.018017: Steps 44/138: batch_recall = 50.04, batch_ndcg = 26.68 
2025-04-08 13:23:05.297244: Steps 45/138: batch_recall = 57.39, batch_ndcg = 30.72 
2025-04-08 13:23:06.557636: Steps 46/138: batch_recall = 54.80, batch_ndcg = 31.71 
2025-04-08 13:23:07.839232: Steps 47/138: batch_recall = 52.64, batch_ndcg = 28.32 
2025-04-08 13:23:09.128258: Steps 48/138: batch_recall = 48.43, batch_ndcg = 28.49 
2025-04-08 13:23:10.410771: Steps 49/138: batch_recall = 54.70, batch_ndcg = 34.49 
2025-04-08 13:23:11.700594: Steps 50/138: batch_recall = 56.88, batch_ndcg = 30.25 
2025-04-08 13:23:12.984518: Steps 51/138: batch_recall = 52.69, batch_ndcg = 30.26 
2025-04-08 13:23:14.264696: Steps 52/138: batch_recall = 60.66, batch_ndcg = 37.06 
2025-04-08 13:23:15.566859: Steps 53/138: batch_recall = 54.36, batch_ndcg = 28.00 
2025-04-08 13:23:16.875246: Steps 54/138: batch_recall = 53.69, batch_ndcg = 29.05 
2025-04-08 13:23:18.186134: Steps 55/138: batch_recall = 51.14, batch_ndcg = 28.67 
2025-04-08 13:23:19.509130: Steps 56/138: batch_recall = 53.98, batch_ndcg = 31.06 
2025-04-08 13:23:20.822856: Steps 57/138: batch_recall = 54.56, batch_ndcg = 30.32 
2025-04-08 13:23:22.134318: Steps 58/138: batch_recall = 61.73, batch_ndcg = 32.81 
2025-04-08 13:23:23.461065: Steps 59/138: batch_recall = 69.69, batch_ndcg = 36.16 
2025-04-08 13:23:24.759070: Steps 60/138: batch_recall = 68.19, batch_ndcg = 34.98 
2025-04-08 13:23:26.061304: Steps 61/138: batch_recall = 56.74, batch_ndcg = 31.14 
2025-04-08 13:23:27.378462: Steps 62/138: batch_recall = 74.14, batch_ndcg = 40.29 
2025-04-08 13:23:28.695801: Steps 63/138: batch_recall = 70.22, batch_ndcg = 39.55 
2025-04-08 13:23:30.031111: Steps 64/138: batch_recall = 56.28, batch_ndcg = 28.75 
2025-04-08 13:23:31.364127: Steps 65/138: batch_recall = 76.92, batch_ndcg = 42.27 
2025-04-08 13:23:32.641747: Steps 66/138: batch_recall = 61.61, batch_ndcg = 36.52 
2025-04-08 13:23:33.904320: Steps 67/138: batch_recall = 79.68, batch_ndcg = 46.55 
2025-04-08 13:23:35.168485: Steps 68/138: batch_recall = 61.57, batch_ndcg = 31.54 
2025-04-08 13:23:36.424133: Steps 69/138: batch_recall = 82.65, batch_ndcg = 47.30 
2025-04-08 13:23:37.691509: Steps 70/138: batch_recall = 66.93, batch_ndcg = 38.70 
2025-04-08 13:23:38.954248: Steps 71/138: batch_recall = 75.95, batch_ndcg = 44.44 
2025-04-08 13:23:40.234084: Steps 72/138: batch_recall = 79.47, batch_ndcg = 47.44 
2025-04-08 13:23:41.500244: Steps 73/138: batch_recall = 79.44, batch_ndcg = 42.39 
2025-04-08 13:23:42.776288: Steps 74/138: batch_recall = 72.18, batch_ndcg = 43.99 
2025-04-08 13:23:44.041348: Steps 75/138: batch_recall = 74.61, batch_ndcg = 44.21 
2025-04-08 13:23:45.298999: Steps 76/138: batch_recall = 87.47, batch_ndcg = 51.74 
2025-04-08 13:23:46.558805: Steps 77/138: batch_recall = 76.31, batch_ndcg = 42.61 
2025-04-08 13:23:47.828001: Steps 78/138: batch_recall = 92.44, batch_ndcg = 46.58 
2025-04-08 13:23:49.095959: Steps 79/138: batch_recall = 80.58, batch_ndcg = 44.40 
2025-04-08 13:23:50.356545: Steps 80/138: batch_recall = 74.84, batch_ndcg = 39.25 
2025-04-08 13:23:51.642171: Steps 81/138: batch_recall = 79.01, batch_ndcg = 45.23 
2025-04-08 13:23:52.902802: Steps 82/138: batch_recall = 78.52, batch_ndcg = 45.07 
2025-04-08 13:23:54.174749: Steps 83/138: batch_recall = 71.77, batch_ndcg = 41.99 
2025-04-08 13:23:55.446175: Steps 84/138: batch_recall = 99.15, batch_ndcg = 52.92 
2025-04-08 13:23:56.733100: Steps 85/138: batch_recall = 86.12, batch_ndcg = 49.04 
2025-04-08 13:23:57.987664: Steps 86/138: batch_recall = 113.30, batch_ndcg = 64.99 
2025-04-08 13:23:59.261988: Steps 87/138: batch_recall = 96.44, batch_ndcg = 53.16 
2025-04-08 13:24:00.559025: Steps 88/138: batch_recall = 98.89, batch_ndcg = 54.46 
2025-04-08 13:24:01.843992: Steps 89/138: batch_recall = 108.91, batch_ndcg = 60.84 
2025-04-08 13:24:03.109003: Steps 90/138: batch_recall = 100.67, batch_ndcg = 54.66 
2025-04-08 13:24:04.362575: Steps 91/138: batch_recall = 117.37, batch_ndcg = 64.37 
2025-04-08 13:24:05.622325: Steps 92/138: batch_recall = 102.35, batch_ndcg = 56.07 
2025-04-08 13:24:06.898091: Steps 93/138: batch_recall = 107.12, batch_ndcg = 61.97 
2025-04-08 13:24:08.170858: Steps 94/138: batch_recall = 105.68, batch_ndcg = 55.75 
2025-04-08 13:24:09.447648: Steps 95/138: batch_recall = 109.79, batch_ndcg = 62.36 
2025-04-08 13:24:10.714972: Steps 96/138: batch_recall = 121.18, batch_ndcg = 68.24 
2025-04-08 13:24:11.992558: Steps 97/138: batch_recall = 129.87, batch_ndcg = 69.24 
2025-04-08 13:24:13.259079: Steps 98/138: batch_recall = 99.59, batch_ndcg = 56.34 
2025-04-08 13:24:14.534810: Steps 99/138: batch_recall = 110.44, batch_ndcg = 60.84 
2025-04-08 13:24:15.820703: Steps 100/138: batch_recall = 110.52, batch_ndcg = 64.91 
2025-04-08 13:24:17.080419: Steps 101/138: batch_recall = 120.94, batch_ndcg = 67.18 
2025-04-08 13:24:18.362600: Steps 102/138: batch_recall = 119.81, batch_ndcg = 65.89 
2025-04-08 13:24:19.628348: Steps 103/138: batch_recall = 123.85, batch_ndcg = 68.52 
2025-04-08 13:24:20.888866: Steps 104/138: batch_recall = 122.46, batch_ndcg = 64.27 
2025-04-08 13:24:22.163855: Steps 105/138: batch_recall = 116.36, batch_ndcg = 64.50 
2025-04-08 13:24:23.433279: Steps 106/138: batch_recall = 104.47, batch_ndcg = 54.66 
2025-04-08 13:24:24.720635: Steps 107/138: batch_recall = 110.57, batch_ndcg = 56.78 
2025-04-08 13:24:26.004355: Steps 108/138: batch_recall = 110.27, batch_ndcg = 65.00 
2025-04-08 13:24:27.304140: Steps 109/138: batch_recall = 122.73, batch_ndcg = 65.47 
2025-04-08 13:24:28.571052: Steps 110/138: batch_recall = 112.41, batch_ndcg = 58.40 
2025-04-08 13:24:29.837198: Steps 111/138: batch_recall = 125.08, batch_ndcg = 72.01 
2025-04-08 13:24:31.108355: Steps 112/138: batch_recall = 145.31, batch_ndcg = 76.09 
2025-04-08 13:24:32.394943: Steps 113/138: batch_recall = 120.74, batch_ndcg = 65.11 
2025-04-08 13:24:33.665724: Steps 114/138: batch_recall = 112.64, batch_ndcg = 61.90 
2025-04-08 13:24:34.922468: Steps 115/138: batch_recall = 105.51, batch_ndcg = 53.41 
2025-04-08 13:24:36.177236: Steps 116/138: batch_recall = 112.69, batch_ndcg = 57.52 
2025-04-08 13:24:37.461223: Steps 117/138: batch_recall = 110.83, batch_ndcg = 61.57 
2025-04-08 13:24:38.743652: Steps 118/138: batch_recall = 123.63, batch_ndcg = 66.80 
2025-04-08 13:24:40.028611: Steps 119/138: batch_recall = 127.48, batch_ndcg = 68.20 
2025-04-08 13:24:41.295731: Steps 120/138: batch_recall = 114.80, batch_ndcg = 61.27 
2025-04-08 13:24:42.588111: Steps 121/138: batch_recall = 137.33, batch_ndcg = 69.60 
2025-04-08 13:24:43.856921: Steps 122/138: batch_recall = 126.44, batch_ndcg = 68.21 
2025-04-08 13:24:45.128458: Steps 123/138: batch_recall = 116.67, batch_ndcg = 63.97 
2025-04-08 13:24:46.391438: Steps 124/138: batch_recall = 139.19, batch_ndcg = 87.39 
2025-04-08 13:24:47.657324: Steps 125/138: batch_recall = 118.92, batch_ndcg = 62.24 
2025-04-08 13:24:48.921889: Steps 126/138: batch_recall = 152.38, batch_ndcg = 82.96 
2025-04-08 13:24:50.183863: Steps 127/138: batch_recall = 128.06, batch_ndcg = 73.08 
2025-04-08 13:24:51.442373: Steps 128/138: batch_recall = 118.64, batch_ndcg = 60.00 
2025-04-08 13:24:52.712062: Steps 129/138: batch_recall = 131.87, batch_ndcg = 79.16 
2025-04-08 13:24:53.987614: Steps 130/138: batch_recall = 117.80, batch_ndcg = 62.02 
2025-04-08 13:24:55.253071: Steps 131/138: batch_recall = 124.00, batch_ndcg = 73.71 
2025-04-08 13:24:56.518442: Steps 132/138: batch_recall = 137.80, batch_ndcg = 76.39 
2025-04-08 13:24:57.783587: Steps 133/138: batch_recall = 139.70, batch_ndcg = 77.07 
2025-04-08 13:24:59.101237: Steps 134/138: batch_recall = 129.46, batch_ndcg = 70.43 
2025-04-08 13:25:00.368252: Steps 135/138: batch_recall = 156.23, batch_ndcg = 87.18 
2025-04-08 13:25:01.636045: Steps 136/138: batch_recall = 144.20, batch_ndcg = 75.40 
2025-04-08 13:25:02.897596: Steps 137/138: batch_recall = 126.70, batch_ndcg = 79.61 
2025-04-08 13:25:02.898118: Epoch 3/1000, Test: Recall = 0.1597, NDCG = 0.0893  

2025-04-08 13:25:04.646224: Training Step 0/354: batchLoss = 0.6402, diffLoss = 3.1460, kgLoss = 0.0137
2025-04-08 13:25:06.257311: Training Step 1/354: batchLoss = 0.6242, diffLoss = 3.0647, kgLoss = 0.0141
2025-04-08 13:25:07.877238: Training Step 2/354: batchLoss = 0.6402, diffLoss = 3.1407, kgLoss = 0.0151
2025-04-08 13:25:09.490584: Training Step 3/354: batchLoss = 0.5844, diffLoss = 2.8690, kgLoss = 0.0132
2025-04-08 13:25:11.118980: Training Step 4/354: batchLoss = 0.8931, diffLoss = 4.3826, kgLoss = 0.0208
2025-04-08 13:25:12.745095: Training Step 5/354: batchLoss = 0.6150, diffLoss = 3.0247, kgLoss = 0.0126
2025-04-08 13:25:14.365257: Training Step 6/354: batchLoss = 0.7493, diffLoss = 3.6809, kgLoss = 0.0164
2025-04-08 13:25:15.988408: Training Step 7/354: batchLoss = 0.6040, diffLoss = 2.9664, kgLoss = 0.0134
2025-04-08 13:25:17.602586: Training Step 8/354: batchLoss = 0.5686, diffLoss = 2.7918, kgLoss = 0.0128
2025-04-08 13:25:19.211496: Training Step 9/354: batchLoss = 0.8491, diffLoss = 4.1748, kgLoss = 0.0177
2025-04-08 13:25:20.824014: Training Step 10/354: batchLoss = 0.7146, diffLoss = 3.5098, kgLoss = 0.0158
2025-04-08 13:25:22.450410: Training Step 11/354: batchLoss = 0.5927, diffLoss = 2.9100, kgLoss = 0.0133
2025-04-08 13:25:24.075889: Training Step 12/354: batchLoss = 0.6608, diffLoss = 3.2452, kgLoss = 0.0147
2025-04-08 13:25:25.700995: Training Step 13/354: batchLoss = 0.5903, diffLoss = 2.8970, kgLoss = 0.0136
2025-04-08 13:25:27.310800: Training Step 14/354: batchLoss = 0.5614, diffLoss = 2.7524, kgLoss = 0.0137
2025-04-08 13:25:28.930849: Training Step 15/354: batchLoss = 0.6293, diffLoss = 3.0912, kgLoss = 0.0138
2025-04-08 13:25:30.545684: Training Step 16/354: batchLoss = 0.6130, diffLoss = 3.0090, kgLoss = 0.0140
2025-04-08 13:25:32.167981: Training Step 17/354: batchLoss = 0.6423, diffLoss = 3.1553, kgLoss = 0.0141
2025-04-08 13:25:33.780290: Training Step 18/354: batchLoss = 0.5978, diffLoss = 2.9364, kgLoss = 0.0132
2025-04-08 13:25:35.392802: Training Step 19/354: batchLoss = 0.6478, diffLoss = 3.1833, kgLoss = 0.0139
2025-04-08 13:25:37.014266: Training Step 20/354: batchLoss = 0.4840, diffLoss = 2.3712, kgLoss = 0.0122
2025-04-08 13:25:38.638248: Training Step 21/354: batchLoss = 0.5906, diffLoss = 2.8982, kgLoss = 0.0136
2025-04-08 13:25:40.256897: Training Step 22/354: batchLoss = 0.5268, diffLoss = 2.5875, kgLoss = 0.0116
2025-04-08 13:25:41.879422: Training Step 23/354: batchLoss = 0.6675, diffLoss = 3.2811, kgLoss = 0.0141
2025-04-08 13:25:43.505139: Training Step 24/354: batchLoss = 0.6688, diffLoss = 3.2878, kgLoss = 0.0140
2025-04-08 13:25:45.118095: Training Step 25/354: batchLoss = 0.5418, diffLoss = 2.6610, kgLoss = 0.0120
2025-04-08 13:25:46.740692: Training Step 26/354: batchLoss = 0.7308, diffLoss = 3.5830, kgLoss = 0.0177
2025-04-08 13:25:48.350635: Training Step 27/354: batchLoss = 0.6375, diffLoss = 3.1310, kgLoss = 0.0141
2025-04-08 13:25:49.967489: Training Step 28/354: batchLoss = 0.5652, diffLoss = 2.7753, kgLoss = 0.0126
2025-04-08 13:25:51.595169: Training Step 29/354: batchLoss = 0.5807, diffLoss = 2.8522, kgLoss = 0.0128
2025-04-08 13:25:53.211710: Training Step 30/354: batchLoss = 0.6256, diffLoss = 3.0715, kgLoss = 0.0142
2025-04-08 13:25:54.828232: Training Step 31/354: batchLoss = 0.6085, diffLoss = 2.9902, kgLoss = 0.0130
2025-04-08 13:25:56.457509: Training Step 32/354: batchLoss = 0.8761, diffLoss = 4.3063, kgLoss = 0.0186
2025-04-08 13:25:58.074777: Training Step 33/354: batchLoss = 0.6366, diffLoss = 3.1271, kgLoss = 0.0139
2025-04-08 13:25:59.693343: Training Step 34/354: batchLoss = 0.6256, diffLoss = 3.0737, kgLoss = 0.0135
2025-04-08 13:26:01.301212: Training Step 35/354: batchLoss = 0.5895, diffLoss = 2.8847, kgLoss = 0.0157
2025-04-08 13:26:02.912432: Training Step 36/354: batchLoss = 0.5638, diffLoss = 2.7661, kgLoss = 0.0132
2025-04-08 13:26:04.526511: Training Step 37/354: batchLoss = 0.7654, diffLoss = 3.7550, kgLoss = 0.0180
2025-04-08 13:26:06.157745: Training Step 38/354: batchLoss = 0.4960, diffLoss = 2.4297, kgLoss = 0.0126
2025-04-08 13:26:07.779267: Training Step 39/354: batchLoss = 1.3104, diffLoss = 6.4469, kgLoss = 0.0263
2025-04-08 13:26:09.399021: Training Step 40/354: batchLoss = 0.6668, diffLoss = 3.2752, kgLoss = 0.0147
2025-04-08 13:26:11.021360: Training Step 41/354: batchLoss = 0.4752, diffLoss = 2.3292, kgLoss = 0.0117
2025-04-08 13:26:12.646101: Training Step 42/354: batchLoss = 0.6744, diffLoss = 3.3130, kgLoss = 0.0147
2025-04-08 13:26:14.270535: Training Step 43/354: batchLoss = 0.6444, diffLoss = 3.1679, kgLoss = 0.0135
2025-04-08 13:26:15.878644: Training Step 44/354: batchLoss = 0.5983, diffLoss = 2.9371, kgLoss = 0.0136
2025-04-08 13:26:17.486435: Training Step 45/354: batchLoss = 0.8467, diffLoss = 4.1560, kgLoss = 0.0193
2025-04-08 13:26:19.098644: Training Step 46/354: batchLoss = 0.6772, diffLoss = 3.3232, kgLoss = 0.0157
2025-04-08 13:26:20.715486: Training Step 47/354: batchLoss = 0.4971, diffLoss = 2.4364, kgLoss = 0.0122
2025-04-08 13:26:22.335970: Training Step 48/354: batchLoss = 0.6454, diffLoss = 3.1676, kgLoss = 0.0148
2025-04-08 13:26:23.946528: Training Step 49/354: batchLoss = 0.6508, diffLoss = 3.1981, kgLoss = 0.0139
2025-04-08 13:26:25.570249: Training Step 50/354: batchLoss = 0.6070, diffLoss = 2.9850, kgLoss = 0.0125
2025-04-08 13:26:27.198747: Training Step 51/354: batchLoss = 0.5589, diffLoss = 2.7426, kgLoss = 0.0130
2025-04-08 13:26:28.831063: Training Step 52/354: batchLoss = 0.5432, diffLoss = 2.6648, kgLoss = 0.0128
2025-04-08 13:26:30.442438: Training Step 53/354: batchLoss = 0.5962, diffLoss = 2.9299, kgLoss = 0.0128
2025-04-08 13:26:32.049999: Training Step 54/354: batchLoss = 0.5339, diffLoss = 2.6174, kgLoss = 0.0130
2025-04-08 13:26:33.671039: Training Step 55/354: batchLoss = 0.6418, diffLoss = 3.1510, kgLoss = 0.0145
2025-04-08 13:26:35.288973: Training Step 56/354: batchLoss = 0.7093, diffLoss = 3.4824, kgLoss = 0.0161
2025-04-08 13:26:36.907360: Training Step 57/354: batchLoss = 0.5033, diffLoss = 2.4692, kgLoss = 0.0118
2025-04-08 13:26:38.545510: Training Step 58/354: batchLoss = 0.6047, diffLoss = 2.9671, kgLoss = 0.0141
2025-04-08 13:26:40.170226: Training Step 59/354: batchLoss = 0.6913, diffLoss = 3.3989, kgLoss = 0.0144
2025-04-08 13:26:41.798662: Training Step 60/354: batchLoss = 0.5492, diffLoss = 2.7041, kgLoss = 0.0105
2025-04-08 13:26:43.417462: Training Step 61/354: batchLoss = 0.6278, diffLoss = 3.0757, kgLoss = 0.0158
2025-04-08 13:26:45.027012: Training Step 62/354: batchLoss = 0.6024, diffLoss = 2.9648, kgLoss = 0.0118
2025-04-08 13:26:46.633819: Training Step 63/354: batchLoss = 0.5878, diffLoss = 2.8811, kgLoss = 0.0145
2025-04-08 13:26:48.240809: Training Step 64/354: batchLoss = 0.7231, diffLoss = 3.5545, kgLoss = 0.0153
2025-04-08 13:26:49.855542: Training Step 65/354: batchLoss = 0.6932, diffLoss = 3.3995, kgLoss = 0.0167
2025-04-08 13:26:51.486306: Training Step 66/354: batchLoss = 0.5117, diffLoss = 2.5094, kgLoss = 0.0123
2025-04-08 13:26:53.107765: Training Step 67/354: batchLoss = 0.7226, diffLoss = 3.5537, kgLoss = 0.0149
2025-04-08 13:26:54.725978: Training Step 68/354: batchLoss = 0.9968, diffLoss = 4.9022, kgLoss = 0.0204
2025-04-08 13:26:56.351558: Training Step 69/354: batchLoss = 0.7010, diffLoss = 3.4405, kgLoss = 0.0161
2025-04-08 13:26:57.989391: Training Step 70/354: batchLoss = 0.6719, diffLoss = 3.2961, kgLoss = 0.0159
2025-04-08 13:26:59.609233: Training Step 71/354: batchLoss = 0.6104, diffLoss = 2.9937, kgLoss = 0.0146
2025-04-08 13:27:01.229084: Training Step 72/354: batchLoss = 0.6550, diffLoss = 3.2192, kgLoss = 0.0140
2025-04-08 13:27:02.839000: Training Step 73/354: batchLoss = 0.5829, diffLoss = 2.8602, kgLoss = 0.0136
2025-04-08 13:27:04.458900: Training Step 74/354: batchLoss = 0.6658, diffLoss = 3.2646, kgLoss = 0.0161
2025-04-08 13:27:06.079406: Training Step 75/354: batchLoss = 0.7226, diffLoss = 3.5444, kgLoss = 0.0172
2025-04-08 13:27:07.696460: Training Step 76/354: batchLoss = 0.5692, diffLoss = 2.7907, kgLoss = 0.0139
2025-04-08 13:27:09.313161: Training Step 77/354: batchLoss = 0.8619, diffLoss = 4.2422, kgLoss = 0.0168
2025-04-08 13:27:10.928982: Training Step 78/354: batchLoss = 0.5992, diffLoss = 2.9419, kgLoss = 0.0135
2025-04-08 13:27:12.546734: Training Step 79/354: batchLoss = 0.6617, diffLoss = 3.2490, kgLoss = 0.0149
2025-04-08 13:27:14.162034: Training Step 80/354: batchLoss = 0.5405, diffLoss = 2.6576, kgLoss = 0.0113
2025-04-08 13:27:15.764918: Training Step 81/354: batchLoss = 0.7058, diffLoss = 3.4702, kgLoss = 0.0146
2025-04-08 13:27:17.382549: Training Step 82/354: batchLoss = 0.7299, diffLoss = 3.5832, kgLoss = 0.0166
2025-04-08 13:27:19.001473: Training Step 83/354: batchLoss = 0.6257, diffLoss = 3.0703, kgLoss = 0.0146
2025-04-08 13:27:20.617219: Training Step 84/354: batchLoss = 0.7362, diffLoss = 3.6204, kgLoss = 0.0152
2025-04-08 13:27:22.240269: Training Step 85/354: batchLoss = 0.5702, diffLoss = 2.7995, kgLoss = 0.0129
2025-04-08 13:27:23.860875: Training Step 86/354: batchLoss = 0.6684, diffLoss = 3.2879, kgLoss = 0.0136
2025-04-08 13:27:25.483796: Training Step 87/354: batchLoss = 0.8234, diffLoss = 4.0306, kgLoss = 0.0216
2025-04-08 13:27:27.098721: Training Step 88/354: batchLoss = 0.7206, diffLoss = 3.5394, kgLoss = 0.0159
2025-04-08 13:27:28.708177: Training Step 89/354: batchLoss = 0.7145, diffLoss = 3.5075, kgLoss = 0.0162
2025-04-08 13:27:30.317346: Training Step 90/354: batchLoss = 0.6622, diffLoss = 3.2452, kgLoss = 0.0164
2025-04-08 13:27:31.921591: Training Step 91/354: batchLoss = 0.5714, diffLoss = 2.7980, kgLoss = 0.0147
2025-04-08 13:27:33.542960: Training Step 92/354: batchLoss = 0.7545, diffLoss = 3.7026, kgLoss = 0.0175
2025-04-08 13:27:35.160761: Training Step 93/354: batchLoss = 0.6929, diffLoss = 3.4093, kgLoss = 0.0138
2025-04-08 13:27:36.772976: Training Step 94/354: batchLoss = 0.5823, diffLoss = 2.8546, kgLoss = 0.0142
2025-04-08 13:27:38.396500: Training Step 95/354: batchLoss = 0.6602, diffLoss = 3.2410, kgLoss = 0.0150
2025-04-08 13:27:40.015199: Training Step 96/354: batchLoss = 0.5297, diffLoss = 2.5959, kgLoss = 0.0131
2025-04-08 13:27:41.630519: Training Step 97/354: batchLoss = 0.8624, diffLoss = 4.2373, kgLoss = 0.0187
2025-04-08 13:27:43.243240: Training Step 98/354: batchLoss = 0.5954, diffLoss = 2.9257, kgLoss = 0.0128
2025-04-08 13:27:44.849619: Training Step 99/354: batchLoss = 0.5399, diffLoss = 2.6436, kgLoss = 0.0140
2025-04-08 13:27:46.461425: Training Step 100/354: batchLoss = 0.7474, diffLoss = 3.6680, kgLoss = 0.0172
2025-04-08 13:27:48.080325: Training Step 101/354: batchLoss = 0.5659, diffLoss = 2.7716, kgLoss = 0.0145
2025-04-08 13:27:49.696124: Training Step 102/354: batchLoss = 0.6917, diffLoss = 3.3967, kgLoss = 0.0155
2025-04-08 13:27:51.308408: Training Step 103/354: batchLoss = 0.7325, diffLoss = 3.6021, kgLoss = 0.0150
2025-04-08 13:27:52.930199: Training Step 104/354: batchLoss = 0.6133, diffLoss = 3.0098, kgLoss = 0.0142
2025-04-08 13:27:54.566447: Training Step 105/354: batchLoss = 0.7593, diffLoss = 3.7295, kgLoss = 0.0167
2025-04-08 13:27:56.198722: Training Step 106/354: batchLoss = 0.6514, diffLoss = 3.1962, kgLoss = 0.0152
2025-04-08 13:27:57.820829: Training Step 107/354: batchLoss = 0.6175, diffLoss = 3.0331, kgLoss = 0.0136
2025-04-08 13:27:59.444993: Training Step 108/354: batchLoss = 0.7231, diffLoss = 3.5457, kgLoss = 0.0175
2025-04-08 13:28:01.073619: Training Step 109/354: batchLoss = 0.6860, diffLoss = 3.3766, kgLoss = 0.0134
2025-04-08 13:28:02.696007: Training Step 110/354: batchLoss = 0.5894, diffLoss = 2.8926, kgLoss = 0.0136
2025-04-08 13:28:04.331755: Training Step 111/354: batchLoss = 0.6109, diffLoss = 3.0066, kgLoss = 0.0120
2025-04-08 13:28:05.987230: Training Step 112/354: batchLoss = 0.6470, diffLoss = 3.1721, kgLoss = 0.0157
2025-04-08 13:28:07.632882: Training Step 113/354: batchLoss = 0.4749, diffLoss = 2.3284, kgLoss = 0.0115
2025-04-08 13:28:09.250850: Training Step 114/354: batchLoss = 0.5904, diffLoss = 2.8988, kgLoss = 0.0133
2025-04-08 13:28:10.864095: Training Step 115/354: batchLoss = 0.6472, diffLoss = 3.1739, kgLoss = 0.0155
2025-04-08 13:28:12.490458: Training Step 116/354: batchLoss = 0.5266, diffLoss = 2.5790, kgLoss = 0.0135
2025-04-08 13:28:14.106670: Training Step 117/354: batchLoss = 0.6437, diffLoss = 3.1507, kgLoss = 0.0170
2025-04-08 13:28:15.713273: Training Step 118/354: batchLoss = 0.6108, diffLoss = 2.9999, kgLoss = 0.0135
2025-04-08 13:28:17.332940: Training Step 119/354: batchLoss = 0.6396, diffLoss = 3.1408, kgLoss = 0.0143
2025-04-08 13:28:18.944557: Training Step 120/354: batchLoss = 0.6653, diffLoss = 3.2737, kgLoss = 0.0132
2025-04-08 13:28:20.571732: Training Step 121/354: batchLoss = 0.7618, diffLoss = 3.7400, kgLoss = 0.0172
2025-04-08 13:28:22.186419: Training Step 122/354: batchLoss = 0.7073, diffLoss = 3.4721, kgLoss = 0.0161
2025-04-08 13:28:23.804758: Training Step 123/354: batchLoss = 0.7982, diffLoss = 3.9237, kgLoss = 0.0168
2025-04-08 13:28:25.423133: Training Step 124/354: batchLoss = 0.6188, diffLoss = 3.0408, kgLoss = 0.0133
2025-04-08 13:28:27.044137: Training Step 125/354: batchLoss = 0.7023, diffLoss = 3.4498, kgLoss = 0.0155
2025-04-08 13:28:28.650853: Training Step 126/354: batchLoss = 0.7051, diffLoss = 3.4662, kgLoss = 0.0149
2025-04-08 13:28:30.263934: Training Step 127/354: batchLoss = 0.6585, diffLoss = 3.2284, kgLoss = 0.0160
2025-04-08 13:28:31.891855: Training Step 128/354: batchLoss = 0.7505, diffLoss = 3.6846, kgLoss = 0.0169
2025-04-08 13:28:33.512864: Training Step 129/354: batchLoss = 0.6141, diffLoss = 3.0062, kgLoss = 0.0161
2025-04-08 13:28:35.128790: Training Step 130/354: batchLoss = 0.5431, diffLoss = 2.6654, kgLoss = 0.0125
2025-04-08 13:28:36.762558: Training Step 131/354: batchLoss = 0.6678, diffLoss = 3.2784, kgLoss = 0.0151
2025-04-08 13:28:38.386574: Training Step 132/354: batchLoss = 0.6752, diffLoss = 3.3133, kgLoss = 0.0157
2025-04-08 13:28:40.008102: Training Step 133/354: batchLoss = 0.5861, diffLoss = 2.8756, kgLoss = 0.0137
2025-04-08 13:28:41.619608: Training Step 134/354: batchLoss = 0.6292, diffLoss = 3.0901, kgLoss = 0.0140
2025-04-08 13:28:43.227280: Training Step 135/354: batchLoss = 0.7433, diffLoss = 3.6562, kgLoss = 0.0151
2025-04-08 13:28:44.832257: Training Step 136/354: batchLoss = 0.6211, diffLoss = 3.0473, kgLoss = 0.0145
2025-04-08 13:28:46.448289: Training Step 137/354: batchLoss = 0.6670, diffLoss = 3.2742, kgLoss = 0.0152
2025-04-08 13:28:48.078994: Training Step 138/354: batchLoss = 0.7397, diffLoss = 3.6402, kgLoss = 0.0146
2025-04-08 13:28:49.709158: Training Step 139/354: batchLoss = 0.5645, diffLoss = 2.7709, kgLoss = 0.0129
2025-04-08 13:28:51.343518: Training Step 140/354: batchLoss = 0.6155, diffLoss = 3.0176, kgLoss = 0.0149
2025-04-08 13:28:52.991855: Training Step 141/354: batchLoss = 0.5989, diffLoss = 2.9417, kgLoss = 0.0132
2025-04-08 13:28:54.604481: Training Step 142/354: batchLoss = 0.6339, diffLoss = 3.1152, kgLoss = 0.0136
2025-04-08 13:28:56.220409: Training Step 143/354: batchLoss = 0.6027, diffLoss = 2.9617, kgLoss = 0.0129
2025-04-08 13:28:57.826699: Training Step 144/354: batchLoss = 0.6032, diffLoss = 2.9636, kgLoss = 0.0131
2025-04-08 13:28:59.439792: Training Step 145/354: batchLoss = 0.5981, diffLoss = 2.9306, kgLoss = 0.0150
2025-04-08 13:29:01.047707: Training Step 146/354: batchLoss = 0.6185, diffLoss = 3.0399, kgLoss = 0.0132
2025-04-08 13:29:02.669707: Training Step 147/354: batchLoss = 0.5688, diffLoss = 2.7871, kgLoss = 0.0143
2025-04-08 13:29:04.301288: Training Step 148/354: batchLoss = 0.6426, diffLoss = 3.1583, kgLoss = 0.0137
2025-04-08 13:29:05.920302: Training Step 149/354: batchLoss = 0.5997, diffLoss = 2.9393, kgLoss = 0.0148
2025-04-08 13:29:07.536834: Training Step 150/354: batchLoss = 0.5928, diffLoss = 2.9069, kgLoss = 0.0142
2025-04-08 13:29:09.155343: Training Step 151/354: batchLoss = 0.5920, diffLoss = 2.9017, kgLoss = 0.0146
2025-04-08 13:29:10.769996: Training Step 152/354: batchLoss = 0.6640, diffLoss = 3.2642, kgLoss = 0.0139
2025-04-08 13:29:12.377889: Training Step 153/354: batchLoss = 0.6448, diffLoss = 3.1704, kgLoss = 0.0134
2025-04-08 13:29:13.983355: Training Step 154/354: batchLoss = 0.6863, diffLoss = 3.3717, kgLoss = 0.0150
2025-04-08 13:29:15.596030: Training Step 155/354: batchLoss = 0.6867, diffLoss = 3.3736, kgLoss = 0.0150
2025-04-08 13:29:17.217925: Training Step 156/354: batchLoss = 0.5376, diffLoss = 2.6316, kgLoss = 0.0141
2025-04-08 13:29:18.839535: Training Step 157/354: batchLoss = 0.5302, diffLoss = 2.6027, kgLoss = 0.0121
2025-04-08 13:29:20.459448: Training Step 158/354: batchLoss = 0.6762, diffLoss = 3.3199, kgLoss = 0.0153
2025-04-08 13:29:22.093693: Training Step 159/354: batchLoss = 0.7011, diffLoss = 3.4388, kgLoss = 0.0167
2025-04-08 13:29:23.714877: Training Step 160/354: batchLoss = 0.6828, diffLoss = 3.3454, kgLoss = 0.0172
2025-04-08 13:29:25.331865: Training Step 161/354: batchLoss = 0.5860, diffLoss = 2.8718, kgLoss = 0.0145
2025-04-08 13:29:26.943075: Training Step 162/354: batchLoss = 0.4559, diffLoss = 2.2310, kgLoss = 0.0121
2025-04-08 13:29:28.549694: Training Step 163/354: batchLoss = 0.5782, diffLoss = 2.8371, kgLoss = 0.0135
2025-04-08 13:29:30.175802: Training Step 164/354: batchLoss = 0.7357, diffLoss = 3.5994, kgLoss = 0.0198
2025-04-08 13:29:31.801254: Training Step 165/354: batchLoss = 0.6278, diffLoss = 3.0849, kgLoss = 0.0135
2025-04-08 13:29:33.417514: Training Step 166/354: batchLoss = 0.5521, diffLoss = 2.7015, kgLoss = 0.0148
2025-04-08 13:29:35.037918: Training Step 167/354: batchLoss = 0.6408, diffLoss = 3.1517, kgLoss = 0.0131
2025-04-08 13:29:36.665398: Training Step 168/354: batchLoss = 0.6888, diffLoss = 3.3823, kgLoss = 0.0155
2025-04-08 13:29:38.285389: Training Step 169/354: batchLoss = 0.6674, diffLoss = 3.2789, kgLoss = 0.0145
2025-04-08 13:29:39.893311: Training Step 170/354: batchLoss = 0.6501, diffLoss = 3.1956, kgLoss = 0.0138
2025-04-08 13:29:41.502623: Training Step 171/354: batchLoss = 0.7108, diffLoss = 3.4953, kgLoss = 0.0147
2025-04-08 13:29:43.112880: Training Step 172/354: batchLoss = 0.6372, diffLoss = 3.1262, kgLoss = 0.0150
2025-04-08 13:29:44.721386: Training Step 173/354: batchLoss = 0.6581, diffLoss = 3.2330, kgLoss = 0.0143
2025-04-08 13:29:46.336628: Training Step 174/354: batchLoss = 0.6113, diffLoss = 2.9990, kgLoss = 0.0144
2025-04-08 13:29:47.955892: Training Step 175/354: batchLoss = 0.5347, diffLoss = 2.6223, kgLoss = 0.0128
2025-04-08 13:29:49.591033: Training Step 176/354: batchLoss = 0.9043, diffLoss = 4.4404, kgLoss = 0.0202
2025-04-08 13:29:51.207947: Training Step 177/354: batchLoss = 0.6537, diffLoss = 3.2115, kgLoss = 0.0143
2025-04-08 13:29:52.827684: Training Step 178/354: batchLoss = 0.5937, diffLoss = 2.9211, kgLoss = 0.0119
2025-04-08 13:29:54.448656: Training Step 179/354: batchLoss = 0.6634, diffLoss = 3.2560, kgLoss = 0.0152
2025-04-08 13:29:56.058742: Training Step 180/354: batchLoss = 0.8721, diffLoss = 4.2854, kgLoss = 0.0188
2025-04-08 13:29:57.671857: Training Step 181/354: batchLoss = 0.6291, diffLoss = 3.0878, kgLoss = 0.0144
2025-04-08 13:29:59.280850: Training Step 182/354: batchLoss = 0.5820, diffLoss = 2.8606, kgLoss = 0.0124
2025-04-08 13:30:00.906385: Training Step 183/354: batchLoss = 0.6414, diffLoss = 3.1511, kgLoss = 0.0140
2025-04-08 13:30:02.526819: Training Step 184/354: batchLoss = 0.7229, diffLoss = 3.5555, kgLoss = 0.0148
2025-04-08 13:30:04.146032: Training Step 185/354: batchLoss = 0.5751, diffLoss = 2.8210, kgLoss = 0.0136
2025-04-08 13:30:05.773786: Training Step 186/354: batchLoss = 0.4902, diffLoss = 2.4018, kgLoss = 0.0123
2025-04-08 13:30:07.407125: Training Step 187/354: batchLoss = 0.6670, diffLoss = 3.2718, kgLoss = 0.0158
2025-04-08 13:30:09.017461: Training Step 188/354: batchLoss = 0.5193, diffLoss = 2.5484, kgLoss = 0.0121
2025-04-08 13:30:10.639697: Training Step 189/354: batchLoss = 0.6004, diffLoss = 2.9418, kgLoss = 0.0151
2025-04-08 13:30:12.249847: Training Step 190/354: batchLoss = 0.6653, diffLoss = 3.2654, kgLoss = 0.0153
2025-04-08 13:30:13.859500: Training Step 191/354: batchLoss = 0.7733, diffLoss = 3.7993, kgLoss = 0.0168
2025-04-08 13:30:15.469419: Training Step 192/354: batchLoss = 0.6627, diffLoss = 3.2474, kgLoss = 0.0165
2025-04-08 13:30:17.088650: Training Step 193/354: batchLoss = 0.4977, diffLoss = 2.4406, kgLoss = 0.0120
2025-04-08 13:30:18.707972: Training Step 194/354: batchLoss = 0.6404, diffLoss = 3.1459, kgLoss = 0.0141
2025-04-08 13:30:20.331057: Training Step 195/354: batchLoss = 0.6390, diffLoss = 3.1321, kgLoss = 0.0158
2025-04-08 13:30:21.962051: Training Step 196/354: batchLoss = 0.5766, diffLoss = 2.8255, kgLoss = 0.0143
2025-04-08 13:30:23.579689: Training Step 197/354: batchLoss = 0.6017, diffLoss = 2.9554, kgLoss = 0.0133
2025-04-08 13:30:25.187291: Training Step 198/354: batchLoss = 0.6517, diffLoss = 3.2010, kgLoss = 0.0144
2025-04-08 13:30:26.791563: Training Step 199/354: batchLoss = 0.5449, diffLoss = 2.6746, kgLoss = 0.0125
2025-04-08 13:30:28.403871: Training Step 200/354: batchLoss = 0.6884, diffLoss = 3.3733, kgLoss = 0.0171
2025-04-08 13:30:30.023853: Training Step 201/354: batchLoss = 0.6191, diffLoss = 3.0424, kgLoss = 0.0133
2025-04-08 13:30:31.651931: Training Step 202/354: batchLoss = 0.5419, diffLoss = 2.6618, kgLoss = 0.0119
2025-04-08 13:30:33.276468: Training Step 203/354: batchLoss = 0.6500, diffLoss = 3.1953, kgLoss = 0.0137
2025-04-08 13:30:34.905810: Training Step 204/354: batchLoss = 0.5883, diffLoss = 2.8899, kgLoss = 0.0128
2025-04-08 13:30:36.535883: Training Step 205/354: batchLoss = 0.6603, diffLoss = 3.2508, kgLoss = 0.0127
2025-04-08 13:30:38.149926: Training Step 206/354: batchLoss = 0.6035, diffLoss = 2.9659, kgLoss = 0.0130
2025-04-08 13:30:39.757696: Training Step 207/354: batchLoss = 0.7274, diffLoss = 3.5729, kgLoss = 0.0160
2025-04-08 13:30:41.368977: Training Step 208/354: batchLoss = 0.5681, diffLoss = 2.7879, kgLoss = 0.0131
2025-04-08 13:30:42.982420: Training Step 209/354: batchLoss = 0.5551, diffLoss = 2.7270, kgLoss = 0.0121
2025-04-08 13:30:44.612624: Training Step 210/354: batchLoss = 0.6070, diffLoss = 2.9736, kgLoss = 0.0153
2025-04-08 13:30:46.245860: Training Step 211/354: batchLoss = 0.6286, diffLoss = 3.0864, kgLoss = 0.0142
2025-04-08 13:30:47.862224: Training Step 212/354: batchLoss = 0.6333, diffLoss = 3.1026, kgLoss = 0.0160
2025-04-08 13:30:49.477101: Training Step 213/354: batchLoss = 0.7129, diffLoss = 3.5038, kgLoss = 0.0152
2025-04-08 13:30:51.099861: Training Step 214/354: batchLoss = 0.5509, diffLoss = 2.7049, kgLoss = 0.0124
2025-04-08 13:30:52.725911: Training Step 215/354: batchLoss = 0.6443, diffLoss = 3.1679, kgLoss = 0.0134
2025-04-08 13:30:54.334018: Training Step 216/354: batchLoss = 0.6591, diffLoss = 3.2374, kgLoss = 0.0145
2025-04-08 13:30:55.942221: Training Step 217/354: batchLoss = 0.6246, diffLoss = 3.0615, kgLoss = 0.0154
2025-04-08 13:30:57.553145: Training Step 218/354: batchLoss = 0.6261, diffLoss = 3.0734, kgLoss = 0.0142
2025-04-08 13:30:59.170070: Training Step 219/354: batchLoss = 0.6212, diffLoss = 3.0523, kgLoss = 0.0134
2025-04-08 13:31:00.786348: Training Step 220/354: batchLoss = 0.5981, diffLoss = 2.9387, kgLoss = 0.0129
2025-04-08 13:31:02.403581: Training Step 221/354: batchLoss = 0.7604, diffLoss = 3.7355, kgLoss = 0.0166
2025-04-08 13:31:04.040313: Training Step 222/354: batchLoss = 0.7450, diffLoss = 3.6597, kgLoss = 0.0163
2025-04-08 13:31:05.659953: Training Step 223/354: batchLoss = 0.5883, diffLoss = 2.8887, kgLoss = 0.0132
2025-04-08 13:31:07.281364: Training Step 224/354: batchLoss = 0.5104, diffLoss = 2.4990, kgLoss = 0.0132
2025-04-08 13:31:08.896302: Training Step 225/354: batchLoss = 0.5575, diffLoss = 2.7321, kgLoss = 0.0139
2025-04-08 13:31:10.505750: Training Step 226/354: batchLoss = 0.6083, diffLoss = 2.9788, kgLoss = 0.0156
2025-04-08 13:31:12.112346: Training Step 227/354: batchLoss = 0.5975, diffLoss = 2.9357, kgLoss = 0.0130
2025-04-08 13:31:13.743016: Training Step 228/354: batchLoss = 0.6804, diffLoss = 3.3460, kgLoss = 0.0140
2025-04-08 13:31:15.368592: Training Step 229/354: batchLoss = 0.5667, diffLoss = 2.7795, kgLoss = 0.0135
2025-04-08 13:31:16.991550: Training Step 230/354: batchLoss = 0.6462, diffLoss = 3.1727, kgLoss = 0.0146
2025-04-08 13:31:18.616715: Training Step 231/354: batchLoss = 0.5208, diffLoss = 2.5526, kgLoss = 0.0129
2025-04-08 13:31:20.237045: Training Step 232/354: batchLoss = 0.5636, diffLoss = 2.7681, kgLoss = 0.0124
2025-04-08 13:31:21.870396: Training Step 233/354: batchLoss = 0.5504, diffLoss = 2.6998, kgLoss = 0.0130
2025-04-08 13:31:23.486735: Training Step 234/354: batchLoss = 0.5718, diffLoss = 2.8076, kgLoss = 0.0128
2025-04-08 13:31:25.092490: Training Step 235/354: batchLoss = 0.6206, diffLoss = 3.0505, kgLoss = 0.0131
2025-04-08 13:31:26.721769: Training Step 236/354: batchLoss = 0.7402, diffLoss = 3.6332, kgLoss = 0.0169
2025-04-08 13:31:28.337080: Training Step 237/354: batchLoss = 0.5024, diffLoss = 2.4648, kgLoss = 0.0118
2025-04-08 13:31:29.967013: Training Step 238/354: batchLoss = 0.6713, diffLoss = 3.2959, kgLoss = 0.0152
2025-04-08 13:31:31.584399: Training Step 239/354: batchLoss = 0.7434, diffLoss = 3.6534, kgLoss = 0.0159
2025-04-08 13:31:33.214006: Training Step 240/354: batchLoss = 0.5828, diffLoss = 2.8488, kgLoss = 0.0162
2025-04-08 13:31:34.835269: Training Step 241/354: batchLoss = 0.5531, diffLoss = 2.7106, kgLoss = 0.0137
2025-04-08 13:31:36.459869: Training Step 242/354: batchLoss = 0.5930, diffLoss = 2.9083, kgLoss = 0.0142
2025-04-08 13:31:38.071749: Training Step 243/354: batchLoss = 0.6351, diffLoss = 3.1165, kgLoss = 0.0148
2025-04-08 13:31:39.675823: Training Step 244/354: batchLoss = 0.4758, diffLoss = 2.3377, kgLoss = 0.0103
2025-04-08 13:31:41.294142: Training Step 245/354: batchLoss = 0.7066, diffLoss = 3.4691, kgLoss = 0.0160
2025-04-08 13:31:42.908999: Training Step 246/354: batchLoss = 0.6373, diffLoss = 3.1278, kgLoss = 0.0147
2025-04-08 13:31:44.524498: Training Step 247/354: batchLoss = 0.7663, diffLoss = 3.7642, kgLoss = 0.0168
2025-04-08 13:31:46.147238: Training Step 248/354: batchLoss = 0.6766, diffLoss = 3.3222, kgLoss = 0.0152
2025-04-08 13:31:47.764093: Training Step 249/354: batchLoss = 0.5911, diffLoss = 2.8980, kgLoss = 0.0144
2025-04-08 13:31:49.393594: Training Step 250/354: batchLoss = 0.7619, diffLoss = 3.7480, kgLoss = 0.0154
2025-04-08 13:31:51.022482: Training Step 251/354: batchLoss = 0.4825, diffLoss = 2.3653, kgLoss = 0.0118
2025-04-08 13:31:52.635666: Training Step 252/354: batchLoss = 0.5295, diffLoss = 2.5962, kgLoss = 0.0128
2025-04-08 13:31:54.253324: Training Step 253/354: batchLoss = 0.6731, diffLoss = 3.3061, kgLoss = 0.0148
2025-04-08 13:31:55.860428: Training Step 254/354: batchLoss = 0.6450, diffLoss = 3.1658, kgLoss = 0.0148
2025-04-08 13:31:57.473091: Training Step 255/354: batchLoss = 0.5562, diffLoss = 2.7137, kgLoss = 0.0168
2025-04-08 13:31:59.090917: Training Step 256/354: batchLoss = 0.7587, diffLoss = 3.7240, kgLoss = 0.0174
2025-04-08 13:32:00.706964: Training Step 257/354: batchLoss = 0.6121, diffLoss = 3.0012, kgLoss = 0.0148
2025-04-08 13:32:02.326663: Training Step 258/354: batchLoss = 0.5452, diffLoss = 2.6785, kgLoss = 0.0119
2025-04-08 13:32:03.942481: Training Step 259/354: batchLoss = 0.6550, diffLoss = 3.2189, kgLoss = 0.0140
2025-04-08 13:32:05.559596: Training Step 260/354: batchLoss = 0.8666, diffLoss = 4.2500, kgLoss = 0.0208
2025-04-08 13:32:07.176555: Training Step 261/354: batchLoss = 0.7172, diffLoss = 3.5204, kgLoss = 0.0165
2025-04-08 13:32:08.788098: Training Step 262/354: batchLoss = 0.7362, diffLoss = 3.6168, kgLoss = 0.0160
2025-04-08 13:32:10.393518: Training Step 263/354: batchLoss = 0.6249, diffLoss = 3.0683, kgLoss = 0.0141
2025-04-08 13:32:12.004541: Training Step 264/354: batchLoss = 0.8339, diffLoss = 4.0959, kgLoss = 0.0184
2025-04-08 13:32:13.623520: Training Step 265/354: batchLoss = 0.6530, diffLoss = 3.2071, kgLoss = 0.0145
2025-04-08 13:32:15.239607: Training Step 266/354: batchLoss = 0.5486, diffLoss = 2.6883, kgLoss = 0.0137
2025-04-08 13:32:16.854304: Training Step 267/354: batchLoss = 0.5463, diffLoss = 2.6663, kgLoss = 0.0163
2025-04-08 13:32:18.473062: Training Step 268/354: batchLoss = 0.6299, diffLoss = 3.0874, kgLoss = 0.0155
2025-04-08 13:32:20.097783: Training Step 269/354: batchLoss = 0.5839, diffLoss = 2.8621, kgLoss = 0.0143
2025-04-08 13:32:21.723048: Training Step 270/354: batchLoss = 0.5631, diffLoss = 2.7661, kgLoss = 0.0123
2025-04-08 13:32:23.331342: Training Step 271/354: batchLoss = 0.6413, diffLoss = 3.1493, kgLoss = 0.0143
2025-04-08 13:32:24.941656: Training Step 272/354: batchLoss = 0.7375, diffLoss = 3.6222, kgLoss = 0.0163
2025-04-08 13:32:26.551854: Training Step 273/354: batchLoss = 0.6395, diffLoss = 3.1378, kgLoss = 0.0149
2025-04-08 13:32:28.167454: Training Step 274/354: batchLoss = 0.6577, diffLoss = 3.2236, kgLoss = 0.0163
2025-04-08 13:32:29.803405: Training Step 275/354: batchLoss = 0.7696, diffLoss = 3.7876, kgLoss = 0.0151
2025-04-08 13:32:31.444745: Training Step 276/354: batchLoss = 1.4700, diffLoss = 7.2288, kgLoss = 0.0303
2025-04-08 13:32:33.074269: Training Step 277/354: batchLoss = 0.6419, diffLoss = 3.1515, kgLoss = 0.0144
2025-04-08 13:32:34.712354: Training Step 278/354: batchLoss = 0.7262, diffLoss = 3.5626, kgLoss = 0.0171
2025-04-08 13:32:36.349012: Training Step 279/354: batchLoss = 0.6217, diffLoss = 3.0522, kgLoss = 0.0140
2025-04-08 13:32:37.967766: Training Step 280/354: batchLoss = 0.6785, diffLoss = 3.3275, kgLoss = 0.0163
2025-04-08 13:32:39.600805: Training Step 281/354: batchLoss = 0.6379, diffLoss = 3.1302, kgLoss = 0.0148
2025-04-08 13:32:41.224336: Training Step 282/354: batchLoss = 0.6487, diffLoss = 3.1811, kgLoss = 0.0156
2025-04-08 13:32:42.859398: Training Step 283/354: batchLoss = 0.6949, diffLoss = 3.4164, kgLoss = 0.0145
2025-04-08 13:32:44.489612: Training Step 284/354: batchLoss = 0.6267, diffLoss = 3.0779, kgLoss = 0.0139
2025-04-08 13:32:46.117448: Training Step 285/354: batchLoss = 0.7618, diffLoss = 3.7484, kgLoss = 0.0152
2025-04-08 13:32:47.735596: Training Step 286/354: batchLoss = 0.5803, diffLoss = 2.8515, kgLoss = 0.0125
2025-04-08 13:32:49.351242: Training Step 287/354: batchLoss = 0.4842, diffLoss = 2.3750, kgLoss = 0.0115
2025-04-08 13:32:50.969154: Training Step 288/354: batchLoss = 0.6608, diffLoss = 3.2381, kgLoss = 0.0165
2025-04-08 13:32:52.588493: Training Step 289/354: batchLoss = 0.5502, diffLoss = 2.6829, kgLoss = 0.0170
2025-04-08 13:32:54.197618: Training Step 290/354: batchLoss = 0.6275, diffLoss = 3.0781, kgLoss = 0.0149
2025-04-08 13:32:55.809308: Training Step 291/354: batchLoss = 0.8526, diffLoss = 4.1835, kgLoss = 0.0198
2025-04-08 13:32:57.419082: Training Step 292/354: batchLoss = 0.6089, diffLoss = 2.9893, kgLoss = 0.0138
2025-04-08 13:32:59.036755: Training Step 293/354: batchLoss = 0.5705, diffLoss = 2.7895, kgLoss = 0.0157
2025-04-08 13:33:00.649215: Training Step 294/354: batchLoss = 0.6627, diffLoss = 3.2554, kgLoss = 0.0145
2025-04-08 13:33:02.278868: Training Step 295/354: batchLoss = 0.5446, diffLoss = 2.6676, kgLoss = 0.0139
2025-04-08 13:33:03.897483: Training Step 296/354: batchLoss = 0.7942, diffLoss = 3.9006, kgLoss = 0.0175
2025-04-08 13:33:05.523772: Training Step 297/354: batchLoss = 0.5650, diffLoss = 2.7707, kgLoss = 0.0136
2025-04-08 13:33:07.135106: Training Step 298/354: batchLoss = 0.7048, diffLoss = 3.4584, kgLoss = 0.0164
2025-04-08 13:33:08.742084: Training Step 299/354: batchLoss = 0.7376, diffLoss = 3.6161, kgLoss = 0.0179
2025-04-08 13:33:10.351131: Training Step 300/354: batchLoss = 0.6118, diffLoss = 3.0004, kgLoss = 0.0147
2025-04-08 13:33:11.959062: Training Step 301/354: batchLoss = 0.5083, diffLoss = 2.4916, kgLoss = 0.0125
2025-04-08 13:33:13.575393: Training Step 302/354: batchLoss = 0.5822, diffLoss = 2.8595, kgLoss = 0.0129
2025-04-08 13:33:15.194396: Training Step 303/354: batchLoss = 0.6413, diffLoss = 3.1485, kgLoss = 0.0145
2025-04-08 13:33:16.815324: Training Step 304/354: batchLoss = 0.5723, diffLoss = 2.8078, kgLoss = 0.0134
2025-04-08 13:33:18.432804: Training Step 305/354: batchLoss = 0.8465, diffLoss = 4.1647, kgLoss = 0.0170
2025-04-08 13:33:20.048188: Training Step 306/354: batchLoss = 0.8228, diffLoss = 4.0441, kgLoss = 0.0175
2025-04-08 13:33:21.660655: Training Step 307/354: batchLoss = 0.6077, diffLoss = 2.9791, kgLoss = 0.0149
2025-04-08 13:33:23.274603: Training Step 308/354: batchLoss = 0.6330, diffLoss = 3.1034, kgLoss = 0.0154
2025-04-08 13:33:24.889066: Training Step 309/354: batchLoss = 0.5573, diffLoss = 2.7332, kgLoss = 0.0133
2025-04-08 13:33:26.510196: Training Step 310/354: batchLoss = 2.2410, diffLoss = 11.0212, kgLoss = 0.0459
2025-04-08 13:33:28.138197: Training Step 311/354: batchLoss = 0.5239, diffLoss = 2.5688, kgLoss = 0.0127
2025-04-08 13:33:29.772801: Training Step 312/354: batchLoss = 0.6627, diffLoss = 3.2557, kgLoss = 0.0145
2025-04-08 13:33:31.388901: Training Step 313/354: batchLoss = 0.5994, diffLoss = 2.9399, kgLoss = 0.0143
2025-04-08 13:33:33.005364: Training Step 314/354: batchLoss = 0.6674, diffLoss = 3.2592, kgLoss = 0.0194
2025-04-08 13:33:34.620235: Training Step 315/354: batchLoss = 0.8595, diffLoss = 4.2299, kgLoss = 0.0169
2025-04-08 13:33:36.246909: Training Step 316/354: batchLoss = 0.7868, diffLoss = 3.8669, kgLoss = 0.0168
2025-04-08 13:33:37.859506: Training Step 317/354: batchLoss = 0.5403, diffLoss = 2.6475, kgLoss = 0.0136
2025-04-08 13:33:39.462872: Training Step 318/354: batchLoss = 0.6108, diffLoss = 2.9954, kgLoss = 0.0146
2025-04-08 13:33:41.071555: Training Step 319/354: batchLoss = 0.7302, diffLoss = 3.5824, kgLoss = 0.0172
2025-04-08 13:33:42.700094: Training Step 320/354: batchLoss = 0.5227, diffLoss = 2.5595, kgLoss = 0.0135
2025-04-08 13:33:44.315375: Training Step 321/354: batchLoss = 0.5286, diffLoss = 2.5906, kgLoss = 0.0131
2025-04-08 13:33:45.933259: Training Step 322/354: batchLoss = 0.5780, diffLoss = 2.8382, kgLoss = 0.0129
2025-04-08 13:33:47.550515: Training Step 323/354: batchLoss = 0.6818, diffLoss = 3.3518, kgLoss = 0.0144
2025-04-08 13:33:49.176106: Training Step 324/354: batchLoss = 0.5012, diffLoss = 2.4520, kgLoss = 0.0135
2025-04-08 13:33:50.798719: Training Step 325/354: batchLoss = 0.5575, diffLoss = 2.7395, kgLoss = 0.0121
2025-04-08 13:33:52.411956: Training Step 326/354: batchLoss = 0.5806, diffLoss = 2.8499, kgLoss = 0.0133
2025-04-08 13:33:54.019109: Training Step 327/354: batchLoss = 0.5545, diffLoss = 2.7150, kgLoss = 0.0144
2025-04-08 13:33:55.627971: Training Step 328/354: batchLoss = 0.6058, diffLoss = 2.9719, kgLoss = 0.0142
2025-04-08 13:33:57.244770: Training Step 329/354: batchLoss = 0.7469, diffLoss = 3.6681, kgLoss = 0.0166
2025-04-08 13:33:58.856783: Training Step 330/354: batchLoss = 0.6286, diffLoss = 3.0836, kgLoss = 0.0149
2025-04-08 13:34:00.478236: Training Step 331/354: batchLoss = 0.5942, diffLoss = 2.9158, kgLoss = 0.0138
2025-04-08 13:34:02.097935: Training Step 332/354: batchLoss = 0.7137, diffLoss = 3.5048, kgLoss = 0.0159
2025-04-08 13:34:03.722998: Training Step 333/354: batchLoss = 0.5084, diffLoss = 2.4890, kgLoss = 0.0133
2025-04-08 13:34:05.349557: Training Step 334/354: batchLoss = 0.5643, diffLoss = 2.7649, kgLoss = 0.0142
2025-04-08 13:34:06.956267: Training Step 335/354: batchLoss = 0.6207, diffLoss = 3.0541, kgLoss = 0.0124
2025-04-08 13:34:08.570673: Training Step 336/354: batchLoss = 0.5785, diffLoss = 2.8374, kgLoss = 0.0138
2025-04-08 13:34:10.178840: Training Step 337/354: batchLoss = 0.6244, diffLoss = 3.0631, kgLoss = 0.0147
2025-04-08 13:34:11.794703: Training Step 338/354: batchLoss = 0.7495, diffLoss = 3.6764, kgLoss = 0.0177
2025-04-08 13:34:13.409477: Training Step 339/354: batchLoss = 0.7517, diffLoss = 3.6992, kgLoss = 0.0148
2025-04-08 13:34:15.031491: Training Step 340/354: batchLoss = 0.6342, diffLoss = 3.1085, kgLoss = 0.0156
2025-04-08 13:34:16.652021: Training Step 341/354: batchLoss = 0.5768, diffLoss = 2.8287, kgLoss = 0.0139
2025-04-08 13:34:18.275161: Training Step 342/354: batchLoss = 0.5501, diffLoss = 2.6939, kgLoss = 0.0142
2025-04-08 13:34:19.890033: Training Step 343/354: batchLoss = 0.6793, diffLoss = 3.3278, kgLoss = 0.0172
2025-04-08 13:34:21.507232: Training Step 344/354: batchLoss = 0.5918, diffLoss = 2.9045, kgLoss = 0.0136
2025-04-08 13:34:23.114756: Training Step 345/354: batchLoss = 0.4805, diffLoss = 2.3522, kgLoss = 0.0126
2025-04-08 13:34:24.724638: Training Step 346/354: batchLoss = 0.5853, diffLoss = 2.8769, kgLoss = 0.0124
2025-04-08 13:34:26.347140: Training Step 347/354: batchLoss = 0.6355, diffLoss = 3.1160, kgLoss = 0.0154
2025-04-08 13:34:27.972184: Training Step 348/354: batchLoss = 0.7008, diffLoss = 3.4388, kgLoss = 0.0163
2025-04-08 13:34:29.591887: Training Step 349/354: batchLoss = 0.5658, diffLoss = 2.7785, kgLoss = 0.0126
2025-04-08 13:34:31.219874: Training Step 350/354: batchLoss = 0.6020, diffLoss = 2.9536, kgLoss = 0.0141
2025-04-08 13:34:32.842896: Training Step 351/354: batchLoss = 0.5769, diffLoss = 2.8309, kgLoss = 0.0134
2025-04-08 13:34:34.457017: Training Step 352/354: batchLoss = 0.6939, diffLoss = 3.4094, kgLoss = 0.0151
2025-04-08 13:34:35.851910: Training Step 353/354: batchLoss = 1.3928, diffLoss = 6.8343, kgLoss = 0.0325
2025-04-08 13:34:35.937776: 
2025-04-08 13:34:35.938394: Epoch 4/1000, Train: epLoss = 1.1480, epDfLoss = 5.6352, epfTransLoss = 0.0000, epKgLoss = 0.0262  
2025-04-08 13:34:37.247776: Steps 0/138: batch_recall = 42.17, batch_ndcg = 24.29 
2025-04-08 13:34:38.550840: Steps 1/138: batch_recall = 42.63, batch_ndcg = 24.99 
2025-04-08 13:34:39.860019: Steps 2/138: batch_recall = 50.81, batch_ndcg = 31.86 
2025-04-08 13:34:41.187644: Steps 3/138: batch_recall = 51.83, batch_ndcg = 29.21 
2025-04-08 13:34:42.487763: Steps 4/138: batch_recall = 59.93, batch_ndcg = 37.95 
2025-04-08 13:34:43.800567: Steps 5/138: batch_recall = 51.16, batch_ndcg = 27.22 
2025-04-08 13:34:45.117269: Steps 6/138: batch_recall = 47.01, batch_ndcg = 27.67 
2025-04-08 13:34:46.417392: Steps 7/138: batch_recall = 57.82, batch_ndcg = 36.10 
2025-04-08 13:34:47.717209: Steps 8/138: batch_recall = 57.14, batch_ndcg = 35.66 
2025-04-08 13:34:49.016555: Steps 9/138: batch_recall = 49.63, batch_ndcg = 29.62 
2025-04-08 13:34:50.312560: Steps 10/138: batch_recall = 45.50, batch_ndcg = 26.23 
2025-04-08 13:34:51.604820: Steps 11/138: batch_recall = 60.26, batch_ndcg = 33.47 
2025-04-08 13:34:52.892347: Steps 12/138: batch_recall = 49.60, batch_ndcg = 28.37 
2025-04-08 13:34:54.168679: Steps 13/138: batch_recall = 44.07, batch_ndcg = 26.24 
2025-04-08 13:34:55.464322: Steps 14/138: batch_recall = 44.41, batch_ndcg = 26.21 
2025-04-08 13:34:56.781686: Steps 15/138: batch_recall = 49.00, batch_ndcg = 30.28 
2025-04-08 13:34:58.079579: Steps 16/138: batch_recall = 51.72, batch_ndcg = 28.71 
2025-04-08 13:34:59.364482: Steps 17/138: batch_recall = 52.54, batch_ndcg = 29.01 
2025-04-08 13:35:00.675563: Steps 18/138: batch_recall = 45.20, batch_ndcg = 28.99 
2025-04-08 13:35:01.990597: Steps 19/138: batch_recall = 50.22, batch_ndcg = 29.60 
2025-04-08 13:35:03.295324: Steps 20/138: batch_recall = 59.13, batch_ndcg = 32.47 
2025-04-08 13:35:04.591984: Steps 21/138: batch_recall = 59.79, batch_ndcg = 37.04 
2025-04-08 13:35:05.890021: Steps 22/138: batch_recall = 53.60, batch_ndcg = 31.39 
2025-04-08 13:35:07.176524: Steps 23/138: batch_recall = 47.45, batch_ndcg = 27.11 
2025-04-08 13:35:08.474999: Steps 24/138: batch_recall = 43.93, batch_ndcg = 26.14 
2025-04-08 13:35:09.773984: Steps 25/138: batch_recall = 50.40, batch_ndcg = 30.43 
2025-04-08 13:35:11.082034: Steps 26/138: batch_recall = 56.43, batch_ndcg = 30.97 
2025-04-08 13:35:12.384315: Steps 27/138: batch_recall = 58.06, batch_ndcg = 33.91 
2025-04-08 13:35:13.660937: Steps 28/138: batch_recall = 52.57, batch_ndcg = 30.06 
2025-04-08 13:35:14.958231: Steps 29/138: batch_recall = 55.53, batch_ndcg = 29.40 
2025-04-08 13:35:16.252920: Steps 30/138: batch_recall = 50.64, batch_ndcg = 32.47 
2025-04-08 13:35:17.541743: Steps 31/138: batch_recall = 36.05, batch_ndcg = 21.84 
2025-04-08 13:35:18.854327: Steps 32/138: batch_recall = 48.03, batch_ndcg = 28.20 
2025-04-08 13:35:20.129801: Steps 33/138: batch_recall = 55.16, batch_ndcg = 30.85 
2025-04-08 13:35:21.399925: Steps 34/138: batch_recall = 48.17, batch_ndcg = 25.47 
2025-04-08 13:35:22.686257: Steps 35/138: batch_recall = 46.46, batch_ndcg = 27.49 
2025-04-08 13:35:23.959427: Steps 36/138: batch_recall = 49.55, batch_ndcg = 27.57 
2025-04-08 13:35:25.277024: Steps 37/138: batch_recall = 63.29, batch_ndcg = 32.88 
2025-04-08 13:35:26.568903: Steps 38/138: batch_recall = 53.44, batch_ndcg = 31.71 
2025-04-08 13:35:27.865322: Steps 39/138: batch_recall = 69.96, batch_ndcg = 36.79 
2025-04-08 13:35:29.148856: Steps 40/138: batch_recall = 46.94, batch_ndcg = 25.40 
2025-04-08 13:35:30.445414: Steps 41/138: batch_recall = 55.56, batch_ndcg = 31.41 
2025-04-08 13:35:31.742651: Steps 42/138: batch_recall = 53.93, batch_ndcg = 30.04 
2025-04-08 13:35:33.035800: Steps 43/138: batch_recall = 52.46, batch_ndcg = 33.31 
2025-04-08 13:35:34.340399: Steps 44/138: batch_recall = 48.85, batch_ndcg = 27.31 
2025-04-08 13:35:35.604030: Steps 45/138: batch_recall = 54.79, batch_ndcg = 30.66 
2025-04-08 13:35:36.868559: Steps 46/138: batch_recall = 55.66, batch_ndcg = 32.77 
2025-04-08 13:35:38.136865: Steps 47/138: batch_recall = 55.26, batch_ndcg = 29.89 
2025-04-08 13:35:39.441543: Steps 48/138: batch_recall = 51.56, batch_ndcg = 29.86 
2025-04-08 13:35:40.728612: Steps 49/138: batch_recall = 57.87, batch_ndcg = 35.77 
2025-04-08 13:35:42.022310: Steps 50/138: batch_recall = 58.60, batch_ndcg = 31.20 
2025-04-08 13:35:43.299400: Steps 51/138: batch_recall = 57.64, batch_ndcg = 32.61 
2025-04-08 13:35:44.576206: Steps 52/138: batch_recall = 64.49, batch_ndcg = 39.41 
2025-04-08 13:35:45.865487: Steps 53/138: batch_recall = 56.60, batch_ndcg = 29.50 
2025-04-08 13:35:47.161114: Steps 54/138: batch_recall = 56.65, batch_ndcg = 31.28 
2025-04-08 13:35:48.448370: Steps 55/138: batch_recall = 51.64, batch_ndcg = 29.29 
2025-04-08 13:35:49.720899: Steps 56/138: batch_recall = 59.01, batch_ndcg = 33.24 
2025-04-08 13:35:50.996806: Steps 57/138: batch_recall = 57.29, batch_ndcg = 30.43 
2025-04-08 13:35:52.280995: Steps 58/138: batch_recall = 61.86, batch_ndcg = 32.99 
2025-04-08 13:35:53.564430: Steps 59/138: batch_recall = 68.80, batch_ndcg = 37.50 
2025-04-08 13:35:54.865947: Steps 60/138: batch_recall = 69.75, batch_ndcg = 36.72 
2025-04-08 13:35:56.150196: Steps 61/138: batch_recall = 58.29, batch_ndcg = 31.42 
2025-04-08 13:35:57.436314: Steps 62/138: batch_recall = 72.38, batch_ndcg = 40.32 
2025-04-08 13:35:58.716833: Steps 63/138: batch_recall = 73.01, batch_ndcg = 41.39 
2025-04-08 13:36:00.024660: Steps 64/138: batch_recall = 58.91, batch_ndcg = 31.15 
2025-04-08 13:36:01.313894: Steps 65/138: batch_recall = 80.40, batch_ndcg = 44.63 
2025-04-08 13:36:02.595670: Steps 66/138: batch_recall = 62.48, batch_ndcg = 37.30 
2025-04-08 13:36:03.864313: Steps 67/138: batch_recall = 80.79, batch_ndcg = 47.64 
2025-04-08 13:36:05.128627: Steps 68/138: batch_recall = 61.88, batch_ndcg = 32.59 
2025-04-08 13:36:06.405149: Steps 69/138: batch_recall = 84.95, batch_ndcg = 47.81 
2025-04-08 13:36:07.684577: Steps 70/138: batch_recall = 74.18, batch_ndcg = 41.71 
2025-04-08 13:36:08.973254: Steps 71/138: batch_recall = 79.84, batch_ndcg = 46.30 
2025-04-08 13:36:10.245618: Steps 72/138: batch_recall = 81.49, batch_ndcg = 48.46 
2025-04-08 13:36:11.517510: Steps 73/138: batch_recall = 86.13, batch_ndcg = 44.40 
2025-04-08 13:36:12.803521: Steps 74/138: batch_recall = 77.17, batch_ndcg = 46.28 
2025-04-08 13:36:14.090072: Steps 75/138: batch_recall = 78.06, batch_ndcg = 45.60 
2025-04-08 13:36:15.381829: Steps 76/138: batch_recall = 91.71, batch_ndcg = 53.13 
2025-04-08 13:36:16.663157: Steps 77/138: batch_recall = 79.83, batch_ndcg = 43.90 
2025-04-08 13:36:17.943906: Steps 78/138: batch_recall = 94.37, batch_ndcg = 48.79 
2025-04-08 13:36:19.223835: Steps 79/138: batch_recall = 82.18, batch_ndcg = 45.59 
2025-04-08 13:36:20.493902: Steps 80/138: batch_recall = 75.67, batch_ndcg = 39.95 
2025-04-08 13:36:21.759683: Steps 81/138: batch_recall = 82.27, batch_ndcg = 45.67 
2025-04-08 13:36:23.025782: Steps 82/138: batch_recall = 84.01, batch_ndcg = 46.84 
2025-04-08 13:36:24.316347: Steps 83/138: batch_recall = 78.89, batch_ndcg = 44.17 
2025-04-08 13:36:25.605717: Steps 84/138: batch_recall = 102.49, batch_ndcg = 55.45 
2025-04-08 13:36:26.887105: Steps 85/138: batch_recall = 92.99, batch_ndcg = 50.99 
2025-04-08 13:36:28.165888: Steps 86/138: batch_recall = 116.86, batch_ndcg = 66.80 
2025-04-08 13:36:29.442674: Steps 87/138: batch_recall = 99.73, batch_ndcg = 55.28 
2025-04-08 13:36:30.715399: Steps 88/138: batch_recall = 103.07, batch_ndcg = 57.15 
2025-04-08 13:36:31.993921: Steps 89/138: batch_recall = 113.16, batch_ndcg = 62.62 
2025-04-08 13:36:33.253092: Steps 90/138: batch_recall = 104.19, batch_ndcg = 56.32 
2025-04-08 13:36:34.514387: Steps 91/138: batch_recall = 116.47, batch_ndcg = 63.97 
2025-04-08 13:36:35.775848: Steps 92/138: batch_recall = 104.15, batch_ndcg = 57.10 
2025-04-08 13:36:37.033317: Steps 93/138: batch_recall = 109.39, batch_ndcg = 64.25 
2025-04-08 13:36:38.298428: Steps 94/138: batch_recall = 111.68, batch_ndcg = 58.94 
2025-04-08 13:36:39.565348: Steps 95/138: batch_recall = 113.13, batch_ndcg = 64.34 
2025-04-08 13:36:40.843428: Steps 96/138: batch_recall = 127.30, batch_ndcg = 70.94 
2025-04-08 13:36:42.109970: Steps 97/138: batch_recall = 127.57, batch_ndcg = 70.24 
2025-04-08 13:36:43.382709: Steps 98/138: batch_recall = 103.67, batch_ndcg = 61.20 
2025-04-08 13:36:44.647131: Steps 99/138: batch_recall = 115.27, batch_ndcg = 63.79 
2025-04-08 13:36:45.928267: Steps 100/138: batch_recall = 113.91, batch_ndcg = 66.09 
2025-04-08 13:36:47.192835: Steps 101/138: batch_recall = 124.11, batch_ndcg = 69.75 
2025-04-08 13:36:48.448963: Steps 102/138: batch_recall = 121.02, batch_ndcg = 67.35 
2025-04-08 13:36:49.707605: Steps 103/138: batch_recall = 128.05, batch_ndcg = 70.56 
2025-04-08 13:36:50.972972: Steps 104/138: batch_recall = 130.78, batch_ndcg = 69.08 
2025-04-08 13:36:52.240773: Steps 105/138: batch_recall = 118.72, batch_ndcg = 66.44 
2025-04-08 13:36:53.516222: Steps 106/138: batch_recall = 108.32, batch_ndcg = 56.42 
2025-04-08 13:36:54.798202: Steps 107/138: batch_recall = 111.29, batch_ndcg = 59.13 
2025-04-08 13:36:56.068629: Steps 108/138: batch_recall = 118.81, batch_ndcg = 68.33 
2025-04-08 13:36:57.334022: Steps 109/138: batch_recall = 129.71, batch_ndcg = 69.82 
2025-04-08 13:36:58.599846: Steps 110/138: batch_recall = 116.34, batch_ndcg = 61.31 
2025-04-08 13:36:59.875725: Steps 111/138: batch_recall = 131.26, batch_ndcg = 77.25 
2025-04-08 13:37:01.146600: Steps 112/138: batch_recall = 149.05, batch_ndcg = 78.03 
2025-04-08 13:37:02.408090: Steps 113/138: batch_recall = 119.93, batch_ndcg = 66.80 
2025-04-08 13:37:03.668409: Steps 114/138: batch_recall = 113.92, batch_ndcg = 63.20 
2025-04-08 13:37:04.940945: Steps 115/138: batch_recall = 107.34, batch_ndcg = 54.59 
2025-04-08 13:37:06.247412: Steps 116/138: batch_recall = 122.06, batch_ndcg = 61.27 
2025-04-08 13:37:07.562914: Steps 117/138: batch_recall = 112.17, batch_ndcg = 62.93 
2025-04-08 13:37:08.875320: Steps 118/138: batch_recall = 122.49, batch_ndcg = 68.13 
2025-04-08 13:37:10.177042: Steps 119/138: batch_recall = 131.41, batch_ndcg = 71.00 
2025-04-08 13:37:11.504224: Steps 120/138: batch_recall = 117.05, batch_ndcg = 63.51 
2025-04-08 13:37:12.783230: Steps 121/138: batch_recall = 140.97, batch_ndcg = 71.41 
2025-04-08 13:37:14.084290: Steps 122/138: batch_recall = 130.87, batch_ndcg = 71.32 
2025-04-08 13:37:15.382278: Steps 123/138: batch_recall = 121.94, batch_ndcg = 66.21 
2025-04-08 13:37:16.679325: Steps 124/138: batch_recall = 140.10, batch_ndcg = 88.20 
2025-04-08 13:37:17.987607: Steps 125/138: batch_recall = 122.33, batch_ndcg = 64.79 
2025-04-08 13:37:19.310957: Steps 126/138: batch_recall = 155.60, batch_ndcg = 86.75 
2025-04-08 13:37:20.599418: Steps 127/138: batch_recall = 134.72, batch_ndcg = 77.37 
2025-04-08 13:37:21.870824: Steps 128/138: batch_recall = 123.28, batch_ndcg = 63.76 
2025-04-08 13:37:23.139099: Steps 129/138: batch_recall = 143.54, batch_ndcg = 84.39 
2025-04-08 13:37:24.426389: Steps 130/138: batch_recall = 124.86, batch_ndcg = 65.78 
2025-04-08 13:37:25.694789: Steps 131/138: batch_recall = 129.83, batch_ndcg = 76.56 
2025-04-08 13:37:26.966901: Steps 132/138: batch_recall = 143.83, batch_ndcg = 80.04 
2025-04-08 13:37:28.232399: Steps 133/138: batch_recall = 141.12, batch_ndcg = 80.36 
2025-04-08 13:37:29.508439: Steps 134/138: batch_recall = 129.63, batch_ndcg = 72.41 
2025-04-08 13:37:30.786241: Steps 135/138: batch_recall = 159.06, batch_ndcg = 88.31 
2025-04-08 13:37:32.051855: Steps 136/138: batch_recall = 146.12, batch_ndcg = 76.78 
2025-04-08 13:37:33.304786: Steps 137/138: batch_recall = 130.25, batch_ndcg = 80.66 
2025-04-08 13:37:33.305267: Epoch 4/1000, Test: Recall = 0.1652, NDCG = 0.0925  

2025-04-08 13:37:35.067837: Training Step 0/354: batchLoss = 0.5612, diffLoss = 2.7577, kgLoss = 0.0120
2025-04-08 13:37:36.688615: Training Step 1/354: batchLoss = 0.6782, diffLoss = 3.3204, kgLoss = 0.0176
2025-04-08 13:37:38.313895: Training Step 2/354: batchLoss = 0.6611, diffLoss = 3.2412, kgLoss = 0.0161
2025-04-08 13:37:39.930946: Training Step 3/354: batchLoss = 0.5670, diffLoss = 2.7803, kgLoss = 0.0137
2025-04-08 13:37:41.549707: Training Step 4/354: batchLoss = 0.4509, diffLoss = 2.2071, kgLoss = 0.0118
2025-04-08 13:37:43.163330: Training Step 5/354: batchLoss = 0.6346, diffLoss = 3.1193, kgLoss = 0.0134
2025-04-08 13:37:44.786809: Training Step 6/354: batchLoss = 0.6730, diffLoss = 3.3064, kgLoss = 0.0146
2025-04-08 13:37:46.411298: Training Step 7/354: batchLoss = 0.7343, diffLoss = 3.6052, kgLoss = 0.0165
2025-04-08 13:37:48.021596: Training Step 8/354: batchLoss = 0.6620, diffLoss = 3.2553, kgLoss = 0.0137
2025-04-08 13:37:49.633104: Training Step 9/354: batchLoss = 0.5752, diffLoss = 2.8259, kgLoss = 0.0125
2025-04-08 13:37:51.242783: Training Step 10/354: batchLoss = 0.6690, diffLoss = 3.2767, kgLoss = 0.0170
2025-04-08 13:37:52.878393: Training Step 11/354: batchLoss = 0.6181, diffLoss = 3.0295, kgLoss = 0.0153
2025-04-08 13:37:54.494999: Training Step 12/354: batchLoss = 0.5952, diffLoss = 2.9214, kgLoss = 0.0136
2025-04-08 13:37:56.111452: Training Step 13/354: batchLoss = 0.4814, diffLoss = 2.3641, kgLoss = 0.0107
2025-04-08 13:37:57.727043: Training Step 14/354: batchLoss = 0.7179, diffLoss = 3.5253, kgLoss = 0.0160
2025-04-08 13:37:59.347497: Training Step 15/354: batchLoss = 0.7312, diffLoss = 3.5917, kgLoss = 0.0160
2025-04-08 13:38:00.969257: Training Step 16/354: batchLoss = 0.6367, diffLoss = 3.1229, kgLoss = 0.0151
2025-04-08 13:38:02.576866: Training Step 17/354: batchLoss = 0.7154, diffLoss = 3.5144, kgLoss = 0.0156
2025-04-08 13:38:04.190645: Training Step 18/354: batchLoss = 0.6820, diffLoss = 3.3449, kgLoss = 0.0163
2025-04-08 13:38:05.808677: Training Step 19/354: batchLoss = 0.6287, diffLoss = 3.0899, kgLoss = 0.0134
2025-04-08 13:38:07.448397: Training Step 20/354: batchLoss = 0.6660, diffLoss = 3.2613, kgLoss = 0.0172
2025-04-08 13:38:09.080260: Training Step 21/354: batchLoss = 0.6755, diffLoss = 3.3068, kgLoss = 0.0176
2025-04-08 13:38:10.713320: Training Step 22/354: batchLoss = 0.7818, diffLoss = 3.8402, kgLoss = 0.0172
2025-04-08 13:38:12.336652: Training Step 23/354: batchLoss = 0.5933, diffLoss = 2.9114, kgLoss = 0.0138
2025-04-08 13:38:13.964018: Training Step 24/354: batchLoss = 0.5837, diffLoss = 2.8607, kgLoss = 0.0144
2025-04-08 13:38:15.582260: Training Step 25/354: batchLoss = 0.5504, diffLoss = 2.7018, kgLoss = 0.0126
2025-04-08 13:38:17.190678: Training Step 26/354: batchLoss = 0.8132, diffLoss = 3.9966, kgLoss = 0.0173
2025-04-08 13:38:18.799843: Training Step 27/354: batchLoss = 0.5565, diffLoss = 2.7269, kgLoss = 0.0139
2025-04-08 13:38:20.411071: Training Step 28/354: batchLoss = 0.5704, diffLoss = 2.7916, kgLoss = 0.0151
2025-04-08 13:38:22.036330: Training Step 29/354: batchLoss = 0.7245, diffLoss = 3.5581, kgLoss = 0.0161
2025-04-08 13:38:23.652346: Training Step 30/354: batchLoss = 0.5691, diffLoss = 2.7843, kgLoss = 0.0153
2025-04-08 13:38:25.269327: Training Step 31/354: batchLoss = 0.5558, diffLoss = 2.7286, kgLoss = 0.0126
2025-04-08 13:38:26.897366: Training Step 32/354: batchLoss = 0.5661, diffLoss = 2.7806, kgLoss = 0.0125
2025-04-08 13:38:28.527946: Training Step 33/354: batchLoss = 0.7220, diffLoss = 3.5459, kgLoss = 0.0160
2025-04-08 13:38:30.144347: Training Step 34/354: batchLoss = 0.5159, diffLoss = 2.5282, kgLoss = 0.0128
2025-04-08 13:38:31.758359: Training Step 35/354: batchLoss = 0.4821, diffLoss = 2.3581, kgLoss = 0.0131
2025-04-08 13:38:33.368878: Training Step 36/354: batchLoss = 0.5570, diffLoss = 2.7339, kgLoss = 0.0128
2025-04-08 13:38:34.981106: Training Step 37/354: batchLoss = 0.6308, diffLoss = 3.0862, kgLoss = 0.0170
2025-04-08 13:38:36.608348: Training Step 38/354: batchLoss = 0.5512, diffLoss = 2.6997, kgLoss = 0.0141
2025-04-08 13:38:38.244988: Training Step 39/354: batchLoss = 0.5966, diffLoss = 2.9244, kgLoss = 0.0147
2025-04-08 13:38:39.866465: Training Step 40/354: batchLoss = 0.6168, diffLoss = 3.0234, kgLoss = 0.0152
2025-04-08 13:38:41.494301: Training Step 41/354: batchLoss = 0.6140, diffLoss = 3.0113, kgLoss = 0.0147
2025-04-08 13:38:43.117766: Training Step 42/354: batchLoss = 0.6705, diffLoss = 3.2930, kgLoss = 0.0148
2025-04-08 13:38:44.738654: Training Step 43/354: batchLoss = 0.6292, diffLoss = 3.0893, kgLoss = 0.0142
2025-04-08 13:38:46.356242: Training Step 44/354: batchLoss = 0.6108, diffLoss = 3.0033, kgLoss = 0.0127
2025-04-08 13:38:47.963091: Training Step 45/354: batchLoss = 0.5192, diffLoss = 2.5535, kgLoss = 0.0107
2025-04-08 13:38:49.579771: Training Step 46/354: batchLoss = 0.6624, diffLoss = 3.2416, kgLoss = 0.0176
2025-04-08 13:38:51.211204: Training Step 47/354: batchLoss = 0.7719, diffLoss = 3.7894, kgLoss = 0.0175
2025-04-08 13:38:52.828554: Training Step 48/354: batchLoss = 0.7189, diffLoss = 3.5295, kgLoss = 0.0162
2025-04-08 13:38:54.466563: Training Step 49/354: batchLoss = 0.6387, diffLoss = 3.1289, kgLoss = 0.0161
2025-04-08 13:38:56.082080: Training Step 50/354: batchLoss = 0.5872, diffLoss = 2.8824, kgLoss = 0.0134
2025-04-08 13:38:57.702644: Training Step 51/354: batchLoss = 0.6519, diffLoss = 3.2038, kgLoss = 0.0139
2025-04-08 13:38:59.338315: Training Step 52/354: batchLoss = 0.5960, diffLoss = 2.9288, kgLoss = 0.0129
2025-04-08 13:39:00.949211: Training Step 53/354: batchLoss = 0.5581, diffLoss = 2.7399, kgLoss = 0.0126
2025-04-08 13:39:02.558733: Training Step 54/354: batchLoss = 0.5571, diffLoss = 2.7241, kgLoss = 0.0154
2025-04-08 13:39:04.181587: Training Step 55/354: batchLoss = 0.6738, diffLoss = 3.3077, kgLoss = 0.0153
2025-04-08 13:39:05.804472: Training Step 56/354: batchLoss = 0.5484, diffLoss = 2.6930, kgLoss = 0.0123
2025-04-08 13:39:07.428784: Training Step 57/354: batchLoss = 0.6251, diffLoss = 3.0677, kgLoss = 0.0145
2025-04-08 13:39:09.056934: Training Step 58/354: batchLoss = 0.7730, diffLoss = 3.8009, kgLoss = 0.0160
2025-04-08 13:39:10.682601: Training Step 59/354: batchLoss = 0.9198, diffLoss = 4.5184, kgLoss = 0.0201
2025-04-08 13:39:12.313193: Training Step 60/354: batchLoss = 0.4923, diffLoss = 2.4052, kgLoss = 0.0141
2025-04-08 13:39:13.948050: Training Step 61/354: batchLoss = 0.8144, diffLoss = 3.9984, kgLoss = 0.0184
2025-04-08 13:39:15.570644: Training Step 62/354: batchLoss = 0.6030, diffLoss = 2.9622, kgLoss = 0.0132
2025-04-08 13:39:17.179930: Training Step 63/354: batchLoss = 0.5152, diffLoss = 2.5181, kgLoss = 0.0145
2025-04-08 13:39:18.801411: Training Step 64/354: batchLoss = 0.5503, diffLoss = 2.6979, kgLoss = 0.0134
2025-04-08 13:39:20.411805: Training Step 65/354: batchLoss = 0.6158, diffLoss = 3.0222, kgLoss = 0.0142
2025-04-08 13:39:22.036453: Training Step 66/354: batchLoss = 0.6279, diffLoss = 3.0836, kgLoss = 0.0139
2025-04-08 13:39:23.657433: Training Step 67/354: batchLoss = 0.5643, diffLoss = 2.7695, kgLoss = 0.0130
2025-04-08 13:39:25.274821: Training Step 68/354: batchLoss = 0.5406, diffLoss = 2.6507, kgLoss = 0.0130
2025-04-08 13:39:26.895868: Training Step 69/354: batchLoss = 0.6417, diffLoss = 3.1503, kgLoss = 0.0146
2025-04-08 13:39:28.517687: Training Step 70/354: batchLoss = 0.6721, diffLoss = 3.2940, kgLoss = 0.0166
2025-04-08 13:39:30.130850: Training Step 71/354: batchLoss = 0.5516, diffLoss = 2.7058, kgLoss = 0.0131
2025-04-08 13:39:31.742729: Training Step 72/354: batchLoss = 0.5190, diffLoss = 2.5429, kgLoss = 0.0130
2025-04-08 13:39:33.349102: Training Step 73/354: batchLoss = 0.5148, diffLoss = 2.5321, kgLoss = 0.0105
2025-04-08 13:39:34.970156: Training Step 74/354: batchLoss = 0.6029, diffLoss = 2.9620, kgLoss = 0.0131
2025-04-08 13:39:36.604878: Training Step 75/354: batchLoss = 0.7648, diffLoss = 3.7523, kgLoss = 0.0179
2025-04-08 13:39:38.232587: Training Step 76/354: batchLoss = 0.5872, diffLoss = 2.8800, kgLoss = 0.0140
2025-04-08 13:39:39.854082: Training Step 77/354: batchLoss = 0.5575, diffLoss = 2.7301, kgLoss = 0.0144
2025-04-08 13:39:41.474005: Training Step 78/354: batchLoss = 0.6161, diffLoss = 3.0210, kgLoss = 0.0149
2025-04-08 13:39:43.094536: Training Step 79/354: batchLoss = 0.6141, diffLoss = 3.0126, kgLoss = 0.0145
2025-04-08 13:39:44.706146: Training Step 80/354: batchLoss = 0.5044, diffLoss = 2.4750, kgLoss = 0.0117
2025-04-08 13:39:46.314968: Training Step 81/354: batchLoss = 0.5378, diffLoss = 2.6407, kgLoss = 0.0121
2025-04-08 13:39:47.927509: Training Step 82/354: batchLoss = 0.6431, diffLoss = 3.1529, kgLoss = 0.0156
2025-04-08 13:39:49.556752: Training Step 83/354: batchLoss = 0.6232, diffLoss = 3.0546, kgLoss = 0.0153
2025-04-08 13:39:51.178509: Training Step 84/354: batchLoss = 0.5590, diffLoss = 2.7470, kgLoss = 0.0120
2025-04-08 13:39:52.799661: Training Step 85/354: batchLoss = 0.6921, diffLoss = 3.3977, kgLoss = 0.0157
2025-04-08 13:39:54.431348: Training Step 86/354: batchLoss = 0.5994, diffLoss = 2.9415, kgLoss = 0.0138
2025-04-08 13:39:56.058063: Training Step 87/354: batchLoss = 0.7267, diffLoss = 3.5722, kgLoss = 0.0153
2025-04-08 13:39:57.680306: Training Step 88/354: batchLoss = 0.8659, diffLoss = 4.2433, kgLoss = 0.0215
2025-04-08 13:39:59.299846: Training Step 89/354: batchLoss = 0.6961, diffLoss = 3.4172, kgLoss = 0.0158
2025-04-08 13:40:00.916727: Training Step 90/354: batchLoss = 0.5673, diffLoss = 2.7831, kgLoss = 0.0134
2025-04-08 13:40:02.527628: Training Step 91/354: batchLoss = 0.5030, diffLoss = 2.4731, kgLoss = 0.0105
2025-04-08 13:40:04.145975: Training Step 92/354: batchLoss = 0.7389, diffLoss = 3.6238, kgLoss = 0.0177
2025-04-08 13:40:05.776547: Training Step 93/354: batchLoss = 0.8095, diffLoss = 3.9754, kgLoss = 0.0180
2025-04-08 13:40:07.405004: Training Step 94/354: batchLoss = 0.8020, diffLoss = 3.9375, kgLoss = 0.0181
2025-04-08 13:40:09.023073: Training Step 95/354: batchLoss = 0.6484, diffLoss = 3.1774, kgLoss = 0.0161
2025-04-08 13:40:10.660484: Training Step 96/354: batchLoss = 0.5816, diffLoss = 2.8625, kgLoss = 0.0114
2025-04-08 13:40:12.273491: Training Step 97/354: batchLoss = 0.6993, diffLoss = 3.4317, kgLoss = 0.0162
2025-04-08 13:40:13.903673: Training Step 98/354: batchLoss = 0.6540, diffLoss = 3.2055, kgLoss = 0.0161
2025-04-08 13:40:15.519159: Training Step 99/354: batchLoss = 0.7744, diffLoss = 3.8052, kgLoss = 0.0167
2025-04-08 13:40:17.143281: Training Step 100/354: batchLoss = 0.6909, diffLoss = 3.3890, kgLoss = 0.0163
2025-04-08 13:40:18.764778: Training Step 101/354: batchLoss = 0.6949, diffLoss = 3.4098, kgLoss = 0.0162
2025-04-08 13:40:20.395708: Training Step 102/354: batchLoss = 0.6205, diffLoss = 3.0481, kgLoss = 0.0136
2025-04-08 13:40:22.026341: Training Step 103/354: batchLoss = 0.5697, diffLoss = 2.7975, kgLoss = 0.0127
2025-04-08 13:40:23.648737: Training Step 104/354: batchLoss = 0.6017, diffLoss = 2.9498, kgLoss = 0.0147
2025-04-08 13:40:25.271723: Training Step 105/354: batchLoss = 0.6005, diffLoss = 2.9409, kgLoss = 0.0154
2025-04-08 13:40:26.889018: Training Step 106/354: batchLoss = 0.5491, diffLoss = 2.6912, kgLoss = 0.0135
2025-04-08 13:40:28.507548: Training Step 107/354: batchLoss = 0.7263, diffLoss = 3.5757, kgLoss = 0.0139
2025-04-08 13:40:30.123780: Training Step 108/354: batchLoss = 0.6868, diffLoss = 3.3687, kgLoss = 0.0163
2025-04-08 13:40:31.732251: Training Step 109/354: batchLoss = 0.5764, diffLoss = 2.8243, kgLoss = 0.0144
2025-04-08 13:40:33.357444: Training Step 110/354: batchLoss = 0.5068, diffLoss = 2.4845, kgLoss = 0.0124
2025-04-08 13:40:34.984455: Training Step 111/354: batchLoss = 0.4792, diffLoss = 2.3481, kgLoss = 0.0120
2025-04-08 13:40:36.603112: Training Step 112/354: batchLoss = 0.7000, diffLoss = 3.4418, kgLoss = 0.0146
2025-04-08 13:40:38.225933: Training Step 113/354: batchLoss = 0.6203, diffLoss = 3.0428, kgLoss = 0.0146
2025-04-08 13:40:39.847062: Training Step 114/354: batchLoss = 0.6725, diffLoss = 3.2961, kgLoss = 0.0166
2025-04-08 13:40:41.480621: Training Step 115/354: batchLoss = 0.5611, diffLoss = 2.7503, kgLoss = 0.0138
2025-04-08 13:40:43.102535: Training Step 116/354: batchLoss = 0.5936, diffLoss = 2.9100, kgLoss = 0.0145
2025-04-08 13:40:44.711925: Training Step 117/354: batchLoss = 0.5450, diffLoss = 2.6738, kgLoss = 0.0128
2025-04-08 13:40:46.330818: Training Step 118/354: batchLoss = 0.7324, diffLoss = 3.6006, kgLoss = 0.0154
2025-04-08 13:40:47.945226: Training Step 119/354: batchLoss = 0.6974, diffLoss = 3.4235, kgLoss = 0.0159
2025-04-08 13:40:49.582411: Training Step 120/354: batchLoss = 1.0688, diffLoss = 5.2590, kgLoss = 0.0213
2025-04-08 13:40:51.205524: Training Step 121/354: batchLoss = 0.5402, diffLoss = 2.6507, kgLoss = 0.0125
2025-04-08 13:40:52.825103: Training Step 122/354: batchLoss = 1.2909, diffLoss = 6.3496, kgLoss = 0.0262
2025-04-08 13:40:54.442365: Training Step 123/354: batchLoss = 0.5124, diffLoss = 2.5134, kgLoss = 0.0122
2025-04-08 13:40:56.071141: Training Step 124/354: batchLoss = 0.6992, diffLoss = 3.4289, kgLoss = 0.0167
2025-04-08 13:40:57.696605: Training Step 125/354: batchLoss = 0.7632, diffLoss = 3.7472, kgLoss = 0.0172
2025-04-08 13:40:59.306009: Training Step 126/354: batchLoss = 0.7392, diffLoss = 3.6172, kgLoss = 0.0197
2025-04-08 13:41:00.923077: Training Step 127/354: batchLoss = 0.7131, diffLoss = 3.4971, kgLoss = 0.0171
2025-04-08 13:41:02.529906: Training Step 128/354: batchLoss = 0.4414, diffLoss = 2.1572, kgLoss = 0.0124
2025-04-08 13:41:04.145003: Training Step 129/354: batchLoss = 0.5972, diffLoss = 2.9334, kgLoss = 0.0132
2025-04-08 13:41:05.769665: Training Step 130/354: batchLoss = 0.5847, diffLoss = 2.8702, kgLoss = 0.0133
2025-04-08 13:41:07.387064: Training Step 131/354: batchLoss = 0.7109, diffLoss = 3.4915, kgLoss = 0.0157
2025-04-08 13:41:09.012547: Training Step 132/354: batchLoss = 0.6444, diffLoss = 3.1601, kgLoss = 0.0155
2025-04-08 13:41:10.636164: Training Step 133/354: batchLoss = 0.7127, diffLoss = 3.4953, kgLoss = 0.0171
2025-04-08 13:41:12.252628: Training Step 134/354: batchLoss = 0.6616, diffLoss = 3.2406, kgLoss = 0.0169
2025-04-08 13:41:13.855865: Training Step 135/354: batchLoss = 0.5575, diffLoss = 2.7359, kgLoss = 0.0129
2025-04-08 13:41:15.470931: Training Step 136/354: batchLoss = 0.5244, diffLoss = 2.5676, kgLoss = 0.0136
2025-04-08 13:41:17.079772: Training Step 137/354: batchLoss = 0.4906, diffLoss = 2.4105, kgLoss = 0.0106
2025-04-08 13:41:18.696880: Training Step 138/354: batchLoss = 0.7040, diffLoss = 3.4574, kgLoss = 0.0157
2025-04-08 13:41:20.310499: Training Step 139/354: batchLoss = 0.6856, diffLoss = 3.3675, kgLoss = 0.0151
2025-04-08 13:41:21.928523: Training Step 140/354: batchLoss = 0.6679, diffLoss = 3.2832, kgLoss = 0.0141
2025-04-08 13:41:23.549500: Training Step 141/354: batchLoss = 0.5832, diffLoss = 2.8592, kgLoss = 0.0142
2025-04-08 13:41:25.168144: Training Step 142/354: batchLoss = 0.7477, diffLoss = 3.6688, kgLoss = 0.0174
2025-04-08 13:41:26.784683: Training Step 143/354: batchLoss = 0.7405, diffLoss = 3.6399, kgLoss = 0.0156
2025-04-08 13:41:28.405612: Training Step 144/354: batchLoss = 0.6194, diffLoss = 3.0170, kgLoss = 0.0200
2025-04-08 13:41:30.015714: Training Step 145/354: batchLoss = 0.6472, diffLoss = 3.1800, kgLoss = 0.0140
2025-04-08 13:41:31.622326: Training Step 146/354: batchLoss = 0.6450, diffLoss = 3.1662, kgLoss = 0.0147
2025-04-08 13:41:33.235032: Training Step 147/354: batchLoss = 0.7052, diffLoss = 3.4553, kgLoss = 0.0177
2025-04-08 13:41:34.854570: Training Step 148/354: batchLoss = 0.6501, diffLoss = 3.1937, kgLoss = 0.0142
2025-04-08 13:41:36.480183: Training Step 149/354: batchLoss = 0.8165, diffLoss = 4.0034, kgLoss = 0.0198
2025-04-08 13:41:38.095247: Training Step 150/354: batchLoss = 0.6378, diffLoss = 3.1328, kgLoss = 0.0141
2025-04-08 13:41:39.709886: Training Step 151/354: batchLoss = 0.5835, diffLoss = 2.8705, kgLoss = 0.0118
2025-04-08 13:41:41.347690: Training Step 152/354: batchLoss = 0.4772, diffLoss = 2.3392, kgLoss = 0.0117
2025-04-08 13:41:42.978604: Training Step 153/354: batchLoss = 0.5913, diffLoss = 2.9026, kgLoss = 0.0134
2025-04-08 13:41:44.610091: Training Step 154/354: batchLoss = 0.7520, diffLoss = 3.6924, kgLoss = 0.0169
2025-04-08 13:41:46.239131: Training Step 155/354: batchLoss = 0.5379, diffLoss = 2.6226, kgLoss = 0.0167
2025-04-08 13:41:47.872846: Training Step 156/354: batchLoss = 0.6290, diffLoss = 3.0915, kgLoss = 0.0134
2025-04-08 13:41:49.506790: Training Step 157/354: batchLoss = 0.7413, diffLoss = 3.6303, kgLoss = 0.0190
2025-04-08 13:41:51.157269: Training Step 158/354: batchLoss = 0.5523, diffLoss = 2.7087, kgLoss = 0.0133
2025-04-08 13:41:52.790924: Training Step 159/354: batchLoss = 0.6218, diffLoss = 3.0510, kgLoss = 0.0145
2025-04-08 13:41:54.431891: Training Step 160/354: batchLoss = 0.5708, diffLoss = 2.8044, kgLoss = 0.0124
2025-04-08 13:41:56.056857: Training Step 161/354: batchLoss = 0.5876, diffLoss = 2.8797, kgLoss = 0.0145
2025-04-08 13:41:57.672160: Training Step 162/354: batchLoss = 0.6342, diffLoss = 3.1167, kgLoss = 0.0135
2025-04-08 13:41:59.283725: Training Step 163/354: batchLoss = 0.5525, diffLoss = 2.7153, kgLoss = 0.0118
2025-04-08 13:42:00.891725: Training Step 164/354: batchLoss = 0.5417, diffLoss = 2.6623, kgLoss = 0.0115
2025-04-08 13:42:02.499465: Training Step 165/354: batchLoss = 0.5974, diffLoss = 2.9352, kgLoss = 0.0129
2025-04-08 13:42:04.117495: Training Step 166/354: batchLoss = 0.7294, diffLoss = 3.5858, kgLoss = 0.0153
2025-04-08 13:42:05.735547: Training Step 167/354: batchLoss = 0.6489, diffLoss = 3.1809, kgLoss = 0.0159
2025-04-08 13:42:07.362867: Training Step 168/354: batchLoss = 0.6809, diffLoss = 3.3442, kgLoss = 0.0151
2025-04-08 13:42:08.989125: Training Step 169/354: batchLoss = 0.4762, diffLoss = 2.3329, kgLoss = 0.0120
2025-04-08 13:42:10.616423: Training Step 170/354: batchLoss = 0.5335, diffLoss = 2.6170, kgLoss = 0.0126
2025-04-08 13:42:12.227326: Training Step 171/354: batchLoss = 0.6115, diffLoss = 2.9947, kgLoss = 0.0157
2025-04-08 13:42:13.834988: Training Step 172/354: batchLoss = 0.5311, diffLoss = 2.5994, kgLoss = 0.0140
2025-04-08 13:42:15.476991: Training Step 173/354: batchLoss = 0.6037, diffLoss = 2.9695, kgLoss = 0.0123
2025-04-08 13:42:17.097792: Training Step 174/354: batchLoss = 0.5786, diffLoss = 2.8399, kgLoss = 0.0133
2025-04-08 13:42:18.722684: Training Step 175/354: batchLoss = 0.6920, diffLoss = 3.3949, kgLoss = 0.0163
2025-04-08 13:42:20.350255: Training Step 176/354: batchLoss = 0.6611, diffLoss = 3.2476, kgLoss = 0.0145
2025-04-08 13:42:21.975511: Training Step 177/354: batchLoss = 0.6054, diffLoss = 2.9711, kgLoss = 0.0140
2025-04-08 13:42:23.591033: Training Step 178/354: batchLoss = 0.5008, diffLoss = 2.4596, kgLoss = 0.0112
2025-04-08 13:42:25.223079: Training Step 179/354: batchLoss = 0.6168, diffLoss = 3.0251, kgLoss = 0.0147
2025-04-08 13:42:26.839165: Training Step 180/354: batchLoss = 0.6325, diffLoss = 3.1046, kgLoss = 0.0144
2025-04-08 13:42:28.457796: Training Step 181/354: batchLoss = 0.6376, diffLoss = 3.1316, kgLoss = 0.0141
2025-04-08 13:42:30.063705: Training Step 182/354: batchLoss = 0.7646, diffLoss = 3.7553, kgLoss = 0.0169
2025-04-08 13:42:31.670350: Training Step 183/354: batchLoss = 0.6480, diffLoss = 3.1750, kgLoss = 0.0162
2025-04-08 13:42:33.293117: Training Step 184/354: batchLoss = 0.6281, diffLoss = 3.0831, kgLoss = 0.0143
2025-04-08 13:42:34.914044: Training Step 185/354: batchLoss = 0.6419, diffLoss = 3.1492, kgLoss = 0.0151
2025-04-08 13:42:36.532583: Training Step 186/354: batchLoss = 0.7776, diffLoss = 3.8125, kgLoss = 0.0188
2025-04-08 13:42:38.155745: Training Step 187/354: batchLoss = 0.7478, diffLoss = 3.6742, kgLoss = 0.0162
2025-04-08 13:42:39.765896: Training Step 188/354: batchLoss = 0.5605, diffLoss = 2.7473, kgLoss = 0.0139
2025-04-08 13:42:41.387270: Training Step 189/354: batchLoss = 0.6498, diffLoss = 3.1851, kgLoss = 0.0159
2025-04-08 13:42:42.996446: Training Step 190/354: batchLoss = 0.7945, diffLoss = 3.9058, kgLoss = 0.0167
2025-04-08 13:42:44.608065: Training Step 191/354: batchLoss = 0.5518, diffLoss = 2.7100, kgLoss = 0.0123
2025-04-08 13:42:46.217054: Training Step 192/354: batchLoss = 0.7029, diffLoss = 3.4508, kgLoss = 0.0160
2025-04-08 13:42:47.837941: Training Step 193/354: batchLoss = 0.4658, diffLoss = 2.2840, kgLoss = 0.0112
2025-04-08 13:42:49.457475: Training Step 194/354: batchLoss = 0.5580, diffLoss = 2.7363, kgLoss = 0.0134
2025-04-08 13:42:51.076471: Training Step 195/354: batchLoss = 0.6206, diffLoss = 3.0515, kgLoss = 0.0129
2025-04-08 13:42:52.698847: Training Step 196/354: batchLoss = 0.5959, diffLoss = 2.9212, kgLoss = 0.0146
2025-04-08 13:42:54.326234: Training Step 197/354: batchLoss = 0.5379, diffLoss = 2.6279, kgLoss = 0.0154
2025-04-08 13:42:55.941271: Training Step 198/354: batchLoss = 0.7050, diffLoss = 3.4659, kgLoss = 0.0148
2025-04-08 13:42:57.551586: Training Step 199/354: batchLoss = 0.7619, diffLoss = 3.7437, kgLoss = 0.0165
2025-04-08 13:42:59.160280: Training Step 200/354: batchLoss = 0.5655, diffLoss = 2.7727, kgLoss = 0.0136
2025-04-08 13:43:00.768102: Training Step 201/354: batchLoss = 0.7049, diffLoss = 3.4613, kgLoss = 0.0158
2025-04-08 13:43:02.382220: Training Step 202/354: batchLoss = 0.6213, diffLoss = 3.0518, kgLoss = 0.0137
2025-04-08 13:43:03.994607: Training Step 203/354: batchLoss = 0.6323, diffLoss = 3.1055, kgLoss = 0.0141
2025-04-08 13:43:05.619326: Training Step 204/354: batchLoss = 0.5552, diffLoss = 2.7282, kgLoss = 0.0120
2025-04-08 13:43:07.238000: Training Step 205/354: batchLoss = 0.6279, diffLoss = 3.0743, kgLoss = 0.0163
2025-04-08 13:43:08.852791: Training Step 206/354: batchLoss = 0.7138, diffLoss = 3.5074, kgLoss = 0.0154
2025-04-08 13:43:10.481252: Training Step 207/354: batchLoss = 0.6038, diffLoss = 2.9616, kgLoss = 0.0143
2025-04-08 13:43:12.093800: Training Step 208/354: batchLoss = 0.5815, diffLoss = 2.8522, kgLoss = 0.0138
2025-04-08 13:43:13.700333: Training Step 209/354: batchLoss = 0.5809, diffLoss = 2.8505, kgLoss = 0.0135
2025-04-08 13:43:15.313108: Training Step 210/354: batchLoss = 0.6311, diffLoss = 3.0969, kgLoss = 0.0146
2025-04-08 13:43:16.936991: Training Step 211/354: batchLoss = 0.4862, diffLoss = 2.3893, kgLoss = 0.0104
2025-04-08 13:43:18.554530: Training Step 212/354: batchLoss = 0.7097, diffLoss = 3.4812, kgLoss = 0.0168
2025-04-08 13:43:20.181965: Training Step 213/354: batchLoss = 0.6109, diffLoss = 3.0026, kgLoss = 0.0129
2025-04-08 13:43:21.802203: Training Step 214/354: batchLoss = 0.5548, diffLoss = 2.7287, kgLoss = 0.0114
2025-04-08 13:43:23.414182: Training Step 215/354: batchLoss = 0.4976, diffLoss = 2.4377, kgLoss = 0.0126
2025-04-08 13:43:25.041902: Training Step 216/354: batchLoss = 0.6221, diffLoss = 3.0455, kgLoss = 0.0162
2025-04-08 13:43:26.652479: Training Step 217/354: batchLoss = 0.5998, diffLoss = 2.9397, kgLoss = 0.0148
2025-04-08 13:43:28.259974: Training Step 218/354: batchLoss = 0.5674, diffLoss = 2.7784, kgLoss = 0.0146
2025-04-08 13:43:29.874188: Training Step 219/354: batchLoss = 0.9030, diffLoss = 4.4393, kgLoss = 0.0189
2025-04-08 13:43:31.489786: Training Step 220/354: batchLoss = 0.5446, diffLoss = 2.6735, kgLoss = 0.0124
2025-04-08 13:43:33.105346: Training Step 221/354: batchLoss = 0.6556, diffLoss = 3.2172, kgLoss = 0.0152
2025-04-08 13:43:34.724881: Training Step 222/354: batchLoss = 0.6566, diffLoss = 3.2239, kgLoss = 0.0148
2025-04-08 13:43:36.358898: Training Step 223/354: batchLoss = 0.6269, diffLoss = 3.0788, kgLoss = 0.0140
2025-04-08 13:43:37.994951: Training Step 224/354: batchLoss = 0.5841, diffLoss = 2.8655, kgLoss = 0.0138
2025-04-08 13:43:39.610451: Training Step 225/354: batchLoss = 0.6457, diffLoss = 3.1648, kgLoss = 0.0159
2025-04-08 13:43:41.232005: Training Step 226/354: batchLoss = 0.5413, diffLoss = 2.6516, kgLoss = 0.0137
2025-04-08 13:43:42.854503: Training Step 227/354: batchLoss = 0.7268, diffLoss = 3.5718, kgLoss = 0.0155
2025-04-08 13:43:44.469847: Training Step 228/354: batchLoss = 0.5253, diffLoss = 2.5806, kgLoss = 0.0114
2025-04-08 13:43:46.087869: Training Step 229/354: batchLoss = 0.5819, diffLoss = 2.8520, kgLoss = 0.0144
2025-04-08 13:43:47.717035: Training Step 230/354: batchLoss = 0.6553, diffLoss = 3.2161, kgLoss = 0.0151
2025-04-08 13:43:49.348710: Training Step 231/354: batchLoss = 0.6249, diffLoss = 3.0585, kgLoss = 0.0164
2025-04-08 13:43:50.969761: Training Step 232/354: batchLoss = 0.5544, diffLoss = 2.7188, kgLoss = 0.0133
2025-04-08 13:43:52.601400: Training Step 233/354: batchLoss = 0.6810, diffLoss = 3.3392, kgLoss = 0.0165
2025-04-08 13:43:54.226148: Training Step 234/354: batchLoss = 0.6976, diffLoss = 3.4281, kgLoss = 0.0149
2025-04-08 13:43:55.845845: Training Step 235/354: batchLoss = 0.5030, diffLoss = 2.4613, kgLoss = 0.0134
2025-04-08 13:43:57.458463: Training Step 236/354: batchLoss = 0.5596, diffLoss = 2.7444, kgLoss = 0.0134
2025-04-08 13:43:59.066150: Training Step 237/354: batchLoss = 0.6443, diffLoss = 3.1612, kgLoss = 0.0151
2025-04-08 13:44:00.683920: Training Step 238/354: batchLoss = 0.6604, diffLoss = 3.2435, kgLoss = 0.0146
2025-04-08 13:44:02.300368: Training Step 239/354: batchLoss = 0.5491, diffLoss = 2.6846, kgLoss = 0.0153
2025-04-08 13:44:03.915084: Training Step 240/354: batchLoss = 0.5695, diffLoss = 2.7933, kgLoss = 0.0136
2025-04-08 13:44:05.532940: Training Step 241/354: batchLoss = 0.7059, diffLoss = 3.4667, kgLoss = 0.0157
2025-04-08 13:44:07.168139: Training Step 242/354: batchLoss = 0.6355, diffLoss = 3.1213, kgLoss = 0.0141
2025-04-08 13:44:08.793106: Training Step 243/354: batchLoss = 0.6270, diffLoss = 3.0749, kgLoss = 0.0150
2025-04-08 13:44:10.407701: Training Step 244/354: batchLoss = 0.5995, diffLoss = 2.9426, kgLoss = 0.0138
2025-04-08 13:44:12.015876: Training Step 245/354: batchLoss = 0.6266, diffLoss = 3.0745, kgLoss = 0.0146
2025-04-08 13:44:13.634993: Training Step 246/354: batchLoss = 0.6275, diffLoss = 3.0805, kgLoss = 0.0143
2025-04-08 13:44:15.256119: Training Step 247/354: batchLoss = 0.5282, diffLoss = 2.5857, kgLoss = 0.0138
2025-04-08 13:44:16.878607: Training Step 248/354: batchLoss = 0.5916, diffLoss = 2.9076, kgLoss = 0.0126
2025-04-08 13:44:18.505472: Training Step 249/354: batchLoss = 0.6115, diffLoss = 2.9905, kgLoss = 0.0167
2025-04-08 13:44:20.126695: Training Step 250/354: batchLoss = 0.7901, diffLoss = 3.8838, kgLoss = 0.0167
2025-04-08 13:44:21.752054: Training Step 251/354: batchLoss = 0.5828, diffLoss = 2.8620, kgLoss = 0.0130
2025-04-08 13:44:23.373490: Training Step 252/354: batchLoss = 0.7291, diffLoss = 3.5821, kgLoss = 0.0159
2025-04-08 13:44:24.981984: Training Step 253/354: batchLoss = 0.6401, diffLoss = 3.1332, kgLoss = 0.0169
2025-04-08 13:44:26.593627: Training Step 254/354: batchLoss = 0.5694, diffLoss = 2.7921, kgLoss = 0.0138
2025-04-08 13:44:28.205923: Training Step 255/354: batchLoss = 0.7367, diffLoss = 3.6185, kgLoss = 0.0162
2025-04-08 13:44:29.814774: Training Step 256/354: batchLoss = 0.5755, diffLoss = 2.8266, kgLoss = 0.0128
2025-04-08 13:44:31.425651: Training Step 257/354: batchLoss = 0.5648, diffLoss = 2.7704, kgLoss = 0.0134
2025-04-08 13:44:33.054345: Training Step 258/354: batchLoss = 0.6589, diffLoss = 3.2386, kgLoss = 0.0140
2025-04-08 13:44:34.681800: Training Step 259/354: batchLoss = 0.6165, diffLoss = 3.0319, kgLoss = 0.0127
2025-04-08 13:44:36.304421: Training Step 260/354: batchLoss = 0.5765, diffLoss = 2.8273, kgLoss = 0.0138
2025-04-08 13:44:37.920571: Training Step 261/354: batchLoss = 0.5222, diffLoss = 2.5590, kgLoss = 0.0130
2025-04-08 13:44:39.534128: Training Step 262/354: batchLoss = 0.6330, diffLoss = 3.1067, kgLoss = 0.0145
2025-04-08 13:44:41.139721: Training Step 263/354: batchLoss = 0.5925, diffLoss = 2.9049, kgLoss = 0.0144
2025-04-08 13:44:42.753216: Training Step 264/354: batchLoss = 1.2748, diffLoss = 6.2733, kgLoss = 0.0251
2025-04-08 13:44:44.357329: Training Step 265/354: batchLoss = 0.5511, diffLoss = 2.7016, kgLoss = 0.0134
2025-04-08 13:44:45.969632: Training Step 266/354: batchLoss = 0.6069, diffLoss = 2.9772, kgLoss = 0.0144
2025-04-08 13:44:47.586633: Training Step 267/354: batchLoss = 0.7123, diffLoss = 3.5003, kgLoss = 0.0153
2025-04-08 13:44:49.201692: Training Step 268/354: batchLoss = 0.5992, diffLoss = 2.9410, kgLoss = 0.0138
2025-04-08 13:44:50.825937: Training Step 269/354: batchLoss = 0.6089, diffLoss = 2.9962, kgLoss = 0.0120
2025-04-08 13:44:52.444013: Training Step 270/354: batchLoss = 0.5988, diffLoss = 2.9384, kgLoss = 0.0139
2025-04-08 13:44:54.058759: Training Step 271/354: batchLoss = 0.5668, diffLoss = 2.7875, kgLoss = 0.0117
2025-04-08 13:44:55.669646: Training Step 272/354: batchLoss = 0.6001, diffLoss = 2.9521, kgLoss = 0.0121
2025-04-08 13:44:57.277263: Training Step 273/354: batchLoss = 0.6542, diffLoss = 3.2171, kgLoss = 0.0135
2025-04-08 13:44:58.888682: Training Step 274/354: batchLoss = 1.0118, diffLoss = 4.9781, kgLoss = 0.0203
2025-04-08 13:45:00.501073: Training Step 275/354: batchLoss = 0.6905, diffLoss = 3.3906, kgLoss = 0.0155
2025-04-08 13:45:02.120582: Training Step 276/354: batchLoss = 0.7703, diffLoss = 3.7897, kgLoss = 0.0154
2025-04-08 13:45:03.741444: Training Step 277/354: batchLoss = 0.8504, diffLoss = 4.1777, kgLoss = 0.0186
2025-04-08 13:45:05.357245: Training Step 278/354: batchLoss = 0.7815, diffLoss = 3.8400, kgLoss = 0.0169
2025-04-08 13:45:06.975164: Training Step 279/354: batchLoss = 0.5242, diffLoss = 2.5640, kgLoss = 0.0142
2025-04-08 13:45:08.594269: Training Step 280/354: batchLoss = 0.5948, diffLoss = 2.9067, kgLoss = 0.0168
2025-04-08 13:45:10.206764: Training Step 281/354: batchLoss = 0.5316, diffLoss = 2.6154, kgLoss = 0.0107
2025-04-08 13:45:11.812294: Training Step 282/354: batchLoss = 0.6182, diffLoss = 3.0305, kgLoss = 0.0152
2025-04-08 13:45:13.448344: Training Step 283/354: batchLoss = 0.5674, diffLoss = 2.7869, kgLoss = 0.0126
2025-04-08 13:45:15.077624: Training Step 284/354: batchLoss = 0.5780, diffLoss = 2.8338, kgLoss = 0.0141
2025-04-08 13:45:16.702475: Training Step 285/354: batchLoss = 0.6115, diffLoss = 3.0011, kgLoss = 0.0141
2025-04-08 13:45:18.324056: Training Step 286/354: batchLoss = 0.6041, diffLoss = 2.9641, kgLoss = 0.0141
2025-04-08 13:45:19.932259: Training Step 287/354: batchLoss = 0.7369, diffLoss = 3.6217, kgLoss = 0.0157
2025-04-08 13:45:21.554894: Training Step 288/354: batchLoss = 0.6988, diffLoss = 3.4346, kgLoss = 0.0149
2025-04-08 13:45:23.173225: Training Step 289/354: batchLoss = 0.5660, diffLoss = 2.7744, kgLoss = 0.0139
2025-04-08 13:45:24.785627: Training Step 290/354: batchLoss = 0.5369, diffLoss = 2.6309, kgLoss = 0.0134
2025-04-08 13:45:26.395759: Training Step 291/354: batchLoss = 0.6977, diffLoss = 3.4231, kgLoss = 0.0163
2025-04-08 13:45:28.003978: Training Step 292/354: batchLoss = 0.6854, diffLoss = 3.3718, kgLoss = 0.0138
2025-04-08 13:45:29.619677: Training Step 293/354: batchLoss = 0.6583, diffLoss = 3.2315, kgLoss = 0.0150
2025-04-08 13:45:31.245647: Training Step 294/354: batchLoss = 0.6407, diffLoss = 3.1451, kgLoss = 0.0145
2025-04-08 13:45:32.868474: Training Step 295/354: batchLoss = 0.7874, diffLoss = 3.8663, kgLoss = 0.0176
2025-04-08 13:45:34.491378: Training Step 296/354: batchLoss = 0.5878, diffLoss = 2.8837, kgLoss = 0.0138
2025-04-08 13:45:36.110471: Training Step 297/354: batchLoss = 0.5342, diffLoss = 2.6227, kgLoss = 0.0121
2025-04-08 13:45:37.724781: Training Step 298/354: batchLoss = 0.5197, diffLoss = 2.5473, kgLoss = 0.0128
2025-04-08 13:45:39.335329: Training Step 299/354: batchLoss = 0.5998, diffLoss = 2.9422, kgLoss = 0.0142
2025-04-08 13:45:40.943053: Training Step 300/354: batchLoss = 0.5361, diffLoss = 2.6247, kgLoss = 0.0139
2025-04-08 13:45:42.547969: Training Step 301/354: batchLoss = 0.5978, diffLoss = 2.9331, kgLoss = 0.0140
2025-04-08 13:45:44.150995: Training Step 302/354: batchLoss = 0.7183, diffLoss = 3.5271, kgLoss = 0.0161
2025-04-08 13:45:45.771894: Training Step 303/354: batchLoss = 0.5612, diffLoss = 2.7539, kgLoss = 0.0130
2025-04-08 13:45:47.383457: Training Step 304/354: batchLoss = 0.6731, diffLoss = 3.3079, kgLoss = 0.0144
2025-04-08 13:45:49.003381: Training Step 305/354: batchLoss = 0.7137, diffLoss = 3.5063, kgLoss = 0.0156
2025-04-08 13:45:50.625486: Training Step 306/354: batchLoss = 0.6210, diffLoss = 3.0408, kgLoss = 0.0160
2025-04-08 13:45:52.244152: Training Step 307/354: batchLoss = 0.7231, diffLoss = 3.5519, kgLoss = 0.0159
2025-04-08 13:45:53.849034: Training Step 308/354: batchLoss = 0.5635, diffLoss = 2.7670, kgLoss = 0.0126
2025-04-08 13:45:55.452117: Training Step 309/354: batchLoss = 0.5884, diffLoss = 2.8897, kgLoss = 0.0131
2025-04-08 13:45:57.070161: Training Step 310/354: batchLoss = 0.6092, diffLoss = 2.9816, kgLoss = 0.0161
2025-04-08 13:45:58.686562: Training Step 311/354: batchLoss = 0.5081, diffLoss = 2.4868, kgLoss = 0.0134
2025-04-08 13:46:00.315168: Training Step 312/354: batchLoss = 0.5851, diffLoss = 2.8791, kgLoss = 0.0116
2025-04-08 13:46:01.932610: Training Step 313/354: batchLoss = 0.6528, diffLoss = 3.1974, kgLoss = 0.0167
2025-04-08 13:46:03.554008: Training Step 314/354: batchLoss = 0.5708, diffLoss = 2.8064, kgLoss = 0.0119
2025-04-08 13:46:05.165413: Training Step 315/354: batchLoss = 0.6380, diffLoss = 3.1336, kgLoss = 0.0141
2025-04-08 13:46:06.785339: Training Step 316/354: batchLoss = 0.5256, diffLoss = 2.5772, kgLoss = 0.0127
2025-04-08 13:46:08.390393: Training Step 317/354: batchLoss = 0.7233, diffLoss = 3.5485, kgLoss = 0.0170
2025-04-08 13:46:09.999715: Training Step 318/354: batchLoss = 0.5149, diffLoss = 2.5160, kgLoss = 0.0146
2025-04-08 13:46:11.606633: Training Step 319/354: batchLoss = 0.6804, diffLoss = 3.3400, kgLoss = 0.0155
2025-04-08 13:46:13.221588: Training Step 320/354: batchLoss = 0.5873, diffLoss = 2.8842, kgLoss = 0.0131
2025-04-08 13:46:14.844111: Training Step 321/354: batchLoss = 0.6255, diffLoss = 3.0689, kgLoss = 0.0147
2025-04-08 13:46:16.484167: Training Step 322/354: batchLoss = 0.5940, diffLoss = 2.9160, kgLoss = 0.0135
2025-04-08 13:46:18.122305: Training Step 323/354: batchLoss = 0.6327, diffLoss = 3.1124, kgLoss = 0.0128
2025-04-08 13:46:19.763579: Training Step 324/354: batchLoss = 0.6704, diffLoss = 3.2882, kgLoss = 0.0159
2025-04-08 13:46:21.404650: Training Step 325/354: batchLoss = 0.6081, diffLoss = 2.9766, kgLoss = 0.0160
2025-04-08 13:46:23.017275: Training Step 326/354: batchLoss = 0.5430, diffLoss = 2.6594, kgLoss = 0.0139
2025-04-08 13:46:24.641855: Training Step 327/354: batchLoss = 0.5664, diffLoss = 2.7823, kgLoss = 0.0124
2025-04-08 13:46:26.274000: Training Step 328/354: batchLoss = 0.5194, diffLoss = 2.5513, kgLoss = 0.0114
2025-04-08 13:46:27.896522: Training Step 329/354: batchLoss = 0.6124, diffLoss = 3.0010, kgLoss = 0.0153
2025-04-08 13:46:29.528080: Training Step 330/354: batchLoss = 0.5706, diffLoss = 2.8014, kgLoss = 0.0129
2025-04-08 13:46:31.148334: Training Step 331/354: batchLoss = 0.5679, diffLoss = 2.7886, kgLoss = 0.0128
2025-04-08 13:46:32.768921: Training Step 332/354: batchLoss = 0.6752, diffLoss = 3.3088, kgLoss = 0.0169
2025-04-08 13:46:34.384378: Training Step 333/354: batchLoss = 0.5735, diffLoss = 2.8124, kgLoss = 0.0138
2025-04-08 13:46:35.998465: Training Step 334/354: batchLoss = 0.4646, diffLoss = 2.2745, kgLoss = 0.0122
2025-04-08 13:46:37.613872: Training Step 335/354: batchLoss = 0.5469, diffLoss = 2.6804, kgLoss = 0.0135
2025-04-08 13:46:39.217518: Training Step 336/354: batchLoss = 0.6588, diffLoss = 3.2397, kgLoss = 0.0136
2025-04-08 13:46:40.824317: Training Step 337/354: batchLoss = 0.7106, diffLoss = 3.4820, kgLoss = 0.0178
2025-04-08 13:46:42.443204: Training Step 338/354: batchLoss = 0.5953, diffLoss = 2.9115, kgLoss = 0.0163
2025-04-08 13:46:44.067436: Training Step 339/354: batchLoss = 0.7043, diffLoss = 3.4623, kgLoss = 0.0148
2025-04-08 13:46:45.690324: Training Step 340/354: batchLoss = 1.9602, diffLoss = 9.6388, kgLoss = 0.0405
2025-04-08 13:46:47.307080: Training Step 341/354: batchLoss = 0.5545, diffLoss = 2.7205, kgLoss = 0.0130
2025-04-08 13:46:48.927207: Training Step 342/354: batchLoss = 0.5824, diffLoss = 2.8641, kgLoss = 0.0120
2025-04-08 13:46:50.545049: Training Step 343/354: batchLoss = 0.7170, diffLoss = 3.5190, kgLoss = 0.0165
2025-04-08 13:46:52.162530: Training Step 344/354: batchLoss = 0.5042, diffLoss = 2.4704, kgLoss = 0.0127
2025-04-08 13:46:53.768355: Training Step 345/354: batchLoss = 0.5697, diffLoss = 2.7956, kgLoss = 0.0132
2025-04-08 13:46:55.377508: Training Step 346/354: batchLoss = 0.6187, diffLoss = 3.0406, kgLoss = 0.0133
2025-04-08 13:46:56.990175: Training Step 347/354: batchLoss = 0.5206, diffLoss = 2.5532, kgLoss = 0.0125
2025-04-08 13:46:58.609730: Training Step 348/354: batchLoss = 0.5638, diffLoss = 2.7667, kgLoss = 0.0130
2025-04-08 13:47:00.223482: Training Step 349/354: batchLoss = 0.5053, diffLoss = 2.4803, kgLoss = 0.0115
2025-04-08 13:47:01.841392: Training Step 350/354: batchLoss = 0.5892, diffLoss = 2.8882, kgLoss = 0.0144
2025-04-08 13:47:03.450662: Training Step 351/354: batchLoss = 0.6106, diffLoss = 2.9956, kgLoss = 0.0144
2025-04-08 13:47:05.048389: Training Step 352/354: batchLoss = 0.5041, diffLoss = 2.4769, kgLoss = 0.0110
2025-04-08 13:47:06.454235: Training Step 353/354: batchLoss = 0.5457, diffLoss = 2.6803, kgLoss = 0.0120
2025-04-08 13:47:06.548837: 
2025-04-08 13:47:06.549478: Epoch 5/1000, Train: epLoss = 1.1198, epDfLoss = 5.4951, epfTransLoss = 0.0000, epKgLoss = 0.0259  
2025-04-08 13:47:07.849953: Steps 0/138: batch_recall = 45.64, batch_ndcg = 25.59 
2025-04-08 13:47:09.164038: Steps 1/138: batch_recall = 43.14, batch_ndcg = 25.70 
2025-04-08 13:47:10.454067: Steps 2/138: batch_recall = 55.45, batch_ndcg = 33.12 
2025-04-08 13:47:11.775965: Steps 3/138: batch_recall = 52.14, batch_ndcg = 30.11 
2025-04-08 13:47:13.098711: Steps 4/138: batch_recall = 61.95, batch_ndcg = 39.35 
2025-04-08 13:47:14.420643: Steps 5/138: batch_recall = 53.98, batch_ndcg = 28.26 
2025-04-08 13:47:15.727016: Steps 6/138: batch_recall = 46.37, batch_ndcg = 27.41 
2025-04-08 13:47:17.032370: Steps 7/138: batch_recall = 60.10, batch_ndcg = 36.72 
2025-04-08 13:47:18.348112: Steps 8/138: batch_recall = 58.96, batch_ndcg = 36.53 
2025-04-08 13:47:19.645528: Steps 9/138: batch_recall = 49.28, batch_ndcg = 30.32 
2025-04-08 13:47:20.939913: Steps 10/138: batch_recall = 47.93, batch_ndcg = 27.41 
2025-04-08 13:47:22.237842: Steps 11/138: batch_recall = 62.25, batch_ndcg = 34.61 
2025-04-08 13:47:23.519332: Steps 12/138: batch_recall = 52.04, batch_ndcg = 29.33 
2025-04-08 13:47:24.815221: Steps 13/138: batch_recall = 47.99, batch_ndcg = 27.21 
2025-04-08 13:47:26.099716: Steps 14/138: batch_recall = 45.88, batch_ndcg = 27.02 
2025-04-08 13:47:27.419397: Steps 15/138: batch_recall = 49.23, batch_ndcg = 30.31 
2025-04-08 13:47:28.738570: Steps 16/138: batch_recall = 55.58, batch_ndcg = 30.11 
2025-04-08 13:47:30.009978: Steps 17/138: batch_recall = 53.22, batch_ndcg = 29.07 
2025-04-08 13:47:31.305780: Steps 18/138: batch_recall = 46.22, batch_ndcg = 29.56 
2025-04-08 13:47:32.620974: Steps 19/138: batch_recall = 50.91, batch_ndcg = 30.05 
2025-04-08 13:47:33.913157: Steps 20/138: batch_recall = 60.15, batch_ndcg = 33.00 
2025-04-08 13:47:35.204197: Steps 21/138: batch_recall = 60.53, batch_ndcg = 37.42 
2025-04-08 13:47:36.487763: Steps 22/138: batch_recall = 52.44, batch_ndcg = 31.04 
2025-04-08 13:47:37.777527: Steps 23/138: batch_recall = 48.65, batch_ndcg = 27.61 
2025-04-08 13:47:39.055611: Steps 24/138: batch_recall = 42.74, batch_ndcg = 26.14 
2025-04-08 13:47:40.336944: Steps 25/138: batch_recall = 52.18, batch_ndcg = 31.37 
2025-04-08 13:47:41.638060: Steps 26/138: batch_recall = 55.90, batch_ndcg = 31.53 
2025-04-08 13:47:42.941615: Steps 27/138: batch_recall = 61.01, batch_ndcg = 34.29 
2025-04-08 13:47:44.242940: Steps 28/138: batch_recall = 52.89, batch_ndcg = 30.68 
2025-04-08 13:47:45.542981: Steps 29/138: batch_recall = 56.89, batch_ndcg = 30.16 
2025-04-08 13:47:46.851764: Steps 30/138: batch_recall = 52.83, batch_ndcg = 33.31 
2025-04-08 13:47:48.141942: Steps 31/138: batch_recall = 37.17, batch_ndcg = 22.20 
2025-04-08 13:47:49.430983: Steps 32/138: batch_recall = 47.70, batch_ndcg = 28.41 
2025-04-08 13:47:50.707563: Steps 33/138: batch_recall = 54.92, batch_ndcg = 31.93 
2025-04-08 13:47:51.977649: Steps 34/138: batch_recall = 49.24, batch_ndcg = 26.63 
2025-04-08 13:47:53.245678: Steps 35/138: batch_recall = 47.74, batch_ndcg = 28.25 
2025-04-08 13:47:54.521998: Steps 36/138: batch_recall = 50.85, batch_ndcg = 27.68 
2025-04-08 13:47:55.797735: Steps 37/138: batch_recall = 61.48, batch_ndcg = 33.37 
2025-04-08 13:47:57.094598: Steps 38/138: batch_recall = 56.93, batch_ndcg = 32.12 
2025-04-08 13:47:58.379270: Steps 39/138: batch_recall = 71.93, batch_ndcg = 37.59 
2025-04-08 13:47:59.670578: Steps 40/138: batch_recall = 49.75, batch_ndcg = 26.76 
2025-04-08 13:48:00.973477: Steps 41/138: batch_recall = 58.80, batch_ndcg = 32.90 
2025-04-08 13:48:02.251758: Steps 42/138: batch_recall = 54.34, batch_ndcg = 30.59 
2025-04-08 13:48:03.527560: Steps 43/138: batch_recall = 51.49, batch_ndcg = 32.65 
2025-04-08 13:48:04.814793: Steps 44/138: batch_recall = 49.72, batch_ndcg = 27.93 
2025-04-08 13:48:06.109239: Steps 45/138: batch_recall = 59.09, batch_ndcg = 32.43 
2025-04-08 13:48:07.375081: Steps 46/138: batch_recall = 57.60, batch_ndcg = 33.27 
2025-04-08 13:48:08.645688: Steps 47/138: batch_recall = 58.12, batch_ndcg = 32.55 
2025-04-08 13:48:09.910887: Steps 48/138: batch_recall = 53.60, batch_ndcg = 31.39 
2025-04-08 13:48:11.192691: Steps 49/138: batch_recall = 59.48, batch_ndcg = 36.71 
2025-04-08 13:48:12.477166: Steps 50/138: batch_recall = 60.20, batch_ndcg = 32.02 
2025-04-08 13:48:13.759261: Steps 51/138: batch_recall = 60.16, batch_ndcg = 34.93 
2025-04-08 13:48:15.037766: Steps 52/138: batch_recall = 64.16, batch_ndcg = 38.81 
2025-04-08 13:48:16.334389: Steps 53/138: batch_recall = 56.07, batch_ndcg = 30.08 
2025-04-08 13:48:17.627487: Steps 54/138: batch_recall = 58.67, batch_ndcg = 32.38 
2025-04-08 13:48:18.924314: Steps 55/138: batch_recall = 55.06, batch_ndcg = 31.15 
2025-04-08 13:48:20.220389: Steps 56/138: batch_recall = 61.50, batch_ndcg = 34.16 
2025-04-08 13:48:21.494703: Steps 57/138: batch_recall = 57.40, batch_ndcg = 31.90 
2025-04-08 13:48:22.766663: Steps 58/138: batch_recall = 63.40, batch_ndcg = 33.18 
2025-04-08 13:48:24.037804: Steps 59/138: batch_recall = 71.03, batch_ndcg = 39.19 
2025-04-08 13:48:25.340363: Steps 60/138: batch_recall = 69.00, batch_ndcg = 38.03 
2025-04-08 13:48:26.631064: Steps 61/138: batch_recall = 61.18, batch_ndcg = 32.69 
2025-04-08 13:48:27.919764: Steps 62/138: batch_recall = 75.47, batch_ndcg = 41.39 
2025-04-08 13:48:29.212929: Steps 63/138: batch_recall = 75.05, batch_ndcg = 43.01 
2025-04-08 13:48:30.497131: Steps 64/138: batch_recall = 58.41, batch_ndcg = 31.92 
2025-04-08 13:48:31.776177: Steps 65/138: batch_recall = 81.72, batch_ndcg = 45.55 
2025-04-08 13:48:33.057678: Steps 66/138: batch_recall = 65.10, batch_ndcg = 38.35 
2025-04-08 13:48:34.354788: Steps 67/138: batch_recall = 79.61, batch_ndcg = 47.71 
2025-04-08 13:48:35.625942: Steps 68/138: batch_recall = 66.18, batch_ndcg = 34.42 
2025-04-08 13:48:36.887087: Steps 69/138: batch_recall = 85.76, batch_ndcg = 48.55 
2025-04-08 13:48:38.149328: Steps 70/138: batch_recall = 78.79, batch_ndcg = 43.18 
2025-04-08 13:48:39.414914: Steps 71/138: batch_recall = 83.00, batch_ndcg = 47.14 
2025-04-08 13:48:40.701643: Steps 72/138: batch_recall = 83.04, batch_ndcg = 49.05 
2025-04-08 13:48:41.985559: Steps 73/138: batch_recall = 89.55, batch_ndcg = 45.86 
2025-04-08 13:48:43.269398: Steps 74/138: batch_recall = 77.55, batch_ndcg = 46.77 
2025-04-08 13:48:44.569108: Steps 75/138: batch_recall = 82.19, batch_ndcg = 47.43 
2025-04-08 13:48:45.849121: Steps 76/138: batch_recall = 95.26, batch_ndcg = 55.62 
2025-04-08 13:48:47.137770: Steps 77/138: batch_recall = 76.86, batch_ndcg = 43.78 
2025-04-08 13:48:48.425955: Steps 78/138: batch_recall = 96.11, batch_ndcg = 50.21 
2025-04-08 13:48:49.707190: Steps 79/138: batch_recall = 86.72, batch_ndcg = 46.75 
2025-04-08 13:48:50.975248: Steps 80/138: batch_recall = 76.22, batch_ndcg = 40.69 
2025-04-08 13:48:52.240543: Steps 81/138: batch_recall = 79.49, batch_ndcg = 45.75 
2025-04-08 13:48:53.515934: Steps 82/138: batch_recall = 84.34, batch_ndcg = 47.71 
2025-04-08 13:48:54.800103: Steps 83/138: batch_recall = 80.11, batch_ndcg = 45.68 
2025-04-08 13:48:56.097635: Steps 84/138: batch_recall = 104.07, batch_ndcg = 56.79 
2025-04-08 13:48:57.389708: Steps 85/138: batch_recall = 94.41, batch_ndcg = 52.31 
2025-04-08 13:48:58.680043: Steps 86/138: batch_recall = 117.11, batch_ndcg = 67.81 
2025-04-08 13:48:59.956160: Steps 87/138: batch_recall = 103.65, batch_ndcg = 56.69 
2025-04-08 13:49:01.244528: Steps 88/138: batch_recall = 102.12, batch_ndcg = 56.81 
2025-04-08 13:49:02.539293: Steps 89/138: batch_recall = 121.19, batch_ndcg = 65.17 
2025-04-08 13:49:03.830458: Steps 90/138: batch_recall = 106.85, batch_ndcg = 56.66 
2025-04-08 13:49:05.086780: Steps 91/138: batch_recall = 119.42, batch_ndcg = 64.10 
2025-04-08 13:49:06.349961: Steps 92/138: batch_recall = 107.33, batch_ndcg = 58.70 
2025-04-08 13:49:07.615660: Steps 93/138: batch_recall = 114.37, batch_ndcg = 67.02 
2025-04-08 13:49:08.895324: Steps 94/138: batch_recall = 109.21, batch_ndcg = 58.97 
2025-04-08 13:49:10.189194: Steps 95/138: batch_recall = 116.72, batch_ndcg = 65.57 
2025-04-08 13:49:11.460355: Steps 96/138: batch_recall = 129.71, batch_ndcg = 72.60 
2025-04-08 13:49:12.734247: Steps 97/138: batch_recall = 132.77, batch_ndcg = 72.86 
2025-04-08 13:49:14.020021: Steps 98/138: batch_recall = 104.09, batch_ndcg = 60.92 
2025-04-08 13:49:15.316476: Steps 99/138: batch_recall = 115.21, batch_ndcg = 63.91 
2025-04-08 13:49:16.594734: Steps 100/138: batch_recall = 115.99, batch_ndcg = 67.17 
2025-04-08 13:49:17.874248: Steps 101/138: batch_recall = 120.72, batch_ndcg = 68.62 
2025-04-08 13:49:19.139762: Steps 102/138: batch_recall = 124.18, batch_ndcg = 70.03 
2025-04-08 13:49:20.410029: Steps 103/138: batch_recall = 134.49, batch_ndcg = 74.77 
2025-04-08 13:49:21.688114: Steps 104/138: batch_recall = 134.61, batch_ndcg = 72.07 
2025-04-08 13:49:22.958434: Steps 105/138: batch_recall = 120.06, batch_ndcg = 67.36 
2025-04-08 13:49:24.228934: Steps 106/138: batch_recall = 109.53, batch_ndcg = 58.56 
2025-04-08 13:49:25.512398: Steps 107/138: batch_recall = 115.24, batch_ndcg = 61.15 
2025-04-08 13:49:26.838783: Steps 108/138: batch_recall = 119.52, batch_ndcg = 69.78 
2025-04-08 13:49:28.127195: Steps 109/138: batch_recall = 135.83, batch_ndcg = 73.06 
2025-04-08 13:49:29.405722: Steps 110/138: batch_recall = 121.47, batch_ndcg = 63.39 
2025-04-08 13:49:30.675291: Steps 111/138: batch_recall = 134.71, batch_ndcg = 79.83 
2025-04-08 13:49:31.958022: Steps 112/138: batch_recall = 146.07, batch_ndcg = 79.16 
2025-04-08 13:49:33.232704: Steps 113/138: batch_recall = 120.35, batch_ndcg = 69.19 
2025-04-08 13:49:34.499738: Steps 114/138: batch_recall = 115.42, batch_ndcg = 64.12 
2025-04-08 13:49:35.762527: Steps 115/138: batch_recall = 111.45, batch_ndcg = 57.22 
2025-04-08 13:49:37.017385: Steps 116/138: batch_recall = 123.97, batch_ndcg = 64.08 
2025-04-08 13:49:38.289394: Steps 117/138: batch_recall = 111.28, batch_ndcg = 63.72 
2025-04-08 13:49:39.549210: Steps 118/138: batch_recall = 124.00, batch_ndcg = 69.13 
2025-04-08 13:49:40.822335: Steps 119/138: batch_recall = 131.19, batch_ndcg = 71.19 
2025-04-08 13:49:42.093360: Steps 120/138: batch_recall = 122.30, batch_ndcg = 65.15 
2025-04-08 13:49:43.360231: Steps 121/138: batch_recall = 143.14, batch_ndcg = 74.29 
2025-04-08 13:49:44.640718: Steps 122/138: batch_recall = 135.37, batch_ndcg = 74.06 
2025-04-08 13:49:45.919631: Steps 123/138: batch_recall = 125.65, batch_ndcg = 68.73 
2025-04-08 13:49:47.195600: Steps 124/138: batch_recall = 142.35, batch_ndcg = 89.16 
2025-04-08 13:49:48.460710: Steps 125/138: batch_recall = 127.67, batch_ndcg = 67.74 
2025-04-08 13:49:49.726509: Steps 126/138: batch_recall = 157.06, batch_ndcg = 89.27 
2025-04-08 13:49:51.000118: Steps 127/138: batch_recall = 138.08, batch_ndcg = 78.74 
2025-04-08 13:49:52.264219: Steps 128/138: batch_recall = 123.15, batch_ndcg = 65.27 
2025-04-08 13:49:53.539694: Steps 129/138: batch_recall = 145.62, batch_ndcg = 86.21 
2025-04-08 13:49:54.814756: Steps 130/138: batch_recall = 127.11, batch_ndcg = 67.22 
2025-04-08 13:49:56.089146: Steps 131/138: batch_recall = 134.79, batch_ndcg = 77.88 
2025-04-08 13:49:57.358247: Steps 132/138: batch_recall = 148.08, batch_ndcg = 82.05 
2025-04-08 13:49:58.644287: Steps 133/138: batch_recall = 140.71, batch_ndcg = 80.42 
2025-04-08 13:49:59.939114: Steps 134/138: batch_recall = 132.79, batch_ndcg = 74.01 
2025-04-08 13:50:01.207846: Steps 135/138: batch_recall = 161.19, batch_ndcg = 90.66 
2025-04-08 13:50:02.465475: Steps 136/138: batch_recall = 149.95, batch_ndcg = 78.44 
2025-04-08 13:50:03.732460: Steps 137/138: batch_recall = 132.62, batch_ndcg = 81.82 
2025-04-08 13:50:03.733000: Epoch 5/1000, Test: Recall = 0.1688, NDCG = 0.0947  

2025-04-08 13:50:05.492659: Training Step 0/354: batchLoss = 0.7250, diffLoss = 3.5576, kgLoss = 0.0169
2025-04-08 13:50:07.114857: Training Step 1/354: batchLoss = 0.5599, diffLoss = 2.7401, kgLoss = 0.0149
2025-04-08 13:50:08.742052: Training Step 2/354: batchLoss = 0.7358, diffLoss = 3.6160, kgLoss = 0.0157
2025-04-08 13:50:10.359047: Training Step 3/354: batchLoss = 0.5560, diffLoss = 2.7286, kgLoss = 0.0129
2025-04-08 13:50:11.979318: Training Step 4/354: batchLoss = 0.7263, diffLoss = 3.5702, kgLoss = 0.0153
2025-04-08 13:50:13.595555: Training Step 5/354: batchLoss = 0.5534, diffLoss = 2.7055, kgLoss = 0.0154
2025-04-08 13:50:15.219872: Training Step 6/354: batchLoss = 0.5982, diffLoss = 2.9387, kgLoss = 0.0131
2025-04-08 13:50:16.846920: Training Step 7/354: batchLoss = 0.4295, diffLoss = 2.1063, kgLoss = 0.0104
2025-04-08 13:50:18.458593: Training Step 8/354: batchLoss = 0.5418, diffLoss = 2.6517, kgLoss = 0.0143
2025-04-08 13:50:20.061601: Training Step 9/354: batchLoss = 0.6180, diffLoss = 3.0394, kgLoss = 0.0126
2025-04-08 13:50:21.672716: Training Step 10/354: batchLoss = 0.7042, diffLoss = 3.4536, kgLoss = 0.0169
2025-04-08 13:50:23.300378: Training Step 11/354: batchLoss = 0.6456, diffLoss = 3.1646, kgLoss = 0.0159
2025-04-08 13:50:24.920985: Training Step 12/354: batchLoss = 0.7996, diffLoss = 3.9284, kgLoss = 0.0174
2025-04-08 13:50:26.539145: Training Step 13/354: batchLoss = 0.7200, diffLoss = 3.5365, kgLoss = 0.0159
2025-04-08 13:50:28.158324: Training Step 14/354: batchLoss = 0.7328, diffLoss = 3.6010, kgLoss = 0.0158
2025-04-08 13:50:29.782844: Training Step 15/354: batchLoss = 0.6552, diffLoss = 3.2130, kgLoss = 0.0157
2025-04-08 13:50:31.409486: Training Step 16/354: batchLoss = 0.7324, diffLoss = 3.5983, kgLoss = 0.0159
2025-04-08 13:50:33.015760: Training Step 17/354: batchLoss = 0.6283, diffLoss = 3.0835, kgLoss = 0.0145
2025-04-08 13:50:34.632914: Training Step 18/354: batchLoss = 0.6181, diffLoss = 3.0341, kgLoss = 0.0141
2025-04-08 13:50:36.249612: Training Step 19/354: batchLoss = 0.6126, diffLoss = 3.0120, kgLoss = 0.0127
2025-04-08 13:50:37.866584: Training Step 20/354: batchLoss = 0.6721, diffLoss = 3.2980, kgLoss = 0.0156
2025-04-08 13:50:39.483941: Training Step 21/354: batchLoss = 0.8219, diffLoss = 4.0382, kgLoss = 0.0178
2025-04-08 13:50:41.097752: Training Step 22/354: batchLoss = 0.7435, diffLoss = 3.6502, kgLoss = 0.0169
2025-04-08 13:50:42.716218: Training Step 23/354: batchLoss = 0.6140, diffLoss = 3.0088, kgLoss = 0.0153
2025-04-08 13:50:44.338004: Training Step 24/354: batchLoss = 0.7579, diffLoss = 3.7243, kgLoss = 0.0164
2025-04-08 13:50:45.961383: Training Step 25/354: batchLoss = 0.5249, diffLoss = 2.5721, kgLoss = 0.0131
2025-04-08 13:50:47.571966: Training Step 26/354: batchLoss = 0.6993, diffLoss = 3.4328, kgLoss = 0.0160
2025-04-08 13:50:49.178679: Training Step 27/354: batchLoss = 0.7333, diffLoss = 3.5894, kgLoss = 0.0193
2025-04-08 13:50:50.799254: Training Step 28/354: batchLoss = 0.8249, diffLoss = 4.0543, kgLoss = 0.0176
2025-04-08 13:50:52.441488: Training Step 29/354: batchLoss = 0.6354, diffLoss = 3.1196, kgLoss = 0.0144
2025-04-08 13:50:54.081368: Training Step 30/354: batchLoss = 0.6075, diffLoss = 2.9802, kgLoss = 0.0143
2025-04-08 13:50:55.715271: Training Step 31/354: batchLoss = 0.6424, diffLoss = 3.1586, kgLoss = 0.0133
2025-04-08 13:50:57.337049: Training Step 32/354: batchLoss = 0.5361, diffLoss = 2.6291, kgLoss = 0.0129
2025-04-08 13:50:58.968241: Training Step 33/354: batchLoss = 0.5860, diffLoss = 2.8783, kgLoss = 0.0129
2025-04-08 13:51:00.603492: Training Step 34/354: batchLoss = 0.6375, diffLoss = 3.1264, kgLoss = 0.0153
2025-04-08 13:51:02.233099: Training Step 35/354: batchLoss = 0.5730, diffLoss = 2.8140, kgLoss = 0.0127
2025-04-08 13:51:03.879853: Training Step 36/354: batchLoss = 0.6109, diffLoss = 3.0033, kgLoss = 0.0128
2025-04-08 13:51:05.483010: Training Step 37/354: batchLoss = 0.6099, diffLoss = 2.9890, kgLoss = 0.0151
2025-04-08 13:51:07.092978: Training Step 38/354: batchLoss = 0.5412, diffLoss = 2.6548, kgLoss = 0.0128
2025-04-08 13:51:08.713715: Training Step 39/354: batchLoss = 0.5816, diffLoss = 2.8548, kgLoss = 0.0132
2025-04-08 13:51:10.332033: Training Step 40/354: batchLoss = 0.5953, diffLoss = 2.9158, kgLoss = 0.0152
2025-04-08 13:51:11.949152: Training Step 41/354: batchLoss = 0.7275, diffLoss = 3.5682, kgLoss = 0.0173
2025-04-08 13:51:13.564467: Training Step 42/354: batchLoss = 0.6164, diffLoss = 3.0230, kgLoss = 0.0147
2025-04-08 13:51:15.182472: Training Step 43/354: batchLoss = 0.6453, diffLoss = 3.1666, kgLoss = 0.0149
2025-04-08 13:51:16.800635: Training Step 44/354: batchLoss = 0.6419, diffLoss = 3.1469, kgLoss = 0.0156
2025-04-08 13:51:18.402840: Training Step 45/354: batchLoss = 0.7057, diffLoss = 3.4662, kgLoss = 0.0156
2025-04-08 13:51:20.002080: Training Step 46/354: batchLoss = 0.5480, diffLoss = 2.6900, kgLoss = 0.0125
2025-04-08 13:51:21.616239: Training Step 47/354: batchLoss = 0.5187, diffLoss = 2.5429, kgLoss = 0.0126
2025-04-08 13:51:23.230815: Training Step 48/354: batchLoss = 0.4944, diffLoss = 2.4285, kgLoss = 0.0109
2025-04-08 13:51:24.854274: Training Step 49/354: batchLoss = 0.5882, diffLoss = 2.8790, kgLoss = 0.0155
2025-04-08 13:51:26.474710: Training Step 50/354: batchLoss = 0.6379, diffLoss = 3.1248, kgLoss = 0.0162
2025-04-08 13:51:28.089100: Training Step 51/354: batchLoss = 0.6125, diffLoss = 3.0067, kgLoss = 0.0139
2025-04-08 13:51:29.703438: Training Step 52/354: batchLoss = 0.5200, diffLoss = 2.5515, kgLoss = 0.0121
2025-04-08 13:51:31.318429: Training Step 53/354: batchLoss = 0.5139, diffLoss = 2.5168, kgLoss = 0.0131
2025-04-08 13:51:32.937343: Training Step 54/354: batchLoss = 0.6331, diffLoss = 3.1075, kgLoss = 0.0145
2025-04-08 13:51:34.546587: Training Step 55/354: batchLoss = 0.5593, diffLoss = 2.6384, kgLoss = 0.0395
2025-04-08 13:51:36.158675: Training Step 56/354: batchLoss = 0.5395, diffLoss = 2.6479, kgLoss = 0.0125
2025-04-08 13:51:37.782845: Training Step 57/354: batchLoss = 0.6253, diffLoss = 3.0672, kgLoss = 0.0148
2025-04-08 13:51:39.397092: Training Step 58/354: batchLoss = 0.4829, diffLoss = 2.3669, kgLoss = 0.0120
2025-04-08 13:51:41.012924: Training Step 59/354: batchLoss = 0.5924, diffLoss = 2.9047, kgLoss = 0.0143
2025-04-08 13:51:42.631083: Training Step 60/354: batchLoss = 0.6844, diffLoss = 3.3649, kgLoss = 0.0143
2025-04-08 13:51:44.260078: Training Step 61/354: batchLoss = 0.5520, diffLoss = 2.7068, kgLoss = 0.0133
2025-04-08 13:51:45.879467: Training Step 62/354: batchLoss = 0.5538, diffLoss = 2.7120, kgLoss = 0.0143
2025-04-08 13:51:47.488615: Training Step 63/354: batchLoss = 0.6113, diffLoss = 2.9999, kgLoss = 0.0142
2025-04-08 13:51:49.094173: Training Step 64/354: batchLoss = 0.7159, diffLoss = 3.5140, kgLoss = 0.0164
2025-04-08 13:51:50.696361: Training Step 65/354: batchLoss = 0.6015, diffLoss = 2.9491, kgLoss = 0.0146
2025-04-08 13:51:52.303074: Training Step 66/354: batchLoss = 0.5790, diffLoss = 2.8359, kgLoss = 0.0147
2025-04-08 13:51:53.931608: Training Step 67/354: batchLoss = 0.7271, diffLoss = 3.5687, kgLoss = 0.0168
2025-04-08 13:51:55.549633: Training Step 68/354: batchLoss = 0.6245, diffLoss = 3.0695, kgLoss = 0.0133
2025-04-08 13:51:57.173509: Training Step 69/354: batchLoss = 0.5290, diffLoss = 2.5944, kgLoss = 0.0127
2025-04-08 13:51:58.792677: Training Step 70/354: batchLoss = 0.7156, diffLoss = 3.5147, kgLoss = 0.0159
2025-04-08 13:52:00.428300: Training Step 71/354: batchLoss = 0.5296, diffLoss = 2.5983, kgLoss = 0.0124
2025-04-08 13:52:02.032217: Training Step 72/354: batchLoss = 0.7347, diffLoss = 3.6088, kgLoss = 0.0162
2025-04-08 13:52:03.643001: Training Step 73/354: batchLoss = 0.5820, diffLoss = 2.8535, kgLoss = 0.0141
2025-04-08 13:52:05.248783: Training Step 74/354: batchLoss = 0.6061, diffLoss = 2.9751, kgLoss = 0.0138
2025-04-08 13:52:06.871427: Training Step 75/354: batchLoss = 1.0395, diffLoss = 5.0873, kgLoss = 0.0276
2025-04-08 13:52:08.487525: Training Step 76/354: batchLoss = 0.5591, diffLoss = 2.7414, kgLoss = 0.0135
2025-04-08 13:52:10.107252: Training Step 77/354: batchLoss = 0.6396, diffLoss = 3.1422, kgLoss = 0.0140
2025-04-08 13:52:11.720609: Training Step 78/354: batchLoss = 0.6334, diffLoss = 3.1138, kgLoss = 0.0133
2025-04-08 13:52:13.343230: Training Step 79/354: batchLoss = 0.5528, diffLoss = 2.7094, kgLoss = 0.0137
2025-04-08 13:52:14.965081: Training Step 80/354: batchLoss = 0.5854, diffLoss = 2.8707, kgLoss = 0.0141
2025-04-08 13:52:16.577566: Training Step 81/354: batchLoss = 0.5485, diffLoss = 2.6884, kgLoss = 0.0135
2025-04-08 13:52:18.189660: Training Step 82/354: batchLoss = 0.6167, diffLoss = 3.0249, kgLoss = 0.0147
2025-04-08 13:52:19.793985: Training Step 83/354: batchLoss = 0.7310, diffLoss = 3.5949, kgLoss = 0.0150
2025-04-08 13:52:21.419160: Training Step 84/354: batchLoss = 0.7885, diffLoss = 3.8711, kgLoss = 0.0179
2025-04-08 13:52:23.035586: Training Step 85/354: batchLoss = 0.6488, diffLoss = 3.1815, kgLoss = 0.0156
2025-04-08 13:52:24.657145: Training Step 86/354: batchLoss = 0.6211, diffLoss = 3.0479, kgLoss = 0.0144
2025-04-08 13:52:26.279680: Training Step 87/354: batchLoss = 0.6154, diffLoss = 3.0231, kgLoss = 0.0134
2025-04-08 13:52:27.894156: Training Step 88/354: batchLoss = 0.7517, diffLoss = 3.6764, kgLoss = 0.0205
2025-04-08 13:52:29.511940: Training Step 89/354: batchLoss = 0.5534, diffLoss = 2.7143, kgLoss = 0.0132
2025-04-08 13:52:31.131779: Training Step 90/354: batchLoss = 0.5381, diffLoss = 2.6394, kgLoss = 0.0128
2025-04-08 13:52:32.739983: Training Step 91/354: batchLoss = 0.8435, diffLoss = 4.1464, kgLoss = 0.0178
2025-04-08 13:52:34.350242: Training Step 92/354: batchLoss = 0.5785, diffLoss = 2.8427, kgLoss = 0.0125
2025-04-08 13:52:35.954174: Training Step 93/354: batchLoss = 0.5109, diffLoss = 2.5017, kgLoss = 0.0132
2025-04-08 13:52:37.566789: Training Step 94/354: batchLoss = 0.6068, diffLoss = 2.9656, kgLoss = 0.0170
2025-04-08 13:52:39.202477: Training Step 95/354: batchLoss = 0.5437, diffLoss = 2.6676, kgLoss = 0.0128
2025-04-08 13:52:40.820612: Training Step 96/354: batchLoss = 0.6968, diffLoss = 3.4198, kgLoss = 0.0161
2025-04-08 13:52:42.435473: Training Step 97/354: batchLoss = 0.7709, diffLoss = 3.7843, kgLoss = 0.0175
2025-04-08 13:52:44.068980: Training Step 98/354: batchLoss = 0.5173, diffLoss = 2.5391, kgLoss = 0.0119
2025-04-08 13:52:45.681434: Training Step 99/354: batchLoss = 0.6004, diffLoss = 2.9480, kgLoss = 0.0134
2025-04-08 13:52:47.290299: Training Step 100/354: batchLoss = 0.6095, diffLoss = 2.9964, kgLoss = 0.0128
2025-04-08 13:52:48.900345: Training Step 101/354: batchLoss = 0.5759, diffLoss = 2.8291, kgLoss = 0.0126
2025-04-08 13:52:50.506749: Training Step 102/354: batchLoss = 0.5665, diffLoss = 2.7787, kgLoss = 0.0135
2025-04-08 13:52:52.120738: Training Step 103/354: batchLoss = 0.5896, diffLoss = 2.8863, kgLoss = 0.0154
2025-04-08 13:52:53.738534: Training Step 104/354: batchLoss = 0.5304, diffLoss = 2.5983, kgLoss = 0.0135
2025-04-08 13:52:55.362766: Training Step 105/354: batchLoss = 0.6542, diffLoss = 3.2042, kgLoss = 0.0167
2025-04-08 13:52:56.981456: Training Step 106/354: batchLoss = 0.6304, diffLoss = 3.0941, kgLoss = 0.0145
2025-04-08 13:52:58.609439: Training Step 107/354: batchLoss = 0.7263, diffLoss = 3.5680, kgLoss = 0.0159
2025-04-08 13:53:00.244972: Training Step 108/354: batchLoss = 0.8268, diffLoss = 4.0655, kgLoss = 0.0171
2025-04-08 13:53:01.856854: Training Step 109/354: batchLoss = 0.6824, diffLoss = 3.3473, kgLoss = 0.0162
2025-04-08 13:53:03.467760: Training Step 110/354: batchLoss = 0.7305, diffLoss = 3.5871, kgLoss = 0.0164
2025-04-08 13:53:05.072927: Training Step 111/354: batchLoss = 0.6698, diffLoss = 3.2926, kgLoss = 0.0141
2025-04-08 13:53:06.682696: Training Step 112/354: batchLoss = 0.6067, diffLoss = 2.9793, kgLoss = 0.0136
2025-04-08 13:53:08.298343: Training Step 113/354: batchLoss = 0.6855, diffLoss = 3.3668, kgLoss = 0.0152
2025-04-08 13:53:09.924649: Training Step 114/354: batchLoss = 0.5138, diffLoss = 2.5239, kgLoss = 0.0113
2025-04-08 13:53:11.542127: Training Step 115/354: batchLoss = 0.7154, diffLoss = 3.5066, kgLoss = 0.0176
2025-04-08 13:53:13.164161: Training Step 116/354: batchLoss = 0.7059, diffLoss = 3.4682, kgLoss = 0.0153
2025-04-08 13:53:14.777593: Training Step 117/354: batchLoss = 0.6846, diffLoss = 3.3645, kgLoss = 0.0146
2025-04-08 13:53:16.384526: Training Step 118/354: batchLoss = 0.6730, diffLoss = 3.3084, kgLoss = 0.0141
2025-04-08 13:53:17.996621: Training Step 119/354: batchLoss = 0.5684, diffLoss = 2.7802, kgLoss = 0.0154
2025-04-08 13:53:19.608604: Training Step 120/354: batchLoss = 0.5714, diffLoss = 2.8116, kgLoss = 0.0113
2025-04-08 13:53:21.219558: Training Step 121/354: batchLoss = 0.6911, diffLoss = 3.3932, kgLoss = 0.0156
2025-04-08 13:53:22.847789: Training Step 122/354: batchLoss = 0.5910, diffLoss = 2.8922, kgLoss = 0.0157
2025-04-08 13:53:24.465057: Training Step 123/354: batchLoss = 0.6416, diffLoss = 3.1414, kgLoss = 0.0167
2025-04-08 13:53:26.084668: Training Step 124/354: batchLoss = 0.6589, diffLoss = 3.2333, kgLoss = 0.0153
2025-04-08 13:53:27.700828: Training Step 125/354: batchLoss = 0.5887, diffLoss = 2.8865, kgLoss = 0.0142
2025-04-08 13:53:29.327641: Training Step 126/354: batchLoss = 0.6931, diffLoss = 3.4085, kgLoss = 0.0143
2025-04-08 13:53:30.944817: Training Step 127/354: batchLoss = 0.5610, diffLoss = 2.7532, kgLoss = 0.0130
2025-04-08 13:53:32.554306: Training Step 128/354: batchLoss = 0.6985, diffLoss = 3.4279, kgLoss = 0.0162
2025-04-08 13:53:34.160572: Training Step 129/354: batchLoss = 0.5099, diffLoss = 2.5042, kgLoss = 0.0114
2025-04-08 13:53:35.764286: Training Step 130/354: batchLoss = 0.6034, diffLoss = 2.9583, kgLoss = 0.0147
2025-04-08 13:53:37.395763: Training Step 131/354: batchLoss = 0.6786, diffLoss = 3.3278, kgLoss = 0.0163
2025-04-08 13:53:39.009546: Training Step 132/354: batchLoss = 0.5406, diffLoss = 2.6460, kgLoss = 0.0143
2025-04-08 13:53:40.629204: Training Step 133/354: batchLoss = 0.5642, diffLoss = 2.7612, kgLoss = 0.0150
2025-04-08 13:53:42.246018: Training Step 134/354: batchLoss = 0.5304, diffLoss = 2.6053, kgLoss = 0.0116
2025-04-08 13:53:43.858279: Training Step 135/354: batchLoss = 0.5699, diffLoss = 2.7979, kgLoss = 0.0130
2025-04-08 13:53:45.484102: Training Step 136/354: batchLoss = 0.6039, diffLoss = 2.9619, kgLoss = 0.0144
2025-04-08 13:53:47.099883: Training Step 137/354: batchLoss = 0.6406, diffLoss = 3.1413, kgLoss = 0.0154
2025-04-08 13:53:48.710484: Training Step 138/354: batchLoss = 0.6476, diffLoss = 3.1858, kgLoss = 0.0131
2025-04-08 13:53:50.328556: Training Step 139/354: batchLoss = 0.6117, diffLoss = 2.9975, kgLoss = 0.0153
2025-04-08 13:53:51.959889: Training Step 140/354: batchLoss = 0.8140, diffLoss = 4.0055, kgLoss = 0.0162
2025-04-08 13:53:53.570954: Training Step 141/354: batchLoss = 0.6576, diffLoss = 3.2304, kgLoss = 0.0144
2025-04-08 13:53:55.200977: Training Step 142/354: batchLoss = 0.6650, diffLoss = 3.2660, kgLoss = 0.0148
2025-04-08 13:53:56.836003: Training Step 143/354: batchLoss = 0.7465, diffLoss = 3.6650, kgLoss = 0.0169
2025-04-08 13:53:58.454233: Training Step 144/354: batchLoss = 0.5895, diffLoss = 2.8797, kgLoss = 0.0169
2025-04-08 13:54:00.083347: Training Step 145/354: batchLoss = 0.6690, diffLoss = 3.2813, kgLoss = 0.0160
2025-04-08 13:54:01.693691: Training Step 146/354: batchLoss = 0.4818, diffLoss = 2.3616, kgLoss = 0.0118
2025-04-08 13:54:03.316668: Training Step 147/354: batchLoss = 0.7251, diffLoss = 3.5621, kgLoss = 0.0158
2025-04-08 13:54:04.923998: Training Step 148/354: batchLoss = 0.5418, diffLoss = 2.6569, kgLoss = 0.0130
2025-04-08 13:54:06.543067: Training Step 149/354: batchLoss = 0.5122, diffLoss = 2.5083, kgLoss = 0.0131
2025-04-08 13:54:08.160058: Training Step 150/354: batchLoss = 0.5172, diffLoss = 2.5299, kgLoss = 0.0140
2025-04-08 13:54:09.776195: Training Step 151/354: batchLoss = 0.5235, diffLoss = 2.5717, kgLoss = 0.0115
2025-04-08 13:54:11.399178: Training Step 152/354: batchLoss = 0.5572, diffLoss = 2.7282, kgLoss = 0.0144
2025-04-08 13:54:13.019647: Training Step 153/354: batchLoss = 0.4991, diffLoss = 2.4460, kgLoss = 0.0124
2025-04-08 13:54:14.641255: Training Step 154/354: batchLoss = 0.6284, diffLoss = 3.0866, kgLoss = 0.0138
2025-04-08 13:54:16.248776: Training Step 155/354: batchLoss = 0.6691, diffLoss = 3.2857, kgLoss = 0.0149
2025-04-08 13:54:17.853780: Training Step 156/354: batchLoss = 0.7264, diffLoss = 3.5676, kgLoss = 0.0161
2025-04-08 13:54:19.462701: Training Step 157/354: batchLoss = 0.6909, diffLoss = 3.3851, kgLoss = 0.0173
2025-04-08 13:54:21.073162: Training Step 158/354: batchLoss = 0.7003, diffLoss = 3.4392, kgLoss = 0.0156
2025-04-08 13:54:22.706309: Training Step 159/354: batchLoss = 0.5817, diffLoss = 2.8564, kgLoss = 0.0131
2025-04-08 13:54:24.325807: Training Step 160/354: batchLoss = 0.5983, diffLoss = 2.9346, kgLoss = 0.0142
2025-04-08 13:54:25.945906: Training Step 161/354: batchLoss = 0.5943, diffLoss = 2.9115, kgLoss = 0.0150
2025-04-08 13:54:27.568885: Training Step 162/354: batchLoss = 0.6391, diffLoss = 3.1329, kgLoss = 0.0156
2025-04-08 13:54:29.200341: Training Step 163/354: batchLoss = 0.6358, diffLoss = 3.1244, kgLoss = 0.0136
2025-04-08 13:54:30.820245: Training Step 164/354: batchLoss = 0.5336, diffLoss = 2.6107, kgLoss = 0.0143
2025-04-08 13:54:32.421695: Training Step 165/354: batchLoss = 0.5787, diffLoss = 2.8342, kgLoss = 0.0149
2025-04-08 13:54:34.026738: Training Step 166/354: batchLoss = 0.5689, diffLoss = 2.7804, kgLoss = 0.0160
2025-04-08 13:54:35.635594: Training Step 167/354: batchLoss = 0.6070, diffLoss = 2.9757, kgLoss = 0.0148
2025-04-08 13:54:37.254554: Training Step 168/354: batchLoss = 0.5555, diffLoss = 2.7232, kgLoss = 0.0136
2025-04-08 13:54:38.879205: Training Step 169/354: batchLoss = 0.5478, diffLoss = 2.6838, kgLoss = 0.0138
2025-04-08 13:54:40.504333: Training Step 170/354: batchLoss = 0.6456, diffLoss = 3.1665, kgLoss = 0.0154
2025-04-08 13:54:42.135722: Training Step 171/354: batchLoss = 0.6236, diffLoss = 3.0519, kgLoss = 0.0165
2025-04-08 13:54:43.755787: Training Step 172/354: batchLoss = 0.5594, diffLoss = 2.7404, kgLoss = 0.0142
2025-04-08 13:54:45.384992: Training Step 173/354: batchLoss = 0.6354, diffLoss = 3.1158, kgLoss = 0.0153
2025-04-08 13:54:46.993554: Training Step 174/354: batchLoss = 0.6392, diffLoss = 3.1297, kgLoss = 0.0166
2025-04-08 13:54:48.601119: Training Step 175/354: batchLoss = 0.5567, diffLoss = 2.7290, kgLoss = 0.0136
2025-04-08 13:54:50.215293: Training Step 176/354: batchLoss = 0.6717, diffLoss = 3.2984, kgLoss = 0.0151
2025-04-08 13:54:51.829782: Training Step 177/354: batchLoss = 0.5665, diffLoss = 2.7814, kgLoss = 0.0127
2025-04-08 13:54:53.456926: Training Step 178/354: batchLoss = 0.8567, diffLoss = 4.2047, kgLoss = 0.0198
2025-04-08 13:54:55.075822: Training Step 179/354: batchLoss = 0.5328, diffLoss = 2.6134, kgLoss = 0.0127
2025-04-08 13:54:56.695880: Training Step 180/354: batchLoss = 0.6162, diffLoss = 3.0296, kgLoss = 0.0128
2025-04-08 13:54:58.314724: Training Step 181/354: batchLoss = 0.5510, diffLoss = 2.7042, kgLoss = 0.0127
2025-04-08 13:54:59.924751: Training Step 182/354: batchLoss = 0.5505, diffLoss = 2.7035, kgLoss = 0.0123
2025-04-08 13:55:01.531842: Training Step 183/354: batchLoss = 0.5146, diffLoss = 2.5107, kgLoss = 0.0156
2025-04-08 13:55:03.142926: Training Step 184/354: batchLoss = 0.5303, diffLoss = 2.6065, kgLoss = 0.0112
2025-04-08 13:55:04.751750: Training Step 185/354: batchLoss = 0.5548, diffLoss = 2.7260, kgLoss = 0.0120
2025-04-08 13:55:06.365501: Training Step 186/354: batchLoss = 0.5493, diffLoss = 2.6925, kgLoss = 0.0135
2025-04-08 13:55:07.985023: Training Step 187/354: batchLoss = 0.6456, diffLoss = 3.1495, kgLoss = 0.0196
2025-04-08 13:55:09.596423: Training Step 188/354: batchLoss = 0.5245, diffLoss = 2.5758, kgLoss = 0.0117
2025-04-08 13:55:11.220671: Training Step 189/354: batchLoss = 0.7241, diffLoss = 3.5612, kgLoss = 0.0148
2025-04-08 13:55:12.834839: Training Step 190/354: batchLoss = 0.6253, diffLoss = 3.0630, kgLoss = 0.0159
2025-04-08 13:55:14.451735: Training Step 191/354: batchLoss = 0.5519, diffLoss = 2.6994, kgLoss = 0.0150
2025-04-08 13:55:16.055153: Training Step 192/354: batchLoss = 0.6824, diffLoss = 3.3488, kgLoss = 0.0158
2025-04-08 13:55:17.658378: Training Step 193/354: batchLoss = 0.5358, diffLoss = 2.6282, kgLoss = 0.0128
2025-04-08 13:55:19.281606: Training Step 194/354: batchLoss = 0.7414, diffLoss = 3.6366, kgLoss = 0.0176
2025-04-08 13:55:20.894978: Training Step 195/354: batchLoss = 0.5984, diffLoss = 2.9372, kgLoss = 0.0136
2025-04-08 13:55:22.513043: Training Step 196/354: batchLoss = 0.7201, diffLoss = 3.5337, kgLoss = 0.0167
2025-04-08 13:55:24.130377: Training Step 197/354: batchLoss = 0.9543, diffLoss = 4.6922, kgLoss = 0.0198
2025-04-08 13:55:25.752303: Training Step 198/354: batchLoss = 0.6884, diffLoss = 3.3823, kgLoss = 0.0149
2025-04-08 13:55:27.391445: Training Step 199/354: batchLoss = 0.5340, diffLoss = 2.6219, kgLoss = 0.0120
2025-04-08 13:55:29.034697: Training Step 200/354: batchLoss = 0.5993, diffLoss = 2.9392, kgLoss = 0.0143
2025-04-08 13:55:30.662556: Training Step 201/354: batchLoss = 0.6003, diffLoss = 2.9410, kgLoss = 0.0151
2025-04-08 13:55:32.296694: Training Step 202/354: batchLoss = 0.5002, diffLoss = 2.4567, kgLoss = 0.0110
2025-04-08 13:55:33.919396: Training Step 203/354: batchLoss = 0.6830, diffLoss = 3.3509, kgLoss = 0.0160
2025-04-08 13:55:35.541951: Training Step 204/354: batchLoss = 0.7091, diffLoss = 3.4843, kgLoss = 0.0153
2025-04-08 13:55:37.184675: Training Step 205/354: batchLoss = 0.7768, diffLoss = 3.8126, kgLoss = 0.0179
2025-04-08 13:55:38.820826: Training Step 206/354: batchLoss = 0.6492, diffLoss = 3.1889, kgLoss = 0.0143
2025-04-08 13:55:40.457457: Training Step 207/354: batchLoss = 0.5955, diffLoss = 2.9265, kgLoss = 0.0127
2025-04-08 13:55:42.076662: Training Step 208/354: batchLoss = 0.6237, diffLoss = 3.0595, kgLoss = 0.0148
2025-04-08 13:55:43.699349: Training Step 209/354: batchLoss = 0.5468, diffLoss = 2.6806, kgLoss = 0.0133
2025-04-08 13:55:45.308442: Training Step 210/354: batchLoss = 0.6107, diffLoss = 2.9847, kgLoss = 0.0171
2025-04-08 13:55:46.923358: Training Step 211/354: batchLoss = 0.5827, diffLoss = 2.8592, kgLoss = 0.0136
2025-04-08 13:55:48.531266: Training Step 212/354: batchLoss = 0.6008, diffLoss = 2.9456, kgLoss = 0.0146
2025-04-08 13:55:50.137050: Training Step 213/354: batchLoss = 0.5366, diffLoss = 2.6306, kgLoss = 0.0131
2025-04-08 13:55:51.757553: Training Step 214/354: batchLoss = 0.5416, diffLoss = 2.6481, kgLoss = 0.0149
2025-04-08 13:55:53.389333: Training Step 215/354: batchLoss = 0.6643, diffLoss = 3.2564, kgLoss = 0.0162
2025-04-08 13:55:55.022634: Training Step 216/354: batchLoss = 0.7506, diffLoss = 3.6809, kgLoss = 0.0180
2025-04-08 13:55:56.637285: Training Step 217/354: batchLoss = 0.6126, diffLoss = 3.0121, kgLoss = 0.0127
2025-04-08 13:55:58.251672: Training Step 218/354: batchLoss = 0.6297, diffLoss = 3.0920, kgLoss = 0.0141
2025-04-08 13:55:59.879427: Training Step 219/354: batchLoss = 0.5793, diffLoss = 2.8384, kgLoss = 0.0145
2025-04-08 13:56:01.494409: Training Step 220/354: batchLoss = 0.6019, diffLoss = 2.9550, kgLoss = 0.0137
2025-04-08 13:56:03.107899: Training Step 221/354: batchLoss = 0.4935, diffLoss = 2.4167, kgLoss = 0.0127
2025-04-08 13:56:04.712167: Training Step 222/354: batchLoss = 0.6049, diffLoss = 2.9666, kgLoss = 0.0145
2025-04-08 13:56:06.326325: Training Step 223/354: batchLoss = 0.4770, diffLoss = 2.3341, kgLoss = 0.0127
2025-04-08 13:56:07.953198: Training Step 224/354: batchLoss = 0.8003, diffLoss = 3.9246, kgLoss = 0.0192
2025-04-08 13:56:09.572726: Training Step 225/354: batchLoss = 0.6962, diffLoss = 3.4210, kgLoss = 0.0150
2025-04-08 13:56:11.192079: Training Step 226/354: batchLoss = 0.4987, diffLoss = 2.4453, kgLoss = 0.0121
2025-04-08 13:56:12.806923: Training Step 227/354: batchLoss = 0.6411, diffLoss = 3.1338, kgLoss = 0.0180
2025-04-08 13:56:14.426067: Training Step 228/354: batchLoss = 0.6688, diffLoss = 3.2874, kgLoss = 0.0142
2025-04-08 13:56:16.031297: Training Step 229/354: batchLoss = 0.5268, diffLoss = 2.5862, kgLoss = 0.0119
2025-04-08 13:56:17.638093: Training Step 230/354: batchLoss = 0.5801, diffLoss = 2.8392, kgLoss = 0.0153
2025-04-08 13:56:19.248688: Training Step 231/354: batchLoss = 0.6018, diffLoss = 2.9503, kgLoss = 0.0147
2025-04-08 13:56:20.860598: Training Step 232/354: batchLoss = 0.5357, diffLoss = 2.6246, kgLoss = 0.0134
2025-04-08 13:56:22.477123: Training Step 233/354: batchLoss = 0.8158, diffLoss = 4.0085, kgLoss = 0.0176
2025-04-08 13:56:24.094951: Training Step 234/354: batchLoss = 0.7713, diffLoss = 3.7864, kgLoss = 0.0176
2025-04-08 13:56:25.712561: Training Step 235/354: batchLoss = 0.6946, diffLoss = 3.4090, kgLoss = 0.0160
2025-04-08 13:56:27.334163: Training Step 236/354: batchLoss = 0.6390, diffLoss = 3.1357, kgLoss = 0.0148
2025-04-08 13:56:28.953210: Training Step 237/354: batchLoss = 0.6198, diffLoss = 3.0394, kgLoss = 0.0149
2025-04-08 13:56:30.561912: Training Step 238/354: batchLoss = 0.5982, diffLoss = 2.9334, kgLoss = 0.0143
2025-04-08 13:56:32.159059: Training Step 239/354: batchLoss = 1.1537, diffLoss = 5.6722, kgLoss = 0.0241
2025-04-08 13:56:33.768426: Training Step 240/354: batchLoss = 0.7992, diffLoss = 3.9289, kgLoss = 0.0168
2025-04-08 13:56:35.391887: Training Step 241/354: batchLoss = 0.5222, diffLoss = 2.5585, kgLoss = 0.0132
2025-04-08 13:56:37.009884: Training Step 242/354: batchLoss = 0.5724, diffLoss = 2.8104, kgLoss = 0.0129
2025-04-08 13:56:38.633452: Training Step 243/354: batchLoss = 0.6417, diffLoss = 3.1524, kgLoss = 0.0140
2025-04-08 13:56:40.252271: Training Step 244/354: batchLoss = 0.6050, diffLoss = 2.9584, kgLoss = 0.0167
2025-04-08 13:56:41.866787: Training Step 245/354: batchLoss = 0.5076, diffLoss = 2.4860, kgLoss = 0.0130
2025-04-08 13:56:43.486876: Training Step 246/354: batchLoss = 0.5812, diffLoss = 2.8510, kgLoss = 0.0138
2025-04-08 13:56:45.096348: Training Step 247/354: batchLoss = 0.5962, diffLoss = 2.9182, kgLoss = 0.0157
2025-04-08 13:56:46.698756: Training Step 248/354: batchLoss = 0.6557, diffLoss = 3.2189, kgLoss = 0.0149
2025-04-08 13:56:48.304748: Training Step 249/354: batchLoss = 0.4942, diffLoss = 2.4269, kgLoss = 0.0110
2025-04-08 13:56:49.923206: Training Step 250/354: batchLoss = 0.6308, diffLoss = 3.1001, kgLoss = 0.0134
2025-04-08 13:56:51.547693: Training Step 251/354: batchLoss = 0.5709, diffLoss = 2.8010, kgLoss = 0.0133
2025-04-08 13:56:53.167026: Training Step 252/354: batchLoss = 0.6449, diffLoss = 3.1657, kgLoss = 0.0147
2025-04-08 13:56:54.788937: Training Step 253/354: batchLoss = 0.7410, diffLoss = 3.6402, kgLoss = 0.0162
2025-04-08 13:56:56.409875: Training Step 254/354: batchLoss = 0.5623, diffLoss = 2.7559, kgLoss = 0.0139
2025-04-08 13:56:58.031088: Training Step 255/354: batchLoss = 0.5623, diffLoss = 2.7569, kgLoss = 0.0137
2025-04-08 13:56:59.638529: Training Step 256/354: batchLoss = 0.5991, diffLoss = 2.9326, kgLoss = 0.0157
2025-04-08 13:57:01.247579: Training Step 257/354: batchLoss = 0.5367, diffLoss = 2.6312, kgLoss = 0.0130
2025-04-08 13:57:02.861720: Training Step 258/354: batchLoss = 0.5683, diffLoss = 2.7909, kgLoss = 0.0127
2025-04-08 13:57:04.471472: Training Step 259/354: batchLoss = 0.6148, diffLoss = 3.0180, kgLoss = 0.0141
2025-04-08 13:57:06.095928: Training Step 260/354: batchLoss = 0.5344, diffLoss = 2.6248, kgLoss = 0.0117
2025-04-08 13:57:07.724642: Training Step 261/354: batchLoss = 0.6511, diffLoss = 3.1933, kgLoss = 0.0155
2025-04-08 13:57:09.341398: Training Step 262/354: batchLoss = 0.5789, diffLoss = 2.8350, kgLoss = 0.0148
2025-04-08 13:57:10.957631: Training Step 263/354: batchLoss = 0.5964, diffLoss = 2.9221, kgLoss = 0.0150
2025-04-08 13:57:12.568959: Training Step 264/354: batchLoss = 0.7103, diffLoss = 3.4828, kgLoss = 0.0172
2025-04-08 13:57:14.179444: Training Step 265/354: batchLoss = 0.5187, diffLoss = 2.5443, kgLoss = 0.0123
2025-04-08 13:57:15.788280: Training Step 266/354: batchLoss = 0.5841, diffLoss = 2.8673, kgLoss = 0.0133
2025-04-08 13:57:17.401668: Training Step 267/354: batchLoss = 0.5022, diffLoss = 2.4637, kgLoss = 0.0118
2025-04-08 13:57:19.009935: Training Step 268/354: batchLoss = 0.6285, diffLoss = 3.0867, kgLoss = 0.0140
2025-04-08 13:57:20.632253: Training Step 269/354: batchLoss = 0.5576, diffLoss = 2.7346, kgLoss = 0.0134
2025-04-08 13:57:22.252710: Training Step 270/354: batchLoss = 0.6272, diffLoss = 3.0869, kgLoss = 0.0122
2025-04-08 13:57:23.883945: Training Step 271/354: batchLoss = 0.6577, diffLoss = 3.2264, kgLoss = 0.0155
2025-04-08 13:57:25.505657: Training Step 272/354: batchLoss = 0.6753, diffLoss = 3.3114, kgLoss = 0.0163
2025-04-08 13:57:27.120893: Training Step 273/354: batchLoss = 0.5650, diffLoss = 2.7735, kgLoss = 0.0129
2025-04-08 13:57:28.736489: Training Step 274/354: batchLoss = 0.5761, diffLoss = 2.8256, kgLoss = 0.0137
2025-04-08 13:57:30.351359: Training Step 275/354: batchLoss = 0.5612, diffLoss = 2.7520, kgLoss = 0.0135
2025-04-08 13:57:31.966953: Training Step 276/354: batchLoss = 0.6897, diffLoss = 3.3854, kgLoss = 0.0157
2025-04-08 13:57:33.576764: Training Step 277/354: batchLoss = 0.5904, diffLoss = 2.8998, kgLoss = 0.0130
2025-04-08 13:57:35.193340: Training Step 278/354: batchLoss = 0.6217, diffLoss = 3.0472, kgLoss = 0.0153
2025-04-08 13:57:36.816373: Training Step 279/354: batchLoss = 0.6319, diffLoss = 3.0996, kgLoss = 0.0149
2025-04-08 13:57:38.427861: Training Step 280/354: batchLoss = 0.8621, diffLoss = 4.2356, kgLoss = 0.0188
2025-04-08 13:57:40.055875: Training Step 281/354: batchLoss = 0.5335, diffLoss = 2.6171, kgLoss = 0.0127
2025-04-08 13:57:41.679691: Training Step 282/354: batchLoss = 0.5358, diffLoss = 2.6338, kgLoss = 0.0113
2025-04-08 13:57:43.297114: Training Step 283/354: batchLoss = 0.5694, diffLoss = 2.7852, kgLoss = 0.0154
2025-04-08 13:57:44.910395: Training Step 284/354: batchLoss = 0.5857, diffLoss = 2.8653, kgLoss = 0.0158
2025-04-08 13:57:46.518541: Training Step 285/354: batchLoss = 0.6703, diffLoss = 3.2795, kgLoss = 0.0180
2025-04-08 13:57:48.134358: Training Step 286/354: batchLoss = 0.6430, diffLoss = 3.1509, kgLoss = 0.0161
2025-04-08 13:57:49.745872: Training Step 287/354: batchLoss = 0.5676, diffLoss = 2.7834, kgLoss = 0.0136
2025-04-08 13:57:51.367290: Training Step 288/354: batchLoss = 0.8244, diffLoss = 4.0580, kgLoss = 0.0159
2025-04-08 13:57:52.983290: Training Step 289/354: batchLoss = 0.6045, diffLoss = 2.9658, kgLoss = 0.0142
2025-04-08 13:57:54.604030: Training Step 290/354: batchLoss = 0.4933, diffLoss = 2.4139, kgLoss = 0.0132
2025-04-08 13:57:56.226442: Training Step 291/354: batchLoss = 0.5029, diffLoss = 2.4678, kgLoss = 0.0116
2025-04-08 13:57:57.849142: Training Step 292/354: batchLoss = 0.6771, diffLoss = 3.3161, kgLoss = 0.0174
2025-04-08 13:57:59.461149: Training Step 293/354: batchLoss = 0.6770, diffLoss = 3.3230, kgLoss = 0.0155
2025-04-08 13:58:01.053105: Training Step 294/354: batchLoss = 0.6023, diffLoss = 2.9526, kgLoss = 0.0147
2025-04-08 13:58:02.655402: Training Step 295/354: batchLoss = 0.6470, diffLoss = 3.1740, kgLoss = 0.0153
2025-04-08 13:58:04.268888: Training Step 296/354: batchLoss = 0.5115, diffLoss = 2.5080, kgLoss = 0.0124
2025-04-08 13:58:05.884133: Training Step 297/354: batchLoss = 0.5265, diffLoss = 2.5804, kgLoss = 0.0130
2025-04-08 13:58:07.501169: Training Step 298/354: batchLoss = 0.5931, diffLoss = 2.9132, kgLoss = 0.0130
2025-04-08 13:58:09.123372: Training Step 299/354: batchLoss = 0.6277, diffLoss = 3.0830, kgLoss = 0.0139
2025-04-08 13:58:10.745622: Training Step 300/354: batchLoss = 0.7560, diffLoss = 3.7117, kgLoss = 0.0171
2025-04-08 13:58:12.363491: Training Step 301/354: batchLoss = 0.5802, diffLoss = 2.8431, kgLoss = 0.0145
2025-04-08 13:58:13.969394: Training Step 302/354: batchLoss = 0.7276, diffLoss = 3.5751, kgLoss = 0.0158
2025-04-08 13:58:15.580056: Training Step 303/354: batchLoss = 0.5762, diffLoss = 2.8289, kgLoss = 0.0130
2025-04-08 13:58:17.186664: Training Step 304/354: batchLoss = 0.6274, diffLoss = 3.0748, kgLoss = 0.0156
2025-04-08 13:58:18.799405: Training Step 305/354: batchLoss = 0.4863, diffLoss = 2.3834, kgLoss = 0.0121
2025-04-08 13:58:20.416370: Training Step 306/354: batchLoss = 0.6815, diffLoss = 3.3523, kgLoss = 0.0138
2025-04-08 13:58:22.032306: Training Step 307/354: batchLoss = 0.7174, diffLoss = 3.5168, kgLoss = 0.0176
2025-04-08 13:58:23.653100: Training Step 308/354: batchLoss = 0.5834, diffLoss = 2.8602, kgLoss = 0.0142
2025-04-08 13:58:25.278455: Training Step 309/354: batchLoss = 0.5229, diffLoss = 2.5661, kgLoss = 0.0120
2025-04-08 13:58:26.893352: Training Step 310/354: batchLoss = 0.6228, diffLoss = 3.0631, kgLoss = 0.0127
2025-04-08 13:58:28.497024: Training Step 311/354: batchLoss = 0.6566, diffLoss = 3.2182, kgLoss = 0.0162
2025-04-08 13:58:30.115387: Training Step 312/354: batchLoss = 0.6774, diffLoss = 3.3229, kgLoss = 0.0161
2025-04-08 13:58:31.729582: Training Step 313/354: batchLoss = 0.6940, diffLoss = 3.4015, kgLoss = 0.0171
2025-04-08 13:58:33.355401: Training Step 314/354: batchLoss = 0.5668, diffLoss = 2.7844, kgLoss = 0.0124
2025-04-08 13:58:34.972607: Training Step 315/354: batchLoss = 0.5680, diffLoss = 2.7845, kgLoss = 0.0139
2025-04-08 13:58:36.584723: Training Step 316/354: batchLoss = 0.8289, diffLoss = 4.0653, kgLoss = 0.0198
2025-04-08 13:58:38.204346: Training Step 317/354: batchLoss = 0.6264, diffLoss = 3.0741, kgLoss = 0.0145
2025-04-08 13:58:39.821236: Training Step 318/354: batchLoss = 0.5422, diffLoss = 2.6536, kgLoss = 0.0143
2025-04-08 13:58:41.455725: Training Step 319/354: batchLoss = 0.6262, diffLoss = 3.0760, kgLoss = 0.0138
2025-04-08 13:58:43.068493: Training Step 320/354: batchLoss = 0.6131, diffLoss = 3.0066, kgLoss = 0.0147
2025-04-08 13:58:44.684974: Training Step 321/354: batchLoss = 0.7180, diffLoss = 3.5158, kgLoss = 0.0185
2025-04-08 13:58:46.295755: Training Step 322/354: batchLoss = 0.6173, diffLoss = 3.0308, kgLoss = 0.0140
2025-04-08 13:58:47.909867: Training Step 323/354: batchLoss = 0.5734, diffLoss = 2.8167, kgLoss = 0.0126
2025-04-08 13:58:49.527776: Training Step 324/354: batchLoss = 0.6133, diffLoss = 3.0095, kgLoss = 0.0143
2025-04-08 13:58:51.154936: Training Step 325/354: batchLoss = 0.5388, diffLoss = 2.6389, kgLoss = 0.0138
2025-04-08 13:58:52.782281: Training Step 326/354: batchLoss = 0.5859, diffLoss = 2.8785, kgLoss = 0.0128
2025-04-08 13:58:54.396526: Training Step 327/354: batchLoss = 0.5020, diffLoss = 2.4564, kgLoss = 0.0134
2025-04-08 13:58:56.018202: Training Step 328/354: batchLoss = 0.7529, diffLoss = 3.7016, kgLoss = 0.0157
2025-04-08 13:58:57.632688: Training Step 329/354: batchLoss = 0.9429, diffLoss = 4.6343, kgLoss = 0.0201
2025-04-08 13:58:59.242803: Training Step 330/354: batchLoss = 0.4834, diffLoss = 2.3592, kgLoss = 0.0145
2025-04-08 13:59:00.863807: Training Step 331/354: batchLoss = 0.6009, diffLoss = 2.9486, kgLoss = 0.0140
2025-04-08 13:59:02.471004: Training Step 332/354: batchLoss = 0.7035, diffLoss = 3.4481, kgLoss = 0.0174
2025-04-08 13:59:04.092571: Training Step 333/354: batchLoss = 0.6712, diffLoss = 3.2992, kgLoss = 0.0142
2025-04-08 13:59:05.724237: Training Step 334/354: batchLoss = 0.6231, diffLoss = 3.0505, kgLoss = 0.0163
2025-04-08 13:59:07.334881: Training Step 335/354: batchLoss = 0.4589, diffLoss = 2.2436, kgLoss = 0.0127
2025-04-08 13:59:08.959330: Training Step 336/354: batchLoss = 0.7144, diffLoss = 3.5046, kgLoss = 0.0168
2025-04-08 13:59:10.584477: Training Step 337/354: batchLoss = 0.5994, diffLoss = 2.9392, kgLoss = 0.0145
2025-04-08 13:59:12.214101: Training Step 338/354: batchLoss = 0.5637, diffLoss = 2.7623, kgLoss = 0.0141
2025-04-08 13:59:13.837659: Training Step 339/354: batchLoss = 0.6849, diffLoss = 3.3564, kgLoss = 0.0170
2025-04-08 13:59:15.463599: Training Step 340/354: batchLoss = 0.5700, diffLoss = 2.7962, kgLoss = 0.0134
2025-04-08 13:59:17.081941: Training Step 341/354: batchLoss = 0.6823, diffLoss = 3.3488, kgLoss = 0.0157
2025-04-08 13:59:18.696812: Training Step 342/354: batchLoss = 0.6017, diffLoss = 2.9536, kgLoss = 0.0138
2025-04-08 13:59:20.317456: Training Step 343/354: batchLoss = 0.6952, diffLoss = 3.4161, kgLoss = 0.0149
2025-04-08 13:59:21.943988: Training Step 344/354: batchLoss = 0.5775, diffLoss = 2.8353, kgLoss = 0.0130
2025-04-08 13:59:23.565879: Training Step 345/354: batchLoss = 0.5982, diffLoss = 2.9347, kgLoss = 0.0141
2025-04-08 13:59:25.192520: Training Step 346/354: batchLoss = 0.5476, diffLoss = 2.6850, kgLoss = 0.0133
2025-04-08 13:59:26.802726: Training Step 347/354: batchLoss = 0.6914, diffLoss = 3.3895, kgLoss = 0.0169
2025-04-08 13:59:28.407933: Training Step 348/354: batchLoss = 0.7565, diffLoss = 3.7162, kgLoss = 0.0166
2025-04-08 13:59:30.019377: Training Step 349/354: batchLoss = 0.7215, diffLoss = 3.5448, kgLoss = 0.0157
2025-04-08 13:59:31.630356: Training Step 350/354: batchLoss = 0.5580, diffLoss = 2.7366, kgLoss = 0.0134
2025-04-08 13:59:33.243473: Training Step 351/354: batchLoss = 0.5541, diffLoss = 2.7196, kgLoss = 0.0127
2025-04-08 13:59:34.842551: Training Step 352/354: batchLoss = 0.7379, diffLoss = 3.6071, kgLoss = 0.0206
2025-04-08 13:59:36.251374: Training Step 353/354: batchLoss = 0.7504, diffLoss = 3.6866, kgLoss = 0.0163
2025-04-08 13:59:36.339162: 
2025-04-08 13:59:36.339855: Epoch 6/1000, Train: epLoss = 1.1063, epDfLoss = 5.4271, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 13:59:37.644955: Steps 0/138: batch_recall = 48.88, batch_ndcg = 26.76 
2025-04-08 13:59:38.962831: Steps 1/138: batch_recall = 45.18, batch_ndcg = 26.88 
2025-04-08 13:59:40.285218: Steps 2/138: batch_recall = 56.07, batch_ndcg = 33.71 
2025-04-08 13:59:41.590410: Steps 3/138: batch_recall = 53.91, batch_ndcg = 31.16 
2025-04-08 13:59:42.883610: Steps 4/138: batch_recall = 62.93, batch_ndcg = 39.99 
2025-04-08 13:59:44.172873: Steps 5/138: batch_recall = 56.49, batch_ndcg = 29.40 
2025-04-08 13:59:45.474062: Steps 6/138: batch_recall = 46.69, batch_ndcg = 27.39 
2025-04-08 13:59:46.770041: Steps 7/138: batch_recall = 59.40, batch_ndcg = 36.96 
2025-04-08 13:59:48.082931: Steps 8/138: batch_recall = 60.15, batch_ndcg = 37.68 
2025-04-08 13:59:49.379089: Steps 9/138: batch_recall = 51.34, batch_ndcg = 31.05 
2025-04-08 13:59:50.686106: Steps 10/138: batch_recall = 47.07, batch_ndcg = 27.38 
2025-04-08 13:59:51.982918: Steps 11/138: batch_recall = 61.59, batch_ndcg = 34.52 
2025-04-08 13:59:53.282230: Steps 12/138: batch_recall = 53.06, batch_ndcg = 29.86 
2025-04-08 13:59:54.582788: Steps 13/138: batch_recall = 49.18, batch_ndcg = 28.34 
2025-04-08 13:59:55.882156: Steps 14/138: batch_recall = 49.43, batch_ndcg = 28.30 
2025-04-08 13:59:57.168644: Steps 15/138: batch_recall = 49.88, batch_ndcg = 29.90 
2025-04-08 13:59:58.449977: Steps 16/138: batch_recall = 56.56, batch_ndcg = 30.63 
2025-04-08 13:59:59.716278: Steps 17/138: batch_recall = 56.89, batch_ndcg = 30.51 
2025-04-08 14:00:01.013399: Steps 18/138: batch_recall = 48.29, batch_ndcg = 30.37 
2025-04-08 14:00:02.312768: Steps 19/138: batch_recall = 49.42, batch_ndcg = 29.75 
2025-04-08 14:00:03.610346: Steps 20/138: batch_recall = 60.96, batch_ndcg = 33.29 
2025-04-08 14:00:04.958703: Steps 21/138: batch_recall = 60.81, batch_ndcg = 37.01 
2025-04-08 14:00:06.292586: Steps 22/138: batch_recall = 54.86, batch_ndcg = 31.64 
2025-04-08 14:00:07.623442: Steps 23/138: batch_recall = 49.14, batch_ndcg = 27.86 
2025-04-08 14:00:08.966444: Steps 24/138: batch_recall = 45.01, batch_ndcg = 26.73 
2025-04-08 14:00:10.293987: Steps 25/138: batch_recall = 53.72, batch_ndcg = 32.19 
2025-04-08 14:00:11.594764: Steps 26/138: batch_recall = 56.59, batch_ndcg = 31.97 
2025-04-08 14:00:12.893273: Steps 27/138: batch_recall = 62.32, batch_ndcg = 34.30 
2025-04-08 14:00:14.217640: Steps 28/138: batch_recall = 52.26, batch_ndcg = 30.37 
2025-04-08 14:00:15.528270: Steps 29/138: batch_recall = 56.92, batch_ndcg = 30.32 
2025-04-08 14:00:16.899745: Steps 30/138: batch_recall = 52.07, batch_ndcg = 33.06 
2025-04-08 14:00:18.244224: Steps 31/138: batch_recall = 38.19, batch_ndcg = 22.32 
2025-04-08 14:00:19.541040: Steps 32/138: batch_recall = 47.08, batch_ndcg = 28.95 
2025-04-08 14:00:20.828248: Steps 33/138: batch_recall = 55.73, batch_ndcg = 32.31 
2025-04-08 14:00:22.121866: Steps 34/138: batch_recall = 48.79, batch_ndcg = 27.04 
2025-04-08 14:00:23.422785: Steps 35/138: batch_recall = 47.40, batch_ndcg = 27.93 
2025-04-08 14:00:24.726098: Steps 36/138: batch_recall = 51.08, batch_ndcg = 28.42 
2025-04-08 14:00:25.995939: Steps 37/138: batch_recall = 61.05, batch_ndcg = 33.58 
2025-04-08 14:00:27.275038: Steps 38/138: batch_recall = 53.89, batch_ndcg = 30.73 
2025-04-08 14:00:28.550247: Steps 39/138: batch_recall = 73.29, batch_ndcg = 38.09 
2025-04-08 14:00:29.851797: Steps 40/138: batch_recall = 48.68, batch_ndcg = 26.88 
2025-04-08 14:00:31.132780: Steps 41/138: batch_recall = 58.05, batch_ndcg = 32.96 
2025-04-08 14:00:32.408070: Steps 42/138: batch_recall = 51.34, batch_ndcg = 29.71 
2025-04-08 14:00:33.694294: Steps 43/138: batch_recall = 52.88, batch_ndcg = 32.31 
2025-04-08 14:00:34.974934: Steps 44/138: batch_recall = 51.15, batch_ndcg = 27.64 
2025-04-08 14:00:36.270137: Steps 45/138: batch_recall = 59.06, batch_ndcg = 32.30 
2025-04-08 14:00:37.553005: Steps 46/138: batch_recall = 57.94, batch_ndcg = 33.39 
2025-04-08 14:00:38.842979: Steps 47/138: batch_recall = 57.81, batch_ndcg = 32.48 
2025-04-08 14:00:40.115338: Steps 48/138: batch_recall = 52.39, batch_ndcg = 31.21 
2025-04-08 14:00:41.389220: Steps 49/138: batch_recall = 60.38, batch_ndcg = 36.98 
2025-04-08 14:00:42.650451: Steps 50/138: batch_recall = 59.61, batch_ndcg = 32.35 
2025-04-08 14:00:43.932177: Steps 51/138: batch_recall = 61.72, batch_ndcg = 36.20 
2025-04-08 14:00:45.211292: Steps 52/138: batch_recall = 65.30, batch_ndcg = 39.65 
2025-04-08 14:00:46.510232: Steps 53/138: batch_recall = 59.86, batch_ndcg = 31.19 
2025-04-08 14:00:47.798011: Steps 54/138: batch_recall = 60.86, batch_ndcg = 34.11 
2025-04-08 14:00:49.081456: Steps 55/138: batch_recall = 55.56, batch_ndcg = 31.32 
2025-04-08 14:00:50.377941: Steps 56/138: batch_recall = 64.25, batch_ndcg = 34.62 
2025-04-08 14:00:51.672702: Steps 57/138: batch_recall = 59.78, batch_ndcg = 32.43 
2025-04-08 14:00:52.957132: Steps 58/138: batch_recall = 65.73, batch_ndcg = 33.80 
2025-04-08 14:00:54.233617: Steps 59/138: batch_recall = 74.02, batch_ndcg = 40.15 
2025-04-08 14:00:55.524930: Steps 60/138: batch_recall = 71.20, batch_ndcg = 39.17 
2025-04-08 14:00:56.808327: Steps 61/138: batch_recall = 61.57, batch_ndcg = 32.91 
2025-04-08 14:00:58.073877: Steps 62/138: batch_recall = 75.63, batch_ndcg = 41.53 
2025-04-08 14:00:59.350336: Steps 63/138: batch_recall = 74.26, batch_ndcg = 42.14 
2025-04-08 14:01:00.630264: Steps 64/138: batch_recall = 60.32, batch_ndcg = 32.79 
2025-04-08 14:01:01.919343: Steps 65/138: batch_recall = 81.29, batch_ndcg = 45.49 
2025-04-08 14:01:03.206501: Steps 66/138: batch_recall = 65.41, batch_ndcg = 38.98 
2025-04-08 14:01:04.504360: Steps 67/138: batch_recall = 75.81, batch_ndcg = 47.45 
2025-04-08 14:01:05.795293: Steps 68/138: batch_recall = 65.43, batch_ndcg = 34.23 
2025-04-08 14:01:07.070783: Steps 69/138: batch_recall = 83.24, batch_ndcg = 47.73 
2025-04-08 14:01:08.344906: Steps 70/138: batch_recall = 81.29, batch_ndcg = 45.39 
2025-04-08 14:01:09.614893: Steps 71/138: batch_recall = 83.44, batch_ndcg = 46.96 
2025-04-08 14:01:10.882244: Steps 72/138: batch_recall = 85.84, batch_ndcg = 49.12 
2025-04-08 14:01:12.153669: Steps 73/138: batch_recall = 88.96, batch_ndcg = 46.48 
2025-04-08 14:01:13.418501: Steps 74/138: batch_recall = 77.62, batch_ndcg = 46.90 
2025-04-08 14:01:14.688374: Steps 75/138: batch_recall = 83.78, batch_ndcg = 48.31 
2025-04-08 14:01:15.997461: Steps 76/138: batch_recall = 96.05, batch_ndcg = 55.76 
2025-04-08 14:01:17.284209: Steps 77/138: batch_recall = 81.30, batch_ndcg = 45.91 
2025-04-08 14:01:18.553804: Steps 78/138: batch_recall = 96.91, batch_ndcg = 50.56 
2025-04-08 14:01:19.823838: Steps 79/138: batch_recall = 87.92, batch_ndcg = 46.86 
2025-04-08 14:01:21.112764: Steps 80/138: batch_recall = 78.18, batch_ndcg = 41.00 
2025-04-08 14:01:22.386900: Steps 81/138: batch_recall = 82.54, batch_ndcg = 46.55 
2025-04-08 14:01:23.664173: Steps 82/138: batch_recall = 83.85, batch_ndcg = 47.89 
2025-04-08 14:01:24.937969: Steps 83/138: batch_recall = 80.61, batch_ndcg = 46.71 
2025-04-08 14:01:26.193150: Steps 84/138: batch_recall = 102.15, batch_ndcg = 55.96 
2025-04-08 14:01:27.451101: Steps 85/138: batch_recall = 95.32, batch_ndcg = 53.58 
2025-04-08 14:01:28.722599: Steps 86/138: batch_recall = 113.40, batch_ndcg = 68.92 
2025-04-08 14:01:30.005953: Steps 87/138: batch_recall = 103.37, batch_ndcg = 57.91 
2025-04-08 14:01:31.301722: Steps 88/138: batch_recall = 103.85, batch_ndcg = 57.29 
2025-04-08 14:01:32.590396: Steps 89/138: batch_recall = 118.19, batch_ndcg = 64.12 
2025-04-08 14:01:33.877996: Steps 90/138: batch_recall = 107.50, batch_ndcg = 57.36 
2025-04-08 14:01:35.171782: Steps 91/138: batch_recall = 119.76, batch_ndcg = 64.81 
2025-04-08 14:01:36.444485: Steps 92/138: batch_recall = 107.24, batch_ndcg = 58.89 
2025-04-08 14:01:37.708005: Steps 93/138: batch_recall = 113.05, batch_ndcg = 66.64 
2025-04-08 14:01:38.994508: Steps 94/138: batch_recall = 108.58, batch_ndcg = 58.70 
2025-04-08 14:01:40.247240: Steps 95/138: batch_recall = 111.66, batch_ndcg = 64.36 
2025-04-08 14:01:41.509844: Steps 96/138: batch_recall = 130.96, batch_ndcg = 72.12 
2025-04-08 14:01:42.794355: Steps 97/138: batch_recall = 132.60, batch_ndcg = 72.86 
2025-04-08 14:01:44.067203: Steps 98/138: batch_recall = 107.76, batch_ndcg = 61.84 
2025-04-08 14:01:45.347308: Steps 99/138: batch_recall = 115.57, batch_ndcg = 63.77 
2025-04-08 14:01:46.623505: Steps 100/138: batch_recall = 120.68, batch_ndcg = 68.76 
2025-04-08 14:01:47.896049: Steps 101/138: batch_recall = 119.69, batch_ndcg = 67.64 
2025-04-08 14:01:49.171062: Steps 102/138: batch_recall = 122.94, batch_ndcg = 70.78 
2025-04-08 14:01:50.459317: Steps 103/138: batch_recall = 134.35, batch_ndcg = 74.14 
2025-04-08 14:01:51.744620: Steps 104/138: batch_recall = 138.98, batch_ndcg = 73.13 
2025-04-08 14:01:53.013833: Steps 105/138: batch_recall = 118.44, batch_ndcg = 67.77 
2025-04-08 14:01:54.286643: Steps 106/138: batch_recall = 108.11, batch_ndcg = 58.20 
2025-04-08 14:01:55.550825: Steps 107/138: batch_recall = 114.51, batch_ndcg = 61.48 
2025-04-08 14:01:56.805297: Steps 108/138: batch_recall = 120.80, batch_ndcg = 70.23 
2025-04-08 14:01:58.066825: Steps 109/138: batch_recall = 137.96, batch_ndcg = 73.95 
2025-04-08 14:01:59.363337: Steps 110/138: batch_recall = 120.20, batch_ndcg = 63.56 
2025-04-08 14:02:00.624208: Steps 111/138: batch_recall = 136.46, batch_ndcg = 81.79 
2025-04-08 14:02:01.894751: Steps 112/138: batch_recall = 148.27, batch_ndcg = 79.64 
2025-04-08 14:02:03.166873: Steps 113/138: batch_recall = 121.85, batch_ndcg = 69.81 
2025-04-08 14:02:04.446507: Steps 114/138: batch_recall = 116.59, batch_ndcg = 65.81 
2025-04-08 14:02:05.733884: Steps 115/138: batch_recall = 113.47, batch_ndcg = 59.18 
2025-04-08 14:02:07.029796: Steps 116/138: batch_recall = 121.65, batch_ndcg = 65.72 
2025-04-08 14:02:08.302316: Steps 117/138: batch_recall = 114.28, batch_ndcg = 65.81 
2025-04-08 14:02:09.563069: Steps 118/138: batch_recall = 125.11, batch_ndcg = 70.04 
2025-04-08 14:02:10.845662: Steps 119/138: batch_recall = 136.19, batch_ndcg = 72.36 
2025-04-08 14:02:12.103476: Steps 120/138: batch_recall = 121.88, batch_ndcg = 66.62 
2025-04-08 14:02:13.377939: Steps 121/138: batch_recall = 144.40, batch_ndcg = 75.77 
2025-04-08 14:02:14.664773: Steps 122/138: batch_recall = 138.24, batch_ndcg = 75.25 
2025-04-08 14:02:15.973382: Steps 123/138: batch_recall = 127.18, batch_ndcg = 70.47 
2025-04-08 14:02:17.262857: Steps 124/138: batch_recall = 144.08, batch_ndcg = 91.25 
2025-04-08 14:02:18.544348: Steps 125/138: batch_recall = 128.75, batch_ndcg = 68.29 
2025-04-08 14:02:19.804809: Steps 126/138: batch_recall = 160.87, batch_ndcg = 89.91 
2025-04-08 14:02:21.070379: Steps 127/138: batch_recall = 138.20, batch_ndcg = 79.16 
2025-04-08 14:02:22.357676: Steps 128/138: batch_recall = 126.12, batch_ndcg = 67.41 
2025-04-08 14:02:23.608470: Steps 129/138: batch_recall = 149.79, batch_ndcg = 87.16 
2025-04-08 14:02:24.869126: Steps 130/138: batch_recall = 130.36, batch_ndcg = 68.23 
2025-04-08 14:02:26.134458: Steps 131/138: batch_recall = 137.45, batch_ndcg = 79.68 
2025-04-08 14:02:27.408298: Steps 132/138: batch_recall = 153.60, batch_ndcg = 85.34 
2025-04-08 14:02:28.666830: Steps 133/138: batch_recall = 141.85, batch_ndcg = 81.08 
2025-04-08 14:02:29.945665: Steps 134/138: batch_recall = 133.71, batch_ndcg = 74.55 
2025-04-08 14:02:31.218548: Steps 135/138: batch_recall = 162.03, batch_ndcg = 91.67 
2025-04-08 14:02:32.490408: Steps 136/138: batch_recall = 148.20, batch_ndcg = 78.41 
2025-04-08 14:02:33.761613: Steps 137/138: batch_recall = 135.15, batch_ndcg = 83.35 
2025-04-08 14:02:33.762104: Epoch 6/1000, Test: Recall = 0.1704, NDCG = 0.0958  

2025-04-08 14:02:35.522510: Training Step 0/354: batchLoss = 0.4926, diffLoss = 2.4177, kgLoss = 0.0113
2025-04-08 14:02:37.136841: Training Step 1/354: batchLoss = 0.5579, diffLoss = 2.7385, kgLoss = 0.0127
2025-04-08 14:02:38.751396: Training Step 2/354: batchLoss = 0.4642, diffLoss = 2.2731, kgLoss = 0.0119
2025-04-08 14:02:40.359658: Training Step 3/354: batchLoss = 0.7829, diffLoss = 3.8462, kgLoss = 0.0171
2025-04-08 14:02:41.972657: Training Step 4/354: batchLoss = 0.5690, diffLoss = 2.7751, kgLoss = 0.0175
2025-04-08 14:02:43.591331: Training Step 5/354: batchLoss = 0.5187, diffLoss = 2.5438, kgLoss = 0.0124
2025-04-08 14:02:45.210860: Training Step 6/354: batchLoss = 0.7759, diffLoss = 3.8134, kgLoss = 0.0165
2025-04-08 14:02:46.830660: Training Step 7/354: batchLoss = 0.5679, diffLoss = 2.7832, kgLoss = 0.0141
2025-04-08 14:02:48.449813: Training Step 8/354: batchLoss = 0.5232, diffLoss = 2.5670, kgLoss = 0.0122
2025-04-08 14:02:50.073542: Training Step 9/354: batchLoss = 0.7682, diffLoss = 3.7664, kgLoss = 0.0187
2025-04-08 14:02:51.690729: Training Step 10/354: batchLoss = 0.6055, diffLoss = 2.9721, kgLoss = 0.0138
2025-04-08 14:02:53.307222: Training Step 11/354: batchLoss = 0.6290, diffLoss = 3.0875, kgLoss = 0.0144
2025-04-08 14:02:54.917285: Training Step 12/354: batchLoss = 0.5352, diffLoss = 2.6248, kgLoss = 0.0128
2025-04-08 14:02:56.528729: Training Step 13/354: batchLoss = 0.5695, diffLoss = 2.7958, kgLoss = 0.0129
2025-04-08 14:02:58.148186: Training Step 14/354: batchLoss = 0.7027, diffLoss = 3.4509, kgLoss = 0.0156
2025-04-08 14:02:59.770493: Training Step 15/354: batchLoss = 0.6565, diffLoss = 3.2272, kgLoss = 0.0138
2025-04-08 14:03:01.386376: Training Step 16/354: batchLoss = 0.4810, diffLoss = 2.3617, kgLoss = 0.0109
2025-04-08 14:03:03.007480: Training Step 17/354: batchLoss = 0.6165, diffLoss = 3.0139, kgLoss = 0.0172
2025-04-08 14:03:04.630555: Training Step 18/354: batchLoss = 0.6397, diffLoss = 3.1398, kgLoss = 0.0147
2025-04-08 14:03:06.258984: Training Step 19/354: batchLoss = 0.6400, diffLoss = 3.1425, kgLoss = 0.0144
2025-04-08 14:03:07.871468: Training Step 20/354: batchLoss = 0.6353, diffLoss = 3.1174, kgLoss = 0.0148
2025-04-08 14:03:09.487462: Training Step 21/354: batchLoss = 0.7863, diffLoss = 3.8668, kgLoss = 0.0161
2025-04-08 14:03:11.099174: Training Step 22/354: batchLoss = 0.6353, diffLoss = 3.1188, kgLoss = 0.0144
2025-04-08 14:03:12.726431: Training Step 23/354: batchLoss = 0.6865, diffLoss = 3.3734, kgLoss = 0.0148
2025-04-08 14:03:14.358470: Training Step 24/354: batchLoss = 0.4849, diffLoss = 2.3726, kgLoss = 0.0130
2025-04-08 14:03:15.981774: Training Step 25/354: batchLoss = 0.4906, diffLoss = 2.4019, kgLoss = 0.0128
2025-04-08 14:03:17.599258: Training Step 26/354: batchLoss = 0.5101, diffLoss = 2.4965, kgLoss = 0.0135
2025-04-08 14:03:19.214476: Training Step 27/354: batchLoss = 0.6929, diffLoss = 3.4059, kgLoss = 0.0147
2025-04-08 14:03:20.843473: Training Step 28/354: batchLoss = 0.7259, diffLoss = 3.5635, kgLoss = 0.0165
2025-04-08 14:03:22.458994: Training Step 29/354: batchLoss = 0.5028, diffLoss = 2.4636, kgLoss = 0.0126
2025-04-08 14:03:24.074073: Training Step 30/354: batchLoss = 0.5166, diffLoss = 2.5337, kgLoss = 0.0124
2025-04-08 14:03:25.684782: Training Step 31/354: batchLoss = 0.6152, diffLoss = 3.0219, kgLoss = 0.0136
2025-04-08 14:03:27.312271: Training Step 32/354: batchLoss = 0.4839, diffLoss = 2.3681, kgLoss = 0.0128
2025-04-08 14:03:28.937169: Training Step 33/354: batchLoss = 0.5747, diffLoss = 2.8157, kgLoss = 0.0145
2025-04-08 14:03:30.556715: Training Step 34/354: batchLoss = 0.5867, diffLoss = 2.8787, kgLoss = 0.0137
2025-04-08 14:03:32.179886: Training Step 35/354: batchLoss = 0.6240, diffLoss = 3.0514, kgLoss = 0.0171
2025-04-08 14:03:33.807953: Training Step 36/354: batchLoss = 0.5792, diffLoss = 2.8401, kgLoss = 0.0139
2025-04-08 14:03:35.425201: Training Step 37/354: batchLoss = 0.7026, diffLoss = 3.4487, kgLoss = 0.0161
2025-04-08 14:03:37.048146: Training Step 38/354: batchLoss = 0.5030, diffLoss = 2.4657, kgLoss = 0.0123
2025-04-08 14:03:38.661440: Training Step 39/354: batchLoss = 0.8178, diffLoss = 4.0073, kgLoss = 0.0204
2025-04-08 14:03:40.270908: Training Step 40/354: batchLoss = 0.7155, diffLoss = 3.5126, kgLoss = 0.0163
2025-04-08 14:03:41.888081: Training Step 41/354: batchLoss = 0.6515, diffLoss = 3.2002, kgLoss = 0.0143
2025-04-08 14:03:43.514105: Training Step 42/354: batchLoss = 0.6109, diffLoss = 3.0029, kgLoss = 0.0129
2025-04-08 14:03:45.139543: Training Step 43/354: batchLoss = 0.5922, diffLoss = 2.8975, kgLoss = 0.0158
2025-04-08 14:03:46.762279: Training Step 44/354: batchLoss = 0.6735, diffLoss = 3.3056, kgLoss = 0.0154
2025-04-08 14:03:48.383263: Training Step 45/354: batchLoss = 0.5643, diffLoss = 2.7683, kgLoss = 0.0133
2025-04-08 14:03:50.005804: Training Step 46/354: batchLoss = 0.6736, diffLoss = 3.3128, kgLoss = 0.0138
2025-04-08 14:03:51.638258: Training Step 47/354: batchLoss = 0.6587, diffLoss = 3.2360, kgLoss = 0.0144
2025-04-08 14:03:53.245786: Training Step 48/354: batchLoss = 0.5183, diffLoss = 2.5380, kgLoss = 0.0134
2025-04-08 14:03:54.852387: Training Step 49/354: batchLoss = 0.5103, diffLoss = 2.5040, kgLoss = 0.0118
2025-04-08 14:03:56.463229: Training Step 50/354: batchLoss = 0.6436, diffLoss = 3.1572, kgLoss = 0.0152
2025-04-08 14:03:58.077828: Training Step 51/354: batchLoss = 0.5297, diffLoss = 2.6000, kgLoss = 0.0121
2025-04-08 14:03:59.706073: Training Step 52/354: batchLoss = 0.6817, diffLoss = 3.3472, kgLoss = 0.0153
2025-04-08 14:04:01.323828: Training Step 53/354: batchLoss = 0.6715, diffLoss = 3.2979, kgLoss = 0.0149
2025-04-08 14:04:02.939399: Training Step 54/354: batchLoss = 0.5693, diffLoss = 2.7908, kgLoss = 0.0139
2025-04-08 14:04:04.559988: Training Step 55/354: batchLoss = 0.6276, diffLoss = 3.0781, kgLoss = 0.0150
2025-04-08 14:04:06.177128: Training Step 56/354: batchLoss = 0.5852, diffLoss = 2.8718, kgLoss = 0.0135
2025-04-08 14:04:07.784987: Training Step 57/354: batchLoss = 0.8466, diffLoss = 4.1608, kgLoss = 0.0181
2025-04-08 14:04:09.396354: Training Step 58/354: batchLoss = 0.6405, diffLoss = 3.1465, kgLoss = 0.0140
2025-04-08 14:04:11.006241: Training Step 59/354: batchLoss = 0.6182, diffLoss = 3.0300, kgLoss = 0.0153
2025-04-08 14:04:12.619819: Training Step 60/354: batchLoss = 0.4782, diffLoss = 2.3500, kgLoss = 0.0102
2025-04-08 14:04:14.233360: Training Step 61/354: batchLoss = 0.5063, diffLoss = 2.4850, kgLoss = 0.0116
2025-04-08 14:04:15.853382: Training Step 62/354: batchLoss = 0.6400, diffLoss = 3.1439, kgLoss = 0.0141
2025-04-08 14:04:17.473810: Training Step 63/354: batchLoss = 0.5518, diffLoss = 2.7107, kgLoss = 0.0121
2025-04-08 14:04:19.085831: Training Step 64/354: batchLoss = 0.6419, diffLoss = 3.1520, kgLoss = 0.0144
2025-04-08 14:04:20.698809: Training Step 65/354: batchLoss = 0.5571, diffLoss = 2.7332, kgLoss = 0.0131
2025-04-08 14:04:22.305461: Training Step 66/354: batchLoss = 0.5892, diffLoss = 2.8879, kgLoss = 0.0145
2025-04-08 14:04:23.917380: Training Step 67/354: batchLoss = 0.7946, diffLoss = 3.8981, kgLoss = 0.0187
2025-04-08 14:04:25.540841: Training Step 68/354: batchLoss = 0.5015, diffLoss = 2.4559, kgLoss = 0.0129
2025-04-08 14:04:27.161720: Training Step 69/354: batchLoss = 0.5723, diffLoss = 2.8062, kgLoss = 0.0139
2025-04-08 14:04:28.777228: Training Step 70/354: batchLoss = 0.6329, diffLoss = 3.1068, kgLoss = 0.0144
2025-04-08 14:04:30.393676: Training Step 71/354: batchLoss = 0.6047, diffLoss = 2.9677, kgLoss = 0.0139
2025-04-08 14:04:32.013378: Training Step 72/354: batchLoss = 0.7890, diffLoss = 3.8723, kgLoss = 0.0182
2025-04-08 14:04:33.624975: Training Step 73/354: batchLoss = 0.5443, diffLoss = 2.6696, kgLoss = 0.0130
2025-04-08 14:04:35.250931: Training Step 74/354: batchLoss = 0.5975, diffLoss = 2.9224, kgLoss = 0.0163
2025-04-08 14:04:36.877808: Training Step 75/354: batchLoss = 0.6387, diffLoss = 3.1310, kgLoss = 0.0156
2025-04-08 14:04:38.507990: Training Step 76/354: batchLoss = 0.6438, diffLoss = 3.1582, kgLoss = 0.0152
2025-04-08 14:04:40.142508: Training Step 77/354: batchLoss = 0.5651, diffLoss = 2.7756, kgLoss = 0.0125
2025-04-08 14:04:41.786726: Training Step 78/354: batchLoss = 0.6120, diffLoss = 3.0078, kgLoss = 0.0131
2025-04-08 14:04:43.416989: Training Step 79/354: batchLoss = 0.6802, diffLoss = 3.3337, kgLoss = 0.0168
2025-04-08 14:04:45.048615: Training Step 80/354: batchLoss = 0.5474, diffLoss = 2.6881, kgLoss = 0.0123
2025-04-08 14:04:46.688172: Training Step 81/354: batchLoss = 0.5966, diffLoss = 2.9252, kgLoss = 0.0144
2025-04-08 14:04:48.352830: Training Step 82/354: batchLoss = 0.7284, diffLoss = 3.5817, kgLoss = 0.0151
2025-04-08 14:04:50.000597: Training Step 83/354: batchLoss = 0.5666, diffLoss = 2.7767, kgLoss = 0.0141
2025-04-08 14:04:51.610238: Training Step 84/354: batchLoss = 0.6248, diffLoss = 3.0599, kgLoss = 0.0160
2025-04-08 14:04:53.223966: Training Step 85/354: batchLoss = 0.6204, diffLoss = 3.0422, kgLoss = 0.0150
2025-04-08 14:04:54.830465: Training Step 86/354: batchLoss = 0.5175, diffLoss = 2.5396, kgLoss = 0.0119
2025-04-08 14:04:56.445196: Training Step 87/354: batchLoss = 0.6197, diffLoss = 3.0414, kgLoss = 0.0142
2025-04-08 14:04:58.059727: Training Step 88/354: batchLoss = 0.7037, diffLoss = 3.4565, kgLoss = 0.0155
2025-04-08 14:04:59.677380: Training Step 89/354: batchLoss = 0.5834, diffLoss = 2.8578, kgLoss = 0.0148
2025-04-08 14:05:01.303236: Training Step 90/354: batchLoss = 0.5101, diffLoss = 2.5023, kgLoss = 0.0121
2025-04-08 14:05:02.921794: Training Step 91/354: batchLoss = 0.5114, diffLoss = 2.5110, kgLoss = 0.0115
2025-04-08 14:05:04.547633: Training Step 92/354: batchLoss = 0.8429, diffLoss = 4.1376, kgLoss = 0.0193
2025-04-08 14:05:06.159951: Training Step 93/354: batchLoss = 0.5424, diffLoss = 2.6605, kgLoss = 0.0128
2025-04-08 14:05:07.766147: Training Step 94/354: batchLoss = 0.6374, diffLoss = 3.1234, kgLoss = 0.0159
2025-04-08 14:05:09.391300: Training Step 95/354: batchLoss = 0.6135, diffLoss = 3.0037, kgLoss = 0.0159
2025-04-08 14:05:11.014646: Training Step 96/354: batchLoss = 0.5467, diffLoss = 2.6744, kgLoss = 0.0148
2025-04-08 14:05:12.637657: Training Step 97/354: batchLoss = 0.6171, diffLoss = 3.0268, kgLoss = 0.0146
2025-04-08 14:05:14.261237: Training Step 98/354: batchLoss = 0.6615, diffLoss = 3.2381, kgLoss = 0.0174
2025-04-08 14:05:15.883331: Training Step 99/354: batchLoss = 0.6280, diffLoss = 3.0818, kgLoss = 0.0145
2025-04-08 14:05:17.512375: Training Step 100/354: batchLoss = 0.6568, diffLoss = 3.2275, kgLoss = 0.0141
2025-04-08 14:05:19.144164: Training Step 101/354: batchLoss = 0.5961, diffLoss = 2.9225, kgLoss = 0.0145
2025-04-08 14:05:20.761520: Training Step 102/354: batchLoss = 0.7146, diffLoss = 3.5093, kgLoss = 0.0159
2025-04-08 14:05:22.379454: Training Step 103/354: batchLoss = 0.5438, diffLoss = 2.6698, kgLoss = 0.0123
2025-04-08 14:05:23.988291: Training Step 104/354: batchLoss = 0.6489, diffLoss = 3.1811, kgLoss = 0.0158
2025-04-08 14:05:25.624266: Training Step 105/354: batchLoss = 0.7291, diffLoss = 3.5781, kgLoss = 0.0169
2025-04-08 14:05:27.241448: Training Step 106/354: batchLoss = 0.7268, diffLoss = 3.5672, kgLoss = 0.0166
2025-04-08 14:05:28.867324: Training Step 107/354: batchLoss = 0.8610, diffLoss = 4.2227, kgLoss = 0.0206
2025-04-08 14:05:30.486504: Training Step 108/354: batchLoss = 0.5808, diffLoss = 2.8551, kgLoss = 0.0122
2025-04-08 14:05:32.101998: Training Step 109/354: batchLoss = 0.5595, diffLoss = 2.7399, kgLoss = 0.0144
2025-04-08 14:05:33.725212: Training Step 110/354: batchLoss = 0.4490, diffLoss = 2.1945, kgLoss = 0.0127
2025-04-08 14:05:35.342531: Training Step 111/354: batchLoss = 0.6132, diffLoss = 3.0065, kgLoss = 0.0149
2025-04-08 14:05:36.959336: Training Step 112/354: batchLoss = 0.6536, diffLoss = 3.2062, kgLoss = 0.0155
2025-04-08 14:05:38.577003: Training Step 113/354: batchLoss = 0.5410, diffLoss = 2.6475, kgLoss = 0.0143
2025-04-08 14:05:40.195102: Training Step 114/354: batchLoss = 0.5750, diffLoss = 2.8134, kgLoss = 0.0153
2025-04-08 14:05:41.812211: Training Step 115/354: batchLoss = 0.5135, diffLoss = 2.5170, kgLoss = 0.0126
2025-04-08 14:05:43.439803: Training Step 116/354: batchLoss = 0.6795, diffLoss = 3.3294, kgLoss = 0.0170
2025-04-08 14:05:45.077852: Training Step 117/354: batchLoss = 1.0158, diffLoss = 4.9973, kgLoss = 0.0204
2025-04-08 14:05:46.706076: Training Step 118/354: batchLoss = 0.4866, diffLoss = 2.3832, kgLoss = 0.0125
2025-04-08 14:05:48.337945: Training Step 119/354: batchLoss = 0.5903, diffLoss = 2.8964, kgLoss = 0.0138
2025-04-08 14:05:49.951998: Training Step 120/354: batchLoss = 0.6376, diffLoss = 3.1271, kgLoss = 0.0152
2025-04-08 14:05:51.563628: Training Step 121/354: batchLoss = 1.2277, diffLoss = 6.0191, kgLoss = 0.0298
2025-04-08 14:05:53.172093: Training Step 122/354: batchLoss = 0.5695, diffLoss = 2.7966, kgLoss = 0.0128
2025-04-08 14:05:54.784134: Training Step 123/354: batchLoss = 0.7350, diffLoss = 3.6070, kgLoss = 0.0170
2025-04-08 14:05:56.399288: Training Step 124/354: batchLoss = 0.6294, diffLoss = 3.0880, kgLoss = 0.0147
2025-04-08 14:05:58.017828: Training Step 125/354: batchLoss = 0.5367, diffLoss = 2.6260, kgLoss = 0.0143
2025-04-08 14:05:59.639491: Training Step 126/354: batchLoss = 0.5999, diffLoss = 2.9358, kgLoss = 0.0159
2025-04-08 14:06:01.269670: Training Step 127/354: batchLoss = 0.6705, diffLoss = 3.2879, kgLoss = 0.0162
2025-04-08 14:06:02.903869: Training Step 128/354: batchLoss = 0.5250, diffLoss = 2.5756, kgLoss = 0.0123
2025-04-08 14:06:04.531197: Training Step 129/354: batchLoss = 0.7255, diffLoss = 3.5675, kgLoss = 0.0150
2025-04-08 14:06:06.140481: Training Step 130/354: batchLoss = 0.5541, diffLoss = 2.7074, kgLoss = 0.0158
2025-04-08 14:06:07.747099: Training Step 131/354: batchLoss = 0.4407, diffLoss = 2.1584, kgLoss = 0.0113
2025-04-08 14:06:09.364428: Training Step 132/354: batchLoss = 0.7201, diffLoss = 3.5377, kgLoss = 0.0157
2025-04-08 14:06:10.976647: Training Step 133/354: batchLoss = 0.7477, diffLoss = 3.6740, kgLoss = 0.0161
2025-04-08 14:06:12.591609: Training Step 134/354: batchLoss = 0.6688, diffLoss = 3.2807, kgLoss = 0.0158
2025-04-08 14:06:14.220091: Training Step 135/354: batchLoss = 0.5795, diffLoss = 2.8442, kgLoss = 0.0133
2025-04-08 14:06:15.850335: Training Step 136/354: batchLoss = 0.7243, diffLoss = 3.5508, kgLoss = 0.0176
2025-04-08 14:06:17.464020: Training Step 137/354: batchLoss = 0.6341, diffLoss = 3.1072, kgLoss = 0.0158
2025-04-08 14:06:19.081432: Training Step 138/354: batchLoss = 0.5064, diffLoss = 2.4813, kgLoss = 0.0127
2025-04-08 14:06:20.693535: Training Step 139/354: batchLoss = 0.6042, diffLoss = 2.9591, kgLoss = 0.0154
2025-04-08 14:06:22.312611: Training Step 140/354: batchLoss = 0.5843, diffLoss = 2.8643, kgLoss = 0.0143
2025-04-08 14:06:23.938189: Training Step 141/354: batchLoss = 0.6209, diffLoss = 3.0402, kgLoss = 0.0160
2025-04-08 14:06:25.560501: Training Step 142/354: batchLoss = 0.5596, diffLoss = 2.7463, kgLoss = 0.0129
2025-04-08 14:06:27.185918: Training Step 143/354: batchLoss = 0.5261, diffLoss = 2.5832, kgLoss = 0.0119
2025-04-08 14:06:28.806476: Training Step 144/354: batchLoss = 0.5172, diffLoss = 2.5258, kgLoss = 0.0151
2025-04-08 14:06:30.424753: Training Step 145/354: batchLoss = 0.5398, diffLoss = 2.6485, kgLoss = 0.0126
2025-04-08 14:06:32.043278: Training Step 146/354: batchLoss = 0.6587, diffLoss = 3.2325, kgLoss = 0.0153
2025-04-08 14:06:33.655228: Training Step 147/354: batchLoss = 0.5726, diffLoss = 2.8020, kgLoss = 0.0152
2025-04-08 14:06:35.265960: Training Step 148/354: batchLoss = 0.6062, diffLoss = 2.9697, kgLoss = 0.0153
2025-04-08 14:06:36.870912: Training Step 149/354: batchLoss = 0.6122, diffLoss = 3.0049, kgLoss = 0.0140
2025-04-08 14:06:38.486300: Training Step 150/354: batchLoss = 0.4966, diffLoss = 2.4400, kgLoss = 0.0108
2025-04-08 14:06:40.113122: Training Step 151/354: batchLoss = 0.6142, diffLoss = 3.0091, kgLoss = 0.0154
2025-04-08 14:06:41.750278: Training Step 152/354: batchLoss = 0.6038, diffLoss = 2.9550, kgLoss = 0.0160
2025-04-08 14:06:43.369999: Training Step 153/354: batchLoss = 0.6996, diffLoss = 3.4339, kgLoss = 0.0161
2025-04-08 14:06:44.992736: Training Step 154/354: batchLoss = 0.5486, diffLoss = 2.6892, kgLoss = 0.0134
2025-04-08 14:06:46.610024: Training Step 155/354: batchLoss = 0.6389, diffLoss = 3.1358, kgLoss = 0.0146
2025-04-08 14:06:48.226836: Training Step 156/354: batchLoss = 0.4960, diffLoss = 2.4259, kgLoss = 0.0135
2025-04-08 14:06:49.838406: Training Step 157/354: batchLoss = 0.5791, diffLoss = 2.8393, kgLoss = 0.0140
2025-04-08 14:06:51.451013: Training Step 158/354: batchLoss = 0.6609, diffLoss = 3.2493, kgLoss = 0.0138
2025-04-08 14:06:53.069920: Training Step 159/354: batchLoss = 0.6812, diffLoss = 3.3408, kgLoss = 0.0163
2025-04-08 14:06:54.702876: Training Step 160/354: batchLoss = 0.6754, diffLoss = 3.3173, kgLoss = 0.0149
2025-04-08 14:06:56.324231: Training Step 161/354: batchLoss = 0.6082, diffLoss = 2.9835, kgLoss = 0.0143
2025-04-08 14:06:57.954274: Training Step 162/354: batchLoss = 0.4907, diffLoss = 2.4096, kgLoss = 0.0110
2025-04-08 14:06:59.567982: Training Step 163/354: batchLoss = 0.5680, diffLoss = 2.7780, kgLoss = 0.0156
2025-04-08 14:07:01.196414: Training Step 164/354: batchLoss = 0.6224, diffLoss = 3.0446, kgLoss = 0.0169
2025-04-08 14:07:02.823219: Training Step 165/354: batchLoss = 0.7515, diffLoss = 3.6868, kgLoss = 0.0177
2025-04-08 14:07:04.436192: Training Step 166/354: batchLoss = 0.6895, diffLoss = 3.3777, kgLoss = 0.0175
2025-04-08 14:07:06.048728: Training Step 167/354: batchLoss = 0.5362, diffLoss = 2.6368, kgLoss = 0.0111
2025-04-08 14:07:07.683644: Training Step 168/354: batchLoss = 0.4498, diffLoss = 2.2001, kgLoss = 0.0122
2025-04-08 14:07:09.308913: Training Step 169/354: batchLoss = 0.7341, diffLoss = 3.6001, kgLoss = 0.0176
2025-04-08 14:07:10.939988: Training Step 170/354: batchLoss = 0.8556, diffLoss = 4.2025, kgLoss = 0.0189
2025-04-08 14:07:12.567633: Training Step 171/354: batchLoss = 0.5150, diffLoss = 2.5238, kgLoss = 0.0128
2025-04-08 14:07:14.191697: Training Step 172/354: batchLoss = 0.6320, diffLoss = 3.0999, kgLoss = 0.0151
2025-04-08 14:07:15.811530: Training Step 173/354: batchLoss = 0.6602, diffLoss = 3.2389, kgLoss = 0.0155
2025-04-08 14:07:17.436688: Training Step 174/354: batchLoss = 0.6350, diffLoss = 3.1198, kgLoss = 0.0138
2025-04-08 14:07:19.046568: Training Step 175/354: batchLoss = 0.6368, diffLoss = 3.1247, kgLoss = 0.0148
2025-04-08 14:07:20.661189: Training Step 176/354: batchLoss = 0.5603, diffLoss = 2.7481, kgLoss = 0.0133
2025-04-08 14:07:22.283751: Training Step 177/354: batchLoss = 0.5401, diffLoss = 2.6487, kgLoss = 0.0129
2025-04-08 14:07:23.895377: Training Step 178/354: batchLoss = 0.6495, diffLoss = 3.1839, kgLoss = 0.0159
2025-04-08 14:07:25.522754: Training Step 179/354: batchLoss = 0.6106, diffLoss = 2.9955, kgLoss = 0.0143
2025-04-08 14:07:27.146766: Training Step 180/354: batchLoss = 0.6721, diffLoss = 3.2986, kgLoss = 0.0155
2025-04-08 14:07:28.768513: Training Step 181/354: batchLoss = 0.5543, diffLoss = 2.7197, kgLoss = 0.0130
2025-04-08 14:07:30.386327: Training Step 182/354: batchLoss = 0.6054, diffLoss = 2.9784, kgLoss = 0.0121
2025-04-08 14:07:32.000349: Training Step 183/354: batchLoss = 0.6337, diffLoss = 2.9997, kgLoss = 0.0423
2025-04-08 14:07:33.609644: Training Step 184/354: batchLoss = 0.5579, diffLoss = 2.7349, kgLoss = 0.0136
2025-04-08 14:07:35.219607: Training Step 185/354: batchLoss = 0.6116, diffLoss = 3.0020, kgLoss = 0.0141
2025-04-08 14:07:36.836234: Training Step 186/354: batchLoss = 0.7001, diffLoss = 3.4329, kgLoss = 0.0169
2025-04-08 14:07:38.458708: Training Step 187/354: batchLoss = 0.6312, diffLoss = 3.0976, kgLoss = 0.0146
2025-04-08 14:07:40.077030: Training Step 188/354: batchLoss = 0.5447, diffLoss = 2.6693, kgLoss = 0.0136
2025-04-08 14:07:41.705273: Training Step 189/354: batchLoss = 0.7128, diffLoss = 3.5005, kgLoss = 0.0159
2025-04-08 14:07:43.322397: Training Step 190/354: batchLoss = 0.6225, diffLoss = 3.0490, kgLoss = 0.0159
2025-04-08 14:07:44.962405: Training Step 191/354: batchLoss = 0.5072, diffLoss = 2.4881, kgLoss = 0.0120
2025-04-08 14:07:46.591042: Training Step 192/354: batchLoss = 0.6230, diffLoss = 3.0590, kgLoss = 0.0139
2025-04-08 14:07:48.203652: Training Step 193/354: batchLoss = 0.6736, diffLoss = 3.3059, kgLoss = 0.0155
2025-04-08 14:07:49.812504: Training Step 194/354: batchLoss = 0.5883, diffLoss = 2.8940, kgLoss = 0.0118
2025-04-08 14:07:51.425009: Training Step 195/354: batchLoss = 0.6954, diffLoss = 3.4128, kgLoss = 0.0160
2025-04-08 14:07:53.067674: Training Step 196/354: batchLoss = 0.6494, diffLoss = 3.1884, kgLoss = 0.0147
2025-04-08 14:07:54.689823: Training Step 197/354: batchLoss = 0.4556, diffLoss = 2.2348, kgLoss = 0.0107
2025-04-08 14:07:56.300957: Training Step 198/354: batchLoss = 0.7643, diffLoss = 3.7572, kgLoss = 0.0161
2025-04-08 14:07:57.912737: Training Step 199/354: batchLoss = 0.5588, diffLoss = 2.7405, kgLoss = 0.0134
2025-04-08 14:07:59.534430: Training Step 200/354: batchLoss = 0.6519, diffLoss = 3.1878, kgLoss = 0.0179
2025-04-08 14:08:01.170815: Training Step 201/354: batchLoss = 0.5779, diffLoss = 2.8393, kgLoss = 0.0125
2025-04-08 14:08:02.790190: Training Step 202/354: batchLoss = 0.5743, diffLoss = 2.8175, kgLoss = 0.0135
2025-04-08 14:08:04.393874: Training Step 203/354: batchLoss = 0.5042, diffLoss = 2.4660, kgLoss = 0.0138
2025-04-08 14:08:06.011711: Training Step 204/354: batchLoss = 0.6871, diffLoss = 3.3679, kgLoss = 0.0169
2025-04-08 14:08:07.639844: Training Step 205/354: batchLoss = 0.5567, diffLoss = 2.7319, kgLoss = 0.0129
2025-04-08 14:08:09.266613: Training Step 206/354: batchLoss = 0.5254, diffLoss = 2.5774, kgLoss = 0.0125
2025-04-08 14:08:10.887233: Training Step 207/354: batchLoss = 0.7137, diffLoss = 3.4978, kgLoss = 0.0177
2025-04-08 14:08:12.506389: Training Step 208/354: batchLoss = 0.6022, diffLoss = 2.9611, kgLoss = 0.0125
2025-04-08 14:08:14.126038: Training Step 209/354: batchLoss = 0.7311, diffLoss = 3.5940, kgLoss = 0.0154
2025-04-08 14:08:15.744612: Training Step 210/354: batchLoss = 0.7598, diffLoss = 3.7263, kgLoss = 0.0182
2025-04-08 14:08:17.356807: Training Step 211/354: batchLoss = 0.5279, diffLoss = 2.5864, kgLoss = 0.0133
2025-04-08 14:08:18.966519: Training Step 212/354: batchLoss = 0.5769, diffLoss = 2.8265, kgLoss = 0.0145
2025-04-08 14:08:20.577092: Training Step 213/354: batchLoss = 0.4995, diffLoss = 2.4479, kgLoss = 0.0124
2025-04-08 14:08:22.202482: Training Step 214/354: batchLoss = 0.7253, diffLoss = 3.5609, kgLoss = 0.0164
2025-04-08 14:08:23.819560: Training Step 215/354: batchLoss = 0.6152, diffLoss = 3.0177, kgLoss = 0.0146
2025-04-08 14:08:25.445066: Training Step 216/354: batchLoss = 0.5193, diffLoss = 2.5427, kgLoss = 0.0134
2025-04-08 14:08:27.068521: Training Step 217/354: batchLoss = 0.6563, diffLoss = 3.2205, kgLoss = 0.0152
2025-04-08 14:08:28.693311: Training Step 218/354: batchLoss = 0.6038, diffLoss = 2.9594, kgLoss = 0.0149
2025-04-08 14:08:30.310241: Training Step 219/354: batchLoss = 0.6167, diffLoss = 3.0236, kgLoss = 0.0149
2025-04-08 14:08:31.919398: Training Step 220/354: batchLoss = 0.6044, diffLoss = 2.9712, kgLoss = 0.0126
2025-04-08 14:08:33.529662: Training Step 221/354: batchLoss = 0.5990, diffLoss = 2.9409, kgLoss = 0.0135
2025-04-08 14:08:35.143991: Training Step 222/354: batchLoss = 0.6113, diffLoss = 3.0074, kgLoss = 0.0123
2025-04-08 14:08:36.766902: Training Step 223/354: batchLoss = 0.5385, diffLoss = 2.6397, kgLoss = 0.0132
2025-04-08 14:08:38.389261: Training Step 224/354: batchLoss = 0.5598, diffLoss = 2.7432, kgLoss = 0.0140
2025-04-08 14:08:40.004398: Training Step 225/354: batchLoss = 0.4739, diffLoss = 2.3222, kgLoss = 0.0119
2025-04-08 14:08:41.624084: Training Step 226/354: batchLoss = 0.5498, diffLoss = 2.6956, kgLoss = 0.0133
2025-04-08 14:08:43.248988: Training Step 227/354: batchLoss = 0.4948, diffLoss = 2.4226, kgLoss = 0.0128
2025-04-08 14:08:44.869831: Training Step 228/354: batchLoss = 0.6537, diffLoss = 3.1946, kgLoss = 0.0185
2025-04-08 14:08:46.481294: Training Step 229/354: batchLoss = 0.6467, diffLoss = 3.1697, kgLoss = 0.0159
2025-04-08 14:08:48.091134: Training Step 230/354: batchLoss = 0.5902, diffLoss = 2.9039, kgLoss = 0.0118
2025-04-08 14:08:49.703434: Training Step 231/354: batchLoss = 0.5413, diffLoss = 2.6533, kgLoss = 0.0134
2025-04-08 14:08:51.325036: Training Step 232/354: batchLoss = 0.4836, diffLoss = 2.3652, kgLoss = 0.0132
2025-04-08 14:08:52.946932: Training Step 233/354: batchLoss = 0.6446, diffLoss = 3.1614, kgLoss = 0.0154
2025-04-08 14:08:54.588853: Training Step 234/354: batchLoss = 0.8293, diffLoss = 4.0668, kgLoss = 0.0199
2025-04-08 14:08:56.212481: Training Step 235/354: batchLoss = 0.6919, diffLoss = 3.3977, kgLoss = 0.0154
2025-04-08 14:08:57.832477: Training Step 236/354: batchLoss = 0.6730, diffLoss = 3.3031, kgLoss = 0.0155
2025-04-08 14:08:59.451326: Training Step 237/354: batchLoss = 0.6286, diffLoss = 3.0480, kgLoss = 0.0237
2025-04-08 14:09:01.070333: Training Step 238/354: batchLoss = 0.5373, diffLoss = 2.6329, kgLoss = 0.0134
2025-04-08 14:09:02.677236: Training Step 239/354: batchLoss = 0.6285, diffLoss = 3.0895, kgLoss = 0.0132
2025-04-08 14:09:04.290941: Training Step 240/354: batchLoss = 0.5545, diffLoss = 2.7218, kgLoss = 0.0126
2025-04-08 14:09:05.919090: Training Step 241/354: batchLoss = 0.5929, diffLoss = 2.8984, kgLoss = 0.0166
2025-04-08 14:09:07.546023: Training Step 242/354: batchLoss = 0.5870, diffLoss = 2.8762, kgLoss = 0.0147
2025-04-08 14:09:09.176431: Training Step 243/354: batchLoss = 0.5388, diffLoss = 2.6385, kgLoss = 0.0139
2025-04-08 14:09:10.818796: Training Step 244/354: batchLoss = 0.6335, diffLoss = 3.1146, kgLoss = 0.0132
2025-04-08 14:09:12.461988: Training Step 245/354: batchLoss = 0.5456, diffLoss = 2.6738, kgLoss = 0.0135
2025-04-08 14:09:14.109990: Training Step 246/354: batchLoss = 0.7989, diffLoss = 3.9254, kgLoss = 0.0173
2025-04-08 14:09:15.739159: Training Step 247/354: batchLoss = 0.7339, diffLoss = 3.5919, kgLoss = 0.0195
2025-04-08 14:09:17.361107: Training Step 248/354: batchLoss = 0.6489, diffLoss = 3.1831, kgLoss = 0.0154
2025-04-08 14:09:18.991917: Training Step 249/354: batchLoss = 0.5547, diffLoss = 2.7212, kgLoss = 0.0131
2025-04-08 14:09:20.628999: Training Step 250/354: batchLoss = 0.6778, diffLoss = 3.3285, kgLoss = 0.0151
2025-04-08 14:09:22.270957: Training Step 251/354: batchLoss = 0.7073, diffLoss = 3.4733, kgLoss = 0.0158
2025-04-08 14:09:23.889736: Training Step 252/354: batchLoss = 0.5004, diffLoss = 2.4481, kgLoss = 0.0135
2025-04-08 14:09:25.518192: Training Step 253/354: batchLoss = 0.6837, diffLoss = 3.3566, kgLoss = 0.0155
2025-04-08 14:09:27.143626: Training Step 254/354: batchLoss = 0.4787, diffLoss = 2.3449, kgLoss = 0.0122
2025-04-08 14:09:28.761511: Training Step 255/354: batchLoss = 0.6085, diffLoss = 2.9829, kgLoss = 0.0149
2025-04-08 14:09:30.369427: Training Step 256/354: batchLoss = 0.7885, diffLoss = 3.8685, kgLoss = 0.0185
2025-04-08 14:09:31.981474: Training Step 257/354: batchLoss = 0.6582, diffLoss = 3.2231, kgLoss = 0.0170
2025-04-08 14:09:33.597901: Training Step 258/354: batchLoss = 0.6503, diffLoss = 3.1907, kgLoss = 0.0152
2025-04-08 14:09:35.236111: Training Step 259/354: batchLoss = 0.6748, diffLoss = 3.3166, kgLoss = 0.0144
2025-04-08 14:09:36.855429: Training Step 260/354: batchLoss = 0.8163, diffLoss = 4.0105, kgLoss = 0.0177
2025-04-08 14:09:38.480475: Training Step 261/354: batchLoss = 0.7327, diffLoss = 3.5968, kgLoss = 0.0167
2025-04-08 14:09:40.104514: Training Step 262/354: batchLoss = 0.6270, diffLoss = 3.0794, kgLoss = 0.0139
2025-04-08 14:09:41.728020: Training Step 263/354: batchLoss = 0.6739, diffLoss = 3.3075, kgLoss = 0.0156
2025-04-08 14:09:43.366206: Training Step 264/354: batchLoss = 0.5757, diffLoss = 2.8247, kgLoss = 0.0135
2025-04-08 14:09:44.972934: Training Step 265/354: batchLoss = 0.4636, diffLoss = 2.2667, kgLoss = 0.0128
2025-04-08 14:09:46.586941: Training Step 266/354: batchLoss = 0.7072, diffLoss = 3.4697, kgLoss = 0.0166
2025-04-08 14:09:48.200857: Training Step 267/354: batchLoss = 0.5238, diffLoss = 2.5735, kgLoss = 0.0113
2025-04-08 14:09:49.825839: Training Step 268/354: batchLoss = 0.6310, diffLoss = 3.0813, kgLoss = 0.0184
2025-04-08 14:09:51.454858: Training Step 269/354: batchLoss = 0.6532, diffLoss = 3.2043, kgLoss = 0.0154
2025-04-08 14:09:53.088498: Training Step 270/354: batchLoss = 0.5915, diffLoss = 2.8982, kgLoss = 0.0148
2025-04-08 14:09:54.710911: Training Step 271/354: batchLoss = 0.5914, diffLoss = 2.8986, kgLoss = 0.0145
2025-04-08 14:09:56.333124: Training Step 272/354: batchLoss = 0.7758, diffLoss = 3.7939, kgLoss = 0.0212
2025-04-08 14:09:57.957360: Training Step 273/354: batchLoss = 0.5838, diffLoss = 2.8611, kgLoss = 0.0144
2025-04-08 14:09:59.570745: Training Step 274/354: batchLoss = 0.6101, diffLoss = 2.9920, kgLoss = 0.0147
2025-04-08 14:10:01.180426: Training Step 275/354: batchLoss = 0.5360, diffLoss = 2.6275, kgLoss = 0.0131
2025-04-08 14:10:02.798345: Training Step 276/354: batchLoss = 0.6246, diffLoss = 3.0704, kgLoss = 0.0132
2025-04-08 14:10:04.417960: Training Step 277/354: batchLoss = 0.7451, diffLoss = 3.6605, kgLoss = 0.0163
2025-04-08 14:10:06.042591: Training Step 278/354: batchLoss = 0.4694, diffLoss = 2.3053, kgLoss = 0.0104
2025-04-08 14:10:07.663288: Training Step 279/354: batchLoss = 0.6432, diffLoss = 3.1614, kgLoss = 0.0137
2025-04-08 14:10:09.291844: Training Step 280/354: batchLoss = 0.6618, diffLoss = 3.2465, kgLoss = 0.0156
2025-04-08 14:10:10.910803: Training Step 281/354: batchLoss = 0.5372, diffLoss = 2.6291, kgLoss = 0.0143
2025-04-08 14:10:12.529427: Training Step 282/354: batchLoss = 0.6171, diffLoss = 3.0201, kgLoss = 0.0163
2025-04-08 14:10:14.142943: Training Step 283/354: batchLoss = 0.6290, diffLoss = 3.0840, kgLoss = 0.0153
2025-04-08 14:10:15.755728: Training Step 284/354: batchLoss = 0.5785, diffLoss = 2.8399, kgLoss = 0.0131
2025-04-08 14:10:17.363554: Training Step 285/354: batchLoss = 0.6294, diffLoss = 3.0894, kgLoss = 0.0144
2025-04-08 14:10:18.984616: Training Step 286/354: batchLoss = 0.7476, diffLoss = 3.6708, kgLoss = 0.0168
2025-04-08 14:10:20.609115: Training Step 287/354: batchLoss = 0.7875, diffLoss = 3.8697, kgLoss = 0.0169
2025-04-08 14:10:22.239126: Training Step 288/354: batchLoss = 0.4951, diffLoss = 2.4262, kgLoss = 0.0123
2025-04-08 14:10:23.867346: Training Step 289/354: batchLoss = 0.5032, diffLoss = 2.4679, kgLoss = 0.0121
2025-04-08 14:10:25.486335: Training Step 290/354: batchLoss = 0.7320, diffLoss = 3.5972, kgLoss = 0.0157
2025-04-08 14:10:27.107246: Training Step 291/354: batchLoss = 0.6005, diffLoss = 2.9443, kgLoss = 0.0145
2025-04-08 14:10:28.725220: Training Step 292/354: batchLoss = 0.6058, diffLoss = 2.9759, kgLoss = 0.0133
2025-04-08 14:10:30.339757: Training Step 293/354: batchLoss = 0.5922, diffLoss = 2.9091, kgLoss = 0.0129
2025-04-08 14:10:31.947202: Training Step 294/354: batchLoss = 0.5111, diffLoss = 2.5014, kgLoss = 0.0136
2025-04-08 14:10:33.577320: Training Step 295/354: batchLoss = 0.5243, diffLoss = 2.5711, kgLoss = 0.0126
2025-04-08 14:10:35.204222: Training Step 296/354: batchLoss = 0.6221, diffLoss = 3.0502, kgLoss = 0.0150
2025-04-08 14:10:36.827128: Training Step 297/354: batchLoss = 0.6181, diffLoss = 3.0295, kgLoss = 0.0153
2025-04-08 14:10:38.457342: Training Step 298/354: batchLoss = 0.6426, diffLoss = 3.1499, kgLoss = 0.0157
2025-04-08 14:10:40.087269: Training Step 299/354: batchLoss = 0.6229, diffLoss = 3.0598, kgLoss = 0.0137
2025-04-08 14:10:41.708964: Training Step 300/354: batchLoss = 0.7565, diffLoss = 3.7135, kgLoss = 0.0173
2025-04-08 14:10:43.321267: Training Step 301/354: batchLoss = 0.5347, diffLoss = 2.6174, kgLoss = 0.0140
2025-04-08 14:10:44.945196: Training Step 302/354: batchLoss = 0.6269, diffLoss = 3.0798, kgLoss = 0.0136
2025-04-08 14:10:46.571900: Training Step 303/354: batchLoss = 0.7217, diffLoss = 3.5503, kgLoss = 0.0146
2025-04-08 14:10:48.194105: Training Step 304/354: batchLoss = 0.6055, diffLoss = 2.9742, kgLoss = 0.0133
2025-04-08 14:10:49.818453: Training Step 305/354: batchLoss = 0.6335, diffLoss = 3.1095, kgLoss = 0.0145
2025-04-08 14:10:51.445897: Training Step 306/354: batchLoss = 0.5670, diffLoss = 2.7803, kgLoss = 0.0136
2025-04-08 14:10:53.061496: Training Step 307/354: batchLoss = 0.7720, diffLoss = 3.7925, kgLoss = 0.0169
2025-04-08 14:10:54.686885: Training Step 308/354: batchLoss = 0.6423, diffLoss = 3.1545, kgLoss = 0.0143
2025-04-08 14:10:56.305184: Training Step 309/354: batchLoss = 0.6083, diffLoss = 2.9886, kgLoss = 0.0133
2025-04-08 14:10:57.910945: Training Step 310/354: batchLoss = 0.6470, diffLoss = 3.1829, kgLoss = 0.0131
2025-04-08 14:10:59.522427: Training Step 311/354: batchLoss = 0.6280, diffLoss = 3.0832, kgLoss = 0.0142
2025-04-08 14:11:01.149340: Training Step 312/354: batchLoss = 0.5707, diffLoss = 2.7998, kgLoss = 0.0135
2025-04-08 14:11:02.780906: Training Step 313/354: batchLoss = 0.5751, diffLoss = 2.8196, kgLoss = 0.0140
2025-04-08 14:11:04.401180: Training Step 314/354: batchLoss = 0.6395, diffLoss = 3.1283, kgLoss = 0.0173
2025-04-08 14:11:06.019812: Training Step 315/354: batchLoss = 0.5388, diffLoss = 2.6380, kgLoss = 0.0140
2025-04-08 14:11:07.645383: Training Step 316/354: batchLoss = 0.5273, diffLoss = 2.5855, kgLoss = 0.0128
2025-04-08 14:11:09.273837: Training Step 317/354: batchLoss = 0.5811, diffLoss = 2.8435, kgLoss = 0.0154
2025-04-08 14:11:10.895352: Training Step 318/354: batchLoss = 0.5198, diffLoss = 2.5514, kgLoss = 0.0120
2025-04-08 14:11:12.505030: Training Step 319/354: batchLoss = 0.5775, diffLoss = 2.8368, kgLoss = 0.0126
2025-04-08 14:11:14.119793: Training Step 320/354: batchLoss = 0.4817, diffLoss = 2.3536, kgLoss = 0.0138
2025-04-08 14:11:15.724743: Training Step 321/354: batchLoss = 0.6021, diffLoss = 2.9550, kgLoss = 0.0139
2025-04-08 14:11:17.349784: Training Step 322/354: batchLoss = 0.6027, diffLoss = 2.9608, kgLoss = 0.0132
2025-04-08 14:11:18.969981: Training Step 323/354: batchLoss = 0.5638, diffLoss = 2.7638, kgLoss = 0.0137
2025-04-08 14:11:20.592670: Training Step 324/354: batchLoss = 0.5681, diffLoss = 2.7925, kgLoss = 0.0120
2025-04-08 14:11:22.217950: Training Step 325/354: batchLoss = 0.5057, diffLoss = 2.4692, kgLoss = 0.0148
2025-04-08 14:11:23.835968: Training Step 326/354: batchLoss = 0.5788, diffLoss = 2.8422, kgLoss = 0.0130
2025-04-08 14:11:25.453964: Training Step 327/354: batchLoss = 0.7384, diffLoss = 3.6292, kgLoss = 0.0157
2025-04-08 14:11:27.061642: Training Step 328/354: batchLoss = 0.5867, diffLoss = 2.8735, kgLoss = 0.0150
2025-04-08 14:11:28.666409: Training Step 329/354: batchLoss = 0.5545, diffLoss = 2.7161, kgLoss = 0.0140
2025-04-08 14:11:30.274418: Training Step 330/354: batchLoss = 0.5897, diffLoss = 2.8927, kgLoss = 0.0140
2025-04-08 14:11:31.889532: Training Step 331/354: batchLoss = 0.5250, diffLoss = 2.5799, kgLoss = 0.0113
2025-04-08 14:11:33.503311: Training Step 332/354: batchLoss = 0.5619, diffLoss = 2.7600, kgLoss = 0.0124
2025-04-08 14:11:35.121982: Training Step 333/354: batchLoss = 0.6853, diffLoss = 3.3550, kgLoss = 0.0179
2025-04-08 14:11:36.749056: Training Step 334/354: batchLoss = 0.6777, diffLoss = 3.3230, kgLoss = 0.0164
2025-04-08 14:11:38.372628: Training Step 335/354: batchLoss = 0.4946, diffLoss = 2.4206, kgLoss = 0.0131
2025-04-08 14:11:39.990473: Training Step 336/354: batchLoss = 0.6436, diffLoss = 3.1569, kgLoss = 0.0153
2025-04-08 14:11:41.599364: Training Step 337/354: batchLoss = 0.7020, diffLoss = 3.4433, kgLoss = 0.0167
2025-04-08 14:11:43.214401: Training Step 338/354: batchLoss = 0.7744, diffLoss = 3.7968, kgLoss = 0.0188
2025-04-08 14:11:44.823555: Training Step 339/354: batchLoss = 0.8168, diffLoss = 4.0072, kgLoss = 0.0193
2025-04-08 14:11:46.439671: Training Step 340/354: batchLoss = 0.6449, diffLoss = 3.1648, kgLoss = 0.0150
2025-04-08 14:11:48.063959: Training Step 341/354: batchLoss = 0.6963, diffLoss = 3.4172, kgLoss = 0.0161
2025-04-08 14:11:49.691023: Training Step 342/354: batchLoss = 0.6229, diffLoss = 3.0551, kgLoss = 0.0149
2025-04-08 14:11:51.315597: Training Step 343/354: batchLoss = 0.5916, diffLoss = 2.8978, kgLoss = 0.0151
2025-04-08 14:11:52.941737: Training Step 344/354: batchLoss = 0.5776, diffLoss = 2.8350, kgLoss = 0.0133
2025-04-08 14:11:54.556995: Training Step 345/354: batchLoss = 0.6874, diffLoss = 3.3674, kgLoss = 0.0174
2025-04-08 14:11:56.183749: Training Step 346/354: batchLoss = 0.6749, diffLoss = 3.3130, kgLoss = 0.0153
2025-04-08 14:11:57.793789: Training Step 347/354: batchLoss = 0.5941, diffLoss = 2.9078, kgLoss = 0.0157
2025-04-08 14:11:59.404506: Training Step 348/354: batchLoss = 0.5884, diffLoss = 2.8885, kgLoss = 0.0134
2025-04-08 14:12:01.020881: Training Step 349/354: batchLoss = 0.7220, diffLoss = 3.5478, kgLoss = 0.0156
2025-04-08 14:12:02.637170: Training Step 350/354: batchLoss = 0.5945, diffLoss = 2.9172, kgLoss = 0.0138
2025-04-08 14:12:04.244532: Training Step 351/354: batchLoss = 0.6527, diffLoss = 3.2032, kgLoss = 0.0151
2025-04-08 14:12:05.843572: Training Step 352/354: batchLoss = 0.5370, diffLoss = 2.6326, kgLoss = 0.0131
2025-04-08 14:12:07.249770: Training Step 353/354: batchLoss = 0.5747, diffLoss = 2.8248, kgLoss = 0.0121
2025-04-08 14:12:07.341486: 
2025-04-08 14:12:07.342189: Epoch 7/1000, Train: epLoss = 1.0925, epDfLoss = 5.3584, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 14:12:08.657596: Steps 0/138: batch_recall = 46.36, batch_ndcg = 26.17 
2025-04-08 14:12:09.975900: Steps 1/138: batch_recall = 46.17, batch_ndcg = 27.37 
2025-04-08 14:12:11.265696: Steps 2/138: batch_recall = 56.68, batch_ndcg = 34.65 
2025-04-08 14:12:12.575100: Steps 3/138: batch_recall = 54.29, batch_ndcg = 31.10 
2025-04-08 14:12:13.880507: Steps 4/138: batch_recall = 63.86, batch_ndcg = 40.50 
2025-04-08 14:12:15.190279: Steps 5/138: batch_recall = 58.62, batch_ndcg = 30.21 
2025-04-08 14:12:16.497844: Steps 6/138: batch_recall = 47.65, batch_ndcg = 27.84 
2025-04-08 14:12:17.798798: Steps 7/138: batch_recall = 60.20, batch_ndcg = 37.83 
2025-04-08 14:12:19.108101: Steps 8/138: batch_recall = 63.20, batch_ndcg = 38.76 
2025-04-08 14:12:20.403083: Steps 9/138: batch_recall = 51.87, batch_ndcg = 31.55 
2025-04-08 14:12:21.695722: Steps 10/138: batch_recall = 50.53, batch_ndcg = 28.81 
2025-04-08 14:12:23.003669: Steps 11/138: batch_recall = 61.04, batch_ndcg = 35.05 
2025-04-08 14:12:24.303183: Steps 12/138: batch_recall = 52.18, batch_ndcg = 29.32 
2025-04-08 14:12:25.606181: Steps 13/138: batch_recall = 50.71, batch_ndcg = 28.37 
2025-04-08 14:12:26.906685: Steps 14/138: batch_recall = 51.79, batch_ndcg = 29.08 
2025-04-08 14:12:28.186626: Steps 15/138: batch_recall = 50.61, batch_ndcg = 30.45 
2025-04-08 14:12:29.481721: Steps 16/138: batch_recall = 58.52, batch_ndcg = 31.50 
2025-04-08 14:12:30.776688: Steps 17/138: batch_recall = 56.91, batch_ndcg = 30.98 
2025-04-08 14:12:32.076593: Steps 18/138: batch_recall = 47.75, batch_ndcg = 30.42 
2025-04-08 14:12:33.388199: Steps 19/138: batch_recall = 49.85, batch_ndcg = 30.65 
2025-04-08 14:12:34.684293: Steps 20/138: batch_recall = 60.53, batch_ndcg = 33.82 
2025-04-08 14:12:35.980931: Steps 21/138: batch_recall = 62.53, batch_ndcg = 37.93 
2025-04-08 14:12:37.265997: Steps 22/138: batch_recall = 55.00, batch_ndcg = 32.09 
2025-04-08 14:12:38.545878: Steps 23/138: batch_recall = 52.21, batch_ndcg = 28.75 
2025-04-08 14:12:39.834187: Steps 24/138: batch_recall = 46.18, batch_ndcg = 27.91 
2025-04-08 14:12:41.118303: Steps 25/138: batch_recall = 56.82, batch_ndcg = 33.14 
2025-04-08 14:12:42.400382: Steps 26/138: batch_recall = 56.52, batch_ndcg = 32.99 
2025-04-08 14:12:43.671390: Steps 27/138: batch_recall = 61.21, batch_ndcg = 34.27 
2025-04-08 14:12:44.966153: Steps 28/138: batch_recall = 53.71, batch_ndcg = 30.71 
2025-04-08 14:12:46.264598: Steps 29/138: batch_recall = 58.38, batch_ndcg = 30.90 
2025-04-08 14:12:47.559693: Steps 30/138: batch_recall = 53.25, batch_ndcg = 32.97 
2025-04-08 14:12:48.845600: Steps 31/138: batch_recall = 39.45, batch_ndcg = 22.85 
2025-04-08 14:12:50.147753: Steps 32/138: batch_recall = 48.02, batch_ndcg = 29.50 
2025-04-08 14:12:51.442023: Steps 33/138: batch_recall = 54.90, batch_ndcg = 32.33 
2025-04-08 14:12:52.733733: Steps 34/138: batch_recall = 50.70, batch_ndcg = 28.34 
2025-04-08 14:12:54.018576: Steps 35/138: batch_recall = 49.32, batch_ndcg = 29.31 
2025-04-08 14:12:55.324671: Steps 36/138: batch_recall = 48.61, batch_ndcg = 27.46 
2025-04-08 14:12:56.608238: Steps 37/138: batch_recall = 59.30, batch_ndcg = 33.74 
2025-04-08 14:12:57.880108: Steps 38/138: batch_recall = 55.02, batch_ndcg = 31.05 
2025-04-08 14:12:59.149857: Steps 39/138: batch_recall = 68.76, batch_ndcg = 36.83 
2025-04-08 14:13:00.435249: Steps 40/138: batch_recall = 51.12, batch_ndcg = 27.60 
2025-04-08 14:13:01.709282: Steps 41/138: batch_recall = 58.85, batch_ndcg = 33.36 
2025-04-08 14:13:02.992187: Steps 42/138: batch_recall = 52.74, batch_ndcg = 30.54 
2025-04-08 14:13:04.278197: Steps 43/138: batch_recall = 54.48, batch_ndcg = 33.03 
2025-04-08 14:13:05.579880: Steps 44/138: batch_recall = 52.30, batch_ndcg = 27.75 
2025-04-08 14:13:06.864804: Steps 45/138: batch_recall = 57.76, batch_ndcg = 31.86 
2025-04-08 14:13:08.132776: Steps 46/138: batch_recall = 60.17, batch_ndcg = 34.26 
2025-04-08 14:13:09.408254: Steps 47/138: batch_recall = 60.09, batch_ndcg = 32.94 
2025-04-08 14:13:10.686484: Steps 48/138: batch_recall = 52.14, batch_ndcg = 31.69 
2025-04-08 14:13:11.961505: Steps 49/138: batch_recall = 59.93, batch_ndcg = 36.73 
2025-04-08 14:13:13.223456: Steps 50/138: batch_recall = 57.48, batch_ndcg = 32.56 
2025-04-08 14:13:14.503316: Steps 51/138: batch_recall = 59.74, batch_ndcg = 36.13 
2025-04-08 14:13:15.798022: Steps 52/138: batch_recall = 66.40, batch_ndcg = 40.60 
2025-04-08 14:13:17.081172: Steps 53/138: batch_recall = 58.78, batch_ndcg = 31.04 
2025-04-08 14:13:18.365915: Steps 54/138: batch_recall = 59.90, batch_ndcg = 34.81 
2025-04-08 14:13:19.652190: Steps 55/138: batch_recall = 55.94, batch_ndcg = 32.01 
2025-04-08 14:13:20.938124: Steps 56/138: batch_recall = 61.75, batch_ndcg = 34.70 
2025-04-08 14:13:22.216229: Steps 57/138: batch_recall = 57.98, batch_ndcg = 32.76 
2025-04-08 14:13:23.498897: Steps 58/138: batch_recall = 66.40, batch_ndcg = 34.69 
2025-04-08 14:13:24.768324: Steps 59/138: batch_recall = 76.02, batch_ndcg = 41.77 
2025-04-08 14:13:26.037553: Steps 60/138: batch_recall = 68.87, batch_ndcg = 39.17 
2025-04-08 14:13:27.315741: Steps 61/138: batch_recall = 61.15, batch_ndcg = 33.47 
2025-04-08 14:13:28.588875: Steps 62/138: batch_recall = 76.83, batch_ndcg = 42.88 
2025-04-08 14:13:29.855192: Steps 63/138: batch_recall = 74.17, batch_ndcg = 42.76 
2025-04-08 14:13:31.134350: Steps 64/138: batch_recall = 60.60, batch_ndcg = 32.87 
2025-04-08 14:13:32.405818: Steps 65/138: batch_recall = 81.71, batch_ndcg = 45.76 
2025-04-08 14:13:33.687002: Steps 66/138: batch_recall = 66.56, batch_ndcg = 39.59 
2025-04-08 14:13:34.967324: Steps 67/138: batch_recall = 78.73, batch_ndcg = 47.41 
2025-04-08 14:13:36.251151: Steps 68/138: batch_recall = 68.29, batch_ndcg = 35.26 
2025-04-08 14:13:37.522899: Steps 69/138: batch_recall = 84.98, batch_ndcg = 48.05 
2025-04-08 14:13:38.801250: Steps 70/138: batch_recall = 79.48, batch_ndcg = 44.61 
2025-04-08 14:13:40.073424: Steps 71/138: batch_recall = 83.53, batch_ndcg = 47.33 
2025-04-08 14:13:41.340782: Steps 72/138: batch_recall = 85.02, batch_ndcg = 49.39 
2025-04-08 14:13:42.613033: Steps 73/138: batch_recall = 91.55, batch_ndcg = 48.11 
2025-04-08 14:13:43.885592: Steps 74/138: batch_recall = 78.42, batch_ndcg = 47.52 
2025-04-08 14:13:45.176903: Steps 75/138: batch_recall = 83.98, batch_ndcg = 47.90 
2025-04-08 14:13:46.491592: Steps 76/138: batch_recall = 95.90, batch_ndcg = 55.68 
2025-04-08 14:13:47.816408: Steps 77/138: batch_recall = 83.50, batch_ndcg = 46.57 
2025-04-08 14:13:49.146725: Steps 78/138: batch_recall = 97.18, batch_ndcg = 50.68 
2025-04-08 14:13:50.474156: Steps 79/138: batch_recall = 88.87, batch_ndcg = 47.05 
2025-04-08 14:13:51.782584: Steps 80/138: batch_recall = 75.09, batch_ndcg = 39.99 
2025-04-08 14:13:53.078273: Steps 81/138: batch_recall = 80.63, batch_ndcg = 46.95 
2025-04-08 14:13:54.368495: Steps 82/138: batch_recall = 83.84, batch_ndcg = 48.14 
2025-04-08 14:13:55.660799: Steps 83/138: batch_recall = 81.61, batch_ndcg = 47.09 
2025-04-08 14:13:56.949907: Steps 84/138: batch_recall = 106.15, batch_ndcg = 56.54 
2025-04-08 14:13:58.270522: Steps 85/138: batch_recall = 96.41, batch_ndcg = 54.63 
2025-04-08 14:13:59.579398: Steps 86/138: batch_recall = 117.23, batch_ndcg = 70.50 
2025-04-08 14:14:00.846042: Steps 87/138: batch_recall = 107.25, batch_ndcg = 58.81 
2025-04-08 14:14:02.140799: Steps 88/138: batch_recall = 102.07, batch_ndcg = 55.71 
2025-04-08 14:14:03.432041: Steps 89/138: batch_recall = 118.61, batch_ndcg = 64.48 
2025-04-08 14:14:04.701556: Steps 90/138: batch_recall = 105.51, batch_ndcg = 57.41 
2025-04-08 14:14:05.973578: Steps 91/138: batch_recall = 120.70, batch_ndcg = 66.05 
2025-04-08 14:14:07.248436: Steps 92/138: batch_recall = 109.60, batch_ndcg = 60.85 
2025-04-08 14:14:08.524160: Steps 93/138: batch_recall = 111.85, batch_ndcg = 67.27 
2025-04-08 14:14:09.801833: Steps 94/138: batch_recall = 113.19, batch_ndcg = 59.51 
2025-04-08 14:14:11.071120: Steps 95/138: batch_recall = 112.63, batch_ndcg = 65.81 
2025-04-08 14:14:12.390558: Steps 96/138: batch_recall = 138.43, batch_ndcg = 74.46 
2025-04-08 14:14:13.649577: Steps 97/138: batch_recall = 147.02, batch_ndcg = 77.97 
2025-04-08 14:14:14.907932: Steps 98/138: batch_recall = 104.23, batch_ndcg = 60.73 
2025-04-08 14:14:16.183904: Steps 99/138: batch_recall = 121.91, batch_ndcg = 65.79 
2025-04-08 14:14:17.470952: Steps 100/138: batch_recall = 121.97, batch_ndcg = 68.92 
2025-04-08 14:14:18.745723: Steps 101/138: batch_recall = 122.19, batch_ndcg = 68.39 
2025-04-08 14:14:20.034359: Steps 102/138: batch_recall = 125.05, batch_ndcg = 72.90 
2025-04-08 14:14:21.315489: Steps 103/138: batch_recall = 136.27, batch_ndcg = 77.24 
2025-04-08 14:14:22.584914: Steps 104/138: batch_recall = 138.40, batch_ndcg = 75.05 
2025-04-08 14:14:23.867074: Steps 105/138: batch_recall = 117.94, batch_ndcg = 66.96 
2025-04-08 14:14:25.159667: Steps 106/138: batch_recall = 107.94, batch_ndcg = 58.36 
2025-04-08 14:14:26.415000: Steps 107/138: batch_recall = 113.21, batch_ndcg = 61.87 
2025-04-08 14:14:27.675167: Steps 108/138: batch_recall = 118.90, batch_ndcg = 71.30 
2025-04-08 14:14:28.949247: Steps 109/138: batch_recall = 139.71, batch_ndcg = 74.81 
2025-04-08 14:14:30.222973: Steps 110/138: batch_recall = 119.77, batch_ndcg = 63.78 
2025-04-08 14:14:31.487440: Steps 111/138: batch_recall = 137.79, batch_ndcg = 83.52 
2025-04-08 14:14:32.759998: Steps 112/138: batch_recall = 167.10, batch_ndcg = 86.28 
2025-04-08 14:14:34.042977: Steps 113/138: batch_recall = 127.92, batch_ndcg = 72.36 
2025-04-08 14:14:35.328176: Steps 114/138: batch_recall = 116.66, batch_ndcg = 65.87 
2025-04-08 14:14:36.604979: Steps 115/138: batch_recall = 115.22, batch_ndcg = 59.59 
2025-04-08 14:14:37.889324: Steps 116/138: batch_recall = 123.90, batch_ndcg = 67.41 
2025-04-08 14:14:39.166958: Steps 117/138: batch_recall = 115.12, batch_ndcg = 66.13 
2025-04-08 14:14:40.443123: Steps 118/138: batch_recall = 118.99, batch_ndcg = 68.60 
2025-04-08 14:14:41.719207: Steps 119/138: batch_recall = 137.11, batch_ndcg = 72.12 
2025-04-08 14:14:42.992631: Steps 120/138: batch_recall = 121.72, batch_ndcg = 66.05 
2025-04-08 14:14:44.262867: Steps 121/138: batch_recall = 150.33, batch_ndcg = 77.76 
2025-04-08 14:14:45.547424: Steps 122/138: batch_recall = 144.07, batch_ndcg = 77.19 
2025-04-08 14:14:46.818162: Steps 123/138: batch_recall = 131.34, batch_ndcg = 71.39 
2025-04-08 14:14:48.096953: Steps 124/138: batch_recall = 151.11, batch_ndcg = 91.99 
2025-04-08 14:14:49.371807: Steps 125/138: batch_recall = 127.67, batch_ndcg = 69.10 
2025-04-08 14:14:50.645292: Steps 126/138: batch_recall = 156.20, batch_ndcg = 88.64 
2025-04-08 14:14:51.923118: Steps 127/138: batch_recall = 137.59, batch_ndcg = 80.37 
2025-04-08 14:14:53.189866: Steps 128/138: batch_recall = 126.96, batch_ndcg = 68.66 
2025-04-08 14:14:54.471939: Steps 129/138: batch_recall = 153.90, batch_ndcg = 87.56 
2025-04-08 14:14:55.757500: Steps 130/138: batch_recall = 131.16, batch_ndcg = 68.20 
2025-04-08 14:14:57.027524: Steps 131/138: batch_recall = 140.29, batch_ndcg = 80.17 
2025-04-08 14:14:58.288974: Steps 132/138: batch_recall = 153.67, batch_ndcg = 86.03 
2025-04-08 14:14:59.557773: Steps 133/138: batch_recall = 142.68, batch_ndcg = 81.94 
2025-04-08 14:15:00.839220: Steps 134/138: batch_recall = 135.63, batch_ndcg = 75.58 
2025-04-08 14:15:02.114784: Steps 135/138: batch_recall = 162.11, batch_ndcg = 91.93 
2025-04-08 14:15:03.371483: Steps 136/138: batch_recall = 145.70, batch_ndcg = 77.77 
2025-04-08 14:15:04.645455: Steps 137/138: batch_recall = 137.15, batch_ndcg = 85.07 
2025-04-08 14:15:04.645998: Epoch 7/1000, Test: Recall = 0.1723, NDCG = 0.0970  

2025-04-08 14:15:06.413461: Training Step 0/354: batchLoss = 0.5650, diffLoss = 2.7737, kgLoss = 0.0129
2025-04-08 14:15:08.030407: Training Step 1/354: batchLoss = 0.4879, diffLoss = 2.3928, kgLoss = 0.0117
2025-04-08 14:15:09.656225: Training Step 2/354: batchLoss = 0.6229, diffLoss = 3.0539, kgLoss = 0.0151
2025-04-08 14:15:11.271835: Training Step 3/354: batchLoss = 0.6199, diffLoss = 3.0399, kgLoss = 0.0149
2025-04-08 14:15:12.893955: Training Step 4/354: batchLoss = 0.6985, diffLoss = 3.4285, kgLoss = 0.0160
2025-04-08 14:15:14.507279: Training Step 5/354: batchLoss = 0.5530, diffLoss = 2.7136, kgLoss = 0.0129
2025-04-08 14:15:16.131397: Training Step 6/354: batchLoss = 0.6210, diffLoss = 3.0522, kgLoss = 0.0132
2025-04-08 14:15:17.745265: Training Step 7/354: batchLoss = 0.5112, diffLoss = 2.5009, kgLoss = 0.0138
2025-04-08 14:15:19.369684: Training Step 8/354: batchLoss = 0.6045, diffLoss = 2.9621, kgLoss = 0.0152
2025-04-08 14:15:20.998738: Training Step 9/354: batchLoss = 0.6404, diffLoss = 3.1403, kgLoss = 0.0154
2025-04-08 14:15:22.617988: Training Step 10/354: batchLoss = 0.5653, diffLoss = 2.7667, kgLoss = 0.0149
2025-04-08 14:15:24.244277: Training Step 11/354: batchLoss = 0.6945, diffLoss = 3.4057, kgLoss = 0.0167
2025-04-08 14:15:25.863768: Training Step 12/354: batchLoss = 0.6378, diffLoss = 3.1256, kgLoss = 0.0158
2025-04-08 14:15:27.473027: Training Step 13/354: batchLoss = 0.7201, diffLoss = 3.5371, kgLoss = 0.0158
2025-04-08 14:15:29.109073: Training Step 14/354: batchLoss = 0.6084, diffLoss = 2.9826, kgLoss = 0.0148
2025-04-08 14:15:30.720962: Training Step 15/354: batchLoss = 0.6141, diffLoss = 3.0134, kgLoss = 0.0143
2025-04-08 14:15:32.343711: Training Step 16/354: batchLoss = 0.5231, diffLoss = 2.5666, kgLoss = 0.0123
2025-04-08 14:15:33.965357: Training Step 17/354: batchLoss = 0.7287, diffLoss = 3.5707, kgLoss = 0.0183
2025-04-08 14:15:35.587760: Training Step 18/354: batchLoss = 0.7675, diffLoss = 3.7690, kgLoss = 0.0172
2025-04-08 14:15:37.213152: Training Step 19/354: batchLoss = 0.5773, diffLoss = 2.8376, kgLoss = 0.0123
2025-04-08 14:15:38.827666: Training Step 20/354: batchLoss = 0.6443, diffLoss = 3.1624, kgLoss = 0.0148
2025-04-08 14:15:40.438826: Training Step 21/354: batchLoss = 0.6863, diffLoss = 3.3660, kgLoss = 0.0164
2025-04-08 14:15:42.049693: Training Step 22/354: batchLoss = 0.6338, diffLoss = 3.1100, kgLoss = 0.0147
2025-04-08 14:15:43.654232: Training Step 23/354: batchLoss = 0.5730, diffLoss = 2.8142, kgLoss = 0.0127
2025-04-08 14:15:45.279031: Training Step 24/354: batchLoss = 0.6938, diffLoss = 3.3991, kgLoss = 0.0174
2025-04-08 14:15:46.894166: Training Step 25/354: batchLoss = 0.5247, diffLoss = 2.5798, kgLoss = 0.0109
2025-04-08 14:15:48.509378: Training Step 26/354: batchLoss = 0.5830, diffLoss = 2.8615, kgLoss = 0.0134
2025-04-08 14:15:50.131187: Training Step 27/354: batchLoss = 0.6851, diffLoss = 3.3648, kgLoss = 0.0152
2025-04-08 14:15:51.747924: Training Step 28/354: batchLoss = 0.6238, diffLoss = 3.0614, kgLoss = 0.0144
2025-04-08 14:15:53.375852: Training Step 29/354: batchLoss = 0.5585, diffLoss = 2.7412, kgLoss = 0.0128
2025-04-08 14:15:54.995306: Training Step 30/354: batchLoss = 0.6708, diffLoss = 3.2932, kgLoss = 0.0151
2025-04-08 14:15:56.606078: Training Step 31/354: batchLoss = 0.4841, diffLoss = 2.3648, kgLoss = 0.0139
2025-04-08 14:15:58.233667: Training Step 32/354: batchLoss = 0.5103, diffLoss = 2.5019, kgLoss = 0.0124
2025-04-08 14:15:59.850821: Training Step 33/354: batchLoss = 0.5693, diffLoss = 2.7842, kgLoss = 0.0156
2025-04-08 14:16:01.468028: Training Step 34/354: batchLoss = 0.4337, diffLoss = 2.1216, kgLoss = 0.0118
2025-04-08 14:16:03.091869: Training Step 35/354: batchLoss = 0.5656, diffLoss = 2.7640, kgLoss = 0.0160
2025-04-08 14:16:04.708665: Training Step 36/354: batchLoss = 0.7229, diffLoss = 3.5398, kgLoss = 0.0187
2025-04-08 14:16:06.333368: Training Step 37/354: batchLoss = 0.6805, diffLoss = 3.3417, kgLoss = 0.0152
2025-04-08 14:16:07.956635: Training Step 38/354: batchLoss = 0.5871, diffLoss = 2.8704, kgLoss = 0.0162
2025-04-08 14:16:09.582990: Training Step 39/354: batchLoss = 0.4811, diffLoss = 2.3576, kgLoss = 0.0119
2025-04-08 14:16:11.192688: Training Step 40/354: batchLoss = 0.7790, diffLoss = 3.8244, kgLoss = 0.0176
2025-04-08 14:16:12.803177: Training Step 41/354: batchLoss = 0.6177, diffLoss = 3.0304, kgLoss = 0.0145
2025-04-08 14:16:14.416197: Training Step 42/354: batchLoss = 0.7038, diffLoss = 3.4587, kgLoss = 0.0150
2025-04-08 14:16:16.037117: Training Step 43/354: batchLoss = 0.5331, diffLoss = 2.6130, kgLoss = 0.0132
2025-04-08 14:16:17.655870: Training Step 44/354: batchLoss = 0.7376, diffLoss = 3.6210, kgLoss = 0.0168
2025-04-08 14:16:19.277194: Training Step 45/354: batchLoss = 0.4846, diffLoss = 2.3750, kgLoss = 0.0119
2025-04-08 14:16:20.906268: Training Step 46/354: batchLoss = 1.0650, diffLoss = 5.2296, kgLoss = 0.0238
2025-04-08 14:16:22.528990: Training Step 47/354: batchLoss = 0.6783, diffLoss = 3.3248, kgLoss = 0.0167
2025-04-08 14:16:24.146305: Training Step 48/354: batchLoss = 0.5998, diffLoss = 2.9469, kgLoss = 0.0131
2025-04-08 14:16:25.763212: Training Step 49/354: batchLoss = 0.5909, diffLoss = 2.9022, kgLoss = 0.0131
2025-04-08 14:16:27.385582: Training Step 50/354: batchLoss = 0.5240, diffLoss = 2.5716, kgLoss = 0.0121
2025-04-08 14:16:29.003943: Training Step 51/354: batchLoss = 0.6241, diffLoss = 3.0588, kgLoss = 0.0155
2025-04-08 14:16:30.623358: Training Step 52/354: batchLoss = 0.6008, diffLoss = 2.9482, kgLoss = 0.0139
2025-04-08 14:16:32.233518: Training Step 53/354: batchLoss = 0.7276, diffLoss = 3.5705, kgLoss = 0.0169
2025-04-08 14:16:33.853896: Training Step 54/354: batchLoss = 0.4869, diffLoss = 2.3881, kgLoss = 0.0115
2025-04-08 14:16:35.482307: Training Step 55/354: batchLoss = 0.7058, diffLoss = 3.4694, kgLoss = 0.0148
2025-04-08 14:16:37.097973: Training Step 56/354: batchLoss = 0.6059, diffLoss = 2.9751, kgLoss = 0.0136
2025-04-08 14:16:38.712249: Training Step 57/354: batchLoss = 0.6486, diffLoss = 3.1824, kgLoss = 0.0151
2025-04-08 14:16:40.333166: Training Step 58/354: batchLoss = 0.5831, diffLoss = 2.8600, kgLoss = 0.0139
2025-04-08 14:16:41.949221: Training Step 59/354: batchLoss = 0.8598, diffLoss = 4.2151, kgLoss = 0.0210
2025-04-08 14:16:43.561461: Training Step 60/354: batchLoss = 0.6635, diffLoss = 3.2481, kgLoss = 0.0173
2025-04-08 14:16:45.186809: Training Step 61/354: batchLoss = 0.7374, diffLoss = 3.6225, kgLoss = 0.0161
2025-04-08 14:16:46.809727: Training Step 62/354: batchLoss = 0.5807, diffLoss = 2.8483, kgLoss = 0.0138
2025-04-08 14:16:48.437275: Training Step 63/354: batchLoss = 0.7088, diffLoss = 3.4730, kgLoss = 0.0178
2025-04-08 14:16:50.063661: Training Step 64/354: batchLoss = 0.5372, diffLoss = 2.6313, kgLoss = 0.0136
2025-04-08 14:16:51.704415: Training Step 65/354: batchLoss = 0.6334, diffLoss = 3.1049, kgLoss = 0.0155
2025-04-08 14:16:53.343000: Training Step 66/354: batchLoss = 0.6634, diffLoss = 3.2549, kgLoss = 0.0155
2025-04-08 14:16:54.957888: Training Step 67/354: batchLoss = 0.7411, diffLoss = 3.6363, kgLoss = 0.0173
2025-04-08 14:16:56.563442: Training Step 68/354: batchLoss = 0.5821, diffLoss = 2.8531, kgLoss = 0.0144
2025-04-08 14:16:58.186590: Training Step 69/354: batchLoss = 0.5736, diffLoss = 2.8121, kgLoss = 0.0140
2025-04-08 14:16:59.804839: Training Step 70/354: batchLoss = 0.7607, diffLoss = 3.7404, kgLoss = 0.0158
2025-04-08 14:17:01.450158: Training Step 71/354: batchLoss = 0.5504, diffLoss = 2.6974, kgLoss = 0.0136
2025-04-08 14:17:03.068796: Training Step 72/354: batchLoss = 0.8553, diffLoss = 4.2035, kgLoss = 0.0182
2025-04-08 14:17:04.687152: Training Step 73/354: batchLoss = 0.6394, diffLoss = 3.1292, kgLoss = 0.0169
2025-04-08 14:17:06.292668: Training Step 74/354: batchLoss = 0.6055, diffLoss = 2.9664, kgLoss = 0.0153
2025-04-08 14:17:07.912486: Training Step 75/354: batchLoss = 0.7240, diffLoss = 3.5545, kgLoss = 0.0164
2025-04-08 14:17:09.520863: Training Step 76/354: batchLoss = 0.5906, diffLoss = 2.9023, kgLoss = 0.0127
2025-04-08 14:17:11.126390: Training Step 77/354: batchLoss = 0.4827, diffLoss = 2.3654, kgLoss = 0.0121
2025-04-08 14:17:12.737154: Training Step 78/354: batchLoss = 0.5850, diffLoss = 2.8647, kgLoss = 0.0151
2025-04-08 14:17:14.351921: Training Step 79/354: batchLoss = 0.5455, diffLoss = 2.6731, kgLoss = 0.0136
2025-04-08 14:17:15.964739: Training Step 80/354: batchLoss = 0.6259, diffLoss = 3.0686, kgLoss = 0.0153
2025-04-08 14:17:17.573936: Training Step 81/354: batchLoss = 0.5364, diffLoss = 2.6324, kgLoss = 0.0124
2025-04-08 14:17:19.201632: Training Step 82/354: batchLoss = 0.6447, diffLoss = 3.1667, kgLoss = 0.0142
2025-04-08 14:17:20.811096: Training Step 83/354: batchLoss = 0.5923, diffLoss = 2.9074, kgLoss = 0.0135
2025-04-08 14:17:22.416985: Training Step 84/354: batchLoss = 0.6594, diffLoss = 3.2313, kgLoss = 0.0165
2025-04-08 14:17:24.027790: Training Step 85/354: batchLoss = 0.5486, diffLoss = 2.6938, kgLoss = 0.0123
2025-04-08 14:17:25.637772: Training Step 86/354: batchLoss = 0.6666, diffLoss = 3.2615, kgLoss = 0.0179
2025-04-08 14:17:27.251669: Training Step 87/354: batchLoss = 0.5081, diffLoss = 2.4879, kgLoss = 0.0131
2025-04-08 14:17:28.860065: Training Step 88/354: batchLoss = 0.5501, diffLoss = 2.6881, kgLoss = 0.0156
2025-04-08 14:17:30.472946: Training Step 89/354: batchLoss = 0.6278, diffLoss = 3.0807, kgLoss = 0.0146
2025-04-08 14:17:32.083614: Training Step 90/354: batchLoss = 0.6976, diffLoss = 3.4210, kgLoss = 0.0168
2025-04-08 14:17:33.705498: Training Step 91/354: batchLoss = 0.5244, diffLoss = 2.5723, kgLoss = 0.0124
2025-04-08 14:17:35.326562: Training Step 92/354: batchLoss = 0.5259, diffLoss = 2.5797, kgLoss = 0.0124
2025-04-08 14:17:36.963768: Training Step 93/354: batchLoss = 0.6689, diffLoss = 3.2848, kgLoss = 0.0150
2025-04-08 14:17:38.572826: Training Step 94/354: batchLoss = 0.6476, diffLoss = 3.1789, kgLoss = 0.0148
2025-04-08 14:17:40.193096: Training Step 95/354: batchLoss = 0.6458, diffLoss = 3.1650, kgLoss = 0.0160
2025-04-08 14:17:41.800739: Training Step 96/354: batchLoss = 0.6406, diffLoss = 3.1443, kgLoss = 0.0147
2025-04-08 14:17:43.415878: Training Step 97/354: batchLoss = 0.6344, diffLoss = 3.1097, kgLoss = 0.0156
2025-04-08 14:17:45.025224: Training Step 98/354: batchLoss = 0.7396, diffLoss = 3.6278, kgLoss = 0.0175
2025-04-08 14:17:46.659053: Training Step 99/354: batchLoss = 0.8055, diffLoss = 3.9527, kgLoss = 0.0187
2025-04-08 14:17:48.277509: Training Step 100/354: batchLoss = 0.5838, diffLoss = 2.8655, kgLoss = 0.0134
2025-04-08 14:17:49.901344: Training Step 101/354: batchLoss = 0.4980, diffLoss = 2.4425, kgLoss = 0.0118
2025-04-08 14:17:51.527752: Training Step 102/354: batchLoss = 0.6646, diffLoss = 3.2694, kgLoss = 0.0134
2025-04-08 14:17:53.140491: Training Step 103/354: batchLoss = 0.7087, diffLoss = 3.4771, kgLoss = 0.0166
2025-04-08 14:17:54.753458: Training Step 104/354: batchLoss = 0.5778, diffLoss = 2.8326, kgLoss = 0.0141
2025-04-08 14:17:56.363820: Training Step 105/354: batchLoss = 0.7239, diffLoss = 3.5440, kgLoss = 0.0188
2025-04-08 14:17:57.996770: Training Step 106/354: batchLoss = 0.6375, diffLoss = 3.1244, kgLoss = 0.0158
2025-04-08 14:17:59.624997: Training Step 107/354: batchLoss = 0.6396, diffLoss = 3.1342, kgLoss = 0.0159
2025-04-08 14:18:01.255233: Training Step 108/354: batchLoss = 0.5265, diffLoss = 2.5881, kgLoss = 0.0111
2025-04-08 14:18:02.883090: Training Step 109/354: batchLoss = 0.6121, diffLoss = 2.9969, kgLoss = 0.0159
2025-04-08 14:18:04.510507: Training Step 110/354: batchLoss = 0.5599, diffLoss = 2.7526, kgLoss = 0.0117
2025-04-08 14:18:06.140523: Training Step 111/354: batchLoss = 0.4829, diffLoss = 2.3624, kgLoss = 0.0130
2025-04-08 14:18:07.751067: Training Step 112/354: batchLoss = 0.6207, diffLoss = 3.0420, kgLoss = 0.0154
2025-04-08 14:18:09.357358: Training Step 113/354: batchLoss = 0.5672, diffLoss = 2.7852, kgLoss = 0.0127
2025-04-08 14:18:10.966549: Training Step 114/354: batchLoss = 0.6652, diffLoss = 3.2707, kgLoss = 0.0138
2025-04-08 14:18:12.581263: Training Step 115/354: batchLoss = 0.6707, diffLoss = 3.2878, kgLoss = 0.0164
2025-04-08 14:18:14.220842: Training Step 116/354: batchLoss = 0.6512, diffLoss = 3.1915, kgLoss = 0.0162
2025-04-08 14:18:15.856430: Training Step 117/354: batchLoss = 0.8484, diffLoss = 4.1650, kgLoss = 0.0193
2025-04-08 14:18:17.504104: Training Step 118/354: batchLoss = 0.5380, diffLoss = 2.6398, kgLoss = 0.0125
2025-04-08 14:18:19.150663: Training Step 119/354: batchLoss = 0.6998, diffLoss = 3.4353, kgLoss = 0.0160
2025-04-08 14:18:20.803304: Training Step 120/354: batchLoss = 0.6167, diffLoss = 3.0303, kgLoss = 0.0132
2025-04-08 14:18:22.441305: Training Step 121/354: batchLoss = 0.4775, diffLoss = 2.3359, kgLoss = 0.0128
2025-04-08 14:18:24.062366: Training Step 122/354: batchLoss = 0.5051, diffLoss = 2.4755, kgLoss = 0.0125
2025-04-08 14:18:25.685737: Training Step 123/354: batchLoss = 0.5277, diffLoss = 2.5865, kgLoss = 0.0130
2025-04-08 14:18:27.353046: Training Step 124/354: batchLoss = 0.8298, diffLoss = 4.0784, kgLoss = 0.0177
2025-04-08 14:18:29.000014: Training Step 125/354: batchLoss = 0.5900, diffLoss = 2.8874, kgLoss = 0.0156
2025-04-08 14:18:30.641721: Training Step 126/354: batchLoss = 0.5250, diffLoss = 2.5723, kgLoss = 0.0132
2025-04-08 14:18:32.262139: Training Step 127/354: batchLoss = 0.6546, diffLoss = 3.2117, kgLoss = 0.0153
2025-04-08 14:18:33.892661: Training Step 128/354: batchLoss = 0.5495, diffLoss = 2.6895, kgLoss = 0.0145
2025-04-08 14:18:35.511943: Training Step 129/354: batchLoss = 0.7853, diffLoss = 3.8525, kgLoss = 0.0185
2025-04-08 14:18:37.125661: Training Step 130/354: batchLoss = 0.5649, diffLoss = 2.7672, kgLoss = 0.0143
2025-04-08 14:18:38.752718: Training Step 131/354: batchLoss = 0.6210, diffLoss = 3.0473, kgLoss = 0.0145
2025-04-08 14:18:40.375433: Training Step 132/354: batchLoss = 0.6871, diffLoss = 3.3722, kgLoss = 0.0158
2025-04-08 14:18:42.004343: Training Step 133/354: batchLoss = 0.6339, diffLoss = 3.1127, kgLoss = 0.0142
2025-04-08 14:18:43.624957: Training Step 134/354: batchLoss = 0.5226, diffLoss = 2.5595, kgLoss = 0.0134
2025-04-08 14:18:45.258472: Training Step 135/354: batchLoss = 0.6472, diffLoss = 3.1740, kgLoss = 0.0155
2025-04-08 14:18:46.891529: Training Step 136/354: batchLoss = 0.6508, diffLoss = 3.1846, kgLoss = 0.0174
2025-04-08 14:18:48.509443: Training Step 137/354: batchLoss = 0.5039, diffLoss = 2.4703, kgLoss = 0.0123
2025-04-08 14:18:50.122034: Training Step 138/354: batchLoss = 0.6827, diffLoss = 3.3491, kgLoss = 0.0161
2025-04-08 14:18:51.731773: Training Step 139/354: batchLoss = 0.5359, diffLoss = 2.6247, kgLoss = 0.0138
2025-04-08 14:18:53.338164: Training Step 140/354: batchLoss = 0.6704, diffLoss = 3.2827, kgLoss = 0.0173
2025-04-08 14:18:54.953765: Training Step 141/354: batchLoss = 0.6509, diffLoss = 3.1930, kgLoss = 0.0154
2025-04-08 14:18:56.563927: Training Step 142/354: batchLoss = 0.6467, diffLoss = 3.1697, kgLoss = 0.0159
2025-04-08 14:18:58.177319: Training Step 143/354: batchLoss = 0.4830, diffLoss = 2.3684, kgLoss = 0.0116
2025-04-08 14:18:59.798216: Training Step 144/354: batchLoss = 0.6575, diffLoss = 3.2256, kgLoss = 0.0154
2025-04-08 14:19:01.412644: Training Step 145/354: batchLoss = 0.5066, diffLoss = 2.4828, kgLoss = 0.0126
2025-04-08 14:19:03.030952: Training Step 146/354: batchLoss = 0.6210, diffLoss = 3.0532, kgLoss = 0.0130
2025-04-08 14:19:04.648325: Training Step 147/354: batchLoss = 0.6386, diffLoss = 3.1241, kgLoss = 0.0172
2025-04-08 14:19:06.263416: Training Step 148/354: batchLoss = 0.8051, diffLoss = 3.9535, kgLoss = 0.0181
2025-04-08 14:19:07.877285: Training Step 149/354: batchLoss = 0.5163, diffLoss = 2.5207, kgLoss = 0.0152
2025-04-08 14:19:09.490644: Training Step 150/354: batchLoss = 0.6133, diffLoss = 3.0116, kgLoss = 0.0137
2025-04-08 14:19:11.101496: Training Step 151/354: batchLoss = 0.5846, diffLoss = 2.8719, kgLoss = 0.0128
2025-04-08 14:19:12.726765: Training Step 152/354: batchLoss = 0.4938, diffLoss = 2.4227, kgLoss = 0.0116
2025-04-08 14:19:14.344042: Training Step 153/354: batchLoss = 0.6084, diffLoss = 2.9865, kgLoss = 0.0138
2025-04-08 14:19:15.960931: Training Step 154/354: batchLoss = 0.7333, diffLoss = 3.5985, kgLoss = 0.0170
2025-04-08 14:19:17.593783: Training Step 155/354: batchLoss = 0.5751, diffLoss = 2.8226, kgLoss = 0.0132
2025-04-08 14:19:19.213351: Training Step 156/354: batchLoss = 0.5694, diffLoss = 2.7844, kgLoss = 0.0156
2025-04-08 14:19:20.817328: Training Step 157/354: batchLoss = 0.5252, diffLoss = 2.5798, kgLoss = 0.0116
2025-04-08 14:19:22.427570: Training Step 158/354: batchLoss = 0.4805, diffLoss = 2.3607, kgLoss = 0.0105
2025-04-08 14:19:24.031631: Training Step 159/354: batchLoss = 0.6265, diffLoss = 3.0714, kgLoss = 0.0152
2025-04-08 14:19:25.648225: Training Step 160/354: batchLoss = 0.5169, diffLoss = 2.5340, kgLoss = 0.0126
2025-04-08 14:19:27.260967: Training Step 161/354: batchLoss = 0.6566, diffLoss = 3.2218, kgLoss = 0.0153
2025-04-08 14:19:28.872645: Training Step 162/354: batchLoss = 0.4863, diffLoss = 2.3852, kgLoss = 0.0116
2025-04-08 14:19:30.486042: Training Step 163/354: batchLoss = 0.4467, diffLoss = 2.1939, kgLoss = 0.0099
2025-04-08 14:19:32.104069: Training Step 164/354: batchLoss = 0.5275, diffLoss = 2.5854, kgLoss = 0.0130
2025-04-08 14:19:33.714767: Training Step 165/354: batchLoss = 0.5178, diffLoss = 2.5277, kgLoss = 0.0154
2025-04-08 14:19:35.323553: Training Step 166/354: batchLoss = 0.6491, diffLoss = 3.1824, kgLoss = 0.0158
2025-04-08 14:19:36.934836: Training Step 167/354: batchLoss = 0.6370, diffLoss = 3.1213, kgLoss = 0.0159
2025-04-08 14:19:38.542945: Training Step 168/354: batchLoss = 0.6661, diffLoss = 3.2707, kgLoss = 0.0149
2025-04-08 14:19:40.152359: Training Step 169/354: batchLoss = 0.4871, diffLoss = 2.3896, kgLoss = 0.0115
2025-04-08 14:19:41.767032: Training Step 170/354: batchLoss = 0.5825, diffLoss = 2.8543, kgLoss = 0.0145
2025-04-08 14:19:43.382885: Training Step 171/354: batchLoss = 0.6597, diffLoss = 3.2397, kgLoss = 0.0147
2025-04-08 14:19:44.998966: Training Step 172/354: batchLoss = 0.5555, diffLoss = 2.7190, kgLoss = 0.0146
2025-04-08 14:19:46.610227: Training Step 173/354: batchLoss = 0.5448, diffLoss = 2.6731, kgLoss = 0.0127
2025-04-08 14:19:48.226574: Training Step 174/354: batchLoss = 0.4743, diffLoss = 2.3244, kgLoss = 0.0118
2025-04-08 14:19:49.840286: Training Step 175/354: batchLoss = 0.6150, diffLoss = 3.0215, kgLoss = 0.0133
2025-04-08 14:19:51.444646: Training Step 176/354: batchLoss = 0.6487, diffLoss = 3.1712, kgLoss = 0.0181
2025-04-08 14:19:53.066274: Training Step 177/354: batchLoss = 0.6194, diffLoss = 3.0355, kgLoss = 0.0154
2025-04-08 14:19:54.691818: Training Step 178/354: batchLoss = 0.6424, diffLoss = 3.1582, kgLoss = 0.0134
2025-04-08 14:19:56.308875: Training Step 179/354: batchLoss = 0.5546, diffLoss = 2.7203, kgLoss = 0.0131
2025-04-08 14:19:57.922690: Training Step 180/354: batchLoss = 1.9463, diffLoss = 9.5679, kgLoss = 0.0409
2025-04-08 14:19:59.537904: Training Step 181/354: batchLoss = 0.6318, diffLoss = 3.1012, kgLoss = 0.0144
2025-04-08 14:20:01.148274: Training Step 182/354: batchLoss = 0.6535, diffLoss = 3.2077, kgLoss = 0.0149
2025-04-08 14:20:02.757609: Training Step 183/354: batchLoss = 0.6244, diffLoss = 3.0669, kgLoss = 0.0138
2025-04-08 14:20:04.372711: Training Step 184/354: batchLoss = 0.6844, diffLoss = 3.3614, kgLoss = 0.0152
2025-04-08 14:20:05.978486: Training Step 185/354: batchLoss = 0.5245, diffLoss = 2.5704, kgLoss = 0.0130
2025-04-08 14:20:07.585489: Training Step 186/354: batchLoss = 0.5504, diffLoss = 2.7054, kgLoss = 0.0117
2025-04-08 14:20:09.200589: Training Step 187/354: batchLoss = 0.6292, diffLoss = 3.0783, kgLoss = 0.0169
2025-04-08 14:20:10.819519: Training Step 188/354: batchLoss = 0.5681, diffLoss = 2.7754, kgLoss = 0.0163
2025-04-08 14:20:12.432278: Training Step 189/354: batchLoss = 0.5547, diffLoss = 2.7182, kgLoss = 0.0138
2025-04-08 14:20:14.047590: Training Step 190/354: batchLoss = 0.5929, diffLoss = 2.9125, kgLoss = 0.0130
2025-04-08 14:20:15.661737: Training Step 191/354: batchLoss = 0.6475, diffLoss = 3.1815, kgLoss = 0.0140
2025-04-08 14:20:17.277381: Training Step 192/354: batchLoss = 0.6568, diffLoss = 3.2276, kgLoss = 0.0141
2025-04-08 14:20:18.887970: Training Step 193/354: batchLoss = 0.5865, diffLoss = 2.8745, kgLoss = 0.0145
2025-04-08 14:20:20.494529: Training Step 194/354: batchLoss = 0.5020, diffLoss = 2.4604, kgLoss = 0.0124
2025-04-08 14:20:22.101788: Training Step 195/354: batchLoss = 0.6643, diffLoss = 3.2607, kgLoss = 0.0152
2025-04-08 14:20:23.712342: Training Step 196/354: batchLoss = 0.6356, diffLoss = 3.1228, kgLoss = 0.0139
2025-04-08 14:20:25.327015: Training Step 197/354: batchLoss = 0.5210, diffLoss = 2.5509, kgLoss = 0.0136
2025-04-08 14:20:26.942890: Training Step 198/354: batchLoss = 0.6055, diffLoss = 2.9739, kgLoss = 0.0134
2025-04-08 14:20:28.561017: Training Step 199/354: batchLoss = 0.6288, diffLoss = 3.0885, kgLoss = 0.0138
2025-04-08 14:20:30.179542: Training Step 200/354: batchLoss = 0.5840, diffLoss = 2.8658, kgLoss = 0.0136
2025-04-08 14:20:31.792852: Training Step 201/354: batchLoss = 0.6744, diffLoss = 3.3091, kgLoss = 0.0157
2025-04-08 14:20:33.411585: Training Step 202/354: batchLoss = 0.7484, diffLoss = 3.6674, kgLoss = 0.0187
2025-04-08 14:20:35.017510: Training Step 203/354: batchLoss = 0.6138, diffLoss = 3.0172, kgLoss = 0.0130
2025-04-08 14:20:36.624863: Training Step 204/354: batchLoss = 0.6904, diffLoss = 3.3826, kgLoss = 0.0174
2025-04-08 14:20:38.234912: Training Step 205/354: batchLoss = 0.6784, diffLoss = 3.3306, kgLoss = 0.0153
2025-04-08 14:20:39.852997: Training Step 206/354: batchLoss = 0.5393, diffLoss = 2.6385, kgLoss = 0.0145
2025-04-08 14:20:41.468814: Training Step 207/354: batchLoss = 0.4980, diffLoss = 2.4404, kgLoss = 0.0124
2025-04-08 14:20:43.083845: Training Step 208/354: batchLoss = 0.6626, diffLoss = 3.2514, kgLoss = 0.0154
2025-04-08 14:20:44.706992: Training Step 209/354: batchLoss = 0.5412, diffLoss = 2.6538, kgLoss = 0.0131
2025-04-08 14:20:46.321814: Training Step 210/354: batchLoss = 0.6800, diffLoss = 3.3417, kgLoss = 0.0146
2025-04-08 14:20:47.928752: Training Step 211/354: batchLoss = 0.7605, diffLoss = 3.7345, kgLoss = 0.0169
2025-04-08 14:20:49.535597: Training Step 212/354: batchLoss = 0.6129, diffLoss = 3.0058, kgLoss = 0.0146
2025-04-08 14:20:51.142584: Training Step 213/354: batchLoss = 0.6774, diffLoss = 3.3168, kgLoss = 0.0176
2025-04-08 14:20:52.757164: Training Step 214/354: batchLoss = 0.6250, diffLoss = 3.0704, kgLoss = 0.0137
2025-04-08 14:20:54.369583: Training Step 215/354: batchLoss = 0.5720, diffLoss = 2.8067, kgLoss = 0.0133
2025-04-08 14:20:55.984405: Training Step 216/354: batchLoss = 0.7404, diffLoss = 3.6434, kgLoss = 0.0147
2025-04-08 14:20:57.605359: Training Step 217/354: batchLoss = 0.5746, diffLoss = 2.8109, kgLoss = 0.0155
2025-04-08 14:20:59.218981: Training Step 218/354: batchLoss = 0.6390, diffLoss = 3.1295, kgLoss = 0.0164
2025-04-08 14:21:00.839787: Training Step 219/354: batchLoss = 0.5619, diffLoss = 2.7582, kgLoss = 0.0128
2025-04-08 14:21:02.446603: Training Step 220/354: batchLoss = 0.5718, diffLoss = 2.7981, kgLoss = 0.0153
2025-04-08 14:21:04.050492: Training Step 221/354: batchLoss = 0.6775, diffLoss = 3.3220, kgLoss = 0.0163
2025-04-08 14:21:05.657014: Training Step 222/354: batchLoss = 0.6083, diffLoss = 2.9847, kgLoss = 0.0141
2025-04-08 14:21:07.265820: Training Step 223/354: batchLoss = 0.5754, diffLoss = 2.8225, kgLoss = 0.0137
2025-04-08 14:21:08.878317: Training Step 224/354: batchLoss = 0.6375, diffLoss = 3.1334, kgLoss = 0.0135
2025-04-08 14:21:10.486151: Training Step 225/354: batchLoss = 0.5732, diffLoss = 2.8085, kgLoss = 0.0144
2025-04-08 14:21:12.100341: Training Step 226/354: batchLoss = 0.6556, diffLoss = 3.2197, kgLoss = 0.0146
2025-04-08 14:21:13.702889: Training Step 227/354: batchLoss = 0.5333, diffLoss = 2.6136, kgLoss = 0.0133
2025-04-08 14:21:15.319502: Training Step 228/354: batchLoss = 0.7552, diffLoss = 3.7093, kgLoss = 0.0167
2025-04-08 14:21:16.929803: Training Step 229/354: batchLoss = 0.5660, diffLoss = 2.7747, kgLoss = 0.0138
2025-04-08 14:21:18.533862: Training Step 230/354: batchLoss = 0.6013, diffLoss = 2.9531, kgLoss = 0.0133
2025-04-08 14:21:20.137821: Training Step 231/354: batchLoss = 0.5796, diffLoss = 2.8354, kgLoss = 0.0156
2025-04-08 14:21:21.740572: Training Step 232/354: batchLoss = 0.6113, diffLoss = 2.9992, kgLoss = 0.0143
2025-04-08 14:21:23.349841: Training Step 233/354: batchLoss = 0.4989, diffLoss = 2.4388, kgLoss = 0.0139
2025-04-08 14:21:24.952742: Training Step 234/354: batchLoss = 0.6351, diffLoss = 3.1168, kgLoss = 0.0146
2025-04-08 14:21:26.568958: Training Step 235/354: batchLoss = 0.4980, diffLoss = 2.4326, kgLoss = 0.0143
2025-04-08 14:21:28.179417: Training Step 236/354: batchLoss = 0.5385, diffLoss = 2.6286, kgLoss = 0.0159
2025-04-08 14:21:29.790258: Training Step 237/354: batchLoss = 0.5138, diffLoss = 2.5103, kgLoss = 0.0146
2025-04-08 14:21:31.401011: Training Step 238/354: batchLoss = 0.6263, diffLoss = 3.0773, kgLoss = 0.0135
2025-04-08 14:21:33.004592: Training Step 239/354: batchLoss = 0.8939, diffLoss = 4.3861, kgLoss = 0.0208
2025-04-08 14:21:34.616027: Training Step 240/354: batchLoss = 0.5779, diffLoss = 2.8356, kgLoss = 0.0134
2025-04-08 14:21:36.216679: Training Step 241/354: batchLoss = 0.5370, diffLoss = 2.6283, kgLoss = 0.0141
2025-04-08 14:21:37.823556: Training Step 242/354: batchLoss = 0.6738, diffLoss = 3.3071, kgLoss = 0.0155
2025-04-08 14:21:39.427867: Training Step 243/354: batchLoss = 0.6007, diffLoss = 2.9508, kgLoss = 0.0132
2025-04-08 14:21:41.038660: Training Step 244/354: batchLoss = 0.5162, diffLoss = 2.5255, kgLoss = 0.0138
2025-04-08 14:21:42.646005: Training Step 245/354: batchLoss = 0.6775, diffLoss = 3.3257, kgLoss = 0.0154
2025-04-08 14:21:44.258227: Training Step 246/354: batchLoss = 0.6352, diffLoss = 3.1175, kgLoss = 0.0146
2025-04-08 14:21:45.866711: Training Step 247/354: batchLoss = 0.6771, diffLoss = 3.3190, kgLoss = 0.0167
2025-04-08 14:21:47.477613: Training Step 248/354: batchLoss = 0.5305, diffLoss = 2.5993, kgLoss = 0.0133
2025-04-08 14:21:49.078953: Training Step 249/354: batchLoss = 0.5906, diffLoss = 2.8990, kgLoss = 0.0135
2025-04-08 14:21:50.689070: Training Step 250/354: batchLoss = 0.6053, diffLoss = 2.9665, kgLoss = 0.0150
2025-04-08 14:21:52.296710: Training Step 251/354: batchLoss = 0.5132, diffLoss = 2.5170, kgLoss = 0.0122
2025-04-08 14:21:53.903475: Training Step 252/354: batchLoss = 0.8766, diffLoss = 4.3020, kgLoss = 0.0202
2025-04-08 14:21:55.511794: Training Step 253/354: batchLoss = 0.5665, diffLoss = 2.7797, kgLoss = 0.0133
2025-04-08 14:21:57.118703: Training Step 254/354: batchLoss = 0.5611, diffLoss = 2.7519, kgLoss = 0.0134
2025-04-08 14:21:58.737099: Training Step 255/354: batchLoss = 0.5774, diffLoss = 2.8235, kgLoss = 0.0159
2025-04-08 14:22:00.343186: Training Step 256/354: batchLoss = 0.6438, diffLoss = 3.1561, kgLoss = 0.0157
2025-04-08 14:22:01.955401: Training Step 257/354: batchLoss = 0.6457, diffLoss = 3.1732, kgLoss = 0.0138
2025-04-08 14:22:03.561252: Training Step 258/354: batchLoss = 0.5579, diffLoss = 2.7396, kgLoss = 0.0125
2025-04-08 14:22:05.166800: Training Step 259/354: batchLoss = 0.5455, diffLoss = 2.6747, kgLoss = 0.0132
2025-04-08 14:22:06.783936: Training Step 260/354: batchLoss = 0.5205, diffLoss = 2.5531, kgLoss = 0.0123
2025-04-08 14:22:08.401138: Training Step 261/354: batchLoss = 0.6537, diffLoss = 3.2123, kgLoss = 0.0140
2025-04-08 14:22:10.016315: Training Step 262/354: batchLoss = 0.5633, diffLoss = 2.7553, kgLoss = 0.0152
2025-04-08 14:22:11.635981: Training Step 263/354: batchLoss = 0.6028, diffLoss = 2.9610, kgLoss = 0.0133
2025-04-08 14:22:13.262674: Training Step 264/354: batchLoss = 0.5719, diffLoss = 2.8106, kgLoss = 0.0123
2025-04-08 14:22:14.881811: Training Step 265/354: batchLoss = 0.6774, diffLoss = 3.3291, kgLoss = 0.0145
2025-04-08 14:22:16.485114: Training Step 266/354: batchLoss = 0.5738, diffLoss = 2.8103, kgLoss = 0.0147
2025-04-08 14:22:18.089549: Training Step 267/354: batchLoss = 0.5389, diffLoss = 2.6451, kgLoss = 0.0124
2025-04-08 14:22:19.700487: Training Step 268/354: batchLoss = 0.5867, diffLoss = 2.8765, kgLoss = 0.0143
2025-04-08 14:22:21.327890: Training Step 269/354: batchLoss = 0.5248, diffLoss = 2.5739, kgLoss = 0.0125
2025-04-08 14:22:22.944145: Training Step 270/354: batchLoss = 0.5781, diffLoss = 2.8245, kgLoss = 0.0166
2025-04-08 14:22:24.565509: Training Step 271/354: batchLoss = 0.8086, diffLoss = 3.9706, kgLoss = 0.0181
2025-04-08 14:22:26.190411: Training Step 272/354: batchLoss = 0.5937, diffLoss = 2.9095, kgLoss = 0.0147
2025-04-08 14:22:27.803449: Training Step 273/354: batchLoss = 0.4921, diffLoss = 2.4120, kgLoss = 0.0121
2025-04-08 14:22:29.413688: Training Step 274/354: batchLoss = 0.5402, diffLoss = 2.6458, kgLoss = 0.0138
2025-04-08 14:22:31.022814: Training Step 275/354: batchLoss = 0.5488, diffLoss = 2.6907, kgLoss = 0.0134
2025-04-08 14:22:32.621739: Training Step 276/354: batchLoss = 0.6354, diffLoss = 3.1147, kgLoss = 0.0156
2025-04-08 14:22:34.229786: Training Step 277/354: batchLoss = 0.5703, diffLoss = 2.7906, kgLoss = 0.0152
2025-04-08 14:22:35.844976: Training Step 278/354: batchLoss = 0.5881, diffLoss = 2.8789, kgLoss = 0.0153
2025-04-08 14:22:37.466986: Training Step 279/354: batchLoss = 0.4776, diffLoss = 2.3420, kgLoss = 0.0115
2025-04-08 14:22:39.088465: Training Step 280/354: batchLoss = 0.5465, diffLoss = 2.6768, kgLoss = 0.0140
2025-04-08 14:22:40.707104: Training Step 281/354: batchLoss = 0.5503, diffLoss = 2.6995, kgLoss = 0.0130
2025-04-08 14:22:42.324849: Training Step 282/354: batchLoss = 0.6185, diffLoss = 3.0311, kgLoss = 0.0153
2025-04-08 14:22:43.941533: Training Step 283/354: batchLoss = 0.5366, diffLoss = 2.6317, kgLoss = 0.0128
2025-04-08 14:22:45.547225: Training Step 284/354: batchLoss = 0.5277, diffLoss = 2.5901, kgLoss = 0.0121
2025-04-08 14:22:47.155793: Training Step 285/354: batchLoss = 0.5561, diffLoss = 2.7298, kgLoss = 0.0127
2025-04-08 14:22:48.757448: Training Step 286/354: batchLoss = 0.4982, diffLoss = 2.4457, kgLoss = 0.0114
2025-04-08 14:22:50.371185: Training Step 287/354: batchLoss = 0.5322, diffLoss = 2.6106, kgLoss = 0.0126
2025-04-08 14:22:52.006773: Training Step 288/354: batchLoss = 0.6339, diffLoss = 3.1107, kgLoss = 0.0147
2025-04-08 14:22:53.644682: Training Step 289/354: batchLoss = 0.6588, diffLoss = 3.2393, kgLoss = 0.0137
2025-04-08 14:22:55.270278: Training Step 290/354: batchLoss = 0.5084, diffLoss = 2.4898, kgLoss = 0.0130
2025-04-08 14:22:56.914115: Training Step 291/354: batchLoss = 0.6492, diffLoss = 3.1811, kgLoss = 0.0162
2025-04-08 14:22:58.537507: Training Step 292/354: batchLoss = 0.6312, diffLoss = 3.1022, kgLoss = 0.0135
2025-04-08 14:23:00.156700: Training Step 293/354: batchLoss = 0.4828, diffLoss = 2.3680, kgLoss = 0.0114
2025-04-08 14:23:01.777059: Training Step 294/354: batchLoss = 0.5994, diffLoss = 2.9385, kgLoss = 0.0147
2025-04-08 14:23:03.412951: Training Step 295/354: batchLoss = 0.6781, diffLoss = 3.3331, kgLoss = 0.0144
2025-04-08 14:23:05.056630: Training Step 296/354: batchLoss = 0.5254, diffLoss = 2.5779, kgLoss = 0.0122
2025-04-08 14:23:06.668459: Training Step 297/354: batchLoss = 0.5279, diffLoss = 2.5820, kgLoss = 0.0143
2025-04-08 14:23:08.282753: Training Step 298/354: batchLoss = 0.4738, diffLoss = 2.3200, kgLoss = 0.0122
2025-04-08 14:23:09.893496: Training Step 299/354: batchLoss = 0.5795, diffLoss = 2.8452, kgLoss = 0.0130
2025-04-08 14:23:11.502979: Training Step 300/354: batchLoss = 0.5772, diffLoss = 2.8324, kgLoss = 0.0134
2025-04-08 14:23:13.135971: Training Step 301/354: batchLoss = 0.6615, diffLoss = 3.2461, kgLoss = 0.0153
2025-04-08 14:23:14.741849: Training Step 302/354: batchLoss = 0.6318, diffLoss = 3.1035, kgLoss = 0.0139
2025-04-08 14:23:16.351296: Training Step 303/354: batchLoss = 0.4712, diffLoss = 2.3086, kgLoss = 0.0119
2025-04-08 14:23:17.973249: Training Step 304/354: batchLoss = 0.6305, diffLoss = 3.0919, kgLoss = 0.0152
2025-04-08 14:23:19.583001: Training Step 305/354: batchLoss = 0.4942, diffLoss = 2.4125, kgLoss = 0.0147
2025-04-08 14:23:21.201914: Training Step 306/354: batchLoss = 0.7329, diffLoss = 3.6013, kgLoss = 0.0158
2025-04-08 14:23:22.810042: Training Step 307/354: batchLoss = 0.5958, diffLoss = 2.9223, kgLoss = 0.0142
2025-04-08 14:23:24.424614: Training Step 308/354: batchLoss = 0.5590, diffLoss = 2.7361, kgLoss = 0.0148
2025-04-08 14:23:26.032159: Training Step 309/354: batchLoss = 0.7162, diffLoss = 3.5045, kgLoss = 0.0192
2025-04-08 14:23:27.645129: Training Step 310/354: batchLoss = 0.5589, diffLoss = 2.7450, kgLoss = 0.0124
2025-04-08 14:23:29.259122: Training Step 311/354: batchLoss = 0.6902, diffLoss = 3.3897, kgLoss = 0.0153
2025-04-08 14:23:30.873090: Training Step 312/354: batchLoss = 0.5621, diffLoss = 2.7509, kgLoss = 0.0149
2025-04-08 14:23:32.482218: Training Step 313/354: batchLoss = 0.6063, diffLoss = 2.9729, kgLoss = 0.0147
2025-04-08 14:23:34.094639: Training Step 314/354: batchLoss = 0.5106, diffLoss = 2.5039, kgLoss = 0.0123
2025-04-08 14:23:35.720163: Training Step 315/354: batchLoss = 0.6072, diffLoss = 2.9791, kgLoss = 0.0142
2025-04-08 14:23:37.345546: Training Step 316/354: batchLoss = 0.6126, diffLoss = 3.0047, kgLoss = 0.0146
2025-04-08 14:23:38.965908: Training Step 317/354: batchLoss = 0.5474, diffLoss = 2.6817, kgLoss = 0.0138
2025-04-08 14:23:40.579577: Training Step 318/354: batchLoss = 0.5060, diffLoss = 2.4760, kgLoss = 0.0135
2025-04-08 14:23:42.189141: Training Step 319/354: batchLoss = 0.6782, diffLoss = 3.3296, kgLoss = 0.0154
2025-04-08 14:23:43.800651: Training Step 320/354: batchLoss = 0.6806, diffLoss = 3.3406, kgLoss = 0.0156
2025-04-08 14:23:45.411869: Training Step 321/354: batchLoss = 0.7369, diffLoss = 3.6216, kgLoss = 0.0158
2025-04-08 14:23:47.024898: Training Step 322/354: batchLoss = 1.0345, diffLoss = 5.0870, kgLoss = 0.0214
2025-04-08 14:23:48.632851: Training Step 323/354: batchLoss = 0.5877, diffLoss = 2.8878, kgLoss = 0.0127
2025-04-08 14:23:50.248039: Training Step 324/354: batchLoss = 0.5556, diffLoss = 2.7297, kgLoss = 0.0121
2025-04-08 14:23:51.875114: Training Step 325/354: batchLoss = 0.5969, diffLoss = 2.9316, kgLoss = 0.0132
2025-04-08 14:23:53.491754: Training Step 326/354: batchLoss = 0.5327, diffLoss = 2.6195, kgLoss = 0.0110
2025-04-08 14:23:55.106214: Training Step 327/354: batchLoss = 0.6065, diffLoss = 2.9704, kgLoss = 0.0156
2025-04-08 14:23:56.719528: Training Step 328/354: batchLoss = 0.5742, diffLoss = 2.8138, kgLoss = 0.0142
2025-04-08 14:23:58.328184: Training Step 329/354: batchLoss = 0.5279, diffLoss = 2.5867, kgLoss = 0.0132
2025-04-08 14:23:59.936649: Training Step 330/354: batchLoss = 0.6989, diffLoss = 3.4359, kgLoss = 0.0147
2025-04-08 14:24:01.546405: Training Step 331/354: batchLoss = 0.6539, diffLoss = 3.1996, kgLoss = 0.0175
2025-04-08 14:24:03.162886: Training Step 332/354: batchLoss = 0.6436, diffLoss = 3.1581, kgLoss = 0.0150
2025-04-08 14:24:04.777268: Training Step 333/354: batchLoss = 0.7360, diffLoss = 3.6129, kgLoss = 0.0167
2025-04-08 14:24:06.385605: Training Step 334/354: batchLoss = 0.4785, diffLoss = 2.3482, kgLoss = 0.0111
2025-04-08 14:24:07.995853: Training Step 335/354: batchLoss = 0.6895, diffLoss = 3.3870, kgLoss = 0.0151
2025-04-08 14:24:09.606447: Training Step 336/354: batchLoss = 0.7153, diffLoss = 3.5131, kgLoss = 0.0159
2025-04-08 14:24:11.214205: Training Step 337/354: batchLoss = 0.6481, diffLoss = 3.1805, kgLoss = 0.0150
2025-04-08 14:24:12.835485: Training Step 338/354: batchLoss = 0.7671, diffLoss = 3.7585, kgLoss = 0.0193
2025-04-08 14:24:14.440417: Training Step 339/354: batchLoss = 0.8371, diffLoss = 4.1126, kgLoss = 0.0183
2025-04-08 14:24:16.051784: Training Step 340/354: batchLoss = 0.5153, diffLoss = 2.5306, kgLoss = 0.0115
2025-04-08 14:24:17.672895: Training Step 341/354: batchLoss = 0.5551, diffLoss = 2.7191, kgLoss = 0.0141
2025-04-08 14:24:19.283565: Training Step 342/354: batchLoss = 0.6104, diffLoss = 2.9998, kgLoss = 0.0130
2025-04-08 14:24:20.895358: Training Step 343/354: batchLoss = 0.5905, diffLoss = 2.9015, kgLoss = 0.0128
2025-04-08 14:24:22.531928: Training Step 344/354: batchLoss = 0.5371, diffLoss = 2.6352, kgLoss = 0.0126
2025-04-08 14:24:24.140756: Training Step 345/354: batchLoss = 0.6006, diffLoss = 2.9423, kgLoss = 0.0152
2025-04-08 14:24:25.759621: Training Step 346/354: batchLoss = 0.6579, diffLoss = 3.2310, kgLoss = 0.0146
2025-04-08 14:24:27.377796: Training Step 347/354: batchLoss = 0.6941, diffLoss = 3.4014, kgLoss = 0.0172
2025-04-08 14:24:28.990327: Training Step 348/354: batchLoss = 0.5734, diffLoss = 2.8080, kgLoss = 0.0147
2025-04-08 14:24:30.595592: Training Step 349/354: batchLoss = 0.6442, diffLoss = 3.1559, kgLoss = 0.0163
2025-04-08 14:24:32.205508: Training Step 350/354: batchLoss = 0.6856, diffLoss = 3.3637, kgLoss = 0.0161
2025-04-08 14:24:33.815121: Training Step 351/354: batchLoss = 0.6055, diffLoss = 2.9616, kgLoss = 0.0165
2025-04-08 14:24:35.404398: Training Step 352/354: batchLoss = 0.6755, diffLoss = 3.3095, kgLoss = 0.0170
2025-04-08 14:24:36.796920: Training Step 353/354: batchLoss = 0.4676, diffLoss = 2.2880, kgLoss = 0.0124
2025-04-08 14:24:36.882014: 
2025-04-08 14:24:36.882659: Epoch 8/1000, Train: epLoss = 1.0886, epDfLoss = 5.3396, epfTransLoss = 0.0000, epKgLoss = 0.0259  
2025-04-08 14:24:38.204585: Steps 0/138: batch_recall = 47.54, batch_ndcg = 26.38 
2025-04-08 14:24:39.514967: Steps 1/138: batch_recall = 45.95, batch_ndcg = 27.30 
2025-04-08 14:24:40.806868: Steps 2/138: batch_recall = 57.54, batch_ndcg = 35.28 
2025-04-08 14:24:42.117070: Steps 3/138: batch_recall = 55.56, batch_ndcg = 31.67 
2025-04-08 14:24:43.409420: Steps 4/138: batch_recall = 64.08, batch_ndcg = 40.29 
2025-04-08 14:24:44.696595: Steps 5/138: batch_recall = 57.39, batch_ndcg = 29.97 
2025-04-08 14:24:45.994213: Steps 6/138: batch_recall = 48.25, batch_ndcg = 28.08 
2025-04-08 14:24:47.277891: Steps 7/138: batch_recall = 60.53, batch_ndcg = 37.85 
2025-04-08 14:24:48.578340: Steps 8/138: batch_recall = 61.46, batch_ndcg = 38.55 
2025-04-08 14:24:49.873115: Steps 9/138: batch_recall = 52.40, batch_ndcg = 32.18 
2025-04-08 14:24:51.171304: Steps 10/138: batch_recall = 49.55, batch_ndcg = 28.18 
2025-04-08 14:24:52.464036: Steps 11/138: batch_recall = 60.78, batch_ndcg = 35.07 
2025-04-08 14:24:53.744493: Steps 12/138: batch_recall = 52.91, batch_ndcg = 29.22 
2025-04-08 14:24:55.026762: Steps 13/138: batch_recall = 51.71, batch_ndcg = 28.97 
2025-04-08 14:24:56.310416: Steps 14/138: batch_recall = 52.03, batch_ndcg = 29.55 
2025-04-08 14:24:57.620618: Steps 15/138: batch_recall = 52.13, batch_ndcg = 30.38 
2025-04-08 14:24:58.903939: Steps 16/138: batch_recall = 59.80, batch_ndcg = 31.98 
2025-04-08 14:25:00.210024: Steps 17/138: batch_recall = 55.41, batch_ndcg = 30.86 
2025-04-08 14:25:01.509106: Steps 18/138: batch_recall = 48.94, batch_ndcg = 30.93 
2025-04-08 14:25:02.803792: Steps 19/138: batch_recall = 50.99, batch_ndcg = 31.34 
2025-04-08 14:25:04.094175: Steps 20/138: batch_recall = 59.87, batch_ndcg = 33.81 
2025-04-08 14:25:05.391392: Steps 21/138: batch_recall = 61.88, batch_ndcg = 37.53 
2025-04-08 14:25:06.682069: Steps 22/138: batch_recall = 55.36, batch_ndcg = 32.07 
2025-04-08 14:25:07.969840: Steps 23/138: batch_recall = 51.68, batch_ndcg = 29.12 
2025-04-08 14:25:09.260063: Steps 24/138: batch_recall = 49.84, batch_ndcg = 28.85 
2025-04-08 14:25:10.543955: Steps 25/138: batch_recall = 57.03, batch_ndcg = 33.06 
2025-04-08 14:25:11.823840: Steps 26/138: batch_recall = 58.32, batch_ndcg = 33.95 
2025-04-08 14:25:13.106712: Steps 27/138: batch_recall = 64.35, batch_ndcg = 34.89 
2025-04-08 14:25:14.386473: Steps 28/138: batch_recall = 54.41, batch_ndcg = 30.80 
2025-04-08 14:25:15.658650: Steps 29/138: batch_recall = 60.03, batch_ndcg = 31.04 
2025-04-08 14:25:16.959287: Steps 30/138: batch_recall = 54.21, batch_ndcg = 33.43 
2025-04-08 14:25:18.233901: Steps 31/138: batch_recall = 40.35, batch_ndcg = 23.43 
2025-04-08 14:25:19.515660: Steps 32/138: batch_recall = 49.17, batch_ndcg = 30.32 
2025-04-08 14:25:20.799577: Steps 33/138: batch_recall = 57.06, batch_ndcg = 33.25 
2025-04-08 14:25:22.085490: Steps 34/138: batch_recall = 49.70, batch_ndcg = 27.62 
2025-04-08 14:25:23.370384: Steps 35/138: batch_recall = 49.52, batch_ndcg = 29.34 
2025-04-08 14:25:24.656737: Steps 36/138: batch_recall = 50.71, batch_ndcg = 27.96 
2025-04-08 14:25:25.939314: Steps 37/138: batch_recall = 58.96, batch_ndcg = 33.37 
2025-04-08 14:25:27.208690: Steps 38/138: batch_recall = 58.05, batch_ndcg = 31.60 
2025-04-08 14:25:28.480117: Steps 39/138: batch_recall = 72.79, batch_ndcg = 37.21 
2025-04-08 14:25:29.761700: Steps 40/138: batch_recall = 51.14, batch_ndcg = 27.74 
2025-04-08 14:25:31.031323: Steps 41/138: batch_recall = 59.48, batch_ndcg = 32.98 
2025-04-08 14:25:32.301097: Steps 42/138: batch_recall = 54.60, batch_ndcg = 30.55 
2025-04-08 14:25:33.589082: Steps 43/138: batch_recall = 54.71, batch_ndcg = 32.43 
2025-04-08 14:25:34.856696: Steps 44/138: batch_recall = 51.55, batch_ndcg = 27.01 
2025-04-08 14:25:36.131836: Steps 45/138: batch_recall = 58.15, batch_ndcg = 32.00 
2025-04-08 14:25:37.391815: Steps 46/138: batch_recall = 57.62, batch_ndcg = 33.28 
2025-04-08 14:25:38.688523: Steps 47/138: batch_recall = 58.68, batch_ndcg = 32.37 
2025-04-08 14:25:39.970020: Steps 48/138: batch_recall = 53.51, batch_ndcg = 31.42 
2025-04-08 14:25:41.264218: Steps 49/138: batch_recall = 61.63, batch_ndcg = 37.21 
2025-04-08 14:25:42.547451: Steps 50/138: batch_recall = 57.02, batch_ndcg = 32.47 
2025-04-08 14:25:43.826239: Steps 51/138: batch_recall = 62.35, batch_ndcg = 36.47 
2025-04-08 14:25:45.086516: Steps 52/138: batch_recall = 65.93, batch_ndcg = 40.61 
2025-04-08 14:25:46.367659: Steps 53/138: batch_recall = 57.95, batch_ndcg = 31.00 
2025-04-08 14:25:47.636054: Steps 54/138: batch_recall = 65.24, batch_ndcg = 36.48 
2025-04-08 14:25:48.911616: Steps 55/138: batch_recall = 55.77, batch_ndcg = 32.14 
2025-04-08 14:25:50.201545: Steps 56/138: batch_recall = 63.61, batch_ndcg = 34.70 
2025-04-08 14:25:51.507542: Steps 57/138: batch_recall = 59.24, batch_ndcg = 33.20 
2025-04-08 14:25:52.807954: Steps 58/138: batch_recall = 68.21, batch_ndcg = 35.67 
2025-04-08 14:25:54.102896: Steps 59/138: batch_recall = 75.35, batch_ndcg = 41.89 
2025-04-08 14:25:55.397327: Steps 60/138: batch_recall = 69.55, batch_ndcg = 39.68 
2025-04-08 14:25:56.687598: Steps 61/138: batch_recall = 58.15, batch_ndcg = 33.32 
2025-04-08 14:25:57.954886: Steps 62/138: batch_recall = 79.51, batch_ndcg = 42.97 
2025-04-08 14:25:59.235243: Steps 63/138: batch_recall = 75.46, batch_ndcg = 43.34 
2025-04-08 14:26:00.503804: Steps 64/138: batch_recall = 61.35, batch_ndcg = 32.84 
2025-04-08 14:26:01.840083: Steps 65/138: batch_recall = 80.21, batch_ndcg = 45.68 
2025-04-08 14:26:03.137480: Steps 66/138: batch_recall = 65.99, batch_ndcg = 39.85 
2025-04-08 14:26:04.436628: Steps 67/138: batch_recall = 80.65, batch_ndcg = 48.00 
2025-04-08 14:26:05.703219: Steps 68/138: batch_recall = 65.94, batch_ndcg = 34.28 
2025-04-08 14:26:06.969953: Steps 69/138: batch_recall = 85.82, batch_ndcg = 49.03 
2025-04-08 14:26:08.251579: Steps 70/138: batch_recall = 78.73, batch_ndcg = 44.22 
2025-04-08 14:26:09.520613: Steps 71/138: batch_recall = 85.79, batch_ndcg = 49.50 
2025-04-08 14:26:10.785533: Steps 72/138: batch_recall = 86.13, batch_ndcg = 50.28 
2025-04-08 14:26:12.041671: Steps 73/138: batch_recall = 90.32, batch_ndcg = 47.96 
2025-04-08 14:26:13.297626: Steps 74/138: batch_recall = 82.84, batch_ndcg = 48.54 
2025-04-08 14:26:14.564429: Steps 75/138: batch_recall = 86.54, batch_ndcg = 48.51 
2025-04-08 14:26:15.833451: Steps 76/138: batch_recall = 97.19, batch_ndcg = 55.61 
2025-04-08 14:26:17.115213: Steps 77/138: batch_recall = 85.52, batch_ndcg = 47.52 
2025-04-08 14:26:18.389279: Steps 78/138: batch_recall = 98.42, batch_ndcg = 51.01 
2025-04-08 14:26:19.672616: Steps 79/138: batch_recall = 89.53, batch_ndcg = 47.10 
2025-04-08 14:26:20.939805: Steps 80/138: batch_recall = 75.32, batch_ndcg = 40.15 
2025-04-08 14:26:22.216714: Steps 81/138: batch_recall = 79.59, batch_ndcg = 47.40 
2025-04-08 14:26:23.511760: Steps 82/138: batch_recall = 87.30, batch_ndcg = 49.78 
2025-04-08 14:26:24.788840: Steps 83/138: batch_recall = 84.14, batch_ndcg = 47.28 
2025-04-08 14:26:26.051128: Steps 84/138: batch_recall = 104.07, batch_ndcg = 55.42 
2025-04-08 14:26:27.304963: Steps 85/138: batch_recall = 97.85, batch_ndcg = 55.17 
2025-04-08 14:26:28.574308: Steps 86/138: batch_recall = 116.06, batch_ndcg = 70.92 
2025-04-08 14:26:29.839443: Steps 87/138: batch_recall = 107.50, batch_ndcg = 59.33 
2025-04-08 14:26:31.109260: Steps 88/138: batch_recall = 100.61, batch_ndcg = 55.52 
2025-04-08 14:26:32.404758: Steps 89/138: batch_recall = 119.57, batch_ndcg = 65.03 
2025-04-08 14:26:33.670169: Steps 90/138: batch_recall = 105.84, batch_ndcg = 57.98 
2025-04-08 14:26:34.952058: Steps 91/138: batch_recall = 120.34, batch_ndcg = 65.67 
2025-04-08 14:26:36.237968: Steps 92/138: batch_recall = 110.73, batch_ndcg = 61.29 
2025-04-08 14:26:37.501733: Steps 93/138: batch_recall = 116.45, batch_ndcg = 67.67 
2025-04-08 14:26:38.768398: Steps 94/138: batch_recall = 117.98, batch_ndcg = 61.08 
2025-04-08 14:26:40.057452: Steps 95/138: batch_recall = 112.71, batch_ndcg = 66.41 
2025-04-08 14:26:41.320444: Steps 96/138: batch_recall = 130.55, batch_ndcg = 71.79 
2025-04-08 14:26:42.579545: Steps 97/138: batch_recall = 134.27, batch_ndcg = 74.32 
2025-04-08 14:26:43.838627: Steps 98/138: batch_recall = 103.57, batch_ndcg = 60.96 
2025-04-08 14:26:45.093068: Steps 99/138: batch_recall = 125.93, batch_ndcg = 67.34 
2025-04-08 14:26:46.363260: Steps 100/138: batch_recall = 120.97, batch_ndcg = 69.80 
2025-04-08 14:26:47.645638: Steps 101/138: batch_recall = 126.31, batch_ndcg = 68.59 
2025-04-08 14:26:48.927898: Steps 102/138: batch_recall = 125.74, batch_ndcg = 74.17 
2025-04-08 14:26:50.198219: Steps 103/138: batch_recall = 137.91, batch_ndcg = 78.59 
2025-04-08 14:26:51.472125: Steps 104/138: batch_recall = 139.48, batch_ndcg = 75.80 
2025-04-08 14:26:52.748991: Steps 105/138: batch_recall = 116.21, batch_ndcg = 66.72 
2025-04-08 14:26:54.036546: Steps 106/138: batch_recall = 107.15, batch_ndcg = 59.35 
2025-04-08 14:26:55.313570: Steps 107/138: batch_recall = 115.09, batch_ndcg = 62.88 
2025-04-08 14:26:56.561280: Steps 108/138: batch_recall = 120.81, batch_ndcg = 71.62 
2025-04-08 14:26:57.825782: Steps 109/138: batch_recall = 134.66, batch_ndcg = 73.30 
2025-04-08 14:26:59.087511: Steps 110/138: batch_recall = 119.73, batch_ndcg = 63.99 
2025-04-08 14:27:00.359191: Steps 111/138: batch_recall = 136.96, batch_ndcg = 83.32 
2025-04-08 14:27:01.657836: Steps 112/138: batch_recall = 149.77, batch_ndcg = 81.55 
2025-04-08 14:27:02.941163: Steps 113/138: batch_recall = 124.50, batch_ndcg = 70.31 
2025-04-08 14:27:04.238968: Steps 114/138: batch_recall = 116.99, batch_ndcg = 67.79 
2025-04-08 14:27:05.517474: Steps 115/138: batch_recall = 117.94, batch_ndcg = 61.01 
2025-04-08 14:27:06.795440: Steps 116/138: batch_recall = 124.40, batch_ndcg = 68.18 
2025-04-08 14:27:08.088017: Steps 117/138: batch_recall = 117.70, batch_ndcg = 67.46 
2025-04-08 14:27:09.369684: Steps 118/138: batch_recall = 123.36, batch_ndcg = 69.72 
2025-04-08 14:27:10.636863: Steps 119/138: batch_recall = 137.61, batch_ndcg = 71.98 
2025-04-08 14:27:11.886332: Steps 120/138: batch_recall = 119.55, batch_ndcg = 65.78 
2025-04-08 14:27:13.131081: Steps 121/138: batch_recall = 143.41, batch_ndcg = 76.70 
2025-04-08 14:27:14.380678: Steps 122/138: batch_recall = 140.40, batch_ndcg = 77.15 
2025-04-08 14:27:15.648370: Steps 123/138: batch_recall = 129.04, batch_ndcg = 71.39 
2025-04-08 14:27:16.920230: Steps 124/138: batch_recall = 151.23, batch_ndcg = 93.44 
2025-04-08 14:27:18.182382: Steps 125/138: batch_recall = 126.17, batch_ndcg = 69.19 
2025-04-08 14:27:19.451508: Steps 126/138: batch_recall = 155.09, batch_ndcg = 88.19 
2025-04-08 14:27:20.733670: Steps 127/138: batch_recall = 139.95, batch_ndcg = 81.13 
2025-04-08 14:27:22.004461: Steps 128/138: batch_recall = 124.35, batch_ndcg = 67.79 
2025-04-08 14:27:23.266005: Steps 129/138: batch_recall = 154.90, batch_ndcg = 88.94 
2025-04-08 14:27:24.529825: Steps 130/138: batch_recall = 134.86, batch_ndcg = 69.31 
2025-04-08 14:27:25.802440: Steps 131/138: batch_recall = 143.95, batch_ndcg = 82.07 
2025-04-08 14:27:27.097408: Steps 132/138: batch_recall = 152.40, batch_ndcg = 85.60 
2025-04-08 14:27:28.388203: Steps 133/138: batch_recall = 140.52, batch_ndcg = 81.24 
2025-04-08 14:27:29.691883: Steps 134/138: batch_recall = 136.88, batch_ndcg = 76.28 
2025-04-08 14:27:30.994243: Steps 135/138: batch_recall = 161.19, batch_ndcg = 91.04 
2025-04-08 14:27:32.297223: Steps 136/138: batch_recall = 146.20, batch_ndcg = 77.13 
2025-04-08 14:27:33.568306: Steps 137/138: batch_recall = 136.82, batch_ndcg = 85.21 
2025-04-08 14:27:33.568892: Epoch 8/1000, Test: Recall = 0.1727, NDCG = 0.0973  

2025-04-08 14:27:35.341135: Training Step 0/354: batchLoss = 0.5829, diffLoss = 2.8547, kgLoss = 0.0149
2025-04-08 14:27:36.959094: Training Step 1/354: batchLoss = 0.6137, diffLoss = 3.0106, kgLoss = 0.0144
2025-04-08 14:27:38.585664: Training Step 2/354: batchLoss = 0.6587, diffLoss = 3.2328, kgLoss = 0.0152
2025-04-08 14:27:40.245115: Training Step 3/354: batchLoss = 0.6185, diffLoss = 3.0325, kgLoss = 0.0151
2025-04-08 14:27:41.866659: Training Step 4/354: batchLoss = 0.4993, diffLoss = 2.4489, kgLoss = 0.0119
2025-04-08 14:27:43.472815: Training Step 5/354: batchLoss = 0.5054, diffLoss = 2.4790, kgLoss = 0.0120
2025-04-08 14:27:45.080230: Training Step 6/354: batchLoss = 0.4508, diffLoss = 2.2089, kgLoss = 0.0113
2025-04-08 14:27:46.695260: Training Step 7/354: batchLoss = 0.7282, diffLoss = 3.5778, kgLoss = 0.0158
2025-04-08 14:27:48.317824: Training Step 8/354: batchLoss = 1.3274, diffLoss = 6.5077, kgLoss = 0.0323
2025-04-08 14:27:49.951075: Training Step 9/354: batchLoss = 0.7023, diffLoss = 3.4481, kgLoss = 0.0159
2025-04-08 14:27:51.567437: Training Step 10/354: batchLoss = 0.4930, diffLoss = 2.4135, kgLoss = 0.0128
2025-04-08 14:27:53.187474: Training Step 11/354: batchLoss = 0.6501, diffLoss = 3.1903, kgLoss = 0.0150
2025-04-08 14:27:54.820298: Training Step 12/354: batchLoss = 0.5246, diffLoss = 2.5674, kgLoss = 0.0139
2025-04-08 14:27:56.432683: Training Step 13/354: batchLoss = 0.6961, diffLoss = 3.4168, kgLoss = 0.0159
2025-04-08 14:27:58.047779: Training Step 14/354: batchLoss = 0.7684, diffLoss = 3.7708, kgLoss = 0.0178
2025-04-08 14:27:59.668952: Training Step 15/354: batchLoss = 0.4820, diffLoss = 2.3643, kgLoss = 0.0115
2025-04-08 14:28:01.303961: Training Step 16/354: batchLoss = 0.5347, diffLoss = 2.6227, kgLoss = 0.0127
2025-04-08 14:28:02.922364: Training Step 17/354: batchLoss = 0.5679, diffLoss = 2.7822, kgLoss = 0.0143
2025-04-08 14:28:04.550279: Training Step 18/354: batchLoss = 0.5247, diffLoss = 2.5748, kgLoss = 0.0122
2025-04-08 14:28:06.177967: Training Step 19/354: batchLoss = 0.6025, diffLoss = 2.9607, kgLoss = 0.0129
2025-04-08 14:28:07.804842: Training Step 20/354: batchLoss = 0.5636, diffLoss = 2.7625, kgLoss = 0.0139
2025-04-08 14:28:09.435533: Training Step 21/354: batchLoss = 0.5437, diffLoss = 2.6628, kgLoss = 0.0139
2025-04-08 14:28:11.044954: Training Step 22/354: batchLoss = 0.5981, diffLoss = 2.9222, kgLoss = 0.0171
2025-04-08 14:28:12.655413: Training Step 23/354: batchLoss = 0.6475, diffLoss = 3.1687, kgLoss = 0.0171
2025-04-08 14:28:14.266338: Training Step 24/354: batchLoss = 0.5550, diffLoss = 2.7268, kgLoss = 0.0121
2025-04-08 14:28:15.893377: Training Step 25/354: batchLoss = 0.6752, diffLoss = 3.3137, kgLoss = 0.0155
2025-04-08 14:28:17.515389: Training Step 26/354: batchLoss = 0.6780, diffLoss = 3.3194, kgLoss = 0.0177
2025-04-08 14:28:19.145581: Training Step 27/354: batchLoss = 0.4697, diffLoss = 2.3021, kgLoss = 0.0116
2025-04-08 14:28:20.765521: Training Step 28/354: batchLoss = 0.5829, diffLoss = 2.8656, kgLoss = 0.0122
2025-04-08 14:28:22.385137: Training Step 29/354: batchLoss = 0.5339, diffLoss = 2.6186, kgLoss = 0.0127
2025-04-08 14:28:24.012256: Training Step 30/354: batchLoss = 0.5422, diffLoss = 2.6632, kgLoss = 0.0120
2025-04-08 14:28:25.618860: Training Step 31/354: batchLoss = 0.5199, diffLoss = 2.5518, kgLoss = 0.0119
2025-04-08 14:28:27.228187: Training Step 32/354: batchLoss = 0.5537, diffLoss = 2.7140, kgLoss = 0.0136
2025-04-08 14:28:28.839092: Training Step 33/354: batchLoss = 0.5272, diffLoss = 2.5822, kgLoss = 0.0134
2025-04-08 14:28:30.448241: Training Step 34/354: batchLoss = 0.6604, diffLoss = 3.2357, kgLoss = 0.0166
2025-04-08 14:28:32.064779: Training Step 35/354: batchLoss = 0.7134, diffLoss = 3.4997, kgLoss = 0.0168
2025-04-08 14:28:33.679641: Training Step 36/354: batchLoss = 0.6133, diffLoss = 3.0079, kgLoss = 0.0146
2025-04-08 14:28:35.296436: Training Step 37/354: batchLoss = 0.6242, diffLoss = 3.0536, kgLoss = 0.0168
2025-04-08 14:28:36.931424: Training Step 38/354: batchLoss = 0.6754, diffLoss = 3.3174, kgLoss = 0.0149
2025-04-08 14:28:38.549095: Training Step 39/354: batchLoss = 0.7163, diffLoss = 3.5184, kgLoss = 0.0158
2025-04-08 14:28:40.156781: Training Step 40/354: batchLoss = 0.6600, diffLoss = 3.2423, kgLoss = 0.0144
2025-04-08 14:28:41.763049: Training Step 41/354: batchLoss = 0.8333, diffLoss = 4.0927, kgLoss = 0.0185
2025-04-08 14:28:43.375474: Training Step 42/354: batchLoss = 0.6796, diffLoss = 3.3247, kgLoss = 0.0183
2025-04-08 14:28:45.000675: Training Step 43/354: batchLoss = 0.6356, diffLoss = 3.1089, kgLoss = 0.0172
2025-04-08 14:28:46.620486: Training Step 44/354: batchLoss = 0.6577, diffLoss = 3.2268, kgLoss = 0.0154
2025-04-08 14:28:48.235575: Training Step 45/354: batchLoss = 0.4885, diffLoss = 2.3932, kgLoss = 0.0124
2025-04-08 14:28:49.860739: Training Step 46/354: batchLoss = 0.4204, diffLoss = 2.0613, kgLoss = 0.0102
2025-04-08 14:28:51.473520: Training Step 47/354: batchLoss = 0.5105, diffLoss = 2.4970, kgLoss = 0.0139
2025-04-08 14:28:53.096407: Training Step 48/354: batchLoss = 0.6220, diffLoss = 3.0412, kgLoss = 0.0172
2025-04-08 14:28:54.696955: Training Step 49/354: batchLoss = 0.6370, diffLoss = 3.1223, kgLoss = 0.0156
2025-04-08 14:28:56.306308: Training Step 50/354: batchLoss = 0.6585, diffLoss = 3.2359, kgLoss = 0.0142
2025-04-08 14:28:57.919310: Training Step 51/354: batchLoss = 0.5984, diffLoss = 2.9281, kgLoss = 0.0160
2025-04-08 14:28:59.534229: Training Step 52/354: batchLoss = 0.6127, diffLoss = 2.9974, kgLoss = 0.0165
2025-04-08 14:29:01.165404: Training Step 53/354: batchLoss = 0.5422, diffLoss = 2.6562, kgLoss = 0.0137
2025-04-08 14:29:02.788770: Training Step 54/354: batchLoss = 0.5799, diffLoss = 2.8430, kgLoss = 0.0141
2025-04-08 14:29:04.423744: Training Step 55/354: batchLoss = 0.6177, diffLoss = 3.0198, kgLoss = 0.0172
2025-04-08 14:29:06.047972: Training Step 56/354: batchLoss = 0.4854, diffLoss = 2.3734, kgLoss = 0.0134
2025-04-08 14:29:07.690278: Training Step 57/354: batchLoss = 0.5711, diffLoss = 2.7968, kgLoss = 0.0146
2025-04-08 14:29:09.301458: Training Step 58/354: batchLoss = 0.5810, diffLoss = 2.8470, kgLoss = 0.0145
2025-04-08 14:29:10.913649: Training Step 59/354: batchLoss = 0.4919, diffLoss = 2.4154, kgLoss = 0.0111
2025-04-08 14:29:12.535965: Training Step 60/354: batchLoss = 0.7295, diffLoss = 3.5783, kgLoss = 0.0173
2025-04-08 14:29:14.159105: Training Step 61/354: batchLoss = 0.5058, diffLoss = 2.4824, kgLoss = 0.0117
2025-04-08 14:29:15.787687: Training Step 62/354: batchLoss = 0.6664, diffLoss = 3.2692, kgLoss = 0.0157
2025-04-08 14:29:17.418361: Training Step 63/354: batchLoss = 0.5509, diffLoss = 2.6934, kgLoss = 0.0152
2025-04-08 14:29:19.043644: Training Step 64/354: batchLoss = 0.5946, diffLoss = 2.9148, kgLoss = 0.0146
2025-04-08 14:29:20.672161: Training Step 65/354: batchLoss = 0.7445, diffLoss = 3.6576, kgLoss = 0.0163
2025-04-08 14:29:22.291259: Training Step 66/354: batchLoss = 0.5785, diffLoss = 2.8383, kgLoss = 0.0135
2025-04-08 14:29:23.910086: Training Step 67/354: batchLoss = 0.5374, diffLoss = 2.6359, kgLoss = 0.0128
2025-04-08 14:29:25.528105: Training Step 68/354: batchLoss = 0.6605, diffLoss = 3.2413, kgLoss = 0.0153
2025-04-08 14:29:27.148380: Training Step 69/354: batchLoss = 0.6066, diffLoss = 2.9767, kgLoss = 0.0141
2025-04-08 14:29:28.772715: Training Step 70/354: batchLoss = 0.5476, diffLoss = 2.6898, kgLoss = 0.0121
2025-04-08 14:29:30.394438: Training Step 71/354: batchLoss = 0.6238, diffLoss = 3.0526, kgLoss = 0.0166
2025-04-08 14:29:32.027918: Training Step 72/354: batchLoss = 0.5616, diffLoss = 2.7520, kgLoss = 0.0140
2025-04-08 14:29:33.652916: Training Step 73/354: batchLoss = 0.5460, diffLoss = 2.6760, kgLoss = 0.0135
2025-04-08 14:29:35.276781: Training Step 74/354: batchLoss = 0.6461, diffLoss = 3.1741, kgLoss = 0.0141
2025-04-08 14:29:36.895384: Training Step 75/354: batchLoss = 0.7278, diffLoss = 3.5677, kgLoss = 0.0178
2025-04-08 14:29:38.518817: Training Step 76/354: batchLoss = 0.5436, diffLoss = 2.6581, kgLoss = 0.0149
2025-04-08 14:29:40.136807: Training Step 77/354: batchLoss = 0.5904, diffLoss = 2.8922, kgLoss = 0.0149
2025-04-08 14:29:41.743362: Training Step 78/354: batchLoss = 0.4825, diffLoss = 2.3632, kgLoss = 0.0123
2025-04-08 14:29:43.353766: Training Step 79/354: batchLoss = 0.5008, diffLoss = 2.4567, kgLoss = 0.0119
2025-04-08 14:29:44.971803: Training Step 80/354: batchLoss = 0.4842, diffLoss = 2.3748, kgLoss = 0.0116
2025-04-08 14:29:46.596430: Training Step 81/354: batchLoss = 0.6016, diffLoss = 2.9506, kgLoss = 0.0144
2025-04-08 14:29:48.215089: Training Step 82/354: batchLoss = 0.5407, diffLoss = 2.6531, kgLoss = 0.0126
2025-04-08 14:29:49.841114: Training Step 83/354: batchLoss = 0.5391, diffLoss = 2.6437, kgLoss = 0.0129
2025-04-08 14:29:51.461219: Training Step 84/354: batchLoss = 0.6419, diffLoss = 3.1511, kgLoss = 0.0146
2025-04-08 14:29:53.065407: Training Step 85/354: batchLoss = 0.7384, diffLoss = 3.6101, kgLoss = 0.0205
2025-04-08 14:29:54.656839: Training Step 86/354: batchLoss = 0.6194, diffLoss = 3.0413, kgLoss = 0.0140
2025-04-08 14:29:56.264344: Training Step 87/354: batchLoss = 0.5793, diffLoss = 2.8392, kgLoss = 0.0143
2025-04-08 14:29:57.872395: Training Step 88/354: batchLoss = 0.7098, diffLoss = 3.4858, kgLoss = 0.0158
2025-04-08 14:29:59.493301: Training Step 89/354: batchLoss = 0.5027, diffLoss = 2.4654, kgLoss = 0.0120
2025-04-08 14:30:01.118349: Training Step 90/354: batchLoss = 0.5434, diffLoss = 2.6655, kgLoss = 0.0129
2025-04-08 14:30:02.732854: Training Step 91/354: batchLoss = 0.6780, diffLoss = 3.3250, kgLoss = 0.0163
2025-04-08 14:30:04.355841: Training Step 92/354: batchLoss = 0.6554, diffLoss = 3.2165, kgLoss = 0.0151
2025-04-08 14:30:05.983784: Training Step 93/354: batchLoss = 0.4762, diffLoss = 2.3337, kgLoss = 0.0119
2025-04-08 14:30:07.607425: Training Step 94/354: batchLoss = 0.5690, diffLoss = 2.7879, kgLoss = 0.0142
2025-04-08 14:30:09.235108: Training Step 95/354: batchLoss = 0.6680, diffLoss = 3.2753, kgLoss = 0.0162
2025-04-08 14:30:10.845522: Training Step 96/354: batchLoss = 0.4592, diffLoss = 2.2479, kgLoss = 0.0120
2025-04-08 14:30:12.462137: Training Step 97/354: batchLoss = 0.5391, diffLoss = 2.6463, kgLoss = 0.0123
2025-04-08 14:30:14.073338: Training Step 98/354: batchLoss = 0.6743, diffLoss = 3.3053, kgLoss = 0.0165
2025-04-08 14:30:15.688289: Training Step 99/354: batchLoss = 0.5572, diffLoss = 2.7281, kgLoss = 0.0144
2025-04-08 14:30:17.306263: Training Step 100/354: batchLoss = 0.5188, diffLoss = 2.5445, kgLoss = 0.0124
2025-04-08 14:30:18.917784: Training Step 101/354: batchLoss = 0.6122, diffLoss = 3.0066, kgLoss = 0.0136
2025-04-08 14:30:20.529823: Training Step 102/354: batchLoss = 0.5123, diffLoss = 2.5066, kgLoss = 0.0137
2025-04-08 14:30:22.147547: Training Step 103/354: batchLoss = 0.7727, diffLoss = 3.7969, kgLoss = 0.0166
2025-04-08 14:30:23.774606: Training Step 104/354: batchLoss = 0.6210, diffLoss = 3.0344, kgLoss = 0.0176
2025-04-08 14:30:25.390288: Training Step 105/354: batchLoss = 0.5530, diffLoss = 2.7108, kgLoss = 0.0136
2025-04-08 14:30:27.005653: Training Step 106/354: batchLoss = 0.7770, diffLoss = 3.8186, kgLoss = 0.0166
2025-04-08 14:30:28.609699: Training Step 107/354: batchLoss = 0.5637, diffLoss = 2.7651, kgLoss = 0.0134
2025-04-08 14:30:30.216214: Training Step 108/354: batchLoss = 0.5157, diffLoss = 2.5175, kgLoss = 0.0153
2025-04-08 14:30:31.827221: Training Step 109/354: batchLoss = 0.6921, diffLoss = 3.3992, kgLoss = 0.0153
2025-04-08 14:30:33.444139: Training Step 110/354: batchLoss = 0.6077, diffLoss = 2.9836, kgLoss = 0.0137
2025-04-08 14:30:35.045705: Training Step 111/354: batchLoss = 0.6359, diffLoss = 3.1151, kgLoss = 0.0161
2025-04-08 14:30:36.667544: Training Step 112/354: batchLoss = 0.5506, diffLoss = 2.7003, kgLoss = 0.0132
2025-04-08 14:30:38.280122: Training Step 113/354: batchLoss = 0.5690, diffLoss = 2.7861, kgLoss = 0.0147
2025-04-08 14:30:39.900450: Training Step 114/354: batchLoss = 0.5351, diffLoss = 2.6278, kgLoss = 0.0119
2025-04-08 14:30:41.530582: Training Step 115/354: batchLoss = 0.6747, diffLoss = 3.3133, kgLoss = 0.0150
2025-04-08 14:30:43.145301: Training Step 116/354: batchLoss = 0.5236, diffLoss = 2.5540, kgLoss = 0.0160
2025-04-08 14:30:44.751697: Training Step 117/354: batchLoss = 0.7988, diffLoss = 3.9189, kgLoss = 0.0188
2025-04-08 14:30:46.361264: Training Step 118/354: batchLoss = 0.5324, diffLoss = 2.6125, kgLoss = 0.0124
2025-04-08 14:30:47.976005: Training Step 119/354: batchLoss = 0.6794, diffLoss = 3.3336, kgLoss = 0.0158
2025-04-08 14:30:49.594823: Training Step 120/354: batchLoss = 0.6696, diffLoss = 3.2789, kgLoss = 0.0173
2025-04-08 14:30:51.212665: Training Step 121/354: batchLoss = 0.6455, diffLoss = 3.1627, kgLoss = 0.0161
2025-04-08 14:30:52.831102: Training Step 122/354: batchLoss = 1.0248, diffLoss = 5.0289, kgLoss = 0.0238
2025-04-08 14:30:54.441681: Training Step 123/354: batchLoss = 0.6509, diffLoss = 3.1889, kgLoss = 0.0165
2025-04-08 14:30:56.057890: Training Step 124/354: batchLoss = 0.5709, diffLoss = 2.8030, kgLoss = 0.0129
2025-04-08 14:30:57.666928: Training Step 125/354: batchLoss = 0.7661, diffLoss = 3.7578, kgLoss = 0.0181
2025-04-08 14:30:59.274820: Training Step 126/354: batchLoss = 0.5731, diffLoss = 2.8088, kgLoss = 0.0141
2025-04-08 14:31:00.880170: Training Step 127/354: batchLoss = 0.5143, diffLoss = 2.5173, kgLoss = 0.0135
2025-04-08 14:31:02.491867: Training Step 128/354: batchLoss = 0.5345, diffLoss = 2.6192, kgLoss = 0.0134
2025-04-08 14:31:04.106013: Training Step 129/354: batchLoss = 0.5720, diffLoss = 2.7916, kgLoss = 0.0171
2025-04-08 14:31:05.717320: Training Step 130/354: batchLoss = 0.5411, diffLoss = 2.6559, kgLoss = 0.0124
2025-04-08 14:31:07.324942: Training Step 131/354: batchLoss = 0.5952, diffLoss = 2.9188, kgLoss = 0.0144
2025-04-08 14:31:08.934371: Training Step 132/354: batchLoss = 0.4818, diffLoss = 2.3590, kgLoss = 0.0125
2025-04-08 14:31:10.555685: Training Step 133/354: batchLoss = 0.5632, diffLoss = 2.7607, kgLoss = 0.0138
2025-04-08 14:31:12.162531: Training Step 134/354: batchLoss = 0.5153, diffLoss = 2.5241, kgLoss = 0.0130
2025-04-08 14:31:13.772570: Training Step 135/354: batchLoss = 0.4667, diffLoss = 2.2883, kgLoss = 0.0113
2025-04-08 14:31:15.386835: Training Step 136/354: batchLoss = 0.7913, diffLoss = 3.8811, kgLoss = 0.0188
2025-04-08 14:31:17.005842: Training Step 137/354: batchLoss = 0.6019, diffLoss = 2.9505, kgLoss = 0.0148
2025-04-08 14:31:18.614750: Training Step 138/354: batchLoss = 0.6856, diffLoss = 3.3629, kgLoss = 0.0162
2025-04-08 14:31:20.226005: Training Step 139/354: batchLoss = 0.6781, diffLoss = 3.3256, kgLoss = 0.0162
2025-04-08 14:31:21.839955: Training Step 140/354: batchLoss = 0.7089, diffLoss = 3.4792, kgLoss = 0.0163
2025-04-08 14:31:23.451415: Training Step 141/354: batchLoss = 0.5630, diffLoss = 2.7610, kgLoss = 0.0135
2025-04-08 14:31:25.069460: Training Step 142/354: batchLoss = 0.5968, diffLoss = 2.9242, kgLoss = 0.0149
2025-04-08 14:31:26.677370: Training Step 143/354: batchLoss = 0.4837, diffLoss = 2.3720, kgLoss = 0.0116
2025-04-08 14:31:28.289499: Training Step 144/354: batchLoss = 0.6784, diffLoss = 3.3291, kgLoss = 0.0158
2025-04-08 14:31:29.897862: Training Step 145/354: batchLoss = 0.5990, diffLoss = 2.9403, kgLoss = 0.0137
2025-04-08 14:31:31.509868: Training Step 146/354: batchLoss = 0.7173, diffLoss = 3.5155, kgLoss = 0.0177
2025-04-08 14:31:33.120926: Training Step 147/354: batchLoss = 0.5595, diffLoss = 2.7255, kgLoss = 0.0180
2025-04-08 14:31:34.734227: Training Step 148/354: batchLoss = 0.6372, diffLoss = 3.1329, kgLoss = 0.0133
2025-04-08 14:31:36.344815: Training Step 149/354: batchLoss = 0.4836, diffLoss = 2.3696, kgLoss = 0.0121
2025-04-08 14:31:37.952630: Training Step 150/354: batchLoss = 0.6256, diffLoss = 3.0689, kgLoss = 0.0148
2025-04-08 14:31:39.556801: Training Step 151/354: batchLoss = 0.7758, diffLoss = 3.8077, kgLoss = 0.0178
2025-04-08 14:31:41.161176: Training Step 152/354: batchLoss = 0.5366, diffLoss = 2.6303, kgLoss = 0.0131
2025-04-08 14:31:42.766797: Training Step 153/354: batchLoss = 0.5767, diffLoss = 2.8282, kgLoss = 0.0138
2025-04-08 14:31:44.378532: Training Step 154/354: batchLoss = 0.6602, diffLoss = 3.2395, kgLoss = 0.0154
2025-04-08 14:31:45.987762: Training Step 155/354: batchLoss = 0.6664, diffLoss = 3.2704, kgLoss = 0.0154
2025-04-08 14:31:47.595205: Training Step 156/354: batchLoss = 0.6770, diffLoss = 3.3187, kgLoss = 0.0165
2025-04-08 14:31:49.201512: Training Step 157/354: batchLoss = 0.6203, diffLoss = 3.0373, kgLoss = 0.0160
2025-04-08 14:31:50.813882: Training Step 158/354: batchLoss = 0.5960, diffLoss = 2.9256, kgLoss = 0.0135
2025-04-08 14:31:52.422890: Training Step 159/354: batchLoss = 0.5896, diffLoss = 2.8910, kgLoss = 0.0143
2025-04-08 14:31:54.035437: Training Step 160/354: batchLoss = 0.6555, diffLoss = 3.2158, kgLoss = 0.0154
2025-04-08 14:31:55.638483: Training Step 161/354: batchLoss = 0.8404, diffLoss = 4.1238, kgLoss = 0.0195
2025-04-08 14:31:57.249175: Training Step 162/354: batchLoss = 0.4969, diffLoss = 2.4398, kgLoss = 0.0112
2025-04-08 14:31:58.850526: Training Step 163/354: batchLoss = 0.6390, diffLoss = 3.1296, kgLoss = 0.0163
2025-04-08 14:32:00.463490: Training Step 164/354: batchLoss = 0.5691, diffLoss = 2.7917, kgLoss = 0.0134
2025-04-08 14:32:02.093323: Training Step 165/354: batchLoss = 0.5901, diffLoss = 2.8937, kgLoss = 0.0142
2025-04-08 14:32:03.721754: Training Step 166/354: batchLoss = 0.5322, diffLoss = 2.6104, kgLoss = 0.0127
2025-04-08 14:32:05.355169: Training Step 167/354: batchLoss = 0.7138, diffLoss = 3.5085, kgLoss = 0.0151
2025-04-08 14:32:06.997071: Training Step 168/354: batchLoss = 0.5947, diffLoss = 2.9181, kgLoss = 0.0138
2025-04-08 14:32:08.617144: Training Step 169/354: batchLoss = 0.6326, diffLoss = 3.1033, kgLoss = 0.0149
2025-04-08 14:32:10.241014: Training Step 170/354: batchLoss = 0.5574, diffLoss = 2.7273, kgLoss = 0.0150
2025-04-08 14:32:11.868270: Training Step 171/354: batchLoss = 0.6129, diffLoss = 3.0102, kgLoss = 0.0136
2025-04-08 14:32:13.513758: Training Step 172/354: batchLoss = 0.6322, diffLoss = 3.0985, kgLoss = 0.0156
2025-04-08 14:32:15.146012: Training Step 173/354: batchLoss = 0.5452, diffLoss = 2.6773, kgLoss = 0.0122
2025-04-08 14:32:16.754243: Training Step 174/354: batchLoss = 0.5122, diffLoss = 2.5072, kgLoss = 0.0135
2025-04-08 14:32:18.366721: Training Step 175/354: batchLoss = 0.4803, diffLoss = 2.3513, kgLoss = 0.0125
2025-04-08 14:32:19.979334: Training Step 176/354: batchLoss = 0.6233, diffLoss = 3.0560, kgLoss = 0.0151
2025-04-08 14:32:21.586845: Training Step 177/354: batchLoss = 0.6889, diffLoss = 3.3776, kgLoss = 0.0167
2025-04-08 14:32:23.193611: Training Step 178/354: batchLoss = 0.5791, diffLoss = 2.8298, kgLoss = 0.0164
2025-04-08 14:32:24.799354: Training Step 179/354: batchLoss = 0.5292, diffLoss = 2.5967, kgLoss = 0.0123
2025-04-08 14:32:26.403848: Training Step 180/354: batchLoss = 0.5851, diffLoss = 2.8649, kgLoss = 0.0151
2025-04-08 14:32:28.007297: Training Step 181/354: batchLoss = 0.5075, diffLoss = 2.4903, kgLoss = 0.0118
2025-04-08 14:32:29.624355: Training Step 182/354: batchLoss = 0.6010, diffLoss = 2.9507, kgLoss = 0.0136
2025-04-08 14:32:31.236707: Training Step 183/354: batchLoss = 0.6934, diffLoss = 3.3915, kgLoss = 0.0188
2025-04-08 14:32:32.847709: Training Step 184/354: batchLoss = 0.6252, diffLoss = 3.0557, kgLoss = 0.0176
2025-04-08 14:32:34.455792: Training Step 185/354: batchLoss = 0.5979, diffLoss = 2.9328, kgLoss = 0.0141
2025-04-08 14:32:36.070026: Training Step 186/354: batchLoss = 0.5845, diffLoss = 2.8633, kgLoss = 0.0148
2025-04-08 14:32:37.688558: Training Step 187/354: batchLoss = 0.5926, diffLoss = 2.8950, kgLoss = 0.0170
2025-04-08 14:32:39.298313: Training Step 188/354: batchLoss = 0.6107, diffLoss = 2.9966, kgLoss = 0.0142
2025-04-08 14:32:40.916354: Training Step 189/354: batchLoss = 0.6059, diffLoss = 2.9733, kgLoss = 0.0141
2025-04-08 14:32:42.527869: Training Step 190/354: batchLoss = 0.5600, diffLoss = 2.7387, kgLoss = 0.0153
2025-04-08 14:32:44.145739: Training Step 191/354: batchLoss = 0.6559, diffLoss = 3.2176, kgLoss = 0.0155
2025-04-08 14:32:45.751164: Training Step 192/354: batchLoss = 0.6019, diffLoss = 2.9451, kgLoss = 0.0161
2025-04-08 14:32:47.361169: Training Step 193/354: batchLoss = 0.5678, diffLoss = 2.7837, kgLoss = 0.0138
2025-04-08 14:32:48.972855: Training Step 194/354: batchLoss = 0.5522, diffLoss = 2.6931, kgLoss = 0.0169
2025-04-08 14:32:50.582807: Training Step 195/354: batchLoss = 0.5572, diffLoss = 2.7212, kgLoss = 0.0162
2025-04-08 14:32:52.196885: Training Step 196/354: batchLoss = 0.6141, diffLoss = 3.0139, kgLoss = 0.0142
2025-04-08 14:32:53.809610: Training Step 197/354: batchLoss = 0.5264, diffLoss = 2.5656, kgLoss = 0.0166
2025-04-08 14:32:55.425526: Training Step 198/354: batchLoss = 0.6834, diffLoss = 3.3499, kgLoss = 0.0168
2025-04-08 14:32:57.026488: Training Step 199/354: batchLoss = 0.5656, diffLoss = 2.7725, kgLoss = 0.0139
2025-04-08 14:32:58.642857: Training Step 200/354: batchLoss = 0.5510, diffLoss = 2.6999, kgLoss = 0.0138
2025-04-08 14:33:00.251840: Training Step 201/354: batchLoss = 0.6572, diffLoss = 3.2262, kgLoss = 0.0150
2025-04-08 14:33:01.876749: Training Step 202/354: batchLoss = 0.5640, diffLoss = 2.7537, kgLoss = 0.0166
2025-04-08 14:33:03.494454: Training Step 203/354: batchLoss = 0.5021, diffLoss = 2.4636, kgLoss = 0.0117
2025-04-08 14:33:05.107237: Training Step 204/354: batchLoss = 0.6631, diffLoss = 3.2559, kgLoss = 0.0149
2025-04-08 14:33:06.716342: Training Step 205/354: batchLoss = 0.6418, diffLoss = 3.1456, kgLoss = 0.0158
2025-04-08 14:33:08.327846: Training Step 206/354: batchLoss = 0.6774, diffLoss = 3.3198, kgLoss = 0.0168
2025-04-08 14:33:09.933371: Training Step 207/354: batchLoss = 0.5203, diffLoss = 2.5474, kgLoss = 0.0136
2025-04-08 14:33:11.541413: Training Step 208/354: batchLoss = 0.7695, diffLoss = 3.7777, kgLoss = 0.0174
2025-04-08 14:33:13.151093: Training Step 209/354: batchLoss = 0.6317, diffLoss = 3.0986, kgLoss = 0.0150
2025-04-08 14:33:14.763032: Training Step 210/354: batchLoss = 0.5961, diffLoss = 2.9257, kgLoss = 0.0137
2025-04-08 14:33:16.374819: Training Step 211/354: batchLoss = 0.6053, diffLoss = 2.9722, kgLoss = 0.0136
2025-04-08 14:33:17.992551: Training Step 212/354: batchLoss = 0.7746, diffLoss = 3.8002, kgLoss = 0.0182
2025-04-08 14:33:19.609911: Training Step 213/354: batchLoss = 0.6575, diffLoss = 3.2274, kgLoss = 0.0150
2025-04-08 14:33:21.231757: Training Step 214/354: batchLoss = 0.5707, diffLoss = 2.7898, kgLoss = 0.0159
2025-04-08 14:33:22.848779: Training Step 215/354: batchLoss = 0.6936, diffLoss = 3.4042, kgLoss = 0.0159
2025-04-08 14:33:24.470346: Training Step 216/354: batchLoss = 0.5356, diffLoss = 2.6239, kgLoss = 0.0135
2025-04-08 14:33:26.083896: Training Step 217/354: batchLoss = 0.6348, diffLoss = 3.1077, kgLoss = 0.0166
2025-04-08 14:33:27.701831: Training Step 218/354: batchLoss = 0.6991, diffLoss = 3.4315, kgLoss = 0.0161
2025-04-08 14:33:29.319112: Training Step 219/354: batchLoss = 0.5430, diffLoss = 2.6559, kgLoss = 0.0148
2025-04-08 14:33:30.926999: Training Step 220/354: batchLoss = 0.5209, diffLoss = 2.5509, kgLoss = 0.0134
2025-04-08 14:33:32.534061: Training Step 221/354: batchLoss = 0.5599, diffLoss = 2.7459, kgLoss = 0.0134
2025-04-08 14:33:34.145673: Training Step 222/354: batchLoss = 0.6437, diffLoss = 3.1591, kgLoss = 0.0148
2025-04-08 14:33:35.765436: Training Step 223/354: batchLoss = 0.6619, diffLoss = 3.2520, kgLoss = 0.0144
2025-04-08 14:33:37.376667: Training Step 224/354: batchLoss = 0.7346, diffLoss = 3.6119, kgLoss = 0.0153
2025-04-08 14:33:38.981985: Training Step 225/354: batchLoss = 0.6333, diffLoss = 3.1061, kgLoss = 0.0151
2025-04-08 14:33:40.591270: Training Step 226/354: batchLoss = 0.4588, diffLoss = 2.2456, kgLoss = 0.0121
2025-04-08 14:33:42.215160: Training Step 227/354: batchLoss = 0.5926, diffLoss = 2.9007, kgLoss = 0.0155
2025-04-08 14:33:43.826615: Training Step 228/354: batchLoss = 0.5865, diffLoss = 2.8778, kgLoss = 0.0137
2025-04-08 14:33:45.439327: Training Step 229/354: batchLoss = 0.5682, diffLoss = 2.7857, kgLoss = 0.0138
2025-04-08 14:33:47.052959: Training Step 230/354: batchLoss = 0.8506, diffLoss = 4.1794, kgLoss = 0.0183
2025-04-08 14:33:48.663474: Training Step 231/354: batchLoss = 0.6115, diffLoss = 2.9992, kgLoss = 0.0146
2025-04-08 14:33:50.274779: Training Step 232/354: batchLoss = 0.7877, diffLoss = 3.8699, kgLoss = 0.0172
2025-04-08 14:33:51.890141: Training Step 233/354: batchLoss = 0.4891, diffLoss = 2.3978, kgLoss = 0.0119
2025-04-08 14:33:53.497806: Training Step 234/354: batchLoss = 0.5862, diffLoss = 2.8795, kgLoss = 0.0129
2025-04-08 14:33:55.120564: Training Step 235/354: batchLoss = 0.5077, diffLoss = 2.4915, kgLoss = 0.0118
2025-04-08 14:33:56.730749: Training Step 236/354: batchLoss = 0.6057, diffLoss = 2.9707, kgLoss = 0.0145
2025-04-08 14:33:58.343814: Training Step 237/354: batchLoss = 0.6777, diffLoss = 3.3260, kgLoss = 0.0156
2025-04-08 14:33:59.954457: Training Step 238/354: batchLoss = 0.8240, diffLoss = 4.0493, kgLoss = 0.0177
2025-04-08 14:34:01.570382: Training Step 239/354: batchLoss = 0.6514, diffLoss = 3.1948, kgLoss = 0.0156
2025-04-08 14:34:03.181607: Training Step 240/354: batchLoss = 0.7125, diffLoss = 3.4968, kgLoss = 0.0164
2025-04-08 14:34:04.793985: Training Step 241/354: batchLoss = 0.4847, diffLoss = 2.3730, kgLoss = 0.0126
2025-04-08 14:34:06.410591: Training Step 242/354: batchLoss = 0.4949, diffLoss = 2.4222, kgLoss = 0.0131
2025-04-08 14:34:08.015895: Training Step 243/354: batchLoss = 0.7030, diffLoss = 3.4507, kgLoss = 0.0160
2025-04-08 14:34:09.617576: Training Step 244/354: batchLoss = 0.6383, diffLoss = 3.1304, kgLoss = 0.0152
2025-04-08 14:34:11.230066: Training Step 245/354: batchLoss = 0.5157, diffLoss = 2.5314, kgLoss = 0.0118
2025-04-08 14:34:12.842386: Training Step 246/354: batchLoss = 0.6636, diffLoss = 3.2534, kgLoss = 0.0162
2025-04-08 14:34:14.454692: Training Step 247/354: batchLoss = 0.5434, diffLoss = 2.6549, kgLoss = 0.0155
2025-04-08 14:34:16.068637: Training Step 248/354: batchLoss = 0.5708, diffLoss = 2.7991, kgLoss = 0.0137
2025-04-08 14:34:17.677231: Training Step 249/354: batchLoss = 0.4601, diffLoss = 2.2551, kgLoss = 0.0114
2025-04-08 14:34:19.285765: Training Step 250/354: batchLoss = 0.6242, diffLoss = 3.0595, kgLoss = 0.0153
2025-04-08 14:34:20.909347: Training Step 251/354: batchLoss = 0.5775, diffLoss = 2.8348, kgLoss = 0.0132
2025-04-08 14:34:22.513392: Training Step 252/354: batchLoss = 0.6618, diffLoss = 3.2424, kgLoss = 0.0166
2025-04-08 14:34:24.123585: Training Step 253/354: batchLoss = 0.6509, diffLoss = 3.1937, kgLoss = 0.0152
2025-04-08 14:34:25.745197: Training Step 254/354: batchLoss = 0.7225, diffLoss = 3.5471, kgLoss = 0.0164
2025-04-08 14:34:27.359145: Training Step 255/354: batchLoss = 0.5252, diffLoss = 2.5741, kgLoss = 0.0130
2025-04-08 14:34:28.977403: Training Step 256/354: batchLoss = 0.6406, diffLoss = 3.1413, kgLoss = 0.0154
2025-04-08 14:34:30.588752: Training Step 257/354: batchLoss = 0.5312, diffLoss = 2.6041, kgLoss = 0.0130
2025-04-08 14:34:32.197368: Training Step 258/354: batchLoss = 0.5227, diffLoss = 2.5646, kgLoss = 0.0123
2025-04-08 14:34:33.807174: Training Step 259/354: batchLoss = 0.6212, diffLoss = 3.0540, kgLoss = 0.0130
2025-04-08 14:34:35.423619: Training Step 260/354: batchLoss = 0.5318, diffLoss = 2.6125, kgLoss = 0.0116
2025-04-08 14:34:37.030607: Training Step 261/354: batchLoss = 0.5266, diffLoss = 2.5818, kgLoss = 0.0128
2025-04-08 14:34:38.638431: Training Step 262/354: batchLoss = 0.7830, diffLoss = 3.7387, kgLoss = 0.0440
2025-04-08 14:34:40.258758: Training Step 263/354: batchLoss = 0.6441, diffLoss = 3.1621, kgLoss = 0.0146
2025-04-08 14:34:41.879401: Training Step 264/354: batchLoss = 0.7720, diffLoss = 3.7989, kgLoss = 0.0153
2025-04-08 14:34:43.492048: Training Step 265/354: batchLoss = 0.4554, diffLoss = 2.2352, kgLoss = 0.0105
2025-04-08 14:34:45.110558: Training Step 266/354: batchLoss = 0.5807, diffLoss = 2.8403, kgLoss = 0.0158
2025-04-08 14:34:46.720000: Training Step 267/354: batchLoss = 0.4945, diffLoss = 2.4240, kgLoss = 0.0121
2025-04-08 14:34:48.331027: Training Step 268/354: batchLoss = 0.5661, diffLoss = 2.7752, kgLoss = 0.0138
2025-04-08 14:34:49.941007: Training Step 269/354: batchLoss = 0.6446, diffLoss = 3.1618, kgLoss = 0.0153
2025-04-08 14:34:51.542630: Training Step 270/354: batchLoss = 0.5212, diffLoss = 2.5509, kgLoss = 0.0138
2025-04-08 14:34:53.151220: Training Step 271/354: batchLoss = 0.6236, diffLoss = 3.0640, kgLoss = 0.0134
2025-04-08 14:34:54.756227: Training Step 272/354: batchLoss = 0.5073, diffLoss = 2.4861, kgLoss = 0.0126
2025-04-08 14:34:56.364629: Training Step 273/354: batchLoss = 0.6443, diffLoss = 3.1665, kgLoss = 0.0137
2025-04-08 14:34:57.973280: Training Step 274/354: batchLoss = 0.5405, diffLoss = 2.6581, kgLoss = 0.0111
2025-04-08 14:34:59.589616: Training Step 275/354: batchLoss = 0.6798, diffLoss = 3.3334, kgLoss = 0.0164
2025-04-08 14:35:01.213181: Training Step 276/354: batchLoss = 0.4429, diffLoss = 2.1685, kgLoss = 0.0115
2025-04-08 14:35:02.823694: Training Step 277/354: batchLoss = 0.6803, diffLoss = 3.3384, kgLoss = 0.0158
2025-04-08 14:35:04.435490: Training Step 278/354: batchLoss = 0.6388, diffLoss = 3.1352, kgLoss = 0.0147
2025-04-08 14:35:06.049565: Training Step 279/354: batchLoss = 0.5073, diffLoss = 2.4823, kgLoss = 0.0136
2025-04-08 14:35:07.653085: Training Step 280/354: batchLoss = 0.5814, diffLoss = 2.8560, kgLoss = 0.0128
2025-04-08 14:35:09.260065: Training Step 281/354: batchLoss = 0.6843, diffLoss = 3.3594, kgLoss = 0.0155
2025-04-08 14:35:10.870745: Training Step 282/354: batchLoss = 0.5788, diffLoss = 2.8398, kgLoss = 0.0136
2025-04-08 14:35:12.498919: Training Step 283/354: batchLoss = 0.7534, diffLoss = 3.6921, kgLoss = 0.0187
2025-04-08 14:35:14.113359: Training Step 284/354: batchLoss = 0.6407, diffLoss = 3.1453, kgLoss = 0.0146
2025-04-08 14:35:15.732745: Training Step 285/354: batchLoss = 0.6800, diffLoss = 3.3357, kgLoss = 0.0161
2025-04-08 14:35:17.347128: Training Step 286/354: batchLoss = 0.6662, diffLoss = 3.2680, kgLoss = 0.0157
2025-04-08 14:35:18.962651: Training Step 287/354: batchLoss = 0.5958, diffLoss = 2.9177, kgLoss = 0.0154
2025-04-08 14:35:20.573127: Training Step 288/354: batchLoss = 0.5071, diffLoss = 2.4814, kgLoss = 0.0135
2025-04-08 14:35:22.184319: Training Step 289/354: batchLoss = 0.7156, diffLoss = 3.5158, kgLoss = 0.0156
2025-04-08 14:35:23.788540: Training Step 290/354: batchLoss = 0.6834, diffLoss = 3.3525, kgLoss = 0.0162
2025-04-08 14:35:25.395320: Training Step 291/354: batchLoss = 0.6024, diffLoss = 2.9551, kgLoss = 0.0142
2025-04-08 14:35:27.007411: Training Step 292/354: batchLoss = 0.6730, diffLoss = 3.2938, kgLoss = 0.0177
2025-04-08 14:35:28.631007: Training Step 293/354: batchLoss = 0.5561, diffLoss = 2.7203, kgLoss = 0.0150
2025-04-08 14:35:30.238470: Training Step 294/354: batchLoss = 0.5384, diffLoss = 2.6441, kgLoss = 0.0120
2025-04-08 14:35:31.851186: Training Step 295/354: batchLoss = 0.5877, diffLoss = 2.8873, kgLoss = 0.0129
2025-04-08 14:35:33.474772: Training Step 296/354: batchLoss = 0.5480, diffLoss = 2.6826, kgLoss = 0.0143
2025-04-08 14:35:35.106430: Training Step 297/354: batchLoss = 0.5364, diffLoss = 2.6286, kgLoss = 0.0134
2025-04-08 14:35:36.718354: Training Step 298/354: batchLoss = 0.7114, diffLoss = 3.4989, kgLoss = 0.0145
2025-04-08 14:35:38.324964: Training Step 299/354: batchLoss = 0.5562, diffLoss = 2.7286, kgLoss = 0.0132
2025-04-08 14:35:39.936626: Training Step 300/354: batchLoss = 0.7032, diffLoss = 3.4430, kgLoss = 0.0183
2025-04-08 14:35:41.553240: Training Step 301/354: batchLoss = 0.6368, diffLoss = 3.1139, kgLoss = 0.0176
2025-04-08 14:35:43.162107: Training Step 302/354: batchLoss = 0.5453, diffLoss = 2.6769, kgLoss = 0.0124
2025-04-08 14:35:44.776072: Training Step 303/354: batchLoss = 0.4649, diffLoss = 2.2716, kgLoss = 0.0132
2025-04-08 14:35:46.402813: Training Step 304/354: batchLoss = 0.7669, diffLoss = 3.7699, kgLoss = 0.0161
2025-04-08 14:35:48.030783: Training Step 305/354: batchLoss = 0.5488, diffLoss = 2.6898, kgLoss = 0.0136
2025-04-08 14:35:49.651219: Training Step 306/354: batchLoss = 0.5021, diffLoss = 2.4626, kgLoss = 0.0120
2025-04-08 14:35:51.265441: Training Step 307/354: batchLoss = 0.6122, diffLoss = 2.9962, kgLoss = 0.0162
2025-04-08 14:35:52.878584: Training Step 308/354: batchLoss = 0.6192, diffLoss = 3.0360, kgLoss = 0.0150
2025-04-08 14:35:54.480096: Training Step 309/354: batchLoss = 0.7843, diffLoss = 3.8536, kgLoss = 0.0170
2025-04-08 14:35:56.096236: Training Step 310/354: batchLoss = 0.6336, diffLoss = 3.1113, kgLoss = 0.0142
2025-04-08 14:35:57.704305: Training Step 311/354: batchLoss = 0.5756, diffLoss = 2.8214, kgLoss = 0.0142
2025-04-08 14:35:59.321493: Training Step 312/354: batchLoss = 0.7423, diffLoss = 3.6435, kgLoss = 0.0170
2025-04-08 14:36:00.945029: Training Step 313/354: batchLoss = 0.5213, diffLoss = 2.5556, kgLoss = 0.0128
2025-04-08 14:36:02.557904: Training Step 314/354: batchLoss = 0.5750, diffLoss = 2.8251, kgLoss = 0.0125
2025-04-08 14:36:04.172851: Training Step 315/354: batchLoss = 0.7053, diffLoss = 3.4627, kgLoss = 0.0160
2025-04-08 14:36:05.787960: Training Step 316/354: batchLoss = 0.5154, diffLoss = 2.5256, kgLoss = 0.0128
2025-04-08 14:36:07.398162: Training Step 317/354: batchLoss = 0.5732, diffLoss = 2.8097, kgLoss = 0.0141
2025-04-08 14:36:09.008437: Training Step 318/354: batchLoss = 0.6351, diffLoss = 3.1211, kgLoss = 0.0136
2025-04-08 14:36:10.620779: Training Step 319/354: batchLoss = 0.4957, diffLoss = 2.4289, kgLoss = 0.0124
2025-04-08 14:36:12.235573: Training Step 320/354: batchLoss = 0.5415, diffLoss = 2.6576, kgLoss = 0.0124
2025-04-08 14:36:13.846189: Training Step 321/354: batchLoss = 0.7082, diffLoss = 3.4736, kgLoss = 0.0169
2025-04-08 14:36:15.452617: Training Step 322/354: batchLoss = 0.7149, diffLoss = 3.5014, kgLoss = 0.0183
2025-04-08 14:36:17.068093: Training Step 323/354: batchLoss = 0.5268, diffLoss = 2.5855, kgLoss = 0.0122
2025-04-08 14:36:18.679956: Training Step 324/354: batchLoss = 0.6592, diffLoss = 3.2316, kgLoss = 0.0161
2025-04-08 14:36:20.288500: Training Step 325/354: batchLoss = 0.5088, diffLoss = 2.4918, kgLoss = 0.0130
2025-04-08 14:36:21.895954: Training Step 326/354: batchLoss = 0.4528, diffLoss = 2.2195, kgLoss = 0.0112
2025-04-08 14:36:23.502388: Training Step 327/354: batchLoss = 0.6214, diffLoss = 3.0470, kgLoss = 0.0150
2025-04-08 14:36:25.112432: Training Step 328/354: batchLoss = 0.6300, diffLoss = 3.0935, kgLoss = 0.0141
2025-04-08 14:36:26.720114: Training Step 329/354: batchLoss = 0.6344, diffLoss = 3.1131, kgLoss = 0.0147
2025-04-08 14:36:28.329110: Training Step 330/354: batchLoss = 0.5771, diffLoss = 2.8219, kgLoss = 0.0159
2025-04-08 14:36:29.944633: Training Step 331/354: batchLoss = 0.5428, diffLoss = 2.6592, kgLoss = 0.0137
2025-04-08 14:36:31.558379: Training Step 332/354: batchLoss = 0.6469, diffLoss = 3.1736, kgLoss = 0.0152
2025-04-08 14:36:33.170583: Training Step 333/354: batchLoss = 0.4788, diffLoss = 2.3464, kgLoss = 0.0119
2025-04-08 14:36:34.777098: Training Step 334/354: batchLoss = 0.5049, diffLoss = 2.4760, kgLoss = 0.0121
2025-04-08 14:36:36.385067: Training Step 335/354: batchLoss = 0.5846, diffLoss = 2.8594, kgLoss = 0.0160
2025-04-08 14:36:38.004960: Training Step 336/354: batchLoss = 0.6228, diffLoss = 3.0570, kgLoss = 0.0143
2025-04-08 14:36:39.627344: Training Step 337/354: batchLoss = 0.5602, diffLoss = 2.7448, kgLoss = 0.0141
2025-04-08 14:36:41.253906: Training Step 338/354: batchLoss = 0.6627, diffLoss = 3.2542, kgLoss = 0.0148
2025-04-08 14:36:42.875590: Training Step 339/354: batchLoss = 0.5982, diffLoss = 2.9347, kgLoss = 0.0141
2025-04-08 14:36:44.500766: Training Step 340/354: batchLoss = 0.5856, diffLoss = 2.8717, kgLoss = 0.0141
2025-04-08 14:36:46.133521: Training Step 341/354: batchLoss = 0.5670, diffLoss = 2.7782, kgLoss = 0.0143
2025-04-08 14:36:47.783152: Training Step 342/354: batchLoss = 0.5640, diffLoss = 2.7663, kgLoss = 0.0134
2025-04-08 14:36:49.441754: Training Step 343/354: batchLoss = 0.6594, diffLoss = 3.2304, kgLoss = 0.0166
2025-04-08 14:36:51.074344: Training Step 344/354: batchLoss = 0.6415, diffLoss = 3.1397, kgLoss = 0.0170
2025-04-08 14:36:52.708699: Training Step 345/354: batchLoss = 0.6639, diffLoss = 3.2481, kgLoss = 0.0179
2025-04-08 14:36:54.352673: Training Step 346/354: batchLoss = 0.5393, diffLoss = 2.6431, kgLoss = 0.0133
2025-04-08 14:36:55.996646: Training Step 347/354: batchLoss = 0.6787, diffLoss = 3.3243, kgLoss = 0.0172
2025-04-08 14:36:57.645095: Training Step 348/354: batchLoss = 0.5959, diffLoss = 2.9237, kgLoss = 0.0139
2025-04-08 14:36:59.279330: Training Step 349/354: batchLoss = 0.7754, diffLoss = 3.8108, kgLoss = 0.0166
2025-04-08 14:37:00.918262: Training Step 350/354: batchLoss = 0.7182, diffLoss = 3.5203, kgLoss = 0.0176
2025-04-08 14:37:02.554718: Training Step 351/354: batchLoss = 0.5999, diffLoss = 2.9421, kgLoss = 0.0144
2025-04-08 14:37:04.180524: Training Step 352/354: batchLoss = 0.5321, diffLoss = 2.6095, kgLoss = 0.0127
2025-04-08 14:37:05.601820: Training Step 353/354: batchLoss = 0.5571, diffLoss = 2.7390, kgLoss = 0.0116
2025-04-08 14:37:05.698358: 
2025-04-08 14:37:05.699009: Epoch 9/1000, Train: epLoss = 1.0739, epDfLoss = 5.2651, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 14:37:07.060642: Steps 0/138: batch_recall = 48.23, batch_ndcg = 26.72 
2025-04-08 14:37:08.404403: Steps 1/138: batch_recall = 46.45, batch_ndcg = 27.73 
2025-04-08 14:37:09.720672: Steps 2/138: batch_recall = 57.22, batch_ndcg = 35.84 
2025-04-08 14:37:11.053440: Steps 3/138: batch_recall = 56.77, batch_ndcg = 32.40 
2025-04-08 14:37:12.349758: Steps 4/138: batch_recall = 65.68, batch_ndcg = 40.22 
2025-04-08 14:37:13.720306: Steps 5/138: batch_recall = 59.20, batch_ndcg = 32.25 
2025-04-08 14:37:15.025188: Steps 6/138: batch_recall = 48.58, batch_ndcg = 28.30 
2025-04-08 14:37:16.328729: Steps 7/138: batch_recall = 60.65, batch_ndcg = 38.68 
2025-04-08 14:37:17.656201: Steps 8/138: batch_recall = 63.33, batch_ndcg = 40.08 
2025-04-08 14:37:18.958745: Steps 9/138: batch_recall = 53.40, batch_ndcg = 32.91 
2025-04-08 14:37:20.228970: Steps 10/138: batch_recall = 49.31, batch_ndcg = 28.12 
2025-04-08 14:37:21.529841: Steps 11/138: batch_recall = 59.54, batch_ndcg = 34.95 
2025-04-08 14:37:22.829215: Steps 12/138: batch_recall = 53.15, batch_ndcg = 29.43 
2025-04-08 14:37:24.122777: Steps 13/138: batch_recall = 51.18, batch_ndcg = 29.09 
2025-04-08 14:37:25.436064: Steps 14/138: batch_recall = 52.71, batch_ndcg = 30.04 
2025-04-08 14:37:26.742140: Steps 15/138: batch_recall = 50.27, batch_ndcg = 29.98 
2025-04-08 14:37:28.024067: Steps 16/138: batch_recall = 59.43, batch_ndcg = 31.94 
2025-04-08 14:37:29.306973: Steps 17/138: batch_recall = 55.91, batch_ndcg = 30.96 
2025-04-08 14:37:30.586016: Steps 18/138: batch_recall = 47.55, batch_ndcg = 31.09 
2025-04-08 14:37:31.876668: Steps 19/138: batch_recall = 50.33, batch_ndcg = 31.15 
2025-04-08 14:37:33.157040: Steps 20/138: batch_recall = 59.46, batch_ndcg = 34.03 
2025-04-08 14:37:34.442311: Steps 21/138: batch_recall = 63.71, batch_ndcg = 38.07 
2025-04-08 14:37:35.724904: Steps 22/138: batch_recall = 55.85, batch_ndcg = 32.34 
2025-04-08 14:37:37.034435: Steps 23/138: batch_recall = 52.71, batch_ndcg = 29.36 
2025-04-08 14:37:38.306413: Steps 24/138: batch_recall = 50.70, batch_ndcg = 29.49 
2025-04-08 14:37:39.590772: Steps 25/138: batch_recall = 57.61, batch_ndcg = 33.57 
2025-04-08 14:37:40.873784: Steps 26/138: batch_recall = 57.67, batch_ndcg = 33.85 
2025-04-08 14:37:42.168011: Steps 27/138: batch_recall = 64.50, batch_ndcg = 35.12 
2025-04-08 14:37:43.438488: Steps 28/138: batch_recall = 53.79, batch_ndcg = 30.97 
2025-04-08 14:37:44.710164: Steps 29/138: batch_recall = 60.79, batch_ndcg = 31.72 
2025-04-08 14:37:45.983103: Steps 30/138: batch_recall = 55.18, batch_ndcg = 33.39 
2025-04-08 14:37:47.261113: Steps 31/138: batch_recall = 40.33, batch_ndcg = 23.45 
2025-04-08 14:37:48.536373: Steps 32/138: batch_recall = 50.65, batch_ndcg = 30.26 
2025-04-08 14:37:49.800183: Steps 33/138: batch_recall = 60.02, batch_ndcg = 33.47 
2025-04-08 14:37:51.065974: Steps 34/138: batch_recall = 50.39, batch_ndcg = 27.68 
2025-04-08 14:37:52.341215: Steps 35/138: batch_recall = 50.02, batch_ndcg = 29.13 
2025-04-08 14:37:53.611160: Steps 36/138: batch_recall = 50.18, batch_ndcg = 27.75 
2025-04-08 14:37:54.880736: Steps 37/138: batch_recall = 61.03, batch_ndcg = 34.45 
2025-04-08 14:37:56.161261: Steps 38/138: batch_recall = 60.18, batch_ndcg = 32.36 
2025-04-08 14:37:57.457229: Steps 39/138: batch_recall = 70.81, batch_ndcg = 36.70 
2025-04-08 14:37:58.736327: Steps 40/138: batch_recall = 54.89, batch_ndcg = 28.42 
2025-04-08 14:38:00.008876: Steps 41/138: batch_recall = 57.91, batch_ndcg = 32.61 
2025-04-08 14:38:01.286617: Steps 42/138: batch_recall = 55.60, batch_ndcg = 30.53 
2025-04-08 14:38:02.554920: Steps 43/138: batch_recall = 55.38, batch_ndcg = 32.23 
2025-04-08 14:38:03.828045: Steps 44/138: batch_recall = 51.80, batch_ndcg = 27.28 
2025-04-08 14:38:05.100207: Steps 45/138: batch_recall = 57.86, batch_ndcg = 32.33 
2025-04-08 14:38:06.363951: Steps 46/138: batch_recall = 63.76, batch_ndcg = 35.15 
2025-04-08 14:38:07.632387: Steps 47/138: batch_recall = 56.36, batch_ndcg = 31.63 
2025-04-08 14:38:08.901967: Steps 48/138: batch_recall = 52.41, batch_ndcg = 31.46 
2025-04-08 14:38:10.178248: Steps 49/138: batch_recall = 59.87, batch_ndcg = 36.49 
2025-04-08 14:38:11.450317: Steps 50/138: batch_recall = 58.39, batch_ndcg = 32.58 
2025-04-08 14:38:12.716084: Steps 51/138: batch_recall = 62.92, batch_ndcg = 36.33 
2025-04-08 14:38:13.982030: Steps 52/138: batch_recall = 66.68, batch_ndcg = 41.23 
2025-04-08 14:38:15.276393: Steps 53/138: batch_recall = 62.34, batch_ndcg = 32.10 
2025-04-08 14:38:16.552213: Steps 54/138: batch_recall = 64.13, batch_ndcg = 35.78 
2025-04-08 14:38:17.826557: Steps 55/138: batch_recall = 58.27, batch_ndcg = 33.02 
2025-04-08 14:38:19.090095: Steps 56/138: batch_recall = 62.53, batch_ndcg = 34.17 
2025-04-08 14:38:20.372224: Steps 57/138: batch_recall = 58.28, batch_ndcg = 33.26 
2025-04-08 14:38:21.644594: Steps 58/138: batch_recall = 71.26, batch_ndcg = 36.98 
2025-04-08 14:38:22.907008: Steps 59/138: batch_recall = 78.22, batch_ndcg = 42.96 
2025-04-08 14:38:24.176736: Steps 60/138: batch_recall = 70.40, batch_ndcg = 39.46 
2025-04-08 14:38:25.447814: Steps 61/138: batch_recall = 58.15, batch_ndcg = 33.50 
2025-04-08 14:38:26.718666: Steps 62/138: batch_recall = 82.60, batch_ndcg = 43.94 
2025-04-08 14:38:27.988366: Steps 63/138: batch_recall = 77.34, batch_ndcg = 44.22 
2025-04-08 14:38:29.262760: Steps 64/138: batch_recall = 60.52, batch_ndcg = 33.27 
2025-04-08 14:38:30.544173: Steps 65/138: batch_recall = 81.46, batch_ndcg = 46.82 
2025-04-08 14:38:31.812210: Steps 66/138: batch_recall = 64.19, batch_ndcg = 39.27 
2025-04-08 14:38:33.082758: Steps 67/138: batch_recall = 80.82, batch_ndcg = 47.20 
2025-04-08 14:38:34.345005: Steps 68/138: batch_recall = 64.08, batch_ndcg = 34.00 
2025-04-08 14:38:35.606663: Steps 69/138: batch_recall = 83.72, batch_ndcg = 48.79 
2025-04-08 14:38:36.860142: Steps 70/138: batch_recall = 79.14, batch_ndcg = 45.49 
2025-04-08 14:38:38.133320: Steps 71/138: batch_recall = 87.89, batch_ndcg = 50.18 
2025-04-08 14:38:39.399651: Steps 72/138: batch_recall = 86.13, batch_ndcg = 50.04 
2025-04-08 14:38:40.651924: Steps 73/138: batch_recall = 89.56, batch_ndcg = 47.94 
2025-04-08 14:38:41.916951: Steps 74/138: batch_recall = 83.03, batch_ndcg = 48.99 
2025-04-08 14:38:43.189389: Steps 75/138: batch_recall = 87.83, batch_ndcg = 48.78 
2025-04-08 14:38:44.459879: Steps 76/138: batch_recall = 100.69, batch_ndcg = 56.74 
2025-04-08 14:38:45.747005: Steps 77/138: batch_recall = 86.16, batch_ndcg = 48.16 
2025-04-08 14:38:47.008866: Steps 78/138: batch_recall = 97.94, batch_ndcg = 50.71 
2025-04-08 14:38:48.279305: Steps 79/138: batch_recall = 92.19, batch_ndcg = 48.86 
2025-04-08 14:38:49.538907: Steps 80/138: batch_recall = 73.86, batch_ndcg = 39.24 
2025-04-08 14:38:50.812261: Steps 81/138: batch_recall = 82.36, batch_ndcg = 47.38 
2025-04-08 14:38:52.074161: Steps 82/138: batch_recall = 86.18, batch_ndcg = 49.90 
2025-04-08 14:38:53.353007: Steps 83/138: batch_recall = 87.35, batch_ndcg = 48.21 
2025-04-08 14:38:54.613557: Steps 84/138: batch_recall = 102.40, batch_ndcg = 55.38 
2025-04-08 14:38:55.902019: Steps 85/138: batch_recall = 98.63, batch_ndcg = 57.52 
2025-04-08 14:38:57.166347: Steps 86/138: batch_recall = 116.80, batch_ndcg = 71.47 
2025-04-08 14:38:58.428622: Steps 87/138: batch_recall = 104.79, batch_ndcg = 57.33 
2025-04-08 14:38:59.693643: Steps 88/138: batch_recall = 103.52, batch_ndcg = 56.82 
2025-04-08 14:39:00.952032: Steps 89/138: batch_recall = 118.54, batch_ndcg = 64.69 
2025-04-08 14:39:02.215038: Steps 90/138: batch_recall = 104.13, batch_ndcg = 57.21 
2025-04-08 14:39:03.478766: Steps 91/138: batch_recall = 122.56, batch_ndcg = 66.51 
2025-04-08 14:39:04.753106: Steps 92/138: batch_recall = 111.29, batch_ndcg = 61.14 
2025-04-08 14:39:06.009249: Steps 93/138: batch_recall = 117.14, batch_ndcg = 67.44 
2025-04-08 14:39:07.261047: Steps 94/138: batch_recall = 120.34, batch_ndcg = 61.67 
2025-04-08 14:39:08.516696: Steps 95/138: batch_recall = 110.90, batch_ndcg = 67.09 
2025-04-08 14:39:09.768128: Steps 96/138: batch_recall = 127.30, batch_ndcg = 70.94 
2025-04-08 14:39:11.031155: Steps 97/138: batch_recall = 134.55, batch_ndcg = 76.07 
2025-04-08 14:39:12.298196: Steps 98/138: batch_recall = 105.84, batch_ndcg = 61.65 
2025-04-08 14:39:13.567042: Steps 99/138: batch_recall = 124.43, batch_ndcg = 68.25 
2025-04-08 14:39:14.856715: Steps 100/138: batch_recall = 123.10, batch_ndcg = 69.86 
2025-04-08 14:39:16.133816: Steps 101/138: batch_recall = 124.77, batch_ndcg = 69.34 
2025-04-08 14:39:17.385368: Steps 102/138: batch_recall = 124.65, batch_ndcg = 73.37 
2025-04-08 14:39:18.637260: Steps 103/138: batch_recall = 143.10, batch_ndcg = 80.79 
2025-04-08 14:39:19.882064: Steps 104/138: batch_recall = 140.03, batch_ndcg = 76.42 
2025-04-08 14:39:21.114747: Steps 105/138: batch_recall = 116.30, batch_ndcg = 66.86 
2025-04-08 14:39:22.374204: Steps 106/138: batch_recall = 105.45, batch_ndcg = 59.27 
2025-04-08 14:39:23.653621: Steps 107/138: batch_recall = 118.11, batch_ndcg = 64.23 
2025-04-08 14:39:24.939742: Steps 108/138: batch_recall = 119.65, batch_ndcg = 71.36 
2025-04-08 14:39:26.213889: Steps 109/138: batch_recall = 137.66, batch_ndcg = 74.27 
2025-04-08 14:39:27.471850: Steps 110/138: batch_recall = 124.05, batch_ndcg = 65.38 
2025-04-08 14:39:28.727604: Steps 111/138: batch_recall = 139.63, batch_ndcg = 84.69 
2025-04-08 14:39:29.983749: Steps 112/138: batch_recall = 151.65, batch_ndcg = 82.77 
2025-04-08 14:39:31.243743: Steps 113/138: batch_recall = 126.11, batch_ndcg = 70.73 
2025-04-08 14:39:32.487851: Steps 114/138: batch_recall = 117.34, batch_ndcg = 68.33 
2025-04-08 14:39:33.741805: Steps 115/138: batch_recall = 120.55, batch_ndcg = 61.52 
2025-04-08 14:39:35.000612: Steps 116/138: batch_recall = 124.45, batch_ndcg = 67.10 
2025-04-08 14:39:36.253866: Steps 117/138: batch_recall = 118.06, batch_ndcg = 67.31 
2025-04-08 14:39:37.507270: Steps 118/138: batch_recall = 122.14, batch_ndcg = 70.08 
2025-04-08 14:39:38.759138: Steps 119/138: batch_recall = 138.44, batch_ndcg = 72.66 
2025-04-08 14:39:40.028886: Steps 120/138: batch_recall = 122.24, batch_ndcg = 66.58 
2025-04-08 14:39:41.301678: Steps 121/138: batch_recall = 145.62, batch_ndcg = 77.96 
2025-04-08 14:39:42.563074: Steps 122/138: batch_recall = 141.35, batch_ndcg = 77.56 
2025-04-08 14:39:43.823329: Steps 123/138: batch_recall = 130.66, batch_ndcg = 71.70 
2025-04-08 14:39:45.083696: Steps 124/138: batch_recall = 153.35, batch_ndcg = 93.71 
2025-04-08 14:39:46.341260: Steps 125/138: batch_recall = 129.25, batch_ndcg = 69.87 
2025-04-08 14:39:47.614058: Steps 126/138: batch_recall = 152.84, batch_ndcg = 86.97 
2025-04-08 14:39:48.868266: Steps 127/138: batch_recall = 140.37, batch_ndcg = 82.00 
2025-04-08 14:39:50.127535: Steps 128/138: batch_recall = 126.85, batch_ndcg = 69.67 
2025-04-08 14:39:51.367589: Steps 129/138: batch_recall = 154.40, batch_ndcg = 88.41 
2025-04-08 14:39:52.620631: Steps 130/138: batch_recall = 138.69, batch_ndcg = 70.64 
2025-04-08 14:39:53.864879: Steps 131/138: batch_recall = 142.62, batch_ndcg = 81.98 
2025-04-08 14:39:55.107921: Steps 132/138: batch_recall = 152.50, batch_ndcg = 86.07 
2025-04-08 14:39:56.334255: Steps 133/138: batch_recall = 149.52, batch_ndcg = 83.98 
2025-04-08 14:39:57.573332: Steps 134/138: batch_recall = 134.63, batch_ndcg = 76.38 
2025-04-08 14:39:58.809037: Steps 135/138: batch_recall = 162.69, batch_ndcg = 91.77 
2025-04-08 14:40:00.054704: Steps 136/138: batch_recall = 147.53, batch_ndcg = 78.19 
2025-04-08 14:40:01.311673: Steps 137/138: batch_recall = 135.82, batch_ndcg = 85.24 
2025-04-08 14:40:01.312158: Epoch 9/1000, Test: Recall = 0.1740, NDCG = 0.0981  

2025-04-08 14:40:03.086535: Training Step 0/354: batchLoss = 0.6550, diffLoss = 3.2107, kgLoss = 0.0160
2025-04-08 14:40:04.713026: Training Step 1/354: batchLoss = 0.6176, diffLoss = 3.0279, kgLoss = 0.0150
2025-04-08 14:40:06.334449: Training Step 2/354: batchLoss = 0.6407, diffLoss = 3.1379, kgLoss = 0.0164
2025-04-08 14:40:07.950030: Training Step 3/354: batchLoss = 0.5922, diffLoss = 2.9044, kgLoss = 0.0141
2025-04-08 14:40:09.579063: Training Step 4/354: batchLoss = 0.5370, diffLoss = 2.6280, kgLoss = 0.0142
2025-04-08 14:40:11.195333: Training Step 5/354: batchLoss = 0.5395, diffLoss = 2.6466, kgLoss = 0.0127
2025-04-08 14:40:12.814820: Training Step 6/354: batchLoss = 0.5562, diffLoss = 2.7194, kgLoss = 0.0153
2025-04-08 14:40:14.430526: Training Step 7/354: batchLoss = 0.5542, diffLoss = 2.7178, kgLoss = 0.0133
2025-04-08 14:40:16.049605: Training Step 8/354: batchLoss = 0.5680, diffLoss = 2.7849, kgLoss = 0.0137
2025-04-08 14:40:17.665305: Training Step 9/354: batchLoss = 0.5581, diffLoss = 2.7332, kgLoss = 0.0143
2025-04-08 14:40:19.285791: Training Step 10/354: batchLoss = 0.5970, diffLoss = 2.9224, kgLoss = 0.0157
2025-04-08 14:40:20.893182: Training Step 11/354: batchLoss = 0.6363, diffLoss = 3.1215, kgLoss = 0.0149
2025-04-08 14:40:22.504467: Training Step 12/354: batchLoss = 0.6015, diffLoss = 2.9528, kgLoss = 0.0137
2025-04-08 14:40:24.123264: Training Step 13/354: batchLoss = 0.4732, diffLoss = 2.3183, kgLoss = 0.0119
2025-04-08 14:40:25.747775: Training Step 14/354: batchLoss = 0.5626, diffLoss = 2.7528, kgLoss = 0.0151
2025-04-08 14:40:27.368434: Training Step 15/354: batchLoss = 1.0717, diffLoss = 5.2602, kgLoss = 0.0245
2025-04-08 14:40:28.988653: Training Step 16/354: batchLoss = 0.6355, diffLoss = 3.1211, kgLoss = 0.0141
2025-04-08 14:40:30.609767: Training Step 17/354: batchLoss = 0.5599, diffLoss = 2.7454, kgLoss = 0.0135
2025-04-08 14:40:32.234335: Training Step 18/354: batchLoss = 0.6484, diffLoss = 3.1731, kgLoss = 0.0172
2025-04-08 14:40:33.845031: Training Step 19/354: batchLoss = 0.5730, diffLoss = 2.8066, kgLoss = 0.0146
2025-04-08 14:40:35.458398: Training Step 20/354: batchLoss = 0.5521, diffLoss = 2.7048, kgLoss = 0.0139
2025-04-08 14:40:37.068337: Training Step 21/354: batchLoss = 0.5449, diffLoss = 2.6748, kgLoss = 0.0124
2025-04-08 14:40:38.684310: Training Step 22/354: batchLoss = 0.5210, diffLoss = 2.5521, kgLoss = 0.0132
2025-04-08 14:40:40.298230: Training Step 23/354: batchLoss = 0.5355, diffLoss = 2.6223, kgLoss = 0.0138
2025-04-08 14:40:41.919243: Training Step 24/354: batchLoss = 0.6576, diffLoss = 3.2218, kgLoss = 0.0166
2025-04-08 14:40:43.531186: Training Step 25/354: batchLoss = 0.6873, diffLoss = 3.3715, kgLoss = 0.0162
2025-04-08 14:40:45.156283: Training Step 26/354: batchLoss = 0.7219, diffLoss = 3.5372, kgLoss = 0.0180
2025-04-08 14:40:46.774463: Training Step 27/354: batchLoss = 0.5445, diffLoss = 2.6698, kgLoss = 0.0132
2025-04-08 14:40:48.385826: Training Step 28/354: batchLoss = 0.5757, diffLoss = 2.8279, kgLoss = 0.0127
2025-04-08 14:40:50.002124: Training Step 29/354: batchLoss = 0.5290, diffLoss = 2.5909, kgLoss = 0.0135
2025-04-08 14:40:51.616507: Training Step 30/354: batchLoss = 0.7266, diffLoss = 3.5683, kgLoss = 0.0161
2025-04-08 14:40:53.232066: Training Step 31/354: batchLoss = 0.5379, diffLoss = 2.6257, kgLoss = 0.0159
2025-04-08 14:40:54.852495: Training Step 32/354: batchLoss = 0.5210, diffLoss = 2.5543, kgLoss = 0.0127
2025-04-08 14:40:56.477980: Training Step 33/354: batchLoss = 0.5868, diffLoss = 2.8721, kgLoss = 0.0155
2025-04-08 14:40:58.098735: Training Step 34/354: batchLoss = 0.6147, diffLoss = 3.0179, kgLoss = 0.0140
2025-04-08 14:40:59.715740: Training Step 35/354: batchLoss = 0.5739, diffLoss = 2.8105, kgLoss = 0.0147
2025-04-08 14:41:01.333085: Training Step 36/354: batchLoss = 0.6497, diffLoss = 3.1888, kgLoss = 0.0149
2025-04-08 14:41:02.943407: Training Step 37/354: batchLoss = 0.5604, diffLoss = 2.7483, kgLoss = 0.0134
2025-04-08 14:41:04.555022: Training Step 38/354: batchLoss = 0.6654, diffLoss = 3.2611, kgLoss = 0.0165
2025-04-08 14:41:06.162941: Training Step 39/354: batchLoss = 0.6698, diffLoss = 3.2851, kgLoss = 0.0160
2025-04-08 14:41:07.782024: Training Step 40/354: batchLoss = 0.7844, diffLoss = 3.8482, kgLoss = 0.0185
2025-04-08 14:41:09.396489: Training Step 41/354: batchLoss = 0.6215, diffLoss = 3.0534, kgLoss = 0.0135
2025-04-08 14:41:11.020347: Training Step 42/354: batchLoss = 0.7716, diffLoss = 3.7884, kgLoss = 0.0175
2025-04-08 14:41:12.637610: Training Step 43/354: batchLoss = 0.6528, diffLoss = 3.2044, kgLoss = 0.0150
2025-04-08 14:41:14.253717: Training Step 44/354: batchLoss = 0.6119, diffLoss = 3.0019, kgLoss = 0.0144
2025-04-08 14:41:15.875171: Training Step 45/354: batchLoss = 0.7414, diffLoss = 3.6458, kgLoss = 0.0153
2025-04-08 14:41:17.487291: Training Step 46/354: batchLoss = 0.5274, diffLoss = 2.5861, kgLoss = 0.0128
2025-04-08 14:41:19.097237: Training Step 47/354: batchLoss = 0.4766, diffLoss = 2.3331, kgLoss = 0.0124
2025-04-08 14:41:20.713019: Training Step 48/354: batchLoss = 0.5320, diffLoss = 2.6066, kgLoss = 0.0134
2025-04-08 14:41:22.331423: Training Step 49/354: batchLoss = 0.6378, diffLoss = 3.1317, kgLoss = 0.0144
2025-04-08 14:41:23.948927: Training Step 50/354: batchLoss = 0.5399, diffLoss = 2.6444, kgLoss = 0.0138
2025-04-08 14:41:25.577602: Training Step 51/354: batchLoss = 0.5003, diffLoss = 2.4511, kgLoss = 0.0126
2025-04-08 14:41:27.199026: Training Step 52/354: batchLoss = 0.5676, diffLoss = 2.7772, kgLoss = 0.0152
2025-04-08 14:41:28.821599: Training Step 53/354: batchLoss = 0.5038, diffLoss = 2.4685, kgLoss = 0.0126
2025-04-08 14:41:30.436384: Training Step 54/354: batchLoss = 0.7038, diffLoss = 3.4473, kgLoss = 0.0179
2025-04-08 14:41:32.054465: Training Step 55/354: batchLoss = 0.5635, diffLoss = 2.7575, kgLoss = 0.0150
2025-04-08 14:41:33.666476: Training Step 56/354: batchLoss = 0.5863, diffLoss = 2.8681, kgLoss = 0.0158
2025-04-08 14:41:35.284065: Training Step 57/354: batchLoss = 0.5827, diffLoss = 2.8576, kgLoss = 0.0140
2025-04-08 14:41:36.908429: Training Step 58/354: batchLoss = 0.5850, diffLoss = 2.7634, kgLoss = 0.0404
2025-04-08 14:41:38.522754: Training Step 59/354: batchLoss = 0.6972, diffLoss = 3.4219, kgLoss = 0.0160
2025-04-08 14:41:40.140914: Training Step 60/354: batchLoss = 0.6089, diffLoss = 2.9896, kgLoss = 0.0137
2025-04-08 14:41:41.760056: Training Step 61/354: batchLoss = 0.6530, diffLoss = 3.2086, kgLoss = 0.0142
2025-04-08 14:41:43.375794: Training Step 62/354: batchLoss = 0.6156, diffLoss = 3.0172, kgLoss = 0.0152
2025-04-08 14:41:44.995279: Training Step 63/354: batchLoss = 0.8610, diffLoss = 4.2266, kgLoss = 0.0196
2025-04-08 14:41:46.621908: Training Step 64/354: batchLoss = 0.6441, diffLoss = 3.1550, kgLoss = 0.0164
2025-04-08 14:41:48.240790: Training Step 65/354: batchLoss = 0.6825, diffLoss = 3.3482, kgLoss = 0.0161
2025-04-08 14:41:49.853410: Training Step 66/354: batchLoss = 0.5632, diffLoss = 2.7617, kgLoss = 0.0136
2025-04-08 14:41:51.473540: Training Step 67/354: batchLoss = 0.6233, diffLoss = 3.0647, kgLoss = 0.0129
2025-04-08 14:41:53.092338: Training Step 68/354: batchLoss = 0.5554, diffLoss = 2.7145, kgLoss = 0.0156
2025-04-08 14:41:54.709023: Training Step 69/354: batchLoss = 0.5684, diffLoss = 2.7792, kgLoss = 0.0157
2025-04-08 14:41:56.326985: Training Step 70/354: batchLoss = 0.5075, diffLoss = 2.4717, kgLoss = 0.0165
2025-04-08 14:41:57.941219: Training Step 71/354: batchLoss = 0.5827, diffLoss = 2.8566, kgLoss = 0.0143
2025-04-08 14:41:59.558945: Training Step 72/354: batchLoss = 0.7512, diffLoss = 3.6880, kgLoss = 0.0170
2025-04-08 14:42:01.171072: Training Step 73/354: batchLoss = 0.5514, diffLoss = 2.7058, kgLoss = 0.0128
2025-04-08 14:42:02.788399: Training Step 74/354: batchLoss = 0.5614, diffLoss = 2.7495, kgLoss = 0.0143
2025-04-08 14:42:04.407802: Training Step 75/354: batchLoss = 0.6411, diffLoss = 3.1478, kgLoss = 0.0144
2025-04-08 14:42:06.026756: Training Step 76/354: batchLoss = 0.5438, diffLoss = 2.6646, kgLoss = 0.0136
2025-04-08 14:42:07.645624: Training Step 77/354: batchLoss = 0.6879, diffLoss = 3.3754, kgLoss = 0.0161
2025-04-08 14:42:09.264194: Training Step 78/354: batchLoss = 0.5582, diffLoss = 2.7288, kgLoss = 0.0156
2025-04-08 14:42:10.890919: Training Step 79/354: batchLoss = 0.5206, diffLoss = 2.5494, kgLoss = 0.0134
2025-04-08 14:42:12.508632: Training Step 80/354: batchLoss = 0.5874, diffLoss = 2.8785, kgLoss = 0.0146
2025-04-08 14:42:14.132211: Training Step 81/354: batchLoss = 0.6346, diffLoss = 3.1121, kgLoss = 0.0153
2025-04-08 14:42:15.745917: Training Step 82/354: batchLoss = 0.6480, diffLoss = 3.1641, kgLoss = 0.0189
2025-04-08 14:42:17.370669: Training Step 83/354: batchLoss = 0.5547, diffLoss = 2.7195, kgLoss = 0.0135
2025-04-08 14:42:18.988020: Training Step 84/354: batchLoss = 0.6773, diffLoss = 3.3331, kgLoss = 0.0134
2025-04-08 14:42:20.604447: Training Step 85/354: batchLoss = 0.6120, diffLoss = 2.9993, kgLoss = 0.0152
2025-04-08 14:42:22.232985: Training Step 86/354: batchLoss = 0.5662, diffLoss = 2.7722, kgLoss = 0.0148
2025-04-08 14:42:23.846369: Training Step 87/354: batchLoss = 0.5249, diffLoss = 2.5663, kgLoss = 0.0145
2025-04-08 14:42:25.476672: Training Step 88/354: batchLoss = 0.5911, diffLoss = 2.9033, kgLoss = 0.0131
2025-04-08 14:42:27.098096: Training Step 89/354: batchLoss = 0.5017, diffLoss = 2.4564, kgLoss = 0.0130
2025-04-08 14:42:28.724714: Training Step 90/354: batchLoss = 0.5387, diffLoss = 2.6348, kgLoss = 0.0147
2025-04-08 14:42:30.333903: Training Step 91/354: batchLoss = 0.5716, diffLoss = 2.8024, kgLoss = 0.0139
2025-04-08 14:42:31.944144: Training Step 92/354: batchLoss = 0.5777, diffLoss = 2.8279, kgLoss = 0.0152
2025-04-08 14:42:33.551830: Training Step 93/354: batchLoss = 0.6737, diffLoss = 3.3068, kgLoss = 0.0154
2025-04-08 14:42:35.172617: Training Step 94/354: batchLoss = 0.5205, diffLoss = 2.5525, kgLoss = 0.0125
2025-04-08 14:42:36.788151: Training Step 95/354: batchLoss = 0.5686, diffLoss = 2.7916, kgLoss = 0.0129
2025-04-08 14:42:38.405710: Training Step 96/354: batchLoss = 0.5133, diffLoss = 2.5144, kgLoss = 0.0130
2025-04-08 14:42:40.020098: Training Step 97/354: batchLoss = 0.6993, diffLoss = 3.4313, kgLoss = 0.0163
2025-04-08 14:42:41.637746: Training Step 98/354: batchLoss = 0.6985, diffLoss = 3.4266, kgLoss = 0.0165
2025-04-08 14:42:43.251508: Training Step 99/354: batchLoss = 0.5268, diffLoss = 2.5721, kgLoss = 0.0155
2025-04-08 14:42:44.865100: Training Step 100/354: batchLoss = 0.6600, diffLoss = 3.2312, kgLoss = 0.0172
2025-04-08 14:42:46.476071: Training Step 101/354: batchLoss = 0.5708, diffLoss = 2.7962, kgLoss = 0.0145
2025-04-08 14:42:48.091062: Training Step 102/354: batchLoss = 0.5896, diffLoss = 2.8924, kgLoss = 0.0139
2025-04-08 14:42:49.702907: Training Step 103/354: batchLoss = 0.5285, diffLoss = 2.5866, kgLoss = 0.0140
2025-04-08 14:42:51.314897: Training Step 104/354: batchLoss = 0.7306, diffLoss = 3.5852, kgLoss = 0.0169
2025-04-08 14:42:52.926601: Training Step 105/354: batchLoss = 0.5725, diffLoss = 2.8019, kgLoss = 0.0151
2025-04-08 14:42:54.542569: Training Step 106/354: batchLoss = 0.5680, diffLoss = 2.7825, kgLoss = 0.0144
2025-04-08 14:42:56.158696: Training Step 107/354: batchLoss = 0.5918, diffLoss = 2.8996, kgLoss = 0.0149
2025-04-08 14:42:57.776228: Training Step 108/354: batchLoss = 0.6531, diffLoss = 3.2042, kgLoss = 0.0153
2025-04-08 14:42:59.382725: Training Step 109/354: batchLoss = 0.4720, diffLoss = 2.3143, kgLoss = 0.0114
2025-04-08 14:43:00.993947: Training Step 110/354: batchLoss = 0.4866, diffLoss = 2.3859, kgLoss = 0.0118
2025-04-08 14:43:02.603641: Training Step 111/354: batchLoss = 0.6129, diffLoss = 3.0036, kgLoss = 0.0152
2025-04-08 14:43:04.216807: Training Step 112/354: batchLoss = 0.5867, diffLoss = 2.8765, kgLoss = 0.0143
2025-04-08 14:43:05.832057: Training Step 113/354: batchLoss = 0.5751, diffLoss = 2.8217, kgLoss = 0.0135
2025-04-08 14:43:07.458804: Training Step 114/354: batchLoss = 0.4962, diffLoss = 2.4269, kgLoss = 0.0136
2025-04-08 14:43:09.099472: Training Step 115/354: batchLoss = 0.5568, diffLoss = 2.7211, kgLoss = 0.0157
2025-04-08 14:43:10.731985: Training Step 116/354: batchLoss = 0.7193, diffLoss = 3.5298, kgLoss = 0.0167
2025-04-08 14:43:12.371145: Training Step 117/354: batchLoss = 0.5281, diffLoss = 2.5813, kgLoss = 0.0148
2025-04-08 14:43:14.005154: Training Step 118/354: batchLoss = 0.5291, diffLoss = 2.5964, kgLoss = 0.0122
2025-04-08 14:43:15.638610: Training Step 119/354: batchLoss = 0.5664, diffLoss = 2.7792, kgLoss = 0.0132
2025-04-08 14:43:17.269203: Training Step 120/354: batchLoss = 0.4688, diffLoss = 2.2966, kgLoss = 0.0118
2025-04-08 14:43:18.890287: Training Step 121/354: batchLoss = 0.5544, diffLoss = 2.7192, kgLoss = 0.0132
2025-04-08 14:43:20.513642: Training Step 122/354: batchLoss = 0.7405, diffLoss = 3.6279, kgLoss = 0.0187
2025-04-08 14:43:22.145431: Training Step 123/354: batchLoss = 0.6092, diffLoss = 2.9847, kgLoss = 0.0153
2025-04-08 14:43:23.784674: Training Step 124/354: batchLoss = 1.1360, diffLoss = 5.5798, kgLoss = 0.0251
2025-04-08 14:43:25.413045: Training Step 125/354: batchLoss = 0.6210, diffLoss = 3.0495, kgLoss = 0.0138
2025-04-08 14:43:27.029997: Training Step 126/354: batchLoss = 0.5732, diffLoss = 2.8118, kgLoss = 0.0135
2025-04-08 14:43:28.646392: Training Step 127/354: batchLoss = 0.5610, diffLoss = 2.7459, kgLoss = 0.0147
2025-04-08 14:43:30.264365: Training Step 128/354: batchLoss = 0.5306, diffLoss = 2.6036, kgLoss = 0.0124
2025-04-08 14:43:31.886912: Training Step 129/354: batchLoss = 0.5479, diffLoss = 2.6829, kgLoss = 0.0142
2025-04-08 14:43:33.513438: Training Step 130/354: batchLoss = 0.5088, diffLoss = 2.4906, kgLoss = 0.0133
2025-04-08 14:43:35.128582: Training Step 131/354: batchLoss = 0.6483, diffLoss = 3.1770, kgLoss = 0.0161
2025-04-08 14:43:36.754323: Training Step 132/354: batchLoss = 0.5651, diffLoss = 2.7721, kgLoss = 0.0134
2025-04-08 14:43:38.378962: Training Step 133/354: batchLoss = 0.6421, diffLoss = 3.1433, kgLoss = 0.0168
2025-04-08 14:43:40.000378: Training Step 134/354: batchLoss = 0.6964, diffLoss = 3.4213, kgLoss = 0.0151
2025-04-08 14:43:41.616182: Training Step 135/354: batchLoss = 0.6521, diffLoss = 3.1987, kgLoss = 0.0155
2025-04-08 14:43:43.231232: Training Step 136/354: batchLoss = 0.4555, diffLoss = 2.2246, kgLoss = 0.0132
2025-04-08 14:43:44.840958: Training Step 137/354: batchLoss = 0.7441, diffLoss = 3.6532, kgLoss = 0.0168
2025-04-08 14:43:46.455257: Training Step 138/354: batchLoss = 0.5212, diffLoss = 2.5536, kgLoss = 0.0131
2025-04-08 14:43:48.075073: Training Step 139/354: batchLoss = 0.5496, diffLoss = 2.6947, kgLoss = 0.0133
2025-04-08 14:43:49.701296: Training Step 140/354: batchLoss = 0.5629, diffLoss = 2.7603, kgLoss = 0.0135
2025-04-08 14:43:51.324656: Training Step 141/354: batchLoss = 0.6253, diffLoss = 3.0606, kgLoss = 0.0164
2025-04-08 14:43:52.943022: Training Step 142/354: batchLoss = 0.5307, diffLoss = 2.6035, kgLoss = 0.0124
2025-04-08 14:43:54.557473: Training Step 143/354: batchLoss = 0.7415, diffLoss = 3.6403, kgLoss = 0.0168
2025-04-08 14:43:56.186040: Training Step 144/354: batchLoss = 0.5418, diffLoss = 2.6513, kgLoss = 0.0145
2025-04-08 14:43:57.802634: Training Step 145/354: batchLoss = 0.5398, diffLoss = 2.6447, kgLoss = 0.0136
2025-04-08 14:43:59.421844: Training Step 146/354: batchLoss = 0.5778, diffLoss = 2.8308, kgLoss = 0.0146
2025-04-08 14:44:01.043640: Training Step 147/354: batchLoss = 0.5486, diffLoss = 2.6889, kgLoss = 0.0135
2025-04-08 14:44:02.669889: Training Step 148/354: batchLoss = 0.6284, diffLoss = 3.0887, kgLoss = 0.0133
2025-04-08 14:44:04.296675: Training Step 149/354: batchLoss = 0.6542, diffLoss = 3.2126, kgLoss = 0.0146
2025-04-08 14:44:05.906876: Training Step 150/354: batchLoss = 0.6096, diffLoss = 2.9864, kgLoss = 0.0153
2025-04-08 14:44:07.534125: Training Step 151/354: batchLoss = 0.6055, diffLoss = 2.9707, kgLoss = 0.0142
2025-04-08 14:44:09.155660: Training Step 152/354: batchLoss = 0.8212, diffLoss = 4.0374, kgLoss = 0.0171
2025-04-08 14:44:10.774810: Training Step 153/354: batchLoss = 0.5684, diffLoss = 2.7894, kgLoss = 0.0132
2025-04-08 14:44:12.397164: Training Step 154/354: batchLoss = 0.5172, diffLoss = 2.5295, kgLoss = 0.0142
2025-04-08 14:44:14.023698: Training Step 155/354: batchLoss = 0.5644, diffLoss = 2.7693, kgLoss = 0.0131
2025-04-08 14:44:15.639491: Training Step 156/354: batchLoss = 0.5529, diffLoss = 2.6995, kgLoss = 0.0163
2025-04-08 14:44:17.278942: Training Step 157/354: batchLoss = 0.7545, diffLoss = 3.6900, kgLoss = 0.0206
2025-04-08 14:44:18.894610: Training Step 158/354: batchLoss = 0.5654, diffLoss = 2.7690, kgLoss = 0.0145
2025-04-08 14:44:20.516496: Training Step 159/354: batchLoss = 0.6130, diffLoss = 3.0065, kgLoss = 0.0147
2025-04-08 14:44:22.134502: Training Step 160/354: batchLoss = 0.5929, diffLoss = 2.9077, kgLoss = 0.0142
2025-04-08 14:44:23.752651: Training Step 161/354: batchLoss = 0.5196, diffLoss = 2.5422, kgLoss = 0.0140
2025-04-08 14:44:25.377046: Training Step 162/354: batchLoss = 0.6236, diffLoss = 3.0621, kgLoss = 0.0140
2025-04-08 14:44:26.993378: Training Step 163/354: batchLoss = 0.6944, diffLoss = 3.4042, kgLoss = 0.0169
2025-04-08 14:44:28.605718: Training Step 164/354: batchLoss = 0.4805, diffLoss = 2.3546, kgLoss = 0.0120
2025-04-08 14:44:30.223825: Training Step 165/354: batchLoss = 0.6798, diffLoss = 3.3378, kgLoss = 0.0152
2025-04-08 14:44:31.835716: Training Step 166/354: batchLoss = 0.5352, diffLoss = 2.6170, kgLoss = 0.0148
2025-04-08 14:44:33.459840: Training Step 167/354: batchLoss = 0.5665, diffLoss = 2.7777, kgLoss = 0.0137
2025-04-08 14:44:35.082731: Training Step 168/354: batchLoss = 0.5547, diffLoss = 2.7133, kgLoss = 0.0151
2025-04-08 14:44:36.705729: Training Step 169/354: batchLoss = 0.7070, diffLoss = 3.4680, kgLoss = 0.0167
2025-04-08 14:44:38.331571: Training Step 170/354: batchLoss = 0.5292, diffLoss = 2.5932, kgLoss = 0.0132
2025-04-08 14:44:39.959184: Training Step 171/354: batchLoss = 0.4924, diffLoss = 2.4088, kgLoss = 0.0133
2025-04-08 14:44:41.569870: Training Step 172/354: batchLoss = 0.5638, diffLoss = 2.7618, kgLoss = 0.0143
2025-04-08 14:44:43.186380: Training Step 173/354: batchLoss = 0.5114, diffLoss = 2.4995, kgLoss = 0.0143
2025-04-08 14:44:44.799352: Training Step 174/354: batchLoss = 0.6924, diffLoss = 3.4042, kgLoss = 0.0144
2025-04-08 14:44:46.426168: Training Step 175/354: batchLoss = 0.5776, diffLoss = 2.8364, kgLoss = 0.0129
2025-04-08 14:44:48.046895: Training Step 176/354: batchLoss = 0.6433, diffLoss = 3.1481, kgLoss = 0.0171
2025-04-08 14:44:49.662985: Training Step 177/354: batchLoss = 0.6334, diffLoss = 3.1115, kgLoss = 0.0139
2025-04-08 14:44:51.280502: Training Step 178/354: batchLoss = 0.5882, diffLoss = 2.8792, kgLoss = 0.0154
2025-04-08 14:44:52.902300: Training Step 179/354: batchLoss = 0.6038, diffLoss = 2.9651, kgLoss = 0.0135
2025-04-08 14:44:54.519365: Training Step 180/354: batchLoss = 0.5702, diffLoss = 2.7943, kgLoss = 0.0142
2025-04-08 14:44:56.145632: Training Step 181/354: batchLoss = 0.6547, diffLoss = 3.2111, kgLoss = 0.0156
2025-04-08 14:44:57.762305: Training Step 182/354: batchLoss = 0.4524, diffLoss = 2.2102, kgLoss = 0.0129
2025-04-08 14:44:59.379434: Training Step 183/354: batchLoss = 0.5239, diffLoss = 2.5686, kgLoss = 0.0127
2025-04-08 14:45:00.994906: Training Step 184/354: batchLoss = 0.5590, diffLoss = 2.7424, kgLoss = 0.0131
2025-04-08 14:45:02.612593: Training Step 185/354: batchLoss = 0.5353, diffLoss = 2.6250, kgLoss = 0.0128
2025-04-08 14:45:04.228499: Training Step 186/354: batchLoss = 0.5384, diffLoss = 2.6418, kgLoss = 0.0126
2025-04-08 14:45:05.863422: Training Step 187/354: batchLoss = 0.4944, diffLoss = 2.4230, kgLoss = 0.0122
2025-04-08 14:45:07.489523: Training Step 188/354: batchLoss = 0.5671, diffLoss = 2.7807, kgLoss = 0.0137
2025-04-08 14:45:09.107296: Training Step 189/354: batchLoss = 0.6688, diffLoss = 3.2790, kgLoss = 0.0162
2025-04-08 14:45:10.727351: Training Step 190/354: batchLoss = 0.7217, diffLoss = 3.5413, kgLoss = 0.0167
2025-04-08 14:45:12.339592: Training Step 191/354: batchLoss = 0.5551, diffLoss = 2.7234, kgLoss = 0.0131
2025-04-08 14:45:13.958173: Training Step 192/354: batchLoss = 0.5291, diffLoss = 2.5930, kgLoss = 0.0131
2025-04-08 14:45:15.581064: Training Step 193/354: batchLoss = 0.5182, diffLoss = 2.5391, kgLoss = 0.0130
2025-04-08 14:45:17.198463: Training Step 194/354: batchLoss = 0.5286, diffLoss = 2.5978, kgLoss = 0.0113
2025-04-08 14:45:18.819699: Training Step 195/354: batchLoss = 0.5714, diffLoss = 2.8047, kgLoss = 0.0131
2025-04-08 14:45:20.441903: Training Step 196/354: batchLoss = 0.5126, diffLoss = 2.5113, kgLoss = 0.0129
2025-04-08 14:45:22.058889: Training Step 197/354: batchLoss = 0.6068, diffLoss = 2.9750, kgLoss = 0.0148
2025-04-08 14:45:23.677049: Training Step 198/354: batchLoss = 0.7376, diffLoss = 3.6185, kgLoss = 0.0174
2025-04-08 14:45:25.292178: Training Step 199/354: batchLoss = 0.7396, diffLoss = 3.6235, kgLoss = 0.0186
2025-04-08 14:45:26.905087: Training Step 200/354: batchLoss = 0.5298, diffLoss = 2.6000, kgLoss = 0.0123
2025-04-08 14:45:28.515389: Training Step 201/354: batchLoss = 0.7251, diffLoss = 3.5538, kgLoss = 0.0179
2025-04-08 14:45:30.131681: Training Step 202/354: batchLoss = 0.6277, diffLoss = 3.0698, kgLoss = 0.0172
2025-04-08 14:45:31.745832: Training Step 203/354: batchLoss = 0.5408, diffLoss = 2.6558, kgLoss = 0.0121
2025-04-08 14:45:33.378368: Training Step 204/354: batchLoss = 0.8709, diffLoss = 4.2657, kgLoss = 0.0222
2025-04-08 14:45:34.996615: Training Step 205/354: batchLoss = 0.5119, diffLoss = 2.4951, kgLoss = 0.0161
2025-04-08 14:45:36.619946: Training Step 206/354: batchLoss = 0.5603, diffLoss = 2.7509, kgLoss = 0.0127
2025-04-08 14:45:38.243156: Training Step 207/354: batchLoss = 0.6352, diffLoss = 3.1136, kgLoss = 0.0156
2025-04-08 14:45:39.865160: Training Step 208/354: batchLoss = 0.5427, diffLoss = 2.6546, kgLoss = 0.0147
2025-04-08 14:45:41.476517: Training Step 209/354: batchLoss = 0.5972, diffLoss = 2.9228, kgLoss = 0.0158
2025-04-08 14:45:43.094298: Training Step 210/354: batchLoss = 0.6928, diffLoss = 3.3879, kgLoss = 0.0190
2025-04-08 14:45:44.710401: Training Step 211/354: batchLoss = 0.4974, diffLoss = 2.4362, kgLoss = 0.0128
2025-04-08 14:45:46.333583: Training Step 212/354: batchLoss = 0.5613, diffLoss = 2.7507, kgLoss = 0.0139
2025-04-08 14:45:47.965379: Training Step 213/354: batchLoss = 0.5404, diffLoss = 2.6561, kgLoss = 0.0115
2025-04-08 14:45:49.584535: Training Step 214/354: batchLoss = 0.6582, diffLoss = 3.2206, kgLoss = 0.0177
2025-04-08 14:45:51.216112: Training Step 215/354: batchLoss = 0.6447, diffLoss = 3.1604, kgLoss = 0.0157
2025-04-08 14:45:52.833867: Training Step 216/354: batchLoss = 0.5219, diffLoss = 2.5543, kgLoss = 0.0138
2025-04-08 14:45:54.453682: Training Step 217/354: batchLoss = 0.5568, diffLoss = 2.7266, kgLoss = 0.0143
2025-04-08 14:45:56.078083: Training Step 218/354: batchLoss = 0.5543, diffLoss = 2.7160, kgLoss = 0.0139
2025-04-08 14:45:57.694917: Training Step 219/354: batchLoss = 0.5686, diffLoss = 2.7902, kgLoss = 0.0132
2025-04-08 14:45:59.312930: Training Step 220/354: batchLoss = 0.6424, diffLoss = 3.1489, kgLoss = 0.0158
2025-04-08 14:46:00.935180: Training Step 221/354: batchLoss = 0.4965, diffLoss = 2.4298, kgLoss = 0.0132
2025-04-08 14:46:02.556301: Training Step 222/354: batchLoss = 0.4759, diffLoss = 2.3309, kgLoss = 0.0122
2025-04-08 14:46:04.165442: Training Step 223/354: batchLoss = 0.5939, diffLoss = 2.9084, kgLoss = 0.0153
2025-04-08 14:46:05.783715: Training Step 224/354: batchLoss = 0.5248, diffLoss = 2.5748, kgLoss = 0.0124
2025-04-08 14:46:07.402613: Training Step 225/354: batchLoss = 0.5681, diffLoss = 2.7834, kgLoss = 0.0143
2025-04-08 14:46:09.015390: Training Step 226/354: batchLoss = 0.7094, diffLoss = 3.4763, kgLoss = 0.0176
2025-04-08 14:46:10.622767: Training Step 227/354: batchLoss = 0.6460, diffLoss = 3.1675, kgLoss = 0.0157
2025-04-08 14:46:12.236630: Training Step 228/354: batchLoss = 0.4578, diffLoss = 2.2403, kgLoss = 0.0121
2025-04-08 14:46:13.851285: Training Step 229/354: batchLoss = 0.6031, diffLoss = 2.9582, kgLoss = 0.0144
2025-04-08 14:46:15.467756: Training Step 230/354: batchLoss = 0.5567, diffLoss = 2.7302, kgLoss = 0.0133
2025-04-08 14:46:17.092039: Training Step 231/354: batchLoss = 0.5128, diffLoss = 2.5116, kgLoss = 0.0131
2025-04-08 14:46:18.713988: Training Step 232/354: batchLoss = 0.6115, diffLoss = 3.0009, kgLoss = 0.0141
2025-04-08 14:46:20.328061: Training Step 233/354: batchLoss = 0.7470, diffLoss = 3.6628, kgLoss = 0.0180
2025-04-08 14:46:21.952003: Training Step 234/354: batchLoss = 0.5963, diffLoss = 2.9213, kgLoss = 0.0150
2025-04-08 14:46:23.579441: Training Step 235/354: batchLoss = 0.5666, diffLoss = 2.7780, kgLoss = 0.0138
2025-04-08 14:46:25.207758: Training Step 236/354: batchLoss = 0.6285, diffLoss = 3.0805, kgLoss = 0.0155
2025-04-08 14:46:26.844302: Training Step 237/354: batchLoss = 0.4827, diffLoss = 2.3634, kgLoss = 0.0126
2025-04-08 14:46:28.475766: Training Step 238/354: batchLoss = 0.7897, diffLoss = 3.8819, kgLoss = 0.0166
2025-04-08 14:46:30.101742: Training Step 239/354: batchLoss = 0.5141, diffLoss = 2.5156, kgLoss = 0.0138
2025-04-08 14:46:31.730103: Training Step 240/354: batchLoss = 0.6558, diffLoss = 3.2192, kgLoss = 0.0149
2025-04-08 14:46:33.358921: Training Step 241/354: batchLoss = 0.6026, diffLoss = 2.9464, kgLoss = 0.0166
2025-04-08 14:46:34.992743: Training Step 242/354: batchLoss = 0.6025, diffLoss = 2.9592, kgLoss = 0.0133
2025-04-08 14:46:36.614774: Training Step 243/354: batchLoss = 0.4881, diffLoss = 2.3993, kgLoss = 0.0103
2025-04-08 14:46:38.235852: Training Step 244/354: batchLoss = 0.7577, diffLoss = 3.7175, kgLoss = 0.0177
2025-04-08 14:46:39.851978: Training Step 245/354: batchLoss = 0.5665, diffLoss = 2.7729, kgLoss = 0.0149
2025-04-08 14:46:41.469197: Training Step 246/354: batchLoss = 0.5550, diffLoss = 2.7140, kgLoss = 0.0153
2025-04-08 14:46:43.082081: Training Step 247/354: batchLoss = 0.6169, diffLoss = 3.0318, kgLoss = 0.0132
2025-04-08 14:46:44.703970: Training Step 248/354: batchLoss = 0.6580, diffLoss = 3.2266, kgLoss = 0.0158
2025-04-08 14:46:46.320519: Training Step 249/354: batchLoss = 0.6568, diffLoss = 3.2219, kgLoss = 0.0155
2025-04-08 14:46:47.942113: Training Step 250/354: batchLoss = 0.5930, diffLoss = 2.9099, kgLoss = 0.0138
2025-04-08 14:46:49.571258: Training Step 251/354: batchLoss = 0.6352, diffLoss = 3.1237, kgLoss = 0.0131
2025-04-08 14:46:51.192098: Training Step 252/354: batchLoss = 0.5319, diffLoss = 2.6000, kgLoss = 0.0149
2025-04-08 14:46:52.804261: Training Step 253/354: batchLoss = 0.5131, diffLoss = 2.5123, kgLoss = 0.0133
2025-04-08 14:46:54.418696: Training Step 254/354: batchLoss = 0.6309, diffLoss = 3.0990, kgLoss = 0.0139
2025-04-08 14:46:56.029867: Training Step 255/354: batchLoss = 0.5212, diffLoss = 2.5540, kgLoss = 0.0130
2025-04-08 14:46:57.644280: Training Step 256/354: batchLoss = 0.4985, diffLoss = 2.4349, kgLoss = 0.0144
2025-04-08 14:46:59.261755: Training Step 257/354: batchLoss = 0.5267, diffLoss = 2.5794, kgLoss = 0.0136
2025-04-08 14:47:00.884346: Training Step 258/354: batchLoss = 0.6256, diffLoss = 3.0660, kgLoss = 0.0154
2025-04-08 14:47:02.499000: Training Step 259/354: batchLoss = 0.5756, diffLoss = 2.8226, kgLoss = 0.0138
2025-04-08 14:47:04.117621: Training Step 260/354: batchLoss = 0.4708, diffLoss = 2.3090, kgLoss = 0.0112
2025-04-08 14:47:05.732924: Training Step 261/354: batchLoss = 0.5237, diffLoss = 2.5656, kgLoss = 0.0132
2025-04-08 14:47:07.350501: Training Step 262/354: batchLoss = 0.5403, diffLoss = 2.6492, kgLoss = 0.0130
2025-04-08 14:47:08.966887: Training Step 263/354: batchLoss = 0.4375, diffLoss = 2.1365, kgLoss = 0.0128
2025-04-08 14:47:10.576777: Training Step 264/354: batchLoss = 0.5164, diffLoss = 2.5327, kgLoss = 0.0123
2025-04-08 14:47:12.189903: Training Step 265/354: batchLoss = 0.5960, diffLoss = 2.9167, kgLoss = 0.0158
2025-04-08 14:47:13.808186: Training Step 266/354: batchLoss = 0.5512, diffLoss = 2.7022, kgLoss = 0.0135
2025-04-08 14:47:15.421082: Training Step 267/354: batchLoss = 0.6195, diffLoss = 3.0426, kgLoss = 0.0137
2025-04-08 14:47:17.043443: Training Step 268/354: batchLoss = 0.4653, diffLoss = 2.2798, kgLoss = 0.0117
2025-04-08 14:47:18.660874: Training Step 269/354: batchLoss = 0.6384, diffLoss = 3.1278, kgLoss = 0.0160
2025-04-08 14:47:20.281025: Training Step 270/354: batchLoss = 0.6837, diffLoss = 3.3538, kgLoss = 0.0161
2025-04-08 14:47:21.898802: Training Step 271/354: batchLoss = 0.6637, diffLoss = 3.2499, kgLoss = 0.0171
2025-04-08 14:47:23.514626: Training Step 272/354: batchLoss = 0.5908, diffLoss = 2.8960, kgLoss = 0.0145
2025-04-08 14:47:25.138492: Training Step 273/354: batchLoss = 0.4950, diffLoss = 2.4229, kgLoss = 0.0131
2025-04-08 14:47:26.755271: Training Step 274/354: batchLoss = 0.5938, diffLoss = 2.9115, kgLoss = 0.0144
2025-04-08 14:47:28.376686: Training Step 275/354: batchLoss = 0.5866, diffLoss = 2.8746, kgLoss = 0.0146
2025-04-08 14:47:29.986899: Training Step 276/354: batchLoss = 0.5006, diffLoss = 2.4519, kgLoss = 0.0128
2025-04-08 14:47:31.602318: Training Step 277/354: batchLoss = 0.9089, diffLoss = 4.4629, kgLoss = 0.0204
2025-04-08 14:47:33.232064: Training Step 278/354: batchLoss = 0.6729, diffLoss = 3.2957, kgLoss = 0.0172
2025-04-08 14:47:34.850585: Training Step 279/354: batchLoss = 0.4727, diffLoss = 2.3136, kgLoss = 0.0124
2025-04-08 14:47:36.469674: Training Step 280/354: batchLoss = 0.6042, diffLoss = 2.9602, kgLoss = 0.0152
2025-04-08 14:47:38.087224: Training Step 281/354: batchLoss = 0.6007, diffLoss = 2.9449, kgLoss = 0.0147
2025-04-08 14:47:39.703246: Training Step 282/354: batchLoss = 0.4999, diffLoss = 2.4471, kgLoss = 0.0131
2025-04-08 14:47:41.332916: Training Step 283/354: batchLoss = 0.5069, diffLoss = 2.4827, kgLoss = 0.0129
2025-04-08 14:47:42.951400: Training Step 284/354: batchLoss = 0.5721, diffLoss = 2.7998, kgLoss = 0.0152
2025-04-08 14:47:44.574046: Training Step 285/354: batchLoss = 0.5719, diffLoss = 2.8079, kgLoss = 0.0129
2025-04-08 14:47:46.193875: Training Step 286/354: batchLoss = 0.4527, diffLoss = 2.2179, kgLoss = 0.0114
2025-04-08 14:47:47.810606: Training Step 287/354: batchLoss = 0.7325, diffLoss = 3.5961, kgLoss = 0.0166
2025-04-08 14:47:49.434574: Training Step 288/354: batchLoss = 0.5827, diffLoss = 2.8549, kgLoss = 0.0147
2025-04-08 14:47:51.055467: Training Step 289/354: batchLoss = 0.6146, diffLoss = 3.0102, kgLoss = 0.0157
2025-04-08 14:47:52.659181: Training Step 290/354: batchLoss = 0.4565, diffLoss = 2.2186, kgLoss = 0.0160
2025-04-08 14:47:54.272711: Training Step 291/354: batchLoss = 0.6277, diffLoss = 3.0874, kgLoss = 0.0127
2025-04-08 14:47:55.886387: Training Step 292/354: batchLoss = 0.6184, diffLoss = 3.0346, kgLoss = 0.0143
2025-04-08 14:47:57.493153: Training Step 293/354: batchLoss = 0.4762, diffLoss = 2.3269, kgLoss = 0.0135
2025-04-08 14:47:59.110221: Training Step 294/354: batchLoss = 0.7808, diffLoss = 3.8302, kgLoss = 0.0185
2025-04-08 14:48:00.723115: Training Step 295/354: batchLoss = 0.6529, diffLoss = 3.2022, kgLoss = 0.0155
2025-04-08 14:48:02.344411: Training Step 296/354: batchLoss = 0.6011, diffLoss = 2.9445, kgLoss = 0.0153
2025-04-08 14:48:03.965236: Training Step 297/354: batchLoss = 0.5092, diffLoss = 2.4914, kgLoss = 0.0137
2025-04-08 14:48:05.587514: Training Step 298/354: batchLoss = 0.6592, diffLoss = 3.2371, kgLoss = 0.0147
2025-04-08 14:48:07.205525: Training Step 299/354: batchLoss = 0.6858, diffLoss = 3.3588, kgLoss = 0.0176
2025-04-08 14:48:08.818720: Training Step 300/354: batchLoss = 0.5926, diffLoss = 2.9106, kgLoss = 0.0131
2025-04-08 14:48:10.428424: Training Step 301/354: batchLoss = 0.5984, diffLoss = 2.9329, kgLoss = 0.0148
2025-04-08 14:48:12.046020: Training Step 302/354: batchLoss = 0.6568, diffLoss = 3.2229, kgLoss = 0.0153
2025-04-08 14:48:13.661894: Training Step 303/354: batchLoss = 0.6133, diffLoss = 3.0157, kgLoss = 0.0127
2025-04-08 14:48:15.284659: Training Step 304/354: batchLoss = 0.6198, diffLoss = 3.0421, kgLoss = 0.0143
2025-04-08 14:48:16.908667: Training Step 305/354: batchLoss = 0.6210, diffLoss = 3.0437, kgLoss = 0.0153
2025-04-08 14:48:18.535957: Training Step 306/354: batchLoss = 0.5447, diffLoss = 2.6694, kgLoss = 0.0136
2025-04-08 14:48:20.154690: Training Step 307/354: batchLoss = 0.6287, diffLoss = 3.0829, kgLoss = 0.0152
2025-04-08 14:48:21.783906: Training Step 308/354: batchLoss = 0.5165, diffLoss = 2.5365, kgLoss = 0.0115
2025-04-08 14:48:23.398929: Training Step 309/354: batchLoss = 0.5997, diffLoss = 2.9260, kgLoss = 0.0181
2025-04-08 14:48:25.011699: Training Step 310/354: batchLoss = 0.5436, diffLoss = 2.6617, kgLoss = 0.0140
2025-04-08 14:48:26.625605: Training Step 311/354: batchLoss = 0.5781, diffLoss = 2.8357, kgLoss = 0.0137
2025-04-08 14:48:28.245715: Training Step 312/354: batchLoss = 0.5215, diffLoss = 2.5587, kgLoss = 0.0121
2025-04-08 14:48:29.871846: Training Step 313/354: batchLoss = 0.4995, diffLoss = 2.4456, kgLoss = 0.0129
2025-04-08 14:48:31.492992: Training Step 314/354: batchLoss = 0.7264, diffLoss = 3.5615, kgLoss = 0.0177
2025-04-08 14:48:33.111266: Training Step 315/354: batchLoss = 0.6671, diffLoss = 3.2741, kgLoss = 0.0153
2025-04-08 14:48:34.727514: Training Step 316/354: batchLoss = 0.5200, diffLoss = 2.5368, kgLoss = 0.0158
2025-04-08 14:48:36.350382: Training Step 317/354: batchLoss = 0.5054, diffLoss = 2.4774, kgLoss = 0.0124
2025-04-08 14:48:37.966690: Training Step 318/354: batchLoss = 0.5820, diffLoss = 2.8553, kgLoss = 0.0136
2025-04-08 14:48:39.581644: Training Step 319/354: batchLoss = 0.6935, diffLoss = 3.4047, kgLoss = 0.0157
2025-04-08 14:48:41.201871: Training Step 320/354: batchLoss = 0.5978, diffLoss = 2.9274, kgLoss = 0.0154
2025-04-08 14:48:42.822634: Training Step 321/354: batchLoss = 0.7308, diffLoss = 3.5911, kgLoss = 0.0158
2025-04-08 14:48:44.443208: Training Step 322/354: batchLoss = 0.5704, diffLoss = 2.7930, kgLoss = 0.0148
2025-04-08 14:48:46.062007: Training Step 323/354: batchLoss = 0.6650, diffLoss = 3.2655, kgLoss = 0.0149
2025-04-08 14:48:47.687172: Training Step 324/354: batchLoss = 0.7219, diffLoss = 3.5422, kgLoss = 0.0168
2025-04-08 14:48:49.312771: Training Step 325/354: batchLoss = 0.6041, diffLoss = 2.9608, kgLoss = 0.0150
2025-04-08 14:48:50.944583: Training Step 326/354: batchLoss = 0.6970, diffLoss = 3.4182, kgLoss = 0.0166
2025-04-08 14:48:52.569113: Training Step 327/354: batchLoss = 0.4758, diffLoss = 2.3272, kgLoss = 0.0130
2025-04-08 14:48:54.177679: Training Step 328/354: batchLoss = 0.7375, diffLoss = 3.6144, kgLoss = 0.0182
2025-04-08 14:48:55.795800: Training Step 329/354: batchLoss = 0.4785, diffLoss = 2.3426, kgLoss = 0.0125
2025-04-08 14:48:57.414399: Training Step 330/354: batchLoss = 0.6294, diffLoss = 3.0891, kgLoss = 0.0145
2025-04-08 14:48:59.034695: Training Step 331/354: batchLoss = 0.5219, diffLoss = 2.5649, kgLoss = 0.0111
2025-04-08 14:49:00.651114: Training Step 332/354: batchLoss = 0.6560, diffLoss = 3.2169, kgLoss = 0.0158
2025-04-08 14:49:02.265153: Training Step 333/354: batchLoss = 0.6132, diffLoss = 3.0109, kgLoss = 0.0137
2025-04-08 14:49:03.884391: Training Step 334/354: batchLoss = 0.5901, diffLoss = 2.8892, kgLoss = 0.0153
2025-04-08 14:49:05.503374: Training Step 335/354: batchLoss = 0.6178, diffLoss = 3.0323, kgLoss = 0.0141
2025-04-08 14:49:07.122118: Training Step 336/354: batchLoss = 0.5495, diffLoss = 2.6881, kgLoss = 0.0149
2025-04-08 14:49:08.733822: Training Step 337/354: batchLoss = 0.6428, diffLoss = 3.1525, kgLoss = 0.0153
2025-04-08 14:49:10.349971: Training Step 338/354: batchLoss = 0.7207, diffLoss = 3.5320, kgLoss = 0.0179
2025-04-08 14:49:11.967600: Training Step 339/354: batchLoss = 0.6610, diffLoss = 3.2442, kgLoss = 0.0152
2025-04-08 14:49:13.579326: Training Step 340/354: batchLoss = 0.6488, diffLoss = 3.1812, kgLoss = 0.0157
2025-04-08 14:49:15.198080: Training Step 341/354: batchLoss = 0.4820, diffLoss = 2.3631, kgLoss = 0.0118
2025-04-08 14:49:16.811625: Training Step 342/354: batchLoss = 0.5254, diffLoss = 2.5750, kgLoss = 0.0130
2025-04-08 14:49:18.430436: Training Step 343/354: batchLoss = 0.4738, diffLoss = 2.3106, kgLoss = 0.0146
2025-04-08 14:49:20.047322: Training Step 344/354: batchLoss = 0.4905, diffLoss = 2.4004, kgLoss = 0.0130
2025-04-08 14:49:21.658687: Training Step 345/354: batchLoss = 0.5188, diffLoss = 2.5350, kgLoss = 0.0147
2025-04-08 14:49:23.278004: Training Step 346/354: batchLoss = 0.5886, diffLoss = 2.8812, kgLoss = 0.0155
2025-04-08 14:49:24.891108: Training Step 347/354: batchLoss = 0.6868, diffLoss = 3.3676, kgLoss = 0.0165
2025-04-08 14:49:26.511499: Training Step 348/354: batchLoss = 0.6031, diffLoss = 2.9588, kgLoss = 0.0142
2025-04-08 14:49:28.127521: Training Step 349/354: batchLoss = 0.5749, diffLoss = 2.8182, kgLoss = 0.0141
2025-04-08 14:49:29.759630: Training Step 350/354: batchLoss = 0.5847, diffLoss = 2.8627, kgLoss = 0.0152
2025-04-08 14:49:31.366233: Training Step 351/354: batchLoss = 0.5720, diffLoss = 2.8022, kgLoss = 0.0144
2025-04-08 14:49:32.973682: Training Step 352/354: batchLoss = 0.6801, diffLoss = 3.3363, kgLoss = 0.0161
2025-04-08 14:49:34.390370: Training Step 353/354: batchLoss = 0.5064, diffLoss = 2.4804, kgLoss = 0.0129
2025-04-08 14:49:34.488067: 
2025-04-08 14:49:34.488751: Epoch 10/1000, Train: epLoss = 1.0544, epDfLoss = 5.1678, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 14:49:35.842875: Steps 0/138: batch_recall = 46.01, batch_ndcg = 26.10 
2025-04-08 14:49:37.194145: Steps 1/138: batch_recall = 47.32, batch_ndcg = 27.77 
2025-04-08 14:49:38.530803: Steps 2/138: batch_recall = 57.10, batch_ndcg = 35.96 
2025-04-08 14:49:39.846717: Steps 3/138: batch_recall = 58.06, batch_ndcg = 33.03 
2025-04-08 14:49:41.153121: Steps 4/138: batch_recall = 65.62, batch_ndcg = 39.92 
2025-04-08 14:49:42.471084: Steps 5/138: batch_recall = 61.53, batch_ndcg = 32.75 
2025-04-08 14:49:43.796448: Steps 6/138: batch_recall = 48.15, batch_ndcg = 28.31 
2025-04-08 14:49:45.127974: Steps 7/138: batch_recall = 62.24, batch_ndcg = 39.45 
2025-04-08 14:49:46.485329: Steps 8/138: batch_recall = 63.62, batch_ndcg = 39.46 
2025-04-08 14:49:47.793677: Steps 9/138: batch_recall = 52.55, batch_ndcg = 32.75 
2025-04-08 14:49:49.112801: Steps 10/138: batch_recall = 53.81, batch_ndcg = 29.46 
2025-04-08 14:49:50.417570: Steps 11/138: batch_recall = 60.27, batch_ndcg = 34.27 
2025-04-08 14:49:51.715907: Steps 12/138: batch_recall = 52.23, batch_ndcg = 28.95 
2025-04-08 14:49:52.999588: Steps 13/138: batch_recall = 51.25, batch_ndcg = 29.21 
2025-04-08 14:49:54.297009: Steps 14/138: batch_recall = 55.35, batch_ndcg = 30.97 
2025-04-08 14:49:55.589741: Steps 15/138: batch_recall = 50.32, batch_ndcg = 29.52 
2025-04-08 14:49:56.887426: Steps 16/138: batch_recall = 59.80, batch_ndcg = 32.76 
2025-04-08 14:49:58.194346: Steps 17/138: batch_recall = 56.82, batch_ndcg = 31.14 
2025-04-08 14:49:59.507845: Steps 18/138: batch_recall = 50.04, batch_ndcg = 31.88 
2025-04-08 14:50:00.805393: Steps 19/138: batch_recall = 51.40, batch_ndcg = 31.48 
2025-04-08 14:50:02.110794: Steps 20/138: batch_recall = 61.04, batch_ndcg = 35.27 
2025-04-08 14:50:03.410488: Steps 21/138: batch_recall = 65.37, batch_ndcg = 39.11 
2025-04-08 14:50:04.706752: Steps 22/138: batch_recall = 56.30, batch_ndcg = 32.55 
2025-04-08 14:50:06.006312: Steps 23/138: batch_recall = 52.40, batch_ndcg = 29.54 
2025-04-08 14:50:07.293206: Steps 24/138: batch_recall = 52.25, batch_ndcg = 30.07 
2025-04-08 14:50:08.581565: Steps 25/138: batch_recall = 57.35, batch_ndcg = 33.44 
2025-04-08 14:50:09.881384: Steps 26/138: batch_recall = 57.68, batch_ndcg = 33.91 
2025-04-08 14:50:11.187412: Steps 27/138: batch_recall = 64.14, batch_ndcg = 34.84 
2025-04-08 14:50:12.481338: Steps 28/138: batch_recall = 55.03, batch_ndcg = 31.56 
2025-04-08 14:50:13.771312: Steps 29/138: batch_recall = 57.69, batch_ndcg = 30.32 
2025-04-08 14:50:15.070464: Steps 30/138: batch_recall = 54.92, batch_ndcg = 33.24 
2025-04-08 14:50:16.374044: Steps 31/138: batch_recall = 43.36, batch_ndcg = 24.35 
2025-04-08 14:50:17.670870: Steps 32/138: batch_recall = 51.71, batch_ndcg = 30.95 
2025-04-08 14:50:18.968349: Steps 33/138: batch_recall = 60.38, batch_ndcg = 33.51 
2025-04-08 14:50:20.251870: Steps 34/138: batch_recall = 53.83, batch_ndcg = 28.55 
2025-04-08 14:50:21.533965: Steps 35/138: batch_recall = 50.48, batch_ndcg = 29.41 
2025-04-08 14:50:22.825258: Steps 36/138: batch_recall = 50.76, batch_ndcg = 27.86 
2025-04-08 14:50:24.127121: Steps 37/138: batch_recall = 62.19, batch_ndcg = 34.90 
2025-04-08 14:50:25.413599: Steps 38/138: batch_recall = 62.33, batch_ndcg = 32.95 
2025-04-08 14:50:26.707230: Steps 39/138: batch_recall = 70.55, batch_ndcg = 36.69 
2025-04-08 14:50:27.999133: Steps 40/138: batch_recall = 54.17, batch_ndcg = 28.31 
2025-04-08 14:50:29.285456: Steps 41/138: batch_recall = 59.89, batch_ndcg = 33.26 
2025-04-08 14:50:30.572969: Steps 42/138: batch_recall = 54.16, batch_ndcg = 29.98 
2025-04-08 14:50:31.867913: Steps 43/138: batch_recall = 52.90, batch_ndcg = 31.51 
2025-04-08 14:50:33.154084: Steps 44/138: batch_recall = 56.02, batch_ndcg = 28.21 
2025-04-08 14:50:34.439447: Steps 45/138: batch_recall = 57.24, batch_ndcg = 32.68 
2025-04-08 14:50:35.707668: Steps 46/138: batch_recall = 59.38, batch_ndcg = 33.92 
2025-04-08 14:50:36.984743: Steps 47/138: batch_recall = 56.99, batch_ndcg = 32.03 
2025-04-08 14:50:38.278710: Steps 48/138: batch_recall = 55.58, batch_ndcg = 32.46 
2025-04-08 14:50:39.558916: Steps 49/138: batch_recall = 63.30, batch_ndcg = 36.91 
2025-04-08 14:50:40.864454: Steps 50/138: batch_recall = 59.85, batch_ndcg = 32.95 
2025-04-08 14:50:42.152226: Steps 51/138: batch_recall = 63.64, batch_ndcg = 36.46 
2025-04-08 14:50:43.436892: Steps 52/138: batch_recall = 65.60, batch_ndcg = 41.23 
2025-04-08 14:50:44.722348: Steps 53/138: batch_recall = 63.22, batch_ndcg = 32.33 
2025-04-08 14:50:46.012958: Steps 54/138: batch_recall = 64.83, batch_ndcg = 37.16 
2025-04-08 14:50:47.293852: Steps 55/138: batch_recall = 57.55, batch_ndcg = 32.77 
2025-04-08 14:50:48.587920: Steps 56/138: batch_recall = 62.14, batch_ndcg = 34.68 
2025-04-08 14:50:49.878278: Steps 57/138: batch_recall = 60.34, batch_ndcg = 33.92 
2025-04-08 14:50:51.147018: Steps 58/138: batch_recall = 69.33, batch_ndcg = 36.36 
2025-04-08 14:50:52.429435: Steps 59/138: batch_recall = 79.57, batch_ndcg = 43.45 
2025-04-08 14:50:53.718558: Steps 60/138: batch_recall = 70.18, batch_ndcg = 38.96 
2025-04-08 14:50:54.995839: Steps 61/138: batch_recall = 60.72, batch_ndcg = 33.53 
2025-04-08 14:50:56.292532: Steps 62/138: batch_recall = 84.76, batch_ndcg = 44.32 
2025-04-08 14:50:57.581979: Steps 63/138: batch_recall = 79.32, batch_ndcg = 44.29 
2025-04-08 14:50:58.891432: Steps 64/138: batch_recall = 62.60, batch_ndcg = 33.76 
2025-04-08 14:51:00.182046: Steps 65/138: batch_recall = 81.79, batch_ndcg = 46.39 
2025-04-08 14:51:01.464039: Steps 66/138: batch_recall = 66.06, batch_ndcg = 40.27 
2025-04-08 14:51:02.745730: Steps 67/138: batch_recall = 80.13, batch_ndcg = 47.35 
2025-04-08 14:51:04.030810: Steps 68/138: batch_recall = 66.13, batch_ndcg = 34.44 
2025-04-08 14:51:05.298561: Steps 69/138: batch_recall = 86.68, batch_ndcg = 49.89 
2025-04-08 14:51:06.567891: Steps 70/138: batch_recall = 80.12, batch_ndcg = 45.63 
2025-04-08 14:51:07.846408: Steps 71/138: batch_recall = 87.78, batch_ndcg = 50.92 
2025-04-08 14:51:09.121126: Steps 72/138: batch_recall = 87.63, batch_ndcg = 50.40 
2025-04-08 14:51:10.398586: Steps 73/138: batch_recall = 92.12, batch_ndcg = 49.49 
2025-04-08 14:51:11.684828: Steps 74/138: batch_recall = 83.58, batch_ndcg = 49.33 
2025-04-08 14:51:12.969495: Steps 75/138: batch_recall = 87.60, batch_ndcg = 48.95 
2025-04-08 14:51:14.262306: Steps 76/138: batch_recall = 99.36, batch_ndcg = 56.45 
2025-04-08 14:51:15.560107: Steps 77/138: batch_recall = 87.96, batch_ndcg = 48.93 
2025-04-08 14:51:16.842292: Steps 78/138: batch_recall = 93.63, batch_ndcg = 49.07 
2025-04-08 14:51:18.141083: Steps 79/138: batch_recall = 90.36, batch_ndcg = 48.67 
2025-04-08 14:51:19.430005: Steps 80/138: batch_recall = 75.29, batch_ndcg = 38.96 
2025-04-08 14:51:20.709415: Steps 81/138: batch_recall = 80.74, batch_ndcg = 47.59 
2025-04-08 14:51:21.990584: Steps 82/138: batch_recall = 88.55, batch_ndcg = 51.07 
2025-04-08 14:51:23.266098: Steps 83/138: batch_recall = 84.20, batch_ndcg = 47.76 
2025-04-08 14:51:24.552860: Steps 84/138: batch_recall = 101.82, batch_ndcg = 55.45 
2025-04-08 14:51:25.830122: Steps 85/138: batch_recall = 100.77, batch_ndcg = 57.96 
2025-04-08 14:51:27.103805: Steps 86/138: batch_recall = 112.86, batch_ndcg = 69.68 
2025-04-08 14:51:28.385307: Steps 87/138: batch_recall = 107.69, batch_ndcg = 57.98 
2025-04-08 14:51:29.666000: Steps 88/138: batch_recall = 103.43, batch_ndcg = 56.86 
2025-04-08 14:51:30.942644: Steps 89/138: batch_recall = 117.86, batch_ndcg = 66.70 
2025-04-08 14:51:32.226195: Steps 90/138: batch_recall = 105.58, batch_ndcg = 58.20 
2025-04-08 14:51:33.499852: Steps 91/138: batch_recall = 121.96, batch_ndcg = 66.85 
2025-04-08 14:51:34.770972: Steps 92/138: batch_recall = 110.74, batch_ndcg = 61.57 
2025-04-08 14:51:36.047088: Steps 93/138: batch_recall = 118.84, batch_ndcg = 67.91 
2025-04-08 14:51:37.336072: Steps 94/138: batch_recall = 120.03, batch_ndcg = 62.97 
2025-04-08 14:51:38.615698: Steps 95/138: batch_recall = 112.10, batch_ndcg = 68.31 
2025-04-08 14:51:39.903568: Steps 96/138: batch_recall = 133.36, batch_ndcg = 73.57 
2025-04-08 14:51:41.179973: Steps 97/138: batch_recall = 149.05, batch_ndcg = 82.30 
2025-04-08 14:51:42.461940: Steps 98/138: batch_recall = 108.92, batch_ndcg = 62.71 
2025-04-08 14:51:43.732226: Steps 99/138: batch_recall = 123.30, batch_ndcg = 68.40 
2025-04-08 14:51:45.015535: Steps 100/138: batch_recall = 125.85, batch_ndcg = 70.18 
2025-04-08 14:51:46.293235: Steps 101/138: batch_recall = 122.94, batch_ndcg = 68.72 
2025-04-08 14:51:47.572577: Steps 102/138: batch_recall = 125.31, batch_ndcg = 73.54 
2025-04-08 14:51:48.839229: Steps 103/138: batch_recall = 142.63, batch_ndcg = 80.69 
2025-04-08 14:51:50.111087: Steps 104/138: batch_recall = 140.11, batch_ndcg = 77.12 
2025-04-08 14:51:51.388932: Steps 105/138: batch_recall = 115.25, batch_ndcg = 66.21 
2025-04-08 14:51:52.653289: Steps 106/138: batch_recall = 104.03, batch_ndcg = 58.58 
2025-04-08 14:51:53.940774: Steps 107/138: batch_recall = 117.05, batch_ndcg = 63.67 
2025-04-08 14:51:55.214944: Steps 108/138: batch_recall = 120.90, batch_ndcg = 71.85 
2025-04-08 14:51:56.491371: Steps 109/138: batch_recall = 137.25, batch_ndcg = 75.60 
2025-04-08 14:51:57.774338: Steps 110/138: batch_recall = 123.17, batch_ndcg = 64.82 
2025-04-08 14:51:59.053693: Steps 111/138: batch_recall = 138.51, batch_ndcg = 83.50 
2025-04-08 14:52:00.340969: Steps 112/138: batch_recall = 154.85, batch_ndcg = 86.36 
2025-04-08 14:52:01.614712: Steps 113/138: batch_recall = 124.86, batch_ndcg = 71.79 
2025-04-08 14:52:02.889955: Steps 114/138: batch_recall = 120.42, batch_ndcg = 68.40 
2025-04-08 14:52:04.157543: Steps 115/138: batch_recall = 120.00, batch_ndcg = 62.35 
2025-04-08 14:52:05.432202: Steps 116/138: batch_recall = 128.36, batch_ndcg = 66.61 
2025-04-08 14:52:06.705898: Steps 117/138: batch_recall = 118.06, batch_ndcg = 67.67 
2025-04-08 14:52:07.984005: Steps 118/138: batch_recall = 121.55, batch_ndcg = 70.82 
2025-04-08 14:52:09.263249: Steps 119/138: batch_recall = 135.07, batch_ndcg = 71.86 
2025-04-08 14:52:10.542947: Steps 120/138: batch_recall = 123.05, batch_ndcg = 66.50 
2025-04-08 14:52:11.812922: Steps 121/138: batch_recall = 153.03, batch_ndcg = 79.70 
2025-04-08 14:52:13.088823: Steps 122/138: batch_recall = 151.08, batch_ndcg = 80.24 
2025-04-08 14:52:14.369208: Steps 123/138: batch_recall = 130.44, batch_ndcg = 71.60 
2025-04-08 14:52:15.648331: Steps 124/138: batch_recall = 153.19, batch_ndcg = 93.85 
2025-04-08 14:52:16.918063: Steps 125/138: batch_recall = 130.45, batch_ndcg = 70.54 
2025-04-08 14:52:18.184366: Steps 126/138: batch_recall = 155.97, batch_ndcg = 87.08 
2025-04-08 14:52:19.459882: Steps 127/138: batch_recall = 140.51, batch_ndcg = 81.22 
2025-04-08 14:52:20.723949: Steps 128/138: batch_recall = 124.51, batch_ndcg = 69.44 
2025-04-08 14:52:21.981111: Steps 129/138: batch_recall = 158.32, batch_ndcg = 89.30 
2025-04-08 14:52:23.256955: Steps 130/138: batch_recall = 136.69, batch_ndcg = 70.23 
2025-04-08 14:52:24.533814: Steps 131/138: batch_recall = 144.62, batch_ndcg = 82.84 
2025-04-08 14:52:25.799441: Steps 132/138: batch_recall = 153.50, batch_ndcg = 85.38 
2025-04-08 14:52:27.068258: Steps 133/138: batch_recall = 148.60, batch_ndcg = 84.25 
2025-04-08 14:52:28.342248: Steps 134/138: batch_recall = 134.63, batch_ndcg = 76.51 
2025-04-08 14:52:29.610277: Steps 135/138: batch_recall = 160.86, batch_ndcg = 91.38 
2025-04-08 14:52:30.887210: Steps 136/138: batch_recall = 145.45, batch_ndcg = 77.50 
2025-04-08 14:52:32.150296: Steps 137/138: batch_recall = 137.52, batch_ndcg = 85.31 
2025-04-08 14:52:32.150833: Epoch 10/1000, Test: Recall = 0.1754, NDCG = 0.0987  

2025-04-08 14:52:33.919958: Training Step 0/354: batchLoss = 0.4420, diffLoss = 2.1660, kgLoss = 0.0111
2025-04-08 14:52:35.532513: Training Step 1/354: batchLoss = 0.6493, diffLoss = 3.1839, kgLoss = 0.0156
2025-04-08 14:52:37.148354: Training Step 2/354: batchLoss = 0.4413, diffLoss = 2.1549, kgLoss = 0.0129
2025-04-08 14:52:38.774084: Training Step 3/354: batchLoss = 0.5706, diffLoss = 2.7962, kgLoss = 0.0142
2025-04-08 14:52:40.405245: Training Step 4/354: batchLoss = 0.5558, diffLoss = 2.7211, kgLoss = 0.0145
2025-04-08 14:52:42.027062: Training Step 5/354: batchLoss = 0.4121, diffLoss = 2.0119, kgLoss = 0.0122
2025-04-08 14:52:43.646476: Training Step 6/354: batchLoss = 0.6244, diffLoss = 3.0620, kgLoss = 0.0150
2025-04-08 14:52:45.275440: Training Step 7/354: batchLoss = 0.4863, diffLoss = 2.3782, kgLoss = 0.0133
2025-04-08 14:52:46.902083: Training Step 8/354: batchLoss = 0.7371, diffLoss = 3.6152, kgLoss = 0.0176
2025-04-08 14:52:48.537379: Training Step 9/354: batchLoss = 0.5662, diffLoss = 2.7660, kgLoss = 0.0162
2025-04-08 14:52:50.169143: Training Step 10/354: batchLoss = 0.4986, diffLoss = 2.4456, kgLoss = 0.0119
2025-04-08 14:52:51.792922: Training Step 11/354: batchLoss = 0.5375, diffLoss = 2.6322, kgLoss = 0.0139
2025-04-08 14:52:53.415154: Training Step 12/354: batchLoss = 0.6753, diffLoss = 3.3138, kgLoss = 0.0157
2025-04-08 14:52:55.044086: Training Step 13/354: batchLoss = 0.5577, diffLoss = 2.7384, kgLoss = 0.0126
2025-04-08 14:52:56.674827: Training Step 14/354: batchLoss = 0.6699, diffLoss = 3.2813, kgLoss = 0.0171
2025-04-08 14:52:58.308619: Training Step 15/354: batchLoss = 0.6332, diffLoss = 3.1020, kgLoss = 0.0159
2025-04-08 14:52:59.941714: Training Step 16/354: batchLoss = 0.5934, diffLoss = 2.9058, kgLoss = 0.0152
2025-04-08 14:53:01.566615: Training Step 17/354: batchLoss = 0.6970, diffLoss = 3.4194, kgLoss = 0.0165
2025-04-08 14:53:03.185250: Training Step 18/354: batchLoss = 0.6069, diffLoss = 2.9783, kgLoss = 0.0140
2025-04-08 14:53:04.801720: Training Step 19/354: batchLoss = 0.6753, diffLoss = 3.3114, kgLoss = 0.0163
2025-04-08 14:53:06.419065: Training Step 20/354: batchLoss = 0.5900, diffLoss = 2.8925, kgLoss = 0.0144
2025-04-08 14:53:08.039889: Training Step 21/354: batchLoss = 0.5294, diffLoss = 2.5953, kgLoss = 0.0129
2025-04-08 14:53:09.664126: Training Step 22/354: batchLoss = 0.6761, diffLoss = 3.3108, kgLoss = 0.0175
2025-04-08 14:53:11.286449: Training Step 23/354: batchLoss = 0.5953, diffLoss = 2.9230, kgLoss = 0.0134
2025-04-08 14:53:12.910569: Training Step 24/354: batchLoss = 0.5417, diffLoss = 2.6563, kgLoss = 0.0131
2025-04-08 14:53:14.534925: Training Step 25/354: batchLoss = 0.6471, diffLoss = 3.1708, kgLoss = 0.0161
2025-04-08 14:53:16.153983: Training Step 26/354: batchLoss = 0.5917, diffLoss = 2.8955, kgLoss = 0.0158
2025-04-08 14:53:17.768039: Training Step 27/354: batchLoss = 0.5652, diffLoss = 2.7597, kgLoss = 0.0166
2025-04-08 14:53:19.371966: Training Step 28/354: batchLoss = 0.5788, diffLoss = 2.8371, kgLoss = 0.0142
2025-04-08 14:53:20.984062: Training Step 29/354: batchLoss = 0.6872, diffLoss = 3.3686, kgLoss = 0.0168
2025-04-08 14:53:22.607408: Training Step 30/354: batchLoss = 0.7153, diffLoss = 3.5094, kgLoss = 0.0168
2025-04-08 14:53:24.229796: Training Step 31/354: batchLoss = 0.5876, diffLoss = 2.8821, kgLoss = 0.0140
2025-04-08 14:53:25.853458: Training Step 32/354: batchLoss = 0.5781, diffLoss = 2.8239, kgLoss = 0.0167
2025-04-08 14:53:27.480384: Training Step 33/354: batchLoss = 0.5342, diffLoss = 2.6211, kgLoss = 0.0125
2025-04-08 14:53:29.099166: Training Step 34/354: batchLoss = 0.5169, diffLoss = 2.5357, kgLoss = 0.0123
2025-04-08 14:53:30.720741: Training Step 35/354: batchLoss = 0.6645, diffLoss = 3.2546, kgLoss = 0.0170
2025-04-08 14:53:32.335360: Training Step 36/354: batchLoss = 0.4813, diffLoss = 2.3598, kgLoss = 0.0116
2025-04-08 14:53:33.948140: Training Step 37/354: batchLoss = 0.6036, diffLoss = 2.9651, kgLoss = 0.0132
2025-04-08 14:53:35.565796: Training Step 38/354: batchLoss = 0.6124, diffLoss = 3.0087, kgLoss = 0.0134
2025-04-08 14:53:37.184895: Training Step 39/354: batchLoss = 0.6690, diffLoss = 3.2799, kgLoss = 0.0163
2025-04-08 14:53:38.802326: Training Step 40/354: batchLoss = 0.5591, diffLoss = 2.7433, kgLoss = 0.0130
2025-04-08 14:53:40.424898: Training Step 41/354: batchLoss = 0.7228, diffLoss = 3.5460, kgLoss = 0.0170
2025-04-08 14:53:42.052408: Training Step 42/354: batchLoss = 0.6211, diffLoss = 3.0500, kgLoss = 0.0138
2025-04-08 14:53:43.680107: Training Step 43/354: batchLoss = 1.2039, diffLoss = 5.9161, kgLoss = 0.0259
2025-04-08 14:53:45.297582: Training Step 44/354: batchLoss = 0.5010, diffLoss = 2.4502, kgLoss = 0.0137
2025-04-08 14:53:46.923697: Training Step 45/354: batchLoss = 0.5486, diffLoss = 2.6877, kgLoss = 0.0139
2025-04-08 14:53:48.536016: Training Step 46/354: batchLoss = 0.5561, diffLoss = 2.7248, kgLoss = 0.0140
2025-04-08 14:53:50.153145: Training Step 47/354: batchLoss = 0.6431, diffLoss = 3.1533, kgLoss = 0.0156
2025-04-08 14:53:51.769919: Training Step 48/354: batchLoss = 0.5321, diffLoss = 2.6094, kgLoss = 0.0127
2025-04-08 14:53:53.400437: Training Step 49/354: batchLoss = 0.5913, diffLoss = 2.9000, kgLoss = 0.0141
2025-04-08 14:53:55.023908: Training Step 50/354: batchLoss = 0.5137, diffLoss = 2.5172, kgLoss = 0.0128
2025-04-08 14:53:56.642816: Training Step 51/354: batchLoss = 0.6912, diffLoss = 3.3955, kgLoss = 0.0151
2025-04-08 14:53:58.271699: Training Step 52/354: batchLoss = 0.6109, diffLoss = 2.9980, kgLoss = 0.0141
2025-04-08 14:53:59.892933: Training Step 53/354: batchLoss = 0.4981, diffLoss = 2.4378, kgLoss = 0.0132
2025-04-08 14:54:01.518703: Training Step 54/354: batchLoss = 0.4759, diffLoss = 2.3335, kgLoss = 0.0115
2025-04-08 14:54:03.132700: Training Step 55/354: batchLoss = 0.4804, diffLoss = 2.3520, kgLoss = 0.0124
2025-04-08 14:54:04.752749: Training Step 56/354: batchLoss = 0.5814, diffLoss = 2.8548, kgLoss = 0.0130
2025-04-08 14:54:06.383639: Training Step 57/354: batchLoss = 0.5020, diffLoss = 2.4593, kgLoss = 0.0127
2025-04-08 14:54:08.011270: Training Step 58/354: batchLoss = 0.6075, diffLoss = 2.9820, kgLoss = 0.0139
2025-04-08 14:54:09.632662: Training Step 59/354: batchLoss = 0.6509, diffLoss = 3.1906, kgLoss = 0.0159
2025-04-08 14:54:11.255524: Training Step 60/354: batchLoss = 0.5976, diffLoss = 2.9329, kgLoss = 0.0138
2025-04-08 14:54:12.874910: Training Step 61/354: batchLoss = 0.6999, diffLoss = 3.4342, kgLoss = 0.0164
2025-04-08 14:54:14.504517: Training Step 62/354: batchLoss = 0.5249, diffLoss = 2.5723, kgLoss = 0.0131
2025-04-08 14:54:16.125702: Training Step 63/354: batchLoss = 0.5260, diffLoss = 2.5795, kgLoss = 0.0126
2025-04-08 14:54:17.746507: Training Step 64/354: batchLoss = 0.7027, diffLoss = 3.4472, kgLoss = 0.0165
2025-04-08 14:54:19.366601: Training Step 65/354: batchLoss = 0.5934, diffLoss = 2.9034, kgLoss = 0.0160
2025-04-08 14:54:20.986958: Training Step 66/354: batchLoss = 0.6104, diffLoss = 2.9941, kgLoss = 0.0145
2025-04-08 14:54:22.607603: Training Step 67/354: batchLoss = 0.6020, diffLoss = 2.9527, kgLoss = 0.0144
2025-04-08 14:54:24.227493: Training Step 68/354: batchLoss = 0.5644, diffLoss = 2.7662, kgLoss = 0.0140
2025-04-08 14:54:25.838777: Training Step 69/354: batchLoss = 0.5119, diffLoss = 2.5141, kgLoss = 0.0114
2025-04-08 14:54:27.450273: Training Step 70/354: batchLoss = 0.5766, diffLoss = 2.8274, kgLoss = 0.0139
2025-04-08 14:54:29.080651: Training Step 71/354: batchLoss = 0.4243, diffLoss = 2.0760, kgLoss = 0.0114
2025-04-08 14:54:30.703373: Training Step 72/354: batchLoss = 0.4880, diffLoss = 2.3852, kgLoss = 0.0137
2025-04-08 14:54:32.317786: Training Step 73/354: batchLoss = 0.6325, diffLoss = 3.1033, kgLoss = 0.0148
2025-04-08 14:54:33.938129: Training Step 74/354: batchLoss = 0.6371, diffLoss = 3.1249, kgLoss = 0.0151
2025-04-08 14:54:35.556914: Training Step 75/354: batchLoss = 0.5575, diffLoss = 2.7168, kgLoss = 0.0177
2025-04-08 14:54:37.172714: Training Step 76/354: batchLoss = 0.5946, diffLoss = 2.9144, kgLoss = 0.0147
2025-04-08 14:54:38.794976: Training Step 77/354: batchLoss = 0.7152, diffLoss = 3.5023, kgLoss = 0.0185
2025-04-08 14:54:40.414776: Training Step 78/354: batchLoss = 0.5510, diffLoss = 2.7027, kgLoss = 0.0131
2025-04-08 14:54:42.033755: Training Step 79/354: batchLoss = 0.5383, diffLoss = 2.6400, kgLoss = 0.0129
2025-04-08 14:54:43.656890: Training Step 80/354: batchLoss = 0.4582, diffLoss = 2.2457, kgLoss = 0.0113
2025-04-08 14:54:45.271927: Training Step 81/354: batchLoss = 0.6493, diffLoss = 3.1635, kgLoss = 0.0208
2025-04-08 14:54:46.893280: Training Step 82/354: batchLoss = 0.7470, diffLoss = 3.6685, kgLoss = 0.0167
2025-04-08 14:54:48.512032: Training Step 83/354: batchLoss = 0.8124, diffLoss = 3.9920, kgLoss = 0.0175
2025-04-08 14:54:50.133119: Training Step 84/354: batchLoss = 0.6253, diffLoss = 3.0661, kgLoss = 0.0151
2025-04-08 14:54:51.759054: Training Step 85/354: batchLoss = 0.6609, diffLoss = 3.2461, kgLoss = 0.0146
2025-04-08 14:54:53.378639: Training Step 86/354: batchLoss = 0.5850, diffLoss = 2.8510, kgLoss = 0.0185
2025-04-08 14:54:54.997201: Training Step 87/354: batchLoss = 0.6876, diffLoss = 3.3720, kgLoss = 0.0165
2025-04-08 14:54:56.614735: Training Step 88/354: batchLoss = 0.5564, diffLoss = 2.7269, kgLoss = 0.0138
2025-04-08 14:54:58.239206: Training Step 89/354: batchLoss = 0.6464, diffLoss = 3.1664, kgLoss = 0.0164
2025-04-08 14:54:59.855884: Training Step 90/354: batchLoss = 0.5566, diffLoss = 2.7321, kgLoss = 0.0127
2025-04-08 14:55:01.473490: Training Step 91/354: batchLoss = 0.6154, diffLoss = 3.0188, kgLoss = 0.0146
2025-04-08 14:55:03.092557: Training Step 92/354: batchLoss = 0.6491, diffLoss = 3.1862, kgLoss = 0.0149
2025-04-08 14:55:04.712087: Training Step 93/354: batchLoss = 0.6817, diffLoss = 3.3440, kgLoss = 0.0162
2025-04-08 14:55:06.330732: Training Step 94/354: batchLoss = 0.5429, diffLoss = 2.6592, kgLoss = 0.0138
2025-04-08 14:55:07.950115: Training Step 95/354: batchLoss = 0.7757, diffLoss = 3.8102, kgLoss = 0.0170
2025-04-08 14:55:09.574586: Training Step 96/354: batchLoss = 0.6870, diffLoss = 3.3635, kgLoss = 0.0179
2025-04-08 14:55:11.189098: Training Step 97/354: batchLoss = 0.5582, diffLoss = 2.7317, kgLoss = 0.0148
2025-04-08 14:55:12.816118: Training Step 98/354: batchLoss = 0.6184, diffLoss = 3.0258, kgLoss = 0.0166
2025-04-08 14:55:14.440813: Training Step 99/354: batchLoss = 0.5909, diffLoss = 2.8992, kgLoss = 0.0138
2025-04-08 14:55:16.054934: Training Step 100/354: batchLoss = 0.5342, diffLoss = 2.6191, kgLoss = 0.0130
2025-04-08 14:55:17.667887: Training Step 101/354: batchLoss = 0.6617, diffLoss = 3.2490, kgLoss = 0.0149
2025-04-08 14:55:19.291062: Training Step 102/354: batchLoss = 0.5337, diffLoss = 2.6206, kgLoss = 0.0120
2025-04-08 14:55:20.911027: Training Step 103/354: batchLoss = 0.5679, diffLoss = 2.7842, kgLoss = 0.0139
2025-04-08 14:55:22.534348: Training Step 104/354: batchLoss = 0.5816, diffLoss = 2.8594, kgLoss = 0.0121
2025-04-08 14:55:24.152514: Training Step 105/354: batchLoss = 0.6897, diffLoss = 3.3853, kgLoss = 0.0158
2025-04-08 14:55:25.767387: Training Step 106/354: batchLoss = 0.5939, diffLoss = 2.9076, kgLoss = 0.0155
2025-04-08 14:55:27.388455: Training Step 107/354: batchLoss = 0.5547, diffLoss = 2.7212, kgLoss = 0.0130
2025-04-08 14:55:29.008000: Training Step 108/354: batchLoss = 0.5772, diffLoss = 2.8292, kgLoss = 0.0142
2025-04-08 14:55:30.627187: Training Step 109/354: batchLoss = 0.5943, diffLoss = 2.9163, kgLoss = 0.0138
2025-04-08 14:55:32.242088: Training Step 110/354: batchLoss = 0.6765, diffLoss = 3.3250, kgLoss = 0.0143
2025-04-08 14:55:33.863163: Training Step 111/354: batchLoss = 0.5560, diffLoss = 2.7260, kgLoss = 0.0136
2025-04-08 14:55:35.478723: Training Step 112/354: batchLoss = 0.5596, diffLoss = 2.7490, kgLoss = 0.0123
2025-04-08 14:55:37.088745: Training Step 113/354: batchLoss = 0.5608, diffLoss = 2.7510, kgLoss = 0.0133
2025-04-08 14:55:38.699229: Training Step 114/354: batchLoss = 0.7017, diffLoss = 3.4488, kgLoss = 0.0149
2025-04-08 14:55:40.313679: Training Step 115/354: batchLoss = 0.5881, diffLoss = 2.8809, kgLoss = 0.0149
2025-04-08 14:55:41.927352: Training Step 116/354: batchLoss = 0.5482, diffLoss = 2.6827, kgLoss = 0.0146
2025-04-08 14:55:43.537790: Training Step 117/354: batchLoss = 0.4674, diffLoss = 2.2879, kgLoss = 0.0122
2025-04-08 14:55:45.150081: Training Step 118/354: batchLoss = 0.5576, diffLoss = 2.7306, kgLoss = 0.0144
2025-04-08 14:55:46.763741: Training Step 119/354: batchLoss = 0.5908, diffLoss = 2.8832, kgLoss = 0.0177
2025-04-08 14:55:48.374679: Training Step 120/354: batchLoss = 0.6844, diffLoss = 3.3553, kgLoss = 0.0166
2025-04-08 14:55:49.983053: Training Step 121/354: batchLoss = 0.5076, diffLoss = 2.4807, kgLoss = 0.0144
2025-04-08 14:55:51.589793: Training Step 122/354: batchLoss = 0.4936, diffLoss = 2.4076, kgLoss = 0.0151
2025-04-08 14:55:53.203652: Training Step 123/354: batchLoss = 0.4866, diffLoss = 2.3790, kgLoss = 0.0135
2025-04-08 14:55:54.820738: Training Step 124/354: batchLoss = 0.6076, diffLoss = 2.9719, kgLoss = 0.0166
2025-04-08 14:55:56.436947: Training Step 125/354: batchLoss = 0.5649, diffLoss = 2.7749, kgLoss = 0.0124
2025-04-08 14:55:58.048485: Training Step 126/354: batchLoss = 0.4750, diffLoss = 2.3245, kgLoss = 0.0126
2025-04-08 14:55:59.679521: Training Step 127/354: batchLoss = 0.6414, diffLoss = 3.1498, kgLoss = 0.0143
2025-04-08 14:56:01.309488: Training Step 128/354: batchLoss = 0.5336, diffLoss = 2.6085, kgLoss = 0.0149
2025-04-08 14:56:02.942709: Training Step 129/354: batchLoss = 0.7609, diffLoss = 3.7302, kgLoss = 0.0186
2025-04-08 14:56:04.563422: Training Step 130/354: batchLoss = 0.5509, diffLoss = 2.6995, kgLoss = 0.0137
2025-04-08 14:56:06.181135: Training Step 131/354: batchLoss = 0.6267, diffLoss = 3.0727, kgLoss = 0.0153
2025-04-08 14:56:07.793484: Training Step 132/354: batchLoss = 0.5652, diffLoss = 2.7717, kgLoss = 0.0136
2025-04-08 14:56:09.417584: Training Step 133/354: batchLoss = 0.7359, diffLoss = 3.6145, kgLoss = 0.0163
2025-04-08 14:56:11.040958: Training Step 134/354: batchLoss = 0.7164, diffLoss = 3.5151, kgLoss = 0.0168
2025-04-08 14:56:12.661927: Training Step 135/354: batchLoss = 0.6468, diffLoss = 3.1782, kgLoss = 0.0140
2025-04-08 14:56:14.293633: Training Step 136/354: batchLoss = 0.4766, diffLoss = 2.3344, kgLoss = 0.0122
2025-04-08 14:56:15.913132: Training Step 137/354: batchLoss = 0.4769, diffLoss = 2.3350, kgLoss = 0.0123
2025-04-08 14:56:17.529901: Training Step 138/354: batchLoss = 0.6080, diffLoss = 2.9756, kgLoss = 0.0161
2025-04-08 14:56:19.146069: Training Step 139/354: batchLoss = 0.5411, diffLoss = 2.6473, kgLoss = 0.0146
2025-04-08 14:56:20.762668: Training Step 140/354: batchLoss = 0.4845, diffLoss = 2.3768, kgLoss = 0.0115
2025-04-08 14:56:22.384293: Training Step 141/354: batchLoss = 1.0985, diffLoss = 5.3921, kgLoss = 0.0251
2025-04-08 14:56:23.999821: Training Step 142/354: batchLoss = 0.5734, diffLoss = 2.8075, kgLoss = 0.0149
2025-04-08 14:56:25.610436: Training Step 143/354: batchLoss = 0.6136, diffLoss = 3.0040, kgLoss = 0.0160
2025-04-08 14:56:27.236842: Training Step 144/354: batchLoss = 0.6057, diffLoss = 2.9681, kgLoss = 0.0151
2025-04-08 14:56:28.851899: Training Step 145/354: batchLoss = 0.5761, diffLoss = 2.8179, kgLoss = 0.0156
2025-04-08 14:56:30.468748: Training Step 146/354: batchLoss = 0.6738, diffLoss = 3.3053, kgLoss = 0.0159
2025-04-08 14:56:32.086338: Training Step 147/354: batchLoss = 0.6998, diffLoss = 3.4381, kgLoss = 0.0152
2025-04-08 14:56:33.714607: Training Step 148/354: batchLoss = 0.4713, diffLoss = 2.3103, kgLoss = 0.0115
2025-04-08 14:56:35.340919: Training Step 149/354: batchLoss = 0.5879, diffLoss = 2.8891, kgLoss = 0.0126
2025-04-08 14:56:36.967549: Training Step 150/354: batchLoss = 0.5606, diffLoss = 2.7481, kgLoss = 0.0137
2025-04-08 14:56:38.585045: Training Step 151/354: batchLoss = 0.6002, diffLoss = 2.9382, kgLoss = 0.0157
2025-04-08 14:56:40.209535: Training Step 152/354: batchLoss = 0.6638, diffLoss = 3.2542, kgLoss = 0.0162
2025-04-08 14:56:41.837580: Training Step 153/354: batchLoss = 0.7257, diffLoss = 3.5572, kgLoss = 0.0178
2025-04-08 14:56:43.455378: Training Step 154/354: batchLoss = 0.7471, diffLoss = 3.6659, kgLoss = 0.0174
2025-04-08 14:56:45.068823: Training Step 155/354: batchLoss = 0.5305, diffLoss = 2.5973, kgLoss = 0.0139
2025-04-08 14:56:46.687103: Training Step 156/354: batchLoss = 0.4912, diffLoss = 2.4059, kgLoss = 0.0125
2025-04-08 14:56:48.318362: Training Step 157/354: batchLoss = 0.6718, diffLoss = 3.2961, kgLoss = 0.0157
2025-04-08 14:56:49.960554: Training Step 158/354: batchLoss = 0.5021, diffLoss = 2.4583, kgLoss = 0.0130
2025-04-08 14:56:51.590156: Training Step 159/354: batchLoss = 0.6200, diffLoss = 3.0296, kgLoss = 0.0175
2025-04-08 14:56:53.210894: Training Step 160/354: batchLoss = 0.7667, diffLoss = 3.7564, kgLoss = 0.0193
2025-04-08 14:56:54.837029: Training Step 161/354: batchLoss = 0.6791, diffLoss = 3.3297, kgLoss = 0.0164
2025-04-08 14:56:56.464798: Training Step 162/354: batchLoss = 0.5195, diffLoss = 2.5486, kgLoss = 0.0122
2025-04-08 14:56:58.090608: Training Step 163/354: batchLoss = 0.5715, diffLoss = 2.8026, kgLoss = 0.0137
2025-04-08 14:56:59.711994: Training Step 164/354: batchLoss = 0.4725, diffLoss = 2.3068, kgLoss = 0.0139
2025-04-08 14:57:01.329277: Training Step 165/354: batchLoss = 0.6692, diffLoss = 3.2827, kgLoss = 0.0159
2025-04-08 14:57:02.956829: Training Step 166/354: batchLoss = 0.6292, diffLoss = 3.0823, kgLoss = 0.0159
2025-04-08 14:57:04.588841: Training Step 167/354: batchLoss = 0.6633, diffLoss = 3.2470, kgLoss = 0.0174
2025-04-08 14:57:06.219394: Training Step 168/354: batchLoss = 0.6870, diffLoss = 3.3720, kgLoss = 0.0158
2025-04-08 14:57:07.845077: Training Step 169/354: batchLoss = 0.6534, diffLoss = 3.2070, kgLoss = 0.0149
2025-04-08 14:57:09.468522: Training Step 170/354: batchLoss = 0.6798, diffLoss = 3.2120, kgLoss = 0.0467
2025-04-08 14:57:11.093525: Training Step 171/354: batchLoss = 0.6518, diffLoss = 3.1933, kgLoss = 0.0164
2025-04-08 14:57:12.713033: Training Step 172/354: batchLoss = 0.4927, diffLoss = 2.4168, kgLoss = 0.0117
2025-04-08 14:57:14.335773: Training Step 173/354: batchLoss = 0.6019, diffLoss = 2.9479, kgLoss = 0.0154
2025-04-08 14:57:15.958218: Training Step 174/354: batchLoss = 0.5682, diffLoss = 2.7811, kgLoss = 0.0150
2025-04-08 14:57:17.586991: Training Step 175/354: batchLoss = 0.4948, diffLoss = 2.4197, kgLoss = 0.0136
2025-04-08 14:57:19.230040: Training Step 176/354: batchLoss = 0.5966, diffLoss = 2.9217, kgLoss = 0.0153
2025-04-08 14:57:20.851869: Training Step 177/354: batchLoss = 0.5273, diffLoss = 2.5836, kgLoss = 0.0133
2025-04-08 14:57:22.475635: Training Step 178/354: batchLoss = 0.6013, diffLoss = 2.9479, kgLoss = 0.0146
2025-04-08 14:57:24.105626: Training Step 179/354: batchLoss = 0.5288, diffLoss = 2.5924, kgLoss = 0.0129
2025-04-08 14:57:25.735067: Training Step 180/354: batchLoss = 0.6380, diffLoss = 3.1292, kgLoss = 0.0152
2025-04-08 14:57:27.349979: Training Step 181/354: batchLoss = 0.6842, diffLoss = 3.3588, kgLoss = 0.0155
2025-04-08 14:57:28.968794: Training Step 182/354: batchLoss = 0.5744, diffLoss = 2.8100, kgLoss = 0.0155
2025-04-08 14:57:30.593749: Training Step 183/354: batchLoss = 0.5812, diffLoss = 2.8429, kgLoss = 0.0158
2025-04-08 14:57:32.225819: Training Step 184/354: batchLoss = 0.5999, diffLoss = 2.9456, kgLoss = 0.0134
2025-04-08 14:57:33.850064: Training Step 185/354: batchLoss = 0.5917, diffLoss = 2.8889, kgLoss = 0.0174
2025-04-08 14:57:35.474788: Training Step 186/354: batchLoss = 0.5846, diffLoss = 2.8688, kgLoss = 0.0136
2025-04-08 14:57:37.097561: Training Step 187/354: batchLoss = 0.5657, diffLoss = 2.7685, kgLoss = 0.0150
2025-04-08 14:57:38.726958: Training Step 188/354: batchLoss = 0.5185, diffLoss = 2.5377, kgLoss = 0.0137
2025-04-08 14:57:40.356796: Training Step 189/354: batchLoss = 0.5496, diffLoss = 2.6983, kgLoss = 0.0124
2025-04-08 14:57:41.979369: Training Step 190/354: batchLoss = 0.5104, diffLoss = 2.4984, kgLoss = 0.0133
2025-04-08 14:57:43.597166: Training Step 191/354: batchLoss = 0.4827, diffLoss = 2.3678, kgLoss = 0.0114
2025-04-08 14:57:45.215170: Training Step 192/354: batchLoss = 0.6132, diffLoss = 3.0018, kgLoss = 0.0161
2025-04-08 14:57:46.832596: Training Step 193/354: batchLoss = 0.5896, diffLoss = 2.8866, kgLoss = 0.0153
2025-04-08 14:57:48.466770: Training Step 194/354: batchLoss = 0.5315, diffLoss = 2.6063, kgLoss = 0.0128
2025-04-08 14:57:50.083025: Training Step 195/354: batchLoss = 0.6048, diffLoss = 2.9652, kgLoss = 0.0147
2025-04-08 14:57:51.709662: Training Step 196/354: batchLoss = 0.5249, diffLoss = 2.5727, kgLoss = 0.0130
2025-04-08 14:57:53.335066: Training Step 197/354: batchLoss = 0.5641, diffLoss = 2.7606, kgLoss = 0.0149
2025-04-08 14:57:54.970901: Training Step 198/354: batchLoss = 0.6342, diffLoss = 3.1081, kgLoss = 0.0157
2025-04-08 14:57:56.596991: Training Step 199/354: batchLoss = 0.9599, diffLoss = 4.7123, kgLoss = 0.0219
2025-04-08 14:57:58.217366: Training Step 200/354: batchLoss = 0.5375, diffLoss = 2.6341, kgLoss = 0.0134
2025-04-08 14:57:59.840123: Training Step 201/354: batchLoss = 0.6386, diffLoss = 3.1285, kgLoss = 0.0161
2025-04-08 14:58:01.455493: Training Step 202/354: batchLoss = 0.6566, diffLoss = 3.2180, kgLoss = 0.0162
2025-04-08 14:58:03.074513: Training Step 203/354: batchLoss = 0.7788, diffLoss = 3.8147, kgLoss = 0.0198
2025-04-08 14:58:04.704211: Training Step 204/354: batchLoss = 0.6894, diffLoss = 3.3877, kgLoss = 0.0148
2025-04-08 14:58:06.328676: Training Step 205/354: batchLoss = 0.5531, diffLoss = 2.7104, kgLoss = 0.0138
2025-04-08 14:58:07.957310: Training Step 206/354: batchLoss = 0.6322, diffLoss = 3.1019, kgLoss = 0.0148
2025-04-08 14:58:09.588628: Training Step 207/354: batchLoss = 0.5368, diffLoss = 2.6314, kgLoss = 0.0131
2025-04-08 14:58:11.216535: Training Step 208/354: batchLoss = 0.5636, diffLoss = 2.7679, kgLoss = 0.0125
2025-04-08 14:58:12.834313: Training Step 209/354: batchLoss = 0.5462, diffLoss = 2.6801, kgLoss = 0.0127
2025-04-08 14:58:14.444859: Training Step 210/354: batchLoss = 0.5365, diffLoss = 2.6293, kgLoss = 0.0133
2025-04-08 14:58:16.077509: Training Step 211/354: batchLoss = 0.5588, diffLoss = 2.7431, kgLoss = 0.0127
2025-04-08 14:58:17.718235: Training Step 212/354: batchLoss = 0.4118, diffLoss = 2.0148, kgLoss = 0.0111
2025-04-08 14:58:19.345434: Training Step 213/354: batchLoss = 0.5923, diffLoss = 2.9031, kgLoss = 0.0146
2025-04-08 14:58:20.971118: Training Step 214/354: batchLoss = 0.8058, diffLoss = 3.9483, kgLoss = 0.0202
2025-04-08 14:58:22.596513: Training Step 215/354: batchLoss = 0.8809, diffLoss = 4.3310, kgLoss = 0.0184
2025-04-08 14:58:24.222461: Training Step 216/354: batchLoss = 0.7118, diffLoss = 3.4890, kgLoss = 0.0175
2025-04-08 14:58:25.846693: Training Step 217/354: batchLoss = 0.5349, diffLoss = 2.6246, kgLoss = 0.0124
2025-04-08 14:58:27.459616: Training Step 218/354: batchLoss = 0.6276, diffLoss = 3.0778, kgLoss = 0.0151
2025-04-08 14:58:29.077912: Training Step 219/354: batchLoss = 0.5469, diffLoss = 2.6826, kgLoss = 0.0130
2025-04-08 14:58:30.698924: Training Step 220/354: batchLoss = 0.6750, diffLoss = 3.3110, kgLoss = 0.0160
2025-04-08 14:58:32.322100: Training Step 221/354: batchLoss = 0.5700, diffLoss = 2.7859, kgLoss = 0.0160
2025-04-08 14:58:33.946450: Training Step 222/354: batchLoss = 0.4584, diffLoss = 2.2424, kgLoss = 0.0124
2025-04-08 14:58:35.573584: Training Step 223/354: batchLoss = 0.5835, diffLoss = 2.8641, kgLoss = 0.0133
2025-04-08 14:58:37.197668: Training Step 224/354: batchLoss = 0.5355, diffLoss = 2.6285, kgLoss = 0.0122
2025-04-08 14:58:38.820626: Training Step 225/354: batchLoss = 0.5725, diffLoss = 2.8111, kgLoss = 0.0129
2025-04-08 14:58:40.438914: Training Step 226/354: batchLoss = 0.5389, diffLoss = 2.6400, kgLoss = 0.0136
2025-04-08 14:58:42.064057: Training Step 227/354: batchLoss = 0.5485, diffLoss = 2.6964, kgLoss = 0.0115
2025-04-08 14:58:43.678972: Training Step 228/354: batchLoss = 0.4573, diffLoss = 2.2356, kgLoss = 0.0128
2025-04-08 14:58:45.296260: Training Step 229/354: batchLoss = 0.5649, diffLoss = 2.7699, kgLoss = 0.0136
2025-04-08 14:58:46.918360: Training Step 230/354: batchLoss = 0.5166, diffLoss = 2.5322, kgLoss = 0.0127
2025-04-08 14:58:48.542753: Training Step 231/354: batchLoss = 0.5877, diffLoss = 2.8850, kgLoss = 0.0133
2025-04-08 14:58:50.167278: Training Step 232/354: batchLoss = 0.6221, diffLoss = 3.0483, kgLoss = 0.0155
2025-04-08 14:58:51.791191: Training Step 233/354: batchLoss = 0.5722, diffLoss = 2.8066, kgLoss = 0.0136
2025-04-08 14:58:53.399889: Training Step 234/354: batchLoss = 0.4970, diffLoss = 2.4297, kgLoss = 0.0138
2025-04-08 14:58:55.030050: Training Step 235/354: batchLoss = 0.6779, diffLoss = 3.3238, kgLoss = 0.0164
2025-04-08 14:58:56.645063: Training Step 236/354: batchLoss = 0.5142, diffLoss = 2.5228, kgLoss = 0.0120
2025-04-08 14:58:58.262984: Training Step 237/354: batchLoss = 0.6296, diffLoss = 3.0891, kgLoss = 0.0147
2025-04-08 14:58:59.879282: Training Step 238/354: batchLoss = 0.5074, diffLoss = 2.4822, kgLoss = 0.0136
2025-04-08 14:59:01.502214: Training Step 239/354: batchLoss = 0.6238, diffLoss = 3.0577, kgLoss = 0.0154
2025-04-08 14:59:03.125138: Training Step 240/354: batchLoss = 0.5379, diffLoss = 2.6345, kgLoss = 0.0137
2025-04-08 14:59:04.749335: Training Step 241/354: batchLoss = 0.5756, diffLoss = 2.8258, kgLoss = 0.0131
2025-04-08 14:59:06.361060: Training Step 242/354: batchLoss = 0.4785, diffLoss = 2.3337, kgLoss = 0.0147
2025-04-08 14:59:07.993469: Training Step 243/354: batchLoss = 0.8118, diffLoss = 3.9812, kgLoss = 0.0195
2025-04-08 14:59:09.616500: Training Step 244/354: batchLoss = 0.7710, diffLoss = 3.7823, kgLoss = 0.0181
2025-04-08 14:59:11.235095: Training Step 245/354: batchLoss = 0.5922, diffLoss = 2.8949, kgLoss = 0.0165
2025-04-08 14:59:12.849553: Training Step 246/354: batchLoss = 0.6438, diffLoss = 3.1565, kgLoss = 0.0156
2025-04-08 14:59:14.459837: Training Step 247/354: batchLoss = 0.6160, diffLoss = 3.0218, kgLoss = 0.0146
2025-04-08 14:59:16.082632: Training Step 248/354: batchLoss = 0.5376, diffLoss = 2.6355, kgLoss = 0.0131
2025-04-08 14:59:17.705556: Training Step 249/354: batchLoss = 0.4588, diffLoss = 2.2467, kgLoss = 0.0118
2025-04-08 14:59:19.336888: Training Step 250/354: batchLoss = 0.5443, diffLoss = 2.6620, kgLoss = 0.0149
2025-04-08 14:59:20.978784: Training Step 251/354: batchLoss = 0.5704, diffLoss = 2.7960, kgLoss = 0.0140
2025-04-08 14:59:22.625639: Training Step 252/354: batchLoss = 0.5727, diffLoss = 2.7937, kgLoss = 0.0174
2025-04-08 14:59:24.269610: Training Step 253/354: batchLoss = 0.6074, diffLoss = 2.9692, kgLoss = 0.0169
2025-04-08 14:59:25.903188: Training Step 254/354: batchLoss = 0.5341, diffLoss = 2.6211, kgLoss = 0.0124
2025-04-08 14:59:27.525369: Training Step 255/354: batchLoss = 0.6184, diffLoss = 3.0342, kgLoss = 0.0145
2025-04-08 14:59:29.156900: Training Step 256/354: batchLoss = 0.4716, diffLoss = 2.3091, kgLoss = 0.0122
2025-04-08 14:59:30.807492: Training Step 257/354: batchLoss = 0.6023, diffLoss = 2.9628, kgLoss = 0.0122
2025-04-08 14:59:32.433305: Training Step 258/354: batchLoss = 0.4740, diffLoss = 2.3176, kgLoss = 0.0131
2025-04-08 14:59:34.069192: Training Step 259/354: batchLoss = 0.5726, diffLoss = 2.8094, kgLoss = 0.0134
2025-04-08 14:59:35.692342: Training Step 260/354: batchLoss = 0.6614, diffLoss = 3.2450, kgLoss = 0.0156
2025-04-08 14:59:37.317418: Training Step 261/354: batchLoss = 0.6679, diffLoss = 3.2739, kgLoss = 0.0164
2025-04-08 14:59:38.940322: Training Step 262/354: batchLoss = 0.5297, diffLoss = 2.5934, kgLoss = 0.0138
2025-04-08 14:59:40.555054: Training Step 263/354: batchLoss = 0.5276, diffLoss = 2.5874, kgLoss = 0.0126
2025-04-08 14:59:42.174442: Training Step 264/354: batchLoss = 0.5501, diffLoss = 2.6933, kgLoss = 0.0143
2025-04-08 14:59:43.776746: Training Step 265/354: batchLoss = 0.4639, diffLoss = 2.2750, kgLoss = 0.0111
2025-04-08 14:59:45.397118: Training Step 266/354: batchLoss = 0.6182, diffLoss = 3.0344, kgLoss = 0.0141
2025-04-08 14:59:47.019359: Training Step 267/354: batchLoss = 0.6513, diffLoss = 3.1983, kgLoss = 0.0146
2025-04-08 14:59:48.640337: Training Step 268/354: batchLoss = 0.6357, diffLoss = 3.1094, kgLoss = 0.0173
2025-04-08 14:59:50.271153: Training Step 269/354: batchLoss = 0.4751, diffLoss = 2.3260, kgLoss = 0.0124
2025-04-08 14:59:51.896498: Training Step 270/354: batchLoss = 0.6457, diffLoss = 3.1679, kgLoss = 0.0151
2025-04-08 14:59:53.516899: Training Step 271/354: batchLoss = 0.6004, diffLoss = 2.9411, kgLoss = 0.0152
2025-04-08 14:59:55.142593: Training Step 272/354: batchLoss = 0.6238, diffLoss = 3.0640, kgLoss = 0.0138
2025-04-08 14:59:56.755912: Training Step 273/354: batchLoss = 0.5675, diffLoss = 2.7809, kgLoss = 0.0141
2025-04-08 14:59:58.365602: Training Step 274/354: batchLoss = 0.8704, diffLoss = 4.2791, kgLoss = 0.0183
2025-04-08 14:59:59.983455: Training Step 275/354: batchLoss = 0.7580, diffLoss = 3.7232, kgLoss = 0.0167
2025-04-08 15:00:01.612882: Training Step 276/354: batchLoss = 0.5330, diffLoss = 2.6086, kgLoss = 0.0142
2025-04-08 15:00:03.240629: Training Step 277/354: batchLoss = 0.5090, diffLoss = 2.4940, kgLoss = 0.0128
2025-04-08 15:00:04.868878: Training Step 278/354: batchLoss = 0.5354, diffLoss = 2.6226, kgLoss = 0.0136
2025-04-08 15:00:06.489143: Training Step 279/354: batchLoss = 0.6773, diffLoss = 3.3185, kgLoss = 0.0170
2025-04-08 15:00:08.107008: Training Step 280/354: batchLoss = 0.6165, diffLoss = 3.0194, kgLoss = 0.0157
2025-04-08 15:00:09.727372: Training Step 281/354: batchLoss = 0.5595, diffLoss = 2.7400, kgLoss = 0.0144
2025-04-08 15:00:11.345076: Training Step 282/354: batchLoss = 0.4569, diffLoss = 2.2426, kgLoss = 0.0105
2025-04-08 15:00:12.960787: Training Step 283/354: batchLoss = 0.5919, diffLoss = 2.9008, kgLoss = 0.0146
2025-04-08 15:00:14.586556: Training Step 284/354: batchLoss = 0.7594, diffLoss = 3.7295, kgLoss = 0.0169
2025-04-08 15:00:16.209481: Training Step 285/354: batchLoss = 0.4837, diffLoss = 2.3642, kgLoss = 0.0135
2025-04-08 15:00:17.843242: Training Step 286/354: batchLoss = 0.5642, diffLoss = 2.7644, kgLoss = 0.0142
2025-04-08 15:00:19.462854: Training Step 287/354: batchLoss = 0.8274, diffLoss = 4.0556, kgLoss = 0.0204
2025-04-08 15:00:21.077630: Training Step 288/354: batchLoss = 0.6647, diffLoss = 3.2571, kgLoss = 0.0166
2025-04-08 15:00:22.702079: Training Step 289/354: batchLoss = 0.5910, diffLoss = 2.8972, kgLoss = 0.0144
2025-04-08 15:00:24.323292: Training Step 290/354: batchLoss = 0.5279, diffLoss = 2.5867, kgLoss = 0.0132
2025-04-08 15:00:25.942468: Training Step 291/354: batchLoss = 0.5228, diffLoss = 2.5593, kgLoss = 0.0137
2025-04-08 15:00:27.559727: Training Step 292/354: batchLoss = 0.5622, diffLoss = 2.7499, kgLoss = 0.0152
2025-04-08 15:00:29.178625: Training Step 293/354: batchLoss = 0.5633, diffLoss = 2.7592, kgLoss = 0.0143
2025-04-08 15:00:30.805936: Training Step 294/354: batchLoss = 0.4994, diffLoss = 2.4476, kgLoss = 0.0123
2025-04-08 15:00:32.428985: Training Step 295/354: batchLoss = 0.5713, diffLoss = 2.7934, kgLoss = 0.0158
2025-04-08 15:00:34.047086: Training Step 296/354: batchLoss = 0.6057, diffLoss = 2.9680, kgLoss = 0.0151
2025-04-08 15:00:35.660276: Training Step 297/354: batchLoss = 0.6082, diffLoss = 2.9776, kgLoss = 0.0158
2025-04-08 15:00:37.281770: Training Step 298/354: batchLoss = 0.6836, diffLoss = 3.3599, kgLoss = 0.0145
2025-04-08 15:00:38.907493: Training Step 299/354: batchLoss = 0.5621, diffLoss = 2.7511, kgLoss = 0.0148
2025-04-08 15:00:40.529333: Training Step 300/354: batchLoss = 0.5383, diffLoss = 2.6368, kgLoss = 0.0137
2025-04-08 15:00:42.146116: Training Step 301/354: batchLoss = 0.6689, diffLoss = 3.2750, kgLoss = 0.0174
2025-04-08 15:00:43.771067: Training Step 302/354: batchLoss = 0.5101, diffLoss = 2.4997, kgLoss = 0.0127
2025-04-08 15:00:45.389765: Training Step 303/354: batchLoss = 0.6216, diffLoss = 3.0442, kgLoss = 0.0160
2025-04-08 15:00:47.005963: Training Step 304/354: batchLoss = 0.6039, diffLoss = 2.9654, kgLoss = 0.0136
2025-04-08 15:00:48.631544: Training Step 305/354: batchLoss = 0.6372, diffLoss = 3.1266, kgLoss = 0.0149
2025-04-08 15:00:50.256816: Training Step 306/354: batchLoss = 0.5602, diffLoss = 2.7441, kgLoss = 0.0142
2025-04-08 15:00:51.873227: Training Step 307/354: batchLoss = 0.5360, diffLoss = 2.6188, kgLoss = 0.0153
2025-04-08 15:00:53.496975: Training Step 308/354: batchLoss = 0.7067, diffLoss = 3.4665, kgLoss = 0.0168
2025-04-08 15:00:55.114674: Training Step 309/354: batchLoss = 0.7059, diffLoss = 3.4625, kgLoss = 0.0167
2025-04-08 15:00:56.736044: Training Step 310/354: batchLoss = 0.4669, diffLoss = 2.2869, kgLoss = 0.0120
2025-04-08 15:00:58.343763: Training Step 311/354: batchLoss = 0.5423, diffLoss = 2.6554, kgLoss = 0.0140
2025-04-08 15:00:59.964113: Training Step 312/354: batchLoss = 0.5598, diffLoss = 2.7452, kgLoss = 0.0134
2025-04-08 15:01:01.596958: Training Step 313/354: batchLoss = 0.5632, diffLoss = 2.7620, kgLoss = 0.0135
2025-04-08 15:01:03.227832: Training Step 314/354: batchLoss = 0.4718, diffLoss = 2.3119, kgLoss = 0.0118
2025-04-08 15:01:04.850738: Training Step 315/354: batchLoss = 0.5370, diffLoss = 2.6275, kgLoss = 0.0144
2025-04-08 15:01:06.473527: Training Step 316/354: batchLoss = 0.5504, diffLoss = 2.6975, kgLoss = 0.0137
2025-04-08 15:01:08.097753: Training Step 317/354: batchLoss = 0.6287, diffLoss = 3.0884, kgLoss = 0.0138
2025-04-08 15:01:09.716677: Training Step 318/354: batchLoss = 0.5190, diffLoss = 2.5448, kgLoss = 0.0125
2025-04-08 15:01:11.330631: Training Step 319/354: batchLoss = 0.5265, diffLoss = 2.5786, kgLoss = 0.0135
2025-04-08 15:01:12.950170: Training Step 320/354: batchLoss = 0.5991, diffLoss = 2.9343, kgLoss = 0.0152
2025-04-08 15:01:14.574016: Training Step 321/354: batchLoss = 0.6271, diffLoss = 3.0709, kgLoss = 0.0161
2025-04-08 15:01:16.199238: Training Step 322/354: batchLoss = 0.4378, diffLoss = 2.1409, kgLoss = 0.0120
2025-04-08 15:01:17.827864: Training Step 323/354: batchLoss = 0.7004, diffLoss = 3.4112, kgLoss = 0.0227
2025-04-08 15:01:19.449520: Training Step 324/354: batchLoss = 0.6764, diffLoss = 3.3230, kgLoss = 0.0147
2025-04-08 15:01:21.067190: Training Step 325/354: batchLoss = 0.6182, diffLoss = 3.0307, kgLoss = 0.0151
2025-04-08 15:01:22.687614: Training Step 326/354: batchLoss = 0.6928, diffLoss = 3.3946, kgLoss = 0.0173
2025-04-08 15:01:24.306254: Training Step 327/354: batchLoss = 0.6338, diffLoss = 3.1060, kgLoss = 0.0158
2025-04-08 15:01:25.922579: Training Step 328/354: batchLoss = 0.6141, diffLoss = 3.0085, kgLoss = 0.0155
2025-04-08 15:01:27.548155: Training Step 329/354: batchLoss = 0.6886, diffLoss = 3.3794, kgLoss = 0.0159
2025-04-08 15:01:29.169531: Training Step 330/354: batchLoss = 0.5684, diffLoss = 2.7786, kgLoss = 0.0158
2025-04-08 15:01:30.797421: Training Step 331/354: batchLoss = 0.5854, diffLoss = 2.8588, kgLoss = 0.0170
2025-04-08 15:01:32.418889: Training Step 332/354: batchLoss = 0.7397, diffLoss = 3.6375, kgLoss = 0.0153
2025-04-08 15:01:34.042813: Training Step 333/354: batchLoss = 0.6636, diffLoss = 3.2501, kgLoss = 0.0169
2025-04-08 15:01:35.663852: Training Step 334/354: batchLoss = 0.4993, diffLoss = 2.4466, kgLoss = 0.0125
2025-04-08 15:01:37.290555: Training Step 335/354: batchLoss = 0.4715, diffLoss = 2.3133, kgLoss = 0.0110
2025-04-08 15:01:38.910672: Training Step 336/354: batchLoss = 0.5235, diffLoss = 2.5629, kgLoss = 0.0137
2025-04-08 15:01:40.530896: Training Step 337/354: batchLoss = 0.5029, diffLoss = 2.4656, kgLoss = 0.0122
2025-04-08 15:01:42.147960: Training Step 338/354: batchLoss = 0.5370, diffLoss = 2.6219, kgLoss = 0.0157
2025-04-08 15:01:43.773110: Training Step 339/354: batchLoss = 0.6385, diffLoss = 3.1304, kgLoss = 0.0155
2025-04-08 15:01:45.386999: Training Step 340/354: batchLoss = 0.5376, diffLoss = 2.6291, kgLoss = 0.0148
2025-04-08 15:01:47.002074: Training Step 341/354: batchLoss = 0.6452, diffLoss = 3.1662, kgLoss = 0.0150
2025-04-08 15:01:48.628203: Training Step 342/354: batchLoss = 0.5865, diffLoss = 2.8792, kgLoss = 0.0133
2025-04-08 15:01:50.249546: Training Step 343/354: batchLoss = 0.5804, diffLoss = 2.8444, kgLoss = 0.0145
2025-04-08 15:01:51.875240: Training Step 344/354: batchLoss = 0.7181, diffLoss = 3.5223, kgLoss = 0.0170
2025-04-08 15:01:53.487731: Training Step 345/354: batchLoss = 0.6480, diffLoss = 3.1770, kgLoss = 0.0158
2025-04-08 15:01:55.102010: Training Step 346/354: batchLoss = 0.6046, diffLoss = 2.9645, kgLoss = 0.0146
2025-04-08 15:01:56.714226: Training Step 347/354: batchLoss = 0.6833, diffLoss = 3.3565, kgLoss = 0.0150
2025-04-08 15:01:58.331140: Training Step 348/354: batchLoss = 0.4816, diffLoss = 2.3532, kgLoss = 0.0138
2025-04-08 15:01:59.953816: Training Step 349/354: batchLoss = 0.6491, diffLoss = 3.1827, kgLoss = 0.0158
2025-04-08 15:02:01.582979: Training Step 350/354: batchLoss = 0.5620, diffLoss = 2.7548, kgLoss = 0.0137
2025-04-08 15:02:03.199853: Training Step 351/354: batchLoss = 0.5440, diffLoss = 2.6633, kgLoss = 0.0142
2025-04-08 15:02:04.794345: Training Step 352/354: batchLoss = 0.7301, diffLoss = 3.5732, kgLoss = 0.0194
2025-04-08 15:02:06.202275: Training Step 353/354: batchLoss = 0.5709, diffLoss = 2.7912, kgLoss = 0.0158
2025-04-08 15:02:06.292495: 
2025-04-08 15:02:06.293202: Epoch 11/1000, Train: epLoss = 1.0551, epDfLoss = 5.1705, epfTransLoss = 0.0000, epKgLoss = 0.0262  
2025-04-08 15:02:07.614896: Steps 0/138: batch_recall = 46.96, batch_ndcg = 26.58 
2025-04-08 15:02:08.925024: Steps 1/138: batch_recall = 46.41, batch_ndcg = 27.49 
2025-04-08 15:02:10.226231: Steps 2/138: batch_recall = 58.66, batch_ndcg = 36.99 
2025-04-08 15:02:11.536691: Steps 3/138: batch_recall = 58.17, batch_ndcg = 33.69 
2025-04-08 15:02:12.836292: Steps 4/138: batch_recall = 66.66, batch_ndcg = 39.95 
2025-04-08 15:02:14.143096: Steps 5/138: batch_recall = 60.57, batch_ndcg = 32.56 
2025-04-08 15:02:15.445046: Steps 6/138: batch_recall = 49.87, batch_ndcg = 29.09 
2025-04-08 15:02:16.752908: Steps 7/138: batch_recall = 62.73, batch_ndcg = 39.70 
2025-04-08 15:02:18.049822: Steps 8/138: batch_recall = 62.28, batch_ndcg = 38.84 
2025-04-08 15:02:19.344875: Steps 9/138: batch_recall = 53.68, batch_ndcg = 33.52 
2025-04-08 15:02:20.631467: Steps 10/138: batch_recall = 54.31, batch_ndcg = 29.76 
2025-04-08 15:02:21.910373: Steps 11/138: batch_recall = 60.06, batch_ndcg = 34.24 
2025-04-08 15:02:23.188995: Steps 12/138: batch_recall = 53.90, batch_ndcg = 29.16 
2025-04-08 15:02:24.484128: Steps 13/138: batch_recall = 51.25, batch_ndcg = 29.25 
2025-04-08 15:02:25.761655: Steps 14/138: batch_recall = 52.79, batch_ndcg = 30.46 
2025-04-08 15:02:27.054095: Steps 15/138: batch_recall = 50.59, batch_ndcg = 29.37 
2025-04-08 15:02:28.358447: Steps 16/138: batch_recall = 61.89, batch_ndcg = 33.14 
2025-04-08 15:02:29.645024: Steps 17/138: batch_recall = 54.66, batch_ndcg = 30.98 
2025-04-08 15:02:30.947603: Steps 18/138: batch_recall = 51.58, batch_ndcg = 32.11 
2025-04-08 15:02:32.245384: Steps 19/138: batch_recall = 54.12, batch_ndcg = 32.08 
2025-04-08 15:02:33.534563: Steps 20/138: batch_recall = 60.40, batch_ndcg = 35.36 
2025-04-08 15:02:34.837317: Steps 21/138: batch_recall = 64.22, batch_ndcg = 38.62 
2025-04-08 15:02:36.122207: Steps 22/138: batch_recall = 56.06, batch_ndcg = 32.42 
2025-04-08 15:02:37.417356: Steps 23/138: batch_recall = 52.69, batch_ndcg = 29.51 
2025-04-08 15:02:38.709858: Steps 24/138: batch_recall = 53.75, batch_ndcg = 30.44 
2025-04-08 15:02:40.030892: Steps 25/138: batch_recall = 58.29, batch_ndcg = 33.58 
2025-04-08 15:02:41.351841: Steps 26/138: batch_recall = 57.79, batch_ndcg = 33.41 
2025-04-08 15:02:42.676530: Steps 27/138: batch_recall = 63.21, batch_ndcg = 34.73 
2025-04-08 15:02:43.992998: Steps 28/138: batch_recall = 56.00, batch_ndcg = 32.14 
2025-04-08 15:02:45.286850: Steps 29/138: batch_recall = 58.49, batch_ndcg = 30.29 
2025-04-08 15:02:46.582074: Steps 30/138: batch_recall = 56.25, batch_ndcg = 33.40 
2025-04-08 15:02:47.879015: Steps 31/138: batch_recall = 40.19, batch_ndcg = 23.41 
2025-04-08 15:02:49.187546: Steps 32/138: batch_recall = 51.55, batch_ndcg = 30.90 
2025-04-08 15:02:50.511987: Steps 33/138: batch_recall = 59.54, batch_ndcg = 33.25 
2025-04-08 15:02:51.829142: Steps 34/138: batch_recall = 53.81, batch_ndcg = 28.62 
2025-04-08 15:02:53.124749: Steps 35/138: batch_recall = 51.97, batch_ndcg = 29.84 
2025-04-08 15:02:54.406183: Steps 36/138: batch_recall = 51.62, batch_ndcg = 28.53 
2025-04-08 15:02:55.683225: Steps 37/138: batch_recall = 61.49, batch_ndcg = 34.26 
2025-04-08 15:02:56.962661: Steps 38/138: batch_recall = 60.60, batch_ndcg = 32.74 
2025-04-08 15:02:58.251035: Steps 39/138: batch_recall = 70.81, batch_ndcg = 37.58 
2025-04-08 15:02:59.547638: Steps 40/138: batch_recall = 54.07, batch_ndcg = 28.18 
2025-04-08 15:03:00.826802: Steps 41/138: batch_recall = 58.45, batch_ndcg = 32.86 
2025-04-08 15:03:02.105162: Steps 42/138: batch_recall = 53.85, batch_ndcg = 29.79 
2025-04-08 15:03:03.408001: Steps 43/138: batch_recall = 53.20, batch_ndcg = 31.78 
2025-04-08 15:03:04.694970: Steps 44/138: batch_recall = 55.92, batch_ndcg = 28.17 
2025-04-08 15:03:05.990919: Steps 45/138: batch_recall = 59.88, batch_ndcg = 33.05 
2025-04-08 15:03:07.275455: Steps 46/138: batch_recall = 60.04, batch_ndcg = 34.25 
2025-04-08 15:03:08.570833: Steps 47/138: batch_recall = 57.49, batch_ndcg = 32.76 
2025-04-08 15:03:09.854791: Steps 48/138: batch_recall = 55.08, batch_ndcg = 32.13 
2025-04-08 15:03:11.142081: Steps 49/138: batch_recall = 60.69, batch_ndcg = 36.37 
2025-04-08 15:03:12.420968: Steps 50/138: batch_recall = 59.39, batch_ndcg = 33.06 
2025-04-08 15:03:13.710202: Steps 51/138: batch_recall = 61.80, batch_ndcg = 36.06 
2025-04-08 15:03:15.007996: Steps 52/138: batch_recall = 67.63, batch_ndcg = 41.83 
2025-04-08 15:03:16.305569: Steps 53/138: batch_recall = 62.46, batch_ndcg = 32.45 
2025-04-08 15:03:17.595836: Steps 54/138: batch_recall = 65.09, batch_ndcg = 37.67 
2025-04-08 15:03:18.874036: Steps 55/138: batch_recall = 61.17, batch_ndcg = 33.89 
2025-04-08 15:03:20.161515: Steps 56/138: batch_recall = 62.44, batch_ndcg = 34.45 
2025-04-08 15:03:21.453323: Steps 57/138: batch_recall = 60.21, batch_ndcg = 33.69 
2025-04-08 15:03:22.735649: Steps 58/138: batch_recall = 68.91, batch_ndcg = 36.27 
2025-04-08 15:03:24.023799: Steps 59/138: batch_recall = 76.78, batch_ndcg = 43.11 
2025-04-08 15:03:25.308326: Steps 60/138: batch_recall = 71.10, batch_ndcg = 38.66 
2025-04-08 15:03:26.615035: Steps 61/138: batch_recall = 58.08, batch_ndcg = 33.30 
2025-04-08 15:03:27.895482: Steps 62/138: batch_recall = 83.76, batch_ndcg = 43.62 
2025-04-08 15:03:29.181948: Steps 63/138: batch_recall = 78.05, batch_ndcg = 43.47 
2025-04-08 15:03:30.478753: Steps 64/138: batch_recall = 63.48, batch_ndcg = 33.93 
2025-04-08 15:03:31.763134: Steps 65/138: batch_recall = 82.59, batch_ndcg = 45.83 
2025-04-08 15:03:33.046491: Steps 66/138: batch_recall = 67.25, batch_ndcg = 40.84 
2025-04-08 15:03:34.334229: Steps 67/138: batch_recall = 80.12, batch_ndcg = 47.93 
2025-04-08 15:03:35.611177: Steps 68/138: batch_recall = 64.51, batch_ndcg = 34.44 
2025-04-08 15:03:36.889567: Steps 69/138: batch_recall = 84.49, batch_ndcg = 49.19 
2025-04-08 15:03:38.167264: Steps 70/138: batch_recall = 80.62, batch_ndcg = 45.40 
2025-04-08 15:03:39.441824: Steps 71/138: batch_recall = 88.26, batch_ndcg = 51.08 
2025-04-08 15:03:40.720363: Steps 72/138: batch_recall = 88.30, batch_ndcg = 51.03 
2025-04-08 15:03:42.003180: Steps 73/138: batch_recall = 89.92, batch_ndcg = 49.00 
2025-04-08 15:03:43.291725: Steps 74/138: batch_recall = 82.81, batch_ndcg = 48.83 
2025-04-08 15:03:44.575761: Steps 75/138: batch_recall = 86.92, batch_ndcg = 49.87 
2025-04-08 15:03:45.863534: Steps 76/138: batch_recall = 97.82, batch_ndcg = 54.99 
2025-04-08 15:03:47.135524: Steps 77/138: batch_recall = 88.08, batch_ndcg = 48.84 
2025-04-08 15:03:48.420111: Steps 78/138: batch_recall = 93.30, batch_ndcg = 48.62 
2025-04-08 15:03:49.696560: Steps 79/138: batch_recall = 93.74, batch_ndcg = 49.71 
2025-04-08 15:03:50.968613: Steps 80/138: batch_recall = 73.38, batch_ndcg = 38.64 
2025-04-08 15:03:52.249202: Steps 81/138: batch_recall = 80.72, batch_ndcg = 48.30 
2025-04-08 15:03:53.525763: Steps 82/138: batch_recall = 85.35, batch_ndcg = 50.51 
2025-04-08 15:03:54.813691: Steps 83/138: batch_recall = 84.29, batch_ndcg = 48.31 
2025-04-08 15:03:56.087351: Steps 84/138: batch_recall = 103.50, batch_ndcg = 56.61 
2025-04-08 15:03:57.360584: Steps 85/138: batch_recall = 99.77, batch_ndcg = 59.09 
2025-04-08 15:03:58.641311: Steps 86/138: batch_recall = 115.37, batch_ndcg = 70.15 
2025-04-08 15:03:59.916839: Steps 87/138: batch_recall = 107.95, batch_ndcg = 57.76 
2025-04-08 15:04:01.205652: Steps 88/138: batch_recall = 105.71, batch_ndcg = 57.81 
2025-04-08 15:04:02.479732: Steps 89/138: batch_recall = 119.77, batch_ndcg = 66.80 
2025-04-08 15:04:03.747695: Steps 90/138: batch_recall = 106.41, batch_ndcg = 59.15 
2025-04-08 15:04:05.031264: Steps 91/138: batch_recall = 120.47, batch_ndcg = 66.47 
2025-04-08 15:04:06.315003: Steps 92/138: batch_recall = 110.44, batch_ndcg = 60.94 
2025-04-08 15:04:07.584110: Steps 93/138: batch_recall = 116.46, batch_ndcg = 67.18 
2025-04-08 15:04:08.849873: Steps 94/138: batch_recall = 120.79, batch_ndcg = 63.26 
2025-04-08 15:04:10.136399: Steps 95/138: batch_recall = 110.52, batch_ndcg = 67.35 
2025-04-08 15:04:11.408302: Steps 96/138: batch_recall = 127.84, batch_ndcg = 72.16 
2025-04-08 15:04:12.698098: Steps 97/138: batch_recall = 134.97, batch_ndcg = 78.62 
2025-04-08 15:04:13.976011: Steps 98/138: batch_recall = 106.96, batch_ndcg = 62.22 
2025-04-08 15:04:15.257101: Steps 99/138: batch_recall = 122.22, batch_ndcg = 69.11 
2025-04-08 15:04:16.528535: Steps 100/138: batch_recall = 128.05, batch_ndcg = 70.46 
2025-04-08 15:04:17.802848: Steps 101/138: batch_recall = 125.77, batch_ndcg = 69.16 
2025-04-08 15:04:19.083166: Steps 102/138: batch_recall = 128.42, batch_ndcg = 74.89 
2025-04-08 15:04:20.349741: Steps 103/138: batch_recall = 144.13, batch_ndcg = 81.26 
2025-04-08 15:04:21.627119: Steps 104/138: batch_recall = 140.03, batch_ndcg = 77.23 
2025-04-08 15:04:22.893509: Steps 105/138: batch_recall = 120.27, batch_ndcg = 68.29 
2025-04-08 15:04:24.171678: Steps 106/138: batch_recall = 102.24, batch_ndcg = 58.18 
2025-04-08 15:04:25.442044: Steps 107/138: batch_recall = 115.33, batch_ndcg = 64.19 
2025-04-08 15:04:26.723830: Steps 108/138: batch_recall = 119.98, batch_ndcg = 71.40 
2025-04-08 15:04:27.998657: Steps 109/138: batch_recall = 136.16, batch_ndcg = 74.72 
2025-04-08 15:04:29.273992: Steps 110/138: batch_recall = 124.07, batch_ndcg = 65.31 
2025-04-08 15:04:30.538046: Steps 111/138: batch_recall = 139.07, batch_ndcg = 83.52 
2025-04-08 15:04:31.814211: Steps 112/138: batch_recall = 151.02, batch_ndcg = 84.09 
2025-04-08 15:04:33.100981: Steps 113/138: batch_recall = 124.25, batch_ndcg = 71.60 
2025-04-08 15:04:34.375713: Steps 114/138: batch_recall = 122.59, batch_ndcg = 69.65 
2025-04-08 15:04:35.649973: Steps 115/138: batch_recall = 122.84, batch_ndcg = 63.42 
2025-04-08 15:04:36.923261: Steps 116/138: batch_recall = 123.86, batch_ndcg = 66.62 
2025-04-08 15:04:38.191662: Steps 117/138: batch_recall = 116.26, batch_ndcg = 66.68 
2025-04-08 15:04:39.469691: Steps 118/138: batch_recall = 120.80, batch_ndcg = 70.55 
2025-04-08 15:04:40.733687: Steps 119/138: batch_recall = 136.67, batch_ndcg = 72.50 
2025-04-08 15:04:42.013584: Steps 120/138: batch_recall = 120.39, batch_ndcg = 65.93 
2025-04-08 15:04:43.297078: Steps 121/138: batch_recall = 148.03, batch_ndcg = 78.11 
2025-04-08 15:04:44.571938: Steps 122/138: batch_recall = 144.41, batch_ndcg = 78.37 
2025-04-08 15:04:45.851182: Steps 123/138: batch_recall = 129.59, batch_ndcg = 71.18 
2025-04-08 15:04:47.129827: Steps 124/138: batch_recall = 150.71, batch_ndcg = 93.42 
2025-04-08 15:04:48.399901: Steps 125/138: batch_recall = 128.07, batch_ndcg = 69.92 
2025-04-08 15:04:49.661024: Steps 126/138: batch_recall = 157.67, batch_ndcg = 87.80 
2025-04-08 15:04:50.932444: Steps 127/138: batch_recall = 138.26, batch_ndcg = 80.03 
2025-04-08 15:04:52.201383: Steps 128/138: batch_recall = 127.51, batch_ndcg = 69.91 
2025-04-08 15:04:53.472639: Steps 129/138: batch_recall = 158.32, batch_ndcg = 89.25 
2025-04-08 15:04:54.746432: Steps 130/138: batch_recall = 133.08, batch_ndcg = 68.97 
2025-04-08 15:04:56.056637: Steps 131/138: batch_recall = 147.12, batch_ndcg = 83.87 
2025-04-08 15:04:57.330702: Steps 132/138: batch_recall = 150.33, batch_ndcg = 85.52 
2025-04-08 15:04:58.603497: Steps 133/138: batch_recall = 145.93, batch_ndcg = 83.02 
2025-04-08 15:04:59.877309: Steps 134/138: batch_recall = 137.63, batch_ndcg = 77.97 
2025-04-08 15:05:01.133144: Steps 135/138: batch_recall = 163.19, batch_ndcg = 92.02 
2025-04-08 15:05:02.402773: Steps 136/138: batch_recall = 148.95, batch_ndcg = 78.16 
2025-04-08 15:05:03.675674: Steps 137/138: batch_recall = 134.68, batch_ndcg = 84.47 
2025-04-08 15:05:03.676360: Epoch 11/1000, Test: Recall = 0.1749, NDCG = 0.0987  

2025-04-08 15:05:05.433418: Training Step 0/354: batchLoss = 0.6226, diffLoss = 3.0522, kgLoss = 0.0152
2025-04-08 15:05:07.050378: Training Step 1/354: batchLoss = 0.6205, diffLoss = 3.0405, kgLoss = 0.0155
2025-04-08 15:05:08.670966: Training Step 2/354: batchLoss = 0.4878, diffLoss = 2.3892, kgLoss = 0.0124
2025-04-08 15:05:10.289740: Training Step 3/354: batchLoss = 0.6331, diffLoss = 3.1017, kgLoss = 0.0160
2025-04-08 15:05:11.909379: Training Step 4/354: batchLoss = 0.6560, diffLoss = 3.2154, kgLoss = 0.0161
2025-04-08 15:05:13.537116: Training Step 5/354: batchLoss = 0.4679, diffLoss = 2.2930, kgLoss = 0.0117
2025-04-08 15:05:15.158067: Training Step 6/354: batchLoss = 0.5059, diffLoss = 2.4767, kgLoss = 0.0132
2025-04-08 15:05:16.775527: Training Step 7/354: batchLoss = 0.5070, diffLoss = 2.4879, kgLoss = 0.0117
2025-04-08 15:05:18.400526: Training Step 8/354: batchLoss = 0.5209, diffLoss = 2.5607, kgLoss = 0.0109
2025-04-08 15:05:20.017543: Training Step 9/354: batchLoss = 0.5050, diffLoss = 2.4649, kgLoss = 0.0150
2025-04-08 15:05:21.636582: Training Step 10/354: batchLoss = 0.5696, diffLoss = 2.7897, kgLoss = 0.0146
2025-04-08 15:05:23.257274: Training Step 11/354: batchLoss = 0.5611, diffLoss = 2.7486, kgLoss = 0.0142
2025-04-08 15:05:24.884458: Training Step 12/354: batchLoss = 0.5407, diffLoss = 2.6446, kgLoss = 0.0148
2025-04-08 15:05:26.517573: Training Step 13/354: batchLoss = 0.6165, diffLoss = 3.0251, kgLoss = 0.0144
2025-04-08 15:05:28.139960: Training Step 14/354: batchLoss = 0.5270, diffLoss = 2.5806, kgLoss = 0.0136
2025-04-08 15:05:29.766754: Training Step 15/354: batchLoss = 0.5645, diffLoss = 2.7704, kgLoss = 0.0131
2025-04-08 15:05:31.385895: Training Step 16/354: batchLoss = 0.6168, diffLoss = 3.0256, kgLoss = 0.0145
2025-04-08 15:05:33.018891: Training Step 17/354: batchLoss = 0.6379, diffLoss = 3.1237, kgLoss = 0.0164
2025-04-08 15:05:34.632499: Training Step 18/354: batchLoss = 0.4913, diffLoss = 2.4039, kgLoss = 0.0132
2025-04-08 15:05:36.250193: Training Step 19/354: batchLoss = 0.4350, diffLoss = 2.1298, kgLoss = 0.0113
2025-04-08 15:05:37.864035: Training Step 20/354: batchLoss = 0.5076, diffLoss = 2.4831, kgLoss = 0.0137
2025-04-08 15:05:39.488547: Training Step 21/354: batchLoss = 0.5087, diffLoss = 2.4941, kgLoss = 0.0124
2025-04-08 15:05:41.106960: Training Step 22/354: batchLoss = 0.5408, diffLoss = 2.6476, kgLoss = 0.0141
2025-04-08 15:05:42.727329: Training Step 23/354: batchLoss = 0.6649, diffLoss = 3.2676, kgLoss = 0.0143
2025-04-08 15:05:44.350560: Training Step 24/354: batchLoss = 0.6463, diffLoss = 3.1665, kgLoss = 0.0162
2025-04-08 15:05:45.965581: Training Step 25/354: batchLoss = 0.5995, diffLoss = 2.9281, kgLoss = 0.0174
2025-04-08 15:05:47.591955: Training Step 26/354: batchLoss = 0.6070, diffLoss = 2.9753, kgLoss = 0.0149
2025-04-08 15:05:49.211185: Training Step 27/354: batchLoss = 0.5939, diffLoss = 2.9112, kgLoss = 0.0145
2025-04-08 15:05:50.818796: Training Step 28/354: batchLoss = 0.5006, diffLoss = 2.4546, kgLoss = 0.0121
2025-04-08 15:05:52.434249: Training Step 29/354: batchLoss = 0.6253, diffLoss = 3.0673, kgLoss = 0.0148
2025-04-08 15:05:54.054969: Training Step 30/354: batchLoss = 0.5110, diffLoss = 2.5055, kgLoss = 0.0124
2025-04-08 15:05:55.674874: Training Step 31/354: batchLoss = 1.1118, diffLoss = 5.4619, kgLoss = 0.0243
2025-04-08 15:05:57.291287: Training Step 32/354: batchLoss = 0.5909, diffLoss = 2.8938, kgLoss = 0.0151
2025-04-08 15:05:58.921623: Training Step 33/354: batchLoss = 0.5574, diffLoss = 2.7368, kgLoss = 0.0126
2025-04-08 15:06:00.559913: Training Step 34/354: batchLoss = 0.5585, diffLoss = 2.7164, kgLoss = 0.0190
2025-04-08 15:06:02.199300: Training Step 35/354: batchLoss = 0.7036, diffLoss = 3.4481, kgLoss = 0.0175
2025-04-08 15:06:03.829719: Training Step 36/354: batchLoss = 0.6049, diffLoss = 2.9707, kgLoss = 0.0134
2025-04-08 15:06:05.455334: Training Step 37/354: batchLoss = 0.5148, diffLoss = 2.5207, kgLoss = 0.0133
2025-04-08 15:06:07.085026: Training Step 38/354: batchLoss = 0.6407, diffLoss = 3.1271, kgLoss = 0.0191
2025-04-08 15:06:08.709137: Training Step 39/354: batchLoss = 0.5201, diffLoss = 2.5417, kgLoss = 0.0146
2025-04-08 15:06:10.359988: Training Step 40/354: batchLoss = 0.5769, diffLoss = 2.8265, kgLoss = 0.0145
2025-04-08 15:06:11.981080: Training Step 41/354: batchLoss = 0.5911, diffLoss = 2.8972, kgLoss = 0.0146
2025-04-08 15:06:13.605602: Training Step 42/354: batchLoss = 0.7408, diffLoss = 3.6401, kgLoss = 0.0159
2025-04-08 15:06:15.228159: Training Step 43/354: batchLoss = 0.6609, diffLoss = 3.2386, kgLoss = 0.0164
2025-04-08 15:06:16.843735: Training Step 44/354: batchLoss = 0.6722, diffLoss = 3.3015, kgLoss = 0.0149
2025-04-08 15:06:18.455995: Training Step 45/354: batchLoss = 0.4475, diffLoss = 2.1879, kgLoss = 0.0124
2025-04-08 15:06:20.069652: Training Step 46/354: batchLoss = 0.5020, diffLoss = 2.4567, kgLoss = 0.0133
2025-04-08 15:06:21.697517: Training Step 47/354: batchLoss = 0.6518, diffLoss = 3.1907, kgLoss = 0.0171
2025-04-08 15:06:23.319304: Training Step 48/354: batchLoss = 0.5244, diffLoss = 2.5751, kgLoss = 0.0118
2025-04-08 15:06:24.947308: Training Step 49/354: batchLoss = 0.6994, diffLoss = 3.4320, kgLoss = 0.0162
2025-04-08 15:06:26.568792: Training Step 50/354: batchLoss = 0.5957, diffLoss = 2.9164, kgLoss = 0.0155
2025-04-08 15:06:28.182406: Training Step 51/354: batchLoss = 0.5699, diffLoss = 2.7960, kgLoss = 0.0134
2025-04-08 15:06:29.801907: Training Step 52/354: batchLoss = 0.5768, diffLoss = 2.8251, kgLoss = 0.0147
2025-04-08 15:06:31.425180: Training Step 53/354: batchLoss = 0.6251, diffLoss = 3.0645, kgLoss = 0.0152
2025-04-08 15:06:33.040671: Training Step 54/354: batchLoss = 0.4926, diffLoss = 2.4108, kgLoss = 0.0131
2025-04-08 15:06:34.653770: Training Step 55/354: batchLoss = 0.8167, diffLoss = 4.0094, kgLoss = 0.0185
2025-04-08 15:06:36.275976: Training Step 56/354: batchLoss = 0.4881, diffLoss = 2.3942, kgLoss = 0.0116
2025-04-08 15:06:37.897194: Training Step 57/354: batchLoss = 0.4571, diffLoss = 2.2364, kgLoss = 0.0123
2025-04-08 15:06:39.515645: Training Step 58/354: batchLoss = 0.5757, diffLoss = 2.8185, kgLoss = 0.0150
2025-04-08 15:06:41.136745: Training Step 59/354: batchLoss = 0.5213, diffLoss = 2.5557, kgLoss = 0.0127
2025-04-08 15:06:42.757047: Training Step 60/354: batchLoss = 0.5527, diffLoss = 2.7066, kgLoss = 0.0143
2025-04-08 15:06:44.376518: Training Step 61/354: batchLoss = 0.6258, diffLoss = 3.0671, kgLoss = 0.0155
2025-04-08 15:06:45.997475: Training Step 62/354: batchLoss = 0.6182, diffLoss = 3.0265, kgLoss = 0.0161
2025-04-08 15:06:47.629459: Training Step 63/354: batchLoss = 0.5401, diffLoss = 2.6415, kgLoss = 0.0148
2025-04-08 15:06:49.247755: Training Step 64/354: batchLoss = 0.5273, diffLoss = 2.5861, kgLoss = 0.0126
2025-04-08 15:06:50.863702: Training Step 65/354: batchLoss = 0.6136, diffLoss = 3.0020, kgLoss = 0.0165
2025-04-08 15:06:52.487576: Training Step 66/354: batchLoss = 0.4737, diffLoss = 2.3206, kgLoss = 0.0120
2025-04-08 15:06:54.110861: Training Step 67/354: batchLoss = 0.5319, diffLoss = 2.6067, kgLoss = 0.0132
2025-04-08 15:06:55.731591: Training Step 68/354: batchLoss = 0.5731, diffLoss = 2.6964, kgLoss = 0.0423
2025-04-08 15:06:57.353217: Training Step 69/354: batchLoss = 0.4652, diffLoss = 2.2759, kgLoss = 0.0126
2025-04-08 15:06:58.973973: Training Step 70/354: batchLoss = 0.5460, diffLoss = 2.6785, kgLoss = 0.0129
2025-04-08 15:07:00.606880: Training Step 71/354: batchLoss = 0.5819, diffLoss = 2.8549, kgLoss = 0.0137
2025-04-08 15:07:02.230297: Training Step 72/354: batchLoss = 0.5006, diffLoss = 2.4516, kgLoss = 0.0129
2025-04-08 15:07:03.842101: Training Step 73/354: batchLoss = 0.5837, diffLoss = 2.8614, kgLoss = 0.0143
2025-04-08 15:07:05.459132: Training Step 74/354: batchLoss = 0.6576, diffLoss = 3.2282, kgLoss = 0.0149
2025-04-08 15:07:07.079234: Training Step 75/354: batchLoss = 0.6208, diffLoss = 3.0428, kgLoss = 0.0153
2025-04-08 15:07:08.697739: Training Step 76/354: batchLoss = 0.6228, diffLoss = 3.0562, kgLoss = 0.0144
2025-04-08 15:07:10.323239: Training Step 77/354: batchLoss = 0.6262, diffLoss = 3.0686, kgLoss = 0.0155
2025-04-08 15:07:11.949990: Training Step 78/354: batchLoss = 0.6142, diffLoss = 3.0048, kgLoss = 0.0165
2025-04-08 15:07:13.572532: Training Step 79/354: batchLoss = 0.7014, diffLoss = 3.4384, kgLoss = 0.0171
2025-04-08 15:07:15.196088: Training Step 80/354: batchLoss = 0.5821, diffLoss = 2.8526, kgLoss = 0.0145
2025-04-08 15:07:16.808808: Training Step 81/354: batchLoss = 0.9140, diffLoss = 4.4912, kgLoss = 0.0197
2025-04-08 15:07:18.432348: Training Step 82/354: batchLoss = 0.5644, diffLoss = 2.7581, kgLoss = 0.0160
2025-04-08 15:07:20.050184: Training Step 83/354: batchLoss = 0.5716, diffLoss = 2.8062, kgLoss = 0.0130
2025-04-08 15:07:21.670763: Training Step 84/354: batchLoss = 0.7480, diffLoss = 3.6747, kgLoss = 0.0164
2025-04-08 15:07:23.298081: Training Step 85/354: batchLoss = 0.7365, diffLoss = 3.6060, kgLoss = 0.0191
2025-04-08 15:07:24.917465: Training Step 86/354: batchLoss = 0.5294, diffLoss = 2.5936, kgLoss = 0.0133
2025-04-08 15:07:26.539802: Training Step 87/354: batchLoss = 0.5934, diffLoss = 2.9052, kgLoss = 0.0155
2025-04-08 15:07:28.176164: Training Step 88/354: batchLoss = 0.7280, diffLoss = 3.5720, kgLoss = 0.0170
2025-04-08 15:07:29.803779: Training Step 89/354: batchLoss = 0.6199, diffLoss = 3.0365, kgLoss = 0.0157
2025-04-08 15:07:31.426304: Training Step 90/354: batchLoss = 0.5745, diffLoss = 2.8184, kgLoss = 0.0135
2025-04-08 15:07:33.042492: Training Step 91/354: batchLoss = 0.5832, diffLoss = 2.8619, kgLoss = 0.0136
2025-04-08 15:07:34.661769: Training Step 92/354: batchLoss = 0.5401, diffLoss = 2.6503, kgLoss = 0.0126
2025-04-08 15:07:36.278806: Training Step 93/354: batchLoss = 0.5615, diffLoss = 2.7514, kgLoss = 0.0140
2025-04-08 15:07:37.899391: Training Step 94/354: batchLoss = 0.5180, diffLoss = 2.5410, kgLoss = 0.0123
2025-04-08 15:07:39.519505: Training Step 95/354: batchLoss = 0.5269, diffLoss = 2.5785, kgLoss = 0.0140
2025-04-08 15:07:41.148358: Training Step 96/354: batchLoss = 0.5530, diffLoss = 2.7093, kgLoss = 0.0140
2025-04-08 15:07:42.776149: Training Step 97/354: batchLoss = 0.5649, diffLoss = 2.7685, kgLoss = 0.0140
2025-04-08 15:07:44.395647: Training Step 98/354: batchLoss = 0.6003, diffLoss = 2.9358, kgLoss = 0.0164
2025-04-08 15:07:46.015317: Training Step 99/354: batchLoss = 0.5035, diffLoss = 2.4754, kgLoss = 0.0106
2025-04-08 15:07:47.637111: Training Step 100/354: batchLoss = 0.8170, diffLoss = 4.0048, kgLoss = 0.0200
2025-04-08 15:07:49.245141: Training Step 101/354: batchLoss = 0.7019, diffLoss = 3.4412, kgLoss = 0.0171
2025-04-08 15:07:50.862280: Training Step 102/354: batchLoss = 0.5210, diffLoss = 2.5567, kgLoss = 0.0121
2025-04-08 15:07:52.480570: Training Step 103/354: batchLoss = 0.5169, diffLoss = 2.5325, kgLoss = 0.0130
2025-04-08 15:07:54.098546: Training Step 104/354: batchLoss = 0.6830, diffLoss = 3.3503, kgLoss = 0.0162
2025-04-08 15:07:55.726330: Training Step 105/354: batchLoss = 0.5621, diffLoss = 2.7604, kgLoss = 0.0126
2025-04-08 15:07:57.348984: Training Step 106/354: batchLoss = 0.4415, diffLoss = 2.1504, kgLoss = 0.0142
2025-04-08 15:07:58.971983: Training Step 107/354: batchLoss = 0.6306, diffLoss = 3.0953, kgLoss = 0.0145
2025-04-08 15:08:00.600397: Training Step 108/354: batchLoss = 0.6984, diffLoss = 3.4289, kgLoss = 0.0158
2025-04-08 15:08:02.217705: Training Step 109/354: batchLoss = 0.6404, diffLoss = 3.1275, kgLoss = 0.0186
2025-04-08 15:08:03.825024: Training Step 110/354: batchLoss = 0.5923, diffLoss = 2.9016, kgLoss = 0.0150
2025-04-08 15:08:05.439526: Training Step 111/354: batchLoss = 0.4474, diffLoss = 2.1921, kgLoss = 0.0113
2025-04-08 15:08:07.055384: Training Step 112/354: batchLoss = 0.8327, diffLoss = 4.0852, kgLoss = 0.0196
2025-04-08 15:08:08.681668: Training Step 113/354: batchLoss = 0.7125, diffLoss = 3.4961, kgLoss = 0.0166
2025-04-08 15:08:10.303275: Training Step 114/354: batchLoss = 0.7092, diffLoss = 3.4804, kgLoss = 0.0164
2025-04-08 15:08:11.927231: Training Step 115/354: batchLoss = 0.5179, diffLoss = 2.5402, kgLoss = 0.0123
2025-04-08 15:08:13.547300: Training Step 116/354: batchLoss = 0.6030, diffLoss = 2.9623, kgLoss = 0.0132
2025-04-08 15:08:15.172246: Training Step 117/354: batchLoss = 0.6058, diffLoss = 2.9683, kgLoss = 0.0151
2025-04-08 15:08:16.788328: Training Step 118/354: batchLoss = 0.6441, diffLoss = 3.1495, kgLoss = 0.0177
2025-04-08 15:08:18.402702: Training Step 119/354: batchLoss = 0.5254, diffLoss = 2.5762, kgLoss = 0.0127
2025-04-08 15:08:20.019527: Training Step 120/354: batchLoss = 0.5281, diffLoss = 2.5879, kgLoss = 0.0132
2025-04-08 15:08:21.632453: Training Step 121/354: batchLoss = 0.5315, diffLoss = 2.6010, kgLoss = 0.0141
2025-04-08 15:08:23.256358: Training Step 122/354: batchLoss = 0.6225, diffLoss = 3.0318, kgLoss = 0.0201
2025-04-08 15:08:24.876169: Training Step 123/354: batchLoss = 0.5529, diffLoss = 2.7098, kgLoss = 0.0137
2025-04-08 15:08:26.514281: Training Step 124/354: batchLoss = 0.5651, diffLoss = 2.7739, kgLoss = 0.0129
2025-04-08 15:08:28.133899: Training Step 125/354: batchLoss = 0.5535, diffLoss = 2.7141, kgLoss = 0.0134
2025-04-08 15:08:29.759757: Training Step 126/354: batchLoss = 0.6323, diffLoss = 3.0968, kgLoss = 0.0161
2025-04-08 15:08:31.380122: Training Step 127/354: batchLoss = 0.5620, diffLoss = 2.7525, kgLoss = 0.0144
2025-04-08 15:08:32.989207: Training Step 128/354: batchLoss = 0.5315, diffLoss = 2.6043, kgLoss = 0.0133
2025-04-08 15:08:34.602567: Training Step 129/354: batchLoss = 0.5106, diffLoss = 2.4939, kgLoss = 0.0148
2025-04-08 15:08:36.228570: Training Step 130/354: batchLoss = 0.5996, diffLoss = 2.9454, kgLoss = 0.0132
2025-04-08 15:08:37.851134: Training Step 131/354: batchLoss = 0.5411, diffLoss = 2.6449, kgLoss = 0.0152
2025-04-08 15:08:39.476763: Training Step 132/354: batchLoss = 0.5962, diffLoss = 2.9181, kgLoss = 0.0157
2025-04-08 15:08:41.101621: Training Step 133/354: batchLoss = 0.6887, diffLoss = 3.3726, kgLoss = 0.0177
2025-04-08 15:08:42.724469: Training Step 134/354: batchLoss = 0.4822, diffLoss = 2.3553, kgLoss = 0.0140
2025-04-08 15:08:44.348079: Training Step 135/354: batchLoss = 0.6246, diffLoss = 3.0616, kgLoss = 0.0154
2025-04-08 15:08:45.965644: Training Step 136/354: batchLoss = 0.5960, diffLoss = 2.9188, kgLoss = 0.0153
2025-04-08 15:08:47.582615: Training Step 137/354: batchLoss = 0.5489, diffLoss = 2.6889, kgLoss = 0.0139
2025-04-08 15:08:49.196124: Training Step 138/354: batchLoss = 0.6735, diffLoss = 3.3050, kgLoss = 0.0156
2025-04-08 15:08:50.809418: Training Step 139/354: batchLoss = 0.6056, diffLoss = 2.9742, kgLoss = 0.0135
2025-04-08 15:08:52.429271: Training Step 140/354: batchLoss = 0.5418, diffLoss = 2.6557, kgLoss = 0.0133
2025-04-08 15:08:54.051076: Training Step 141/354: batchLoss = 0.7118, diffLoss = 3.4858, kgLoss = 0.0183
2025-04-08 15:08:55.668942: Training Step 142/354: batchLoss = 0.8169, diffLoss = 4.0047, kgLoss = 0.0200
2025-04-08 15:08:57.289969: Training Step 143/354: batchLoss = 0.4699, diffLoss = 2.3015, kgLoss = 0.0120
2025-04-08 15:08:58.910697: Training Step 144/354: batchLoss = 0.6442, diffLoss = 3.1455, kgLoss = 0.0189
2025-04-08 15:09:00.521767: Training Step 145/354: batchLoss = 0.5973, diffLoss = 2.9273, kgLoss = 0.0148
2025-04-08 15:09:02.132410: Training Step 146/354: batchLoss = 0.5468, diffLoss = 2.6755, kgLoss = 0.0146
2025-04-08 15:09:03.750743: Training Step 147/354: batchLoss = 1.0752, diffLoss = 5.2759, kgLoss = 0.0251
2025-04-08 15:09:05.365959: Training Step 148/354: batchLoss = 0.4990, diffLoss = 2.4477, kgLoss = 0.0118
2025-04-08 15:09:06.989462: Training Step 149/354: batchLoss = 0.6665, diffLoss = 3.2685, kgLoss = 0.0160
2025-04-08 15:09:08.612520: Training Step 150/354: batchLoss = 0.6276, diffLoss = 3.0763, kgLoss = 0.0154
2025-04-08 15:09:10.235363: Training Step 151/354: batchLoss = 0.5032, diffLoss = 2.4669, kgLoss = 0.0123
2025-04-08 15:09:11.860111: Training Step 152/354: batchLoss = 0.5153, diffLoss = 2.5291, kgLoss = 0.0118
2025-04-08 15:09:13.485735: Training Step 153/354: batchLoss = 0.5319, diffLoss = 2.6020, kgLoss = 0.0144
2025-04-08 15:09:15.130405: Training Step 154/354: batchLoss = 0.5605, diffLoss = 2.7458, kgLoss = 0.0142
2025-04-08 15:09:16.761441: Training Step 155/354: batchLoss = 0.6014, diffLoss = 2.9417, kgLoss = 0.0163
2025-04-08 15:09:18.398297: Training Step 156/354: batchLoss = 0.5746, diffLoss = 2.8110, kgLoss = 0.0155
2025-04-08 15:09:20.034660: Training Step 157/354: batchLoss = 0.7115, diffLoss = 3.4924, kgLoss = 0.0163
2025-04-08 15:09:21.662417: Training Step 158/354: batchLoss = 0.5489, diffLoss = 2.6905, kgLoss = 0.0135
2025-04-08 15:09:23.294806: Training Step 159/354: batchLoss = 0.5335, diffLoss = 2.6126, kgLoss = 0.0138
2025-04-08 15:09:24.929873: Training Step 160/354: batchLoss = 0.5444, diffLoss = 2.6678, kgLoss = 0.0136
2025-04-08 15:09:26.572199: Training Step 161/354: batchLoss = 0.5561, diffLoss = 2.7172, kgLoss = 0.0158
2025-04-08 15:09:28.190072: Training Step 162/354: batchLoss = 0.6042, diffLoss = 2.9639, kgLoss = 0.0142
2025-04-08 15:09:29.817774: Training Step 163/354: batchLoss = 0.5340, diffLoss = 2.6087, kgLoss = 0.0153
2025-04-08 15:09:31.433010: Training Step 164/354: batchLoss = 0.5054, diffLoss = 2.4829, kgLoss = 0.0111
2025-04-08 15:09:33.061213: Training Step 165/354: batchLoss = 0.4605, diffLoss = 2.2602, kgLoss = 0.0106
2025-04-08 15:09:34.686404: Training Step 166/354: batchLoss = 0.4937, diffLoss = 2.4202, kgLoss = 0.0121
2025-04-08 15:09:36.295318: Training Step 167/354: batchLoss = 0.6734, diffLoss = 3.3054, kgLoss = 0.0154
2025-04-08 15:09:37.923169: Training Step 168/354: batchLoss = 0.6422, diffLoss = 3.1464, kgLoss = 0.0161
2025-04-08 15:09:39.549287: Training Step 169/354: batchLoss = 0.6318, diffLoss = 3.0994, kgLoss = 0.0149
2025-04-08 15:09:41.174568: Training Step 170/354: batchLoss = 0.5898, diffLoss = 2.8961, kgLoss = 0.0133
2025-04-08 15:09:42.797386: Training Step 171/354: batchLoss = 0.5584, diffLoss = 2.7322, kgLoss = 0.0150
2025-04-08 15:09:44.419443: Training Step 172/354: batchLoss = 0.6515, diffLoss = 3.1922, kgLoss = 0.0163
2025-04-08 15:09:46.030691: Training Step 173/354: batchLoss = 0.5192, diffLoss = 2.5433, kgLoss = 0.0132
2025-04-08 15:09:47.644834: Training Step 174/354: batchLoss = 0.5722, diffLoss = 2.7997, kgLoss = 0.0153
2025-04-08 15:09:49.260801: Training Step 175/354: batchLoss = 0.7358, diffLoss = 3.6148, kgLoss = 0.0161
2025-04-08 15:09:50.883289: Training Step 176/354: batchLoss = 0.6447, diffLoss = 3.1658, kgLoss = 0.0144
2025-04-08 15:09:52.507561: Training Step 177/354: batchLoss = 0.5689, diffLoss = 2.7818, kgLoss = 0.0156
2025-04-08 15:09:54.130886: Training Step 178/354: batchLoss = 0.5791, diffLoss = 2.8343, kgLoss = 0.0153
2025-04-08 15:09:55.749497: Training Step 179/354: batchLoss = 0.5140, diffLoss = 2.5161, kgLoss = 0.0135
2025-04-08 15:09:57.370055: Training Step 180/354: batchLoss = 0.5184, diffLoss = 2.5385, kgLoss = 0.0134
2025-04-08 15:09:58.984463: Training Step 181/354: batchLoss = 0.6436, diffLoss = 3.1555, kgLoss = 0.0157
2025-04-08 15:10:00.596678: Training Step 182/354: batchLoss = 0.5738, diffLoss = 2.8167, kgLoss = 0.0131
2025-04-08 15:10:02.204349: Training Step 183/354: batchLoss = 0.5724, diffLoss = 2.8054, kgLoss = 0.0141
2025-04-08 15:10:03.818755: Training Step 184/354: batchLoss = 0.8497, diffLoss = 4.1641, kgLoss = 0.0211
2025-04-08 15:10:05.442266: Training Step 185/354: batchLoss = 0.5734, diffLoss = 2.8139, kgLoss = 0.0133
2025-04-08 15:10:07.061665: Training Step 186/354: batchLoss = 0.6018, diffLoss = 2.9534, kgLoss = 0.0139
2025-04-08 15:10:08.679881: Training Step 187/354: batchLoss = 0.5775, diffLoss = 2.8289, kgLoss = 0.0146
2025-04-08 15:10:10.298823: Training Step 188/354: batchLoss = 0.5421, diffLoss = 2.6591, kgLoss = 0.0129
2025-04-08 15:10:11.929629: Training Step 189/354: batchLoss = 0.5597, diffLoss = 2.7445, kgLoss = 0.0135
2025-04-08 15:10:13.554742: Training Step 190/354: batchLoss = 0.6052, diffLoss = 2.9688, kgLoss = 0.0144
2025-04-08 15:10:15.170675: Training Step 191/354: batchLoss = 0.5510, diffLoss = 2.6889, kgLoss = 0.0165
2025-04-08 15:10:16.787854: Training Step 192/354: batchLoss = 0.6219, diffLoss = 3.0515, kgLoss = 0.0145
2025-04-08 15:10:18.410888: Training Step 193/354: batchLoss = 0.5260, diffLoss = 2.5803, kgLoss = 0.0124
2025-04-08 15:10:20.031166: Training Step 194/354: batchLoss = 0.5881, diffLoss = 2.8825, kgLoss = 0.0144
2025-04-08 15:10:21.653075: Training Step 195/354: batchLoss = 0.5859, diffLoss = 2.8720, kgLoss = 0.0144
2025-04-08 15:10:23.276436: Training Step 196/354: batchLoss = 0.7894, diffLoss = 3.8761, kgLoss = 0.0177
2025-04-08 15:10:24.892902: Training Step 197/354: batchLoss = 0.5930, diffLoss = 2.9033, kgLoss = 0.0154
2025-04-08 15:10:26.510246: Training Step 198/354: batchLoss = 0.5392, diffLoss = 2.6422, kgLoss = 0.0134
2025-04-08 15:10:28.128417: Training Step 199/354: batchLoss = 0.6473, diffLoss = 3.1711, kgLoss = 0.0164
2025-04-08 15:10:29.746240: Training Step 200/354: batchLoss = 0.4518, diffLoss = 2.2116, kgLoss = 0.0119
2025-04-08 15:10:31.361139: Training Step 201/354: batchLoss = 0.5839, diffLoss = 2.8552, kgLoss = 0.0161
2025-04-08 15:10:32.974228: Training Step 202/354: batchLoss = 0.5375, diffLoss = 2.6365, kgLoss = 0.0128
2025-04-08 15:10:34.586365: Training Step 203/354: batchLoss = 0.5674, diffLoss = 2.7763, kgLoss = 0.0152
2025-04-08 15:10:36.207207: Training Step 204/354: batchLoss = 0.5454, diffLoss = 2.6726, kgLoss = 0.0136
2025-04-08 15:10:37.817779: Training Step 205/354: batchLoss = 0.5881, diffLoss = 2.8822, kgLoss = 0.0146
2025-04-08 15:10:39.433422: Training Step 206/354: batchLoss = 0.5331, diffLoss = 2.6088, kgLoss = 0.0142
2025-04-08 15:10:41.057472: Training Step 207/354: batchLoss = 0.5771, diffLoss = 2.8292, kgLoss = 0.0141
2025-04-08 15:10:42.675499: Training Step 208/354: batchLoss = 0.5105, diffLoss = 2.4999, kgLoss = 0.0131
2025-04-08 15:10:44.297410: Training Step 209/354: batchLoss = 0.6890, diffLoss = 3.3815, kgLoss = 0.0159
2025-04-08 15:10:45.912599: Training Step 210/354: batchLoss = 0.5001, diffLoss = 2.4522, kgLoss = 0.0120
2025-04-08 15:10:47.539603: Training Step 211/354: batchLoss = 0.4933, diffLoss = 2.4141, kgLoss = 0.0131
2025-04-08 15:10:49.154607: Training Step 212/354: batchLoss = 0.5375, diffLoss = 2.6174, kgLoss = 0.0176
2025-04-08 15:10:50.772974: Training Step 213/354: batchLoss = 0.6397, diffLoss = 3.1366, kgLoss = 0.0154
2025-04-08 15:10:52.378474: Training Step 214/354: batchLoss = 0.6089, diffLoss = 2.9790, kgLoss = 0.0164
2025-04-08 15:10:53.997363: Training Step 215/354: batchLoss = 0.5363, diffLoss = 2.6297, kgLoss = 0.0129
2025-04-08 15:10:55.621478: Training Step 216/354: batchLoss = 0.5420, diffLoss = 2.6517, kgLoss = 0.0145
2025-04-08 15:10:57.247359: Training Step 217/354: batchLoss = 0.5792, diffLoss = 2.8255, kgLoss = 0.0176
2025-04-08 15:10:58.866527: Training Step 218/354: batchLoss = 0.5737, diffLoss = 2.8218, kgLoss = 0.0116
2025-04-08 15:11:00.478396: Training Step 219/354: batchLoss = 0.5197, diffLoss = 2.5429, kgLoss = 0.0139
2025-04-08 15:11:02.100960: Training Step 220/354: batchLoss = 0.7647, diffLoss = 3.7549, kgLoss = 0.0171
2025-04-08 15:11:03.719445: Training Step 221/354: batchLoss = 0.5460, diffLoss = 2.6772, kgLoss = 0.0132
2025-04-08 15:11:05.346636: Training Step 222/354: batchLoss = 0.5611, diffLoss = 2.7446, kgLoss = 0.0152
2025-04-08 15:11:06.960625: Training Step 223/354: batchLoss = 0.5927, diffLoss = 2.9057, kgLoss = 0.0144
2025-04-08 15:11:08.579067: Training Step 224/354: batchLoss = 0.6811, diffLoss = 3.3353, kgLoss = 0.0176
2025-04-08 15:11:10.196613: Training Step 225/354: batchLoss = 0.5650, diffLoss = 2.7714, kgLoss = 0.0134
2025-04-08 15:11:11.815614: Training Step 226/354: batchLoss = 0.5790, diffLoss = 2.8440, kgLoss = 0.0127
2025-04-08 15:11:13.430380: Training Step 227/354: batchLoss = 0.4724, diffLoss = 2.3164, kgLoss = 0.0114
2025-04-08 15:11:15.043038: Training Step 228/354: batchLoss = 0.6190, diffLoss = 3.0359, kgLoss = 0.0148
2025-04-08 15:11:16.660004: Training Step 229/354: batchLoss = 0.6851, diffLoss = 3.3625, kgLoss = 0.0157
2025-04-08 15:11:18.275008: Training Step 230/354: batchLoss = 0.5792, diffLoss = 2.8436, kgLoss = 0.0131
2025-04-08 15:11:19.892635: Training Step 231/354: batchLoss = 0.5882, diffLoss = 2.8822, kgLoss = 0.0147
2025-04-08 15:11:21.511572: Training Step 232/354: batchLoss = 0.4831, diffLoss = 2.3681, kgLoss = 0.0119
2025-04-08 15:11:23.123218: Training Step 233/354: batchLoss = 0.6862, diffLoss = 3.3704, kgLoss = 0.0151
2025-04-08 15:11:24.745325: Training Step 234/354: batchLoss = 0.5903, diffLoss = 2.8974, kgLoss = 0.0135
2025-04-08 15:11:26.362939: Training Step 235/354: batchLoss = 0.5874, diffLoss = 2.8819, kgLoss = 0.0137
2025-04-08 15:11:27.992268: Training Step 236/354: batchLoss = 0.5569, diffLoss = 2.7206, kgLoss = 0.0159
2025-04-08 15:11:29.598497: Training Step 237/354: batchLoss = 0.5895, diffLoss = 2.8827, kgLoss = 0.0162
2025-04-08 15:11:31.213547: Training Step 238/354: batchLoss = 0.5833, diffLoss = 2.8622, kgLoss = 0.0136
2025-04-08 15:11:32.823719: Training Step 239/354: batchLoss = 0.5306, diffLoss = 2.5941, kgLoss = 0.0147
2025-04-08 15:11:34.438850: Training Step 240/354: batchLoss = 0.6862, diffLoss = 3.3696, kgLoss = 0.0153
2025-04-08 15:11:36.061460: Training Step 241/354: batchLoss = 0.5080, diffLoss = 2.4939, kgLoss = 0.0115
2025-04-08 15:11:37.679124: Training Step 242/354: batchLoss = 0.6090, diffLoss = 2.9967, kgLoss = 0.0121
2025-04-08 15:11:39.290909: Training Step 243/354: batchLoss = 0.5550, diffLoss = 2.7244, kgLoss = 0.0126
2025-04-08 15:11:40.906628: Training Step 244/354: batchLoss = 0.5426, diffLoss = 2.6597, kgLoss = 0.0133
2025-04-08 15:11:42.516650: Training Step 245/354: batchLoss = 0.5650, diffLoss = 2.7658, kgLoss = 0.0147
2025-04-08 15:11:44.139123: Training Step 246/354: batchLoss = 0.5370, diffLoss = 2.6331, kgLoss = 0.0129
2025-04-08 15:11:45.748424: Training Step 247/354: batchLoss = 0.5134, diffLoss = 2.5160, kgLoss = 0.0128
2025-04-08 15:11:47.361440: Training Step 248/354: batchLoss = 0.7342, diffLoss = 3.6102, kgLoss = 0.0152
2025-04-08 15:11:48.980270: Training Step 249/354: batchLoss = 0.5420, diffLoss = 2.6535, kgLoss = 0.0141
2025-04-08 15:11:50.603843: Training Step 250/354: batchLoss = 0.5691, diffLoss = 2.7864, kgLoss = 0.0147
2025-04-08 15:11:52.220976: Training Step 251/354: batchLoss = 0.4925, diffLoss = 2.4120, kgLoss = 0.0127
2025-04-08 15:11:53.839252: Training Step 252/354: batchLoss = 0.4988, diffLoss = 2.4366, kgLoss = 0.0143
2025-04-08 15:11:55.456441: Training Step 253/354: batchLoss = 0.6070, diffLoss = 2.9759, kgLoss = 0.0148
2025-04-08 15:11:57.072958: Training Step 254/354: batchLoss = 0.8330, diffLoss = 4.0866, kgLoss = 0.0196
2025-04-08 15:11:58.692119: Training Step 255/354: batchLoss = 0.5954, diffLoss = 2.9213, kgLoss = 0.0139
2025-04-08 15:12:00.302098: Training Step 256/354: batchLoss = 0.5641, diffLoss = 2.7659, kgLoss = 0.0137
2025-04-08 15:12:01.920491: Training Step 257/354: batchLoss = 0.6168, diffLoss = 3.0269, kgLoss = 0.0143
2025-04-08 15:12:03.536193: Training Step 258/354: batchLoss = 0.5260, diffLoss = 2.5728, kgLoss = 0.0143
2025-04-08 15:12:05.149100: Training Step 259/354: batchLoss = 0.6103, diffLoss = 2.9878, kgLoss = 0.0159
2025-04-08 15:12:06.765045: Training Step 260/354: batchLoss = 0.5470, diffLoss = 2.6880, kgLoss = 0.0117
2025-04-08 15:12:08.383025: Training Step 261/354: batchLoss = 0.5490, diffLoss = 2.6833, kgLoss = 0.0154
2025-04-08 15:12:10.001494: Training Step 262/354: batchLoss = 0.5682, diffLoss = 2.7815, kgLoss = 0.0149
2025-04-08 15:12:11.615314: Training Step 263/354: batchLoss = 0.6539, diffLoss = 3.2111, kgLoss = 0.0146
2025-04-08 15:12:13.228664: Training Step 264/354: batchLoss = 0.5635, diffLoss = 2.7630, kgLoss = 0.0136
2025-04-08 15:12:14.838219: Training Step 265/354: batchLoss = 0.5904, diffLoss = 2.8833, kgLoss = 0.0172
2025-04-08 15:12:16.449470: Training Step 266/354: batchLoss = 0.5923, diffLoss = 2.9022, kgLoss = 0.0148
2025-04-08 15:12:18.062072: Training Step 267/354: batchLoss = 0.6048, diffLoss = 2.9628, kgLoss = 0.0153
2025-04-08 15:12:19.685144: Training Step 268/354: batchLoss = 0.5405, diffLoss = 2.6475, kgLoss = 0.0137
2025-04-08 15:12:21.294807: Training Step 269/354: batchLoss = 0.6791, diffLoss = 3.3348, kgLoss = 0.0152
2025-04-08 15:12:22.913025: Training Step 270/354: batchLoss = 0.5953, diffLoss = 2.9098, kgLoss = 0.0166
2025-04-08 15:12:24.528593: Training Step 271/354: batchLoss = 0.5413, diffLoss = 2.6557, kgLoss = 0.0127
2025-04-08 15:12:26.151999: Training Step 272/354: batchLoss = 0.6002, diffLoss = 2.9459, kgLoss = 0.0137
2025-04-08 15:12:27.768600: Training Step 273/354: batchLoss = 0.6409, diffLoss = 3.1461, kgLoss = 0.0146
2025-04-08 15:12:29.385552: Training Step 274/354: batchLoss = 0.6054, diffLoss = 2.9691, kgLoss = 0.0145
2025-04-08 15:12:31.009464: Training Step 275/354: batchLoss = 0.5265, diffLoss = 2.5799, kgLoss = 0.0132
2025-04-08 15:12:32.641845: Training Step 276/354: batchLoss = 0.5771, diffLoss = 2.8242, kgLoss = 0.0154
2025-04-08 15:12:34.286043: Training Step 277/354: batchLoss = 0.5338, diffLoss = 2.6196, kgLoss = 0.0123
2025-04-08 15:12:35.906544: Training Step 278/354: batchLoss = 0.4985, diffLoss = 2.4449, kgLoss = 0.0119
2025-04-08 15:12:37.528756: Training Step 279/354: batchLoss = 0.5071, diffLoss = 2.4724, kgLoss = 0.0157
2025-04-08 15:12:39.153778: Training Step 280/354: batchLoss = 0.6349, diffLoss = 3.1160, kgLoss = 0.0146
2025-04-08 15:12:40.801828: Training Step 281/354: batchLoss = 0.5944, diffLoss = 2.9160, kgLoss = 0.0139
2025-04-08 15:12:42.416130: Training Step 282/354: batchLoss = 0.4400, diffLoss = 2.1523, kgLoss = 0.0120
2025-04-08 15:12:44.024762: Training Step 283/354: batchLoss = 0.6761, diffLoss = 3.3212, kgLoss = 0.0149
2025-04-08 15:12:45.642575: Training Step 284/354: batchLoss = 0.8308, diffLoss = 4.0757, kgLoss = 0.0196
2025-04-08 15:12:47.258887: Training Step 285/354: batchLoss = 0.4983, diffLoss = 2.4339, kgLoss = 0.0144
2025-04-08 15:12:48.880977: Training Step 286/354: batchLoss = 0.5455, diffLoss = 2.6767, kgLoss = 0.0127
2025-04-08 15:12:50.497146: Training Step 287/354: batchLoss = 0.5780, diffLoss = 2.8308, kgLoss = 0.0147
2025-04-08 15:12:52.111108: Training Step 288/354: batchLoss = 0.5385, diffLoss = 2.6338, kgLoss = 0.0147
2025-04-08 15:12:53.738958: Training Step 289/354: batchLoss = 0.5494, diffLoss = 2.6933, kgLoss = 0.0134
2025-04-08 15:12:55.352792: Training Step 290/354: batchLoss = 0.7181, diffLoss = 3.5234, kgLoss = 0.0168
2025-04-08 15:12:56.980876: Training Step 291/354: batchLoss = 0.6808, diffLoss = 3.3356, kgLoss = 0.0171
2025-04-08 15:12:58.594056: Training Step 292/354: batchLoss = 0.6429, diffLoss = 3.1502, kgLoss = 0.0160
2025-04-08 15:13:00.207750: Training Step 293/354: batchLoss = 0.5603, diffLoss = 2.7440, kgLoss = 0.0144
2025-04-08 15:13:01.828752: Training Step 294/354: batchLoss = 0.6476, diffLoss = 3.1802, kgLoss = 0.0144
2025-04-08 15:13:03.445423: Training Step 295/354: batchLoss = 0.5463, diffLoss = 2.6790, kgLoss = 0.0132
2025-04-08 15:13:05.064131: Training Step 296/354: batchLoss = 0.8416, diffLoss = 4.1266, kgLoss = 0.0203
2025-04-08 15:13:06.678593: Training Step 297/354: batchLoss = 0.5388, diffLoss = 2.6440, kgLoss = 0.0125
2025-04-08 15:13:08.300244: Training Step 298/354: batchLoss = 0.5436, diffLoss = 2.6655, kgLoss = 0.0132
2025-04-08 15:13:09.925861: Training Step 299/354: batchLoss = 0.6296, diffLoss = 3.0900, kgLoss = 0.0145
2025-04-08 15:13:11.543865: Training Step 300/354: batchLoss = 0.6195, diffLoss = 3.0386, kgLoss = 0.0148
2025-04-08 15:13:13.158712: Training Step 301/354: batchLoss = 0.6139, diffLoss = 3.0101, kgLoss = 0.0148
2025-04-08 15:13:14.775950: Training Step 302/354: batchLoss = 0.6388, diffLoss = 3.1261, kgLoss = 0.0170
2025-04-08 15:13:16.397054: Training Step 303/354: batchLoss = 0.4898, diffLoss = 2.4052, kgLoss = 0.0110
2025-04-08 15:13:18.018261: Training Step 304/354: batchLoss = 0.5451, diffLoss = 2.6713, kgLoss = 0.0136
2025-04-08 15:13:19.640212: Training Step 305/354: batchLoss = 0.6800, diffLoss = 3.3393, kgLoss = 0.0152
2025-04-08 15:13:21.254496: Training Step 306/354: batchLoss = 0.4695, diffLoss = 2.2998, kgLoss = 0.0120
2025-04-08 15:13:22.865365: Training Step 307/354: batchLoss = 1.0457, diffLoss = 5.1362, kgLoss = 0.0231
2025-04-08 15:13:24.494332: Training Step 308/354: batchLoss = 0.7117, diffLoss = 3.4951, kgLoss = 0.0159
2025-04-08 15:13:26.109145: Training Step 309/354: batchLoss = 0.5311, diffLoss = 2.6057, kgLoss = 0.0124
2025-04-08 15:13:27.727008: Training Step 310/354: batchLoss = 0.5228, diffLoss = 2.5680, kgLoss = 0.0115
2025-04-08 15:13:29.342770: Training Step 311/354: batchLoss = 0.4981, diffLoss = 2.4430, kgLoss = 0.0118
2025-04-08 15:13:30.960100: Training Step 312/354: batchLoss = 0.6420, diffLoss = 3.1470, kgLoss = 0.0158
2025-04-08 15:13:32.578007: Training Step 313/354: batchLoss = 0.6344, diffLoss = 3.1113, kgLoss = 0.0152
2025-04-08 15:13:34.197801: Training Step 314/354: batchLoss = 0.6203, diffLoss = 3.0391, kgLoss = 0.0156
2025-04-08 15:13:35.811178: Training Step 315/354: batchLoss = 0.4918, diffLoss = 2.4081, kgLoss = 0.0127
2025-04-08 15:13:37.430188: Training Step 316/354: batchLoss = 0.5568, diffLoss = 2.7295, kgLoss = 0.0136
2025-04-08 15:13:39.048693: Training Step 317/354: batchLoss = 0.5673, diffLoss = 2.7824, kgLoss = 0.0135
2025-04-08 15:13:40.661067: Training Step 318/354: batchLoss = 0.6860, diffLoss = 3.3676, kgLoss = 0.0156
2025-04-08 15:13:42.266020: Training Step 319/354: batchLoss = 0.6121, diffLoss = 3.0023, kgLoss = 0.0145
2025-04-08 15:13:43.881815: Training Step 320/354: batchLoss = 0.5804, diffLoss = 2.8338, kgLoss = 0.0170
2025-04-08 15:13:45.497988: Training Step 321/354: batchLoss = 0.5841, diffLoss = 2.8644, kgLoss = 0.0140
2025-04-08 15:13:47.118284: Training Step 322/354: batchLoss = 0.5945, diffLoss = 2.9201, kgLoss = 0.0131
2025-04-08 15:13:48.727150: Training Step 323/354: batchLoss = 0.6216, diffLoss = 3.0498, kgLoss = 0.0145
2025-04-08 15:13:50.345649: Training Step 324/354: batchLoss = 0.7953, diffLoss = 3.9033, kgLoss = 0.0183
2025-04-08 15:13:51.962509: Training Step 325/354: batchLoss = 0.5966, diffLoss = 2.9223, kgLoss = 0.0152
2025-04-08 15:13:53.583782: Training Step 326/354: batchLoss = 0.4839, diffLoss = 2.3730, kgLoss = 0.0116
2025-04-08 15:13:55.199480: Training Step 327/354: batchLoss = 0.5199, diffLoss = 2.5496, kgLoss = 0.0124
2025-04-08 15:13:56.814190: Training Step 328/354: batchLoss = 0.5137, diffLoss = 2.5116, kgLoss = 0.0142
2025-04-08 15:13:58.435397: Training Step 329/354: batchLoss = 0.5964, diffLoss = 2.9257, kgLoss = 0.0141
2025-04-08 15:14:00.048318: Training Step 330/354: batchLoss = 0.5371, diffLoss = 2.6357, kgLoss = 0.0125
2025-04-08 15:14:01.671007: Training Step 331/354: batchLoss = 0.5036, diffLoss = 2.4629, kgLoss = 0.0138
2025-04-08 15:14:03.287160: Training Step 332/354: batchLoss = 0.7233, diffLoss = 3.5473, kgLoss = 0.0173
2025-04-08 15:14:04.915041: Training Step 333/354: batchLoss = 0.6212, diffLoss = 3.0465, kgLoss = 0.0149
2025-04-08 15:14:06.542276: Training Step 334/354: batchLoss = 0.5530, diffLoss = 2.7025, kgLoss = 0.0156
2025-04-08 15:14:08.163976: Training Step 335/354: batchLoss = 0.5923, diffLoss = 2.9095, kgLoss = 0.0130
2025-04-08 15:14:09.784580: Training Step 336/354: batchLoss = 0.4637, diffLoss = 2.2663, kgLoss = 0.0130
2025-04-08 15:14:11.396554: Training Step 337/354: batchLoss = 0.4178, diffLoss = 2.0440, kgLoss = 0.0112
2025-04-08 15:14:13.022246: Training Step 338/354: batchLoss = 0.5492, diffLoss = 2.6960, kgLoss = 0.0125
2025-04-08 15:14:14.642208: Training Step 339/354: batchLoss = 0.6423, diffLoss = 3.1591, kgLoss = 0.0131
2025-04-08 15:14:16.265909: Training Step 340/354: batchLoss = 0.6813, diffLoss = 3.3375, kgLoss = 0.0172
2025-04-08 15:14:17.886583: Training Step 341/354: batchLoss = 0.7149, diffLoss = 3.5071, kgLoss = 0.0169
2025-04-08 15:14:19.513813: Training Step 342/354: batchLoss = 0.6120, diffLoss = 2.9962, kgLoss = 0.0159
2025-04-08 15:14:21.138590: Training Step 343/354: batchLoss = 0.5609, diffLoss = 2.7462, kgLoss = 0.0146
2025-04-08 15:14:22.759548: Training Step 344/354: batchLoss = 0.5992, diffLoss = 2.9412, kgLoss = 0.0137
2025-04-08 15:14:24.377865: Training Step 345/354: batchLoss = 0.6556, diffLoss = 3.2113, kgLoss = 0.0167
2025-04-08 15:14:26.002594: Training Step 346/354: batchLoss = 0.7464, diffLoss = 3.6640, kgLoss = 0.0170
2025-04-08 15:14:27.622620: Training Step 347/354: batchLoss = 0.5274, diffLoss = 2.5809, kgLoss = 0.0140
2025-04-08 15:14:29.242499: Training Step 348/354: batchLoss = 0.5912, diffLoss = 2.8998, kgLoss = 0.0140
2025-04-08 15:14:30.872691: Training Step 349/354: batchLoss = 0.6936, diffLoss = 3.4048, kgLoss = 0.0158
2025-04-08 15:14:32.500031: Training Step 350/354: batchLoss = 0.6602, diffLoss = 3.2303, kgLoss = 0.0176
2025-04-08 15:14:34.117512: Training Step 351/354: batchLoss = 0.6212, diffLoss = 3.0412, kgLoss = 0.0162
2025-04-08 15:14:35.714025: Training Step 352/354: batchLoss = 0.6422, diffLoss = 3.1483, kgLoss = 0.0157
2025-04-08 15:14:37.118218: Training Step 353/354: batchLoss = 0.6397, diffLoss = 3.1355, kgLoss = 0.0158
2025-04-08 15:14:37.212015: 
2025-04-08 15:14:37.212636: Epoch 12/1000, Train: epLoss = 1.0478, epDfLoss = 5.1348, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 15:14:38.524784: Steps 0/138: batch_recall = 46.92, batch_ndcg = 26.52 
2025-04-08 15:14:39.848587: Steps 1/138: batch_recall = 46.33, batch_ndcg = 28.02 
2025-04-08 15:14:41.152205: Steps 2/138: batch_recall = 58.41, batch_ndcg = 36.89 
2025-04-08 15:14:42.480494: Steps 3/138: batch_recall = 59.80, batch_ndcg = 34.42 
2025-04-08 15:14:43.786902: Steps 4/138: batch_recall = 67.52, batch_ndcg = 40.31 
2025-04-08 15:14:45.099279: Steps 5/138: batch_recall = 60.27, batch_ndcg = 32.61 
2025-04-08 15:14:46.401203: Steps 6/138: batch_recall = 50.19, batch_ndcg = 29.59 
2025-04-08 15:14:47.699664: Steps 7/138: batch_recall = 63.37, batch_ndcg = 40.35 
2025-04-08 15:14:48.996063: Steps 8/138: batch_recall = 61.50, batch_ndcg = 39.14 
2025-04-08 15:14:50.293565: Steps 9/138: batch_recall = 54.92, batch_ndcg = 34.13 
2025-04-08 15:14:51.583974: Steps 10/138: batch_recall = 53.28, batch_ndcg = 29.92 
2025-04-08 15:14:52.894013: Steps 11/138: batch_recall = 58.53, batch_ndcg = 33.75 
2025-04-08 15:14:54.187673: Steps 12/138: batch_recall = 54.61, batch_ndcg = 29.53 
2025-04-08 15:14:55.467311: Steps 13/138: batch_recall = 51.39, batch_ndcg = 29.88 
2025-04-08 15:14:56.760710: Steps 14/138: batch_recall = 51.78, batch_ndcg = 30.34 
2025-04-08 15:14:58.054907: Steps 15/138: batch_recall = 47.67, batch_ndcg = 28.97 
2025-04-08 15:14:59.361262: Steps 16/138: batch_recall = 60.08, batch_ndcg = 32.94 
2025-04-08 15:15:00.653142: Steps 17/138: batch_recall = 55.50, batch_ndcg = 31.13 
2025-04-08 15:15:01.953237: Steps 18/138: batch_recall = 50.90, batch_ndcg = 32.54 
2025-04-08 15:15:03.253684: Steps 19/138: batch_recall = 54.54, batch_ndcg = 32.20 
2025-04-08 15:15:04.554273: Steps 20/138: batch_recall = 60.59, batch_ndcg = 35.47 
2025-04-08 15:15:05.859544: Steps 21/138: batch_recall = 63.78, batch_ndcg = 38.64 
2025-04-08 15:15:07.150328: Steps 22/138: batch_recall = 55.74, batch_ndcg = 32.64 
2025-04-08 15:15:08.442586: Steps 23/138: batch_recall = 52.15, batch_ndcg = 30.00 
2025-04-08 15:15:09.740852: Steps 24/138: batch_recall = 55.20, batch_ndcg = 30.62 
2025-04-08 15:15:11.031364: Steps 25/138: batch_recall = 59.56, batch_ndcg = 34.28 
2025-04-08 15:15:12.330494: Steps 26/138: batch_recall = 56.53, batch_ndcg = 33.11 
2025-04-08 15:15:13.620741: Steps 27/138: batch_recall = 61.67, batch_ndcg = 34.81 
2025-04-08 15:15:14.905858: Steps 28/138: batch_recall = 57.26, batch_ndcg = 32.97 
2025-04-08 15:15:16.198671: Steps 29/138: batch_recall = 59.03, batch_ndcg = 30.86 
2025-04-08 15:15:17.497767: Steps 30/138: batch_recall = 56.56, batch_ndcg = 33.52 
2025-04-08 15:15:18.781738: Steps 31/138: batch_recall = 41.19, batch_ndcg = 23.82 
2025-04-08 15:15:20.063029: Steps 32/138: batch_recall = 50.68, batch_ndcg = 30.85 
2025-04-08 15:15:21.347376: Steps 33/138: batch_recall = 61.02, batch_ndcg = 33.96 
2025-04-08 15:15:22.628511: Steps 34/138: batch_recall = 53.41, batch_ndcg = 28.72 
2025-04-08 15:15:23.909431: Steps 35/138: batch_recall = 49.88, batch_ndcg = 29.59 
2025-04-08 15:15:25.200443: Steps 36/138: batch_recall = 51.64, batch_ndcg = 28.27 
2025-04-08 15:15:26.470337: Steps 37/138: batch_recall = 59.96, batch_ndcg = 34.63 
2025-04-08 15:15:27.747305: Steps 38/138: batch_recall = 60.06, batch_ndcg = 32.43 
2025-04-08 15:15:29.025055: Steps 39/138: batch_recall = 68.58, batch_ndcg = 37.16 
2025-04-08 15:15:30.305166: Steps 40/138: batch_recall = 53.55, batch_ndcg = 28.92 
2025-04-08 15:15:31.586595: Steps 41/138: batch_recall = 59.87, batch_ndcg = 32.84 
2025-04-08 15:15:32.868114: Steps 42/138: batch_recall = 54.92, batch_ndcg = 30.12 
2025-04-08 15:15:34.141916: Steps 43/138: batch_recall = 54.53, batch_ndcg = 32.28 
2025-04-08 15:15:35.423279: Steps 44/138: batch_recall = 55.10, batch_ndcg = 28.25 
2025-04-08 15:15:36.696432: Steps 45/138: batch_recall = 59.41, batch_ndcg = 33.25 
2025-04-08 15:15:37.960928: Steps 46/138: batch_recall = 60.74, batch_ndcg = 34.45 
2025-04-08 15:15:39.234438: Steps 47/138: batch_recall = 54.41, batch_ndcg = 32.02 
2025-04-08 15:15:40.511545: Steps 48/138: batch_recall = 54.08, batch_ndcg = 32.12 
2025-04-08 15:15:41.780806: Steps 49/138: batch_recall = 62.69, batch_ndcg = 36.46 
2025-04-08 15:15:43.060228: Steps 50/138: batch_recall = 57.98, batch_ndcg = 32.18 
2025-04-08 15:15:44.339026: Steps 51/138: batch_recall = 59.94, batch_ndcg = 35.29 
2025-04-08 15:15:45.641233: Steps 52/138: batch_recall = 65.91, batch_ndcg = 41.04 
2025-04-08 15:15:46.955721: Steps 53/138: batch_recall = 64.29, batch_ndcg = 32.73 
2025-04-08 15:15:48.273457: Steps 54/138: batch_recall = 67.11, batch_ndcg = 37.92 
2025-04-08 15:15:49.573550: Steps 55/138: batch_recall = 62.13, batch_ndcg = 34.47 
2025-04-08 15:15:50.865620: Steps 56/138: batch_recall = 62.62, batch_ndcg = 34.90 
2025-04-08 15:15:52.149294: Steps 57/138: batch_recall = 59.52, batch_ndcg = 33.53 
2025-04-08 15:15:53.429024: Steps 58/138: batch_recall = 70.58, batch_ndcg = 37.02 
2025-04-08 15:15:54.725703: Steps 59/138: batch_recall = 75.16, batch_ndcg = 42.52 
2025-04-08 15:15:56.031767: Steps 60/138: batch_recall = 73.90, batch_ndcg = 39.91 
2025-04-08 15:15:57.337878: Steps 61/138: batch_recall = 63.05, batch_ndcg = 34.96 
2025-04-08 15:15:58.620807: Steps 62/138: batch_recall = 85.88, batch_ndcg = 44.30 
2025-04-08 15:15:59.899094: Steps 63/138: batch_recall = 79.71, batch_ndcg = 44.37 
2025-04-08 15:16:01.182549: Steps 64/138: batch_recall = 64.20, batch_ndcg = 34.37 
2025-04-08 15:16:02.461966: Steps 65/138: batch_recall = 80.99, batch_ndcg = 45.39 
2025-04-08 15:16:03.751608: Steps 66/138: batch_recall = 68.63, batch_ndcg = 41.55 
2025-04-08 15:16:05.038183: Steps 67/138: batch_recall = 82.20, batch_ndcg = 47.74 
2025-04-08 15:16:06.310512: Steps 68/138: batch_recall = 67.95, batch_ndcg = 35.78 
2025-04-08 15:16:07.580450: Steps 69/138: batch_recall = 85.94, batch_ndcg = 50.22 
2025-04-08 15:16:08.843934: Steps 70/138: batch_recall = 81.48, batch_ndcg = 46.12 
2025-04-08 15:16:10.103942: Steps 71/138: batch_recall = 86.76, batch_ndcg = 50.71 
2025-04-08 15:16:11.390505: Steps 72/138: batch_recall = 89.75, batch_ndcg = 51.48 
2025-04-08 15:16:12.658452: Steps 73/138: batch_recall = 88.75, batch_ndcg = 48.51 
2025-04-08 15:16:13.941754: Steps 74/138: batch_recall = 80.62, batch_ndcg = 48.20 
2025-04-08 15:16:15.212302: Steps 75/138: batch_recall = 85.30, batch_ndcg = 49.96 
2025-04-08 15:16:16.494568: Steps 76/138: batch_recall = 97.91, batch_ndcg = 55.51 
2025-04-08 15:16:17.764492: Steps 77/138: batch_recall = 85.48, batch_ndcg = 49.02 
2025-04-08 15:16:19.033889: Steps 78/138: batch_recall = 92.60, batch_ndcg = 47.75 
2025-04-08 15:16:20.321642: Steps 79/138: batch_recall = 91.19, batch_ndcg = 48.63 
2025-04-08 15:16:21.588145: Steps 80/138: batch_recall = 72.19, batch_ndcg = 38.19 
2025-04-08 15:16:22.867261: Steps 81/138: batch_recall = 80.66, batch_ndcg = 48.13 
2025-04-08 15:16:24.141685: Steps 82/138: batch_recall = 88.64, batch_ndcg = 51.97 
2025-04-08 15:16:25.420623: Steps 83/138: batch_recall = 83.91, batch_ndcg = 48.27 
2025-04-08 15:16:26.698947: Steps 84/138: batch_recall = 105.11, batch_ndcg = 57.82 
2025-04-08 15:16:27.982205: Steps 85/138: batch_recall = 98.10, batch_ndcg = 58.46 
2025-04-08 15:16:29.252248: Steps 86/138: batch_recall = 117.44, batch_ndcg = 70.28 
2025-04-08 15:16:30.528822: Steps 87/138: batch_recall = 109.13, batch_ndcg = 57.76 
2025-04-08 15:16:31.796369: Steps 88/138: batch_recall = 103.55, batch_ndcg = 57.37 
2025-04-08 15:16:33.072019: Steps 89/138: batch_recall = 116.52, batch_ndcg = 66.17 
2025-04-08 15:16:34.356173: Steps 90/138: batch_recall = 103.36, batch_ndcg = 59.03 
2025-04-08 15:16:35.638323: Steps 91/138: batch_recall = 118.80, batch_ndcg = 66.30 
2025-04-08 15:16:36.906407: Steps 92/138: batch_recall = 111.71, batch_ndcg = 61.90 
2025-04-08 15:16:38.179444: Steps 93/138: batch_recall = 116.08, batch_ndcg = 67.21 
2025-04-08 15:16:39.454430: Steps 94/138: batch_recall = 122.31, batch_ndcg = 63.35 
2025-04-08 15:16:40.728713: Steps 95/138: batch_recall = 114.66, batch_ndcg = 67.87 
2025-04-08 15:16:41.996728: Steps 96/138: batch_recall = 136.94, batch_ndcg = 76.03 
2025-04-08 15:16:43.270996: Steps 97/138: batch_recall = 150.35, batch_ndcg = 86.81 
2025-04-08 15:16:44.560759: Steps 98/138: batch_recall = 109.96, batch_ndcg = 62.77 
2025-04-08 15:16:45.830525: Steps 99/138: batch_recall = 123.26, batch_ndcg = 70.37 
2025-04-08 15:16:47.095393: Steps 100/138: batch_recall = 128.33, batch_ndcg = 71.85 
2025-04-08 15:16:48.374164: Steps 101/138: batch_recall = 127.17, batch_ndcg = 69.52 
2025-04-08 15:16:49.653806: Steps 102/138: batch_recall = 127.24, batch_ndcg = 74.56 
2025-04-08 15:16:50.910353: Steps 103/138: batch_recall = 143.80, batch_ndcg = 80.87 
2025-04-08 15:16:52.180032: Steps 104/138: batch_recall = 139.44, batch_ndcg = 77.42 
2025-04-08 15:16:53.450737: Steps 105/138: batch_recall = 118.35, batch_ndcg = 67.14 
2025-04-08 15:16:54.720562: Steps 106/138: batch_recall = 104.34, batch_ndcg = 58.63 
2025-04-08 15:16:55.996783: Steps 107/138: batch_recall = 115.31, batch_ndcg = 63.65 
2025-04-08 15:16:57.273188: Steps 108/138: batch_recall = 119.18, batch_ndcg = 71.41 
2025-04-08 15:16:58.539900: Steps 109/138: batch_recall = 134.25, batch_ndcg = 74.65 
2025-04-08 15:16:59.803631: Steps 110/138: batch_recall = 125.55, batch_ndcg = 65.32 
2025-04-08 15:17:01.072947: Steps 111/138: batch_recall = 141.32, batch_ndcg = 83.20 
2025-04-08 15:17:02.349094: Steps 112/138: batch_recall = 172.03, batch_ndcg = 94.28 
2025-04-08 15:17:03.619465: Steps 113/138: batch_recall = 127.40, batch_ndcg = 72.36 
2025-04-08 15:17:04.894263: Steps 114/138: batch_recall = 123.17, batch_ndcg = 69.28 
2025-04-08 15:17:06.165636: Steps 115/138: batch_recall = 122.79, batch_ndcg = 64.41 
2025-04-08 15:17:07.441851: Steps 116/138: batch_recall = 125.82, batch_ndcg = 66.89 
2025-04-08 15:17:08.708702: Steps 117/138: batch_recall = 118.84, batch_ndcg = 67.58 
2025-04-08 15:17:09.985014: Steps 118/138: batch_recall = 123.56, batch_ndcg = 72.24 
2025-04-08 15:17:11.249737: Steps 119/138: batch_recall = 138.29, batch_ndcg = 73.10 
2025-04-08 15:17:12.526393: Steps 120/138: batch_recall = 121.49, batch_ndcg = 66.77 
2025-04-08 15:17:13.793368: Steps 121/138: batch_recall = 152.43, batch_ndcg = 80.22 
2025-04-08 15:17:15.068059: Steps 122/138: batch_recall = 149.98, batch_ndcg = 80.77 
2025-04-08 15:17:16.334349: Steps 123/138: batch_recall = 131.26, batch_ndcg = 72.15 
2025-04-08 15:17:17.609813: Steps 124/138: batch_recall = 151.56, batch_ndcg = 93.87 
2025-04-08 15:17:18.898400: Steps 125/138: batch_recall = 130.28, batch_ndcg = 70.61 
2025-04-08 15:17:20.174563: Steps 126/138: batch_recall = 157.47, batch_ndcg = 88.19 
2025-04-08 15:17:21.435530: Steps 127/138: batch_recall = 145.01, batch_ndcg = 81.23 
2025-04-08 15:17:22.698221: Steps 128/138: batch_recall = 125.60, batch_ndcg = 69.57 
2025-04-08 15:17:23.954242: Steps 129/138: batch_recall = 159.82, batch_ndcg = 89.94 
2025-04-08 15:17:25.210974: Steps 130/138: batch_recall = 133.24, batch_ndcg = 69.42 
2025-04-08 15:17:26.468907: Steps 131/138: batch_recall = 147.04, batch_ndcg = 84.11 
2025-04-08 15:17:27.736307: Steps 132/138: batch_recall = 152.41, batch_ndcg = 86.49 
2025-04-08 15:17:29.006103: Steps 133/138: batch_recall = 146.10, batch_ndcg = 83.36 
2025-04-08 15:17:30.278406: Steps 134/138: batch_recall = 138.29, batch_ndcg = 78.86 
2025-04-08 15:17:31.545546: Steps 135/138: batch_recall = 164.86, batch_ndcg = 93.16 
2025-04-08 15:17:32.810531: Steps 136/138: batch_recall = 150.62, batch_ndcg = 78.36 
2025-04-08 15:17:34.081485: Steps 137/138: batch_recall = 135.93, batch_ndcg = 85.15 
2025-04-08 15:17:34.082239: Epoch 12/1000, Test: Recall = 0.1762, NDCG = 0.0995  

2025-04-08 15:17:35.884598: Training Step 0/354: batchLoss = 0.5532, diffLoss = 2.7164, kgLoss = 0.0124
2025-04-08 15:17:37.496390: Training Step 1/354: batchLoss = 0.6298, diffLoss = 3.0896, kgLoss = 0.0148
2025-04-08 15:17:39.113911: Training Step 2/354: batchLoss = 0.4909, diffLoss = 2.4059, kgLoss = 0.0122
2025-04-08 15:17:40.726240: Training Step 3/354: batchLoss = 0.4327, diffLoss = 2.1146, kgLoss = 0.0122
2025-04-08 15:17:42.346311: Training Step 4/354: batchLoss = 0.6535, diffLoss = 3.1931, kgLoss = 0.0186
2025-04-08 15:17:43.964614: Training Step 5/354: batchLoss = 0.7654, diffLoss = 3.7530, kgLoss = 0.0185
2025-04-08 15:17:45.589708: Training Step 6/354: batchLoss = 0.6945, diffLoss = 3.4003, kgLoss = 0.0181
2025-04-08 15:17:47.212118: Training Step 7/354: batchLoss = 0.5066, diffLoss = 2.4658, kgLoss = 0.0167
2025-04-08 15:17:48.832267: Training Step 8/354: batchLoss = 0.5147, diffLoss = 2.5133, kgLoss = 0.0151
2025-04-08 15:17:50.450383: Training Step 9/354: batchLoss = 0.6052, diffLoss = 2.9735, kgLoss = 0.0131
2025-04-08 15:17:52.069595: Training Step 10/354: batchLoss = 0.4990, diffLoss = 2.4476, kgLoss = 0.0118
2025-04-08 15:17:53.688163: Training Step 11/354: batchLoss = 0.5937, diffLoss = 2.9087, kgLoss = 0.0149
2025-04-08 15:17:55.310336: Training Step 12/354: batchLoss = 0.5972, diffLoss = 2.9308, kgLoss = 0.0138
2025-04-08 15:17:56.929984: Training Step 13/354: batchLoss = 0.4894, diffLoss = 2.3897, kgLoss = 0.0144
2025-04-08 15:17:58.551207: Training Step 14/354: batchLoss = 0.5165, diffLoss = 2.5246, kgLoss = 0.0145
2025-04-08 15:18:00.171556: Training Step 15/354: batchLoss = 0.6348, diffLoss = 3.1140, kgLoss = 0.0151
2025-04-08 15:18:01.790055: Training Step 16/354: batchLoss = 0.5174, diffLoss = 2.5301, kgLoss = 0.0142
2025-04-08 15:18:03.408982: Training Step 17/354: batchLoss = 0.6070, diffLoss = 2.9758, kgLoss = 0.0148
2025-04-08 15:18:05.024705: Training Step 18/354: batchLoss = 0.6411, diffLoss = 3.1498, kgLoss = 0.0139
2025-04-08 15:18:06.643166: Training Step 19/354: batchLoss = 0.7519, diffLoss = 3.6836, kgLoss = 0.0190
2025-04-08 15:18:08.259434: Training Step 20/354: batchLoss = 0.5560, diffLoss = 2.7271, kgLoss = 0.0133
2025-04-08 15:18:09.871007: Training Step 21/354: batchLoss = 0.6193, diffLoss = 3.0368, kgLoss = 0.0149
2025-04-08 15:18:11.483400: Training Step 22/354: batchLoss = 0.5767, diffLoss = 2.8302, kgLoss = 0.0133
2025-04-08 15:18:13.101632: Training Step 23/354: batchLoss = 0.7645, diffLoss = 3.7528, kgLoss = 0.0174
2025-04-08 15:18:14.724209: Training Step 24/354: batchLoss = 0.5089, diffLoss = 2.4960, kgLoss = 0.0121
2025-04-08 15:18:16.346201: Training Step 25/354: batchLoss = 0.6145, diffLoss = 3.0124, kgLoss = 0.0151
2025-04-08 15:18:17.962633: Training Step 26/354: batchLoss = 0.5027, diffLoss = 2.4653, kgLoss = 0.0120
2025-04-08 15:18:19.581010: Training Step 27/354: batchLoss = 0.5539, diffLoss = 2.7149, kgLoss = 0.0136
2025-04-08 15:18:21.196829: Training Step 28/354: batchLoss = 0.5377, diffLoss = 2.6303, kgLoss = 0.0145
2025-04-08 15:18:22.805603: Training Step 29/354: batchLoss = 0.5224, diffLoss = 2.5554, kgLoss = 0.0142
2025-04-08 15:18:24.422503: Training Step 30/354: batchLoss = 0.6188, diffLoss = 3.0345, kgLoss = 0.0149
2025-04-08 15:18:26.047041: Training Step 31/354: batchLoss = 0.9609, diffLoss = 4.7127, kgLoss = 0.0229
2025-04-08 15:18:27.669498: Training Step 32/354: batchLoss = 0.5539, diffLoss = 2.7117, kgLoss = 0.0145
2025-04-08 15:18:29.292052: Training Step 33/354: batchLoss = 0.5618, diffLoss = 2.7530, kgLoss = 0.0140
2025-04-08 15:18:30.905626: Training Step 34/354: batchLoss = 0.5870, diffLoss = 2.8770, kgLoss = 0.0145
2025-04-08 15:18:32.519055: Training Step 35/354: batchLoss = 0.6142, diffLoss = 3.0129, kgLoss = 0.0145
2025-04-08 15:18:34.133705: Training Step 36/354: batchLoss = 0.6588, diffLoss = 3.2298, kgLoss = 0.0160
2025-04-08 15:18:35.740476: Training Step 37/354: batchLoss = 0.5159, diffLoss = 2.5227, kgLoss = 0.0142
2025-04-08 15:18:37.356071: Training Step 38/354: batchLoss = 0.5948, diffLoss = 2.9153, kgLoss = 0.0146
2025-04-08 15:18:38.968868: Training Step 39/354: batchLoss = 0.5665, diffLoss = 2.7736, kgLoss = 0.0147
2025-04-08 15:18:40.580191: Training Step 40/354: batchLoss = 0.6189, diffLoss = 3.0321, kgLoss = 0.0156
2025-04-08 15:18:42.207579: Training Step 41/354: batchLoss = 0.4555, diffLoss = 2.2281, kgLoss = 0.0123
2025-04-08 15:18:43.824484: Training Step 42/354: batchLoss = 0.7029, diffLoss = 3.4386, kgLoss = 0.0189
2025-04-08 15:18:45.442027: Training Step 43/354: batchLoss = 0.4996, diffLoss = 2.4484, kgLoss = 0.0124
2025-04-08 15:18:47.057665: Training Step 44/354: batchLoss = 0.5587, diffLoss = 2.7361, kgLoss = 0.0143
2025-04-08 15:18:48.676133: Training Step 45/354: batchLoss = 0.6032, diffLoss = 2.9604, kgLoss = 0.0139
2025-04-08 15:18:50.288263: Training Step 46/354: batchLoss = 0.5488, diffLoss = 2.6887, kgLoss = 0.0139
2025-04-08 15:18:51.902867: Training Step 47/354: batchLoss = 0.5608, diffLoss = 2.7320, kgLoss = 0.0180
2025-04-08 15:18:53.511031: Training Step 48/354: batchLoss = 0.5832, diffLoss = 2.8585, kgLoss = 0.0144
2025-04-08 15:18:55.128522: Training Step 49/354: batchLoss = 0.5223, diffLoss = 2.5613, kgLoss = 0.0125
2025-04-08 15:18:56.748200: Training Step 50/354: batchLoss = 0.6429, diffLoss = 3.1519, kgLoss = 0.0157
2025-04-08 15:18:58.368374: Training Step 51/354: batchLoss = 0.6009, diffLoss = 2.9445, kgLoss = 0.0150
2025-04-08 15:18:59.994326: Training Step 52/354: batchLoss = 0.5539, diffLoss = 2.7104, kgLoss = 0.0147
2025-04-08 15:19:01.627281: Training Step 53/354: batchLoss = 0.5992, diffLoss = 2.9334, kgLoss = 0.0157
2025-04-08 15:19:03.259418: Training Step 54/354: batchLoss = 0.4946, diffLoss = 2.4235, kgLoss = 0.0123
2025-04-08 15:19:04.894116: Training Step 55/354: batchLoss = 0.6347, diffLoss = 3.1076, kgLoss = 0.0165
2025-04-08 15:19:06.517033: Training Step 56/354: batchLoss = 0.7291, diffLoss = 3.5832, kgLoss = 0.0156
2025-04-08 15:19:08.134088: Training Step 57/354: batchLoss = 0.5798, diffLoss = 2.8468, kgLoss = 0.0130
2025-04-08 15:19:09.754564: Training Step 58/354: batchLoss = 0.6332, diffLoss = 3.1039, kgLoss = 0.0156
2025-04-08 15:19:11.389150: Training Step 59/354: batchLoss = 0.4706, diffLoss = 2.2990, kgLoss = 0.0135
2025-04-08 15:19:13.016869: Training Step 60/354: batchLoss = 0.6162, diffLoss = 3.0137, kgLoss = 0.0168
2025-04-08 15:19:14.641954: Training Step 61/354: batchLoss = 0.6133, diffLoss = 3.0007, kgLoss = 0.0164
2025-04-08 15:19:16.260766: Training Step 62/354: batchLoss = 0.5554, diffLoss = 2.7140, kgLoss = 0.0158
2025-04-08 15:19:17.885684: Training Step 63/354: batchLoss = 0.5845, diffLoss = 2.8687, kgLoss = 0.0135
2025-04-08 15:19:19.510244: Training Step 64/354: batchLoss = 0.7026, diffLoss = 3.4416, kgLoss = 0.0178
2025-04-08 15:19:21.125689: Training Step 65/354: batchLoss = 0.5424, diffLoss = 2.6554, kgLoss = 0.0141
2025-04-08 15:19:22.742279: Training Step 66/354: batchLoss = 0.5336, diffLoss = 2.6124, kgLoss = 0.0139
2025-04-08 15:19:24.357328: Training Step 67/354: batchLoss = 0.4703, diffLoss = 2.3008, kgLoss = 0.0126
2025-04-08 15:19:25.980329: Training Step 68/354: batchLoss = 0.6346, diffLoss = 3.1136, kgLoss = 0.0148
2025-04-08 15:19:27.597308: Training Step 69/354: batchLoss = 0.6029, diffLoss = 2.9521, kgLoss = 0.0157
2025-04-08 15:19:29.217767: Training Step 70/354: batchLoss = 0.5355, diffLoss = 2.6278, kgLoss = 0.0125
2025-04-08 15:19:30.836870: Training Step 71/354: batchLoss = 0.4876, diffLoss = 2.3924, kgLoss = 0.0113
2025-04-08 15:19:32.459607: Training Step 72/354: batchLoss = 0.5962, diffLoss = 2.9229, kgLoss = 0.0146
2025-04-08 15:19:34.075001: Training Step 73/354: batchLoss = 0.5665, diffLoss = 2.7705, kgLoss = 0.0156
2025-04-08 15:19:35.695516: Training Step 74/354: batchLoss = 0.6186, diffLoss = 3.0354, kgLoss = 0.0144
2025-04-08 15:19:37.311890: Training Step 75/354: batchLoss = 0.5781, diffLoss = 2.8352, kgLoss = 0.0138
2025-04-08 15:19:38.931583: Training Step 76/354: batchLoss = 0.5843, diffLoss = 2.8670, kgLoss = 0.0137
2025-04-08 15:19:40.547663: Training Step 77/354: batchLoss = 0.5191, diffLoss = 2.5447, kgLoss = 0.0127
2025-04-08 15:19:42.165648: Training Step 78/354: batchLoss = 0.5335, diffLoss = 2.6094, kgLoss = 0.0145
2025-04-08 15:19:43.788307: Training Step 79/354: batchLoss = 0.5246, diffLoss = 2.5629, kgLoss = 0.0150
2025-04-08 15:19:45.405632: Training Step 80/354: batchLoss = 0.5575, diffLoss = 2.7336, kgLoss = 0.0135
2025-04-08 15:19:47.024320: Training Step 81/354: batchLoss = 0.6578, diffLoss = 3.2305, kgLoss = 0.0147
2025-04-08 15:19:48.645522: Training Step 82/354: batchLoss = 0.5873, diffLoss = 2.8753, kgLoss = 0.0153
2025-04-08 15:19:50.262133: Training Step 83/354: batchLoss = 0.7672, diffLoss = 3.7613, kgLoss = 0.0187
2025-04-08 15:19:51.881647: Training Step 84/354: batchLoss = 0.7450, diffLoss = 3.6476, kgLoss = 0.0193
2025-04-08 15:19:53.501290: Training Step 85/354: batchLoss = 0.5957, diffLoss = 2.9184, kgLoss = 0.0151
2025-04-08 15:19:55.115508: Training Step 86/354: batchLoss = 0.6509, diffLoss = 3.1958, kgLoss = 0.0147
2025-04-08 15:19:56.739194: Training Step 87/354: batchLoss = 0.5027, diffLoss = 2.4620, kgLoss = 0.0129
2025-04-08 15:19:58.358698: Training Step 88/354: batchLoss = 0.6958, diffLoss = 3.4142, kgLoss = 0.0162
2025-04-08 15:19:59.981612: Training Step 89/354: batchLoss = 0.5680, diffLoss = 2.7850, kgLoss = 0.0137
2025-04-08 15:20:01.606205: Training Step 90/354: batchLoss = 0.6000, diffLoss = 2.9423, kgLoss = 0.0144
2025-04-08 15:20:03.228392: Training Step 91/354: batchLoss = 0.5857, diffLoss = 2.8647, kgLoss = 0.0159
2025-04-08 15:20:04.845052: Training Step 92/354: batchLoss = 0.5161, diffLoss = 2.5276, kgLoss = 0.0132
2025-04-08 15:20:06.465110: Training Step 93/354: batchLoss = 0.4737, diffLoss = 2.3193, kgLoss = 0.0123
2025-04-08 15:20:08.077334: Training Step 94/354: batchLoss = 0.4876, diffLoss = 2.3902, kgLoss = 0.0119
2025-04-08 15:20:09.700385: Training Step 95/354: batchLoss = 0.5912, diffLoss = 2.8873, kgLoss = 0.0172
2025-04-08 15:20:11.318777: Training Step 96/354: batchLoss = 0.7468, diffLoss = 3.6567, kgLoss = 0.0193
2025-04-08 15:20:12.943072: Training Step 97/354: batchLoss = 0.5477, diffLoss = 2.6870, kgLoss = 0.0129
2025-04-08 15:20:14.566158: Training Step 98/354: batchLoss = 0.5609, diffLoss = 2.7494, kgLoss = 0.0137
2025-04-08 15:20:16.183112: Training Step 99/354: batchLoss = 0.6239, diffLoss = 3.0628, kgLoss = 0.0142
2025-04-08 15:20:17.799003: Training Step 100/354: batchLoss = 0.6707, diffLoss = 3.2955, kgLoss = 0.0145
2025-04-08 15:20:19.416392: Training Step 101/354: batchLoss = 0.5858, diffLoss = 2.8739, kgLoss = 0.0138
2025-04-08 15:20:21.031275: Training Step 102/354: batchLoss = 0.6657, diffLoss = 3.2650, kgLoss = 0.0158
2025-04-08 15:20:22.637963: Training Step 103/354: batchLoss = 0.5095, diffLoss = 2.4879, kgLoss = 0.0150
2025-04-08 15:20:24.258954: Training Step 104/354: batchLoss = 0.6020, diffLoss = 2.9491, kgLoss = 0.0152
2025-04-08 15:20:25.885499: Training Step 105/354: batchLoss = 0.5302, diffLoss = 2.5982, kgLoss = 0.0132
2025-04-08 15:20:27.498790: Training Step 106/354: batchLoss = 0.6312, diffLoss = 3.0932, kgLoss = 0.0158
2025-04-08 15:20:29.121256: Training Step 107/354: batchLoss = 0.5153, diffLoss = 2.5228, kgLoss = 0.0135
2025-04-08 15:20:30.751924: Training Step 108/354: batchLoss = 0.6430, diffLoss = 3.1527, kgLoss = 0.0155
2025-04-08 15:20:32.371991: Training Step 109/354: batchLoss = 0.5668, diffLoss = 2.7750, kgLoss = 0.0147
2025-04-08 15:20:33.986540: Training Step 110/354: batchLoss = 0.5907, diffLoss = 2.8938, kgLoss = 0.0149
2025-04-08 15:20:35.601240: Training Step 111/354: batchLoss = 0.5854, diffLoss = 2.8708, kgLoss = 0.0141
2025-04-08 15:20:37.214925: Training Step 112/354: batchLoss = 0.6256, diffLoss = 3.0659, kgLoss = 0.0155
2025-04-08 15:20:38.824776: Training Step 113/354: batchLoss = 0.5396, diffLoss = 2.6482, kgLoss = 0.0125
2025-04-08 15:20:40.449570: Training Step 114/354: batchLoss = 0.7562, diffLoss = 3.7067, kgLoss = 0.0186
2025-04-08 15:20:42.071789: Training Step 115/354: batchLoss = 0.8388, diffLoss = 4.1203, kgLoss = 0.0184
2025-04-08 15:20:43.690636: Training Step 116/354: batchLoss = 0.5679, diffLoss = 2.7852, kgLoss = 0.0136
2025-04-08 15:20:45.309981: Training Step 117/354: batchLoss = 0.5519, diffLoss = 2.6826, kgLoss = 0.0193
2025-04-08 15:20:46.931386: Training Step 118/354: batchLoss = 0.5443, diffLoss = 2.6704, kgLoss = 0.0128
2025-04-08 15:20:48.547094: Training Step 119/354: batchLoss = 0.6759, diffLoss = 3.3134, kgLoss = 0.0166
2025-04-08 15:20:50.168236: Training Step 120/354: batchLoss = 0.4700, diffLoss = 2.2906, kgLoss = 0.0148
2025-04-08 15:20:51.786960: Training Step 121/354: batchLoss = 0.5140, diffLoss = 2.5214, kgLoss = 0.0122
2025-04-08 15:20:53.405523: Training Step 122/354: batchLoss = 0.4943, diffLoss = 2.4201, kgLoss = 0.0129
2025-04-08 15:20:55.027744: Training Step 123/354: batchLoss = 0.7417, diffLoss = 3.6311, kgLoss = 0.0194
2025-04-08 15:20:56.644163: Training Step 124/354: batchLoss = 0.4684, diffLoss = 2.2854, kgLoss = 0.0142
2025-04-08 15:20:58.268078: Training Step 125/354: batchLoss = 0.4697, diffLoss = 2.3012, kgLoss = 0.0118
2025-04-08 15:20:59.887740: Training Step 126/354: batchLoss = 0.5444, diffLoss = 2.6666, kgLoss = 0.0139
2025-04-08 15:21:01.507770: Training Step 127/354: batchLoss = 0.6175, diffLoss = 3.0103, kgLoss = 0.0192
2025-04-08 15:21:03.124200: Training Step 128/354: batchLoss = 0.6884, diffLoss = 3.3725, kgLoss = 0.0173
2025-04-08 15:21:04.744386: Training Step 129/354: batchLoss = 0.6844, diffLoss = 3.3506, kgLoss = 0.0179
2025-04-08 15:21:06.349533: Training Step 130/354: batchLoss = 0.7036, diffLoss = 3.4463, kgLoss = 0.0179
2025-04-08 15:21:07.967846: Training Step 131/354: batchLoss = 0.5956, diffLoss = 2.9089, kgLoss = 0.0172
2025-04-08 15:21:09.588916: Training Step 132/354: batchLoss = 0.6325, diffLoss = 2.9944, kgLoss = 0.0420
2025-04-08 15:21:11.207761: Training Step 133/354: batchLoss = 0.6312, diffLoss = 3.1010, kgLoss = 0.0138
2025-04-08 15:21:12.831699: Training Step 134/354: batchLoss = 0.5495, diffLoss = 2.6915, kgLoss = 0.0140
2025-04-08 15:21:14.455835: Training Step 135/354: batchLoss = 0.5500, diffLoss = 2.6971, kgLoss = 0.0132
2025-04-08 15:21:16.077243: Training Step 136/354: batchLoss = 0.6621, diffLoss = 3.2431, kgLoss = 0.0168
2025-04-08 15:21:17.693275: Training Step 137/354: batchLoss = 0.6356, diffLoss = 3.1156, kgLoss = 0.0156
2025-04-08 15:21:19.310028: Training Step 138/354: batchLoss = 0.5948, diffLoss = 2.9213, kgLoss = 0.0132
2025-04-08 15:21:20.920575: Training Step 139/354: batchLoss = 0.6331, diffLoss = 3.1057, kgLoss = 0.0150
2025-04-08 15:21:22.537102: Training Step 140/354: batchLoss = 0.6373, diffLoss = 3.1201, kgLoss = 0.0166
2025-04-08 15:21:24.156555: Training Step 141/354: batchLoss = 0.5358, diffLoss = 2.6343, kgLoss = 0.0112
2025-04-08 15:21:25.778758: Training Step 142/354: batchLoss = 0.5247, diffLoss = 2.5688, kgLoss = 0.0137
2025-04-08 15:21:27.400802: Training Step 143/354: batchLoss = 0.5163, diffLoss = 2.5268, kgLoss = 0.0137
2025-04-08 15:21:29.028843: Training Step 144/354: batchLoss = 0.6520, diffLoss = 3.1943, kgLoss = 0.0164
2025-04-08 15:21:30.639156: Training Step 145/354: batchLoss = 0.5973, diffLoss = 2.9345, kgLoss = 0.0130
2025-04-08 15:21:32.254503: Training Step 146/354: batchLoss = 0.6043, diffLoss = 2.9621, kgLoss = 0.0149
2025-04-08 15:21:33.870430: Training Step 147/354: batchLoss = 0.5329, diffLoss = 2.6039, kgLoss = 0.0152
2025-04-08 15:21:35.483952: Training Step 148/354: batchLoss = 0.5139, diffLoss = 2.5222, kgLoss = 0.0119
2025-04-08 15:21:37.106986: Training Step 149/354: batchLoss = 0.5925, diffLoss = 2.9062, kgLoss = 0.0140
2025-04-08 15:21:38.726353: Training Step 150/354: batchLoss = 0.4941, diffLoss = 2.4232, kgLoss = 0.0118
2025-04-08 15:21:40.350188: Training Step 151/354: batchLoss = 0.5264, diffLoss = 2.5713, kgLoss = 0.0151
2025-04-08 15:21:41.976053: Training Step 152/354: batchLoss = 0.6503, diffLoss = 3.1828, kgLoss = 0.0172
2025-04-08 15:21:43.590937: Training Step 153/354: batchLoss = 0.5110, diffLoss = 2.5008, kgLoss = 0.0135
2025-04-08 15:21:45.221124: Training Step 154/354: batchLoss = 0.6157, diffLoss = 3.0210, kgLoss = 0.0144
2025-04-08 15:21:46.834903: Training Step 155/354: batchLoss = 0.6353, diffLoss = 3.1099, kgLoss = 0.0166
2025-04-08 15:21:48.446171: Training Step 156/354: batchLoss = 0.5751, diffLoss = 2.8194, kgLoss = 0.0140
2025-04-08 15:21:50.058010: Training Step 157/354: batchLoss = 0.6682, diffLoss = 3.2715, kgLoss = 0.0174
2025-04-08 15:21:51.668937: Training Step 158/354: batchLoss = 0.5895, diffLoss = 2.8964, kgLoss = 0.0128
2025-04-08 15:21:53.283774: Training Step 159/354: batchLoss = 0.7029, diffLoss = 3.4502, kgLoss = 0.0160
2025-04-08 15:21:54.906565: Training Step 160/354: batchLoss = 0.5228, diffLoss = 2.5626, kgLoss = 0.0129
2025-04-08 15:21:56.528035: Training Step 161/354: batchLoss = 0.5730, diffLoss = 2.8118, kgLoss = 0.0133
2025-04-08 15:21:58.145771: Training Step 162/354: batchLoss = 0.5943, diffLoss = 2.9165, kgLoss = 0.0137
2025-04-08 15:21:59.764033: Training Step 163/354: batchLoss = 0.6980, diffLoss = 3.4216, kgLoss = 0.0171
2025-04-08 15:22:01.380488: Training Step 164/354: batchLoss = 0.5334, diffLoss = 2.6151, kgLoss = 0.0130
2025-04-08 15:22:02.996244: Training Step 165/354: batchLoss = 0.5119, diffLoss = 2.4996, kgLoss = 0.0150
2025-04-08 15:22:04.607328: Training Step 166/354: batchLoss = 0.6240, diffLoss = 3.0662, kgLoss = 0.0134
2025-04-08 15:22:06.221625: Training Step 167/354: batchLoss = 0.5665, diffLoss = 2.7658, kgLoss = 0.0166
2025-04-08 15:22:07.837613: Training Step 168/354: batchLoss = 0.5815, diffLoss = 2.8471, kgLoss = 0.0151
2025-04-08 15:22:09.454716: Training Step 169/354: batchLoss = 0.6379, diffLoss = 3.1291, kgLoss = 0.0151
2025-04-08 15:22:11.070303: Training Step 170/354: batchLoss = 0.6502, diffLoss = 3.1901, kgLoss = 0.0152
2025-04-08 15:22:12.696838: Training Step 171/354: batchLoss = 0.6686, diffLoss = 3.2741, kgLoss = 0.0173
2025-04-08 15:22:14.330237: Training Step 172/354: batchLoss = 0.5800, diffLoss = 2.8427, kgLoss = 0.0143
2025-04-08 15:22:15.972468: Training Step 173/354: batchLoss = 0.6600, diffLoss = 3.2379, kgLoss = 0.0155
2025-04-08 15:22:17.604754: Training Step 174/354: batchLoss = 0.5219, diffLoss = 2.5586, kgLoss = 0.0127
2025-04-08 15:22:19.229591: Training Step 175/354: batchLoss = 0.5549, diffLoss = 2.7200, kgLoss = 0.0136
2025-04-08 15:22:20.852720: Training Step 176/354: batchLoss = 0.6104, diffLoss = 2.9933, kgLoss = 0.0146
2025-04-08 15:22:22.479943: Training Step 177/354: batchLoss = 0.6307, diffLoss = 3.0937, kgLoss = 0.0149
2025-04-08 15:22:24.112957: Training Step 178/354: batchLoss = 0.5282, diffLoss = 2.5860, kgLoss = 0.0137
2025-04-08 15:22:25.742401: Training Step 179/354: batchLoss = 0.5415, diffLoss = 2.6571, kgLoss = 0.0126
2025-04-08 15:22:27.364783: Training Step 180/354: batchLoss = 0.5426, diffLoss = 2.6547, kgLoss = 0.0146
2025-04-08 15:22:28.982099: Training Step 181/354: batchLoss = 0.5029, diffLoss = 2.4727, kgLoss = 0.0105
2025-04-08 15:22:30.601521: Training Step 182/354: batchLoss = 0.6888, diffLoss = 3.3829, kgLoss = 0.0153
2025-04-08 15:22:32.220709: Training Step 183/354: batchLoss = 0.5332, diffLoss = 2.6083, kgLoss = 0.0144
2025-04-08 15:22:33.842076: Training Step 184/354: batchLoss = 0.5128, diffLoss = 2.5138, kgLoss = 0.0126
2025-04-08 15:22:35.461215: Training Step 185/354: batchLoss = 0.7489, diffLoss = 3.6743, kgLoss = 0.0175
2025-04-08 15:22:37.077430: Training Step 186/354: batchLoss = 0.6161, diffLoss = 3.0191, kgLoss = 0.0154
2025-04-08 15:22:38.695527: Training Step 187/354: batchLoss = 0.5432, diffLoss = 2.6581, kgLoss = 0.0145
2025-04-08 15:22:40.315121: Training Step 188/354: batchLoss = 0.7152, diffLoss = 3.5155, kgLoss = 0.0151
2025-04-08 15:22:41.928394: Training Step 189/354: batchLoss = 0.5468, diffLoss = 2.6806, kgLoss = 0.0133
2025-04-08 15:22:43.552878: Training Step 190/354: batchLoss = 0.5595, diffLoss = 2.7381, kgLoss = 0.0148
2025-04-08 15:22:45.163220: Training Step 191/354: batchLoss = 0.4873, diffLoss = 2.3852, kgLoss = 0.0128
2025-04-08 15:22:46.776882: Training Step 192/354: batchLoss = 0.4262, diffLoss = 2.0845, kgLoss = 0.0116
2025-04-08 15:22:48.388823: Training Step 193/354: batchLoss = 0.6575, diffLoss = 3.2242, kgLoss = 0.0158
2025-04-08 15:22:50.005261: Training Step 194/354: batchLoss = 0.4322, diffLoss = 2.1162, kgLoss = 0.0113
2025-04-08 15:22:51.627660: Training Step 195/354: batchLoss = 0.5256, diffLoss = 2.5723, kgLoss = 0.0139
2025-04-08 15:22:53.241912: Training Step 196/354: batchLoss = 0.5399, diffLoss = 2.6483, kgLoss = 0.0128
2025-04-08 15:22:54.860617: Training Step 197/354: batchLoss = 0.6400, diffLoss = 3.1396, kgLoss = 0.0151
2025-04-08 15:22:56.484356: Training Step 198/354: batchLoss = 0.5545, diffLoss = 2.7156, kgLoss = 0.0142
2025-04-08 15:22:58.105625: Training Step 199/354: batchLoss = 0.5842, diffLoss = 2.8636, kgLoss = 0.0144
2025-04-08 15:22:59.720955: Training Step 200/354: batchLoss = 0.4987, diffLoss = 2.4434, kgLoss = 0.0125
2025-04-08 15:23:01.329182: Training Step 201/354: batchLoss = 0.5069, diffLoss = 2.4772, kgLoss = 0.0144
2025-04-08 15:23:02.940963: Training Step 202/354: batchLoss = 0.6204, diffLoss = 3.0418, kgLoss = 0.0151
2025-04-08 15:23:04.552579: Training Step 203/354: batchLoss = 0.5425, diffLoss = 2.6594, kgLoss = 0.0133
2025-04-08 15:23:06.173462: Training Step 204/354: batchLoss = 0.5376, diffLoss = 2.6319, kgLoss = 0.0140
2025-04-08 15:23:07.791980: Training Step 205/354: batchLoss = 0.6501, diffLoss = 3.1899, kgLoss = 0.0152
2025-04-08 15:23:09.412845: Training Step 206/354: batchLoss = 0.5437, diffLoss = 2.6637, kgLoss = 0.0137
2025-04-08 15:23:11.025578: Training Step 207/354: batchLoss = 0.5599, diffLoss = 2.7469, kgLoss = 0.0132
2025-04-08 15:23:12.645784: Training Step 208/354: batchLoss = 0.6164, diffLoss = 3.0243, kgLoss = 0.0145
2025-04-08 15:23:14.259279: Training Step 209/354: batchLoss = 0.5647, diffLoss = 2.7686, kgLoss = 0.0137
2025-04-08 15:23:15.893467: Training Step 210/354: batchLoss = 0.6629, diffLoss = 3.2541, kgLoss = 0.0151
2025-04-08 15:23:17.501720: Training Step 211/354: batchLoss = 0.6357, diffLoss = 3.1156, kgLoss = 0.0157
2025-04-08 15:23:19.122930: Training Step 212/354: batchLoss = 0.5789, diffLoss = 2.8347, kgLoss = 0.0150
2025-04-08 15:23:20.737500: Training Step 213/354: batchLoss = 0.6170, diffLoss = 3.0193, kgLoss = 0.0164
2025-04-08 15:23:22.363579: Training Step 214/354: batchLoss = 0.4499, diffLoss = 2.2034, kgLoss = 0.0115
2025-04-08 15:23:23.988336: Training Step 215/354: batchLoss = 0.6510, diffLoss = 3.1952, kgLoss = 0.0149
2025-04-08 15:23:25.610038: Training Step 216/354: batchLoss = 0.5490, diffLoss = 2.6915, kgLoss = 0.0134
2025-04-08 15:23:27.237720: Training Step 217/354: batchLoss = 0.5867, diffLoss = 2.8768, kgLoss = 0.0142
2025-04-08 15:23:28.854929: Training Step 218/354: batchLoss = 0.7364, diffLoss = 3.6183, kgLoss = 0.0159
2025-04-08 15:23:30.464789: Training Step 219/354: batchLoss = 0.4564, diffLoss = 2.2304, kgLoss = 0.0129
2025-04-08 15:23:32.078891: Training Step 220/354: batchLoss = 0.5819, diffLoss = 2.8512, kgLoss = 0.0145
2025-04-08 15:23:33.695943: Training Step 221/354: batchLoss = 0.7454, diffLoss = 3.6549, kgLoss = 0.0180
2025-04-08 15:23:35.312422: Training Step 222/354: batchLoss = 0.6149, diffLoss = 3.0069, kgLoss = 0.0169
2025-04-08 15:23:36.934594: Training Step 223/354: batchLoss = 0.4783, diffLoss = 2.3405, kgLoss = 0.0128
2025-04-08 15:23:38.557821: Training Step 224/354: batchLoss = 0.5578, diffLoss = 2.7336, kgLoss = 0.0138
2025-04-08 15:23:40.182868: Training Step 225/354: batchLoss = 0.5665, diffLoss = 2.7759, kgLoss = 0.0142
2025-04-08 15:23:41.803534: Training Step 226/354: batchLoss = 0.6474, diffLoss = 3.1686, kgLoss = 0.0171
2025-04-08 15:23:43.425895: Training Step 227/354: batchLoss = 0.5846, diffLoss = 2.8613, kgLoss = 0.0155
2025-04-08 15:23:45.042381: Training Step 228/354: batchLoss = 0.6907, diffLoss = 3.3942, kgLoss = 0.0148
2025-04-08 15:23:46.654886: Training Step 229/354: batchLoss = 0.4888, diffLoss = 2.3967, kgLoss = 0.0118
2025-04-08 15:23:48.269361: Training Step 230/354: batchLoss = 0.5123, diffLoss = 2.5101, kgLoss = 0.0128
2025-04-08 15:23:49.886763: Training Step 231/354: batchLoss = 0.6097, diffLoss = 2.9884, kgLoss = 0.0151
2025-04-08 15:23:51.515292: Training Step 232/354: batchLoss = 0.5245, diffLoss = 2.5635, kgLoss = 0.0147
2025-04-08 15:23:53.131690: Training Step 233/354: batchLoss = 0.5581, diffLoss = 2.7348, kgLoss = 0.0140
2025-04-08 15:23:54.752237: Training Step 234/354: batchLoss = 0.7020, diffLoss = 3.4428, kgLoss = 0.0168
2025-04-08 15:23:56.374358: Training Step 235/354: batchLoss = 0.7321, diffLoss = 3.5882, kgLoss = 0.0180
2025-04-08 15:23:57.997562: Training Step 236/354: batchLoss = 0.5482, diffLoss = 2.6892, kgLoss = 0.0130
2025-04-08 15:23:59.616642: Training Step 237/354: batchLoss = 0.6008, diffLoss = 2.9525, kgLoss = 0.0129
2025-04-08 15:24:01.234871: Training Step 238/354: batchLoss = 0.4677, diffLoss = 2.2869, kgLoss = 0.0129
2025-04-08 15:24:02.857218: Training Step 239/354: batchLoss = 0.6189, diffLoss = 3.0256, kgLoss = 0.0173
2025-04-08 15:24:04.474817: Training Step 240/354: batchLoss = 0.6371, diffLoss = 3.1295, kgLoss = 0.0140
2025-04-08 15:24:06.098918: Training Step 241/354: batchLoss = 0.5271, diffLoss = 2.5814, kgLoss = 0.0136
2025-04-08 15:24:07.722517: Training Step 242/354: batchLoss = 0.7750, diffLoss = 3.8090, kgLoss = 0.0165
2025-04-08 15:24:09.346500: Training Step 243/354: batchLoss = 0.5598, diffLoss = 2.7442, kgLoss = 0.0137
2025-04-08 15:24:10.973776: Training Step 244/354: batchLoss = 0.5514, diffLoss = 2.6998, kgLoss = 0.0143
2025-04-08 15:24:12.589464: Training Step 245/354: batchLoss = 0.5577, diffLoss = 2.7299, kgLoss = 0.0146
2025-04-08 15:24:14.210298: Training Step 246/354: batchLoss = 0.6917, diffLoss = 3.3910, kgLoss = 0.0169
2025-04-08 15:24:15.831216: Training Step 247/354: batchLoss = 0.6155, diffLoss = 3.0148, kgLoss = 0.0157
2025-04-08 15:24:17.439965: Training Step 248/354: batchLoss = 0.5501, diffLoss = 2.6929, kgLoss = 0.0145
2025-04-08 15:24:19.056766: Training Step 249/354: batchLoss = 0.4606, diffLoss = 2.2551, kgLoss = 0.0120
2025-04-08 15:24:20.692024: Training Step 250/354: batchLoss = 0.5822, diffLoss = 2.8479, kgLoss = 0.0157
2025-04-08 15:24:22.309991: Training Step 251/354: batchLoss = 0.7396, diffLoss = 3.6285, kgLoss = 0.0174
2025-04-08 15:24:23.933928: Training Step 252/354: batchLoss = 0.6067, diffLoss = 2.9765, kgLoss = 0.0143
2025-04-08 15:24:25.554713: Training Step 253/354: batchLoss = 0.6404, diffLoss = 3.1387, kgLoss = 0.0159
2025-04-08 15:24:27.172813: Training Step 254/354: batchLoss = 0.5559, diffLoss = 2.7184, kgLoss = 0.0153
2025-04-08 15:24:28.788998: Training Step 255/354: batchLoss = 0.5536, diffLoss = 2.7107, kgLoss = 0.0144
2025-04-08 15:24:30.405889: Training Step 256/354: batchLoss = 0.8092, diffLoss = 3.9256, kgLoss = 0.0301
2025-04-08 15:24:32.019077: Training Step 257/354: batchLoss = 0.7967, diffLoss = 3.9093, kgLoss = 0.0185
2025-04-08 15:24:33.634839: Training Step 258/354: batchLoss = 0.6194, diffLoss = 3.0278, kgLoss = 0.0173
2025-04-08 15:24:35.251297: Training Step 259/354: batchLoss = 0.6775, diffLoss = 3.3212, kgLoss = 0.0165
2025-04-08 15:24:36.867555: Training Step 260/354: batchLoss = 0.6782, diffLoss = 3.3248, kgLoss = 0.0166
2025-04-08 15:24:38.485391: Training Step 261/354: batchLoss = 0.6387, diffLoss = 3.1320, kgLoss = 0.0154
2025-04-08 15:24:40.104308: Training Step 262/354: batchLoss = 0.7112, diffLoss = 3.4798, kgLoss = 0.0191
2025-04-08 15:24:41.723643: Training Step 263/354: batchLoss = 0.6065, diffLoss = 2.9747, kgLoss = 0.0145
2025-04-08 15:24:43.342972: Training Step 264/354: batchLoss = 0.6233, diffLoss = 3.0595, kgLoss = 0.0143
2025-04-08 15:24:44.958401: Training Step 265/354: batchLoss = 0.6032, diffLoss = 2.9518, kgLoss = 0.0161
2025-04-08 15:24:46.574497: Training Step 266/354: batchLoss = 0.5569, diffLoss = 2.7357, kgLoss = 0.0122
2025-04-08 15:24:48.198381: Training Step 267/354: batchLoss = 0.6384, diffLoss = 3.1329, kgLoss = 0.0148
2025-04-08 15:24:49.820032: Training Step 268/354: batchLoss = 0.6697, diffLoss = 3.2776, kgLoss = 0.0178
2025-04-08 15:24:51.441305: Training Step 269/354: batchLoss = 0.4723, diffLoss = 2.3081, kgLoss = 0.0134
2025-04-08 15:24:53.060726: Training Step 270/354: batchLoss = 0.5826, diffLoss = 2.8488, kgLoss = 0.0160
2025-04-08 15:24:54.679478: Training Step 271/354: batchLoss = 0.5266, diffLoss = 2.5768, kgLoss = 0.0140
2025-04-08 15:24:56.298786: Training Step 272/354: batchLoss = 0.6358, diffLoss = 3.1136, kgLoss = 0.0164
2025-04-08 15:24:57.908566: Training Step 273/354: batchLoss = 0.5403, diffLoss = 2.6364, kgLoss = 0.0163
2025-04-08 15:24:59.520685: Training Step 274/354: batchLoss = 0.4881, diffLoss = 2.3853, kgLoss = 0.0138
2025-04-08 15:25:01.135112: Training Step 275/354: batchLoss = 0.6026, diffLoss = 2.9559, kgLoss = 0.0143
2025-04-08 15:25:02.755563: Training Step 276/354: batchLoss = 0.5294, diffLoss = 2.5973, kgLoss = 0.0125
2025-04-08 15:25:04.369013: Training Step 277/354: batchLoss = 0.5654, diffLoss = 2.7699, kgLoss = 0.0143
2025-04-08 15:25:05.988810: Training Step 278/354: batchLoss = 0.6234, diffLoss = 3.0545, kgLoss = 0.0157
2025-04-08 15:25:07.602745: Training Step 279/354: batchLoss = 0.6324, diffLoss = 3.1057, kgLoss = 0.0141
2025-04-08 15:25:09.221825: Training Step 280/354: batchLoss = 0.5795, diffLoss = 2.8387, kgLoss = 0.0147
2025-04-08 15:25:10.837853: Training Step 281/354: batchLoss = 0.5411, diffLoss = 2.6517, kgLoss = 0.0135
2025-04-08 15:25:12.452167: Training Step 282/354: batchLoss = 0.5519, diffLoss = 2.7061, kgLoss = 0.0134
2025-04-08 15:25:14.064922: Training Step 283/354: batchLoss = 0.5604, diffLoss = 2.7519, kgLoss = 0.0125
2025-04-08 15:25:15.681357: Training Step 284/354: batchLoss = 0.4845, diffLoss = 2.3755, kgLoss = 0.0117
2025-04-08 15:25:17.297485: Training Step 285/354: batchLoss = 0.6447, diffLoss = 3.1538, kgLoss = 0.0174
2025-04-08 15:25:18.907804: Training Step 286/354: batchLoss = 0.6237, diffLoss = 3.0580, kgLoss = 0.0151
2025-04-08 15:25:20.521382: Training Step 287/354: batchLoss = 0.5414, diffLoss = 2.6569, kgLoss = 0.0125
2025-04-08 15:25:22.135941: Training Step 288/354: batchLoss = 0.5548, diffLoss = 2.7145, kgLoss = 0.0148
2025-04-08 15:25:23.760929: Training Step 289/354: batchLoss = 0.5506, diffLoss = 2.7018, kgLoss = 0.0128
2025-04-08 15:25:25.376269: Training Step 290/354: batchLoss = 0.4735, diffLoss = 2.3208, kgLoss = 0.0116
2025-04-08 15:25:26.992805: Training Step 291/354: batchLoss = 0.6051, diffLoss = 2.9672, kgLoss = 0.0145
2025-04-08 15:25:28.625039: Training Step 292/354: batchLoss = 0.6356, diffLoss = 3.1157, kgLoss = 0.0155
2025-04-08 15:25:30.250937: Training Step 293/354: batchLoss = 0.5587, diffLoss = 2.7380, kgLoss = 0.0139
2025-04-08 15:25:31.889322: Training Step 294/354: batchLoss = 0.6269, diffLoss = 3.0737, kgLoss = 0.0152
2025-04-08 15:25:33.529136: Training Step 295/354: batchLoss = 0.5490, diffLoss = 2.6917, kgLoss = 0.0133
2025-04-08 15:25:35.169318: Training Step 296/354: batchLoss = 0.5101, diffLoss = 2.5015, kgLoss = 0.0123
2025-04-08 15:25:36.811555: Training Step 297/354: batchLoss = 0.7617, diffLoss = 3.7326, kgLoss = 0.0190
2025-04-08 15:25:38.460864: Training Step 298/354: batchLoss = 0.5894, diffLoss = 2.8914, kgLoss = 0.0139
2025-04-08 15:25:40.092989: Training Step 299/354: batchLoss = 0.5433, diffLoss = 2.6678, kgLoss = 0.0121
2025-04-08 15:25:41.729529: Training Step 300/354: batchLoss = 0.6273, diffLoss = 3.0770, kgLoss = 0.0148
2025-04-08 15:25:43.378037: Training Step 301/354: batchLoss = 0.5765, diffLoss = 2.8202, kgLoss = 0.0155
2025-04-08 15:25:45.019574: Training Step 302/354: batchLoss = 0.6812, diffLoss = 3.3435, kgLoss = 0.0156
2025-04-08 15:25:46.656683: Training Step 303/354: batchLoss = 0.5282, diffLoss = 2.5896, kgLoss = 0.0129
2025-04-08 15:25:48.295984: Training Step 304/354: batchLoss = 0.5250, diffLoss = 2.5731, kgLoss = 0.0130
2025-04-08 15:25:49.934360: Training Step 305/354: batchLoss = 0.7825, diffLoss = 3.8331, kgLoss = 0.0199
2025-04-08 15:25:51.573158: Training Step 306/354: batchLoss = 0.5364, diffLoss = 2.6223, kgLoss = 0.0149
2025-04-08 15:25:53.217193: Training Step 307/354: batchLoss = 0.6259, diffLoss = 3.0668, kgLoss = 0.0157
2025-04-08 15:25:54.843116: Training Step 308/354: batchLoss = 0.5036, diffLoss = 2.4689, kgLoss = 0.0123
2025-04-08 15:25:56.490161: Training Step 309/354: batchLoss = 0.5130, diffLoss = 2.5057, kgLoss = 0.0149
2025-04-08 15:25:58.124938: Training Step 310/354: batchLoss = 0.5560, diffLoss = 2.7241, kgLoss = 0.0139
2025-04-08 15:25:59.754410: Training Step 311/354: batchLoss = 0.4773, diffLoss = 2.3348, kgLoss = 0.0129
2025-04-08 15:26:01.386168: Training Step 312/354: batchLoss = 0.5775, diffLoss = 2.8367, kgLoss = 0.0127
2025-04-08 15:26:03.014508: Training Step 313/354: batchLoss = 0.6173, diffLoss = 3.0257, kgLoss = 0.0152
2025-04-08 15:26:04.657304: Training Step 314/354: batchLoss = 0.6145, diffLoss = 3.0198, kgLoss = 0.0132
2025-04-08 15:26:06.282865: Training Step 315/354: batchLoss = 0.6807, diffLoss = 3.3361, kgLoss = 0.0169
2025-04-08 15:26:07.912122: Training Step 316/354: batchLoss = 0.5243, diffLoss = 2.5624, kgLoss = 0.0148
2025-04-08 15:26:09.537579: Training Step 317/354: batchLoss = 0.4663, diffLoss = 2.2827, kgLoss = 0.0122
2025-04-08 15:26:11.163677: Training Step 318/354: batchLoss = 0.5119, diffLoss = 2.5121, kgLoss = 0.0118
2025-04-08 15:26:12.789636: Training Step 319/354: batchLoss = 0.6846, diffLoss = 3.3531, kgLoss = 0.0175
2025-04-08 15:26:14.413443: Training Step 320/354: batchLoss = 0.7712, diffLoss = 3.7758, kgLoss = 0.0200
2025-04-08 15:26:16.042644: Training Step 321/354: batchLoss = 0.6164, diffLoss = 3.0242, kgLoss = 0.0144
2025-04-08 15:26:17.668579: Training Step 322/354: batchLoss = 0.6287, diffLoss = 3.0848, kgLoss = 0.0147
2025-04-08 15:26:19.298361: Training Step 323/354: batchLoss = 0.5745, diffLoss = 2.8165, kgLoss = 0.0140
2025-04-08 15:26:20.924287: Training Step 324/354: batchLoss = 0.4857, diffLoss = 2.3778, kgLoss = 0.0126
2025-04-08 15:26:22.551200: Training Step 325/354: batchLoss = 0.6554, diffLoss = 3.2116, kgLoss = 0.0164
2025-04-08 15:26:24.175531: Training Step 326/354: batchLoss = 0.4756, diffLoss = 2.3280, kgLoss = 0.0125
2025-04-08 15:26:25.796169: Training Step 327/354: batchLoss = 0.6121, diffLoss = 3.0031, kgLoss = 0.0143
2025-04-08 15:26:27.421028: Training Step 328/354: batchLoss = 0.7199, diffLoss = 3.5327, kgLoss = 0.0167
2025-04-08 15:26:29.041523: Training Step 329/354: batchLoss = 0.5059, diffLoss = 2.4775, kgLoss = 0.0130
2025-04-08 15:26:30.665179: Training Step 330/354: batchLoss = 0.5064, diffLoss = 2.4842, kgLoss = 0.0119
2025-04-08 15:26:32.301998: Training Step 331/354: batchLoss = 0.5279, diffLoss = 2.5874, kgLoss = 0.0130
2025-04-08 15:26:33.933611: Training Step 332/354: batchLoss = 0.6020, diffLoss = 2.9513, kgLoss = 0.0147
2025-04-08 15:26:35.565432: Training Step 333/354: batchLoss = 0.5525, diffLoss = 2.6933, kgLoss = 0.0173
2025-04-08 15:26:37.191259: Training Step 334/354: batchLoss = 0.4596, diffLoss = 2.2470, kgLoss = 0.0128
2025-04-08 15:26:38.823362: Training Step 335/354: batchLoss = 0.6190, diffLoss = 3.0273, kgLoss = 0.0170
2025-04-08 15:26:40.449850: Training Step 336/354: batchLoss = 0.5247, diffLoss = 2.5684, kgLoss = 0.0138
2025-04-08 15:26:42.076430: Training Step 337/354: batchLoss = 0.6166, diffLoss = 3.0194, kgLoss = 0.0159
2025-04-08 15:26:43.701974: Training Step 338/354: batchLoss = 0.5728, diffLoss = 2.8068, kgLoss = 0.0143
2025-04-08 15:26:45.322519: Training Step 339/354: batchLoss = 0.5740, diffLoss = 2.8119, kgLoss = 0.0145
2025-04-08 15:26:46.950918: Training Step 340/354: batchLoss = 0.5357, diffLoss = 2.6172, kgLoss = 0.0153
2025-04-08 15:26:48.585101: Training Step 341/354: batchLoss = 0.5588, diffLoss = 2.7378, kgLoss = 0.0140
2025-04-08 15:26:50.216957: Training Step 342/354: batchLoss = 0.5342, diffLoss = 2.6180, kgLoss = 0.0133
2025-04-08 15:26:51.846938: Training Step 343/354: batchLoss = 0.6231, diffLoss = 3.0623, kgLoss = 0.0133
2025-04-08 15:26:53.473606: Training Step 344/354: batchLoss = 0.4969, diffLoss = 2.4342, kgLoss = 0.0126
2025-04-08 15:26:55.097490: Training Step 345/354: batchLoss = 0.5100, diffLoss = 2.4905, kgLoss = 0.0148
2025-04-08 15:26:56.720092: Training Step 346/354: batchLoss = 0.5574, diffLoss = 2.7321, kgLoss = 0.0137
2025-04-08 15:26:58.342956: Training Step 347/354: batchLoss = 0.5185, diffLoss = 2.5363, kgLoss = 0.0140
2025-04-08 15:26:59.968322: Training Step 348/354: batchLoss = 0.9322, diffLoss = 4.5745, kgLoss = 0.0216
2025-04-08 15:27:01.597055: Training Step 349/354: batchLoss = 0.5103, diffLoss = 2.4965, kgLoss = 0.0138
2025-04-08 15:27:03.227460: Training Step 350/354: batchLoss = 0.4802, diffLoss = 2.3525, kgLoss = 0.0122
2025-04-08 15:27:04.851433: Training Step 351/354: batchLoss = 0.5147, diffLoss = 2.5227, kgLoss = 0.0127
2025-04-08 15:27:06.461150: Training Step 352/354: batchLoss = 0.6208, diffLoss = 3.0442, kgLoss = 0.0149
2025-04-08 15:27:07.870923: Training Step 353/354: batchLoss = 0.5938, diffLoss = 2.9086, kgLoss = 0.0150
2025-04-08 15:27:07.963171: 
2025-04-08 15:27:07.963816: Epoch 13/1000, Train: epLoss = 1.0407, epDfLoss = 5.0989, epfTransLoss = 0.0000, epKgLoss = 0.0262  
2025-04-08 15:27:09.288218: Steps 0/138: batch_recall = 47.06, batch_ndcg = 26.64 
2025-04-08 15:27:10.614454: Steps 1/138: batch_recall = 45.50, batch_ndcg = 27.64 
2025-04-08 15:27:11.925591: Steps 2/138: batch_recall = 56.94, batch_ndcg = 36.28 
2025-04-08 15:27:13.252104: Steps 3/138: batch_recall = 59.82, batch_ndcg = 34.40 
2025-04-08 15:27:14.559509: Steps 4/138: batch_recall = 67.81, batch_ndcg = 40.48 
2025-04-08 15:27:15.889367: Steps 5/138: batch_recall = 60.11, batch_ndcg = 32.68 
2025-04-08 15:27:17.205544: Steps 6/138: batch_recall = 50.69, batch_ndcg = 30.18 
2025-04-08 15:27:18.532025: Steps 7/138: batch_recall = 63.27, batch_ndcg = 40.62 
2025-04-08 15:27:19.835331: Steps 8/138: batch_recall = 62.07, batch_ndcg = 38.80 
2025-04-08 15:27:21.144675: Steps 9/138: batch_recall = 55.65, batch_ndcg = 34.27 
2025-04-08 15:27:22.462262: Steps 10/138: batch_recall = 54.80, batch_ndcg = 30.17 
2025-04-08 15:27:23.766168: Steps 11/138: batch_recall = 58.31, batch_ndcg = 33.87 
2025-04-08 15:27:25.077054: Steps 12/138: batch_recall = 52.71, batch_ndcg = 29.16 
2025-04-08 15:27:26.377332: Steps 13/138: batch_recall = 51.72, batch_ndcg = 30.10 
2025-04-08 15:27:27.673446: Steps 14/138: batch_recall = 52.11, batch_ndcg = 31.11 
2025-04-08 15:27:28.973425: Steps 15/138: batch_recall = 48.94, batch_ndcg = 29.50 
2025-04-08 15:27:30.274137: Steps 16/138: batch_recall = 60.27, batch_ndcg = 33.09 
2025-04-08 15:27:31.560782: Steps 17/138: batch_recall = 55.89, batch_ndcg = 31.89 
2025-04-08 15:27:32.866291: Steps 18/138: batch_recall = 50.07, batch_ndcg = 32.22 
2025-04-08 15:27:34.186435: Steps 19/138: batch_recall = 53.67, batch_ndcg = 31.95 
2025-04-08 15:27:35.511860: Steps 20/138: batch_recall = 59.60, batch_ndcg = 36.00 
2025-04-08 15:27:36.821181: Steps 21/138: batch_recall = 67.32, batch_ndcg = 40.14 
2025-04-08 15:27:38.129179: Steps 22/138: batch_recall = 55.78, batch_ndcg = 32.67 
2025-04-08 15:27:39.437486: Steps 23/138: batch_recall = 52.79, batch_ndcg = 30.37 
2025-04-08 15:27:40.734736: Steps 24/138: batch_recall = 56.21, batch_ndcg = 30.81 
2025-04-08 15:27:42.037287: Steps 25/138: batch_recall = 59.36, batch_ndcg = 34.68 
2025-04-08 15:27:43.336105: Steps 26/138: batch_recall = 56.22, batch_ndcg = 33.38 
2025-04-08 15:27:44.648780: Steps 27/138: batch_recall = 63.62, batch_ndcg = 35.39 
2025-04-08 15:27:45.958817: Steps 28/138: batch_recall = 56.80, batch_ndcg = 32.98 
2025-04-08 15:27:47.267084: Steps 29/138: batch_recall = 61.37, batch_ndcg = 31.82 
2025-04-08 15:27:48.570494: Steps 30/138: batch_recall = 57.23, batch_ndcg = 33.64 
2025-04-08 15:27:49.875768: Steps 31/138: batch_recall = 41.01, batch_ndcg = 23.90 
2025-04-08 15:27:51.158415: Steps 32/138: batch_recall = 51.18, batch_ndcg = 30.78 
2025-04-08 15:27:52.441134: Steps 33/138: batch_recall = 61.73, batch_ndcg = 34.16 
2025-04-08 15:27:53.744130: Steps 34/138: batch_recall = 54.56, batch_ndcg = 28.81 
2025-04-08 15:27:55.062089: Steps 35/138: batch_recall = 52.50, batch_ndcg = 30.32 
2025-04-08 15:27:56.352753: Steps 36/138: batch_recall = 51.47, batch_ndcg = 28.48 
2025-04-08 15:27:57.640949: Steps 37/138: batch_recall = 59.41, batch_ndcg = 34.43 
2025-04-08 15:27:58.938654: Steps 38/138: batch_recall = 57.47, batch_ndcg = 31.77 
2025-04-08 15:28:00.233843: Steps 39/138: batch_recall = 67.75, batch_ndcg = 37.35 
2025-04-08 15:28:01.528392: Steps 40/138: batch_recall = 53.81, batch_ndcg = 29.19 
2025-04-08 15:28:02.822236: Steps 41/138: batch_recall = 60.78, batch_ndcg = 33.23 
2025-04-08 15:28:04.117551: Steps 42/138: batch_recall = 56.79, batch_ndcg = 30.72 
2025-04-08 15:28:05.424381: Steps 43/138: batch_recall = 55.66, batch_ndcg = 32.72 
2025-04-08 15:28:06.725747: Steps 44/138: batch_recall = 56.85, batch_ndcg = 28.98 
2025-04-08 15:28:08.022725: Steps 45/138: batch_recall = 61.52, batch_ndcg = 34.48 
2025-04-08 15:28:09.309156: Steps 46/138: batch_recall = 61.15, batch_ndcg = 34.28 
2025-04-08 15:28:10.601390: Steps 47/138: batch_recall = 54.16, batch_ndcg = 32.09 
2025-04-08 15:28:11.888952: Steps 48/138: batch_recall = 55.75, batch_ndcg = 33.03 
2025-04-08 15:28:13.164156: Steps 49/138: batch_recall = 62.75, batch_ndcg = 36.52 
2025-04-08 15:28:14.447475: Steps 50/138: batch_recall = 59.58, batch_ndcg = 32.16 
2025-04-08 15:28:15.737597: Steps 51/138: batch_recall = 61.77, batch_ndcg = 35.70 
2025-04-08 15:28:17.031515: Steps 52/138: batch_recall = 66.74, batch_ndcg = 41.70 
2025-04-08 15:28:18.342297: Steps 53/138: batch_recall = 65.25, batch_ndcg = 33.33 
2025-04-08 15:28:19.635800: Steps 54/138: batch_recall = 64.91, batch_ndcg = 37.81 
2025-04-08 15:28:20.932928: Steps 55/138: batch_recall = 60.24, batch_ndcg = 33.79 
2025-04-08 15:28:22.220100: Steps 56/138: batch_recall = 62.12, batch_ndcg = 34.87 
2025-04-08 15:28:23.507364: Steps 57/138: batch_recall = 58.78, batch_ndcg = 33.59 
2025-04-08 15:28:24.789682: Steps 58/138: batch_recall = 70.48, batch_ndcg = 36.92 
2025-04-08 15:28:26.060784: Steps 59/138: batch_recall = 74.20, batch_ndcg = 42.39 
2025-04-08 15:28:27.333520: Steps 60/138: batch_recall = 72.41, batch_ndcg = 39.33 
2025-04-08 15:28:28.605760: Steps 61/138: batch_recall = 62.68, batch_ndcg = 34.95 
2025-04-08 15:28:29.898646: Steps 62/138: batch_recall = 83.77, batch_ndcg = 44.05 
2025-04-08 15:28:31.192761: Steps 63/138: batch_recall = 78.71, batch_ndcg = 44.44 
2025-04-08 15:28:32.493551: Steps 64/138: batch_recall = 65.70, batch_ndcg = 34.59 
2025-04-08 15:28:33.772528: Steps 65/138: batch_recall = 80.85, batch_ndcg = 45.86 
2025-04-08 15:28:35.067166: Steps 66/138: batch_recall = 69.84, batch_ndcg = 41.69 
2025-04-08 15:28:36.357859: Steps 67/138: batch_recall = 78.77, batch_ndcg = 47.78 
2025-04-08 15:28:37.656227: Steps 68/138: batch_recall = 66.28, batch_ndcg = 35.25 
2025-04-08 15:28:38.943358: Steps 69/138: batch_recall = 89.27, batch_ndcg = 50.97 
2025-04-08 15:28:40.222734: Steps 70/138: batch_recall = 80.81, batch_ndcg = 46.39 
2025-04-08 15:28:41.503847: Steps 71/138: batch_recall = 88.40, batch_ndcg = 51.56 
2025-04-08 15:28:42.793386: Steps 72/138: batch_recall = 87.53, batch_ndcg = 51.26 
2025-04-08 15:28:44.075008: Steps 73/138: batch_recall = 87.75, batch_ndcg = 48.96 
2025-04-08 15:28:45.362088: Steps 74/138: batch_recall = 82.52, batch_ndcg = 49.12 
2025-04-08 15:28:46.655052: Steps 75/138: batch_recall = 86.35, batch_ndcg = 50.87 
2025-04-08 15:28:47.937272: Steps 76/138: batch_recall = 97.70, batch_ndcg = 55.33 
2025-04-08 15:28:49.228287: Steps 77/138: batch_recall = 86.42, batch_ndcg = 49.30 
2025-04-08 15:28:50.530503: Steps 78/138: batch_recall = 93.52, batch_ndcg = 48.07 
2025-04-08 15:28:51.828923: Steps 79/138: batch_recall = 91.21, batch_ndcg = 48.57 
2025-04-08 15:28:53.117763: Steps 80/138: batch_recall = 73.04, batch_ndcg = 38.75 
2025-04-08 15:28:54.406193: Steps 81/138: batch_recall = 82.96, batch_ndcg = 48.90 
2025-04-08 15:28:55.687994: Steps 82/138: batch_recall = 88.90, batch_ndcg = 51.74 
2025-04-08 15:28:56.966701: Steps 83/138: batch_recall = 85.74, batch_ndcg = 48.48 
2025-04-08 15:28:58.235429: Steps 84/138: batch_recall = 101.92, batch_ndcg = 57.16 
2025-04-08 15:28:59.504261: Steps 85/138: batch_recall = 101.41, batch_ndcg = 59.86 
2025-04-08 15:29:00.791695: Steps 86/138: batch_recall = 116.55, batch_ndcg = 69.70 
2025-04-08 15:29:02.081495: Steps 87/138: batch_recall = 108.25, batch_ndcg = 57.38 
2025-04-08 15:29:03.369976: Steps 88/138: batch_recall = 103.30, batch_ndcg = 57.22 
2025-04-08 15:29:04.658782: Steps 89/138: batch_recall = 117.19, batch_ndcg = 66.44 
2025-04-08 15:29:05.943516: Steps 90/138: batch_recall = 104.23, batch_ndcg = 58.34 
2025-04-08 15:29:07.230191: Steps 91/138: batch_recall = 117.69, batch_ndcg = 66.46 
2025-04-08 15:29:08.522167: Steps 92/138: batch_recall = 113.35, batch_ndcg = 61.99 
2025-04-08 15:29:09.810924: Steps 93/138: batch_recall = 119.49, batch_ndcg = 67.76 
2025-04-08 15:29:11.096364: Steps 94/138: batch_recall = 119.03, batch_ndcg = 63.03 
2025-04-08 15:29:12.379200: Steps 95/138: batch_recall = 115.56, batch_ndcg = 68.33 
2025-04-08 15:29:13.655575: Steps 96/138: batch_recall = 134.72, batch_ndcg = 74.57 
2025-04-08 15:29:14.933406: Steps 97/138: batch_recall = 146.55, batch_ndcg = 83.17 
2025-04-08 15:29:16.224686: Steps 98/138: batch_recall = 110.60, batch_ndcg = 62.84 
2025-04-08 15:29:17.505324: Steps 99/138: batch_recall = 124.17, batch_ndcg = 70.41 
2025-04-08 15:29:18.783918: Steps 100/138: batch_recall = 130.41, batch_ndcg = 72.23 
2025-04-08 15:29:20.072778: Steps 101/138: batch_recall = 129.72, batch_ndcg = 69.98 
2025-04-08 15:29:21.354246: Steps 102/138: batch_recall = 125.55, batch_ndcg = 74.04 
2025-04-08 15:29:22.645541: Steps 103/138: batch_recall = 143.91, batch_ndcg = 81.40 
2025-04-08 15:29:23.930543: Steps 104/138: batch_recall = 136.94, batch_ndcg = 76.53 
2025-04-08 15:29:25.209328: Steps 105/138: batch_recall = 118.51, batch_ndcg = 67.54 
2025-04-08 15:29:26.482062: Steps 106/138: batch_recall = 104.54, batch_ndcg = 58.39 
2025-04-08 15:29:27.761265: Steps 107/138: batch_recall = 115.97, batch_ndcg = 65.04 
2025-04-08 15:29:29.047421: Steps 108/138: batch_recall = 121.18, batch_ndcg = 71.68 
2025-04-08 15:29:30.326195: Steps 109/138: batch_recall = 135.41, batch_ndcg = 76.20 
2025-04-08 15:29:31.615068: Steps 110/138: batch_recall = 126.28, batch_ndcg = 65.43 
2025-04-08 15:29:32.892697: Steps 111/138: batch_recall = 140.09, batch_ndcg = 84.44 
2025-04-08 15:29:34.173035: Steps 112/138: batch_recall = 167.87, batch_ndcg = 92.05 
2025-04-08 15:29:35.456147: Steps 113/138: batch_recall = 129.82, batch_ndcg = 72.99 
2025-04-08 15:29:36.747654: Steps 114/138: batch_recall = 125.74, batch_ndcg = 70.67 
2025-04-08 15:29:38.025808: Steps 115/138: batch_recall = 124.29, batch_ndcg = 64.02 
2025-04-08 15:29:39.311321: Steps 116/138: batch_recall = 126.15, batch_ndcg = 65.77 
2025-04-08 15:29:40.589116: Steps 117/138: batch_recall = 116.01, batch_ndcg = 67.21 
2025-04-08 15:29:41.870418: Steps 118/138: batch_recall = 122.62, batch_ndcg = 71.77 
2025-04-08 15:29:43.152549: Steps 119/138: batch_recall = 137.61, batch_ndcg = 74.01 
2025-04-08 15:29:44.433806: Steps 120/138: batch_recall = 121.66, batch_ndcg = 68.06 
2025-04-08 15:29:45.709026: Steps 121/138: batch_recall = 143.82, batch_ndcg = 76.81 
2025-04-08 15:29:46.989991: Steps 122/138: batch_recall = 146.86, batch_ndcg = 80.88 
2025-04-08 15:29:48.265503: Steps 123/138: batch_recall = 129.37, batch_ndcg = 71.43 
2025-04-08 15:29:49.542028: Steps 124/138: batch_recall = 153.40, batch_ndcg = 95.73 
2025-04-08 15:29:50.814177: Steps 125/138: batch_recall = 131.95, batch_ndcg = 71.68 
2025-04-08 15:29:52.088278: Steps 126/138: batch_recall = 159.22, batch_ndcg = 88.48 
2025-04-08 15:29:53.373122: Steps 127/138: batch_recall = 143.26, batch_ndcg = 80.69 
2025-04-08 15:29:54.660989: Steps 128/138: batch_recall = 125.60, batch_ndcg = 69.07 
2025-04-08 15:29:55.953201: Steps 129/138: batch_recall = 161.82, batch_ndcg = 90.39 
2025-04-08 15:29:57.256817: Steps 130/138: batch_recall = 134.83, batch_ndcg = 69.73 
2025-04-08 15:29:58.544083: Steps 131/138: batch_recall = 149.20, batch_ndcg = 84.24 
2025-04-08 15:29:59.827614: Steps 132/138: batch_recall = 152.33, batch_ndcg = 86.47 
2025-04-08 15:30:01.100745: Steps 133/138: batch_recall = 144.18, batch_ndcg = 82.98 
2025-04-08 15:30:02.381583: Steps 134/138: batch_recall = 138.29, batch_ndcg = 78.59 
2025-04-08 15:30:03.656426: Steps 135/138: batch_recall = 164.03, batch_ndcg = 93.56 
2025-04-08 15:30:04.938217: Steps 136/138: batch_recall = 152.95, batch_ndcg = 78.56 
2025-04-08 15:30:06.213973: Steps 137/138: batch_recall = 138.93, batch_ndcg = 86.20 
2025-04-08 15:30:06.214543: Epoch 13/1000, Test: Recall = 0.1765, NDCG = 0.0997  

2025-04-08 15:30:07.989168: Training Step 0/354: batchLoss = 0.5919, diffLoss = 2.8986, kgLoss = 0.0152
2025-04-08 15:30:09.611834: Training Step 1/354: batchLoss = 0.5895, diffLoss = 2.8956, kgLoss = 0.0129
2025-04-08 15:30:11.229572: Training Step 2/354: batchLoss = 0.5669, diffLoss = 2.7691, kgLoss = 0.0164
2025-04-08 15:30:12.850590: Training Step 3/354: batchLoss = 0.4882, diffLoss = 2.3941, kgLoss = 0.0118
2025-04-08 15:30:14.472234: Training Step 4/354: batchLoss = 0.4529, diffLoss = 2.2242, kgLoss = 0.0100
2025-04-08 15:30:16.097556: Training Step 5/354: batchLoss = 0.5301, diffLoss = 2.5985, kgLoss = 0.0131
2025-04-08 15:30:17.721466: Training Step 6/354: batchLoss = 0.4813, diffLoss = 2.3539, kgLoss = 0.0132
2025-04-08 15:30:19.347641: Training Step 7/354: batchLoss = 0.5415, diffLoss = 2.6509, kgLoss = 0.0141
2025-04-08 15:30:20.966668: Training Step 8/354: batchLoss = 0.5703, diffLoss = 2.7890, kgLoss = 0.0156
2025-04-08 15:30:22.581679: Training Step 9/354: batchLoss = 0.5515, diffLoss = 2.6956, kgLoss = 0.0154
2025-04-08 15:30:24.202642: Training Step 10/354: batchLoss = 0.4886, diffLoss = 2.3887, kgLoss = 0.0135
2025-04-08 15:30:25.823981: Training Step 11/354: batchLoss = 0.5979, diffLoss = 2.9322, kgLoss = 0.0144
2025-04-08 15:30:27.442497: Training Step 12/354: batchLoss = 0.5591, diffLoss = 2.7436, kgLoss = 0.0130
2025-04-08 15:30:29.060709: Training Step 13/354: batchLoss = 0.7801, diffLoss = 3.8257, kgLoss = 0.0187
2025-04-08 15:30:30.680003: Training Step 14/354: batchLoss = 0.6087, diffLoss = 2.9835, kgLoss = 0.0150
2025-04-08 15:30:32.296438: Training Step 15/354: batchLoss = 0.5073, diffLoss = 2.4734, kgLoss = 0.0158
2025-04-08 15:30:33.914316: Training Step 16/354: batchLoss = 0.6274, diffLoss = 3.0754, kgLoss = 0.0154
2025-04-08 15:30:35.538034: Training Step 17/354: batchLoss = 0.5778, diffLoss = 2.8307, kgLoss = 0.0146
2025-04-08 15:30:37.152767: Training Step 18/354: batchLoss = 0.5689, diffLoss = 2.7919, kgLoss = 0.0132
2025-04-08 15:30:38.774293: Training Step 19/354: batchLoss = 0.6562, diffLoss = 3.2129, kgLoss = 0.0170
2025-04-08 15:30:40.391454: Training Step 20/354: batchLoss = 0.4762, diffLoss = 2.3283, kgLoss = 0.0131
2025-04-08 15:30:42.009741: Training Step 21/354: batchLoss = 0.5994, diffLoss = 2.9394, kgLoss = 0.0144
2025-04-08 15:30:43.632276: Training Step 22/354: batchLoss = 0.5872, diffLoss = 2.8824, kgLoss = 0.0134
2025-04-08 15:30:45.258408: Training Step 23/354: batchLoss = 0.5984, diffLoss = 2.9281, kgLoss = 0.0160
2025-04-08 15:30:46.884555: Training Step 24/354: batchLoss = 0.5234, diffLoss = 2.5588, kgLoss = 0.0145
2025-04-08 15:30:48.510373: Training Step 25/354: batchLoss = 0.6549, diffLoss = 3.2024, kgLoss = 0.0180
2025-04-08 15:30:50.128296: Training Step 26/354: batchLoss = 0.6030, diffLoss = 2.9593, kgLoss = 0.0139
2025-04-08 15:30:51.744864: Training Step 27/354: batchLoss = 0.5171, diffLoss = 2.5313, kgLoss = 0.0135
2025-04-08 15:30:53.364174: Training Step 28/354: batchLoss = 0.6195, diffLoss = 3.0349, kgLoss = 0.0156
2025-04-08 15:30:54.984119: Training Step 29/354: batchLoss = 0.7937, diffLoss = 3.8909, kgLoss = 0.0194
2025-04-08 15:30:56.600998: Training Step 30/354: batchLoss = 0.5858, diffLoss = 2.8701, kgLoss = 0.0147
2025-04-08 15:30:58.221427: Training Step 31/354: batchLoss = 0.5437, diffLoss = 2.6693, kgLoss = 0.0124
2025-04-08 15:30:59.840241: Training Step 32/354: batchLoss = 0.6061, diffLoss = 2.9643, kgLoss = 0.0166
2025-04-08 15:31:01.460750: Training Step 33/354: batchLoss = 0.5887, diffLoss = 2.8841, kgLoss = 0.0148
2025-04-08 15:31:03.081492: Training Step 34/354: batchLoss = 0.6469, diffLoss = 3.1673, kgLoss = 0.0168
2025-04-08 15:31:04.699872: Training Step 35/354: batchLoss = 0.5400, diffLoss = 2.6484, kgLoss = 0.0129
2025-04-08 15:31:06.325742: Training Step 36/354: batchLoss = 0.5378, diffLoss = 2.6416, kgLoss = 0.0119
2025-04-08 15:31:07.941288: Training Step 37/354: batchLoss = 0.5296, diffLoss = 2.5894, kgLoss = 0.0146
2025-04-08 15:31:09.562980: Training Step 38/354: batchLoss = 0.6413, diffLoss = 3.1486, kgLoss = 0.0145
2025-04-08 15:31:11.182939: Training Step 39/354: batchLoss = 0.5943, diffLoss = 2.9148, kgLoss = 0.0142
2025-04-08 15:31:12.802696: Training Step 40/354: batchLoss = 0.4859, diffLoss = 2.3821, kgLoss = 0.0118
2025-04-08 15:31:14.418902: Training Step 41/354: batchLoss = 0.6926, diffLoss = 3.3944, kgLoss = 0.0171
2025-04-08 15:31:16.034255: Training Step 42/354: batchLoss = 0.6137, diffLoss = 3.0112, kgLoss = 0.0143
2025-04-08 15:31:17.649008: Training Step 43/354: batchLoss = 0.4861, diffLoss = 2.3765, kgLoss = 0.0135
2025-04-08 15:31:19.269203: Training Step 44/354: batchLoss = 0.5302, diffLoss = 2.5944, kgLoss = 0.0142
2025-04-08 15:31:20.891019: Training Step 45/354: batchLoss = 0.5872, diffLoss = 2.8615, kgLoss = 0.0187
2025-04-08 15:31:22.500293: Training Step 46/354: batchLoss = 0.5721, diffLoss = 2.8036, kgLoss = 0.0142
2025-04-08 15:31:24.118108: Training Step 47/354: batchLoss = 0.6597, diffLoss = 3.2312, kgLoss = 0.0168
2025-04-08 15:31:25.741406: Training Step 48/354: batchLoss = 0.5812, diffLoss = 2.8489, kgLoss = 0.0142
2025-04-08 15:31:27.375043: Training Step 49/354: batchLoss = 0.5164, diffLoss = 2.5217, kgLoss = 0.0151
2025-04-08 15:31:29.010102: Training Step 50/354: batchLoss = 0.8272, diffLoss = 4.0628, kgLoss = 0.0183
2025-04-08 15:31:30.640029: Training Step 51/354: batchLoss = 0.7464, diffLoss = 3.6658, kgLoss = 0.0165
2025-04-08 15:31:32.268917: Training Step 52/354: batchLoss = 0.6345, diffLoss = 3.1141, kgLoss = 0.0147
2025-04-08 15:31:33.891028: Training Step 53/354: batchLoss = 0.5783, diffLoss = 2.8310, kgLoss = 0.0151
2025-04-08 15:31:35.523021: Training Step 54/354: batchLoss = 0.5460, diffLoss = 2.6717, kgLoss = 0.0146
2025-04-08 15:31:37.145736: Training Step 55/354: batchLoss = 0.5506, diffLoss = 2.6980, kgLoss = 0.0138
2025-04-08 15:31:38.763892: Training Step 56/354: batchLoss = 0.5139, diffLoss = 2.5215, kgLoss = 0.0119
2025-04-08 15:31:40.381975: Training Step 57/354: batchLoss = 0.6737, diffLoss = 3.3039, kgLoss = 0.0161
2025-04-08 15:31:42.000964: Training Step 58/354: batchLoss = 0.5779, diffLoss = 2.8352, kgLoss = 0.0136
2025-04-08 15:31:43.623118: Training Step 59/354: batchLoss = 0.5953, diffLoss = 2.9167, kgLoss = 0.0150
2025-04-08 15:31:45.253979: Training Step 60/354: batchLoss = 0.4998, diffLoss = 2.4509, kgLoss = 0.0120
2025-04-08 15:31:46.876892: Training Step 61/354: batchLoss = 0.6638, diffLoss = 3.2573, kgLoss = 0.0154
2025-04-08 15:31:48.506062: Training Step 62/354: batchLoss = 0.6789, diffLoss = 3.3243, kgLoss = 0.0176
2025-04-08 15:31:50.132728: Training Step 63/354: batchLoss = 0.6081, diffLoss = 2.9678, kgLoss = 0.0182
2025-04-08 15:31:51.764419: Training Step 64/354: batchLoss = 0.5722, diffLoss = 2.8038, kgLoss = 0.0143
2025-04-08 15:31:53.386314: Training Step 65/354: batchLoss = 0.5821, diffLoss = 2.8580, kgLoss = 0.0131
2025-04-08 15:31:55.008512: Training Step 66/354: batchLoss = 0.5282, diffLoss = 2.5912, kgLoss = 0.0125
2025-04-08 15:31:56.624266: Training Step 67/354: batchLoss = 0.4962, diffLoss = 2.4375, kgLoss = 0.0109
2025-04-08 15:31:58.264058: Training Step 68/354: batchLoss = 0.5814, diffLoss = 2.8467, kgLoss = 0.0150
2025-04-08 15:31:59.890921: Training Step 69/354: batchLoss = 0.5174, diffLoss = 2.5386, kgLoss = 0.0121
2025-04-08 15:32:01.515090: Training Step 70/354: batchLoss = 0.4966, diffLoss = 2.4346, kgLoss = 0.0121
2025-04-08 15:32:03.139104: Training Step 71/354: batchLoss = 0.6348, diffLoss = 3.1078, kgLoss = 0.0165
2025-04-08 15:32:04.774928: Training Step 72/354: batchLoss = 0.6245, diffLoss = 3.0628, kgLoss = 0.0150
2025-04-08 15:32:06.413640: Training Step 73/354: batchLoss = 0.6652, diffLoss = 3.2587, kgLoss = 0.0168
2025-04-08 15:32:08.029704: Training Step 74/354: batchLoss = 0.6031, diffLoss = 2.9557, kgLoss = 0.0149
2025-04-08 15:32:09.649446: Training Step 75/354: batchLoss = 0.7673, diffLoss = 3.7689, kgLoss = 0.0169
2025-04-08 15:32:11.273058: Training Step 76/354: batchLoss = 0.5435, diffLoss = 2.6618, kgLoss = 0.0140
2025-04-08 15:32:12.896009: Training Step 77/354: batchLoss = 0.4425, diffLoss = 2.1578, kgLoss = 0.0137
2025-04-08 15:32:14.519693: Training Step 78/354: batchLoss = 0.6313, diffLoss = 3.0950, kgLoss = 0.0154
2025-04-08 15:32:16.143611: Training Step 79/354: batchLoss = 0.6239, diffLoss = 3.0606, kgLoss = 0.0147
2025-04-08 15:32:17.769236: Training Step 80/354: batchLoss = 0.4873, diffLoss = 2.3824, kgLoss = 0.0136
2025-04-08 15:32:19.396752: Training Step 81/354: batchLoss = 0.5648, diffLoss = 2.7723, kgLoss = 0.0129
2025-04-08 15:32:21.020385: Training Step 82/354: batchLoss = 0.5943, diffLoss = 2.9141, kgLoss = 0.0144
2025-04-08 15:32:22.637659: Training Step 83/354: batchLoss = 0.5878, diffLoss = 2.8730, kgLoss = 0.0165
2025-04-08 15:32:24.251390: Training Step 84/354: batchLoss = 0.6233, diffLoss = 3.0572, kgLoss = 0.0148
2025-04-08 15:32:25.873872: Training Step 85/354: batchLoss = 0.4941, diffLoss = 2.4169, kgLoss = 0.0134
2025-04-08 15:32:27.498583: Training Step 86/354: batchLoss = 0.4873, diffLoss = 2.3866, kgLoss = 0.0125
2025-04-08 15:32:29.128275: Training Step 87/354: batchLoss = 0.5839, diffLoss = 2.8685, kgLoss = 0.0127
2025-04-08 15:32:30.752953: Training Step 88/354: batchLoss = 0.7222, diffLoss = 3.5406, kgLoss = 0.0177
2025-04-08 15:32:32.371774: Training Step 89/354: batchLoss = 0.5163, diffLoss = 2.5291, kgLoss = 0.0132
2025-04-08 15:32:33.991824: Training Step 90/354: batchLoss = 0.6243, diffLoss = 3.0563, kgLoss = 0.0163
2025-04-08 15:32:35.619424: Training Step 91/354: batchLoss = 0.4529, diffLoss = 2.2209, kgLoss = 0.0109
2025-04-08 15:32:37.236050: Training Step 92/354: batchLoss = 0.5405, diffLoss = 2.6377, kgLoss = 0.0161
2025-04-08 15:32:38.851835: Training Step 93/354: batchLoss = 0.5387, diffLoss = 2.6363, kgLoss = 0.0143
2025-04-08 15:32:40.475098: Training Step 94/354: batchLoss = 0.6070, diffLoss = 2.9822, kgLoss = 0.0132
2025-04-08 15:32:42.106877: Training Step 95/354: batchLoss = 0.7803, diffLoss = 3.8317, kgLoss = 0.0175
2025-04-08 15:32:43.724655: Training Step 96/354: batchLoss = 0.5895, diffLoss = 2.8854, kgLoss = 0.0155
2025-04-08 15:32:45.349457: Training Step 97/354: batchLoss = 0.5447, diffLoss = 2.6747, kgLoss = 0.0122
2025-04-08 15:32:46.965651: Training Step 98/354: batchLoss = 0.5847, diffLoss = 2.8699, kgLoss = 0.0134
2025-04-08 15:32:48.584594: Training Step 99/354: batchLoss = 0.4905, diffLoss = 2.3852, kgLoss = 0.0169
2025-04-08 15:32:50.209167: Training Step 100/354: batchLoss = 0.5002, diffLoss = 2.4455, kgLoss = 0.0138
2025-04-08 15:32:51.831700: Training Step 101/354: batchLoss = 0.5767, diffLoss = 2.8246, kgLoss = 0.0147
2025-04-08 15:32:53.451449: Training Step 102/354: batchLoss = 0.5643, diffLoss = 2.7673, kgLoss = 0.0135
2025-04-08 15:32:55.072770: Training Step 103/354: batchLoss = 0.6223, diffLoss = 3.0432, kgLoss = 0.0171
2025-04-08 15:32:56.699880: Training Step 104/354: batchLoss = 0.6051, diffLoss = 2.9630, kgLoss = 0.0157
2025-04-08 15:32:58.333202: Training Step 105/354: batchLoss = 0.5534, diffLoss = 2.7010, kgLoss = 0.0166
2025-04-08 15:32:59.957740: Training Step 106/354: batchLoss = 0.6498, diffLoss = 3.1834, kgLoss = 0.0164
2025-04-08 15:33:01.583066: Training Step 107/354: batchLoss = 0.5627, diffLoss = 2.7585, kgLoss = 0.0137
2025-04-08 15:33:03.204611: Training Step 108/354: batchLoss = 0.6744, diffLoss = 3.3065, kgLoss = 0.0164
2025-04-08 15:33:04.825271: Training Step 109/354: batchLoss = 0.5218, diffLoss = 2.5594, kgLoss = 0.0124
2025-04-08 15:33:06.449883: Training Step 110/354: batchLoss = 0.5556, diffLoss = 2.6741, kgLoss = 0.0260
2025-04-08 15:33:08.066180: Training Step 111/354: batchLoss = 0.5106, diffLoss = 2.4928, kgLoss = 0.0151
2025-04-08 15:33:09.690443: Training Step 112/354: batchLoss = 0.5397, diffLoss = 2.6366, kgLoss = 0.0154
2025-04-08 15:33:11.315154: Training Step 113/354: batchLoss = 0.5935, diffLoss = 2.9028, kgLoss = 0.0162
2025-04-08 15:33:12.937014: Training Step 114/354: batchLoss = 0.6788, diffLoss = 3.3251, kgLoss = 0.0172
2025-04-08 15:33:14.564745: Training Step 115/354: batchLoss = 0.5162, diffLoss = 2.5246, kgLoss = 0.0141
2025-04-08 15:33:16.185021: Training Step 116/354: batchLoss = 0.5625, diffLoss = 2.7627, kgLoss = 0.0125
2025-04-08 15:33:17.808532: Training Step 117/354: batchLoss = 0.6115, diffLoss = 2.9947, kgLoss = 0.0157
2025-04-08 15:33:19.437839: Training Step 118/354: batchLoss = 0.5193, diffLoss = 2.5446, kgLoss = 0.0130
2025-04-08 15:33:21.055931: Training Step 119/354: batchLoss = 0.6773, diffLoss = 3.3206, kgLoss = 0.0165
2025-04-08 15:33:22.678805: Training Step 120/354: batchLoss = 0.4800, diffLoss = 2.3466, kgLoss = 0.0133
2025-04-08 15:33:24.305377: Training Step 121/354: batchLoss = 0.4906, diffLoss = 2.4049, kgLoss = 0.0120
2025-04-08 15:33:25.933042: Training Step 122/354: batchLoss = 0.5553, diffLoss = 2.7167, kgLoss = 0.0150
2025-04-08 15:33:27.564236: Training Step 123/354: batchLoss = 0.4326, diffLoss = 2.1162, kgLoss = 0.0117
2025-04-08 15:33:29.187391: Training Step 124/354: batchLoss = 0.7457, diffLoss = 3.6523, kgLoss = 0.0191
2025-04-08 15:33:30.815078: Training Step 125/354: batchLoss = 0.6148, diffLoss = 3.0181, kgLoss = 0.0140
2025-04-08 15:33:32.438387: Training Step 126/354: batchLoss = 0.5262, diffLoss = 2.5730, kgLoss = 0.0144
2025-04-08 15:33:34.058670: Training Step 127/354: batchLoss = 0.4542, diffLoss = 2.2285, kgLoss = 0.0106
2025-04-08 15:33:35.675849: Training Step 128/354: batchLoss = 0.5708, diffLoss = 2.7924, kgLoss = 0.0154
2025-04-08 15:33:37.299354: Training Step 129/354: batchLoss = 0.4955, diffLoss = 2.4229, kgLoss = 0.0136
2025-04-08 15:33:38.918523: Training Step 130/354: batchLoss = 0.5381, diffLoss = 2.6359, kgLoss = 0.0137
2025-04-08 15:33:40.542953: Training Step 131/354: batchLoss = 0.5897, diffLoss = 2.8868, kgLoss = 0.0154
2025-04-08 15:33:42.164827: Training Step 132/354: batchLoss = 0.6602, diffLoss = 3.2358, kgLoss = 0.0163
2025-04-08 15:33:43.794046: Training Step 133/354: batchLoss = 0.6748, diffLoss = 3.3055, kgLoss = 0.0171
2025-04-08 15:33:45.418354: Training Step 134/354: batchLoss = 0.6294, diffLoss = 3.0820, kgLoss = 0.0163
2025-04-08 15:33:47.043659: Training Step 135/354: batchLoss = 0.6984, diffLoss = 3.4269, kgLoss = 0.0163
2025-04-08 15:33:48.663986: Training Step 136/354: batchLoss = 0.5016, diffLoss = 2.4583, kgLoss = 0.0124
2025-04-08 15:33:50.286625: Training Step 137/354: batchLoss = 0.5372, diffLoss = 2.6236, kgLoss = 0.0156
2025-04-08 15:33:51.906513: Training Step 138/354: batchLoss = 0.4837, diffLoss = 2.3707, kgLoss = 0.0119
2025-04-08 15:33:53.528617: Training Step 139/354: batchLoss = 0.6915, diffLoss = 3.3897, kgLoss = 0.0170
2025-04-08 15:33:55.164450: Training Step 140/354: batchLoss = 0.4665, diffLoss = 2.2835, kgLoss = 0.0123
2025-04-08 15:33:56.787472: Training Step 141/354: batchLoss = 0.5177, diffLoss = 2.5330, kgLoss = 0.0139
2025-04-08 15:33:58.418607: Training Step 142/354: batchLoss = 0.5836, diffLoss = 2.8642, kgLoss = 0.0134
2025-04-08 15:34:00.035067: Training Step 143/354: batchLoss = 0.6708, diffLoss = 3.2892, kgLoss = 0.0162
2025-04-08 15:34:01.656759: Training Step 144/354: batchLoss = 0.5592, diffLoss = 2.7393, kgLoss = 0.0141
2025-04-08 15:34:03.285894: Training Step 145/354: batchLoss = 0.4681, diffLoss = 2.2899, kgLoss = 0.0126
2025-04-08 15:34:04.913851: Training Step 146/354: batchLoss = 0.4845, diffLoss = 2.3730, kgLoss = 0.0124
2025-04-08 15:34:06.536193: Training Step 147/354: batchLoss = 0.7383, diffLoss = 3.6193, kgLoss = 0.0181
2025-04-08 15:34:08.157584: Training Step 148/354: batchLoss = 0.6553, diffLoss = 3.2159, kgLoss = 0.0151
2025-04-08 15:34:09.781932: Training Step 149/354: batchLoss = 0.5048, diffLoss = 2.4677, kgLoss = 0.0140
2025-04-08 15:34:11.400805: Training Step 150/354: batchLoss = 0.6910, diffLoss = 3.3957, kgLoss = 0.0148
2025-04-08 15:34:13.024333: Training Step 151/354: batchLoss = 0.5553, diffLoss = 2.7182, kgLoss = 0.0145
2025-04-08 15:34:14.650561: Training Step 152/354: batchLoss = 0.5415, diffLoss = 2.6480, kgLoss = 0.0149
2025-04-08 15:34:16.281057: Training Step 153/354: batchLoss = 0.5462, diffLoss = 2.6791, kgLoss = 0.0130
2025-04-08 15:34:17.909423: Training Step 154/354: batchLoss = 0.5170, diffLoss = 2.5359, kgLoss = 0.0123
2025-04-08 15:34:19.532910: Training Step 155/354: batchLoss = 0.5727, diffLoss = 2.8094, kgLoss = 0.0136
2025-04-08 15:34:21.148178: Training Step 156/354: batchLoss = 0.5627, diffLoss = 2.7531, kgLoss = 0.0151
2025-04-08 15:34:22.758066: Training Step 157/354: batchLoss = 0.5850, diffLoss = 2.8624, kgLoss = 0.0157
2025-04-08 15:34:24.385912: Training Step 158/354: batchLoss = 0.5168, diffLoss = 2.5289, kgLoss = 0.0138
2025-04-08 15:34:26.016303: Training Step 159/354: batchLoss = 0.5126, diffLoss = 2.5127, kgLoss = 0.0125
2025-04-08 15:34:27.645812: Training Step 160/354: batchLoss = 0.4850, diffLoss = 2.3763, kgLoss = 0.0122
2025-04-08 15:34:29.269180: Training Step 161/354: batchLoss = 0.5876, diffLoss = 2.8871, kgLoss = 0.0127
2025-04-08 15:34:30.888333: Training Step 162/354: batchLoss = 0.7914, diffLoss = 3.8859, kgLoss = 0.0178
2025-04-08 15:34:32.514897: Training Step 163/354: batchLoss = 0.7727, diffLoss = 3.6875, kgLoss = 0.0440
2025-04-08 15:34:34.135207: Training Step 164/354: batchLoss = 0.5750, diffLoss = 2.8168, kgLoss = 0.0145
2025-04-08 15:34:35.754039: Training Step 165/354: batchLoss = 0.5609, diffLoss = 2.7470, kgLoss = 0.0144
2025-04-08 15:34:37.367919: Training Step 166/354: batchLoss = 0.5255, diffLoss = 2.5686, kgLoss = 0.0148
2025-04-08 15:34:38.999352: Training Step 167/354: batchLoss = 0.4540, diffLoss = 2.2195, kgLoss = 0.0126
2025-04-08 15:34:40.624755: Training Step 168/354: batchLoss = 0.5825, diffLoss = 2.8530, kgLoss = 0.0149
2025-04-08 15:34:42.246794: Training Step 169/354: batchLoss = 0.6133, diffLoss = 3.0055, kgLoss = 0.0152
2025-04-08 15:34:43.860580: Training Step 170/354: batchLoss = 0.6881, diffLoss = 3.3734, kgLoss = 0.0168
2025-04-08 15:34:45.486839: Training Step 171/354: batchLoss = 0.5994, diffLoss = 2.9340, kgLoss = 0.0157
2025-04-08 15:34:47.105957: Training Step 172/354: batchLoss = 0.6629, diffLoss = 3.2449, kgLoss = 0.0174
2025-04-08 15:34:48.726524: Training Step 173/354: batchLoss = 0.5913, diffLoss = 2.8939, kgLoss = 0.0156
2025-04-08 15:34:50.344905: Training Step 174/354: batchLoss = 0.6890, diffLoss = 3.3835, kgLoss = 0.0154
2025-04-08 15:34:51.958994: Training Step 175/354: batchLoss = 0.6179, diffLoss = 3.0257, kgLoss = 0.0160
2025-04-08 15:34:53.586378: Training Step 176/354: batchLoss = 0.6057, diffLoss = 2.9690, kgLoss = 0.0148
2025-04-08 15:34:55.209780: Training Step 177/354: batchLoss = 0.5786, diffLoss = 2.8370, kgLoss = 0.0140
2025-04-08 15:34:56.831523: Training Step 178/354: batchLoss = 0.5020, diffLoss = 2.4594, kgLoss = 0.0127
2025-04-08 15:34:58.460475: Training Step 179/354: batchLoss = 0.5515, diffLoss = 2.6994, kgLoss = 0.0146
2025-04-08 15:35:00.084156: Training Step 180/354: batchLoss = 0.5819, diffLoss = 2.8572, kgLoss = 0.0131
2025-04-08 15:35:01.704611: Training Step 181/354: batchLoss = 0.5693, diffLoss = 2.7877, kgLoss = 0.0148
2025-04-08 15:35:03.320160: Training Step 182/354: batchLoss = 0.4745, diffLoss = 2.3252, kgLoss = 0.0119
2025-04-08 15:35:04.937704: Training Step 183/354: batchLoss = 0.5616, diffLoss = 2.7484, kgLoss = 0.0149
2025-04-08 15:35:06.559661: Training Step 184/354: batchLoss = 0.6764, diffLoss = 3.3204, kgLoss = 0.0154
2025-04-08 15:35:08.184322: Training Step 185/354: batchLoss = 0.5067, diffLoss = 2.4868, kgLoss = 0.0117
2025-04-08 15:35:09.808769: Training Step 186/354: batchLoss = 0.5706, diffLoss = 2.7883, kgLoss = 0.0162
2025-04-08 15:35:11.437105: Training Step 187/354: batchLoss = 0.5149, diffLoss = 2.5168, kgLoss = 0.0144
2025-04-08 15:35:13.063886: Training Step 188/354: batchLoss = 0.5203, diffLoss = 2.5477, kgLoss = 0.0135
2025-04-08 15:35:14.687889: Training Step 189/354: batchLoss = 0.5666, diffLoss = 2.7784, kgLoss = 0.0137
2025-04-08 15:35:16.320497: Training Step 190/354: batchLoss = 0.4602, diffLoss = 2.2570, kgLoss = 0.0110
2025-04-08 15:35:17.942998: Training Step 191/354: batchLoss = 0.5033, diffLoss = 2.4676, kgLoss = 0.0122
2025-04-08 15:35:19.567875: Training Step 192/354: batchLoss = 0.6240, diffLoss = 3.0531, kgLoss = 0.0168
2025-04-08 15:35:21.187305: Training Step 193/354: batchLoss = 0.5196, diffLoss = 2.5405, kgLoss = 0.0144
2025-04-08 15:35:22.816553: Training Step 194/354: batchLoss = 0.6467, diffLoss = 3.1611, kgLoss = 0.0180
2025-04-08 15:35:24.443261: Training Step 195/354: batchLoss = 0.5993, diffLoss = 2.9275, kgLoss = 0.0173
2025-04-08 15:35:26.068051: Training Step 196/354: batchLoss = 0.6082, diffLoss = 2.9752, kgLoss = 0.0165
2025-04-08 15:35:27.691493: Training Step 197/354: batchLoss = 0.5753, diffLoss = 2.8121, kgLoss = 0.0160
2025-04-08 15:35:29.327296: Training Step 198/354: batchLoss = 0.6629, diffLoss = 3.2488, kgLoss = 0.0165
2025-04-08 15:35:30.954766: Training Step 199/354: batchLoss = 0.4628, diffLoss = 2.2637, kgLoss = 0.0126
2025-04-08 15:35:32.574235: Training Step 200/354: batchLoss = 0.5918, diffLoss = 2.8969, kgLoss = 0.0155
2025-04-08 15:35:34.190221: Training Step 201/354: batchLoss = 0.5554, diffLoss = 2.7177, kgLoss = 0.0148
2025-04-08 15:35:35.812459: Training Step 202/354: batchLoss = 0.7052, diffLoss = 3.4541, kgLoss = 0.0179
2025-04-08 15:35:37.437626: Training Step 203/354: batchLoss = 0.5022, diffLoss = 2.4616, kgLoss = 0.0124
2025-04-08 15:35:39.068543: Training Step 204/354: batchLoss = 0.5700, diffLoss = 2.7946, kgLoss = 0.0138
2025-04-08 15:35:40.696547: Training Step 205/354: batchLoss = 0.4817, diffLoss = 2.3592, kgLoss = 0.0123
2025-04-08 15:35:42.324146: Training Step 206/354: batchLoss = 0.5713, diffLoss = 2.8047, kgLoss = 0.0130
2025-04-08 15:35:43.943035: Training Step 207/354: batchLoss = 0.5824, diffLoss = 2.8590, kgLoss = 0.0132
2025-04-08 15:35:45.571516: Training Step 208/354: batchLoss = 0.4806, diffLoss = 2.3546, kgLoss = 0.0121
2025-04-08 15:35:47.198889: Training Step 209/354: batchLoss = 0.5053, diffLoss = 2.4745, kgLoss = 0.0130
2025-04-08 15:35:48.815944: Training Step 210/354: batchLoss = 0.5250, diffLoss = 2.5692, kgLoss = 0.0139
2025-04-08 15:35:50.427613: Training Step 211/354: batchLoss = 0.6842, diffLoss = 3.3538, kgLoss = 0.0168
2025-04-08 15:35:52.050476: Training Step 212/354: batchLoss = 0.6192, diffLoss = 3.0332, kgLoss = 0.0157
2025-04-08 15:35:53.669301: Training Step 213/354: batchLoss = 0.5646, diffLoss = 2.7626, kgLoss = 0.0151
2025-04-08 15:35:55.287991: Training Step 214/354: batchLoss = 0.4686, diffLoss = 2.2931, kgLoss = 0.0125
2025-04-08 15:35:56.910974: Training Step 215/354: batchLoss = 0.5478, diffLoss = 2.6850, kgLoss = 0.0136
2025-04-08 15:35:58.532458: Training Step 216/354: batchLoss = 0.4959, diffLoss = 2.4267, kgLoss = 0.0132
2025-04-08 15:36:00.155347: Training Step 217/354: batchLoss = 0.6704, diffLoss = 3.2910, kgLoss = 0.0153
2025-04-08 15:36:01.775241: Training Step 218/354: batchLoss = 0.4863, diffLoss = 2.3806, kgLoss = 0.0128
2025-04-08 15:36:03.396755: Training Step 219/354: batchLoss = 0.5733, diffLoss = 2.8149, kgLoss = 0.0129
2025-04-08 15:36:05.016336: Training Step 220/354: batchLoss = 0.6447, diffLoss = 3.1619, kgLoss = 0.0154
2025-04-08 15:36:06.639075: Training Step 221/354: batchLoss = 0.6521, diffLoss = 3.1967, kgLoss = 0.0159
2025-04-08 15:36:08.264022: Training Step 222/354: batchLoss = 0.7337, diffLoss = 3.5963, kgLoss = 0.0180
2025-04-08 15:36:09.891614: Training Step 223/354: batchLoss = 0.7663, diffLoss = 3.7633, kgLoss = 0.0170
2025-04-08 15:36:11.513746: Training Step 224/354: batchLoss = 0.5723, diffLoss = 2.8039, kgLoss = 0.0144
2025-04-08 15:36:13.134114: Training Step 225/354: batchLoss = 0.4968, diffLoss = 2.4284, kgLoss = 0.0139
2025-04-08 15:36:14.758000: Training Step 226/354: batchLoss = 0.7694, diffLoss = 3.7748, kgLoss = 0.0180
2025-04-08 15:36:16.373102: Training Step 227/354: batchLoss = 0.5481, diffLoss = 2.6735, kgLoss = 0.0167
2025-04-08 15:36:17.993653: Training Step 228/354: batchLoss = 0.6692, diffLoss = 3.2816, kgLoss = 0.0162
2025-04-08 15:36:19.610921: Training Step 229/354: batchLoss = 0.5135, diffLoss = 2.5119, kgLoss = 0.0139
2025-04-08 15:36:21.231040: Training Step 230/354: batchLoss = 0.5622, diffLoss = 2.7604, kgLoss = 0.0127
2025-04-08 15:36:22.857460: Training Step 231/354: batchLoss = 0.6665, diffLoss = 3.2695, kgLoss = 0.0157
2025-04-08 15:36:24.481319: Training Step 232/354: batchLoss = 0.5668, diffLoss = 2.7820, kgLoss = 0.0130
2025-04-08 15:36:26.108880: Training Step 233/354: batchLoss = 0.4888, diffLoss = 2.3992, kgLoss = 0.0113
2025-04-08 15:36:27.730617: Training Step 234/354: batchLoss = 0.6183, diffLoss = 3.0382, kgLoss = 0.0133
2025-04-08 15:36:29.349621: Training Step 235/354: batchLoss = 0.4992, diffLoss = 2.4461, kgLoss = 0.0124
2025-04-08 15:36:30.969804: Training Step 236/354: batchLoss = 0.5328, diffLoss = 2.6070, kgLoss = 0.0143
2025-04-08 15:36:32.590712: Training Step 237/354: batchLoss = 0.6874, diffLoss = 3.3675, kgLoss = 0.0174
2025-04-08 15:36:34.208377: Training Step 238/354: batchLoss = 0.6487, diffLoss = 3.1780, kgLoss = 0.0164
2025-04-08 15:36:35.828724: Training Step 239/354: batchLoss = 0.5842, diffLoss = 2.8640, kgLoss = 0.0142
2025-04-08 15:36:37.451157: Training Step 240/354: batchLoss = 0.6705, diffLoss = 3.2929, kgLoss = 0.0149
2025-04-08 15:36:39.079224: Training Step 241/354: batchLoss = 0.5439, diffLoss = 2.6559, kgLoss = 0.0159
2025-04-08 15:36:40.698974: Training Step 242/354: batchLoss = 0.5413, diffLoss = 2.6539, kgLoss = 0.0131
2025-04-08 15:36:42.318800: Training Step 243/354: batchLoss = 0.5302, diffLoss = 2.6012, kgLoss = 0.0125
2025-04-08 15:36:43.944338: Training Step 244/354: batchLoss = 0.6937, diffLoss = 3.4030, kgLoss = 0.0163
2025-04-08 15:36:45.558977: Training Step 245/354: batchLoss = 0.5626, diffLoss = 2.7476, kgLoss = 0.0164
2025-04-08 15:36:47.175579: Training Step 246/354: batchLoss = 0.5569, diffLoss = 2.7244, kgLoss = 0.0150
2025-04-08 15:36:48.793420: Training Step 247/354: batchLoss = 0.4789, diffLoss = 2.3410, kgLoss = 0.0133
2025-04-08 15:36:50.413101: Training Step 248/354: batchLoss = 0.5284, diffLoss = 2.5882, kgLoss = 0.0134
2025-04-08 15:36:52.035684: Training Step 249/354: batchLoss = 0.5601, diffLoss = 2.7406, kgLoss = 0.0149
2025-04-08 15:36:53.654096: Training Step 250/354: batchLoss = 0.7211, diffLoss = 3.5352, kgLoss = 0.0176
2025-04-08 15:36:55.283348: Training Step 251/354: batchLoss = 0.5694, diffLoss = 2.7940, kgLoss = 0.0132
2025-04-08 15:36:56.904732: Training Step 252/354: batchLoss = 0.6293, diffLoss = 3.0866, kgLoss = 0.0150
2025-04-08 15:36:58.522347: Training Step 253/354: batchLoss = 0.5185, diffLoss = 2.5389, kgLoss = 0.0134
2025-04-08 15:37:00.142396: Training Step 254/354: batchLoss = 0.6462, diffLoss = 3.1586, kgLoss = 0.0181
2025-04-08 15:37:01.766310: Training Step 255/354: batchLoss = 0.5208, diffLoss = 2.5511, kgLoss = 0.0132
2025-04-08 15:37:03.390549: Training Step 256/354: batchLoss = 0.4953, diffLoss = 2.4204, kgLoss = 0.0140
2025-04-08 15:37:05.021583: Training Step 257/354: batchLoss = 0.5562, diffLoss = 2.7232, kgLoss = 0.0144
2025-04-08 15:37:06.642936: Training Step 258/354: batchLoss = 0.6178, diffLoss = 3.0342, kgLoss = 0.0137
2025-04-08 15:37:08.265480: Training Step 259/354: batchLoss = 0.6094, diffLoss = 2.9779, kgLoss = 0.0172
2025-04-08 15:37:09.894629: Training Step 260/354: batchLoss = 1.1786, diffLoss = 5.7796, kgLoss = 0.0284
2025-04-08 15:37:11.524161: Training Step 261/354: batchLoss = 0.5923, diffLoss = 2.8984, kgLoss = 0.0157
2025-04-08 15:37:13.154706: Training Step 262/354: batchLoss = 0.6445, diffLoss = 3.1587, kgLoss = 0.0160
2025-04-08 15:37:14.774698: Training Step 263/354: batchLoss = 0.5412, diffLoss = 2.6511, kgLoss = 0.0138
2025-04-08 15:37:16.395843: Training Step 264/354: batchLoss = 0.5537, diffLoss = 2.7112, kgLoss = 0.0143
2025-04-08 15:37:18.013399: Training Step 265/354: batchLoss = 0.5451, diffLoss = 2.6717, kgLoss = 0.0134
2025-04-08 15:37:19.635433: Training Step 266/354: batchLoss = 0.4718, diffLoss = 2.2948, kgLoss = 0.0161
2025-04-08 15:37:21.260405: Training Step 267/354: batchLoss = 0.5502, diffLoss = 2.6936, kgLoss = 0.0143
2025-04-08 15:37:22.883961: Training Step 268/354: batchLoss = 0.6303, diffLoss = 3.0970, kgLoss = 0.0136
2025-04-08 15:37:24.509249: Training Step 269/354: batchLoss = 0.6017, diffLoss = 2.9537, kgLoss = 0.0137
2025-04-08 15:37:26.130674: Training Step 270/354: batchLoss = 0.5499, diffLoss = 2.6905, kgLoss = 0.0147
2025-04-08 15:37:27.759063: Training Step 271/354: batchLoss = 0.5077, diffLoss = 2.4845, kgLoss = 0.0135
2025-04-08 15:37:29.384429: Training Step 272/354: batchLoss = 0.6447, diffLoss = 3.1705, kgLoss = 0.0132
2025-04-08 15:37:31.006905: Training Step 273/354: batchLoss = 0.5096, diffLoss = 2.4891, kgLoss = 0.0147
2025-04-08 15:37:32.631928: Training Step 274/354: batchLoss = 0.5350, diffLoss = 2.6245, kgLoss = 0.0127
2025-04-08 15:37:34.247277: Training Step 275/354: batchLoss = 0.7675, diffLoss = 3.7586, kgLoss = 0.0197
2025-04-08 15:37:35.880949: Training Step 276/354: batchLoss = 0.5304, diffLoss = 2.5890, kgLoss = 0.0158
2025-04-08 15:37:37.502688: Training Step 277/354: batchLoss = 0.5978, diffLoss = 2.9302, kgLoss = 0.0147
2025-04-08 15:37:39.127554: Training Step 278/354: batchLoss = 0.5177, diffLoss = 2.5361, kgLoss = 0.0131
2025-04-08 15:37:40.754391: Training Step 279/354: batchLoss = 0.5663, diffLoss = 2.7754, kgLoss = 0.0140
2025-04-08 15:37:42.367985: Training Step 280/354: batchLoss = 0.6121, diffLoss = 2.9965, kgLoss = 0.0160
2025-04-08 15:37:43.992285: Training Step 281/354: batchLoss = 0.4992, diffLoss = 2.4515, kgLoss = 0.0112
2025-04-08 15:37:45.613294: Training Step 282/354: batchLoss = 0.5336, diffLoss = 2.6165, kgLoss = 0.0129
2025-04-08 15:37:47.231016: Training Step 283/354: batchLoss = 0.5059, diffLoss = 2.4792, kgLoss = 0.0125
2025-04-08 15:37:48.836600: Training Step 284/354: batchLoss = 0.5881, diffLoss = 2.8832, kgLoss = 0.0144
2025-04-08 15:37:50.454993: Training Step 285/354: batchLoss = 0.6304, diffLoss = 3.0890, kgLoss = 0.0157
2025-04-08 15:37:52.067672: Training Step 286/354: batchLoss = 0.5935, diffLoss = 2.9103, kgLoss = 0.0143
2025-04-08 15:37:53.693066: Training Step 287/354: batchLoss = 0.5907, diffLoss = 2.8991, kgLoss = 0.0136
2025-04-08 15:37:55.324288: Training Step 288/354: batchLoss = 0.5587, diffLoss = 2.7351, kgLoss = 0.0146
2025-04-08 15:37:56.949904: Training Step 289/354: batchLoss = 0.5507, diffLoss = 2.6968, kgLoss = 0.0142
2025-04-08 15:37:58.582997: Training Step 290/354: batchLoss = 0.5173, diffLoss = 2.5319, kgLoss = 0.0137
2025-04-08 15:38:00.203733: Training Step 291/354: batchLoss = 0.5249, diffLoss = 2.5691, kgLoss = 0.0139
2025-04-08 15:38:01.824432: Training Step 292/354: batchLoss = 0.5143, diffLoss = 2.5203, kgLoss = 0.0128
2025-04-08 15:38:03.444934: Training Step 293/354: batchLoss = 0.7348, diffLoss = 3.6042, kgLoss = 0.0175
2025-04-08 15:38:05.068508: Training Step 294/354: batchLoss = 0.5596, diffLoss = 2.7452, kgLoss = 0.0132
2025-04-08 15:38:06.693064: Training Step 295/354: batchLoss = 0.6102, diffLoss = 2.9853, kgLoss = 0.0164
2025-04-08 15:38:08.315333: Training Step 296/354: batchLoss = 0.6467, diffLoss = 3.1729, kgLoss = 0.0152
2025-04-08 15:38:09.938678: Training Step 297/354: batchLoss = 0.7352, diffLoss = 3.6152, kgLoss = 0.0152
2025-04-08 15:38:11.561119: Training Step 298/354: batchLoss = 0.6956, diffLoss = 3.4075, kgLoss = 0.0176
2025-04-08 15:38:13.181351: Training Step 299/354: batchLoss = 0.5812, diffLoss = 2.8432, kgLoss = 0.0157
2025-04-08 15:38:14.822373: Training Step 300/354: batchLoss = 0.5606, diffLoss = 2.7517, kgLoss = 0.0128
2025-04-08 15:38:16.442150: Training Step 301/354: batchLoss = 0.7142, diffLoss = 3.5080, kgLoss = 0.0157
2025-04-08 15:38:18.063140: Training Step 302/354: batchLoss = 0.4326, diffLoss = 2.1210, kgLoss = 0.0105
2025-04-08 15:38:19.688465: Training Step 303/354: batchLoss = 0.6013, diffLoss = 2.9464, kgLoss = 0.0151
2025-04-08 15:38:21.304978: Training Step 304/354: batchLoss = 0.5107, diffLoss = 2.5094, kgLoss = 0.0111
2025-04-08 15:38:22.933185: Training Step 305/354: batchLoss = 0.6661, diffLoss = 3.2599, kgLoss = 0.0177
2025-04-08 15:38:24.557518: Training Step 306/354: batchLoss = 0.7304, diffLoss = 3.5837, kgLoss = 0.0171
2025-04-08 15:38:26.185501: Training Step 307/354: batchLoss = 0.6175, diffLoss = 3.0205, kgLoss = 0.0167
2025-04-08 15:38:27.812182: Training Step 308/354: batchLoss = 0.6061, diffLoss = 2.9706, kgLoss = 0.0150
2025-04-08 15:38:29.442847: Training Step 309/354: batchLoss = 0.6421, diffLoss = 3.1448, kgLoss = 0.0164
2025-04-08 15:38:31.057471: Training Step 310/354: batchLoss = 0.5582, diffLoss = 2.7362, kgLoss = 0.0137
2025-04-08 15:38:32.672686: Training Step 311/354: batchLoss = 0.5258, diffLoss = 2.5751, kgLoss = 0.0134
2025-04-08 15:38:34.292408: Training Step 312/354: batchLoss = 0.6956, diffLoss = 3.3983, kgLoss = 0.0199
2025-04-08 15:38:35.927290: Training Step 313/354: batchLoss = 0.5886, diffLoss = 2.8876, kgLoss = 0.0138
2025-04-08 15:38:37.554701: Training Step 314/354: batchLoss = 0.7502, diffLoss = 3.6738, kgLoss = 0.0193
2025-04-08 15:38:39.178675: Training Step 315/354: batchLoss = 0.9450, diffLoss = 4.6440, kgLoss = 0.0202
2025-04-08 15:38:40.802207: Training Step 316/354: batchLoss = 0.6155, diffLoss = 3.0096, kgLoss = 0.0170
2025-04-08 15:38:42.422901: Training Step 317/354: batchLoss = 0.6418, diffLoss = 3.1480, kgLoss = 0.0152
2025-04-08 15:38:44.048723: Training Step 318/354: batchLoss = 0.5301, diffLoss = 2.5919, kgLoss = 0.0147
2025-04-08 15:38:45.670921: Training Step 319/354: batchLoss = 0.5433, diffLoss = 2.6549, kgLoss = 0.0154
2025-04-08 15:38:47.281499: Training Step 320/354: batchLoss = 0.4884, diffLoss = 2.3889, kgLoss = 0.0133
2025-04-08 15:38:48.899096: Training Step 321/354: batchLoss = 0.4802, diffLoss = 2.3495, kgLoss = 0.0129
2025-04-08 15:38:50.522868: Training Step 322/354: batchLoss = 0.5137, diffLoss = 2.5106, kgLoss = 0.0145
2025-04-08 15:38:52.153525: Training Step 323/354: batchLoss = 0.5271, diffLoss = 2.5907, kgLoss = 0.0112
2025-04-08 15:38:53.773240: Training Step 324/354: batchLoss = 0.5027, diffLoss = 2.4602, kgLoss = 0.0133
2025-04-08 15:38:55.401260: Training Step 325/354: batchLoss = 0.5052, diffLoss = 2.4714, kgLoss = 0.0136
2025-04-08 15:38:57.037398: Training Step 326/354: batchLoss = 0.5948, diffLoss = 2.9183, kgLoss = 0.0139
2025-04-08 15:38:58.657121: Training Step 327/354: batchLoss = 0.5878, diffLoss = 2.8807, kgLoss = 0.0145
2025-04-08 15:39:00.278720: Training Step 328/354: batchLoss = 0.6439, diffLoss = 3.1606, kgLoss = 0.0147
2025-04-08 15:39:01.897199: Training Step 329/354: batchLoss = 0.5445, diffLoss = 2.6658, kgLoss = 0.0141
2025-04-08 15:39:03.522681: Training Step 330/354: batchLoss = 0.4882, diffLoss = 2.3864, kgLoss = 0.0137
2025-04-08 15:39:05.156083: Training Step 331/354: batchLoss = 0.7055, diffLoss = 3.4517, kgLoss = 0.0189
2025-04-08 15:39:06.788397: Training Step 332/354: batchLoss = 0.5410, diffLoss = 2.6516, kgLoss = 0.0134
2025-04-08 15:39:08.414131: Training Step 333/354: batchLoss = 0.6339, diffLoss = 3.1031, kgLoss = 0.0166
2025-04-08 15:39:10.043735: Training Step 334/354: batchLoss = 0.5559, diffLoss = 2.7211, kgLoss = 0.0146
2025-04-08 15:39:11.672622: Training Step 335/354: batchLoss = 0.5548, diffLoss = 2.7114, kgLoss = 0.0157
2025-04-08 15:39:13.297862: Training Step 336/354: batchLoss = 0.5961, diffLoss = 2.9204, kgLoss = 0.0150
2025-04-08 15:39:14.919454: Training Step 337/354: batchLoss = 0.6219, diffLoss = 3.0490, kgLoss = 0.0151
2025-04-08 15:39:16.533905: Training Step 338/354: batchLoss = 0.6647, diffLoss = 3.2669, kgLoss = 0.0141
2025-04-08 15:39:18.156384: Training Step 339/354: batchLoss = 0.6242, diffLoss = 3.0573, kgLoss = 0.0159
2025-04-08 15:39:19.782908: Training Step 340/354: batchLoss = 0.4643, diffLoss = 2.2762, kgLoss = 0.0113
2025-04-08 15:39:21.398836: Training Step 341/354: batchLoss = 0.5318, diffLoss = 2.6013, kgLoss = 0.0144
2025-04-08 15:39:23.026684: Training Step 342/354: batchLoss = 0.6114, diffLoss = 2.9974, kgLoss = 0.0149
2025-04-08 15:39:24.654331: Training Step 343/354: batchLoss = 0.6618, diffLoss = 3.2374, kgLoss = 0.0179
2025-04-08 15:39:26.299408: Training Step 344/354: batchLoss = 0.4889, diffLoss = 2.3976, kgLoss = 0.0117
2025-04-08 15:39:27.927600: Training Step 345/354: batchLoss = 0.6175, diffLoss = 3.0245, kgLoss = 0.0158
2025-04-08 15:39:29.544373: Training Step 346/354: batchLoss = 0.6209, diffLoss = 3.0456, kgLoss = 0.0147
2025-04-08 15:39:31.160663: Training Step 347/354: batchLoss = 0.6121, diffLoss = 2.9994, kgLoss = 0.0152
2025-04-08 15:39:32.782800: Training Step 348/354: batchLoss = 0.4147, diffLoss = 2.0288, kgLoss = 0.0112
2025-04-08 15:39:34.395963: Training Step 349/354: batchLoss = 0.5799, diffLoss = 2.8414, kgLoss = 0.0145
2025-04-08 15:39:36.020311: Training Step 350/354: batchLoss = 0.5853, diffLoss = 2.8619, kgLoss = 0.0162
2025-04-08 15:39:37.639338: Training Step 351/354: batchLoss = 0.4914, diffLoss = 2.4075, kgLoss = 0.0124
2025-04-08 15:39:39.239116: Training Step 352/354: batchLoss = 0.5495, diffLoss = 2.6977, kgLoss = 0.0124
2025-04-08 15:39:40.645322: Training Step 353/354: batchLoss = 0.4564, diffLoss = 2.2313, kgLoss = 0.0127
2025-04-08 15:39:40.735168: 
2025-04-08 15:39:40.735814: Epoch 14/1000, Train: epLoss = 1.0301, epDfLoss = 5.0461, epfTransLoss = 0.0000, epKgLoss = 0.0262  
2025-04-08 15:39:42.063744: Steps 0/138: batch_recall = 47.21, batch_ndcg = 26.51 
2025-04-08 15:39:43.393799: Steps 1/138: batch_recall = 47.23, batch_ndcg = 27.95 
2025-04-08 15:39:44.685177: Steps 2/138: batch_recall = 58.34, batch_ndcg = 36.82 
2025-04-08 15:39:45.993905: Steps 3/138: batch_recall = 58.60, batch_ndcg = 33.96 
2025-04-08 15:39:47.281478: Steps 4/138: batch_recall = 67.92, batch_ndcg = 40.73 
2025-04-08 15:39:48.574251: Steps 5/138: batch_recall = 58.52, batch_ndcg = 32.46 
2025-04-08 15:39:49.878619: Steps 6/138: batch_recall = 51.07, batch_ndcg = 30.57 
2025-04-08 15:39:51.191106: Steps 7/138: batch_recall = 63.67, batch_ndcg = 40.85 
2025-04-08 15:39:52.512884: Steps 8/138: batch_recall = 63.42, batch_ndcg = 39.63 
2025-04-08 15:39:53.839898: Steps 9/138: batch_recall = 56.32, batch_ndcg = 34.23 
2025-04-08 15:39:55.147722: Steps 10/138: batch_recall = 53.73, batch_ndcg = 29.90 
2025-04-08 15:39:56.454209: Steps 11/138: batch_recall = 58.10, batch_ndcg = 33.78 
2025-04-08 15:39:57.754701: Steps 12/138: batch_recall = 54.60, batch_ndcg = 29.57 
2025-04-08 15:39:59.059058: Steps 13/138: batch_recall = 51.97, batch_ndcg = 30.24 
2025-04-08 15:40:00.363046: Steps 14/138: batch_recall = 53.79, batch_ndcg = 31.33 
2025-04-08 15:40:01.648232: Steps 15/138: batch_recall = 49.23, batch_ndcg = 29.34 
2025-04-08 15:40:02.940808: Steps 16/138: batch_recall = 58.87, batch_ndcg = 32.70 
2025-04-08 15:40:04.228705: Steps 17/138: batch_recall = 55.47, batch_ndcg = 31.80 
2025-04-08 15:40:05.535293: Steps 18/138: batch_recall = 51.95, batch_ndcg = 32.82 
2025-04-08 15:40:06.843922: Steps 19/138: batch_recall = 53.70, batch_ndcg = 32.08 
2025-04-08 15:40:08.148869: Steps 20/138: batch_recall = 60.92, batch_ndcg = 36.36 
2025-04-08 15:40:09.453185: Steps 21/138: batch_recall = 66.84, batch_ndcg = 39.76 
2025-04-08 15:40:10.749346: Steps 22/138: batch_recall = 55.40, batch_ndcg = 32.84 
2025-04-08 15:40:12.056968: Steps 23/138: batch_recall = 52.98, batch_ndcg = 30.19 
2025-04-08 15:40:13.351340: Steps 24/138: batch_recall = 55.98, batch_ndcg = 30.33 
2025-04-08 15:40:14.649076: Steps 25/138: batch_recall = 60.28, batch_ndcg = 35.56 
2025-04-08 15:40:15.944533: Steps 26/138: batch_recall = 56.37, batch_ndcg = 33.19 
2025-04-08 15:40:17.237950: Steps 27/138: batch_recall = 62.12, batch_ndcg = 35.24 
2025-04-08 15:40:18.532277: Steps 28/138: batch_recall = 58.10, batch_ndcg = 33.46 
2025-04-08 15:40:19.831445: Steps 29/138: batch_recall = 59.20, batch_ndcg = 31.05 
2025-04-08 15:40:21.150970: Steps 30/138: batch_recall = 57.55, batch_ndcg = 33.85 
2025-04-08 15:40:22.448422: Steps 31/138: batch_recall = 40.79, batch_ndcg = 23.86 
2025-04-08 15:40:23.736780: Steps 32/138: batch_recall = 51.91, batch_ndcg = 30.75 
2025-04-08 15:40:25.036676: Steps 33/138: batch_recall = 60.67, batch_ndcg = 34.10 
2025-04-08 15:40:26.338878: Steps 34/138: batch_recall = 54.06, batch_ndcg = 28.96 
2025-04-08 15:40:27.646624: Steps 35/138: batch_recall = 54.66, batch_ndcg = 30.71 
2025-04-08 15:40:28.942117: Steps 36/138: batch_recall = 52.97, batch_ndcg = 28.82 
2025-04-08 15:40:30.226961: Steps 37/138: batch_recall = 59.47, batch_ndcg = 34.75 
2025-04-08 15:40:31.516282: Steps 38/138: batch_recall = 58.14, batch_ndcg = 31.81 
2025-04-08 15:40:32.802545: Steps 39/138: batch_recall = 66.37, batch_ndcg = 37.13 
2025-04-08 15:40:34.089592: Steps 40/138: batch_recall = 53.63, batch_ndcg = 29.04 
2025-04-08 15:40:35.377249: Steps 41/138: batch_recall = 62.63, batch_ndcg = 33.72 
2025-04-08 15:40:36.667872: Steps 42/138: batch_recall = 57.10, batch_ndcg = 30.57 
2025-04-08 15:40:37.962860: Steps 43/138: batch_recall = 54.30, batch_ndcg = 32.63 
2025-04-08 15:40:39.257887: Steps 44/138: batch_recall = 57.41, batch_ndcg = 29.74 
2025-04-08 15:40:40.551372: Steps 45/138: batch_recall = 64.52, batch_ndcg = 35.54 
2025-04-08 15:40:41.844046: Steps 46/138: batch_recall = 61.55, batch_ndcg = 35.25 
2025-04-08 15:40:43.123705: Steps 47/138: batch_recall = 52.12, batch_ndcg = 31.68 
2025-04-08 15:40:44.414948: Steps 48/138: batch_recall = 56.35, batch_ndcg = 33.28 
2025-04-08 15:40:45.687925: Steps 49/138: batch_recall = 64.25, batch_ndcg = 36.78 
2025-04-08 15:40:46.969741: Steps 50/138: batch_recall = 60.31, batch_ndcg = 32.36 
2025-04-08 15:40:48.253354: Steps 51/138: batch_recall = 62.33, batch_ndcg = 35.76 
2025-04-08 15:40:49.541613: Steps 52/138: batch_recall = 66.91, batch_ndcg = 42.31 
2025-04-08 15:40:50.841274: Steps 53/138: batch_recall = 67.52, batch_ndcg = 34.14 
2025-04-08 15:40:52.131485: Steps 54/138: batch_recall = 64.06, batch_ndcg = 37.13 
2025-04-08 15:40:53.409912: Steps 55/138: batch_recall = 62.66, batch_ndcg = 34.58 
2025-04-08 15:40:54.704114: Steps 56/138: batch_recall = 63.26, batch_ndcg = 34.86 
2025-04-08 15:40:55.997852: Steps 57/138: batch_recall = 58.04, batch_ndcg = 33.37 
2025-04-08 15:40:57.303805: Steps 58/138: batch_recall = 69.74, batch_ndcg = 37.06 
2025-04-08 15:40:58.590276: Steps 59/138: batch_recall = 75.49, batch_ndcg = 43.24 
2025-04-08 15:40:59.877378: Steps 60/138: batch_recall = 71.08, batch_ndcg = 39.04 
2025-04-08 15:41:01.159106: Steps 61/138: batch_recall = 63.27, batch_ndcg = 35.41 
2025-04-08 15:41:02.448534: Steps 62/138: batch_recall = 83.57, batch_ndcg = 43.73 
2025-04-08 15:41:03.729819: Steps 63/138: batch_recall = 76.80, batch_ndcg = 44.13 
2025-04-08 15:41:05.012345: Steps 64/138: batch_recall = 65.11, batch_ndcg = 34.02 
2025-04-08 15:41:06.296219: Steps 65/138: batch_recall = 80.49, batch_ndcg = 45.71 
2025-04-08 15:41:07.584939: Steps 66/138: batch_recall = 67.99, batch_ndcg = 40.79 
2025-04-08 15:41:08.867197: Steps 67/138: batch_recall = 78.17, batch_ndcg = 47.32 
2025-04-08 15:41:10.158613: Steps 68/138: batch_recall = 66.28, batch_ndcg = 34.88 
2025-04-08 15:41:11.446608: Steps 69/138: batch_recall = 88.60, batch_ndcg = 51.28 
2025-04-08 15:41:12.721174: Steps 70/138: batch_recall = 81.20, batch_ndcg = 46.46 
2025-04-08 15:41:14.000042: Steps 71/138: batch_recall = 87.66, batch_ndcg = 51.39 
2025-04-08 15:41:15.271613: Steps 72/138: batch_recall = 87.55, batch_ndcg = 51.26 
2025-04-08 15:41:16.552446: Steps 73/138: batch_recall = 87.97, batch_ndcg = 49.36 
2025-04-08 15:41:17.831319: Steps 74/138: batch_recall = 85.18, batch_ndcg = 49.58 
2025-04-08 15:41:19.116868: Steps 75/138: batch_recall = 86.65, batch_ndcg = 50.28 
2025-04-08 15:41:20.424961: Steps 76/138: batch_recall = 96.89, batch_ndcg = 55.09 
2025-04-08 15:41:21.716827: Steps 77/138: batch_recall = 88.87, batch_ndcg = 50.38 
2025-04-08 15:41:23.009648: Steps 78/138: batch_recall = 93.65, batch_ndcg = 48.01 
2025-04-08 15:41:24.293381: Steps 79/138: batch_recall = 92.83, batch_ndcg = 48.91 
2025-04-08 15:41:25.584662: Steps 80/138: batch_recall = 73.93, batch_ndcg = 39.19 
2025-04-08 15:41:26.878288: Steps 81/138: batch_recall = 79.85, batch_ndcg = 47.82 
2025-04-08 15:41:28.192571: Steps 82/138: batch_recall = 88.74, batch_ndcg = 52.29 
2025-04-08 15:41:29.473151: Steps 83/138: batch_recall = 84.04, batch_ndcg = 48.37 
2025-04-08 15:41:30.767619: Steps 84/138: batch_recall = 101.86, batch_ndcg = 56.82 
2025-04-08 15:41:32.053926: Steps 85/138: batch_recall = 100.16, batch_ndcg = 59.27 
2025-04-08 15:41:33.344333: Steps 86/138: batch_recall = 117.43, batch_ndcg = 70.16 
2025-04-08 15:41:34.615660: Steps 87/138: batch_recall = 110.07, batch_ndcg = 57.82 
2025-04-08 15:41:35.899133: Steps 88/138: batch_recall = 101.37, batch_ndcg = 56.67 
2025-04-08 15:41:37.180086: Steps 89/138: batch_recall = 118.06, batch_ndcg = 66.64 
2025-04-08 15:41:38.456550: Steps 90/138: batch_recall = 102.61, batch_ndcg = 57.75 
2025-04-08 15:41:39.732175: Steps 91/138: batch_recall = 118.58, batch_ndcg = 67.16 
2025-04-08 15:41:41.011849: Steps 92/138: batch_recall = 116.78, batch_ndcg = 62.05 
2025-04-08 15:41:42.289721: Steps 93/138: batch_recall = 120.24, batch_ndcg = 68.21 
2025-04-08 15:41:43.572216: Steps 94/138: batch_recall = 121.10, batch_ndcg = 62.93 
2025-04-08 15:41:44.849043: Steps 95/138: batch_recall = 113.69, batch_ndcg = 67.94 
2025-04-08 15:41:46.121601: Steps 96/138: batch_recall = 129.39, batch_ndcg = 78.35 
2025-04-08 15:41:47.397534: Steps 97/138: batch_recall = 144.19, batch_ndcg = 88.99 
2025-04-08 15:41:48.667395: Steps 98/138: batch_recall = 108.93, batch_ndcg = 62.50 
2025-04-08 15:41:49.937995: Steps 99/138: batch_recall = 124.02, batch_ndcg = 70.50 
2025-04-08 15:41:51.219856: Steps 100/138: batch_recall = 128.55, batch_ndcg = 71.87 
2025-04-08 15:41:52.496895: Steps 101/138: batch_recall = 126.09, batch_ndcg = 69.57 
2025-04-08 15:41:53.777857: Steps 102/138: batch_recall = 126.75, batch_ndcg = 73.88 
2025-04-08 15:41:55.071439: Steps 103/138: batch_recall = 143.62, batch_ndcg = 81.23 
2025-04-08 15:41:56.360972: Steps 104/138: batch_recall = 137.40, batch_ndcg = 77.74 
2025-04-08 15:41:57.654581: Steps 105/138: batch_recall = 116.60, batch_ndcg = 67.14 
2025-04-08 15:41:58.927401: Steps 106/138: batch_recall = 107.53, batch_ndcg = 59.28 
2025-04-08 15:42:00.213415: Steps 107/138: batch_recall = 118.31, batch_ndcg = 65.05 
2025-04-08 15:42:01.479178: Steps 108/138: batch_recall = 122.68, batch_ndcg = 71.73 
2025-04-08 15:42:02.739542: Steps 109/138: batch_recall = 136.91, batch_ndcg = 76.09 
2025-04-08 15:42:04.015829: Steps 110/138: batch_recall = 125.45, batch_ndcg = 64.53 
2025-04-08 15:42:05.283198: Steps 111/138: batch_recall = 142.59, batch_ndcg = 85.58 
2025-04-08 15:42:06.561754: Steps 112/138: batch_recall = 152.06, batch_ndcg = 88.39 
2025-04-08 15:42:07.843362: Steps 113/138: batch_recall = 125.32, batch_ndcg = 72.00 
2025-04-08 15:42:09.119011: Steps 114/138: batch_recall = 123.74, batch_ndcg = 70.39 
2025-04-08 15:42:10.396473: Steps 115/138: batch_recall = 123.03, batch_ndcg = 63.61 
2025-04-08 15:42:11.672614: Steps 116/138: batch_recall = 121.78, batch_ndcg = 64.78 
2025-04-08 15:42:12.955567: Steps 117/138: batch_recall = 116.76, batch_ndcg = 67.47 
2025-04-08 15:42:14.228854: Steps 118/138: batch_recall = 124.39, batch_ndcg = 72.04 
2025-04-08 15:42:15.509546: Steps 119/138: batch_recall = 138.28, batch_ndcg = 74.35 
2025-04-08 15:42:16.781019: Steps 120/138: batch_recall = 122.99, batch_ndcg = 69.24 
2025-04-08 15:42:18.063523: Steps 121/138: batch_recall = 143.22, batch_ndcg = 77.03 
2025-04-08 15:42:19.336370: Steps 122/138: batch_recall = 148.71, batch_ndcg = 80.62 
2025-04-08 15:42:20.610512: Steps 123/138: batch_recall = 132.99, batch_ndcg = 72.88 
2025-04-08 15:42:21.894466: Steps 124/138: batch_recall = 151.94, batch_ndcg = 94.11 
2025-04-08 15:42:23.178134: Steps 125/138: batch_recall = 132.70, batch_ndcg = 72.04 
2025-04-08 15:42:24.449708: Steps 126/138: batch_recall = 156.05, batch_ndcg = 87.66 
2025-04-08 15:42:25.731822: Steps 127/138: batch_recall = 147.01, batch_ndcg = 82.24 
2025-04-08 15:42:27.005902: Steps 128/138: batch_recall = 127.10, batch_ndcg = 69.68 
2025-04-08 15:42:28.273264: Steps 129/138: batch_recall = 162.16, batch_ndcg = 90.84 
2025-04-08 15:42:29.543558: Steps 130/138: batch_recall = 135.49, batch_ndcg = 70.09 
2025-04-08 15:42:30.815822: Steps 131/138: batch_recall = 148.37, batch_ndcg = 84.55 
2025-04-08 15:42:32.092010: Steps 132/138: batch_recall = 153.83, batch_ndcg = 86.30 
2025-04-08 15:42:33.357030: Steps 133/138: batch_recall = 145.52, batch_ndcg = 83.77 
2025-04-08 15:42:34.622928: Steps 134/138: batch_recall = 139.46, batch_ndcg = 79.60 
2025-04-08 15:42:35.891632: Steps 135/138: batch_recall = 166.19, batch_ndcg = 94.71 
2025-04-08 15:42:37.169070: Steps 136/138: batch_recall = 153.87, batch_ndcg = 78.23 
2025-04-08 15:42:38.443722: Steps 137/138: batch_recall = 138.18, batch_ndcg = 86.57 
2025-04-08 15:42:38.444314: Epoch 14/1000, Test: Recall = 0.1765, NDCG = 0.1000  

2025-04-08 15:42:40.208100: Training Step 0/354: batchLoss = 0.6101, diffLoss = 2.9840, kgLoss = 0.0166
2025-04-08 15:42:41.831659: Training Step 1/354: batchLoss = 0.6129, diffLoss = 2.9982, kgLoss = 0.0166
2025-04-08 15:42:43.449360: Training Step 2/354: batchLoss = 0.7260, diffLoss = 3.5637, kgLoss = 0.0165
2025-04-08 15:42:45.066405: Training Step 3/354: batchLoss = 0.6831, diffLoss = 3.3449, kgLoss = 0.0177
2025-04-08 15:42:46.690828: Training Step 4/354: batchLoss = 0.5731, diffLoss = 2.8083, kgLoss = 0.0143
2025-04-08 15:42:48.315063: Training Step 5/354: batchLoss = 0.6844, diffLoss = 3.3548, kgLoss = 0.0168
2025-04-08 15:42:49.930115: Training Step 6/354: batchLoss = 0.4888, diffLoss = 2.3912, kgLoss = 0.0132
2025-04-08 15:42:51.551774: Training Step 7/354: batchLoss = 0.5528, diffLoss = 2.7047, kgLoss = 0.0148
2025-04-08 15:42:53.177284: Training Step 8/354: batchLoss = 0.5417, diffLoss = 2.6546, kgLoss = 0.0135
2025-04-08 15:42:54.802644: Training Step 9/354: batchLoss = 0.5408, diffLoss = 2.6501, kgLoss = 0.0134
2025-04-08 15:42:56.425497: Training Step 10/354: batchLoss = 0.5738, diffLoss = 2.8053, kgLoss = 0.0159
2025-04-08 15:42:58.050585: Training Step 11/354: batchLoss = 0.5443, diffLoss = 2.6650, kgLoss = 0.0141
2025-04-08 15:42:59.668079: Training Step 12/354: batchLoss = 0.5011, diffLoss = 2.4610, kgLoss = 0.0111
2025-04-08 15:43:01.283787: Training Step 13/354: batchLoss = 0.5369, diffLoss = 2.6360, kgLoss = 0.0122
2025-04-08 15:43:02.906289: Training Step 14/354: batchLoss = 0.6307, diffLoss = 3.0938, kgLoss = 0.0150
2025-04-08 15:43:04.527734: Training Step 15/354: batchLoss = 0.5767, diffLoss = 2.8244, kgLoss = 0.0147
2025-04-08 15:43:06.145111: Training Step 16/354: batchLoss = 0.5403, diffLoss = 2.6433, kgLoss = 0.0146
2025-04-08 15:43:07.761572: Training Step 17/354: batchLoss = 0.6060, diffLoss = 2.9722, kgLoss = 0.0145
2025-04-08 15:43:09.383877: Training Step 18/354: batchLoss = 0.5477, diffLoss = 2.6825, kgLoss = 0.0141
2025-04-08 15:43:11.011102: Training Step 19/354: batchLoss = 0.6169, diffLoss = 3.0267, kgLoss = 0.0144
2025-04-08 15:43:12.642169: Training Step 20/354: batchLoss = 0.5390, diffLoss = 2.6362, kgLoss = 0.0147
2025-04-08 15:43:14.257888: Training Step 21/354: batchLoss = 0.5621, diffLoss = 2.7557, kgLoss = 0.0137
2025-04-08 15:43:15.888784: Training Step 22/354: batchLoss = 0.4522, diffLoss = 2.2158, kgLoss = 0.0113
2025-04-08 15:43:17.506159: Training Step 23/354: batchLoss = 0.5710, diffLoss = 2.7977, kgLoss = 0.0143
2025-04-08 15:43:19.124132: Training Step 24/354: batchLoss = 0.5807, diffLoss = 2.8470, kgLoss = 0.0141
2025-04-08 15:43:20.750693: Training Step 25/354: batchLoss = 0.6636, diffLoss = 3.2483, kgLoss = 0.0174
2025-04-08 15:43:22.374997: Training Step 26/354: batchLoss = 0.6814, diffLoss = 3.3435, kgLoss = 0.0158
2025-04-08 15:43:23.999254: Training Step 27/354: batchLoss = 0.5837, diffLoss = 2.8564, kgLoss = 0.0155
2025-04-08 15:43:25.620428: Training Step 28/354: batchLoss = 0.4886, diffLoss = 2.3885, kgLoss = 0.0136
2025-04-08 15:43:27.236057: Training Step 29/354: batchLoss = 0.5808, diffLoss = 2.8459, kgLoss = 0.0146
2025-04-08 15:43:28.851015: Training Step 30/354: batchLoss = 0.6690, diffLoss = 3.2711, kgLoss = 0.0185
2025-04-08 15:43:30.470098: Training Step 31/354: batchLoss = 0.7396, diffLoss = 3.6248, kgLoss = 0.0183
2025-04-08 15:43:32.084530: Training Step 32/354: batchLoss = 0.5753, diffLoss = 2.8129, kgLoss = 0.0159
2025-04-08 15:43:33.707857: Training Step 33/354: batchLoss = 0.5150, diffLoss = 2.5253, kgLoss = 0.0124
2025-04-08 15:43:35.336554: Training Step 34/354: batchLoss = 0.5156, diffLoss = 2.5206, kgLoss = 0.0144
2025-04-08 15:43:36.952881: Training Step 35/354: batchLoss = 0.5169, diffLoss = 2.5328, kgLoss = 0.0130
2025-04-08 15:43:38.574934: Training Step 36/354: batchLoss = 0.6818, diffLoss = 3.3441, kgLoss = 0.0162
2025-04-08 15:43:40.197292: Training Step 37/354: batchLoss = 0.5681, diffLoss = 2.7876, kgLoss = 0.0132
2025-04-08 15:43:41.827290: Training Step 38/354: batchLoss = 0.5848, diffLoss = 2.8631, kgLoss = 0.0152
2025-04-08 15:43:43.450514: Training Step 39/354: batchLoss = 0.5218, diffLoss = 2.5570, kgLoss = 0.0130
2025-04-08 15:43:45.071153: Training Step 40/354: batchLoss = 0.6333, diffLoss = 3.1038, kgLoss = 0.0157
2025-04-08 15:43:46.693106: Training Step 41/354: batchLoss = 0.6322, diffLoss = 3.1001, kgLoss = 0.0153
2025-04-08 15:43:48.320447: Training Step 42/354: batchLoss = 0.5389, diffLoss = 2.6269, kgLoss = 0.0169
2025-04-08 15:43:49.940694: Training Step 43/354: batchLoss = 0.5221, diffLoss = 2.5563, kgLoss = 0.0136
2025-04-08 15:43:51.566362: Training Step 44/354: batchLoss = 0.9049, diffLoss = 4.4418, kgLoss = 0.0207
2025-04-08 15:43:53.189482: Training Step 45/354: batchLoss = 0.6827, diffLoss = 3.3505, kgLoss = 0.0157
2025-04-08 15:43:54.812333: Training Step 46/354: batchLoss = 0.6059, diffLoss = 2.9674, kgLoss = 0.0155
2025-04-08 15:43:56.433797: Training Step 47/354: batchLoss = 0.6723, diffLoss = 3.2946, kgLoss = 0.0168
2025-04-08 15:43:58.053266: Training Step 48/354: batchLoss = 0.5669, diffLoss = 2.7788, kgLoss = 0.0139
2025-04-08 15:43:59.668834: Training Step 49/354: batchLoss = 0.5028, diffLoss = 2.4517, kgLoss = 0.0156
2025-04-08 15:44:01.288531: Training Step 50/354: batchLoss = 0.5575, diffLoss = 2.7374, kgLoss = 0.0126
2025-04-08 15:44:02.915822: Training Step 51/354: batchLoss = 0.5528, diffLoss = 2.7064, kgLoss = 0.0144
2025-04-08 15:44:04.539739: Training Step 52/354: batchLoss = 0.5729, diffLoss = 2.8109, kgLoss = 0.0134
2025-04-08 15:44:06.157903: Training Step 53/354: batchLoss = 0.5079, diffLoss = 2.4896, kgLoss = 0.0125
2025-04-08 15:44:07.780884: Training Step 54/354: batchLoss = 0.4767, diffLoss = 2.3324, kgLoss = 0.0128
2025-04-08 15:44:09.397050: Training Step 55/354: batchLoss = 0.5266, diffLoss = 2.5823, kgLoss = 0.0127
2025-04-08 15:44:11.024386: Training Step 56/354: batchLoss = 0.5671, diffLoss = 2.7821, kgLoss = 0.0134
2025-04-08 15:44:12.649454: Training Step 57/354: batchLoss = 0.5759, diffLoss = 2.8227, kgLoss = 0.0142
2025-04-08 15:44:14.268875: Training Step 58/354: batchLoss = 0.5230, diffLoss = 2.5613, kgLoss = 0.0134
2025-04-08 15:44:15.886935: Training Step 59/354: batchLoss = 0.5588, diffLoss = 2.7301, kgLoss = 0.0160
2025-04-08 15:44:17.504883: Training Step 60/354: batchLoss = 0.5875, diffLoss = 2.8660, kgLoss = 0.0179
2025-04-08 15:44:19.124172: Training Step 61/354: batchLoss = 0.5639, diffLoss = 2.7636, kgLoss = 0.0140
2025-04-08 15:44:20.744988: Training Step 62/354: batchLoss = 0.5689, diffLoss = 2.7911, kgLoss = 0.0134
2025-04-08 15:44:22.368762: Training Step 63/354: batchLoss = 0.4943, diffLoss = 2.4240, kgLoss = 0.0118
2025-04-08 15:44:23.992551: Training Step 64/354: batchLoss = 0.5711, diffLoss = 2.8010, kgLoss = 0.0137
2025-04-08 15:44:25.618789: Training Step 65/354: batchLoss = 0.7737, diffLoss = 3.7945, kgLoss = 0.0185
2025-04-08 15:44:27.238552: Training Step 66/354: batchLoss = 0.5098, diffLoss = 2.4971, kgLoss = 0.0129
2025-04-08 15:44:28.849401: Training Step 67/354: batchLoss = 0.7065, diffLoss = 3.4690, kgLoss = 0.0159
2025-04-08 15:44:30.465662: Training Step 68/354: batchLoss = 0.6267, diffLoss = 3.0692, kgLoss = 0.0161
2025-04-08 15:44:32.086439: Training Step 69/354: batchLoss = 0.4767, diffLoss = 2.3384, kgLoss = 0.0113
2025-04-08 15:44:33.706637: Training Step 70/354: batchLoss = 0.4206, diffLoss = 2.0555, kgLoss = 0.0119
2025-04-08 15:44:35.326268: Training Step 71/354: batchLoss = 0.5352, diffLoss = 2.6209, kgLoss = 0.0137
2025-04-08 15:44:36.944822: Training Step 72/354: batchLoss = 0.4570, diffLoss = 2.2244, kgLoss = 0.0151
2025-04-08 15:44:38.563856: Training Step 73/354: batchLoss = 0.6425, diffLoss = 3.1482, kgLoss = 0.0161
2025-04-08 15:44:40.184736: Training Step 74/354: batchLoss = 0.5749, diffLoss = 2.8257, kgLoss = 0.0122
2025-04-08 15:44:41.806401: Training Step 75/354: batchLoss = 0.6577, diffLoss = 3.2227, kgLoss = 0.0164
2025-04-08 15:44:43.423030: Training Step 76/354: batchLoss = 0.6400, diffLoss = 3.1417, kgLoss = 0.0146
2025-04-08 15:44:45.044789: Training Step 77/354: batchLoss = 0.5712, diffLoss = 2.7976, kgLoss = 0.0146
2025-04-08 15:44:46.675382: Training Step 78/354: batchLoss = 0.5853, diffLoss = 2.8695, kgLoss = 0.0143
2025-04-08 15:44:48.299130: Training Step 79/354: batchLoss = 0.7016, diffLoss = 3.4480, kgLoss = 0.0150
2025-04-08 15:44:49.921479: Training Step 80/354: batchLoss = 0.5092, diffLoss = 2.4983, kgLoss = 0.0120
2025-04-08 15:44:51.543006: Training Step 81/354: batchLoss = 0.5835, diffLoss = 2.8584, kgLoss = 0.0147
2025-04-08 15:44:53.163325: Training Step 82/354: batchLoss = 0.7668, diffLoss = 3.7613, kgLoss = 0.0182
2025-04-08 15:44:54.782925: Training Step 83/354: batchLoss = 0.6651, diffLoss = 3.2642, kgLoss = 0.0153
2025-04-08 15:44:56.401774: Training Step 84/354: batchLoss = 0.6350, diffLoss = 3.1080, kgLoss = 0.0167
2025-04-08 15:44:58.029077: Training Step 85/354: batchLoss = 0.4958, diffLoss = 2.4227, kgLoss = 0.0141
2025-04-08 15:44:59.642893: Training Step 86/354: batchLoss = 0.6655, diffLoss = 3.2472, kgLoss = 0.0200
2025-04-08 15:45:01.269466: Training Step 87/354: batchLoss = 0.4582, diffLoss = 2.2427, kgLoss = 0.0120
2025-04-08 15:45:02.888637: Training Step 88/354: batchLoss = 0.5147, diffLoss = 2.5256, kgLoss = 0.0119
2025-04-08 15:45:04.514178: Training Step 89/354: batchLoss = 0.5325, diffLoss = 2.6087, kgLoss = 0.0134
2025-04-08 15:45:06.134436: Training Step 90/354: batchLoss = 0.4732, diffLoss = 2.3201, kgLoss = 0.0115
2025-04-08 15:45:07.751069: Training Step 91/354: batchLoss = 0.5357, diffLoss = 2.6198, kgLoss = 0.0146
2025-04-08 15:45:09.376939: Training Step 92/354: batchLoss = 0.7122, diffLoss = 3.4963, kgLoss = 0.0161
2025-04-08 15:45:11.004172: Training Step 93/354: batchLoss = 0.5893, diffLoss = 2.8815, kgLoss = 0.0162
2025-04-08 15:45:12.632490: Training Step 94/354: batchLoss = 0.5991, diffLoss = 2.9332, kgLoss = 0.0156
2025-04-08 15:45:14.249422: Training Step 95/354: batchLoss = 0.5536, diffLoss = 2.7168, kgLoss = 0.0128
2025-04-08 15:45:15.874888: Training Step 96/354: batchLoss = 0.5512, diffLoss = 2.7045, kgLoss = 0.0128
2025-04-08 15:45:17.506670: Training Step 97/354: batchLoss = 0.5403, diffLoss = 2.6432, kgLoss = 0.0146
2025-04-08 15:45:19.131275: Training Step 98/354: batchLoss = 0.4795, diffLoss = 2.3508, kgLoss = 0.0116
2025-04-08 15:45:20.754599: Training Step 99/354: batchLoss = 0.5020, diffLoss = 2.4606, kgLoss = 0.0123
2025-04-08 15:45:22.381007: Training Step 100/354: batchLoss = 0.7257, diffLoss = 3.5642, kgLoss = 0.0161
2025-04-08 15:45:24.002171: Training Step 101/354: batchLoss = 0.5379, diffLoss = 2.6318, kgLoss = 0.0145
2025-04-08 15:45:25.624591: Training Step 102/354: batchLoss = 0.4526, diffLoss = 2.2219, kgLoss = 0.0103
2025-04-08 15:45:27.247353: Training Step 103/354: batchLoss = 0.4883, diffLoss = 2.3922, kgLoss = 0.0124
2025-04-08 15:45:28.868621: Training Step 104/354: batchLoss = 0.5463, diffLoss = 2.6680, kgLoss = 0.0159
2025-04-08 15:45:30.492344: Training Step 105/354: batchLoss = 0.5651, diffLoss = 2.7762, kgLoss = 0.0123
2025-04-08 15:45:32.118637: Training Step 106/354: batchLoss = 0.4439, diffLoss = 2.1756, kgLoss = 0.0110
2025-04-08 15:45:33.737259: Training Step 107/354: batchLoss = 0.5949, diffLoss = 2.9158, kgLoss = 0.0147
2025-04-08 15:45:35.425740: Training Step 108/354: batchLoss = 0.6916, diffLoss = 3.3906, kgLoss = 0.0168
2025-04-08 15:45:37.041043: Training Step 109/354: batchLoss = 0.5777, diffLoss = 2.8315, kgLoss = 0.0143
2025-04-08 15:45:38.665691: Training Step 110/354: batchLoss = 0.5742, diffLoss = 2.8102, kgLoss = 0.0152
2025-04-08 15:45:40.291332: Training Step 111/354: batchLoss = 0.6721, diffLoss = 3.2922, kgLoss = 0.0171
2025-04-08 15:45:41.922479: Training Step 112/354: batchLoss = 0.5662, diffLoss = 2.7746, kgLoss = 0.0141
2025-04-08 15:45:43.541974: Training Step 113/354: batchLoss = 0.6769, diffLoss = 3.3184, kgLoss = 0.0165
2025-04-08 15:45:45.163513: Training Step 114/354: batchLoss = 0.6835, diffLoss = 3.3532, kgLoss = 0.0160
2025-04-08 15:45:46.789297: Training Step 115/354: batchLoss = 0.5794, diffLoss = 2.8322, kgLoss = 0.0162
2025-04-08 15:45:48.417572: Training Step 116/354: batchLoss = 0.7320, diffLoss = 3.5957, kgLoss = 0.0161
2025-04-08 15:45:50.042053: Training Step 117/354: batchLoss = 0.5083, diffLoss = 2.4872, kgLoss = 0.0136
2025-04-08 15:45:51.666091: Training Step 118/354: batchLoss = 0.7403, diffLoss = 3.6165, kgLoss = 0.0212
2025-04-08 15:45:53.290312: Training Step 119/354: batchLoss = 0.6283, diffLoss = 3.0795, kgLoss = 0.0154
2025-04-08 15:45:54.905680: Training Step 120/354: batchLoss = 0.5511, diffLoss = 2.7021, kgLoss = 0.0134
2025-04-08 15:45:56.526299: Training Step 121/354: batchLoss = 0.5108, diffLoss = 2.5045, kgLoss = 0.0124
2025-04-08 15:45:58.142047: Training Step 122/354: batchLoss = 0.5542, diffLoss = 2.7156, kgLoss = 0.0139
2025-04-08 15:45:59.761664: Training Step 123/354: batchLoss = 0.7017, diffLoss = 3.4490, kgLoss = 0.0149
2025-04-08 15:46:01.381206: Training Step 124/354: batchLoss = 0.5626, diffLoss = 2.7605, kgLoss = 0.0132
2025-04-08 15:46:03.000943: Training Step 125/354: batchLoss = 0.5407, diffLoss = 2.6499, kgLoss = 0.0134
2025-04-08 15:46:04.619567: Training Step 126/354: batchLoss = 0.4928, diffLoss = 2.4173, kgLoss = 0.0116
2025-04-08 15:46:06.242223: Training Step 127/354: batchLoss = 0.4682, diffLoss = 2.2941, kgLoss = 0.0117
2025-04-08 15:46:07.866134: Training Step 128/354: batchLoss = 0.5623, diffLoss = 2.7555, kgLoss = 0.0141
2025-04-08 15:46:09.489302: Training Step 129/354: batchLoss = 0.5228, diffLoss = 2.5552, kgLoss = 0.0147
2025-04-08 15:46:11.104316: Training Step 130/354: batchLoss = 0.6884, diffLoss = 3.3764, kgLoss = 0.0164
2025-04-08 15:46:12.725969: Training Step 131/354: batchLoss = 0.5549, diffLoss = 2.7172, kgLoss = 0.0143
2025-04-08 15:46:14.348039: Training Step 132/354: batchLoss = 0.5512, diffLoss = 2.7042, kgLoss = 0.0130
2025-04-08 15:46:15.979943: Training Step 133/354: batchLoss = 0.5021, diffLoss = 2.4597, kgLoss = 0.0128
2025-04-08 15:46:17.604147: Training Step 134/354: batchLoss = 0.5515, diffLoss = 2.6996, kgLoss = 0.0145
2025-04-08 15:46:19.227661: Training Step 135/354: batchLoss = 0.6062, diffLoss = 2.9714, kgLoss = 0.0149
2025-04-08 15:46:20.843908: Training Step 136/354: batchLoss = 0.5352, diffLoss = 2.6124, kgLoss = 0.0159
2025-04-08 15:46:22.459700: Training Step 137/354: batchLoss = 0.6456, diffLoss = 3.1633, kgLoss = 0.0162
2025-04-08 15:46:24.073530: Training Step 138/354: batchLoss = 0.5362, diffLoss = 2.6171, kgLoss = 0.0159
2025-04-08 15:46:25.696646: Training Step 139/354: batchLoss = 0.5503, diffLoss = 2.6945, kgLoss = 0.0143
2025-04-08 15:46:27.317398: Training Step 140/354: batchLoss = 0.5243, diffLoss = 2.5644, kgLoss = 0.0142
2025-04-08 15:46:28.936120: Training Step 141/354: batchLoss = 0.5516, diffLoss = 2.6958, kgLoss = 0.0155
2025-04-08 15:46:30.558612: Training Step 142/354: batchLoss = 0.5668, diffLoss = 2.7799, kgLoss = 0.0135
2025-04-08 15:46:32.183882: Training Step 143/354: batchLoss = 0.8185, diffLoss = 4.0133, kgLoss = 0.0198
2025-04-08 15:46:33.801365: Training Step 144/354: batchLoss = 0.4712, diffLoss = 2.3120, kgLoss = 0.0110
2025-04-08 15:46:35.429236: Training Step 145/354: batchLoss = 0.5252, diffLoss = 2.5478, kgLoss = 0.0196
2025-04-08 15:46:37.047934: Training Step 146/354: batchLoss = 0.5368, diffLoss = 2.6304, kgLoss = 0.0134
2025-04-08 15:46:38.669315: Training Step 147/354: batchLoss = 0.5428, diffLoss = 2.6600, kgLoss = 0.0135
2025-04-08 15:46:40.285009: Training Step 148/354: batchLoss = 0.4560, diffLoss = 2.2307, kgLoss = 0.0123
2025-04-08 15:46:41.908121: Training Step 149/354: batchLoss = 0.6026, diffLoss = 2.9497, kgLoss = 0.0158
2025-04-08 15:46:43.533150: Training Step 150/354: batchLoss = 0.5046, diffLoss = 2.4690, kgLoss = 0.0135
2025-04-08 15:46:45.152491: Training Step 151/354: batchLoss = 0.6269, diffLoss = 3.0658, kgLoss = 0.0172
2025-04-08 15:46:46.789322: Training Step 152/354: batchLoss = 0.5608, diffLoss = 2.7520, kgLoss = 0.0130
2025-04-08 15:46:48.406855: Training Step 153/354: batchLoss = 0.4704, diffLoss = 2.3060, kgLoss = 0.0115
2025-04-08 15:46:50.032546: Training Step 154/354: batchLoss = 0.5888, diffLoss = 2.8831, kgLoss = 0.0152
2025-04-08 15:46:51.648107: Training Step 155/354: batchLoss = 0.6750, diffLoss = 3.3068, kgLoss = 0.0171
2025-04-08 15:46:53.266317: Training Step 156/354: batchLoss = 0.6000, diffLoss = 2.9367, kgLoss = 0.0159
2025-04-08 15:46:54.886114: Training Step 157/354: batchLoss = 0.4522, diffLoss = 2.2183, kgLoss = 0.0107
2025-04-08 15:46:56.498349: Training Step 158/354: batchLoss = 0.6572, diffLoss = 3.2218, kgLoss = 0.0161
2025-04-08 15:46:58.114281: Training Step 159/354: batchLoss = 0.5182, diffLoss = 2.5286, kgLoss = 0.0156
2025-04-08 15:46:59.731523: Training Step 160/354: batchLoss = 0.5604, diffLoss = 2.7397, kgLoss = 0.0155
2025-04-08 15:47:01.357504: Training Step 161/354: batchLoss = 0.5091, diffLoss = 2.4878, kgLoss = 0.0144
2025-04-08 15:47:02.980154: Training Step 162/354: batchLoss = 0.6928, diffLoss = 3.3966, kgLoss = 0.0169
2025-04-08 15:47:04.607079: Training Step 163/354: batchLoss = 0.6266, diffLoss = 3.0718, kgLoss = 0.0153
2025-04-08 15:47:06.225431: Training Step 164/354: batchLoss = 0.5222, diffLoss = 2.5581, kgLoss = 0.0133
2025-04-08 15:47:07.844566: Training Step 165/354: batchLoss = 0.6667, diffLoss = 3.2691, kgLoss = 0.0162
2025-04-08 15:47:09.458110: Training Step 166/354: batchLoss = 0.5296, diffLoss = 2.5911, kgLoss = 0.0142
2025-04-08 15:47:11.072056: Training Step 167/354: batchLoss = 0.6346, diffLoss = 3.1180, kgLoss = 0.0137
2025-04-08 15:47:12.690964: Training Step 168/354: batchLoss = 0.5072, diffLoss = 2.4765, kgLoss = 0.0149
2025-04-08 15:47:14.316065: Training Step 169/354: batchLoss = 0.5998, diffLoss = 2.9393, kgLoss = 0.0150
2025-04-08 15:47:15.948333: Training Step 170/354: batchLoss = 0.6389, diffLoss = 3.1375, kgLoss = 0.0142
2025-04-08 15:47:17.578833: Training Step 171/354: batchLoss = 0.8153, diffLoss = 4.0051, kgLoss = 0.0179
2025-04-08 15:47:19.197949: Training Step 172/354: batchLoss = 0.6227, diffLoss = 3.0546, kgLoss = 0.0147
2025-04-08 15:47:20.815385: Training Step 173/354: batchLoss = 0.6597, diffLoss = 3.2362, kgLoss = 0.0156
2025-04-08 15:47:22.434980: Training Step 174/354: batchLoss = 0.5584, diffLoss = 2.7308, kgLoss = 0.0153
2025-04-08 15:47:24.048447: Training Step 175/354: batchLoss = 0.6304, diffLoss = 3.0846, kgLoss = 0.0168
2025-04-08 15:47:25.674912: Training Step 176/354: batchLoss = 0.6774, diffLoss = 3.3203, kgLoss = 0.0166
2025-04-08 15:47:27.297150: Training Step 177/354: batchLoss = 0.4616, diffLoss = 2.2585, kgLoss = 0.0123
2025-04-08 15:47:28.923203: Training Step 178/354: batchLoss = 0.5938, diffLoss = 2.9145, kgLoss = 0.0136
2025-04-08 15:47:30.547552: Training Step 179/354: batchLoss = 0.6148, diffLoss = 3.0128, kgLoss = 0.0153
2025-04-08 15:47:32.181477: Training Step 180/354: batchLoss = 0.5006, diffLoss = 2.4510, kgLoss = 0.0130
2025-04-08 15:47:33.803492: Training Step 181/354: batchLoss = 0.6353, diffLoss = 3.1150, kgLoss = 0.0153
2025-04-08 15:47:35.427311: Training Step 182/354: batchLoss = 0.4883, diffLoss = 2.3907, kgLoss = 0.0127
2025-04-08 15:47:37.042447: Training Step 183/354: batchLoss = 0.5959, diffLoss = 2.9123, kgLoss = 0.0169
2025-04-08 15:47:38.655492: Training Step 184/354: batchLoss = 0.6054, diffLoss = 2.9725, kgLoss = 0.0137
2025-04-08 15:47:40.271675: Training Step 185/354: batchLoss = 0.6337, diffLoss = 3.1031, kgLoss = 0.0163
2025-04-08 15:47:41.910270: Training Step 186/354: batchLoss = 0.6661, diffLoss = 3.2663, kgLoss = 0.0161
2025-04-08 15:47:43.530996: Training Step 187/354: batchLoss = 0.5201, diffLoss = 2.5452, kgLoss = 0.0138
2025-04-08 15:47:45.160428: Training Step 188/354: batchLoss = 0.5541, diffLoss = 2.7154, kgLoss = 0.0138
2025-04-08 15:47:46.781680: Training Step 189/354: batchLoss = 0.4922, diffLoss = 2.4091, kgLoss = 0.0129
2025-04-08 15:47:48.408909: Training Step 190/354: batchLoss = 0.5595, diffLoss = 2.7441, kgLoss = 0.0133
2025-04-08 15:47:50.030190: Training Step 191/354: batchLoss = 0.4707, diffLoss = 2.3053, kgLoss = 0.0121
2025-04-08 15:47:51.650244: Training Step 192/354: batchLoss = 0.6115, diffLoss = 2.9963, kgLoss = 0.0153
2025-04-08 15:47:53.262229: Training Step 193/354: batchLoss = 0.5026, diffLoss = 2.4586, kgLoss = 0.0136
2025-04-08 15:47:54.873015: Training Step 194/354: batchLoss = 0.4881, diffLoss = 2.3863, kgLoss = 0.0136
2025-04-08 15:47:56.496062: Training Step 195/354: batchLoss = 0.4195, diffLoss = 2.0534, kgLoss = 0.0110
2025-04-08 15:47:58.120444: Training Step 196/354: batchLoss = 0.7340, diffLoss = 3.6044, kgLoss = 0.0164
2025-04-08 15:47:59.742308: Training Step 197/354: batchLoss = 0.5326, diffLoss = 2.6119, kgLoss = 0.0127
2025-04-08 15:48:01.365877: Training Step 198/354: batchLoss = 0.5050, diffLoss = 2.4740, kgLoss = 0.0127
2025-04-08 15:48:02.991098: Training Step 199/354: batchLoss = 0.5856, diffLoss = 2.8726, kgLoss = 0.0138
2025-04-08 15:48:04.618584: Training Step 200/354: batchLoss = 0.6343, diffLoss = 3.1099, kgLoss = 0.0153
2025-04-08 15:48:06.243020: Training Step 201/354: batchLoss = 0.5669, diffLoss = 2.7761, kgLoss = 0.0146
2025-04-08 15:48:07.853065: Training Step 202/354: batchLoss = 0.5866, diffLoss = 2.8787, kgLoss = 0.0135
2025-04-08 15:48:09.471392: Training Step 203/354: batchLoss = 0.5143, diffLoss = 2.5148, kgLoss = 0.0142
2025-04-08 15:48:11.098992: Training Step 204/354: batchLoss = 0.5405, diffLoss = 2.6421, kgLoss = 0.0151
2025-04-08 15:48:12.716325: Training Step 205/354: batchLoss = 0.5338, diffLoss = 2.6185, kgLoss = 0.0127
2025-04-08 15:48:14.339413: Training Step 206/354: batchLoss = 0.6016, diffLoss = 2.9468, kgLoss = 0.0153
2025-04-08 15:48:15.966743: Training Step 207/354: batchLoss = 0.4813, diffLoss = 2.3594, kgLoss = 0.0118
2025-04-08 15:48:17.591662: Training Step 208/354: batchLoss = 0.5523, diffLoss = 2.6956, kgLoss = 0.0165
2025-04-08 15:48:19.207415: Training Step 209/354: batchLoss = 0.5799, diffLoss = 2.8510, kgLoss = 0.0122
2025-04-08 15:48:20.828212: Training Step 210/354: batchLoss = 0.6566, diffLoss = 3.2165, kgLoss = 0.0166
2025-04-08 15:48:22.445393: Training Step 211/354: batchLoss = 0.5912, diffLoss = 2.9003, kgLoss = 0.0139
2025-04-08 15:48:24.062731: Training Step 212/354: batchLoss = 0.5856, diffLoss = 2.8697, kgLoss = 0.0146
2025-04-08 15:48:25.690812: Training Step 213/354: batchLoss = 0.6489, diffLoss = 3.1741, kgLoss = 0.0176
2025-04-08 15:48:27.326849: Training Step 214/354: batchLoss = 0.5006, diffLoss = 2.4375, kgLoss = 0.0163
2025-04-08 15:48:28.953242: Training Step 215/354: batchLoss = 0.6509, diffLoss = 3.1899, kgLoss = 0.0161
2025-04-08 15:48:30.574619: Training Step 216/354: batchLoss = 0.5855, diffLoss = 2.8695, kgLoss = 0.0145
2025-04-08 15:48:32.199200: Training Step 217/354: batchLoss = 0.6794, diffLoss = 3.3336, kgLoss = 0.0158
2025-04-08 15:48:33.824401: Training Step 218/354: batchLoss = 0.4857, diffLoss = 2.3807, kgLoss = 0.0120
2025-04-08 15:48:35.443256: Training Step 219/354: batchLoss = 0.5471, diffLoss = 2.6749, kgLoss = 0.0152
2025-04-08 15:48:37.073831: Training Step 220/354: batchLoss = 0.5654, diffLoss = 2.7696, kgLoss = 0.0143
2025-04-08 15:48:38.695094: Training Step 221/354: batchLoss = 0.6982, diffLoss = 3.4187, kgLoss = 0.0180
2025-04-08 15:48:40.316328: Training Step 222/354: batchLoss = 0.5766, diffLoss = 2.8235, kgLoss = 0.0149
2025-04-08 15:48:41.937534: Training Step 223/354: batchLoss = 0.5826, diffLoss = 2.8519, kgLoss = 0.0152
2025-04-08 15:48:43.559887: Training Step 224/354: batchLoss = 0.5958, diffLoss = 2.9222, kgLoss = 0.0143
2025-04-08 15:48:45.173605: Training Step 225/354: batchLoss = 0.6916, diffLoss = 3.3903, kgLoss = 0.0170
2025-04-08 15:48:46.794609: Training Step 226/354: batchLoss = 0.5379, diffLoss = 2.6334, kgLoss = 0.0140
2025-04-08 15:48:48.420702: Training Step 227/354: batchLoss = 0.4081, diffLoss = 1.9966, kgLoss = 0.0109
2025-04-08 15:48:50.043908: Training Step 228/354: batchLoss = 0.4703, diffLoss = 2.3062, kgLoss = 0.0113
2025-04-08 15:48:51.661192: Training Step 229/354: batchLoss = 0.6295, diffLoss = 3.0825, kgLoss = 0.0162
2025-04-08 15:48:53.272738: Training Step 230/354: batchLoss = 0.5570, diffLoss = 2.7323, kgLoss = 0.0132
2025-04-08 15:48:54.890688: Training Step 231/354: batchLoss = 0.7329, diffLoss = 3.6020, kgLoss = 0.0157
2025-04-08 15:48:56.520271: Training Step 232/354: batchLoss = 0.9970, diffLoss = 4.8907, kgLoss = 0.0235
2025-04-08 15:48:58.148699: Training Step 233/354: batchLoss = 0.5484, diffLoss = 2.6910, kgLoss = 0.0127
2025-04-08 15:48:59.769821: Training Step 234/354: batchLoss = 0.5579, diffLoss = 2.7299, kgLoss = 0.0149
2025-04-08 15:49:01.402677: Training Step 235/354: batchLoss = 0.5662, diffLoss = 2.7783, kgLoss = 0.0132
2025-04-08 15:49:03.024285: Training Step 236/354: batchLoss = 0.5832, diffLoss = 2.8569, kgLoss = 0.0147
2025-04-08 15:49:04.649188: Training Step 237/354: batchLoss = 0.5930, diffLoss = 2.9109, kgLoss = 0.0135
2025-04-08 15:49:06.268229: Training Step 238/354: batchLoss = 0.5768, diffLoss = 2.8289, kgLoss = 0.0138
2025-04-08 15:49:07.883704: Training Step 239/354: batchLoss = 0.7768, diffLoss = 3.8081, kgLoss = 0.0190
2025-04-08 15:49:09.505633: Training Step 240/354: batchLoss = 0.5700, diffLoss = 2.7960, kgLoss = 0.0134
2025-04-08 15:49:11.141550: Training Step 241/354: batchLoss = 0.4509, diffLoss = 2.2147, kgLoss = 0.0100
2025-04-08 15:49:12.769593: Training Step 242/354: batchLoss = 0.4476, diffLoss = 2.1939, kgLoss = 0.0111
2025-04-08 15:49:14.392506: Training Step 243/354: batchLoss = 0.5969, diffLoss = 2.9182, kgLoss = 0.0166
2025-04-08 15:49:16.012976: Training Step 244/354: batchLoss = 0.5632, diffLoss = 2.7600, kgLoss = 0.0140
2025-04-08 15:49:17.640393: Training Step 245/354: batchLoss = 0.4486, diffLoss = 2.1960, kgLoss = 0.0117
2025-04-08 15:49:19.257440: Training Step 246/354: batchLoss = 0.6031, diffLoss = 2.9546, kgLoss = 0.0152
2025-04-08 15:49:20.874510: Training Step 247/354: batchLoss = 0.5972, diffLoss = 2.9212, kgLoss = 0.0163
2025-04-08 15:49:22.488235: Training Step 248/354: batchLoss = 0.6233, diffLoss = 3.0534, kgLoss = 0.0157
2025-04-08 15:49:24.111446: Training Step 249/354: batchLoss = 0.7304, diffLoss = 3.5824, kgLoss = 0.0174
2025-04-08 15:49:25.740938: Training Step 250/354: batchLoss = 0.4929, diffLoss = 2.4114, kgLoss = 0.0133
2025-04-08 15:49:27.368134: Training Step 251/354: batchLoss = 0.5941, diffLoss = 2.9053, kgLoss = 0.0162
2025-04-08 15:49:29.001641: Training Step 252/354: batchLoss = 0.4691, diffLoss = 2.2988, kgLoss = 0.0117
2025-04-08 15:49:30.630712: Training Step 253/354: batchLoss = 0.5611, diffLoss = 2.7459, kgLoss = 0.0149
2025-04-08 15:49:32.257321: Training Step 254/354: batchLoss = 0.4345, diffLoss = 2.1272, kgLoss = 0.0113
2025-04-08 15:49:33.875836: Training Step 255/354: batchLoss = 0.5153, diffLoss = 2.5224, kgLoss = 0.0136
2025-04-08 15:49:35.487133: Training Step 256/354: batchLoss = 0.6052, diffLoss = 2.9591, kgLoss = 0.0167
2025-04-08 15:49:37.104223: Training Step 257/354: batchLoss = 0.6071, diffLoss = 2.9801, kgLoss = 0.0139
2025-04-08 15:49:38.727545: Training Step 258/354: batchLoss = 0.7000, diffLoss = 3.4317, kgLoss = 0.0170
2025-04-08 15:49:40.351436: Training Step 259/354: batchLoss = 0.5110, diffLoss = 2.4932, kgLoss = 0.0155
2025-04-08 15:49:41.967765: Training Step 260/354: batchLoss = 0.5085, diffLoss = 2.4927, kgLoss = 0.0125
2025-04-08 15:49:43.587269: Training Step 261/354: batchLoss = 0.5417, diffLoss = 2.6527, kgLoss = 0.0140
2025-04-08 15:49:45.208458: Training Step 262/354: batchLoss = 0.4841, diffLoss = 2.3586, kgLoss = 0.0155
2025-04-08 15:49:46.832488: Training Step 263/354: batchLoss = 0.5737, diffLoss = 2.8126, kgLoss = 0.0140
2025-04-08 15:49:48.465462: Training Step 264/354: batchLoss = 0.6764, diffLoss = 3.3165, kgLoss = 0.0164
2025-04-08 15:49:50.091889: Training Step 265/354: batchLoss = 0.5619, diffLoss = 2.7577, kgLoss = 0.0130
2025-04-08 15:49:51.713612: Training Step 266/354: batchLoss = 0.5252, diffLoss = 2.5769, kgLoss = 0.0123
2025-04-08 15:49:53.332338: Training Step 267/354: batchLoss = 0.6583, diffLoss = 3.2278, kgLoss = 0.0159
2025-04-08 15:49:54.952975: Training Step 268/354: batchLoss = 0.5565, diffLoss = 2.7267, kgLoss = 0.0140
2025-04-08 15:49:56.568542: Training Step 269/354: batchLoss = 0.4603, diffLoss = 2.2550, kgLoss = 0.0117
2025-04-08 15:49:58.190849: Training Step 270/354: batchLoss = 0.5091, diffLoss = 2.4920, kgLoss = 0.0133
2025-04-08 15:49:59.810748: Training Step 271/354: batchLoss = 0.4803, diffLoss = 2.3452, kgLoss = 0.0141
2025-04-08 15:50:01.431636: Training Step 272/354: batchLoss = 0.5312, diffLoss = 2.5890, kgLoss = 0.0167
2025-04-08 15:50:03.049734: Training Step 273/354: batchLoss = 0.6165, diffLoss = 3.0168, kgLoss = 0.0164
2025-04-08 15:50:04.672603: Training Step 274/354: batchLoss = 0.6066, diffLoss = 2.9737, kgLoss = 0.0148
2025-04-08 15:50:06.288236: Training Step 275/354: batchLoss = 0.6590, diffLoss = 3.2325, kgLoss = 0.0156
2025-04-08 15:50:07.906767: Training Step 276/354: batchLoss = 0.5204, diffLoss = 2.5504, kgLoss = 0.0129
2025-04-08 15:50:09.526623: Training Step 277/354: batchLoss = 0.6284, diffLoss = 3.0827, kgLoss = 0.0148
2025-04-08 15:50:11.155330: Training Step 278/354: batchLoss = 0.5359, diffLoss = 2.6241, kgLoss = 0.0138
2025-04-08 15:50:12.781244: Training Step 279/354: batchLoss = 0.5343, diffLoss = 2.6209, kgLoss = 0.0127
2025-04-08 15:50:14.406162: Training Step 280/354: batchLoss = 0.6101, diffLoss = 2.9946, kgLoss = 0.0140
2025-04-08 15:50:16.026143: Training Step 281/354: batchLoss = 0.5172, diffLoss = 2.5304, kgLoss = 0.0138
2025-04-08 15:50:17.656025: Training Step 282/354: batchLoss = 0.5287, diffLoss = 2.5940, kgLoss = 0.0124
2025-04-08 15:50:19.277578: Training Step 283/354: batchLoss = 0.6267, diffLoss = 3.0718, kgLoss = 0.0155
2025-04-08 15:50:20.896627: Training Step 284/354: batchLoss = 0.5569, diffLoss = 2.7261, kgLoss = 0.0146
2025-04-08 15:50:22.517030: Training Step 285/354: batchLoss = 0.7244, diffLoss = 3.5528, kgLoss = 0.0174
2025-04-08 15:50:24.137621: Training Step 286/354: batchLoss = 0.5644, diffLoss = 2.7684, kgLoss = 0.0133
2025-04-08 15:50:25.759027: Training Step 287/354: batchLoss = 0.7588, diffLoss = 3.7266, kgLoss = 0.0168
2025-04-08 15:50:27.381891: Training Step 288/354: batchLoss = 0.5647, diffLoss = 2.7612, kgLoss = 0.0156
2025-04-08 15:50:29.010002: Training Step 289/354: batchLoss = 0.5313, diffLoss = 2.6047, kgLoss = 0.0129
2025-04-08 15:50:30.632761: Training Step 290/354: batchLoss = 0.7104, diffLoss = 3.4790, kgLoss = 0.0183
2025-04-08 15:50:32.249015: Training Step 291/354: batchLoss = 0.5149, diffLoss = 2.5248, kgLoss = 0.0124
2025-04-08 15:50:33.870703: Training Step 292/354: batchLoss = 0.7813, diffLoss = 3.8263, kgLoss = 0.0200
2025-04-08 15:50:35.493792: Training Step 293/354: batchLoss = 0.4982, diffLoss = 2.4408, kgLoss = 0.0126
2025-04-08 15:50:37.111168: Training Step 294/354: batchLoss = 0.5427, diffLoss = 2.6572, kgLoss = 0.0140
2025-04-08 15:50:38.725208: Training Step 295/354: batchLoss = 0.6321, diffLoss = 3.0956, kgLoss = 0.0162
2025-04-08 15:50:40.348418: Training Step 296/354: batchLoss = 0.6084, diffLoss = 2.9884, kgLoss = 0.0134
2025-04-08 15:50:41.970613: Training Step 297/354: batchLoss = 0.5866, diffLoss = 2.8683, kgLoss = 0.0162
2025-04-08 15:50:43.592354: Training Step 298/354: batchLoss = 0.5062, diffLoss = 2.4843, kgLoss = 0.0117
2025-04-08 15:50:45.213221: Training Step 299/354: batchLoss = 0.6446, diffLoss = 3.1618, kgLoss = 0.0153
2025-04-08 15:50:46.835502: Training Step 300/354: batchLoss = 0.4329, diffLoss = 2.1188, kgLoss = 0.0114
2025-04-08 15:50:48.455578: Training Step 301/354: batchLoss = 0.6872, diffLoss = 3.3648, kgLoss = 0.0178
2025-04-08 15:50:50.078350: Training Step 302/354: batchLoss = 0.5823, diffLoss = 2.8534, kgLoss = 0.0145
2025-04-08 15:50:51.700120: Training Step 303/354: batchLoss = 0.6004, diffLoss = 2.9452, kgLoss = 0.0142
2025-04-08 15:50:53.319065: Training Step 304/354: batchLoss = 0.6517, diffLoss = 3.1933, kgLoss = 0.0163
2025-04-08 15:50:54.937551: Training Step 305/354: batchLoss = 0.6347, diffLoss = 3.1107, kgLoss = 0.0157
2025-04-08 15:50:56.562533: Training Step 306/354: batchLoss = 0.6530, diffLoss = 3.2042, kgLoss = 0.0152
2025-04-08 15:50:58.188696: Training Step 307/354: batchLoss = 0.5822, diffLoss = 2.8560, kgLoss = 0.0137
2025-04-08 15:50:59.808216: Training Step 308/354: batchLoss = 0.6186, diffLoss = 3.0279, kgLoss = 0.0163
2025-04-08 15:51:01.429611: Training Step 309/354: batchLoss = 0.4752, diffLoss = 2.3237, kgLoss = 0.0131
2025-04-08 15:51:03.052502: Training Step 310/354: batchLoss = 0.6404, diffLoss = 3.1343, kgLoss = 0.0169
2025-04-08 15:51:04.669541: Training Step 311/354: batchLoss = 0.5558, diffLoss = 2.7217, kgLoss = 0.0144
2025-04-08 15:51:06.286398: Training Step 312/354: batchLoss = 0.5871, diffLoss = 2.8721, kgLoss = 0.0158
2025-04-08 15:51:07.905432: Training Step 313/354: batchLoss = 0.6882, diffLoss = 3.3814, kgLoss = 0.0149
2025-04-08 15:51:09.529636: Training Step 314/354: batchLoss = 0.7363, diffLoss = 3.6033, kgLoss = 0.0196
2025-04-08 15:51:11.157928: Training Step 315/354: batchLoss = 0.6029, diffLoss = 2.9503, kgLoss = 0.0160
2025-04-08 15:51:12.786838: Training Step 316/354: batchLoss = 0.4934, diffLoss = 2.4118, kgLoss = 0.0138
2025-04-08 15:51:14.417304: Training Step 317/354: batchLoss = 0.6468, diffLoss = 3.1741, kgLoss = 0.0150
2025-04-08 15:51:16.042826: Training Step 318/354: batchLoss = 0.5995, diffLoss = 2.9396, kgLoss = 0.0145
2025-04-08 15:51:17.661728: Training Step 319/354: batchLoss = 0.5780, diffLoss = 2.8276, kgLoss = 0.0156
2025-04-08 15:51:19.278942: Training Step 320/354: batchLoss = 0.6353, diffLoss = 3.1099, kgLoss = 0.0167
2025-04-08 15:51:20.899962: Training Step 321/354: batchLoss = 0.5868, diffLoss = 2.8753, kgLoss = 0.0147
2025-04-08 15:51:22.514796: Training Step 322/354: batchLoss = 0.5742, diffLoss = 2.8127, kgLoss = 0.0145
2025-04-08 15:51:24.138248: Training Step 323/354: batchLoss = 0.5358, diffLoss = 2.6209, kgLoss = 0.0145
2025-04-08 15:51:25.750867: Training Step 324/354: batchLoss = 0.4390, diffLoss = 2.1510, kgLoss = 0.0110
2025-04-08 15:51:27.366212: Training Step 325/354: batchLoss = 0.7320, diffLoss = 3.5868, kgLoss = 0.0183
2025-04-08 15:51:28.988143: Training Step 326/354: batchLoss = 0.4475, diffLoss = 2.1924, kgLoss = 0.0112
2025-04-08 15:51:30.607180: Training Step 327/354: batchLoss = 0.5856, diffLoss = 2.8723, kgLoss = 0.0139
2025-04-08 15:51:32.226801: Training Step 328/354: batchLoss = 0.4923, diffLoss = 2.4093, kgLoss = 0.0131
2025-04-08 15:51:33.838606: Training Step 329/354: batchLoss = 0.5867, diffLoss = 2.8727, kgLoss = 0.0152
2025-04-08 15:51:35.459753: Training Step 330/354: batchLoss = 0.4944, diffLoss = 2.4220, kgLoss = 0.0125
2025-04-08 15:51:37.075892: Training Step 331/354: batchLoss = 0.4859, diffLoss = 2.3837, kgLoss = 0.0114
2025-04-08 15:51:38.694738: Training Step 332/354: batchLoss = 0.5524, diffLoss = 2.7101, kgLoss = 0.0130
2025-04-08 15:51:40.313371: Training Step 333/354: batchLoss = 0.4563, diffLoss = 2.2324, kgLoss = 0.0122
2025-04-08 15:51:41.933116: Training Step 334/354: batchLoss = 1.0657, diffLoss = 5.2248, kgLoss = 0.0259
2025-04-08 15:51:43.555929: Training Step 335/354: batchLoss = 0.6548, diffLoss = 3.2110, kgLoss = 0.0157
2025-04-08 15:51:45.178106: Training Step 336/354: batchLoss = 0.5990, diffLoss = 2.9378, kgLoss = 0.0143
2025-04-08 15:51:46.802187: Training Step 337/354: batchLoss = 0.4727, diffLoss = 2.3123, kgLoss = 0.0128
2025-04-08 15:51:48.422955: Training Step 338/354: batchLoss = 0.5601, diffLoss = 2.7484, kgLoss = 0.0131
2025-04-08 15:51:50.040247: Training Step 339/354: batchLoss = 0.5306, diffLoss = 2.6014, kgLoss = 0.0129
2025-04-08 15:51:51.652128: Training Step 340/354: batchLoss = 0.4689, diffLoss = 2.2929, kgLoss = 0.0129
2025-04-08 15:51:53.270519: Training Step 341/354: batchLoss = 0.4986, diffLoss = 2.4421, kgLoss = 0.0127
2025-04-08 15:51:54.894047: Training Step 342/354: batchLoss = 0.5207, diffLoss = 2.5493, kgLoss = 0.0136
2025-04-08 15:51:56.527961: Training Step 343/354: batchLoss = 0.6351, diffLoss = 3.1151, kgLoss = 0.0151
2025-04-08 15:51:58.151964: Training Step 344/354: batchLoss = 0.5831, diffLoss = 2.8620, kgLoss = 0.0134
2025-04-08 15:51:59.779054: Training Step 345/354: batchLoss = 0.5610, diffLoss = 2.7458, kgLoss = 0.0148
2025-04-08 15:52:01.404002: Training Step 346/354: batchLoss = 0.7564, diffLoss = 3.5993, kgLoss = 0.0457
2025-04-08 15:52:03.025772: Training Step 347/354: batchLoss = 0.5283, diffLoss = 2.5878, kgLoss = 0.0135
2025-04-08 15:52:04.645771: Training Step 348/354: batchLoss = 0.8106, diffLoss = 3.9761, kgLoss = 0.0192
2025-04-08 15:52:06.268358: Training Step 349/354: batchLoss = 0.5925, diffLoss = 2.9031, kgLoss = 0.0148
2025-04-08 15:52:07.896804: Training Step 350/354: batchLoss = 0.5008, diffLoss = 2.4572, kgLoss = 0.0116
2025-04-08 15:52:09.511999: Training Step 351/354: batchLoss = 0.6439, diffLoss = 3.1470, kgLoss = 0.0181
2025-04-08 15:52:11.111321: Training Step 352/354: batchLoss = 0.4658, diffLoss = 2.2801, kgLoss = 0.0123
2025-04-08 15:52:12.519612: Training Step 353/354: batchLoss = 0.4997, diffLoss = 2.4489, kgLoss = 0.0124
2025-04-08 15:52:12.610329: 
2025-04-08 15:52:12.611006: Epoch 15/1000, Train: epLoss = 1.0265, epDfLoss = 5.0285, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 15:52:13.932075: Steps 0/138: batch_recall = 46.44, batch_ndcg = 26.69 
2025-04-08 15:52:15.258916: Steps 1/138: batch_recall = 47.27, batch_ndcg = 27.84 
2025-04-08 15:52:16.570332: Steps 2/138: batch_recall = 58.32, batch_ndcg = 36.64 
2025-04-08 15:52:17.884369: Steps 3/138: batch_recall = 59.14, batch_ndcg = 34.47 
2025-04-08 15:52:19.187436: Steps 4/138: batch_recall = 68.80, batch_ndcg = 40.76 
2025-04-08 15:52:20.496877: Steps 5/138: batch_recall = 58.36, batch_ndcg = 32.47 
2025-04-08 15:52:21.796050: Steps 6/138: batch_recall = 52.12, batch_ndcg = 30.50 
2025-04-08 15:52:23.109245: Steps 7/138: batch_recall = 62.93, batch_ndcg = 40.75 
2025-04-08 15:52:24.416443: Steps 8/138: batch_recall = 63.37, batch_ndcg = 39.39 
2025-04-08 15:52:25.725798: Steps 9/138: batch_recall = 56.79, batch_ndcg = 34.24 
2025-04-08 15:52:27.035531: Steps 10/138: batch_recall = 56.21, batch_ndcg = 31.36 
2025-04-08 15:52:28.326421: Steps 11/138: batch_recall = 58.44, batch_ndcg = 34.02 
2025-04-08 15:52:29.635796: Steps 12/138: batch_recall = 53.69, batch_ndcg = 29.30 
2025-04-08 15:52:30.926654: Steps 13/138: batch_recall = 51.06, batch_ndcg = 29.81 
2025-04-08 15:52:32.230012: Steps 14/138: batch_recall = 54.42, batch_ndcg = 31.64 
2025-04-08 15:52:33.520116: Steps 15/138: batch_recall = 50.40, batch_ndcg = 29.92 
2025-04-08 15:52:34.817496: Steps 16/138: batch_recall = 60.22, batch_ndcg = 33.33 
2025-04-08 15:52:36.131228: Steps 17/138: batch_recall = 55.18, batch_ndcg = 32.01 
2025-04-08 15:52:37.438265: Steps 18/138: batch_recall = 52.02, batch_ndcg = 32.55 
2025-04-08 15:52:38.742322: Steps 19/138: batch_recall = 55.44, batch_ndcg = 32.87 
2025-04-08 15:52:40.044979: Steps 20/138: batch_recall = 61.33, batch_ndcg = 36.82 
2025-04-08 15:52:41.350671: Steps 21/138: batch_recall = 66.74, batch_ndcg = 39.54 
2025-04-08 15:52:42.660033: Steps 22/138: batch_recall = 56.18, batch_ndcg = 32.95 
2025-04-08 15:52:43.960123: Steps 23/138: batch_recall = 52.42, batch_ndcg = 30.10 
2025-04-08 15:52:45.262371: Steps 24/138: batch_recall = 55.98, batch_ndcg = 31.11 
2025-04-08 15:52:46.568334: Steps 25/138: batch_recall = 61.04, batch_ndcg = 35.63 
2025-04-08 15:52:47.872666: Steps 26/138: batch_recall = 56.99, batch_ndcg = 33.29 
2025-04-08 15:52:49.167969: Steps 27/138: batch_recall = 61.32, batch_ndcg = 35.05 
2025-04-08 15:52:50.452777: Steps 28/138: batch_recall = 57.69, batch_ndcg = 33.16 
2025-04-08 15:52:51.776500: Steps 29/138: batch_recall = 58.95, batch_ndcg = 31.41 
2025-04-08 15:52:53.075172: Steps 30/138: batch_recall = 58.57, batch_ndcg = 33.88 
2025-04-08 15:52:54.366717: Steps 31/138: batch_recall = 41.27, batch_ndcg = 24.09 
2025-04-08 15:52:55.657739: Steps 32/138: batch_recall = 51.57, batch_ndcg = 30.69 
2025-04-08 15:52:56.952704: Steps 33/138: batch_recall = 60.47, batch_ndcg = 34.16 
2025-04-08 15:52:58.246164: Steps 34/138: batch_recall = 54.70, batch_ndcg = 28.78 
2025-04-08 15:52:59.548777: Steps 35/138: batch_recall = 55.10, batch_ndcg = 31.18 
2025-04-08 15:53:00.844032: Steps 36/138: batch_recall = 49.52, batch_ndcg = 28.01 
2025-04-08 15:53:02.137442: Steps 37/138: batch_recall = 58.17, batch_ndcg = 34.14 
2025-04-08 15:53:03.422718: Steps 38/138: batch_recall = 59.07, batch_ndcg = 33.12 
2025-04-08 15:53:04.704775: Steps 39/138: batch_recall = 66.11, batch_ndcg = 37.58 
2025-04-08 15:53:05.992740: Steps 40/138: batch_recall = 54.73, batch_ndcg = 29.43 
2025-04-08 15:53:07.273767: Steps 41/138: batch_recall = 61.94, batch_ndcg = 33.71 
2025-04-08 15:53:08.578509: Steps 42/138: batch_recall = 56.33, batch_ndcg = 30.19 
2025-04-08 15:53:09.872237: Steps 43/138: batch_recall = 55.15, batch_ndcg = 32.91 
2025-04-08 15:53:11.161229: Steps 44/138: batch_recall = 55.12, batch_ndcg = 29.21 
2025-04-08 15:53:12.458858: Steps 45/138: batch_recall = 63.15, batch_ndcg = 35.21 
2025-04-08 15:53:13.748697: Steps 46/138: batch_recall = 61.09, batch_ndcg = 34.84 
2025-04-08 15:53:15.044814: Steps 47/138: batch_recall = 52.08, batch_ndcg = 31.55 
2025-04-08 15:53:16.337006: Steps 48/138: batch_recall = 56.75, batch_ndcg = 33.45 
2025-04-08 15:53:17.625025: Steps 49/138: batch_recall = 64.15, batch_ndcg = 36.88 
2025-04-08 15:53:18.916750: Steps 50/138: batch_recall = 56.09, batch_ndcg = 31.03 
2025-04-08 15:53:20.193793: Steps 51/138: batch_recall = 59.00, batch_ndcg = 34.74 
2025-04-08 15:53:21.476884: Steps 52/138: batch_recall = 66.27, batch_ndcg = 42.39 
2025-04-08 15:53:22.773147: Steps 53/138: batch_recall = 67.31, batch_ndcg = 34.01 
2025-04-08 15:53:24.066054: Steps 54/138: batch_recall = 67.06, batch_ndcg = 37.94 
2025-04-08 15:53:25.353493: Steps 55/138: batch_recall = 61.66, batch_ndcg = 33.74 
2025-04-08 15:53:26.646165: Steps 56/138: batch_recall = 63.44, batch_ndcg = 35.44 
2025-04-08 15:53:27.950627: Steps 57/138: batch_recall = 59.36, batch_ndcg = 33.52 
2025-04-08 15:53:29.249697: Steps 58/138: batch_recall = 71.06, batch_ndcg = 38.20 
2025-04-08 15:53:30.547708: Steps 59/138: batch_recall = 72.91, batch_ndcg = 42.25 
2025-04-08 15:53:31.849718: Steps 60/138: batch_recall = 72.18, batch_ndcg = 38.58 
2025-04-08 15:53:33.136059: Steps 61/138: batch_recall = 64.93, batch_ndcg = 35.55 
2025-04-08 15:53:34.426813: Steps 62/138: batch_recall = 84.32, batch_ndcg = 44.20 
2025-04-08 15:53:35.709220: Steps 63/138: batch_recall = 78.24, batch_ndcg = 44.32 
2025-04-08 15:53:36.990299: Steps 64/138: batch_recall = 64.83, batch_ndcg = 34.27 
2025-04-08 15:53:38.284331: Steps 65/138: batch_recall = 81.16, batch_ndcg = 45.21 
2025-04-08 15:53:39.572982: Steps 66/138: batch_recall = 68.54, batch_ndcg = 40.42 
2025-04-08 15:53:40.858087: Steps 67/138: batch_recall = 77.91, batch_ndcg = 47.18 
2025-04-08 15:53:42.144899: Steps 68/138: batch_recall = 65.09, batch_ndcg = 34.22 
2025-04-08 15:53:43.435335: Steps 69/138: batch_recall = 86.61, batch_ndcg = 50.62 
2025-04-08 15:53:44.720576: Steps 70/138: batch_recall = 82.87, batch_ndcg = 46.28 
2025-04-08 15:53:46.023453: Steps 71/138: batch_recall = 86.57, batch_ndcg = 50.13 
2025-04-08 15:53:47.310823: Steps 72/138: batch_recall = 84.57, batch_ndcg = 49.71 
2025-04-08 15:53:48.587413: Steps 73/138: batch_recall = 86.39, batch_ndcg = 48.70 
2025-04-08 15:53:49.861948: Steps 74/138: batch_recall = 85.60, batch_ndcg = 49.94 
2025-04-08 15:53:51.149927: Steps 75/138: batch_recall = 89.77, batch_ndcg = 50.98 
2025-04-08 15:53:52.431363: Steps 76/138: batch_recall = 96.08, batch_ndcg = 54.70 
2025-04-08 15:53:53.717941: Steps 77/138: batch_recall = 88.58, batch_ndcg = 49.98 
2025-04-08 15:53:55.003952: Steps 78/138: batch_recall = 91.67, batch_ndcg = 47.84 
2025-04-08 15:53:56.287140: Steps 79/138: batch_recall = 93.96, batch_ndcg = 49.15 
2025-04-08 15:53:57.570110: Steps 80/138: batch_recall = 74.44, batch_ndcg = 39.32 
2025-04-08 15:53:58.847691: Steps 81/138: batch_recall = 81.76, batch_ndcg = 48.65 
2025-04-08 15:54:00.144082: Steps 82/138: batch_recall = 89.81, batch_ndcg = 52.88 
2025-04-08 15:54:01.445661: Steps 83/138: batch_recall = 82.88, batch_ndcg = 47.94 
2025-04-08 15:54:02.729486: Steps 84/138: batch_recall = 101.13, batch_ndcg = 56.47 
2025-04-08 15:54:04.013573: Steps 85/138: batch_recall = 100.41, batch_ndcg = 58.93 
2025-04-08 15:54:05.287886: Steps 86/138: batch_recall = 118.66, batch_ndcg = 70.34 
2025-04-08 15:54:06.572940: Steps 87/138: batch_recall = 109.36, batch_ndcg = 57.03 
2025-04-08 15:54:07.868815: Steps 88/138: batch_recall = 100.81, batch_ndcg = 56.74 
2025-04-08 15:54:09.148874: Steps 89/138: batch_recall = 115.91, batch_ndcg = 66.75 
2025-04-08 15:54:10.428766: Steps 90/138: batch_recall = 103.57, batch_ndcg = 59.26 
2025-04-08 15:54:11.715526: Steps 91/138: batch_recall = 120.33, batch_ndcg = 66.79 
2025-04-08 15:54:12.999924: Steps 92/138: batch_recall = 115.78, batch_ndcg = 62.07 
2025-04-08 15:54:14.276387: Steps 93/138: batch_recall = 120.40, batch_ndcg = 68.61 
2025-04-08 15:54:15.570441: Steps 94/138: batch_recall = 119.04, batch_ndcg = 63.09 
2025-04-08 15:54:16.851172: Steps 95/138: batch_recall = 113.54, batch_ndcg = 67.83 
2025-04-08 15:54:18.125122: Steps 96/138: batch_recall = 134.08, batch_ndcg = 74.81 
2025-04-08 15:54:19.394929: Steps 97/138: batch_recall = 148.30, batch_ndcg = 83.89 
2025-04-08 15:54:20.671813: Steps 98/138: batch_recall = 110.93, batch_ndcg = 63.05 
2025-04-08 15:54:21.949882: Steps 99/138: batch_recall = 127.22, batch_ndcg = 71.28 
2025-04-08 15:54:23.234635: Steps 100/138: batch_recall = 129.12, batch_ndcg = 72.14 
2025-04-08 15:54:24.521917: Steps 101/138: batch_recall = 126.42, batch_ndcg = 70.38 
2025-04-08 15:54:25.817036: Steps 102/138: batch_recall = 125.72, batch_ndcg = 73.45 
2025-04-08 15:54:27.109135: Steps 103/138: batch_recall = 144.29, batch_ndcg = 81.16 
2025-04-08 15:54:28.394934: Steps 104/138: batch_recall = 132.53, batch_ndcg = 76.75 
2025-04-08 15:54:29.692912: Steps 105/138: batch_recall = 116.68, batch_ndcg = 66.41 
2025-04-08 15:54:30.966478: Steps 106/138: batch_recall = 104.76, batch_ndcg = 58.18 
2025-04-08 15:54:32.248359: Steps 107/138: batch_recall = 115.62, batch_ndcg = 64.08 
2025-04-08 15:54:33.526720: Steps 108/138: batch_recall = 119.90, batch_ndcg = 70.81 
2025-04-08 15:54:34.800561: Steps 109/138: batch_recall = 135.20, batch_ndcg = 75.31 
2025-04-08 15:54:36.089656: Steps 110/138: batch_recall = 125.32, batch_ndcg = 64.42 
2025-04-08 15:54:37.370400: Steps 111/138: batch_recall = 141.92, batch_ndcg = 84.90 
2025-04-08 15:54:38.652956: Steps 112/138: batch_recall = 169.00, batch_ndcg = 94.43 
2025-04-08 15:54:39.930036: Steps 113/138: batch_recall = 126.99, batch_ndcg = 72.05 
2025-04-08 15:54:41.208010: Steps 114/138: batch_recall = 124.66, batch_ndcg = 70.66 
2025-04-08 15:54:42.490963: Steps 115/138: batch_recall = 124.02, batch_ndcg = 64.15 
2025-04-08 15:54:43.769263: Steps 116/138: batch_recall = 127.32, batch_ndcg = 66.29 
2025-04-08 15:54:45.053411: Steps 117/138: batch_recall = 118.12, batch_ndcg = 67.66 
2025-04-08 15:54:46.326381: Steps 118/138: batch_recall = 125.30, batch_ndcg = 71.84 
2025-04-08 15:54:47.600393: Steps 119/138: batch_recall = 138.99, batch_ndcg = 75.30 
2025-04-08 15:54:48.882551: Steps 120/138: batch_recall = 125.74, batch_ndcg = 71.12 
2025-04-08 15:54:50.159271: Steps 121/138: batch_recall = 152.67, batch_ndcg = 80.51 
2025-04-08 15:54:51.437927: Steps 122/138: batch_recall = 150.96, batch_ndcg = 83.30 
2025-04-08 15:54:52.718094: Steps 123/138: batch_recall = 129.79, batch_ndcg = 71.74 
2025-04-08 15:54:54.001088: Steps 124/138: batch_recall = 153.85, batch_ndcg = 94.20 
2025-04-08 15:54:55.271172: Steps 125/138: batch_recall = 131.20, batch_ndcg = 71.35 
2025-04-08 15:54:56.546793: Steps 126/138: batch_recall = 158.34, batch_ndcg = 88.37 
2025-04-08 15:54:57.818043: Steps 127/138: batch_recall = 146.51, batch_ndcg = 81.66 
2025-04-08 15:54:59.101172: Steps 128/138: batch_recall = 130.35, batch_ndcg = 71.23 
2025-04-08 15:55:00.380875: Steps 129/138: batch_recall = 157.66, batch_ndcg = 90.97 
2025-04-08 15:55:01.665882: Steps 130/138: batch_recall = 131.83, batch_ndcg = 68.78 
2025-04-08 15:55:02.934573: Steps 131/138: batch_recall = 149.84, batch_ndcg = 85.22 
2025-04-08 15:55:04.207383: Steps 132/138: batch_recall = 150.66, batch_ndcg = 86.32 
2025-04-08 15:55:05.481881: Steps 133/138: batch_recall = 149.02, batch_ndcg = 84.27 
2025-04-08 15:55:06.756708: Steps 134/138: batch_recall = 138.13, batch_ndcg = 79.04 
2025-04-08 15:55:08.026621: Steps 135/138: batch_recall = 166.53, batch_ndcg = 93.94 
2025-04-08 15:55:09.299374: Steps 136/138: batch_recall = 154.53, batch_ndcg = 78.57 
2025-04-08 15:55:10.574764: Steps 137/138: batch_recall = 138.13, batch_ndcg = 85.74 
2025-04-08 15:55:10.575370: Epoch 15/1000, Test: Recall = 0.1770, NDCG = 0.1000  

2025-04-08 15:55:12.351281: Training Step 0/354: batchLoss = 0.5680, diffLoss = 2.7773, kgLoss = 0.0156
2025-04-08 15:55:13.973674: Training Step 1/354: batchLoss = 0.5453, diffLoss = 2.6714, kgLoss = 0.0138
2025-04-08 15:55:15.598438: Training Step 2/354: batchLoss = 0.7164, diffLoss = 3.5225, kgLoss = 0.0149
2025-04-08 15:55:17.221296: Training Step 3/354: batchLoss = 0.5836, diffLoss = 2.8648, kgLoss = 0.0133
2025-04-08 15:55:18.841658: Training Step 4/354: batchLoss = 0.4934, diffLoss = 2.4138, kgLoss = 0.0132
2025-04-08 15:55:20.461089: Training Step 5/354: batchLoss = 0.5367, diffLoss = 2.6199, kgLoss = 0.0159
2025-04-08 15:55:22.083817: Training Step 6/354: batchLoss = 0.4372, diffLoss = 2.1353, kgLoss = 0.0127
2025-04-08 15:55:23.707287: Training Step 7/354: batchLoss = 0.6017, diffLoss = 2.9497, kgLoss = 0.0147
2025-04-08 15:55:25.336564: Training Step 8/354: batchLoss = 0.6543, diffLoss = 3.2093, kgLoss = 0.0156
2025-04-08 15:55:26.967971: Training Step 9/354: batchLoss = 0.5532, diffLoss = 2.7115, kgLoss = 0.0137
2025-04-08 15:55:28.588868: Training Step 10/354: batchLoss = 0.5898, diffLoss = 2.8917, kgLoss = 0.0143
2025-04-08 15:55:30.212229: Training Step 11/354: batchLoss = 0.6785, diffLoss = 3.3308, kgLoss = 0.0154
2025-04-08 15:55:31.844767: Training Step 12/354: batchLoss = 0.4577, diffLoss = 2.2430, kgLoss = 0.0114
2025-04-08 15:55:33.466725: Training Step 13/354: batchLoss = 0.6295, diffLoss = 3.0856, kgLoss = 0.0154
2025-04-08 15:55:35.087820: Training Step 14/354: batchLoss = 0.5793, diffLoss = 2.8384, kgLoss = 0.0145
2025-04-08 15:55:36.707799: Training Step 15/354: batchLoss = 0.6744, diffLoss = 3.3093, kgLoss = 0.0157
2025-04-08 15:55:38.336060: Training Step 16/354: batchLoss = 0.7930, diffLoss = 3.8960, kgLoss = 0.0173
2025-04-08 15:55:39.953449: Training Step 17/354: batchLoss = 0.6384, diffLoss = 3.1276, kgLoss = 0.0161
2025-04-08 15:55:41.587423: Training Step 18/354: batchLoss = 0.7135, diffLoss = 3.5009, kgLoss = 0.0167
2025-04-08 15:55:43.208626: Training Step 19/354: batchLoss = 0.5972, diffLoss = 2.9190, kgLoss = 0.0167
2025-04-08 15:55:44.834549: Training Step 20/354: batchLoss = 0.6559, diffLoss = 3.2181, kgLoss = 0.0154
2025-04-08 15:55:46.458288: Training Step 21/354: batchLoss = 0.5876, diffLoss = 2.8787, kgLoss = 0.0148
2025-04-08 15:55:48.071972: Training Step 22/354: batchLoss = 0.5234, diffLoss = 2.5647, kgLoss = 0.0130
2025-04-08 15:55:49.699860: Training Step 23/354: batchLoss = 0.5684, diffLoss = 2.7890, kgLoss = 0.0132
2025-04-08 15:55:51.321077: Training Step 24/354: batchLoss = 0.5510, diffLoss = 2.6976, kgLoss = 0.0144
2025-04-08 15:55:52.952567: Training Step 25/354: batchLoss = 0.5374, diffLoss = 2.6205, kgLoss = 0.0166
2025-04-08 15:55:54.579473: Training Step 26/354: batchLoss = 0.5915, diffLoss = 2.8985, kgLoss = 0.0147
2025-04-08 15:55:56.202568: Training Step 27/354: batchLoss = 0.5037, diffLoss = 2.4645, kgLoss = 0.0135
2025-04-08 15:55:57.830234: Training Step 28/354: batchLoss = 0.6306, diffLoss = 3.0894, kgLoss = 0.0159
2025-04-08 15:55:59.455865: Training Step 29/354: batchLoss = 0.5810, diffLoss = 2.8490, kgLoss = 0.0141
2025-04-08 15:56:01.075243: Training Step 30/354: batchLoss = 0.4783, diffLoss = 2.3399, kgLoss = 0.0128
2025-04-08 15:56:02.691768: Training Step 31/354: batchLoss = 0.6228, diffLoss = 3.0499, kgLoss = 0.0160
2025-04-08 15:56:04.316331: Training Step 32/354: batchLoss = 0.6731, diffLoss = 3.2971, kgLoss = 0.0171
2025-04-08 15:56:05.937777: Training Step 33/354: batchLoss = 0.5363, diffLoss = 2.6197, kgLoss = 0.0154
2025-04-08 15:56:07.563003: Training Step 34/354: batchLoss = 0.5141, diffLoss = 2.5068, kgLoss = 0.0159
2025-04-08 15:56:09.193587: Training Step 35/354: batchLoss = 0.5830, diffLoss = 2.8545, kgLoss = 0.0151
2025-04-08 15:56:10.817742: Training Step 36/354: batchLoss = 0.4961, diffLoss = 2.4323, kgLoss = 0.0120
2025-04-08 15:56:12.442897: Training Step 37/354: batchLoss = 0.6987, diffLoss = 3.4284, kgLoss = 0.0162
2025-04-08 15:56:14.059808: Training Step 38/354: batchLoss = 0.5209, diffLoss = 2.5567, kgLoss = 0.0120
2025-04-08 15:56:15.670594: Training Step 39/354: batchLoss = 0.5829, diffLoss = 2.8525, kgLoss = 0.0155
2025-04-08 15:56:17.280290: Training Step 40/354: batchLoss = 0.5466, diffLoss = 2.6723, kgLoss = 0.0152
2025-04-08 15:56:18.897448: Training Step 41/354: batchLoss = 0.6145, diffLoss = 3.0133, kgLoss = 0.0148
2025-04-08 15:56:20.520972: Training Step 42/354: batchLoss = 0.5479, diffLoss = 2.6882, kgLoss = 0.0129
2025-04-08 15:56:22.151454: Training Step 43/354: batchLoss = 0.5535, diffLoss = 2.7097, kgLoss = 0.0145
2025-04-08 15:56:23.774977: Training Step 44/354: batchLoss = 0.6540, diffLoss = 3.2092, kgLoss = 0.0152
2025-04-08 15:56:25.397142: Training Step 45/354: batchLoss = 0.8180, diffLoss = 4.0162, kgLoss = 0.0184
2025-04-08 15:56:27.017370: Training Step 46/354: batchLoss = 0.5279, diffLoss = 2.5789, kgLoss = 0.0151
2025-04-08 15:56:28.637037: Training Step 47/354: batchLoss = 0.4714, diffLoss = 2.3106, kgLoss = 0.0116
2025-04-08 15:56:30.255991: Training Step 48/354: batchLoss = 0.6572, diffLoss = 3.2202, kgLoss = 0.0165
2025-04-08 15:56:31.880634: Training Step 49/354: batchLoss = 0.6189, diffLoss = 3.0353, kgLoss = 0.0148
2025-04-08 15:56:33.500302: Training Step 50/354: batchLoss = 0.6957, diffLoss = 3.4150, kgLoss = 0.0158
2025-04-08 15:56:35.124657: Training Step 51/354: batchLoss = 0.5112, diffLoss = 2.4995, kgLoss = 0.0141
2025-04-08 15:56:36.745457: Training Step 52/354: batchLoss = 0.5103, diffLoss = 2.4976, kgLoss = 0.0135
2025-04-08 15:56:38.368188: Training Step 53/354: batchLoss = 0.6264, diffLoss = 3.0695, kgLoss = 0.0156
2025-04-08 15:56:39.987517: Training Step 54/354: batchLoss = 0.5338, diffLoss = 2.6178, kgLoss = 0.0128
2025-04-08 15:56:41.609469: Training Step 55/354: batchLoss = 0.6205, diffLoss = 3.0409, kgLoss = 0.0154
2025-04-08 15:56:43.229707: Training Step 56/354: batchLoss = 0.5706, diffLoss = 2.7947, kgLoss = 0.0146
2025-04-08 15:56:44.844682: Training Step 57/354: batchLoss = 0.5217, diffLoss = 2.5598, kgLoss = 0.0122
2025-04-08 15:56:46.456670: Training Step 58/354: batchLoss = 0.6728, diffLoss = 3.3009, kgLoss = 0.0157
2025-04-08 15:56:48.078526: Training Step 59/354: batchLoss = 0.4886, diffLoss = 2.3951, kgLoss = 0.0120
2025-04-08 15:56:49.703921: Training Step 60/354: batchLoss = 0.6411, diffLoss = 3.1405, kgLoss = 0.0162
2025-04-08 15:56:51.316438: Training Step 61/354: batchLoss = 0.4945, diffLoss = 2.4202, kgLoss = 0.0130
2025-04-08 15:56:52.937562: Training Step 62/354: batchLoss = 0.6298, diffLoss = 3.0895, kgLoss = 0.0149
2025-04-08 15:56:54.567902: Training Step 63/354: batchLoss = 0.6201, diffLoss = 3.0395, kgLoss = 0.0153
2025-04-08 15:56:56.186440: Training Step 64/354: batchLoss = 0.4806, diffLoss = 2.3490, kgLoss = 0.0135
2025-04-08 15:56:57.807638: Training Step 65/354: batchLoss = 0.5717, diffLoss = 2.7974, kgLoss = 0.0153
2025-04-08 15:56:59.431442: Training Step 66/354: batchLoss = 0.4956, diffLoss = 2.4258, kgLoss = 0.0130
2025-04-08 15:57:01.051027: Training Step 67/354: batchLoss = 0.5764, diffLoss = 2.8214, kgLoss = 0.0151
2025-04-08 15:57:02.665208: Training Step 68/354: batchLoss = 0.8291, diffLoss = 4.0671, kgLoss = 0.0196
2025-04-08 15:57:04.284895: Training Step 69/354: batchLoss = 0.5163, diffLoss = 2.5314, kgLoss = 0.0126
2025-04-08 15:57:05.910937: Training Step 70/354: batchLoss = 0.5427, diffLoss = 2.6563, kgLoss = 0.0143
2025-04-08 15:57:07.530110: Training Step 71/354: batchLoss = 0.6901, diffLoss = 3.3902, kgLoss = 0.0150
2025-04-08 15:57:09.150862: Training Step 72/354: batchLoss = 0.5746, diffLoss = 2.8134, kgLoss = 0.0149
2025-04-08 15:57:10.772361: Training Step 73/354: batchLoss = 0.5044, diffLoss = 2.4662, kgLoss = 0.0140
2025-04-08 15:57:12.393393: Training Step 74/354: batchLoss = 0.5569, diffLoss = 2.7139, kgLoss = 0.0177
2025-04-08 15:57:14.008597: Training Step 75/354: batchLoss = 0.5399, diffLoss = 2.6414, kgLoss = 0.0145
2025-04-08 15:57:15.634162: Training Step 76/354: batchLoss = 0.5523, diffLoss = 2.7057, kgLoss = 0.0140
2025-04-08 15:57:17.263188: Training Step 77/354: batchLoss = 0.5218, diffLoss = 2.5502, kgLoss = 0.0147
2025-04-08 15:57:18.884251: Training Step 78/354: batchLoss = 0.6049, diffLoss = 2.9676, kgLoss = 0.0142
2025-04-08 15:57:20.510215: Training Step 79/354: batchLoss = 0.5920, diffLoss = 2.9086, kgLoss = 0.0128
2025-04-08 15:57:22.132037: Training Step 80/354: batchLoss = 0.5322, diffLoss = 2.6042, kgLoss = 0.0141
2025-04-08 15:57:23.750137: Training Step 81/354: batchLoss = 0.5633, diffLoss = 2.7653, kgLoss = 0.0129
2025-04-08 15:57:25.368305: Training Step 82/354: batchLoss = 0.5364, diffLoss = 2.6242, kgLoss = 0.0145
2025-04-08 15:57:26.993815: Training Step 83/354: batchLoss = 0.6215, diffLoss = 3.0478, kgLoss = 0.0150
2025-04-08 15:57:28.617077: Training Step 84/354: batchLoss = 0.6276, diffLoss = 3.0694, kgLoss = 0.0171
2025-04-08 15:57:30.229627: Training Step 85/354: batchLoss = 0.5962, diffLoss = 2.9268, kgLoss = 0.0135
2025-04-08 15:57:31.856205: Training Step 86/354: batchLoss = 0.7461, diffLoss = 3.6599, kgLoss = 0.0176
2025-04-08 15:57:33.481067: Training Step 87/354: batchLoss = 0.5347, diffLoss = 2.6167, kgLoss = 0.0142
2025-04-08 15:57:35.111711: Training Step 88/354: batchLoss = 0.4887, diffLoss = 2.3864, kgLoss = 0.0143
2025-04-08 15:57:36.736027: Training Step 89/354: batchLoss = 0.6897, diffLoss = 3.3803, kgLoss = 0.0170
2025-04-08 15:57:38.355803: Training Step 90/354: batchLoss = 0.6195, diffLoss = 3.0373, kgLoss = 0.0150
2025-04-08 15:57:39.978753: Training Step 91/354: batchLoss = 0.7297, diffLoss = 3.5770, kgLoss = 0.0179
2025-04-08 15:57:41.609413: Training Step 92/354: batchLoss = 0.4805, diffLoss = 2.3567, kgLoss = 0.0115
2025-04-08 15:57:43.234311: Training Step 93/354: batchLoss = 0.5296, diffLoss = 2.6009, kgLoss = 0.0118
2025-04-08 15:57:44.850865: Training Step 94/354: batchLoss = 0.5715, diffLoss = 2.7977, kgLoss = 0.0150
2025-04-08 15:57:46.471347: Training Step 95/354: batchLoss = 0.6286, diffLoss = 3.0854, kgLoss = 0.0144
2025-04-08 15:57:48.089879: Training Step 96/354: batchLoss = 0.6151, diffLoss = 3.0161, kgLoss = 0.0149
2025-04-08 15:57:49.712695: Training Step 97/354: batchLoss = 0.5432, diffLoss = 2.6646, kgLoss = 0.0129
2025-04-08 15:57:51.337701: Training Step 98/354: batchLoss = 0.5287, diffLoss = 2.5970, kgLoss = 0.0117
2025-04-08 15:57:52.962214: Training Step 99/354: batchLoss = 0.5441, diffLoss = 2.6691, kgLoss = 0.0128
2025-04-08 15:57:54.581511: Training Step 100/354: batchLoss = 1.1172, diffLoss = 5.4773, kgLoss = 0.0272
2025-04-08 15:57:56.199597: Training Step 101/354: batchLoss = 0.4667, diffLoss = 2.2907, kgLoss = 0.0108
2025-04-08 15:57:57.824579: Training Step 102/354: batchLoss = 0.6862, diffLoss = 3.3577, kgLoss = 0.0184
2025-04-08 15:57:59.440515: Training Step 103/354: batchLoss = 0.6612, diffLoss = 3.2413, kgLoss = 0.0162
2025-04-08 15:58:01.055832: Training Step 104/354: batchLoss = 0.5385, diffLoss = 2.6358, kgLoss = 0.0141
2025-04-08 15:58:02.676998: Training Step 105/354: batchLoss = 0.5873, diffLoss = 2.8839, kgLoss = 0.0132
2025-04-08 15:58:04.297468: Training Step 106/354: batchLoss = 0.6741, diffLoss = 3.3054, kgLoss = 0.0163
2025-04-08 15:58:05.926215: Training Step 107/354: batchLoss = 0.6324, diffLoss = 3.1069, kgLoss = 0.0137
2025-04-08 15:58:07.549779: Training Step 108/354: batchLoss = 0.6408, diffLoss = 3.1463, kgLoss = 0.0144
2025-04-08 15:58:09.171120: Training Step 109/354: batchLoss = 0.5603, diffLoss = 2.7511, kgLoss = 0.0126
2025-04-08 15:58:10.794455: Training Step 110/354: batchLoss = 0.5277, diffLoss = 2.5733, kgLoss = 0.0163
2025-04-08 15:58:12.419998: Training Step 111/354: batchLoss = 0.7679, diffLoss = 3.7667, kgLoss = 0.0182
2025-04-08 15:58:14.047213: Training Step 112/354: batchLoss = 0.5139, diffLoss = 2.5140, kgLoss = 0.0139
2025-04-08 15:58:15.671180: Training Step 113/354: batchLoss = 0.8049, diffLoss = 3.9489, kgLoss = 0.0189
2025-04-08 15:58:17.297582: Training Step 114/354: batchLoss = 0.6246, diffLoss = 3.0668, kgLoss = 0.0140
2025-04-08 15:58:18.918456: Training Step 115/354: batchLoss = 0.6064, diffLoss = 2.9615, kgLoss = 0.0177
2025-04-08 15:58:20.541531: Training Step 116/354: batchLoss = 0.5252, diffLoss = 2.5674, kgLoss = 0.0147
2025-04-08 15:58:22.174102: Training Step 117/354: batchLoss = 0.5290, diffLoss = 2.5921, kgLoss = 0.0132
2025-04-08 15:58:23.803166: Training Step 118/354: batchLoss = 0.5517, diffLoss = 2.7021, kgLoss = 0.0141
2025-04-08 15:58:25.433476: Training Step 119/354: batchLoss = 0.7037, diffLoss = 3.4462, kgLoss = 0.0181
2025-04-08 15:58:27.055830: Training Step 120/354: batchLoss = 0.6626, diffLoss = 3.2523, kgLoss = 0.0152
2025-04-08 15:58:28.694740: Training Step 121/354: batchLoss = 0.5291, diffLoss = 2.5920, kgLoss = 0.0133
2025-04-08 15:58:30.324879: Training Step 122/354: batchLoss = 0.5482, diffLoss = 2.6884, kgLoss = 0.0131
2025-04-08 15:58:31.947432: Training Step 123/354: batchLoss = 0.5116, diffLoss = 2.5034, kgLoss = 0.0137
2025-04-08 15:58:33.573144: Training Step 124/354: batchLoss = 0.5550, diffLoss = 2.7215, kgLoss = 0.0134
2025-04-08 15:58:35.205751: Training Step 125/354: batchLoss = 0.4783, diffLoss = 2.3457, kgLoss = 0.0115
2025-04-08 15:58:36.828526: Training Step 126/354: batchLoss = 0.8261, diffLoss = 4.0519, kgLoss = 0.0197
2025-04-08 15:58:38.463806: Training Step 127/354: batchLoss = 0.6807, diffLoss = 3.3378, kgLoss = 0.0164
2025-04-08 15:58:40.090320: Training Step 128/354: batchLoss = 0.5890, diffLoss = 2.8877, kgLoss = 0.0143
2025-04-08 15:58:41.718305: Training Step 129/354: batchLoss = 0.5314, diffLoss = 2.6009, kgLoss = 0.0140
2025-04-08 15:58:43.343241: Training Step 130/354: batchLoss = 0.5904, diffLoss = 2.8938, kgLoss = 0.0145
2025-04-08 15:58:44.963297: Training Step 131/354: batchLoss = 0.5209, diffLoss = 2.5430, kgLoss = 0.0154
2025-04-08 15:58:46.587245: Training Step 132/354: batchLoss = 0.4583, diffLoss = 2.2455, kgLoss = 0.0116
2025-04-08 15:58:48.207720: Training Step 133/354: batchLoss = 0.6904, diffLoss = 3.3900, kgLoss = 0.0155
2025-04-08 15:58:49.834209: Training Step 134/354: batchLoss = 0.6830, diffLoss = 3.3459, kgLoss = 0.0173
2025-04-08 15:58:51.448280: Training Step 135/354: batchLoss = 0.4513, diffLoss = 2.2010, kgLoss = 0.0138
2025-04-08 15:58:53.069000: Training Step 136/354: batchLoss = 0.6325, diffLoss = 3.0982, kgLoss = 0.0161
2025-04-08 15:58:54.682318: Training Step 137/354: batchLoss = 0.6954, diffLoss = 3.4116, kgLoss = 0.0163
2025-04-08 15:58:56.297241: Training Step 138/354: batchLoss = 0.5103, diffLoss = 2.5004, kgLoss = 0.0127
2025-04-08 15:58:57.914494: Training Step 139/354: batchLoss = 0.4995, diffLoss = 2.4426, kgLoss = 0.0137
2025-04-08 15:58:59.529940: Training Step 140/354: batchLoss = 0.5900, diffLoss = 2.8893, kgLoss = 0.0151
2025-04-08 15:59:01.147244: Training Step 141/354: batchLoss = 0.5132, diffLoss = 2.5127, kgLoss = 0.0134
2025-04-08 15:59:02.771892: Training Step 142/354: batchLoss = 0.6870, diffLoss = 3.3668, kgLoss = 0.0171
2025-04-08 15:59:04.392118: Training Step 143/354: batchLoss = 0.4586, diffLoss = 2.2450, kgLoss = 0.0120
2025-04-08 15:59:06.008897: Training Step 144/354: batchLoss = 0.5433, diffLoss = 2.6626, kgLoss = 0.0134
2025-04-08 15:59:07.632261: Training Step 145/354: batchLoss = 0.9401, diffLoss = 4.6132, kgLoss = 0.0218
2025-04-08 15:59:09.259434: Training Step 146/354: batchLoss = 0.5150, diffLoss = 2.5259, kgLoss = 0.0123
2025-04-08 15:59:10.870405: Training Step 147/354: batchLoss = 0.5098, diffLoss = 2.4957, kgLoss = 0.0133
2025-04-08 15:59:12.490135: Training Step 148/354: batchLoss = 0.5303, diffLoss = 2.5949, kgLoss = 0.0142
2025-04-08 15:59:14.108710: Training Step 149/354: batchLoss = 0.5168, diffLoss = 2.5292, kgLoss = 0.0137
2025-04-08 15:59:15.731135: Training Step 150/354: batchLoss = 0.5486, diffLoss = 2.6903, kgLoss = 0.0132
2025-04-08 15:59:17.349740: Training Step 151/354: batchLoss = 0.5501, diffLoss = 2.6946, kgLoss = 0.0140
2025-04-08 15:59:18.978917: Training Step 152/354: batchLoss = 0.5847, diffLoss = 2.8637, kgLoss = 0.0149
2025-04-08 15:59:20.601742: Training Step 153/354: batchLoss = 0.5770, diffLoss = 2.8298, kgLoss = 0.0138
2025-04-08 15:59:22.219171: Training Step 154/354: batchLoss = 0.6167, diffLoss = 3.0252, kgLoss = 0.0145
2025-04-08 15:59:23.844939: Training Step 155/354: batchLoss = 0.4931, diffLoss = 2.4155, kgLoss = 0.0124
2025-04-08 15:59:25.460288: Training Step 156/354: batchLoss = 0.5894, diffLoss = 2.8915, kgLoss = 0.0139
2025-04-08 15:59:27.076701: Training Step 157/354: batchLoss = 0.5137, diffLoss = 2.5110, kgLoss = 0.0144
2025-04-08 15:59:28.692353: Training Step 158/354: batchLoss = 0.4344, diffLoss = 2.1206, kgLoss = 0.0128
2025-04-08 15:59:30.311015: Training Step 159/354: batchLoss = 0.6858, diffLoss = 3.3643, kgLoss = 0.0162
2025-04-08 15:59:31.927070: Training Step 160/354: batchLoss = 0.5837, diffLoss = 2.8618, kgLoss = 0.0142
2025-04-08 15:59:33.545720: Training Step 161/354: batchLoss = 0.5013, diffLoss = 2.4588, kgLoss = 0.0120
2025-04-08 15:59:35.163973: Training Step 162/354: batchLoss = 0.5757, diffLoss = 2.8286, kgLoss = 0.0125
2025-04-08 15:59:36.784360: Training Step 163/354: batchLoss = 0.5175, diffLoss = 2.5418, kgLoss = 0.0114
2025-04-08 15:59:38.408167: Training Step 164/354: batchLoss = 0.4827, diffLoss = 2.3601, kgLoss = 0.0133
2025-04-08 15:59:40.027567: Training Step 165/354: batchLoss = 0.6396, diffLoss = 3.1343, kgLoss = 0.0159
2025-04-08 15:59:41.642282: Training Step 166/354: batchLoss = 0.5011, diffLoss = 2.4480, kgLoss = 0.0143
2025-04-08 15:59:43.255895: Training Step 167/354: batchLoss = 0.5371, diffLoss = 2.6302, kgLoss = 0.0138
2025-04-08 15:59:44.872669: Training Step 168/354: batchLoss = 0.5642, diffLoss = 2.7664, kgLoss = 0.0136
2025-04-08 15:59:46.494932: Training Step 169/354: batchLoss = 0.4958, diffLoss = 2.4242, kgLoss = 0.0138
2025-04-08 15:59:48.117326: Training Step 170/354: batchLoss = 0.5494, diffLoss = 2.6858, kgLoss = 0.0152
2025-04-08 15:59:49.734935: Training Step 171/354: batchLoss = 0.5517, diffLoss = 2.7071, kgLoss = 0.0128
2025-04-08 15:59:51.355306: Training Step 172/354: batchLoss = 0.5125, diffLoss = 2.5075, kgLoss = 0.0137
2025-04-08 15:59:52.971700: Training Step 173/354: batchLoss = 0.4850, diffLoss = 2.3767, kgLoss = 0.0121
2025-04-08 15:59:54.590291: Training Step 174/354: batchLoss = 0.5402, diffLoss = 2.6427, kgLoss = 0.0146
2025-04-08 15:59:56.203936: Training Step 175/354: batchLoss = 0.6333, diffLoss = 3.1102, kgLoss = 0.0141
2025-04-08 15:59:57.823318: Training Step 176/354: batchLoss = 0.7369, diffLoss = 3.6155, kgLoss = 0.0172
2025-04-08 15:59:59.438782: Training Step 177/354: batchLoss = 0.6678, diffLoss = 3.2714, kgLoss = 0.0169
2025-04-08 16:00:01.058177: Training Step 178/354: batchLoss = 0.6374, diffLoss = 3.1264, kgLoss = 0.0151
2025-04-08 16:00:02.679570: Training Step 179/354: batchLoss = 0.4975, diffLoss = 2.4337, kgLoss = 0.0135
2025-04-08 16:00:04.295102: Training Step 180/354: batchLoss = 0.6423, diffLoss = 3.1560, kgLoss = 0.0139
2025-04-08 16:00:05.913567: Training Step 181/354: batchLoss = 0.7551, diffLoss = 3.7092, kgLoss = 0.0166
2025-04-08 16:00:07.544785: Training Step 182/354: batchLoss = 0.6583, diffLoss = 3.2334, kgLoss = 0.0146
2025-04-08 16:00:09.155628: Training Step 183/354: batchLoss = 0.5689, diffLoss = 2.7881, kgLoss = 0.0141
2025-04-08 16:00:10.771349: Training Step 184/354: batchLoss = 0.5146, diffLoss = 2.5270, kgLoss = 0.0115
2025-04-08 16:00:12.387414: Training Step 185/354: batchLoss = 0.4850, diffLoss = 2.3805, kgLoss = 0.0111
2025-04-08 16:00:14.010492: Training Step 186/354: batchLoss = 0.7254, diffLoss = 3.5586, kgLoss = 0.0171
2025-04-08 16:00:15.633322: Training Step 187/354: batchLoss = 0.5526, diffLoss = 2.6990, kgLoss = 0.0160
2025-04-08 16:00:17.250497: Training Step 188/354: batchLoss = 0.6401, diffLoss = 3.1345, kgLoss = 0.0165
2025-04-08 16:00:18.874607: Training Step 189/354: batchLoss = 0.5173, diffLoss = 2.5386, kgLoss = 0.0119
2025-04-08 16:00:20.501134: Training Step 190/354: batchLoss = 0.6072, diffLoss = 2.9728, kgLoss = 0.0158
2025-04-08 16:00:22.121080: Training Step 191/354: batchLoss = 0.5713, diffLoss = 2.7939, kgLoss = 0.0156
2025-04-08 16:00:23.735336: Training Step 192/354: batchLoss = 0.6154, diffLoss = 3.0194, kgLoss = 0.0144
2025-04-08 16:00:25.350022: Training Step 193/354: batchLoss = 0.5159, diffLoss = 2.5254, kgLoss = 0.0136
2025-04-08 16:00:26.970295: Training Step 194/354: batchLoss = 0.6776, diffLoss = 3.3198, kgLoss = 0.0170
2025-04-08 16:00:28.587944: Training Step 195/354: batchLoss = 0.6570, diffLoss = 3.2181, kgLoss = 0.0167
2025-04-08 16:00:30.209075: Training Step 196/354: batchLoss = 0.5878, diffLoss = 2.8785, kgLoss = 0.0152
2025-04-08 16:00:31.832876: Training Step 197/354: batchLoss = 0.6099, diffLoss = 2.9879, kgLoss = 0.0155
2025-04-08 16:00:33.456383: Training Step 198/354: batchLoss = 0.5212, diffLoss = 2.5537, kgLoss = 0.0131
2025-04-08 16:00:35.077770: Training Step 199/354: batchLoss = 0.4470, diffLoss = 2.1861, kgLoss = 0.0122
2025-04-08 16:00:36.692981: Training Step 200/354: batchLoss = 0.6485, diffLoss = 3.1742, kgLoss = 0.0171
2025-04-08 16:00:38.309856: Training Step 201/354: batchLoss = 0.4636, diffLoss = 2.2535, kgLoss = 0.0161
2025-04-08 16:00:39.922222: Training Step 202/354: batchLoss = 0.5884, diffLoss = 2.8882, kgLoss = 0.0135
2025-04-08 16:00:41.536945: Training Step 203/354: batchLoss = 0.5214, diffLoss = 2.5529, kgLoss = 0.0135
2025-04-08 16:00:43.156198: Training Step 204/354: batchLoss = 0.4753, diffLoss = 2.3312, kgLoss = 0.0113
2025-04-08 16:00:44.772675: Training Step 205/354: batchLoss = 0.6323, diffLoss = 3.0955, kgLoss = 0.0165
2025-04-08 16:00:46.392464: Training Step 206/354: batchLoss = 0.4497, diffLoss = 2.2059, kgLoss = 0.0107
2025-04-08 16:00:48.015112: Training Step 207/354: batchLoss = 0.5230, diffLoss = 2.5625, kgLoss = 0.0131
2025-04-08 16:00:49.636708: Training Step 208/354: batchLoss = 0.6802, diffLoss = 3.3401, kgLoss = 0.0153
2025-04-08 16:00:51.255217: Training Step 209/354: batchLoss = 0.5016, diffLoss = 2.4482, kgLoss = 0.0150
2025-04-08 16:00:52.873345: Training Step 210/354: batchLoss = 0.5137, diffLoss = 2.5169, kgLoss = 0.0129
2025-04-08 16:00:54.489959: Training Step 211/354: batchLoss = 0.5863, diffLoss = 2.8699, kgLoss = 0.0154
2025-04-08 16:00:56.104478: Training Step 212/354: batchLoss = 0.6378, diffLoss = 3.1303, kgLoss = 0.0147
2025-04-08 16:00:57.719971: Training Step 213/354: batchLoss = 0.7742, diffLoss = 3.7977, kgLoss = 0.0183
2025-04-08 16:00:59.339530: Training Step 214/354: batchLoss = 0.5638, diffLoss = 2.7706, kgLoss = 0.0121
2025-04-08 16:01:00.961623: Training Step 215/354: batchLoss = 0.5201, diffLoss = 2.5475, kgLoss = 0.0133
2025-04-08 16:01:02.584761: Training Step 216/354: batchLoss = 0.5593, diffLoss = 2.7324, kgLoss = 0.0161
2025-04-08 16:01:04.203915: Training Step 217/354: batchLoss = 0.5394, diffLoss = 2.6415, kgLoss = 0.0139
2025-04-08 16:01:05.824393: Training Step 218/354: batchLoss = 0.6870, diffLoss = 3.3699, kgLoss = 0.0163
2025-04-08 16:01:07.448855: Training Step 219/354: batchLoss = 0.5095, diffLoss = 2.4951, kgLoss = 0.0132
2025-04-08 16:01:09.060403: Training Step 220/354: batchLoss = 0.5176, diffLoss = 2.5299, kgLoss = 0.0145
2025-04-08 16:01:10.676963: Training Step 221/354: batchLoss = 0.6987, diffLoss = 3.4314, kgLoss = 0.0156
2025-04-08 16:01:12.292085: Training Step 222/354: batchLoss = 0.5735, diffLoss = 2.8096, kgLoss = 0.0144
2025-04-08 16:01:13.920647: Training Step 223/354: batchLoss = 0.5176, diffLoss = 2.5330, kgLoss = 0.0138
2025-04-08 16:01:15.537250: Training Step 224/354: batchLoss = 0.6235, diffLoss = 3.0520, kgLoss = 0.0163
2025-04-08 16:01:17.154106: Training Step 225/354: batchLoss = 0.4385, diffLoss = 2.1357, kgLoss = 0.0142
2025-04-08 16:01:18.779462: Training Step 226/354: batchLoss = 0.6292, diffLoss = 3.0824, kgLoss = 0.0159
2025-04-08 16:01:20.404656: Training Step 227/354: batchLoss = 0.5274, diffLoss = 2.5827, kgLoss = 0.0136
2025-04-08 16:01:22.024203: Training Step 228/354: batchLoss = 0.5194, diffLoss = 2.5339, kgLoss = 0.0158
2025-04-08 16:01:23.637296: Training Step 229/354: batchLoss = 0.5611, diffLoss = 2.7502, kgLoss = 0.0139
2025-04-08 16:01:25.259899: Training Step 230/354: batchLoss = 0.4444, diffLoss = 2.1781, kgLoss = 0.0110
2025-04-08 16:01:26.872760: Training Step 231/354: batchLoss = 0.5157, diffLoss = 2.5290, kgLoss = 0.0124
2025-04-08 16:01:28.496655: Training Step 232/354: batchLoss = 0.6495, diffLoss = 3.1836, kgLoss = 0.0160
2025-04-08 16:01:30.119174: Training Step 233/354: batchLoss = 0.5939, diffLoss = 2.9128, kgLoss = 0.0141
2025-04-08 16:01:31.735299: Training Step 234/354: batchLoss = 0.6916, diffLoss = 3.3945, kgLoss = 0.0159
2025-04-08 16:01:33.357762: Training Step 235/354: batchLoss = 0.6730, diffLoss = 3.2934, kgLoss = 0.0179
2025-04-08 16:01:34.975396: Training Step 236/354: batchLoss = 0.5840, diffLoss = 2.8642, kgLoss = 0.0140
2025-04-08 16:01:36.593328: Training Step 237/354: batchLoss = 0.5378, diffLoss = 2.6366, kgLoss = 0.0131
2025-04-08 16:01:38.210210: Training Step 238/354: batchLoss = 0.6099, diffLoss = 2.9920, kgLoss = 0.0144
2025-04-08 16:01:39.824157: Training Step 239/354: batchLoss = 0.5639, diffLoss = 2.7634, kgLoss = 0.0140
2025-04-08 16:01:41.439671: Training Step 240/354: batchLoss = 0.5818, diffLoss = 2.8513, kgLoss = 0.0144
2025-04-08 16:01:43.058709: Training Step 241/354: batchLoss = 0.5701, diffLoss = 2.7937, kgLoss = 0.0141
2025-04-08 16:01:44.680456: Training Step 242/354: batchLoss = 0.5259, diffLoss = 2.5728, kgLoss = 0.0141
2025-04-08 16:01:46.300499: Training Step 243/354: batchLoss = 0.6375, diffLoss = 3.1287, kgLoss = 0.0148
2025-04-08 16:01:47.919841: Training Step 244/354: batchLoss = 0.4778, diffLoss = 2.3359, kgLoss = 0.0133
2025-04-08 16:01:49.534502: Training Step 245/354: batchLoss = 0.5152, diffLoss = 2.5256, kgLoss = 0.0126
2025-04-08 16:01:51.153388: Training Step 246/354: batchLoss = 0.5795, diffLoss = 2.8403, kgLoss = 0.0143
2025-04-08 16:01:52.774615: Training Step 247/354: batchLoss = 0.6672, diffLoss = 3.2688, kgLoss = 0.0168
2025-04-08 16:01:54.394197: Training Step 248/354: batchLoss = 0.5900, diffLoss = 2.8797, kgLoss = 0.0176
2025-04-08 16:01:56.011905: Training Step 249/354: batchLoss = 0.5171, diffLoss = 2.5261, kgLoss = 0.0148
2025-04-08 16:01:57.629903: Training Step 250/354: batchLoss = 0.5398, diffLoss = 2.6459, kgLoss = 0.0132
2025-04-08 16:01:59.248017: Training Step 251/354: batchLoss = 0.5777, diffLoss = 2.8330, kgLoss = 0.0139
2025-04-08 16:02:00.868565: Training Step 252/354: batchLoss = 0.4434, diffLoss = 2.1656, kgLoss = 0.0129
2025-04-08 16:02:02.486297: Training Step 253/354: batchLoss = 0.5492, diffLoss = 2.6857, kgLoss = 0.0151
2025-04-08 16:02:04.108628: Training Step 254/354: batchLoss = 0.6750, diffLoss = 3.3139, kgLoss = 0.0152
2025-04-08 16:02:05.733644: Training Step 255/354: batchLoss = 0.5411, diffLoss = 2.6491, kgLoss = 0.0141
2025-04-08 16:02:07.357762: Training Step 256/354: batchLoss = 0.5956, diffLoss = 2.9251, kgLoss = 0.0132
2025-04-08 16:02:08.973239: Training Step 257/354: batchLoss = 0.4486, diffLoss = 2.1885, kgLoss = 0.0137
2025-04-08 16:02:10.589622: Training Step 258/354: batchLoss = 0.4126, diffLoss = 2.0161, kgLoss = 0.0117
2025-04-08 16:02:12.209405: Training Step 259/354: batchLoss = 0.5816, diffLoss = 2.8460, kgLoss = 0.0155
2025-04-08 16:02:13.829738: Training Step 260/354: batchLoss = 0.6016, diffLoss = 2.9383, kgLoss = 0.0174
2025-04-08 16:02:15.446912: Training Step 261/354: batchLoss = 0.7283, diffLoss = 3.5618, kgLoss = 0.0200
2025-04-08 16:02:17.073052: Training Step 262/354: batchLoss = 0.5012, diffLoss = 2.4551, kgLoss = 0.0127
2025-04-08 16:02:18.687806: Training Step 263/354: batchLoss = 0.4943, diffLoss = 2.4251, kgLoss = 0.0115
2025-04-08 16:02:20.308036: Training Step 264/354: batchLoss = 0.5691, diffLoss = 2.7875, kgLoss = 0.0146
2025-04-08 16:02:21.926934: Training Step 265/354: batchLoss = 0.5045, diffLoss = 2.4744, kgLoss = 0.0120
2025-04-08 16:02:23.544065: Training Step 266/354: batchLoss = 0.5606, diffLoss = 2.7003, kgLoss = 0.0257
2025-04-08 16:02:25.153773: Training Step 267/354: batchLoss = 0.7502, diffLoss = 3.6752, kgLoss = 0.0190
2025-04-08 16:02:26.772485: Training Step 268/354: batchLoss = 0.6115, diffLoss = 2.9940, kgLoss = 0.0159
2025-04-08 16:02:28.392642: Training Step 269/354: batchLoss = 0.5915, diffLoss = 2.9013, kgLoss = 0.0141
2025-04-08 16:02:30.014163: Training Step 270/354: batchLoss = 0.5913, diffLoss = 2.8897, kgLoss = 0.0167
2025-04-08 16:02:31.634986: Training Step 271/354: batchLoss = 0.5173, diffLoss = 2.5384, kgLoss = 0.0120
2025-04-08 16:02:33.250394: Training Step 272/354: batchLoss = 0.5928, diffLoss = 2.9076, kgLoss = 0.0141
2025-04-08 16:02:34.865515: Training Step 273/354: batchLoss = 0.5072, diffLoss = 2.4880, kgLoss = 0.0120
2025-04-08 16:02:36.482677: Training Step 274/354: batchLoss = 0.4640, diffLoss = 2.2738, kgLoss = 0.0116
2025-04-08 16:02:38.096950: Training Step 275/354: batchLoss = 0.4758, diffLoss = 2.3350, kgLoss = 0.0110
2025-04-08 16:02:39.708395: Training Step 276/354: batchLoss = 0.5610, diffLoss = 2.7442, kgLoss = 0.0153
2025-04-08 16:02:41.322527: Training Step 277/354: batchLoss = 0.5239, diffLoss = 2.5581, kgLoss = 0.0154
2025-04-08 16:02:42.947880: Training Step 278/354: batchLoss = 0.5912, diffLoss = 2.7918, kgLoss = 0.0410
2025-04-08 16:02:44.566938: Training Step 279/354: batchLoss = 0.5575, diffLoss = 2.7341, kgLoss = 0.0134
2025-04-08 16:02:46.198619: Training Step 280/354: batchLoss = 0.6619, diffLoss = 3.2393, kgLoss = 0.0175
2025-04-08 16:02:47.818780: Training Step 281/354: batchLoss = 0.6571, diffLoss = 3.2147, kgLoss = 0.0177
2025-04-08 16:02:49.440020: Training Step 282/354: batchLoss = 0.5157, diffLoss = 2.5235, kgLoss = 0.0137
2025-04-08 16:02:51.065055: Training Step 283/354: batchLoss = 0.4591, diffLoss = 2.2452, kgLoss = 0.0125
2025-04-08 16:02:52.683801: Training Step 284/354: batchLoss = 0.5908, diffLoss = 2.8900, kgLoss = 0.0160
2025-04-08 16:02:54.293831: Training Step 285/354: batchLoss = 0.6021, diffLoss = 2.9439, kgLoss = 0.0166
2025-04-08 16:02:55.907060: Training Step 286/354: batchLoss = 0.6302, diffLoss = 3.0738, kgLoss = 0.0193
2025-04-08 16:02:57.524376: Training Step 287/354: batchLoss = 0.5766, diffLoss = 2.8256, kgLoss = 0.0143
2025-04-08 16:02:59.148325: Training Step 288/354: batchLoss = 0.6806, diffLoss = 3.3308, kgLoss = 0.0181
2025-04-08 16:03:00.771507: Training Step 289/354: batchLoss = 0.9194, diffLoss = 4.5106, kgLoss = 0.0216
2025-04-08 16:03:02.390294: Training Step 290/354: batchLoss = 0.6631, diffLoss = 3.2485, kgLoss = 0.0168
2025-04-08 16:03:04.009158: Training Step 291/354: batchLoss = 0.4346, diffLoss = 2.1219, kgLoss = 0.0127
2025-04-08 16:03:05.626306: Training Step 292/354: batchLoss = 0.5500, diffLoss = 2.6969, kgLoss = 0.0133
2025-04-08 16:03:07.245120: Training Step 293/354: batchLoss = 0.5179, diffLoss = 2.5290, kgLoss = 0.0152
2025-04-08 16:03:08.855403: Training Step 294/354: batchLoss = 0.6181, diffLoss = 3.0292, kgLoss = 0.0154
2025-04-08 16:03:10.477466: Training Step 295/354: batchLoss = 0.6057, diffLoss = 2.9692, kgLoss = 0.0149
2025-04-08 16:03:12.096486: Training Step 296/354: batchLoss = 0.6033, diffLoss = 2.9509, kgLoss = 0.0164
2025-04-08 16:03:13.711107: Training Step 297/354: batchLoss = 0.5572, diffLoss = 2.7370, kgLoss = 0.0122
2025-04-08 16:03:15.331469: Training Step 298/354: batchLoss = 0.6353, diffLoss = 3.1127, kgLoss = 0.0160
2025-04-08 16:03:16.955459: Training Step 299/354: batchLoss = 0.5658, diffLoss = 2.7750, kgLoss = 0.0135
2025-04-08 16:03:18.579584: Training Step 300/354: batchLoss = 0.4706, diffLoss = 2.3046, kgLoss = 0.0121
2025-04-08 16:03:20.196496: Training Step 301/354: batchLoss = 0.5802, diffLoss = 2.8406, kgLoss = 0.0151
2025-04-08 16:03:21.885377: Training Step 302/354: batchLoss = 0.5436, diffLoss = 2.6668, kgLoss = 0.0128
2025-04-08 16:03:23.499070: Training Step 303/354: batchLoss = 0.5629, diffLoss = 2.7567, kgLoss = 0.0144
2025-04-08 16:03:25.114730: Training Step 304/354: batchLoss = 0.5727, diffLoss = 2.8049, kgLoss = 0.0146
2025-04-08 16:03:26.727836: Training Step 305/354: batchLoss = 0.4967, diffLoss = 2.4308, kgLoss = 0.0132
2025-04-08 16:03:28.346496: Training Step 306/354: batchLoss = 0.5084, diffLoss = 2.4826, kgLoss = 0.0148
2025-04-08 16:03:29.956953: Training Step 307/354: batchLoss = 0.5728, diffLoss = 2.8039, kgLoss = 0.0150
2025-04-08 16:03:31.579448: Training Step 308/354: batchLoss = 0.6572, diffLoss = 3.2196, kgLoss = 0.0166
2025-04-08 16:03:33.195665: Training Step 309/354: batchLoss = 0.5487, diffLoss = 2.6910, kgLoss = 0.0132
2025-04-08 16:03:34.820397: Training Step 310/354: batchLoss = 0.6752, diffLoss = 3.3031, kgLoss = 0.0183
2025-04-08 16:03:36.442943: Training Step 311/354: batchLoss = 0.4960, diffLoss = 2.4305, kgLoss = 0.0124
2025-04-08 16:03:38.057715: Training Step 312/354: batchLoss = 0.6604, diffLoss = 3.2402, kgLoss = 0.0154
2025-04-08 16:03:39.669973: Training Step 313/354: batchLoss = 0.5268, diffLoss = 2.5830, kgLoss = 0.0127
2025-04-08 16:03:41.286961: Training Step 314/354: batchLoss = 0.7195, diffLoss = 3.5329, kgLoss = 0.0161
2025-04-08 16:03:42.903316: Training Step 315/354: batchLoss = 0.5883, diffLoss = 2.8860, kgLoss = 0.0139
2025-04-08 16:03:44.521056: Training Step 316/354: batchLoss = 0.5167, diffLoss = 2.5383, kgLoss = 0.0114
2025-04-08 16:03:46.135027: Training Step 317/354: batchLoss = 0.6027, diffLoss = 2.9565, kgLoss = 0.0143
2025-04-08 16:03:47.754856: Training Step 318/354: batchLoss = 0.5926, diffLoss = 2.9012, kgLoss = 0.0154
2025-04-08 16:03:49.378219: Training Step 319/354: batchLoss = 0.7106, diffLoss = 3.4855, kgLoss = 0.0169
2025-04-08 16:03:51.000671: Training Step 320/354: batchLoss = 0.6150, diffLoss = 3.0120, kgLoss = 0.0157
2025-04-08 16:03:52.626745: Training Step 321/354: batchLoss = 0.5551, diffLoss = 2.7225, kgLoss = 0.0133
2025-04-08 16:03:54.243029: Training Step 322/354: batchLoss = 0.5477, diffLoss = 2.6819, kgLoss = 0.0141
2025-04-08 16:03:55.859093: Training Step 323/354: batchLoss = 0.5778, diffLoss = 2.8280, kgLoss = 0.0153
2025-04-08 16:03:57.479081: Training Step 324/354: batchLoss = 0.6047, diffLoss = 2.9682, kgLoss = 0.0139
2025-04-08 16:03:59.100502: Training Step 325/354: batchLoss = 0.6747, diffLoss = 3.3063, kgLoss = 0.0168
2025-04-08 16:04:00.719142: Training Step 326/354: batchLoss = 0.6087, diffLoss = 2.9894, kgLoss = 0.0135
2025-04-08 16:04:02.335590: Training Step 327/354: batchLoss = 0.6348, diffLoss = 3.1125, kgLoss = 0.0154
2025-04-08 16:04:03.957862: Training Step 328/354: batchLoss = 0.6042, diffLoss = 2.9652, kgLoss = 0.0139
2025-04-08 16:04:05.581752: Training Step 329/354: batchLoss = 0.4339, diffLoss = 2.1174, kgLoss = 0.0131
2025-04-08 16:04:07.192964: Training Step 330/354: batchLoss = 0.5587, diffLoss = 2.7355, kgLoss = 0.0145
2025-04-08 16:04:08.812661: Training Step 331/354: batchLoss = 0.5078, diffLoss = 2.4836, kgLoss = 0.0138
2025-04-08 16:04:10.435657: Training Step 332/354: batchLoss = 0.5947, diffLoss = 2.9128, kgLoss = 0.0151
2025-04-08 16:04:12.056396: Training Step 333/354: batchLoss = 0.4780, diffLoss = 2.3420, kgLoss = 0.0121
2025-04-08 16:04:13.681555: Training Step 334/354: batchLoss = 0.4922, diffLoss = 2.4131, kgLoss = 0.0120
2025-04-08 16:04:15.308184: Training Step 335/354: batchLoss = 0.5344, diffLoss = 2.6163, kgLoss = 0.0139
2025-04-08 16:04:16.932611: Training Step 336/354: batchLoss = 0.5028, diffLoss = 2.4557, kgLoss = 0.0146
2025-04-08 16:04:18.552545: Training Step 337/354: batchLoss = 0.6514, diffLoss = 3.1969, kgLoss = 0.0151
2025-04-08 16:04:20.173444: Training Step 338/354: batchLoss = 0.4801, diffLoss = 2.3539, kgLoss = 0.0116
2025-04-08 16:04:21.784348: Training Step 339/354: batchLoss = 0.5587, diffLoss = 2.7377, kgLoss = 0.0139
2025-04-08 16:04:23.406440: Training Step 340/354: batchLoss = 0.6598, diffLoss = 3.2330, kgLoss = 0.0165
2025-04-08 16:04:25.023866: Training Step 341/354: batchLoss = 0.4820, diffLoss = 2.3615, kgLoss = 0.0121
2025-04-08 16:04:26.646119: Training Step 342/354: batchLoss = 0.5857, diffLoss = 2.8702, kgLoss = 0.0146
2025-04-08 16:04:28.264980: Training Step 343/354: batchLoss = 0.6418, diffLoss = 3.1452, kgLoss = 0.0160
2025-04-08 16:04:29.895282: Training Step 344/354: batchLoss = 0.4925, diffLoss = 2.4150, kgLoss = 0.0118
2025-04-08 16:04:31.521575: Training Step 345/354: batchLoss = 0.4603, diffLoss = 2.2554, kgLoss = 0.0115
2025-04-08 16:04:33.139907: Training Step 346/354: batchLoss = 0.5197, diffLoss = 2.5373, kgLoss = 0.0153
2025-04-08 16:04:34.760499: Training Step 347/354: batchLoss = 0.4964, diffLoss = 2.4266, kgLoss = 0.0139
2025-04-08 16:04:36.400298: Training Step 348/354: batchLoss = 0.6680, diffLoss = 3.2773, kgLoss = 0.0156
2025-04-08 16:04:38.016921: Training Step 349/354: batchLoss = 0.7101, diffLoss = 3.4883, kgLoss = 0.0155
2025-04-08 16:04:39.636408: Training Step 350/354: batchLoss = 0.6620, diffLoss = 3.2486, kgLoss = 0.0153
2025-04-08 16:04:41.244855: Training Step 351/354: batchLoss = 0.4295, diffLoss = 2.1023, kgLoss = 0.0113
2025-04-08 16:04:42.851105: Training Step 352/354: batchLoss = 0.5247, diffLoss = 2.5613, kgLoss = 0.0156
2025-04-08 16:04:44.247859: Training Step 353/354: batchLoss = 0.8831, diffLoss = 4.3383, kgLoss = 0.0193
2025-04-08 16:04:44.341318: 
2025-04-08 16:04:44.341979: Epoch 16/1000, Train: epLoss = 1.0279, epDfLoss = 5.0355, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 16:04:45.679191: Steps 0/138: batch_recall = 45.21, batch_ndcg = 26.40 
2025-04-08 16:04:46.998473: Steps 1/138: batch_recall = 47.45, batch_ndcg = 27.89 
2025-04-08 16:04:48.306758: Steps 2/138: batch_recall = 58.35, batch_ndcg = 36.47 
2025-04-08 16:04:49.623471: Steps 3/138: batch_recall = 59.52, batch_ndcg = 34.22 
2025-04-08 16:04:50.926713: Steps 4/138: batch_recall = 68.27, batch_ndcg = 40.51 
2025-04-08 16:04:52.235678: Steps 5/138: batch_recall = 58.88, batch_ndcg = 32.73 
2025-04-08 16:04:53.535400: Steps 6/138: batch_recall = 52.60, batch_ndcg = 31.23 
2025-04-08 16:04:54.855931: Steps 7/138: batch_recall = 64.37, batch_ndcg = 41.43 
2025-04-08 16:04:56.162097: Steps 8/138: batch_recall = 63.38, batch_ndcg = 39.55 
2025-04-08 16:04:57.470354: Steps 9/138: batch_recall = 56.79, batch_ndcg = 33.88 
2025-04-08 16:04:58.771566: Steps 10/138: batch_recall = 55.16, batch_ndcg = 31.26 
2025-04-08 16:05:00.078448: Steps 11/138: batch_recall = 59.65, batch_ndcg = 34.27 
2025-04-08 16:05:01.375914: Steps 12/138: batch_recall = 53.70, batch_ndcg = 29.48 
2025-04-08 16:05:02.674626: Steps 13/138: batch_recall = 50.66, batch_ndcg = 29.96 
2025-04-08 16:05:03.966285: Steps 14/138: batch_recall = 54.89, batch_ndcg = 31.70 
2025-04-08 16:05:05.252431: Steps 15/138: batch_recall = 49.40, batch_ndcg = 29.48 
2025-04-08 16:05:06.541345: Steps 16/138: batch_recall = 58.90, batch_ndcg = 33.11 
2025-04-08 16:05:07.820448: Steps 17/138: batch_recall = 56.51, batch_ndcg = 32.65 
2025-04-08 16:05:09.111211: Steps 18/138: batch_recall = 51.48, batch_ndcg = 32.52 
2025-04-08 16:05:10.410074: Steps 19/138: batch_recall = 55.60, batch_ndcg = 32.96 
2025-04-08 16:05:11.714635: Steps 20/138: batch_recall = 60.79, batch_ndcg = 36.53 
2025-04-08 16:05:13.013180: Steps 21/138: batch_recall = 67.98, batch_ndcg = 40.40 
2025-04-08 16:05:14.311498: Steps 22/138: batch_recall = 57.54, batch_ndcg = 33.73 
2025-04-08 16:05:15.609467: Steps 23/138: batch_recall = 52.01, batch_ndcg = 30.34 
2025-04-08 16:05:16.907133: Steps 24/138: batch_recall = 57.59, batch_ndcg = 31.21 
2025-04-08 16:05:18.199432: Steps 25/138: batch_recall = 60.06, batch_ndcg = 35.47 
2025-04-08 16:05:19.491756: Steps 26/138: batch_recall = 58.06, batch_ndcg = 33.74 
2025-04-08 16:05:20.777589: Steps 27/138: batch_recall = 63.34, batch_ndcg = 35.41 
2025-04-08 16:05:22.057016: Steps 28/138: batch_recall = 57.24, batch_ndcg = 33.50 
2025-04-08 16:05:23.339387: Steps 29/138: batch_recall = 59.70, batch_ndcg = 31.54 
2025-04-08 16:05:24.623215: Steps 30/138: batch_recall = 57.99, batch_ndcg = 34.52 
2025-04-08 16:05:25.904328: Steps 31/138: batch_recall = 40.77, batch_ndcg = 24.04 
2025-04-08 16:05:27.191484: Steps 32/138: batch_recall = 51.56, batch_ndcg = 30.50 
2025-04-08 16:05:28.468983: Steps 33/138: batch_recall = 60.42, batch_ndcg = 33.75 
2025-04-08 16:05:29.746842: Steps 34/138: batch_recall = 55.59, batch_ndcg = 29.26 
2025-04-08 16:05:31.044569: Steps 35/138: batch_recall = 55.75, batch_ndcg = 31.25 
2025-04-08 16:05:32.345394: Steps 36/138: batch_recall = 50.15, batch_ndcg = 28.76 
2025-04-08 16:05:33.650601: Steps 37/138: batch_recall = 59.42, batch_ndcg = 34.89 
2025-04-08 16:05:34.961155: Steps 38/138: batch_recall = 56.92, batch_ndcg = 32.38 
2025-04-08 16:05:36.276072: Steps 39/138: batch_recall = 66.71, batch_ndcg = 37.81 
2025-04-08 16:05:37.587099: Steps 40/138: batch_recall = 57.16, batch_ndcg = 30.06 
2025-04-08 16:05:38.889645: Steps 41/138: batch_recall = 61.54, batch_ndcg = 33.70 
2025-04-08 16:05:40.169070: Steps 42/138: batch_recall = 55.66, batch_ndcg = 29.98 
2025-04-08 16:05:41.450675: Steps 43/138: batch_recall = 55.45, batch_ndcg = 32.88 
2025-04-08 16:05:42.739456: Steps 44/138: batch_recall = 56.25, batch_ndcg = 30.02 
2025-04-08 16:05:44.024271: Steps 45/138: batch_recall = 63.19, batch_ndcg = 35.47 
2025-04-08 16:05:45.305353: Steps 46/138: batch_recall = 61.79, batch_ndcg = 35.25 
2025-04-08 16:05:46.596452: Steps 47/138: batch_recall = 52.16, batch_ndcg = 31.63 
2025-04-08 16:05:47.877231: Steps 48/138: batch_recall = 56.68, batch_ndcg = 33.51 
2025-04-08 16:05:49.157503: Steps 49/138: batch_recall = 64.52, batch_ndcg = 36.77 
2025-04-08 16:05:50.435740: Steps 50/138: batch_recall = 58.47, batch_ndcg = 31.75 
2025-04-08 16:05:51.716743: Steps 51/138: batch_recall = 58.44, batch_ndcg = 34.73 
2025-04-08 16:05:52.993569: Steps 52/138: batch_recall = 67.02, batch_ndcg = 42.59 
2025-04-08 16:05:54.275055: Steps 53/138: batch_recall = 70.45, batch_ndcg = 35.48 
2025-04-08 16:05:55.551167: Steps 54/138: batch_recall = 65.10, batch_ndcg = 37.54 
2025-04-08 16:05:56.838185: Steps 55/138: batch_recall = 61.41, batch_ndcg = 33.72 
2025-04-08 16:05:58.120185: Steps 56/138: batch_recall = 63.30, batch_ndcg = 34.90 
2025-04-08 16:05:59.405851: Steps 57/138: batch_recall = 57.45, batch_ndcg = 32.83 
2025-04-08 16:06:00.680626: Steps 58/138: batch_recall = 69.80, batch_ndcg = 38.02 
2025-04-08 16:06:01.956331: Steps 59/138: batch_recall = 72.31, batch_ndcg = 42.30 
2025-04-08 16:06:03.228060: Steps 60/138: batch_recall = 68.69, batch_ndcg = 37.80 
2025-04-08 16:06:04.510427: Steps 61/138: batch_recall = 64.57, batch_ndcg = 35.68 
2025-04-08 16:06:05.792835: Steps 62/138: batch_recall = 84.56, batch_ndcg = 43.71 
2025-04-08 16:06:07.079428: Steps 63/138: batch_recall = 74.91, batch_ndcg = 43.58 
2025-04-08 16:06:08.361483: Steps 64/138: batch_recall = 63.51, batch_ndcg = 33.94 
2025-04-08 16:06:09.644186: Steps 65/138: batch_recall = 83.48, batch_ndcg = 46.37 
2025-04-08 16:06:10.923164: Steps 66/138: batch_recall = 68.93, batch_ndcg = 40.57 
2025-04-08 16:06:12.217968: Steps 67/138: batch_recall = 78.05, batch_ndcg = 48.36 
2025-04-08 16:06:13.503410: Steps 68/138: batch_recall = 63.74, batch_ndcg = 33.86 
2025-04-08 16:06:14.787026: Steps 69/138: batch_recall = 87.81, batch_ndcg = 50.72 
2025-04-08 16:06:16.058623: Steps 70/138: batch_recall = 84.39, batch_ndcg = 47.14 
2025-04-08 16:06:17.341331: Steps 71/138: batch_recall = 87.40, batch_ndcg = 50.48 
2025-04-08 16:06:18.622800: Steps 72/138: batch_recall = 85.71, batch_ndcg = 50.38 
2025-04-08 16:06:19.901895: Steps 73/138: batch_recall = 87.49, batch_ndcg = 48.69 
2025-04-08 16:06:21.187427: Steps 74/138: batch_recall = 82.68, batch_ndcg = 49.05 
2025-04-08 16:06:22.467920: Steps 75/138: batch_recall = 87.77, batch_ndcg = 50.67 
2025-04-08 16:06:23.751409: Steps 76/138: batch_recall = 96.23, batch_ndcg = 54.74 
2025-04-08 16:06:25.032580: Steps 77/138: batch_recall = 89.80, batch_ndcg = 49.96 
2025-04-08 16:06:26.318818: Steps 78/138: batch_recall = 90.00, batch_ndcg = 47.68 
2025-04-08 16:06:27.593375: Steps 79/138: batch_recall = 92.78, batch_ndcg = 49.25 
2025-04-08 16:06:28.875187: Steps 80/138: batch_recall = 71.47, batch_ndcg = 39.23 
2025-04-08 16:06:30.159680: Steps 81/138: batch_recall = 79.93, batch_ndcg = 47.47 
2025-04-08 16:06:31.461302: Steps 82/138: batch_recall = 88.09, batch_ndcg = 51.63 
2025-04-08 16:06:32.740305: Steps 83/138: batch_recall = 83.55, batch_ndcg = 48.45 
2025-04-08 16:06:34.017056: Steps 84/138: batch_recall = 101.30, batch_ndcg = 56.94 
2025-04-08 16:06:35.302080: Steps 85/138: batch_recall = 103.88, batch_ndcg = 59.99 
2025-04-08 16:06:36.578948: Steps 86/138: batch_recall = 117.83, batch_ndcg = 70.21 
2025-04-08 16:06:37.868210: Steps 87/138: batch_recall = 109.61, batch_ndcg = 56.61 
2025-04-08 16:06:39.145879: Steps 88/138: batch_recall = 100.85, batch_ndcg = 56.71 
2025-04-08 16:06:40.423267: Steps 89/138: batch_recall = 117.81, batch_ndcg = 67.58 
2025-04-08 16:06:41.712791: Steps 90/138: batch_recall = 103.16, batch_ndcg = 59.21 
2025-04-08 16:06:42.997097: Steps 91/138: batch_recall = 119.97, batch_ndcg = 66.88 
2025-04-08 16:06:44.279309: Steps 92/138: batch_recall = 116.39, batch_ndcg = 63.03 
2025-04-08 16:06:45.562430: Steps 93/138: batch_recall = 118.15, batch_ndcg = 67.98 
2025-04-08 16:06:46.836633: Steps 94/138: batch_recall = 122.20, batch_ndcg = 63.82 
2025-04-08 16:06:48.102231: Steps 95/138: batch_recall = 113.13, batch_ndcg = 67.59 
2025-04-08 16:06:49.371254: Steps 96/138: batch_recall = 123.77, batch_ndcg = 73.35 
2025-04-08 16:06:50.639846: Steps 97/138: batch_recall = 134.05, batch_ndcg = 80.62 
2025-04-08 16:06:51.914761: Steps 98/138: batch_recall = 110.27, batch_ndcg = 63.31 
2025-04-08 16:06:53.192084: Steps 99/138: batch_recall = 128.06, batch_ndcg = 71.61 
2025-04-08 16:06:54.473150: Steps 100/138: batch_recall = 127.18, batch_ndcg = 71.91 
2025-04-08 16:06:55.744171: Steps 101/138: batch_recall = 124.76, batch_ndcg = 70.26 
2025-04-08 16:06:57.013937: Steps 102/138: batch_recall = 126.01, batch_ndcg = 74.06 
2025-04-08 16:06:58.296165: Steps 103/138: batch_recall = 140.96, batch_ndcg = 80.75 
2025-04-08 16:06:59.588121: Steps 104/138: batch_recall = 133.40, batch_ndcg = 77.41 
2025-04-08 16:07:00.856502: Steps 105/138: batch_recall = 115.48, batch_ndcg = 66.31 
2025-04-08 16:07:02.120215: Steps 106/138: batch_recall = 106.01, batch_ndcg = 59.07 
2025-04-08 16:07:03.394778: Steps 107/138: batch_recall = 117.09, batch_ndcg = 65.21 
2025-04-08 16:07:04.669947: Steps 108/138: batch_recall = 119.08, batch_ndcg = 70.94 
2025-04-08 16:07:05.940525: Steps 109/138: batch_recall = 134.41, batch_ndcg = 75.33 
2025-04-08 16:07:07.223933: Steps 110/138: batch_recall = 124.30, batch_ndcg = 64.91 
2025-04-08 16:07:08.499238: Steps 111/138: batch_recall = 142.81, batch_ndcg = 85.95 
2025-04-08 16:07:09.768143: Steps 112/138: batch_recall = 151.75, batch_ndcg = 87.63 
2025-04-08 16:07:11.042246: Steps 113/138: batch_recall = 126.83, batch_ndcg = 72.48 
2025-04-08 16:07:12.325442: Steps 114/138: batch_recall = 125.61, batch_ndcg = 71.48 
2025-04-08 16:07:13.604126: Steps 115/138: batch_recall = 125.03, batch_ndcg = 64.58 
2025-04-08 16:07:14.877639: Steps 116/138: batch_recall = 126.51, batch_ndcg = 66.42 
2025-04-08 16:07:16.138435: Steps 117/138: batch_recall = 115.45, batch_ndcg = 66.31 
2025-04-08 16:07:17.390876: Steps 118/138: batch_recall = 126.23, batch_ndcg = 71.52 
2025-04-08 16:07:18.644558: Steps 119/138: batch_recall = 139.36, batch_ndcg = 74.89 
2025-04-08 16:07:19.903496: Steps 120/138: batch_recall = 126.24, batch_ndcg = 70.83 
2025-04-08 16:07:21.173648: Steps 121/138: batch_recall = 151.17, batch_ndcg = 79.22 
2025-04-08 16:07:22.454226: Steps 122/138: batch_recall = 150.89, batch_ndcg = 82.36 
2025-04-08 16:07:23.745864: Steps 123/138: batch_recall = 130.37, batch_ndcg = 72.56 
2025-04-08 16:07:25.045749: Steps 124/138: batch_recall = 151.73, batch_ndcg = 93.86 
2025-04-08 16:07:26.328119: Steps 125/138: batch_recall = 129.17, batch_ndcg = 71.12 
2025-04-08 16:07:27.592681: Steps 126/138: batch_recall = 160.22, batch_ndcg = 89.59 
2025-04-08 16:07:28.875001: Steps 127/138: batch_recall = 144.26, batch_ndcg = 81.66 
2025-04-08 16:07:30.147645: Steps 128/138: batch_recall = 129.60, batch_ndcg = 71.15 
2025-04-08 16:07:31.429681: Steps 129/138: batch_recall = 158.16, batch_ndcg = 90.38 
2025-04-08 16:07:32.706934: Steps 130/138: batch_recall = 132.83, batch_ndcg = 69.07 
2025-04-08 16:07:33.977800: Steps 131/138: batch_recall = 149.59, batch_ndcg = 85.01 
2025-04-08 16:07:35.248644: Steps 132/138: batch_recall = 151.25, batch_ndcg = 86.17 
2025-04-08 16:07:36.525271: Steps 133/138: batch_recall = 147.85, batch_ndcg = 85.09 
2025-04-08 16:07:37.803658: Steps 134/138: batch_recall = 139.13, batch_ndcg = 80.29 
2025-04-08 16:07:39.068127: Steps 135/138: batch_recall = 164.53, batch_ndcg = 93.63 
2025-04-08 16:07:40.342264: Steps 136/138: batch_recall = 151.57, batch_ndcg = 78.31 
2025-04-08 16:07:41.618289: Steps 137/138: batch_recall = 139.27, batch_ndcg = 86.16 
2025-04-08 16:07:41.618871: Epoch 16/1000, Test: Recall = 0.1762, NDCG = 0.1000  

2025-04-08 16:07:43.382272: Training Step 0/354: batchLoss = 0.5474, diffLoss = 2.6831, kgLoss = 0.0135
2025-04-08 16:07:45.004389: Training Step 1/354: batchLoss = 0.5369, diffLoss = 2.6309, kgLoss = 0.0134
2025-04-08 16:07:46.618464: Training Step 2/354: batchLoss = 0.5858, diffLoss = 2.8760, kgLoss = 0.0133
2025-04-08 16:07:48.227089: Training Step 3/354: batchLoss = 0.6713, diffLoss = 3.2926, kgLoss = 0.0160
2025-04-08 16:07:49.849104: Training Step 4/354: batchLoss = 0.6434, diffLoss = 3.1567, kgLoss = 0.0151
2025-04-08 16:07:51.466805: Training Step 5/354: batchLoss = 0.5667, diffLoss = 2.7574, kgLoss = 0.0190
2025-04-08 16:07:53.084644: Training Step 6/354: batchLoss = 0.5521, diffLoss = 2.7092, kgLoss = 0.0129
2025-04-08 16:07:54.707762: Training Step 7/354: batchLoss = 0.4364, diffLoss = 2.1380, kgLoss = 0.0110
2025-04-08 16:07:56.331545: Training Step 8/354: batchLoss = 0.5327, diffLoss = 2.6079, kgLoss = 0.0139
2025-04-08 16:07:57.948285: Training Step 9/354: batchLoss = 0.5094, diffLoss = 2.4825, kgLoss = 0.0161
2025-04-08 16:07:59.565923: Training Step 10/354: batchLoss = 0.5507, diffLoss = 2.7001, kgLoss = 0.0133
2025-04-08 16:08:01.187622: Training Step 11/354: batchLoss = 0.6521, diffLoss = 3.1888, kgLoss = 0.0179
2025-04-08 16:08:02.810046: Training Step 12/354: batchLoss = 0.6199, diffLoss = 3.0337, kgLoss = 0.0164
2025-04-08 16:08:04.429927: Training Step 13/354: batchLoss = 0.6304, diffLoss = 3.0865, kgLoss = 0.0163
2025-04-08 16:08:06.064144: Training Step 14/354: batchLoss = 0.5472, diffLoss = 2.6868, kgLoss = 0.0123
2025-04-08 16:08:07.687556: Training Step 15/354: batchLoss = 0.7213, diffLoss = 3.5376, kgLoss = 0.0172
2025-04-08 16:08:09.305544: Training Step 16/354: batchLoss = 0.5624, diffLoss = 2.7586, kgLoss = 0.0134
2025-04-08 16:08:10.931234: Training Step 17/354: batchLoss = 0.6519, diffLoss = 3.1920, kgLoss = 0.0169
2025-04-08 16:08:12.555047: Training Step 18/354: batchLoss = 0.6572, diffLoss = 3.2239, kgLoss = 0.0155
2025-04-08 16:08:14.169871: Training Step 19/354: batchLoss = 0.5811, diffLoss = 2.8528, kgLoss = 0.0132
2025-04-08 16:08:15.786704: Training Step 20/354: batchLoss = 0.4502, diffLoss = 2.2020, kgLoss = 0.0122
2025-04-08 16:08:17.407467: Training Step 21/354: batchLoss = 0.5276, diffLoss = 2.5772, kgLoss = 0.0152
2025-04-08 16:08:19.022715: Training Step 22/354: batchLoss = 0.5404, diffLoss = 2.6481, kgLoss = 0.0134
2025-04-08 16:08:20.644240: Training Step 23/354: batchLoss = 0.6175, diffLoss = 3.0191, kgLoss = 0.0171
2025-04-08 16:08:22.264725: Training Step 24/354: batchLoss = 0.6318, diffLoss = 3.0969, kgLoss = 0.0156
2025-04-08 16:08:23.886534: Training Step 25/354: batchLoss = 0.7658, diffLoss = 3.7612, kgLoss = 0.0169
2025-04-08 16:08:25.509608: Training Step 26/354: batchLoss = 0.6487, diffLoss = 3.1796, kgLoss = 0.0160
2025-04-08 16:08:27.124680: Training Step 27/354: batchLoss = 0.5446, diffLoss = 2.6698, kgLoss = 0.0133
2025-04-08 16:08:28.742159: Training Step 28/354: batchLoss = 0.4934, diffLoss = 2.4219, kgLoss = 0.0112
2025-04-08 16:08:30.360371: Training Step 29/354: batchLoss = 0.6128, diffLoss = 3.0033, kgLoss = 0.0152
2025-04-08 16:08:31.975807: Training Step 30/354: batchLoss = 0.5066, diffLoss = 2.4764, kgLoss = 0.0141
2025-04-08 16:08:33.593730: Training Step 31/354: batchLoss = 0.5074, diffLoss = 2.4822, kgLoss = 0.0137
2025-04-08 16:08:35.214763: Training Step 32/354: batchLoss = 0.6082, diffLoss = 2.9828, kgLoss = 0.0146
2025-04-08 16:08:36.828749: Training Step 33/354: batchLoss = 0.5975, diffLoss = 2.9335, kgLoss = 0.0135
2025-04-08 16:08:38.444551: Training Step 34/354: batchLoss = 0.5055, diffLoss = 2.4738, kgLoss = 0.0134
2025-04-08 16:08:40.062150: Training Step 35/354: batchLoss = 0.6046, diffLoss = 2.9590, kgLoss = 0.0160
2025-04-08 16:08:41.675556: Training Step 36/354: batchLoss = 0.5471, diffLoss = 2.6836, kgLoss = 0.0129
2025-04-08 16:08:43.297134: Training Step 37/354: batchLoss = 0.5438, diffLoss = 2.6657, kgLoss = 0.0134
2025-04-08 16:08:44.906516: Training Step 38/354: batchLoss = 0.6164, diffLoss = 3.0206, kgLoss = 0.0154
2025-04-08 16:08:46.515140: Training Step 39/354: batchLoss = 0.6302, diffLoss = 3.0939, kgLoss = 0.0143
2025-04-08 16:08:48.131980: Training Step 40/354: batchLoss = 0.6077, diffLoss = 2.9788, kgLoss = 0.0150
2025-04-08 16:08:49.750939: Training Step 41/354: batchLoss = 0.4728, diffLoss = 2.3128, kgLoss = 0.0127
2025-04-08 16:08:51.370125: Training Step 42/354: batchLoss = 0.5371, diffLoss = 2.6327, kgLoss = 0.0132
2025-04-08 16:08:52.985601: Training Step 43/354: batchLoss = 0.5569, diffLoss = 2.7315, kgLoss = 0.0132
2025-04-08 16:08:54.608983: Training Step 44/354: batchLoss = 0.4873, diffLoss = 2.3905, kgLoss = 0.0114
2025-04-08 16:08:56.226129: Training Step 45/354: batchLoss = 0.5469, diffLoss = 2.6803, kgLoss = 0.0136
2025-04-08 16:08:57.843212: Training Step 46/354: batchLoss = 0.5725, diffLoss = 2.7929, kgLoss = 0.0174
2025-04-08 16:08:59.455545: Training Step 47/354: batchLoss = 0.6614, diffLoss = 3.2443, kgLoss = 0.0157
2025-04-08 16:09:01.073432: Training Step 48/354: batchLoss = 0.5703, diffLoss = 2.7882, kgLoss = 0.0159
2025-04-08 16:09:02.690861: Training Step 49/354: batchLoss = 0.5978, diffLoss = 2.9260, kgLoss = 0.0157
2025-04-08 16:09:04.308996: Training Step 50/354: batchLoss = 0.6037, diffLoss = 2.9562, kgLoss = 0.0155
2025-04-08 16:09:05.929027: Training Step 51/354: batchLoss = 0.5578, diffLoss = 2.7291, kgLoss = 0.0150
2025-04-08 16:09:07.563504: Training Step 52/354: batchLoss = 0.5953, diffLoss = 2.9091, kgLoss = 0.0169
2025-04-08 16:09:09.192694: Training Step 53/354: batchLoss = 0.4964, diffLoss = 2.4256, kgLoss = 0.0141
2025-04-08 16:09:10.814325: Training Step 54/354: batchLoss = 0.5173, diffLoss = 2.5421, kgLoss = 0.0111
2025-04-08 16:09:12.433297: Training Step 55/354: batchLoss = 0.6029, diffLoss = 2.9520, kgLoss = 0.0156
2025-04-08 16:09:14.065816: Training Step 56/354: batchLoss = 0.5508, diffLoss = 2.6951, kgLoss = 0.0148
2025-04-08 16:09:15.690317: Training Step 57/354: batchLoss = 0.6276, diffLoss = 3.0753, kgLoss = 0.0157
2025-04-08 16:09:17.305271: Training Step 58/354: batchLoss = 0.5249, diffLoss = 2.5721, kgLoss = 0.0131
2025-04-08 16:09:18.918104: Training Step 59/354: batchLoss = 0.4821, diffLoss = 2.3570, kgLoss = 0.0134
2025-04-08 16:09:20.534778: Training Step 60/354: batchLoss = 0.5719, diffLoss = 2.8004, kgLoss = 0.0148
2025-04-08 16:09:22.166608: Training Step 61/354: batchLoss = 0.5766, diffLoss = 2.8292, kgLoss = 0.0134
2025-04-08 16:09:23.785902: Training Step 62/354: batchLoss = 0.6613, diffLoss = 3.2392, kgLoss = 0.0168
2025-04-08 16:09:25.402027: Training Step 63/354: batchLoss = 0.4988, diffLoss = 2.4430, kgLoss = 0.0127
2025-04-08 16:09:27.021457: Training Step 64/354: batchLoss = 0.5312, diffLoss = 2.6089, kgLoss = 0.0118
2025-04-08 16:09:28.634431: Training Step 65/354: batchLoss = 0.4285, diffLoss = 2.0882, kgLoss = 0.0136
2025-04-08 16:09:30.246448: Training Step 66/354: batchLoss = 0.4757, diffLoss = 2.3225, kgLoss = 0.0140
2025-04-08 16:09:31.859294: Training Step 67/354: batchLoss = 0.4788, diffLoss = 2.3455, kgLoss = 0.0121
2025-04-08 16:09:33.471354: Training Step 68/354: batchLoss = 0.5007, diffLoss = 2.4492, kgLoss = 0.0136
2025-04-08 16:09:35.087107: Training Step 69/354: batchLoss = 0.6093, diffLoss = 2.9828, kgLoss = 0.0159
2025-04-08 16:09:36.701602: Training Step 70/354: batchLoss = 0.6443, diffLoss = 3.1611, kgLoss = 0.0151
2025-04-08 16:09:38.323740: Training Step 71/354: batchLoss = 0.6205, diffLoss = 3.0451, kgLoss = 0.0143
2025-04-08 16:09:39.944796: Training Step 72/354: batchLoss = 0.4709, diffLoss = 2.3080, kgLoss = 0.0116
2025-04-08 16:09:41.559828: Training Step 73/354: batchLoss = 0.6858, diffLoss = 3.3583, kgLoss = 0.0177
2025-04-08 16:09:43.175162: Training Step 74/354: batchLoss = 0.4740, diffLoss = 2.3183, kgLoss = 0.0129
2025-04-08 16:09:44.790132: Training Step 75/354: batchLoss = 0.6466, diffLoss = 3.1704, kgLoss = 0.0157
2025-04-08 16:09:46.403383: Training Step 76/354: batchLoss = 0.4828, diffLoss = 2.3606, kgLoss = 0.0133
2025-04-08 16:09:48.020938: Training Step 77/354: batchLoss = 0.6760, diffLoss = 3.3130, kgLoss = 0.0168
2025-04-08 16:09:49.637571: Training Step 78/354: batchLoss = 0.5167, diffLoss = 2.5298, kgLoss = 0.0134
2025-04-08 16:09:51.251058: Training Step 79/354: batchLoss = 0.6062, diffLoss = 2.9714, kgLoss = 0.0150
2025-04-08 16:09:52.870480: Training Step 80/354: batchLoss = 0.4619, diffLoss = 2.2580, kgLoss = 0.0128
2025-04-08 16:09:54.488667: Training Step 81/354: batchLoss = 0.5259, diffLoss = 2.5790, kgLoss = 0.0126
2025-04-08 16:09:56.106749: Training Step 82/354: batchLoss = 0.7351, diffLoss = 3.6019, kgLoss = 0.0183
2025-04-08 16:09:57.729304: Training Step 83/354: batchLoss = 0.5722, diffLoss = 2.8025, kgLoss = 0.0147
2025-04-08 16:09:59.339488: Training Step 84/354: batchLoss = 0.5072, diffLoss = 2.4855, kgLoss = 0.0127
2025-04-08 16:10:00.959110: Training Step 85/354: batchLoss = 0.5211, diffLoss = 2.5475, kgLoss = 0.0144
2025-04-08 16:10:02.576763: Training Step 86/354: batchLoss = 0.5127, diffLoss = 2.5138, kgLoss = 0.0125
2025-04-08 16:10:04.194798: Training Step 87/354: batchLoss = 0.6703, diffLoss = 3.2863, kgLoss = 0.0164
2025-04-08 16:10:05.806118: Training Step 88/354: batchLoss = 0.5633, diffLoss = 2.7604, kgLoss = 0.0140
2025-04-08 16:10:07.425992: Training Step 89/354: batchLoss = 0.5900, diffLoss = 2.8904, kgLoss = 0.0149
2025-04-08 16:10:09.044451: Training Step 90/354: batchLoss = 0.4549, diffLoss = 2.2251, kgLoss = 0.0123
2025-04-08 16:10:10.665621: Training Step 91/354: batchLoss = 0.5598, diffLoss = 2.7438, kgLoss = 0.0138
2025-04-08 16:10:12.286149: Training Step 92/354: batchLoss = 0.5603, diffLoss = 2.7402, kgLoss = 0.0153
2025-04-08 16:10:13.899052: Training Step 93/354: batchLoss = 0.5732, diffLoss = 2.8066, kgLoss = 0.0149
2025-04-08 16:10:15.511004: Training Step 94/354: batchLoss = 0.5621, diffLoss = 2.7557, kgLoss = 0.0137
2025-04-08 16:10:17.129630: Training Step 95/354: batchLoss = 0.6209, diffLoss = 3.0468, kgLoss = 0.0144
2025-04-08 16:10:18.747670: Training Step 96/354: batchLoss = 0.7060, diffLoss = 3.4646, kgLoss = 0.0163
2025-04-08 16:10:20.370064: Training Step 97/354: batchLoss = 0.5084, diffLoss = 2.4814, kgLoss = 0.0152
2025-04-08 16:10:21.983799: Training Step 98/354: batchLoss = 0.5388, diffLoss = 2.6401, kgLoss = 0.0135
2025-04-08 16:10:23.602451: Training Step 99/354: batchLoss = 0.6114, diffLoss = 2.9983, kgLoss = 0.0147
2025-04-08 16:10:25.222144: Training Step 100/354: batchLoss = 0.5179, diffLoss = 2.5310, kgLoss = 0.0146
2025-04-08 16:10:26.838593: Training Step 101/354: batchLoss = 0.7401, diffLoss = 3.6245, kgLoss = 0.0190
2025-04-08 16:10:28.454895: Training Step 102/354: batchLoss = 0.4973, diffLoss = 2.4317, kgLoss = 0.0137
2025-04-08 16:10:30.068863: Training Step 103/354: batchLoss = 0.4656, diffLoss = 2.2749, kgLoss = 0.0133
2025-04-08 16:10:31.683899: Training Step 104/354: batchLoss = 0.4856, diffLoss = 2.3702, kgLoss = 0.0145
2025-04-08 16:10:33.303600: Training Step 105/354: batchLoss = 0.5688, diffLoss = 2.7942, kgLoss = 0.0124
2025-04-08 16:10:34.915614: Training Step 106/354: batchLoss = 0.5660, diffLoss = 2.7751, kgLoss = 0.0137
2025-04-08 16:10:36.526603: Training Step 107/354: batchLoss = 2.0158, diffLoss = 9.9166, kgLoss = 0.0406
2025-04-08 16:10:38.144388: Training Step 108/354: batchLoss = 0.5282, diffLoss = 2.5658, kgLoss = 0.0187
2025-04-08 16:10:39.764394: Training Step 109/354: batchLoss = 0.5484, diffLoss = 2.6831, kgLoss = 0.0147
2025-04-08 16:10:41.379187: Training Step 110/354: batchLoss = 0.6119, diffLoss = 2.9961, kgLoss = 0.0158
2025-04-08 16:10:42.987297: Training Step 111/354: batchLoss = 0.5327, diffLoss = 2.6138, kgLoss = 0.0125
2025-04-08 16:10:44.605402: Training Step 112/354: batchLoss = 0.5692, diffLoss = 2.7867, kgLoss = 0.0148
2025-04-08 16:10:46.219648: Training Step 113/354: batchLoss = 0.7971, diffLoss = 3.9068, kgLoss = 0.0197
2025-04-08 16:10:47.837086: Training Step 114/354: batchLoss = 0.5044, diffLoss = 2.4698, kgLoss = 0.0131
2025-04-08 16:10:49.463852: Training Step 115/354: batchLoss = 0.7839, diffLoss = 3.8501, kgLoss = 0.0174
2025-04-08 16:10:51.091099: Training Step 116/354: batchLoss = 0.6855, diffLoss = 3.3619, kgLoss = 0.0163
2025-04-08 16:10:52.723655: Training Step 117/354: batchLoss = 0.5604, diffLoss = 2.7382, kgLoss = 0.0159
2025-04-08 16:10:54.339223: Training Step 118/354: batchLoss = 0.5945, diffLoss = 2.9137, kgLoss = 0.0147
2025-04-08 16:10:55.955527: Training Step 119/354: batchLoss = 0.6207, diffLoss = 3.0451, kgLoss = 0.0146
2025-04-08 16:10:57.571297: Training Step 120/354: batchLoss = 0.6196, diffLoss = 3.0314, kgLoss = 0.0167
2025-04-08 16:10:59.192668: Training Step 121/354: batchLoss = 0.5454, diffLoss = 2.6765, kgLoss = 0.0127
2025-04-08 16:11:00.815523: Training Step 122/354: batchLoss = 0.6590, diffLoss = 3.2366, kgLoss = 0.0145
2025-04-08 16:11:02.446516: Training Step 123/354: batchLoss = 0.5913, diffLoss = 2.8968, kgLoss = 0.0149
2025-04-08 16:11:04.070590: Training Step 124/354: batchLoss = 0.4875, diffLoss = 2.3866, kgLoss = 0.0127
2025-04-08 16:11:05.696259: Training Step 125/354: batchLoss = 0.5462, diffLoss = 2.6761, kgLoss = 0.0137
2025-04-08 16:11:07.321068: Training Step 126/354: batchLoss = 0.4793, diffLoss = 2.3491, kgLoss = 0.0119
2025-04-08 16:11:08.932146: Training Step 127/354: batchLoss = 0.5371, diffLoss = 2.6355, kgLoss = 0.0125
2025-04-08 16:11:10.547993: Training Step 128/354: batchLoss = 0.5041, diffLoss = 2.4701, kgLoss = 0.0126
2025-04-08 16:11:12.164146: Training Step 129/354: batchLoss = 0.5136, diffLoss = 2.5144, kgLoss = 0.0134
2025-04-08 16:11:13.787162: Training Step 130/354: batchLoss = 0.5582, diffLoss = 2.7295, kgLoss = 0.0154
2025-04-08 16:11:15.408068: Training Step 131/354: batchLoss = 0.6990, diffLoss = 3.4191, kgLoss = 0.0190
2025-04-08 16:11:17.023398: Training Step 132/354: batchLoss = 0.6924, diffLoss = 3.3936, kgLoss = 0.0171
2025-04-08 16:11:18.652218: Training Step 133/354: batchLoss = 0.5099, diffLoss = 2.4994, kgLoss = 0.0125
2025-04-08 16:11:20.269186: Training Step 134/354: batchLoss = 0.5447, diffLoss = 2.6686, kgLoss = 0.0137
2025-04-08 16:11:21.892713: Training Step 135/354: batchLoss = 0.5325, diffLoss = 2.6004, kgLoss = 0.0155
2025-04-08 16:11:23.511588: Training Step 136/354: batchLoss = 0.6218, diffLoss = 3.0409, kgLoss = 0.0170
2025-04-08 16:11:25.127295: Training Step 137/354: batchLoss = 0.6844, diffLoss = 3.3573, kgLoss = 0.0161
2025-04-08 16:11:26.740850: Training Step 138/354: batchLoss = 0.5863, diffLoss = 2.8786, kgLoss = 0.0132
2025-04-08 16:11:28.354744: Training Step 139/354: batchLoss = 0.5303, diffLoss = 2.5957, kgLoss = 0.0140
2025-04-08 16:11:29.976461: Training Step 140/354: batchLoss = 0.5005, diffLoss = 2.4516, kgLoss = 0.0127
2025-04-08 16:11:31.588759: Training Step 141/354: batchLoss = 0.8580, diffLoss = 4.2085, kgLoss = 0.0204
2025-04-08 16:11:33.211467: Training Step 142/354: batchLoss = 0.5304, diffLoss = 2.6016, kgLoss = 0.0126
2025-04-08 16:11:34.821555: Training Step 143/354: batchLoss = 0.5265, diffLoss = 2.5768, kgLoss = 0.0140
2025-04-08 16:11:36.515891: Training Step 144/354: batchLoss = 0.4551, diffLoss = 2.2230, kgLoss = 0.0131
2025-04-08 16:11:38.447427: Training Step 145/354: batchLoss = 0.4309, diffLoss = 2.1052, kgLoss = 0.0123
2025-04-08 16:11:40.056363: Training Step 146/354: batchLoss = 0.5202, diffLoss = 2.5483, kgLoss = 0.0132
2025-04-08 16:11:41.667342: Training Step 147/354: batchLoss = 0.5818, diffLoss = 2.8527, kgLoss = 0.0140
2025-04-08 16:11:43.282298: Training Step 148/354: batchLoss = 0.5693, diffLoss = 2.7855, kgLoss = 0.0152
2025-04-08 16:11:44.904097: Training Step 149/354: batchLoss = 0.4743, diffLoss = 2.3205, kgLoss = 0.0128
2025-04-08 16:11:46.531461: Training Step 150/354: batchLoss = 0.6231, diffLoss = 3.0538, kgLoss = 0.0154
2025-04-08 16:11:48.156163: Training Step 151/354: batchLoss = 0.4821, diffLoss = 2.3619, kgLoss = 0.0122
2025-04-08 16:11:49.764639: Training Step 152/354: batchLoss = 0.5123, diffLoss = 2.4843, kgLoss = 0.0194
2025-04-08 16:11:51.390587: Training Step 153/354: batchLoss = 0.5246, diffLoss = 2.5732, kgLoss = 0.0125
2025-04-08 16:11:53.006328: Training Step 154/354: batchLoss = 0.5491, diffLoss = 2.6827, kgLoss = 0.0157
2025-04-08 16:11:54.626181: Training Step 155/354: batchLoss = 0.7588, diffLoss = 3.7243, kgLoss = 0.0175
2025-04-08 16:11:56.264864: Training Step 156/354: batchLoss = 0.5497, diffLoss = 2.6931, kgLoss = 0.0138
2025-04-08 16:11:57.896805: Training Step 157/354: batchLoss = 0.6802, diffLoss = 3.3400, kgLoss = 0.0152
2025-04-08 16:11:59.529742: Training Step 158/354: batchLoss = 0.5805, diffLoss = 2.8487, kgLoss = 0.0134
2025-04-08 16:12:01.156138: Training Step 159/354: batchLoss = 0.6685, diffLoss = 3.2809, kgLoss = 0.0154
2025-04-08 16:12:02.791880: Training Step 160/354: batchLoss = 0.6366, diffLoss = 3.1165, kgLoss = 0.0166
2025-04-08 16:12:04.417276: Training Step 161/354: batchLoss = 0.4906, diffLoss = 2.3967, kgLoss = 0.0141
2025-04-08 16:12:06.039342: Training Step 162/354: batchLoss = 0.5432, diffLoss = 2.6613, kgLoss = 0.0137
2025-04-08 16:12:07.666584: Training Step 163/354: batchLoss = 0.4939, diffLoss = 2.4107, kgLoss = 0.0147
2025-04-08 16:12:09.287817: Training Step 164/354: batchLoss = 0.5701, diffLoss = 2.7946, kgLoss = 0.0140
2025-04-08 16:12:10.913412: Training Step 165/354: batchLoss = 0.5794, diffLoss = 2.8438, kgLoss = 0.0133
2025-04-08 16:12:12.544697: Training Step 166/354: batchLoss = 0.5178, diffLoss = 2.5241, kgLoss = 0.0163
2025-04-08 16:12:14.165744: Training Step 167/354: batchLoss = 0.4746, diffLoss = 2.3261, kgLoss = 0.0117
2025-04-08 16:12:15.798691: Training Step 168/354: batchLoss = 0.6850, diffLoss = 3.3520, kgLoss = 0.0182
2025-04-08 16:12:17.428011: Training Step 169/354: batchLoss = 0.6257, diffLoss = 3.0637, kgLoss = 0.0162
2025-04-08 16:12:19.061439: Training Step 170/354: batchLoss = 0.6305, diffLoss = 3.0882, kgLoss = 0.0161
2025-04-08 16:12:20.686067: Training Step 171/354: batchLoss = 0.5706, diffLoss = 2.7975, kgLoss = 0.0139
2025-04-08 16:12:22.303198: Training Step 172/354: batchLoss = 0.5252, diffLoss = 2.5782, kgLoss = 0.0120
2025-04-08 16:12:23.921642: Training Step 173/354: batchLoss = 0.5909, diffLoss = 2.9001, kgLoss = 0.0136
2025-04-08 16:12:25.541014: Training Step 174/354: batchLoss = 0.5931, diffLoss = 2.9070, kgLoss = 0.0146
2025-04-08 16:12:27.160948: Training Step 175/354: batchLoss = 0.6506, diffLoss = 3.1896, kgLoss = 0.0159
2025-04-08 16:12:28.792078: Training Step 176/354: batchLoss = 0.5522, diffLoss = 2.7073, kgLoss = 0.0134
2025-04-08 16:12:30.421153: Training Step 177/354: batchLoss = 0.4639, diffLoss = 2.2647, kgLoss = 0.0136
2025-04-08 16:12:32.050434: Training Step 178/354: batchLoss = 0.6277, diffLoss = 3.0721, kgLoss = 0.0166
2025-04-08 16:12:33.693831: Training Step 179/354: batchLoss = 0.5556, diffLoss = 2.7212, kgLoss = 0.0142
2025-04-08 16:12:35.326437: Training Step 180/354: batchLoss = 0.5286, diffLoss = 2.5861, kgLoss = 0.0142
2025-04-08 16:12:36.952410: Training Step 181/354: batchLoss = 0.5788, diffLoss = 2.8413, kgLoss = 0.0132
2025-04-08 16:12:38.574697: Training Step 182/354: batchLoss = 0.6345, diffLoss = 3.1022, kgLoss = 0.0176
2025-04-08 16:12:40.199472: Training Step 183/354: batchLoss = 0.5598, diffLoss = 2.7408, kgLoss = 0.0145
2025-04-08 16:12:41.839377: Training Step 184/354: batchLoss = 0.6004, diffLoss = 2.9463, kgLoss = 0.0139
2025-04-08 16:12:43.462194: Training Step 185/354: batchLoss = 0.5434, diffLoss = 2.6683, kgLoss = 0.0122
2025-04-08 16:12:45.090636: Training Step 186/354: batchLoss = 0.5770, diffLoss = 2.8244, kgLoss = 0.0151
2025-04-08 16:12:46.718517: Training Step 187/354: batchLoss = 0.5026, diffLoss = 2.4583, kgLoss = 0.0137
2025-04-08 16:12:48.355669: Training Step 188/354: batchLoss = 0.6039, diffLoss = 2.9629, kgLoss = 0.0141
2025-04-08 16:12:50.007460: Training Step 189/354: batchLoss = 0.5468, diffLoss = 2.6825, kgLoss = 0.0129
2025-04-08 16:12:51.633589: Training Step 190/354: batchLoss = 0.6884, diffLoss = 3.3698, kgLoss = 0.0180
2025-04-08 16:12:53.261762: Training Step 191/354: batchLoss = 0.6135, diffLoss = 3.0064, kgLoss = 0.0152
2025-04-08 16:12:54.884477: Training Step 192/354: batchLoss = 0.4814, diffLoss = 2.3553, kgLoss = 0.0129
2025-04-08 16:12:56.495987: Training Step 193/354: batchLoss = 0.6193, diffLoss = 3.0279, kgLoss = 0.0171
2025-04-08 16:12:58.123225: Training Step 194/354: batchLoss = 0.4327, diffLoss = 2.1161, kgLoss = 0.0118
2025-04-08 16:12:59.746206: Training Step 195/354: batchLoss = 0.6758, diffLoss = 3.3133, kgLoss = 0.0164
2025-04-08 16:13:01.370116: Training Step 196/354: batchLoss = 0.5695, diffLoss = 2.7869, kgLoss = 0.0151
2025-04-08 16:13:02.998827: Training Step 197/354: batchLoss = 0.4438, diffLoss = 2.1695, kgLoss = 0.0124
2025-04-08 16:13:04.629267: Training Step 198/354: batchLoss = 0.5644, diffLoss = 2.7668, kgLoss = 0.0138
2025-04-08 16:13:06.254693: Training Step 199/354: batchLoss = 0.4987, diffLoss = 2.4448, kgLoss = 0.0121
2025-04-08 16:13:07.884832: Training Step 200/354: batchLoss = 0.5313, diffLoss = 2.5949, kgLoss = 0.0155
2025-04-08 16:13:09.513483: Training Step 201/354: batchLoss = 0.5478, diffLoss = 2.6844, kgLoss = 0.0137
2025-04-08 16:13:11.133514: Training Step 202/354: batchLoss = 0.4599, diffLoss = 2.2530, kgLoss = 0.0117
2025-04-08 16:13:12.755109: Training Step 203/354: batchLoss = 0.6556, diffLoss = 3.1974, kgLoss = 0.0202
2025-04-08 16:13:14.380616: Training Step 204/354: batchLoss = 0.5008, diffLoss = 2.4459, kgLoss = 0.0146
2025-04-08 16:13:16.012465: Training Step 205/354: batchLoss = 0.5907, diffLoss = 2.8963, kgLoss = 0.0143
2025-04-08 16:13:17.647317: Training Step 206/354: batchLoss = 0.5505, diffLoss = 2.7001, kgLoss = 0.0131
2025-04-08 16:13:19.275686: Training Step 207/354: batchLoss = 0.4787, diffLoss = 2.3465, kgLoss = 0.0118
2025-04-08 16:13:20.904872: Training Step 208/354: batchLoss = 0.5351, diffLoss = 2.6210, kgLoss = 0.0136
2025-04-08 16:13:22.536562: Training Step 209/354: batchLoss = 0.5272, diffLoss = 2.5780, kgLoss = 0.0145
2025-04-08 16:13:24.174235: Training Step 210/354: batchLoss = 0.4861, diffLoss = 2.3871, kgLoss = 0.0108
2025-04-08 16:13:25.798687: Training Step 211/354: batchLoss = 0.5428, diffLoss = 2.6628, kgLoss = 0.0128
2025-04-08 16:13:27.410499: Training Step 212/354: batchLoss = 0.6243, diffLoss = 3.0599, kgLoss = 0.0154
2025-04-08 16:13:29.032085: Training Step 213/354: batchLoss = 0.6428, diffLoss = 3.1502, kgLoss = 0.0159
2025-04-08 16:13:30.656462: Training Step 214/354: batchLoss = 0.6283, diffLoss = 3.0792, kgLoss = 0.0156
2025-04-08 16:13:32.278467: Training Step 215/354: batchLoss = 0.6409, diffLoss = 3.1433, kgLoss = 0.0153
2025-04-08 16:13:33.911176: Training Step 216/354: batchLoss = 0.5651, diffLoss = 2.7653, kgLoss = 0.0151
2025-04-08 16:13:35.536834: Training Step 217/354: batchLoss = 0.5885, diffLoss = 2.8788, kgLoss = 0.0159
2025-04-08 16:13:37.158454: Training Step 218/354: batchLoss = 0.6485, diffLoss = 3.1776, kgLoss = 0.0162
2025-04-08 16:13:38.790180: Training Step 219/354: batchLoss = 0.6850, diffLoss = 3.3575, kgLoss = 0.0169
2025-04-08 16:13:40.419847: Training Step 220/354: batchLoss = 0.4723, diffLoss = 2.3046, kgLoss = 0.0142
2025-04-08 16:13:42.033148: Training Step 221/354: batchLoss = 0.6374, diffLoss = 3.1240, kgLoss = 0.0158
2025-04-08 16:13:43.659417: Training Step 222/354: batchLoss = 0.5958, diffLoss = 2.9183, kgLoss = 0.0152
2025-04-08 16:13:45.281678: Training Step 223/354: batchLoss = 0.7037, diffLoss = 3.4496, kgLoss = 0.0172
2025-04-08 16:13:46.912007: Training Step 224/354: batchLoss = 0.5283, diffLoss = 2.5840, kgLoss = 0.0143
2025-04-08 16:13:48.544147: Training Step 225/354: batchLoss = 0.5651, diffLoss = 2.7673, kgLoss = 0.0146
2025-04-08 16:13:50.166045: Training Step 226/354: batchLoss = 0.6585, diffLoss = 3.2281, kgLoss = 0.0162
2025-04-08 16:13:51.804843: Training Step 227/354: batchLoss = 0.6638, diffLoss = 3.2549, kgLoss = 0.0160
2025-04-08 16:13:53.439405: Training Step 228/354: batchLoss = 0.4693, diffLoss = 2.2946, kgLoss = 0.0130
2025-04-08 16:13:55.071273: Training Step 229/354: batchLoss = 0.5014, diffLoss = 2.4594, kgLoss = 0.0119
2025-04-08 16:13:56.704384: Training Step 230/354: batchLoss = 0.5001, diffLoss = 2.4465, kgLoss = 0.0135
2025-04-08 16:13:58.315760: Training Step 231/354: batchLoss = 0.5777, diffLoss = 2.8198, kgLoss = 0.0172
2025-04-08 16:13:59.937720: Training Step 232/354: batchLoss = 0.5332, diffLoss = 2.6189, kgLoss = 0.0118
2025-04-08 16:14:01.556192: Training Step 233/354: batchLoss = 0.6207, diffLoss = 3.0412, kgLoss = 0.0155
2025-04-08 16:14:03.174348: Training Step 234/354: batchLoss = 0.6326, diffLoss = 3.1044, kgLoss = 0.0147
2025-04-08 16:14:04.798979: Training Step 235/354: batchLoss = 0.4751, diffLoss = 2.3244, kgLoss = 0.0128
2025-04-08 16:14:06.422728: Training Step 236/354: batchLoss = 0.5840, diffLoss = 2.8547, kgLoss = 0.0164
2025-04-08 16:14:08.050715: Training Step 237/354: batchLoss = 0.5486, diffLoss = 2.6893, kgLoss = 0.0134
2025-04-08 16:14:09.681012: Training Step 238/354: batchLoss = 0.6094, diffLoss = 2.9935, kgLoss = 0.0134
2025-04-08 16:14:11.305246: Training Step 239/354: batchLoss = 0.4590, diffLoss = 2.2439, kgLoss = 0.0128
2025-04-08 16:14:12.927270: Training Step 240/354: batchLoss = 0.5809, diffLoss = 2.8440, kgLoss = 0.0151
2025-04-08 16:14:14.540960: Training Step 241/354: batchLoss = 0.4534, diffLoss = 2.2228, kgLoss = 0.0110
2025-04-08 16:14:16.166234: Training Step 242/354: batchLoss = 0.5330, diffLoss = 2.6133, kgLoss = 0.0130
2025-04-08 16:14:17.795387: Training Step 243/354: batchLoss = 0.7865, diffLoss = 3.8622, kgLoss = 0.0176
2025-04-08 16:14:19.416755: Training Step 244/354: batchLoss = 0.5293, diffLoss = 2.5964, kgLoss = 0.0125
2025-04-08 16:14:21.039284: Training Step 245/354: batchLoss = 0.6009, diffLoss = 2.9437, kgLoss = 0.0152
2025-04-08 16:14:22.658501: Training Step 246/354: batchLoss = 0.4669, diffLoss = 2.2837, kgLoss = 0.0127
2025-04-08 16:14:24.286791: Training Step 247/354: batchLoss = 0.5599, diffLoss = 2.7484, kgLoss = 0.0128
2025-04-08 16:14:25.920024: Training Step 248/354: batchLoss = 0.5180, diffLoss = 2.5374, kgLoss = 0.0132
2025-04-08 16:14:27.547495: Training Step 249/354: batchLoss = 0.5640, diffLoss = 2.7643, kgLoss = 0.0140
2025-04-08 16:14:29.175664: Training Step 250/354: batchLoss = 0.5189, diffLoss = 2.5440, kgLoss = 0.0127
2025-04-08 16:14:30.791750: Training Step 251/354: batchLoss = 0.4342, diffLoss = 2.1269, kgLoss = 0.0110
2025-04-08 16:14:32.411895: Training Step 252/354: batchLoss = 0.5239, diffLoss = 2.5644, kgLoss = 0.0138
2025-04-08 16:14:34.021451: Training Step 253/354: batchLoss = 0.4985, diffLoss = 2.4377, kgLoss = 0.0137
2025-04-08 16:14:35.648045: Training Step 254/354: batchLoss = 0.5396, diffLoss = 2.6352, kgLoss = 0.0157
2025-04-08 16:14:37.274854: Training Step 255/354: batchLoss = 1.1091, diffLoss = 5.4302, kgLoss = 0.0288
2025-04-08 16:14:38.910962: Training Step 256/354: batchLoss = 0.5348, diffLoss = 2.6196, kgLoss = 0.0137
2025-04-08 16:14:40.539306: Training Step 257/354: batchLoss = 0.5560, diffLoss = 2.7192, kgLoss = 0.0152
2025-04-08 16:14:42.175114: Training Step 258/354: batchLoss = 0.6269, diffLoss = 3.0785, kgLoss = 0.0140
2025-04-08 16:14:43.802557: Training Step 259/354: batchLoss = 0.6866, diffLoss = 3.3650, kgLoss = 0.0169
2025-04-08 16:14:45.435616: Training Step 260/354: batchLoss = 0.6402, diffLoss = 3.1327, kgLoss = 0.0171
2025-04-08 16:14:47.049743: Training Step 261/354: batchLoss = 0.4790, diffLoss = 2.3438, kgLoss = 0.0128
2025-04-08 16:14:48.673700: Training Step 262/354: batchLoss = 0.5690, diffLoss = 2.7865, kgLoss = 0.0146
2025-04-08 16:14:50.303722: Training Step 263/354: batchLoss = 0.5962, diffLoss = 2.9185, kgLoss = 0.0156
2025-04-08 16:14:51.934295: Training Step 264/354: batchLoss = 0.6269, diffLoss = 3.0772, kgLoss = 0.0143
2025-04-08 16:14:53.555525: Training Step 265/354: batchLoss = 0.5073, diffLoss = 2.4866, kgLoss = 0.0124
2025-04-08 16:14:55.187271: Training Step 266/354: batchLoss = 0.6742, diffLoss = 3.3035, kgLoss = 0.0169
2025-04-08 16:14:56.821243: Training Step 267/354: batchLoss = 0.6006, diffLoss = 2.9354, kgLoss = 0.0169
2025-04-08 16:14:58.452611: Training Step 268/354: batchLoss = 0.5362, diffLoss = 2.6310, kgLoss = 0.0125
2025-04-08 16:15:00.087311: Training Step 269/354: batchLoss = 0.5182, diffLoss = 2.5356, kgLoss = 0.0139
2025-04-08 16:15:01.716559: Training Step 270/354: batchLoss = 0.6316, diffLoss = 3.0854, kgLoss = 0.0182
2025-04-08 16:15:03.342129: Training Step 271/354: batchLoss = 0.6205, diffLoss = 3.0398, kgLoss = 0.0157
2025-04-08 16:15:04.972289: Training Step 272/354: batchLoss = 0.6010, diffLoss = 2.9420, kgLoss = 0.0157
2025-04-08 16:15:06.591718: Training Step 273/354: batchLoss = 0.5410, diffLoss = 2.6455, kgLoss = 0.0148
2025-04-08 16:15:08.222584: Training Step 274/354: batchLoss = 0.5024, diffLoss = 2.4654, kgLoss = 0.0117
2025-04-08 16:15:09.858804: Training Step 275/354: batchLoss = 0.5141, diffLoss = 2.5184, kgLoss = 0.0130
2025-04-08 16:15:11.493120: Training Step 276/354: batchLoss = 0.5302, diffLoss = 2.5946, kgLoss = 0.0141
2025-04-08 16:15:13.121316: Training Step 277/354: batchLoss = 0.6648, diffLoss = 3.2599, kgLoss = 0.0160
2025-04-08 16:15:14.751839: Training Step 278/354: batchLoss = 0.5710, diffLoss = 2.7964, kgLoss = 0.0147
2025-04-08 16:15:16.383432: Training Step 279/354: batchLoss = 0.5803, diffLoss = 2.8422, kgLoss = 0.0148
2025-04-08 16:15:18.002831: Training Step 280/354: batchLoss = 0.5967, diffLoss = 2.9212, kgLoss = 0.0156
2025-04-08 16:15:19.612879: Training Step 281/354: batchLoss = 0.5551, diffLoss = 2.7047, kgLoss = 0.0177
2025-04-08 16:15:21.222689: Training Step 282/354: batchLoss = 0.4720, diffLoss = 2.3118, kgLoss = 0.0120
2025-04-08 16:15:22.831324: Training Step 283/354: batchLoss = 0.4831, diffLoss = 2.3716, kgLoss = 0.0110
2025-04-08 16:15:24.446463: Training Step 284/354: batchLoss = 0.5645, diffLoss = 2.7646, kgLoss = 0.0145
2025-04-08 16:15:26.074826: Training Step 285/354: batchLoss = 0.5708, diffLoss = 2.7975, kgLoss = 0.0141
2025-04-08 16:15:27.704096: Training Step 286/354: batchLoss = 0.5316, diffLoss = 2.5983, kgLoss = 0.0149
2025-04-08 16:15:29.336036: Training Step 287/354: batchLoss = 0.5059, diffLoss = 2.4816, kgLoss = 0.0120
2025-04-08 16:15:30.976618: Training Step 288/354: batchLoss = 0.5366, diffLoss = 2.6265, kgLoss = 0.0142
2025-04-08 16:15:32.613549: Training Step 289/354: batchLoss = 0.6493, diffLoss = 3.1779, kgLoss = 0.0172
2025-04-08 16:15:34.232907: Training Step 290/354: batchLoss = 0.8287, diffLoss = 4.0622, kgLoss = 0.0203
2025-04-08 16:15:35.851860: Training Step 291/354: batchLoss = 0.6159, diffLoss = 3.0168, kgLoss = 0.0157
2025-04-08 16:15:37.469734: Training Step 292/354: batchLoss = 0.8678, diffLoss = 4.2650, kgLoss = 0.0185
2025-04-08 16:15:39.085721: Training Step 293/354: batchLoss = 0.5607, diffLoss = 2.7416, kgLoss = 0.0155
2025-04-08 16:15:40.706859: Training Step 294/354: batchLoss = 0.4529, diffLoss = 2.2103, kgLoss = 0.0135
2025-04-08 16:15:42.330323: Training Step 295/354: batchLoss = 0.5371, diffLoss = 2.6242, kgLoss = 0.0154
2025-04-08 16:15:43.952183: Training Step 296/354: batchLoss = 0.4979, diffLoss = 2.4272, kgLoss = 0.0156
2025-04-08 16:15:45.580063: Training Step 297/354: batchLoss = 0.5749, diffLoss = 2.8105, kgLoss = 0.0160
2025-04-08 16:15:47.204465: Training Step 298/354: batchLoss = 0.6206, diffLoss = 3.0438, kgLoss = 0.0148
2025-04-08 16:15:48.837937: Training Step 299/354: batchLoss = 0.5106, diffLoss = 2.5001, kgLoss = 0.0132
2025-04-08 16:15:50.458337: Training Step 300/354: batchLoss = 0.5760, diffLoss = 2.8215, kgLoss = 0.0147
2025-04-08 16:15:52.072080: Training Step 301/354: batchLoss = 0.6092, diffLoss = 2.9809, kgLoss = 0.0163
2025-04-08 16:15:53.692117: Training Step 302/354: batchLoss = 0.4431, diffLoss = 2.1714, kgLoss = 0.0110
2025-04-08 16:15:55.317311: Training Step 303/354: batchLoss = 0.6984, diffLoss = 3.4269, kgLoss = 0.0163
2025-04-08 16:15:56.949188: Training Step 304/354: batchLoss = 0.5923, diffLoss = 2.9021, kgLoss = 0.0148
2025-04-08 16:15:58.581061: Training Step 305/354: batchLoss = 0.5014, diffLoss = 2.4529, kgLoss = 0.0135
2025-04-08 16:16:00.221022: Training Step 306/354: batchLoss = 0.5466, diffLoss = 2.6756, kgLoss = 0.0143
2025-04-08 16:16:01.857061: Training Step 307/354: batchLoss = 0.6110, diffLoss = 2.9828, kgLoss = 0.0181
2025-04-08 16:16:03.483292: Training Step 308/354: batchLoss = 0.6783, diffLoss = 3.3233, kgLoss = 0.0171
2025-04-08 16:16:05.109843: Training Step 309/354: batchLoss = 0.6500, diffLoss = 3.1867, kgLoss = 0.0158
2025-04-08 16:16:06.732845: Training Step 310/354: batchLoss = 0.6039, diffLoss = 2.9573, kgLoss = 0.0156
2025-04-08 16:16:08.357914: Training Step 311/354: batchLoss = 0.6000, diffLoss = 2.9384, kgLoss = 0.0154
2025-04-08 16:16:09.980328: Training Step 312/354: batchLoss = 0.6102, diffLoss = 2.9952, kgLoss = 0.0139
2025-04-08 16:16:11.597655: Training Step 313/354: batchLoss = 0.7625, diffLoss = 3.7382, kgLoss = 0.0185
2025-04-08 16:16:13.217941: Training Step 314/354: batchLoss = 0.5404, diffLoss = 2.6472, kgLoss = 0.0137
2025-04-08 16:16:14.853221: Training Step 315/354: batchLoss = 0.5352, diffLoss = 2.6195, kgLoss = 0.0141
2025-04-08 16:16:16.478862: Training Step 316/354: batchLoss = 0.6719, diffLoss = 3.2943, kgLoss = 0.0163
2025-04-08 16:16:18.108234: Training Step 317/354: batchLoss = 0.6387, diffLoss = 3.1320, kgLoss = 0.0154
2025-04-08 16:16:19.737108: Training Step 318/354: batchLoss = 0.6520, diffLoss = 3.1935, kgLoss = 0.0167
2025-04-08 16:16:21.367759: Training Step 319/354: batchLoss = 0.5412, diffLoss = 2.6520, kgLoss = 0.0134
2025-04-08 16:16:22.993232: Training Step 320/354: batchLoss = 0.6475, diffLoss = 3.1716, kgLoss = 0.0165
2025-04-08 16:16:24.610164: Training Step 321/354: batchLoss = 0.5336, diffLoss = 2.6073, kgLoss = 0.0152
2025-04-08 16:16:26.238717: Training Step 322/354: batchLoss = 0.6309, diffLoss = 3.0955, kgLoss = 0.0148
2025-04-08 16:16:27.867780: Training Step 323/354: batchLoss = 0.6569, diffLoss = 3.2260, kgLoss = 0.0146
2025-04-08 16:16:29.494555: Training Step 324/354: batchLoss = 0.6877, diffLoss = 3.3742, kgLoss = 0.0161
2025-04-08 16:16:31.121550: Training Step 325/354: batchLoss = 0.5196, diffLoss = 2.5395, kgLoss = 0.0146
2025-04-08 16:16:32.753446: Training Step 326/354: batchLoss = 0.5660, diffLoss = 2.7754, kgLoss = 0.0136
2025-04-08 16:16:34.387712: Training Step 327/354: batchLoss = 0.5140, diffLoss = 2.5156, kgLoss = 0.0136
2025-04-08 16:16:36.024584: Training Step 328/354: batchLoss = 0.5284, diffLoss = 2.5899, kgLoss = 0.0130
2025-04-08 16:16:37.650041: Training Step 329/354: batchLoss = 0.4522, diffLoss = 2.2075, kgLoss = 0.0133
2025-04-08 16:16:39.270091: Training Step 330/354: batchLoss = 0.5434, diffLoss = 2.6646, kgLoss = 0.0131
2025-04-08 16:16:40.895950: Training Step 331/354: batchLoss = 0.5928, diffLoss = 2.9073, kgLoss = 0.0142
2025-04-08 16:16:42.521488: Training Step 332/354: batchLoss = 0.5659, diffLoss = 2.7750, kgLoss = 0.0136
2025-04-08 16:16:44.150704: Training Step 333/354: batchLoss = 0.5394, diffLoss = 2.6403, kgLoss = 0.0142
2025-04-08 16:16:45.778952: Training Step 334/354: batchLoss = 0.5561, diffLoss = 2.7259, kgLoss = 0.0136
2025-04-08 16:16:47.403872: Training Step 335/354: batchLoss = 0.7703, diffLoss = 3.7677, kgLoss = 0.0210
2025-04-08 16:16:49.040328: Training Step 336/354: batchLoss = 0.4966, diffLoss = 2.4307, kgLoss = 0.0130
2025-04-08 16:16:50.670152: Training Step 337/354: batchLoss = 0.5545, diffLoss = 2.7204, kgLoss = 0.0130
2025-04-08 16:16:52.302661: Training Step 338/354: batchLoss = 0.7546, diffLoss = 3.7016, kgLoss = 0.0179
2025-04-08 16:16:53.920115: Training Step 339/354: batchLoss = 0.5847, diffLoss = 2.8652, kgLoss = 0.0145
2025-04-08 16:16:55.542817: Training Step 340/354: batchLoss = 0.5791, diffLoss = 2.8415, kgLoss = 0.0135
2025-04-08 16:16:57.169414: Training Step 341/354: batchLoss = 0.6212, diffLoss = 3.0427, kgLoss = 0.0158
2025-04-08 16:16:58.791066: Training Step 342/354: batchLoss = 0.4408, diffLoss = 2.1606, kgLoss = 0.0108
2025-04-08 16:17:00.423106: Training Step 343/354: batchLoss = 0.5169, diffLoss = 2.5321, kgLoss = 0.0131
2025-04-08 16:17:02.049455: Training Step 344/354: batchLoss = 0.6819, diffLoss = 3.3412, kgLoss = 0.0170
2025-04-08 16:17:03.670556: Training Step 345/354: batchLoss = 0.4885, diffLoss = 2.3899, kgLoss = 0.0131
2025-04-08 16:17:05.298985: Training Step 346/354: batchLoss = 0.5944, diffLoss = 2.9116, kgLoss = 0.0151
2025-04-08 16:17:06.924212: Training Step 347/354: batchLoss = 0.5359, diffLoss = 2.6176, kgLoss = 0.0155
2025-04-08 16:17:08.556719: Training Step 348/354: batchLoss = 0.4780, diffLoss = 2.3399, kgLoss = 0.0125
2025-04-08 16:17:10.176686: Training Step 349/354: batchLoss = 0.5516, diffLoss = 2.6983, kgLoss = 0.0149
2025-04-08 16:17:11.807981: Training Step 350/354: batchLoss = 1.0118, diffLoss = 4.9614, kgLoss = 0.0244
2025-04-08 16:17:13.425380: Training Step 351/354: batchLoss = 0.5893, diffLoss = 2.8775, kgLoss = 0.0172
2025-04-08 16:17:15.030167: Training Step 352/354: batchLoss = 0.6023, diffLoss = 2.9518, kgLoss = 0.0149
2025-04-08 16:17:16.431480: Training Step 353/354: batchLoss = 0.3579, diffLoss = 1.7432, kgLoss = 0.0115
2025-04-08 16:17:16.518697: 
2025-04-08 16:17:16.519354: Epoch 17/1000, Train: epLoss = 1.0241, epDfLoss = 5.0163, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 16:17:17.849356: Steps 0/138: batch_recall = 45.46, batch_ndcg = 25.99 
2025-04-08 16:17:19.186256: Steps 1/138: batch_recall = 47.46, batch_ndcg = 27.78 
2025-04-08 16:17:20.506412: Steps 2/138: batch_recall = 58.66, batch_ndcg = 36.45 
2025-04-08 16:17:21.843898: Steps 3/138: batch_recall = 60.26, batch_ndcg = 34.69 
2025-04-08 16:17:23.160799: Steps 4/138: batch_recall = 68.17, batch_ndcg = 40.39 
2025-04-08 16:17:24.483938: Steps 5/138: batch_recall = 60.65, batch_ndcg = 33.13 
2025-04-08 16:17:25.785078: Steps 6/138: batch_recall = 52.17, batch_ndcg = 31.34 
2025-04-08 16:17:27.091736: Steps 7/138: batch_recall = 63.69, batch_ndcg = 41.48 
2025-04-08 16:17:28.403957: Steps 8/138: batch_recall = 64.19, batch_ndcg = 40.33 
2025-04-08 16:17:29.703116: Steps 9/138: batch_recall = 57.36, batch_ndcg = 34.09 
2025-04-08 16:17:31.015716: Steps 10/138: batch_recall = 55.14, batch_ndcg = 30.52 
2025-04-08 16:17:32.319793: Steps 11/138: batch_recall = 57.92, batch_ndcg = 33.34 
2025-04-08 16:17:33.633923: Steps 12/138: batch_recall = 52.86, batch_ndcg = 29.03 
2025-04-08 16:17:34.935774: Steps 13/138: batch_recall = 51.62, batch_ndcg = 30.27 
2025-04-08 16:17:36.231560: Steps 14/138: batch_recall = 53.61, batch_ndcg = 31.06 
2025-04-08 16:17:37.535291: Steps 15/138: batch_recall = 50.74, batch_ndcg = 29.68 
2025-04-08 16:17:38.833121: Steps 16/138: batch_recall = 59.25, batch_ndcg = 33.13 
2025-04-08 16:17:40.123533: Steps 17/138: batch_recall = 57.70, batch_ndcg = 32.66 
2025-04-08 16:17:41.423642: Steps 18/138: batch_recall = 51.22, batch_ndcg = 32.29 
2025-04-08 16:17:42.735309: Steps 19/138: batch_recall = 56.81, batch_ndcg = 32.73 
2025-04-08 16:17:44.054603: Steps 20/138: batch_recall = 61.52, batch_ndcg = 36.31 
2025-04-08 16:17:45.376056: Steps 21/138: batch_recall = 67.76, batch_ndcg = 39.74 
2025-04-08 16:17:46.667690: Steps 22/138: batch_recall = 56.92, batch_ndcg = 33.32 
2025-04-08 16:17:47.975943: Steps 23/138: batch_recall = 51.17, batch_ndcg = 29.94 
2025-04-08 16:17:49.289633: Steps 24/138: batch_recall = 57.42, batch_ndcg = 31.01 
2025-04-08 16:17:50.602722: Steps 25/138: batch_recall = 59.86, batch_ndcg = 35.22 
2025-04-08 16:17:51.912111: Steps 26/138: batch_recall = 56.79, batch_ndcg = 32.87 
2025-04-08 16:17:53.212395: Steps 27/138: batch_recall = 62.97, batch_ndcg = 34.81 
2025-04-08 16:17:54.507657: Steps 28/138: batch_recall = 59.74, batch_ndcg = 33.70 
2025-04-08 16:17:55.794994: Steps 29/138: batch_recall = 61.42, batch_ndcg = 32.15 
2025-04-08 16:17:57.093134: Steps 30/138: batch_recall = 57.85, batch_ndcg = 34.35 
2025-04-08 16:17:58.378975: Steps 31/138: batch_recall = 40.88, batch_ndcg = 23.96 
2025-04-08 16:17:59.664452: Steps 32/138: batch_recall = 50.69, batch_ndcg = 30.26 
2025-04-08 16:18:00.952084: Steps 33/138: batch_recall = 58.47, batch_ndcg = 33.16 
2025-04-08 16:18:02.236753: Steps 34/138: batch_recall = 54.34, batch_ndcg = 28.26 
2025-04-08 16:18:03.535653: Steps 35/138: batch_recall = 54.68, batch_ndcg = 30.92 
2025-04-08 16:18:04.835639: Steps 36/138: batch_recall = 50.15, batch_ndcg = 28.45 
2025-04-08 16:18:06.133161: Steps 37/138: batch_recall = 59.59, batch_ndcg = 34.58 
2025-04-08 16:18:07.423052: Steps 38/138: batch_recall = 57.63, batch_ndcg = 31.81 
2025-04-08 16:18:08.705470: Steps 39/138: batch_recall = 65.47, batch_ndcg = 37.31 
2025-04-08 16:18:10.000246: Steps 40/138: batch_recall = 58.24, batch_ndcg = 30.03 
2025-04-08 16:18:11.282657: Steps 41/138: batch_recall = 64.11, batch_ndcg = 34.01 
2025-04-08 16:18:12.571677: Steps 42/138: batch_recall = 55.91, batch_ndcg = 29.93 
2025-04-08 16:18:13.869980: Steps 43/138: batch_recall = 57.37, batch_ndcg = 34.27 
2025-04-08 16:18:15.161042: Steps 44/138: batch_recall = 58.75, batch_ndcg = 30.43 
2025-04-08 16:18:16.461565: Steps 45/138: batch_recall = 62.64, batch_ndcg = 35.31 
2025-04-08 16:18:17.737342: Steps 46/138: batch_recall = 60.29, batch_ndcg = 35.05 
2025-04-08 16:18:19.008530: Steps 47/138: batch_recall = 51.77, batch_ndcg = 31.51 
2025-04-08 16:18:20.304043: Steps 48/138: batch_recall = 57.25, batch_ndcg = 33.74 
2025-04-08 16:18:21.592733: Steps 49/138: batch_recall = 64.39, batch_ndcg = 36.64 
2025-04-08 16:18:22.893279: Steps 50/138: batch_recall = 59.59, batch_ndcg = 32.12 
2025-04-08 16:18:24.194543: Steps 51/138: batch_recall = 59.12, batch_ndcg = 35.00 
2025-04-08 16:18:25.486811: Steps 52/138: batch_recall = 65.63, batch_ndcg = 41.70 
2025-04-08 16:18:26.791796: Steps 53/138: batch_recall = 68.69, batch_ndcg = 35.08 
2025-04-08 16:18:28.109880: Steps 54/138: batch_recall = 67.01, batch_ndcg = 38.33 
2025-04-08 16:18:29.412989: Steps 55/138: batch_recall = 62.32, batch_ndcg = 33.64 
2025-04-08 16:18:30.709004: Steps 56/138: batch_recall = 62.96, batch_ndcg = 34.53 
2025-04-08 16:18:31.999216: Steps 57/138: batch_recall = 56.69, batch_ndcg = 32.33 
2025-04-08 16:18:33.287352: Steps 58/138: batch_recall = 69.52, batch_ndcg = 37.18 
2025-04-08 16:18:34.574004: Steps 59/138: batch_recall = 71.23, batch_ndcg = 41.55 
2025-04-08 16:18:35.858833: Steps 60/138: batch_recall = 70.19, batch_ndcg = 37.46 
2025-04-08 16:18:37.152571: Steps 61/138: batch_recall = 64.13, batch_ndcg = 35.22 
2025-04-08 16:18:38.448854: Steps 62/138: batch_recall = 86.13, batch_ndcg = 43.83 
2025-04-08 16:18:39.748577: Steps 63/138: batch_recall = 76.03, batch_ndcg = 43.04 
2025-04-08 16:18:41.048612: Steps 64/138: batch_recall = 62.51, batch_ndcg = 33.37 
2025-04-08 16:18:42.348489: Steps 65/138: batch_recall = 84.74, batch_ndcg = 46.71 
2025-04-08 16:18:43.643701: Steps 66/138: batch_recall = 70.26, batch_ndcg = 40.58 
2025-04-08 16:18:44.944625: Steps 67/138: batch_recall = 77.35, batch_ndcg = 47.56 
2025-04-08 16:18:46.242722: Steps 68/138: batch_recall = 64.70, batch_ndcg = 34.22 
2025-04-08 16:18:47.530928: Steps 69/138: batch_recall = 88.52, batch_ndcg = 51.23 
2025-04-08 16:18:48.825794: Steps 70/138: batch_recall = 80.89, batch_ndcg = 46.47 
2025-04-08 16:18:50.113722: Steps 71/138: batch_recall = 89.24, batch_ndcg = 50.85 
2025-04-08 16:18:51.399799: Steps 72/138: batch_recall = 86.25, batch_ndcg = 49.13 
2025-04-08 16:18:52.692910: Steps 73/138: batch_recall = 87.63, batch_ndcg = 48.45 
2025-04-08 16:18:53.978431: Steps 74/138: batch_recall = 81.21, batch_ndcg = 48.60 
2025-04-08 16:18:55.273945: Steps 75/138: batch_recall = 85.74, batch_ndcg = 50.51 
2025-04-08 16:18:56.569592: Steps 76/138: batch_recall = 95.49, batch_ndcg = 54.45 
2025-04-08 16:18:57.857305: Steps 77/138: batch_recall = 89.36, batch_ndcg = 50.05 
2025-04-08 16:18:59.158863: Steps 78/138: batch_recall = 89.47, batch_ndcg = 47.61 
2025-04-08 16:19:00.446517: Steps 79/138: batch_recall = 96.24, batch_ndcg = 49.98 
2025-04-08 16:19:01.735757: Steps 80/138: batch_recall = 73.34, batch_ndcg = 38.79 
2025-04-08 16:19:03.017304: Steps 81/138: batch_recall = 78.77, batch_ndcg = 46.63 
2025-04-08 16:19:04.299897: Steps 82/138: batch_recall = 88.20, batch_ndcg = 51.37 
2025-04-08 16:19:05.587047: Steps 83/138: batch_recall = 80.24, batch_ndcg = 47.65 
2025-04-08 16:19:06.869455: Steps 84/138: batch_recall = 100.11, batch_ndcg = 57.96 
2025-04-08 16:19:08.139329: Steps 85/138: batch_recall = 103.38, batch_ndcg = 60.49 
2025-04-08 16:19:09.428830: Steps 86/138: batch_recall = 119.32, batch_ndcg = 70.73 
2025-04-08 16:19:10.710193: Steps 87/138: batch_recall = 108.75, batch_ndcg = 56.28 
2025-04-08 16:19:12.005357: Steps 88/138: batch_recall = 99.29, batch_ndcg = 56.18 
2025-04-08 16:19:13.288115: Steps 89/138: batch_recall = 119.47, batch_ndcg = 67.19 
2025-04-08 16:19:14.580853: Steps 90/138: batch_recall = 102.54, batch_ndcg = 58.75 
2025-04-08 16:19:15.875745: Steps 91/138: batch_recall = 119.81, batch_ndcg = 65.55 
2025-04-08 16:19:17.167855: Steps 92/138: batch_recall = 114.95, batch_ndcg = 62.82 
2025-04-08 16:19:18.443667: Steps 93/138: batch_recall = 118.27, batch_ndcg = 68.17 
2025-04-08 16:19:19.736625: Steps 94/138: batch_recall = 124.36, batch_ndcg = 64.85 
2025-04-08 16:19:21.005956: Steps 95/138: batch_recall = 113.79, batch_ndcg = 68.05 
2025-04-08 16:19:22.282369: Steps 96/138: batch_recall = 124.45, batch_ndcg = 72.94 
2025-04-08 16:19:23.547789: Steps 97/138: batch_recall = 135.55, batch_ndcg = 78.71 
2025-04-08 16:19:24.824956: Steps 98/138: batch_recall = 107.77, batch_ndcg = 61.98 
2025-04-08 16:19:26.099541: Steps 99/138: batch_recall = 126.14, batch_ndcg = 70.55 
2025-04-08 16:19:27.385852: Steps 100/138: batch_recall = 130.09, batch_ndcg = 72.69 
2025-04-08 16:19:28.665618: Steps 101/138: batch_recall = 127.89, batch_ndcg = 70.57 
2025-04-08 16:19:29.951772: Steps 102/138: batch_recall = 125.77, batch_ndcg = 73.18 
2025-04-08 16:19:31.256172: Steps 103/138: batch_recall = 143.98, batch_ndcg = 79.61 
2025-04-08 16:19:32.560898: Steps 104/138: batch_recall = 133.29, batch_ndcg = 76.61 
2025-04-08 16:19:33.846987: Steps 105/138: batch_recall = 117.50, batch_ndcg = 66.69 
2025-04-08 16:19:35.117941: Steps 106/138: batch_recall = 104.82, batch_ndcg = 59.17 
2025-04-08 16:19:36.390890: Steps 107/138: batch_recall = 113.39, batch_ndcg = 63.27 
2025-04-08 16:19:37.675565: Steps 108/138: batch_recall = 115.88, batch_ndcg = 69.32 
2025-04-08 16:19:38.958750: Steps 109/138: batch_recall = 134.28, batch_ndcg = 74.99 
2025-04-08 16:19:40.228353: Steps 110/138: batch_recall = 123.32, batch_ndcg = 64.31 
2025-04-08 16:19:41.513388: Steps 111/138: batch_recall = 140.75, batch_ndcg = 84.83 
2025-04-08 16:19:42.798424: Steps 112/138: batch_recall = 164.41, batch_ndcg = 91.36 
2025-04-08 16:19:44.078647: Steps 113/138: batch_recall = 127.29, batch_ndcg = 71.91 
2025-04-08 16:19:45.373958: Steps 114/138: batch_recall = 125.06, batch_ndcg = 71.72 
2025-04-08 16:19:46.650685: Steps 115/138: batch_recall = 121.16, batch_ndcg = 63.74 
2025-04-08 16:19:47.939358: Steps 116/138: batch_recall = 128.22, batch_ndcg = 66.32 
2025-04-08 16:19:49.219200: Steps 117/138: batch_recall = 116.01, batch_ndcg = 66.51 
2025-04-08 16:19:50.500035: Steps 118/138: batch_recall = 125.48, batch_ndcg = 71.06 
2025-04-08 16:19:51.781905: Steps 119/138: batch_recall = 138.32, batch_ndcg = 74.47 
2025-04-08 16:19:53.052463: Steps 120/138: batch_recall = 126.08, batch_ndcg = 70.91 
2025-04-08 16:19:54.325341: Steps 121/138: batch_recall = 151.59, batch_ndcg = 79.69 
2025-04-08 16:19:55.599305: Steps 122/138: batch_recall = 147.10, batch_ndcg = 81.56 
2025-04-08 16:19:56.869831: Steps 123/138: batch_recall = 130.12, batch_ndcg = 71.63 
2025-04-08 16:19:58.163446: Steps 124/138: batch_recall = 151.56, batch_ndcg = 93.59 
2025-04-08 16:19:59.441646: Steps 125/138: batch_recall = 132.75, batch_ndcg = 72.78 
2025-04-08 16:20:00.724764: Steps 126/138: batch_recall = 159.14, batch_ndcg = 89.14 
2025-04-08 16:20:02.003340: Steps 127/138: batch_recall = 142.34, batch_ndcg = 80.22 
2025-04-08 16:20:03.287371: Steps 128/138: batch_recall = 129.80, batch_ndcg = 71.65 
2025-04-08 16:20:04.562760: Steps 129/138: batch_recall = 159.16, batch_ndcg = 90.85 
2025-04-08 16:20:05.846262: Steps 130/138: batch_recall = 131.83, batch_ndcg = 69.18 
2025-04-08 16:20:07.118498: Steps 131/138: batch_recall = 151.67, batch_ndcg = 86.30 
2025-04-08 16:20:08.389369: Steps 132/138: batch_recall = 151.00, batch_ndcg = 86.17 
2025-04-08 16:20:09.657012: Steps 133/138: batch_recall = 146.02, batch_ndcg = 84.19 
2025-04-08 16:20:10.922919: Steps 134/138: batch_recall = 140.46, batch_ndcg = 80.50 
2025-04-08 16:20:12.195955: Steps 135/138: batch_recall = 162.53, batch_ndcg = 92.92 
2025-04-08 16:20:13.479668: Steps 136/138: batch_recall = 150.68, batch_ndcg = 78.10 
2025-04-08 16:20:14.757893: Steps 137/138: batch_recall = 141.10, batch_ndcg = 86.67 
2025-04-08 16:20:14.758465: Epoch 17/1000, Test: Recall = 0.1764, NDCG = 0.0997  

2025-04-08 16:20:16.527957: Training Step 0/354: batchLoss = 0.4514, diffLoss = 2.2009, kgLoss = 0.0140
2025-04-08 16:20:18.160880: Training Step 1/354: batchLoss = 0.5398, diffLoss = 2.6452, kgLoss = 0.0134
2025-04-08 16:20:19.793653: Training Step 2/354: batchLoss = 0.5091, diffLoss = 2.4962, kgLoss = 0.0123
2025-04-08 16:20:21.417285: Training Step 3/354: batchLoss = 0.5668, diffLoss = 2.7810, kgLoss = 0.0132
2025-04-08 16:20:23.033080: Training Step 4/354: batchLoss = 0.4894, diffLoss = 2.3972, kgLoss = 0.0125
2025-04-08 16:20:24.646741: Training Step 5/354: batchLoss = 0.5157, diffLoss = 2.5299, kgLoss = 0.0121
2025-04-08 16:20:26.267936: Training Step 6/354: batchLoss = 0.6357, diffLoss = 3.1145, kgLoss = 0.0160
2025-04-08 16:20:27.887016: Training Step 7/354: batchLoss = 0.6624, diffLoss = 3.2368, kgLoss = 0.0188
2025-04-08 16:20:29.516352: Training Step 8/354: batchLoss = 0.5057, diffLoss = 2.4776, kgLoss = 0.0127
2025-04-08 16:20:31.147721: Training Step 9/354: batchLoss = 0.7008, diffLoss = 3.4291, kgLoss = 0.0188
2025-04-08 16:20:32.774480: Training Step 10/354: batchLoss = 0.5415, diffLoss = 2.6583, kgLoss = 0.0123
2025-04-08 16:20:34.406181: Training Step 11/354: batchLoss = 0.5278, diffLoss = 2.5799, kgLoss = 0.0147
2025-04-08 16:20:36.041307: Training Step 12/354: batchLoss = 0.5257, diffLoss = 2.5778, kgLoss = 0.0127
2025-04-08 16:20:37.669654: Training Step 13/354: batchLoss = 0.4701, diffLoss = 2.3014, kgLoss = 0.0123
2025-04-08 16:20:39.282971: Training Step 14/354: batchLoss = 0.5212, diffLoss = 2.5490, kgLoss = 0.0143
2025-04-08 16:20:40.894131: Training Step 15/354: batchLoss = 0.6923, diffLoss = 3.3924, kgLoss = 0.0173
2025-04-08 16:20:42.516175: Training Step 16/354: batchLoss = 0.5366, diffLoss = 2.6237, kgLoss = 0.0148
2025-04-08 16:20:44.133302: Training Step 17/354: batchLoss = 0.7208, diffLoss = 3.5313, kgLoss = 0.0181
2025-04-08 16:20:45.755757: Training Step 18/354: batchLoss = 0.5437, diffLoss = 2.6604, kgLoss = 0.0145
2025-04-08 16:20:47.380557: Training Step 19/354: batchLoss = 0.5966, diffLoss = 2.9202, kgLoss = 0.0157
2025-04-08 16:20:49.010053: Training Step 20/354: batchLoss = 0.5145, diffLoss = 2.5217, kgLoss = 0.0128
2025-04-08 16:20:50.637804: Training Step 21/354: batchLoss = 0.6024, diffLoss = 2.9447, kgLoss = 0.0168
2025-04-08 16:20:52.268696: Training Step 22/354: batchLoss = 0.4679, diffLoss = 2.2916, kgLoss = 0.0120
2025-04-08 16:20:53.891447: Training Step 23/354: batchLoss = 0.5978, diffLoss = 2.9299, kgLoss = 0.0147
2025-04-08 16:20:55.511354: Training Step 24/354: batchLoss = 0.5852, diffLoss = 2.8716, kgLoss = 0.0136
2025-04-08 16:20:57.114728: Training Step 25/354: batchLoss = 0.6021, diffLoss = 2.9531, kgLoss = 0.0143
2025-04-08 16:20:58.729245: Training Step 26/354: batchLoss = 0.4900, diffLoss = 2.4006, kgLoss = 0.0124
2025-04-08 16:21:00.343375: Training Step 27/354: batchLoss = 0.5873, diffLoss = 2.8779, kgLoss = 0.0147
2025-04-08 16:21:01.967175: Training Step 28/354: batchLoss = 0.5505, diffLoss = 2.6865, kgLoss = 0.0165
2025-04-08 16:21:03.599046: Training Step 29/354: batchLoss = 0.6278, diffLoss = 3.0737, kgLoss = 0.0164
2025-04-08 16:21:05.230231: Training Step 30/354: batchLoss = 0.5286, diffLoss = 2.5804, kgLoss = 0.0157
2025-04-08 16:21:06.855455: Training Step 31/354: batchLoss = 0.6946, diffLoss = 3.4087, kgLoss = 0.0161
2025-04-08 16:21:08.483303: Training Step 32/354: batchLoss = 0.5265, diffLoss = 2.5665, kgLoss = 0.0165
2025-04-08 16:21:10.103282: Training Step 33/354: batchLoss = 0.5161, diffLoss = 2.5265, kgLoss = 0.0135
2025-04-08 16:21:11.718163: Training Step 34/354: batchLoss = 0.5597, diffLoss = 2.7421, kgLoss = 0.0141
2025-04-08 16:21:13.334758: Training Step 35/354: batchLoss = 0.6599, diffLoss = 3.2382, kgLoss = 0.0153
2025-04-08 16:21:14.951239: Training Step 36/354: batchLoss = 0.5184, diffLoss = 2.5431, kgLoss = 0.0122
2025-04-08 16:21:16.577243: Training Step 37/354: batchLoss = 0.5240, diffLoss = 2.5667, kgLoss = 0.0133
2025-04-08 16:21:18.209284: Training Step 38/354: batchLoss = 0.5251, diffLoss = 2.5724, kgLoss = 0.0132
2025-04-08 16:21:19.836699: Training Step 39/354: batchLoss = 0.6254, diffLoss = 3.0572, kgLoss = 0.0175
2025-04-08 16:21:21.465051: Training Step 40/354: batchLoss = 0.4619, diffLoss = 2.2629, kgLoss = 0.0116
2025-04-08 16:21:23.094279: Training Step 41/354: batchLoss = 0.5116, diffLoss = 2.5083, kgLoss = 0.0124
2025-04-08 16:21:24.720480: Training Step 42/354: batchLoss = 0.5662, diffLoss = 2.7729, kgLoss = 0.0146
2025-04-08 16:21:26.348395: Training Step 43/354: batchLoss = 0.5450, diffLoss = 2.6717, kgLoss = 0.0133
2025-04-08 16:21:27.973334: Training Step 44/354: batchLoss = 0.7085, diffLoss = 3.4721, kgLoss = 0.0177
2025-04-08 16:21:29.590208: Training Step 45/354: batchLoss = 0.6535, diffLoss = 3.2055, kgLoss = 0.0155
2025-04-08 16:21:31.211466: Training Step 46/354: batchLoss = 0.5134, diffLoss = 2.5156, kgLoss = 0.0129
2025-04-08 16:21:32.841695: Training Step 47/354: batchLoss = 0.5333, diffLoss = 2.6066, kgLoss = 0.0149
2025-04-08 16:21:34.473645: Training Step 48/354: batchLoss = 0.5264, diffLoss = 2.5797, kgLoss = 0.0130
2025-04-08 16:21:36.099476: Training Step 49/354: batchLoss = 0.6008, diffLoss = 2.9408, kgLoss = 0.0158
2025-04-08 16:21:37.726420: Training Step 50/354: batchLoss = 0.4471, diffLoss = 2.1899, kgLoss = 0.0114
2025-04-08 16:21:39.355500: Training Step 51/354: batchLoss = 0.6054, diffLoss = 2.9610, kgLoss = 0.0164
2025-04-08 16:21:40.989228: Training Step 52/354: batchLoss = 0.4665, diffLoss = 2.2828, kgLoss = 0.0124
2025-04-08 16:21:42.619307: Training Step 53/354: batchLoss = 0.6030, diffLoss = 2.9483, kgLoss = 0.0166
2025-04-08 16:21:44.240736: Training Step 54/354: batchLoss = 0.5661, diffLoss = 2.7732, kgLoss = 0.0143
2025-04-08 16:21:45.856468: Training Step 55/354: batchLoss = 0.4997, diffLoss = 2.4413, kgLoss = 0.0143
2025-04-08 16:21:47.474259: Training Step 56/354: batchLoss = 0.6679, diffLoss = 3.2740, kgLoss = 0.0164
2025-04-08 16:21:49.100599: Training Step 57/354: batchLoss = 0.6068, diffLoss = 2.9712, kgLoss = 0.0157
2025-04-08 16:21:50.723449: Training Step 58/354: batchLoss = 0.6039, diffLoss = 2.9622, kgLoss = 0.0143
2025-04-08 16:21:52.358369: Training Step 59/354: batchLoss = 0.4933, diffLoss = 2.4105, kgLoss = 0.0139
2025-04-08 16:21:53.991999: Training Step 60/354: batchLoss = 0.6174, diffLoss = 3.0208, kgLoss = 0.0165
2025-04-08 16:21:55.616891: Training Step 61/354: batchLoss = 0.5492, diffLoss = 2.6876, kgLoss = 0.0147
2025-04-08 16:21:57.249263: Training Step 62/354: batchLoss = 0.8547, diffLoss = 4.1903, kgLoss = 0.0208
2025-04-08 16:21:58.879813: Training Step 63/354: batchLoss = 0.4520, diffLoss = 2.2060, kgLoss = 0.0134
2025-04-08 16:22:00.497187: Training Step 64/354: batchLoss = 0.4791, diffLoss = 2.3400, kgLoss = 0.0139
2025-04-08 16:22:02.118947: Training Step 65/354: batchLoss = 0.4673, diffLoss = 2.2907, kgLoss = 0.0114
2025-04-08 16:22:03.744968: Training Step 66/354: batchLoss = 0.5552, diffLoss = 2.7232, kgLoss = 0.0132
2025-04-08 16:22:05.367416: Training Step 67/354: batchLoss = 0.4953, diffLoss = 2.4282, kgLoss = 0.0121
2025-04-08 16:22:06.999906: Training Step 68/354: batchLoss = 0.7107, diffLoss = 3.4841, kgLoss = 0.0173
2025-04-08 16:22:08.633880: Training Step 69/354: batchLoss = 0.6098, diffLoss = 2.9804, kgLoss = 0.0172
2025-04-08 16:22:10.271728: Training Step 70/354: batchLoss = 0.6557, diffLoss = 3.2151, kgLoss = 0.0159
2025-04-08 16:22:11.909369: Training Step 71/354: batchLoss = 0.5364, diffLoss = 2.6221, kgLoss = 0.0150
2025-04-08 16:22:13.543334: Training Step 72/354: batchLoss = 0.5890, diffLoss = 2.8823, kgLoss = 0.0156
2025-04-08 16:22:15.169071: Training Step 73/354: batchLoss = 0.5150, diffLoss = 2.5241, kgLoss = 0.0127
2025-04-08 16:22:16.794520: Training Step 74/354: batchLoss = 0.4948, diffLoss = 2.4221, kgLoss = 0.0129
2025-04-08 16:22:18.414911: Training Step 75/354: batchLoss = 0.5795, diffLoss = 2.8387, kgLoss = 0.0147
2025-04-08 16:22:20.045245: Training Step 76/354: batchLoss = 0.6000, diffLoss = 2.9403, kgLoss = 0.0149
2025-04-08 16:22:21.668886: Training Step 77/354: batchLoss = 0.6996, diffLoss = 3.4296, kgLoss = 0.0171
2025-04-08 16:22:23.303137: Training Step 78/354: batchLoss = 0.6329, diffLoss = 3.1029, kgLoss = 0.0154
2025-04-08 16:22:24.938182: Training Step 79/354: batchLoss = 0.7455, diffLoss = 3.6539, kgLoss = 0.0184
2025-04-08 16:22:26.563127: Training Step 80/354: batchLoss = 0.5632, diffLoss = 2.7652, kgLoss = 0.0127
2025-04-08 16:22:28.199272: Training Step 81/354: batchLoss = 0.5901, diffLoss = 2.8897, kgLoss = 0.0152
2025-04-08 16:22:29.824261: Training Step 82/354: batchLoss = 0.4870, diffLoss = 2.3822, kgLoss = 0.0133
2025-04-08 16:22:31.448132: Training Step 83/354: batchLoss = 0.5969, diffLoss = 2.9255, kgLoss = 0.0148
2025-04-08 16:22:33.069177: Training Step 84/354: batchLoss = 0.5745, diffLoss = 2.8151, kgLoss = 0.0143
2025-04-08 16:22:34.688089: Training Step 85/354: batchLoss = 0.5921, diffLoss = 2.9000, kgLoss = 0.0151
2025-04-08 16:22:36.306118: Training Step 86/354: batchLoss = 0.5655, diffLoss = 2.7734, kgLoss = 0.0136
2025-04-08 16:22:37.936497: Training Step 87/354: batchLoss = 0.5027, diffLoss = 2.4613, kgLoss = 0.0131
2025-04-08 16:22:39.564534: Training Step 88/354: batchLoss = 0.6864, diffLoss = 3.3672, kgLoss = 0.0162
2025-04-08 16:22:41.196624: Training Step 89/354: batchLoss = 0.6818, diffLoss = 3.3447, kgLoss = 0.0161
2025-04-08 16:22:42.833897: Training Step 90/354: batchLoss = 0.7493, diffLoss = 3.6721, kgLoss = 0.0187
2025-04-08 16:22:44.464971: Training Step 91/354: batchLoss = 0.5942, diffLoss = 2.9098, kgLoss = 0.0153
2025-04-08 16:22:46.091805: Training Step 92/354: batchLoss = 0.5228, diffLoss = 2.5611, kgLoss = 0.0132
2025-04-08 16:22:47.718997: Training Step 93/354: batchLoss = 0.8639, diffLoss = 4.2317, kgLoss = 0.0220
2025-04-08 16:22:49.336256: Training Step 94/354: batchLoss = 0.6671, diffLoss = 3.2664, kgLoss = 0.0173
2025-04-08 16:22:50.958577: Training Step 95/354: batchLoss = 0.5157, diffLoss = 2.5306, kgLoss = 0.0119
2025-04-08 16:22:52.571831: Training Step 96/354: batchLoss = 0.5274, diffLoss = 2.5780, kgLoss = 0.0147
2025-04-08 16:22:54.202616: Training Step 97/354: batchLoss = 0.5547, diffLoss = 2.7164, kgLoss = 0.0143
2025-04-08 16:22:55.838717: Training Step 98/354: batchLoss = 0.5158, diffLoss = 2.5259, kgLoss = 0.0132
2025-04-08 16:22:57.464152: Training Step 99/354: batchLoss = 0.5343, diffLoss = 2.6208, kgLoss = 0.0126
2025-04-08 16:22:59.100316: Training Step 100/354: batchLoss = 0.6016, diffLoss = 2.9489, kgLoss = 0.0148
2025-04-08 16:23:00.727229: Training Step 101/354: batchLoss = 0.5744, diffLoss = 2.8199, kgLoss = 0.0130
2025-04-08 16:23:02.352992: Training Step 102/354: batchLoss = 0.5903, diffLoss = 2.8944, kgLoss = 0.0143
2025-04-08 16:23:03.973546: Training Step 103/354: batchLoss = 0.5517, diffLoss = 2.6999, kgLoss = 0.0147
2025-04-08 16:23:05.585876: Training Step 104/354: batchLoss = 0.5333, diffLoss = 2.6107, kgLoss = 0.0140
2025-04-08 16:23:07.207970: Training Step 105/354: batchLoss = 0.5942, diffLoss = 2.9020, kgLoss = 0.0173
2025-04-08 16:23:08.835416: Training Step 106/354: batchLoss = 0.6066, diffLoss = 2.9692, kgLoss = 0.0160
2025-04-08 16:23:10.472768: Training Step 107/354: batchLoss = 0.5747, diffLoss = 2.8210, kgLoss = 0.0131
2025-04-08 16:23:12.109766: Training Step 108/354: batchLoss = 0.7036, diffLoss = 3.4477, kgLoss = 0.0175
2025-04-08 16:23:13.739157: Training Step 109/354: batchLoss = 0.5707, diffLoss = 2.7940, kgLoss = 0.0148
2025-04-08 16:23:15.373535: Training Step 110/354: batchLoss = 0.6546, diffLoss = 3.2141, kgLoss = 0.0148
2025-04-08 16:23:17.007704: Training Step 111/354: batchLoss = 0.5589, diffLoss = 2.7375, kgLoss = 0.0143
2025-04-08 16:23:18.632237: Training Step 112/354: batchLoss = 0.5904, diffLoss = 2.8909, kgLoss = 0.0153
2025-04-08 16:23:20.250975: Training Step 113/354: batchLoss = 0.5278, diffLoss = 2.5864, kgLoss = 0.0131
2025-04-08 16:23:21.875206: Training Step 114/354: batchLoss = 0.5498, diffLoss = 2.6922, kgLoss = 0.0141
2025-04-08 16:23:23.497452: Training Step 115/354: batchLoss = 0.4870, diffLoss = 2.3882, kgLoss = 0.0118
2025-04-08 16:23:25.119452: Training Step 116/354: batchLoss = 0.5044, diffLoss = 2.4712, kgLoss = 0.0127
2025-04-08 16:23:26.747086: Training Step 117/354: batchLoss = 0.5728, diffLoss = 2.8123, kgLoss = 0.0129
2025-04-08 16:23:28.375499: Training Step 118/354: batchLoss = 0.4952, diffLoss = 2.4203, kgLoss = 0.0139
2025-04-08 16:23:30.002579: Training Step 119/354: batchLoss = 0.5481, diffLoss = 2.6807, kgLoss = 0.0149
2025-04-08 16:23:31.632135: Training Step 120/354: batchLoss = 0.4672, diffLoss = 2.2846, kgLoss = 0.0129
2025-04-08 16:23:33.256410: Training Step 121/354: batchLoss = 0.5041, diffLoss = 2.4658, kgLoss = 0.0136
2025-04-08 16:23:34.888109: Training Step 122/354: batchLoss = 0.6628, diffLoss = 3.2434, kgLoss = 0.0177
2025-04-08 16:23:36.506626: Training Step 123/354: batchLoss = 0.4921, diffLoss = 2.4069, kgLoss = 0.0134
2025-04-08 16:23:38.124716: Training Step 124/354: batchLoss = 0.4634, diffLoss = 2.2677, kgLoss = 0.0124
2025-04-08 16:23:39.734578: Training Step 125/354: batchLoss = 0.4728, diffLoss = 2.3131, kgLoss = 0.0127
2025-04-08 16:23:41.355400: Training Step 126/354: batchLoss = 0.5259, diffLoss = 2.5748, kgLoss = 0.0137
2025-04-08 16:23:42.980392: Training Step 127/354: batchLoss = 0.5631, diffLoss = 2.7622, kgLoss = 0.0133
2025-04-08 16:23:44.617611: Training Step 128/354: batchLoss = 0.6091, diffLoss = 2.9866, kgLoss = 0.0147
2025-04-08 16:23:46.250516: Training Step 129/354: batchLoss = 0.4834, diffLoss = 2.3610, kgLoss = 0.0141
2025-04-08 16:23:47.963659: Training Step 130/354: batchLoss = 0.4476, diffLoss = 2.1883, kgLoss = 0.0125
2025-04-08 16:23:49.589148: Training Step 131/354: batchLoss = 0.5167, diffLoss = 2.5273, kgLoss = 0.0140
2025-04-08 16:23:51.220007: Training Step 132/354: batchLoss = 0.5136, diffLoss = 2.5140, kgLoss = 0.0135
2025-04-08 16:23:52.836732: Training Step 133/354: batchLoss = 0.5610, diffLoss = 2.7501, kgLoss = 0.0137
2025-04-08 16:23:54.459082: Training Step 134/354: batchLoss = 0.6060, diffLoss = 2.9632, kgLoss = 0.0167
2025-04-08 16:23:56.074633: Training Step 135/354: batchLoss = 0.4610, diffLoss = 2.2532, kgLoss = 0.0129
2025-04-08 16:23:57.695070: Training Step 136/354: batchLoss = 0.5025, diffLoss = 2.4544, kgLoss = 0.0146
2025-04-08 16:23:59.324165: Training Step 137/354: batchLoss = 0.3693, diffLoss = 1.8016, kgLoss = 0.0112
2025-04-08 16:24:00.947104: Training Step 138/354: batchLoss = 0.5422, diffLoss = 2.6585, kgLoss = 0.0131
2025-04-08 16:24:02.575527: Training Step 139/354: batchLoss = 0.4810, diffLoss = 2.3441, kgLoss = 0.0153
2025-04-08 16:24:04.199251: Training Step 140/354: batchLoss = 0.4970, diffLoss = 2.4310, kgLoss = 0.0135
2025-04-08 16:24:05.825231: Training Step 141/354: batchLoss = 0.6616, diffLoss = 3.2356, kgLoss = 0.0181
2025-04-08 16:24:07.466185: Training Step 142/354: batchLoss = 0.5471, diffLoss = 2.6761, kgLoss = 0.0148
2025-04-08 16:24:09.086837: Training Step 143/354: batchLoss = 0.6488, diffLoss = 3.1726, kgLoss = 0.0178
2025-04-08 16:24:10.711889: Training Step 144/354: batchLoss = 0.4963, diffLoss = 2.4304, kgLoss = 0.0128
2025-04-08 16:24:12.336374: Training Step 145/354: batchLoss = 0.5157, diffLoss = 2.5258, kgLoss = 0.0132
2025-04-08 16:24:13.968996: Training Step 146/354: batchLoss = 0.6649, diffLoss = 3.2603, kgLoss = 0.0161
2025-04-08 16:24:15.602081: Training Step 147/354: batchLoss = 0.5892, diffLoss = 2.8832, kgLoss = 0.0157
2025-04-08 16:24:17.228317: Training Step 148/354: batchLoss = 0.5819, diffLoss = 2.8474, kgLoss = 0.0155
2025-04-08 16:24:18.856922: Training Step 149/354: batchLoss = 0.5947, diffLoss = 2.9117, kgLoss = 0.0155
2025-04-08 16:24:20.480145: Training Step 150/354: batchLoss = 0.5586, diffLoss = 2.7429, kgLoss = 0.0126
2025-04-08 16:24:22.116441: Training Step 151/354: batchLoss = 0.7359, diffLoss = 3.6060, kgLoss = 0.0184
2025-04-08 16:24:23.742997: Training Step 152/354: batchLoss = 0.5712, diffLoss = 2.7961, kgLoss = 0.0150
2025-04-08 16:24:25.372985: Training Step 153/354: batchLoss = 0.5013, diffLoss = 2.4580, kgLoss = 0.0121
2025-04-08 16:24:26.988348: Training Step 154/354: batchLoss = 0.4553, diffLoss = 2.2316, kgLoss = 0.0113
2025-04-08 16:24:28.611470: Training Step 155/354: batchLoss = 0.5164, diffLoss = 2.5269, kgLoss = 0.0138
2025-04-08 16:24:30.242216: Training Step 156/354: batchLoss = 0.6206, diffLoss = 3.0412, kgLoss = 0.0155
2025-04-08 16:24:31.868260: Training Step 157/354: batchLoss = 0.5852, diffLoss = 2.8668, kgLoss = 0.0148
2025-04-08 16:24:33.496413: Training Step 158/354: batchLoss = 0.5516, diffLoss = 2.6992, kgLoss = 0.0147
2025-04-08 16:24:35.121015: Training Step 159/354: batchLoss = 0.6131, diffLoss = 3.0072, kgLoss = 0.0146
2025-04-08 16:24:36.748351: Training Step 160/354: batchLoss = 0.6338, diffLoss = 3.1108, kgLoss = 0.0145
2025-04-08 16:24:38.395185: Training Step 161/354: batchLoss = 0.5735, diffLoss = 2.8114, kgLoss = 0.0140
2025-04-08 16:24:40.020891: Training Step 162/354: batchLoss = 0.5485, diffLoss = 2.6837, kgLoss = 0.0147
2025-04-08 16:24:41.636529: Training Step 163/354: batchLoss = 0.5894, diffLoss = 2.8968, kgLoss = 0.0126
2025-04-08 16:24:43.268870: Training Step 164/354: batchLoss = 0.5428, diffLoss = 2.6607, kgLoss = 0.0133
2025-04-08 16:24:44.888459: Training Step 165/354: batchLoss = 0.5228, diffLoss = 2.5674, kgLoss = 0.0116
2025-04-08 16:24:46.519300: Training Step 166/354: batchLoss = 0.5116, diffLoss = 2.4996, kgLoss = 0.0146
2025-04-08 16:24:48.145851: Training Step 167/354: batchLoss = 0.5340, diffLoss = 2.6120, kgLoss = 0.0146
2025-04-08 16:24:49.772379: Training Step 168/354: batchLoss = 0.5539, diffLoss = 2.7175, kgLoss = 0.0130
2025-04-08 16:24:51.393146: Training Step 169/354: batchLoss = 0.6042, diffLoss = 2.9566, kgLoss = 0.0161
2025-04-08 16:24:53.019137: Training Step 170/354: batchLoss = 0.6244, diffLoss = 3.0531, kgLoss = 0.0172
2025-04-08 16:24:54.651438: Training Step 171/354: batchLoss = 0.4932, diffLoss = 2.4052, kgLoss = 0.0152
2025-04-08 16:24:56.278585: Training Step 172/354: batchLoss = 0.4970, diffLoss = 2.4333, kgLoss = 0.0129
2025-04-08 16:24:57.911281: Training Step 173/354: batchLoss = 0.5733, diffLoss = 2.8050, kgLoss = 0.0154
2025-04-08 16:24:59.525272: Training Step 174/354: batchLoss = 0.5070, diffLoss = 2.4829, kgLoss = 0.0130
2025-04-08 16:25:01.159421: Training Step 175/354: batchLoss = 0.5399, diffLoss = 2.6441, kgLoss = 0.0139
2025-04-08 16:25:02.783914: Training Step 176/354: batchLoss = 0.4625, diffLoss = 2.2584, kgLoss = 0.0135
2025-04-08 16:25:04.423059: Training Step 177/354: batchLoss = 0.4922, diffLoss = 2.3951, kgLoss = 0.0165
2025-04-08 16:25:06.051367: Training Step 178/354: batchLoss = 0.4452, diffLoss = 2.1783, kgLoss = 0.0120
2025-04-08 16:25:07.675041: Training Step 179/354: batchLoss = 0.7739, diffLoss = 3.7951, kgLoss = 0.0186
2025-04-08 16:25:09.301103: Training Step 180/354: batchLoss = 0.6014, diffLoss = 2.9487, kgLoss = 0.0146
2025-04-08 16:25:10.922663: Training Step 181/354: batchLoss = 0.5805, diffLoss = 2.8439, kgLoss = 0.0146
2025-04-08 16:25:12.543241: Training Step 182/354: batchLoss = 0.5656, diffLoss = 2.7767, kgLoss = 0.0129
2025-04-08 16:25:14.176223: Training Step 183/354: batchLoss = 0.4966, diffLoss = 2.4285, kgLoss = 0.0137
2025-04-08 16:25:15.802862: Training Step 184/354: batchLoss = 0.5989, diffLoss = 2.9373, kgLoss = 0.0143
2025-04-08 16:25:17.428434: Training Step 185/354: batchLoss = 0.6324, diffLoss = 3.1005, kgLoss = 0.0153
2025-04-08 16:25:19.053572: Training Step 186/354: batchLoss = 0.7136, diffLoss = 3.5022, kgLoss = 0.0164
2025-04-08 16:25:20.681145: Training Step 187/354: batchLoss = 0.4718, diffLoss = 2.2970, kgLoss = 0.0155
2025-04-08 16:25:22.307424: Training Step 188/354: batchLoss = 0.6630, diffLoss = 3.2469, kgLoss = 0.0171
2025-04-08 16:25:23.934523: Training Step 189/354: batchLoss = 0.6699, diffLoss = 3.2876, kgLoss = 0.0155
2025-04-08 16:25:25.563978: Training Step 190/354: batchLoss = 0.6533, diffLoss = 3.2019, kgLoss = 0.0162
2025-04-08 16:25:27.195485: Training Step 191/354: batchLoss = 0.7523, diffLoss = 3.6951, kgLoss = 0.0166
2025-04-08 16:25:28.819642: Training Step 192/354: batchLoss = 0.6277, diffLoss = 3.0713, kgLoss = 0.0168
2025-04-08 16:25:30.442638: Training Step 193/354: batchLoss = 0.5541, diffLoss = 2.7216, kgLoss = 0.0122
2025-04-08 16:25:32.057318: Training Step 194/354: batchLoss = 0.5712, diffLoss = 2.7970, kgLoss = 0.0148
2025-04-08 16:25:33.687365: Training Step 195/354: batchLoss = 0.6152, diffLoss = 3.0139, kgLoss = 0.0156
2025-04-08 16:25:35.311740: Training Step 196/354: batchLoss = 0.5547, diffLoss = 2.7228, kgLoss = 0.0127
2025-04-08 16:25:36.937030: Training Step 197/354: batchLoss = 0.5888, diffLoss = 2.8869, kgLoss = 0.0142
2025-04-08 16:25:38.560119: Training Step 198/354: batchLoss = 0.5101, diffLoss = 2.4902, kgLoss = 0.0151
2025-04-08 16:25:40.190697: Training Step 199/354: batchLoss = 0.5448, diffLoss = 2.6712, kgLoss = 0.0132
2025-04-08 16:25:41.823786: Training Step 200/354: batchLoss = 0.4825, diffLoss = 2.3582, kgLoss = 0.0136
2025-04-08 16:25:43.455853: Training Step 201/354: batchLoss = 0.6073, diffLoss = 2.9686, kgLoss = 0.0170
2025-04-08 16:25:45.081070: Training Step 202/354: batchLoss = 0.5711, diffLoss = 2.8028, kgLoss = 0.0132
2025-04-08 16:25:46.706585: Training Step 203/354: batchLoss = 0.5766, diffLoss = 2.8303, kgLoss = 0.0132
2025-04-08 16:25:48.326568: Training Step 204/354: batchLoss = 0.6616, diffLoss = 3.2466, kgLoss = 0.0154
2025-04-08 16:25:49.966303: Training Step 205/354: batchLoss = 0.5298, diffLoss = 2.5920, kgLoss = 0.0142
2025-04-08 16:25:51.600322: Training Step 206/354: batchLoss = 0.5616, diffLoss = 2.7516, kgLoss = 0.0141
2025-04-08 16:25:53.224761: Training Step 207/354: batchLoss = 0.5249, diffLoss = 2.5676, kgLoss = 0.0143
2025-04-08 16:25:54.851415: Training Step 208/354: batchLoss = 0.6407, diffLoss = 3.1460, kgLoss = 0.0144
2025-04-08 16:25:56.478276: Training Step 209/354: batchLoss = 0.5651, diffLoss = 2.7724, kgLoss = 0.0133
2025-04-08 16:25:58.099282: Training Step 210/354: batchLoss = 0.5400, diffLoss = 2.6434, kgLoss = 0.0142
2025-04-08 16:25:59.734415: Training Step 211/354: batchLoss = 0.4322, diffLoss = 2.1155, kgLoss = 0.0113
2025-04-08 16:26:01.359373: Training Step 212/354: batchLoss = 0.7838, diffLoss = 3.8557, kgLoss = 0.0158
2025-04-08 16:26:02.992286: Training Step 213/354: batchLoss = 0.5248, diffLoss = 2.5736, kgLoss = 0.0126
2025-04-08 16:26:04.617169: Training Step 214/354: batchLoss = 0.5872, diffLoss = 2.8794, kgLoss = 0.0142
2025-04-08 16:26:06.242038: Training Step 215/354: batchLoss = 0.5609, diffLoss = 2.7506, kgLoss = 0.0135
2025-04-08 16:26:07.862608: Training Step 216/354: batchLoss = 0.5680, diffLoss = 2.7850, kgLoss = 0.0137
2025-04-08 16:26:09.492760: Training Step 217/354: batchLoss = 0.5798, diffLoss = 2.8280, kgLoss = 0.0177
2025-04-08 16:26:11.125313: Training Step 218/354: batchLoss = 0.5249, diffLoss = 2.5684, kgLoss = 0.0140
2025-04-08 16:26:12.753404: Training Step 219/354: batchLoss = 0.6108, diffLoss = 2.9941, kgLoss = 0.0150
2025-04-08 16:26:14.380927: Training Step 220/354: batchLoss = 1.0666, diffLoss = 5.2285, kgLoss = 0.0262
2025-04-08 16:26:16.018215: Training Step 221/354: batchLoss = 0.5454, diffLoss = 2.6745, kgLoss = 0.0131
2025-04-08 16:26:17.639169: Training Step 222/354: batchLoss = 0.4482, diffLoss = 2.1887, kgLoss = 0.0131
2025-04-08 16:26:19.262564: Training Step 223/354: batchLoss = 0.5833, diffLoss = 2.8559, kgLoss = 0.0152
2025-04-08 16:26:20.881141: Training Step 224/354: batchLoss = 0.6070, diffLoss = 2.9783, kgLoss = 0.0142
2025-04-08 16:26:22.501153: Training Step 225/354: batchLoss = 0.5736, diffLoss = 2.8123, kgLoss = 0.0139
2025-04-08 16:26:24.128936: Training Step 226/354: batchLoss = 0.5266, diffLoss = 2.5800, kgLoss = 0.0133
2025-04-08 16:26:25.754439: Training Step 227/354: batchLoss = 0.4042, diffLoss = 1.9654, kgLoss = 0.0139
2025-04-08 16:26:27.384610: Training Step 228/354: batchLoss = 0.6087, diffLoss = 2.9755, kgLoss = 0.0171
2025-04-08 16:26:29.029134: Training Step 229/354: batchLoss = 0.5496, diffLoss = 2.6921, kgLoss = 0.0140
2025-04-08 16:26:30.660336: Training Step 230/354: batchLoss = 0.6039, diffLoss = 2.9549, kgLoss = 0.0161
2025-04-08 16:26:32.289256: Training Step 231/354: batchLoss = 0.7321, diffLoss = 3.5948, kgLoss = 0.0165
2025-04-08 16:26:33.911333: Training Step 232/354: batchLoss = 0.4927, diffLoss = 2.4132, kgLoss = 0.0126
2025-04-08 16:26:35.538079: Training Step 233/354: batchLoss = 0.4444, diffLoss = 2.1678, kgLoss = 0.0136
2025-04-08 16:26:37.151239: Training Step 234/354: batchLoss = 0.5647, diffLoss = 2.7692, kgLoss = 0.0136
2025-04-08 16:26:38.770553: Training Step 235/354: batchLoss = 0.6484, diffLoss = 3.1764, kgLoss = 0.0164
2025-04-08 16:26:40.393943: Training Step 236/354: batchLoss = 0.5469, diffLoss = 2.6784, kgLoss = 0.0140
2025-04-08 16:26:42.018338: Training Step 237/354: batchLoss = 0.4519, diffLoss = 2.2046, kgLoss = 0.0137
2025-04-08 16:26:43.650108: Training Step 238/354: batchLoss = 0.5328, diffLoss = 2.6089, kgLoss = 0.0138
2025-04-08 16:26:45.281455: Training Step 239/354: batchLoss = 0.4206, diffLoss = 2.0544, kgLoss = 0.0122
2025-04-08 16:26:46.914763: Training Step 240/354: batchLoss = 0.6697, diffLoss = 3.2814, kgLoss = 0.0167
2025-04-08 16:26:48.541880: Training Step 241/354: batchLoss = 0.6538, diffLoss = 3.2091, kgLoss = 0.0149
2025-04-08 16:26:50.162904: Training Step 242/354: batchLoss = 0.5570, diffLoss = 2.7306, kgLoss = 0.0136
2025-04-08 16:26:51.774199: Training Step 243/354: batchLoss = 0.5537, diffLoss = 2.7110, kgLoss = 0.0144
2025-04-08 16:26:53.391171: Training Step 244/354: batchLoss = 0.7312, diffLoss = 3.5885, kgLoss = 0.0169
2025-04-08 16:26:55.014585: Training Step 245/354: batchLoss = 0.5450, diffLoss = 2.6683, kgLoss = 0.0141
2025-04-08 16:26:56.658906: Training Step 246/354: batchLoss = 0.4967, diffLoss = 2.4309, kgLoss = 0.0131
2025-04-08 16:26:58.281658: Training Step 247/354: batchLoss = 0.5156, diffLoss = 2.5239, kgLoss = 0.0135
2025-04-08 16:26:59.907046: Training Step 248/354: batchLoss = 0.5224, diffLoss = 2.5582, kgLoss = 0.0134
2025-04-08 16:27:01.538111: Training Step 249/354: batchLoss = 0.5280, diffLoss = 2.5782, kgLoss = 0.0155
2025-04-08 16:27:03.166418: Training Step 250/354: batchLoss = 0.6364, diffLoss = 3.1196, kgLoss = 0.0155
2025-04-08 16:27:04.792861: Training Step 251/354: batchLoss = 0.5515, diffLoss = 2.7070, kgLoss = 0.0126
2025-04-08 16:27:06.419616: Training Step 252/354: batchLoss = 0.6514, diffLoss = 3.1896, kgLoss = 0.0168
2025-04-08 16:27:08.041978: Training Step 253/354: batchLoss = 0.6064, diffLoss = 2.9716, kgLoss = 0.0151
2025-04-08 16:27:09.656383: Training Step 254/354: batchLoss = 0.5427, diffLoss = 2.6600, kgLoss = 0.0133
2025-04-08 16:27:11.281195: Training Step 255/354: batchLoss = 0.5936, diffLoss = 2.9008, kgLoss = 0.0168
2025-04-08 16:27:12.914289: Training Step 256/354: batchLoss = 0.5279, diffLoss = 2.5886, kgLoss = 0.0127
2025-04-08 16:27:14.538360: Training Step 257/354: batchLoss = 0.8141, diffLoss = 4.0001, kgLoss = 0.0176
2025-04-08 16:27:16.165740: Training Step 258/354: batchLoss = 0.5248, diffLoss = 2.5733, kgLoss = 0.0126
2025-04-08 16:27:17.793659: Training Step 259/354: batchLoss = 0.5244, diffLoss = 2.5717, kgLoss = 0.0126
2025-04-08 16:27:19.425361: Training Step 260/354: batchLoss = 0.5384, diffLoss = 2.6292, kgLoss = 0.0157
2025-04-08 16:27:21.048451: Training Step 261/354: batchLoss = 0.5959, diffLoss = 2.9161, kgLoss = 0.0158
2025-04-08 16:27:22.667396: Training Step 262/354: batchLoss = 0.5888, diffLoss = 2.8857, kgLoss = 0.0146
2025-04-08 16:27:24.287092: Training Step 263/354: batchLoss = 0.6445, diffLoss = 3.1520, kgLoss = 0.0176
2025-04-08 16:27:25.905581: Training Step 264/354: batchLoss = 0.5014, diffLoss = 2.4517, kgLoss = 0.0138
2025-04-08 16:27:27.531360: Training Step 265/354: batchLoss = 0.6195, diffLoss = 3.0377, kgLoss = 0.0150
2025-04-08 16:27:29.153652: Training Step 266/354: batchLoss = 0.4998, diffLoss = 2.4456, kgLoss = 0.0133
2025-04-08 16:27:30.778600: Training Step 267/354: batchLoss = 0.4548, diffLoss = 2.2298, kgLoss = 0.0111
2025-04-08 16:27:32.410437: Training Step 268/354: batchLoss = 0.5412, diffLoss = 2.6488, kgLoss = 0.0143
2025-04-08 16:27:34.042145: Training Step 269/354: batchLoss = 0.6117, diffLoss = 2.9978, kgLoss = 0.0151
2025-04-08 16:27:35.670460: Training Step 270/354: batchLoss = 0.6376, diffLoss = 3.1214, kgLoss = 0.0166
2025-04-08 16:27:37.287550: Training Step 271/354: batchLoss = 0.4251, diffLoss = 2.0800, kgLoss = 0.0114
2025-04-08 16:27:38.907190: Training Step 272/354: batchLoss = 0.5504, diffLoss = 2.6965, kgLoss = 0.0138
2025-04-08 16:27:40.528274: Training Step 273/354: batchLoss = 0.7887, diffLoss = 3.8682, kgLoss = 0.0188
2025-04-08 16:27:42.150819: Training Step 274/354: batchLoss = 0.6348, diffLoss = 3.1130, kgLoss = 0.0153
2025-04-08 16:27:43.771962: Training Step 275/354: batchLoss = 0.5988, diffLoss = 2.9414, kgLoss = 0.0131
2025-04-08 16:27:45.400168: Training Step 276/354: batchLoss = 0.4843, diffLoss = 2.3665, kgLoss = 0.0138
2025-04-08 16:27:47.030829: Training Step 277/354: batchLoss = 0.5446, diffLoss = 2.6653, kgLoss = 0.0144
2025-04-08 16:27:48.660521: Training Step 278/354: batchLoss = 0.9649, diffLoss = 4.7332, kgLoss = 0.0228
2025-04-08 16:27:50.293709: Training Step 279/354: batchLoss = 0.6480, diffLoss = 3.1724, kgLoss = 0.0169
2025-04-08 16:27:51.925352: Training Step 280/354: batchLoss = 0.5507, diffLoss = 2.6988, kgLoss = 0.0137
2025-04-08 16:27:53.548019: Training Step 281/354: batchLoss = 0.6481, diffLoss = 3.1781, kgLoss = 0.0156
2025-04-08 16:27:55.176807: Training Step 282/354: batchLoss = 0.6550, diffLoss = 3.2119, kgLoss = 0.0158
2025-04-08 16:27:56.796980: Training Step 283/354: batchLoss = 0.7940, diffLoss = 3.8833, kgLoss = 0.0217
2025-04-08 16:27:58.425917: Training Step 284/354: batchLoss = 0.5788, diffLoss = 2.8355, kgLoss = 0.0146
2025-04-08 16:28:00.061513: Training Step 285/354: batchLoss = 0.5720, diffLoss = 2.8006, kgLoss = 0.0148
2025-04-08 16:28:01.699062: Training Step 286/354: batchLoss = 0.4642, diffLoss = 2.2698, kgLoss = 0.0128
2025-04-08 16:28:03.332940: Training Step 287/354: batchLoss = 0.5451, diffLoss = 2.6722, kgLoss = 0.0133
2025-04-08 16:28:04.957231: Training Step 288/354: batchLoss = 0.4948, diffLoss = 2.4207, kgLoss = 0.0133
2025-04-08 16:28:06.593181: Training Step 289/354: batchLoss = 0.5898, diffLoss = 2.8482, kgLoss = 0.0252
2025-04-08 16:28:08.220098: Training Step 290/354: batchLoss = 0.5803, diffLoss = 2.8428, kgLoss = 0.0146
2025-04-08 16:28:09.840755: Training Step 291/354: batchLoss = 1.9212, diffLoss = 9.4510, kgLoss = 0.0387
2025-04-08 16:28:11.461637: Training Step 292/354: batchLoss = 0.5566, diffLoss = 2.7267, kgLoss = 0.0140
2025-04-08 16:28:13.083193: Training Step 293/354: batchLoss = 0.6120, diffLoss = 3.0028, kgLoss = 0.0143
2025-04-08 16:28:14.708188: Training Step 294/354: batchLoss = 0.5764, diffLoss = 2.8201, kgLoss = 0.0155
2025-04-08 16:28:16.335193: Training Step 295/354: batchLoss = 0.7190, diffLoss = 3.5263, kgLoss = 0.0171
2025-04-08 16:28:17.959273: Training Step 296/354: batchLoss = 0.5148, diffLoss = 2.5183, kgLoss = 0.0139
2025-04-08 16:28:19.590740: Training Step 297/354: batchLoss = 0.5187, diffLoss = 2.5436, kgLoss = 0.0125
2025-04-08 16:28:21.219191: Training Step 298/354: batchLoss = 0.6020, diffLoss = 2.9460, kgLoss = 0.0160
2025-04-08 16:28:22.848291: Training Step 299/354: batchLoss = 0.6466, diffLoss = 3.1726, kgLoss = 0.0151
2025-04-08 16:28:24.469051: Training Step 300/354: batchLoss = 0.5022, diffLoss = 2.4594, kgLoss = 0.0129
2025-04-08 16:28:26.080760: Training Step 301/354: batchLoss = 0.5212, diffLoss = 2.5493, kgLoss = 0.0142
2025-04-08 16:28:27.697410: Training Step 302/354: batchLoss = 0.4815, diffLoss = 2.3506, kgLoss = 0.0143
2025-04-08 16:28:29.310250: Training Step 303/354: batchLoss = 0.7025, diffLoss = 3.4346, kgLoss = 0.0195
2025-04-08 16:28:30.926780: Training Step 304/354: batchLoss = 0.5152, diffLoss = 2.5254, kgLoss = 0.0126
2025-04-08 16:28:32.558847: Training Step 305/354: batchLoss = 0.5661, diffLoss = 2.7669, kgLoss = 0.0159
2025-04-08 16:28:34.185407: Training Step 306/354: batchLoss = 0.7323, diffLoss = 3.5966, kgLoss = 0.0162
2025-04-08 16:28:35.806805: Training Step 307/354: batchLoss = 0.5037, diffLoss = 2.4662, kgLoss = 0.0131
2025-04-08 16:28:37.429113: Training Step 308/354: batchLoss = 0.4751, diffLoss = 2.3178, kgLoss = 0.0145
2025-04-08 16:28:39.051154: Training Step 309/354: batchLoss = 0.5069, diffLoss = 2.4830, kgLoss = 0.0129
2025-04-08 16:28:40.673511: Training Step 310/354: batchLoss = 0.5445, diffLoss = 2.6726, kgLoss = 0.0125
2025-04-08 16:28:42.289446: Training Step 311/354: batchLoss = 0.5009, diffLoss = 2.4508, kgLoss = 0.0134
2025-04-08 16:28:43.902467: Training Step 312/354: batchLoss = 0.4920, diffLoss = 2.4153, kgLoss = 0.0111
2025-04-08 16:28:45.518913: Training Step 313/354: batchLoss = 0.7506, diffLoss = 3.6872, kgLoss = 0.0165
2025-04-08 16:28:47.140365: Training Step 314/354: batchLoss = 0.4765, diffLoss = 2.3337, kgLoss = 0.0122
2025-04-08 16:28:48.766015: Training Step 315/354: batchLoss = 0.4570, diffLoss = 2.2330, kgLoss = 0.0129
2025-04-08 16:28:50.402209: Training Step 316/354: batchLoss = 0.5445, diffLoss = 2.6689, kgLoss = 0.0134
2025-04-08 16:28:52.032416: Training Step 317/354: batchLoss = 0.7562, diffLoss = 3.7070, kgLoss = 0.0185
2025-04-08 16:28:53.650893: Training Step 318/354: batchLoss = 0.5509, diffLoss = 2.6971, kgLoss = 0.0143
2025-04-08 16:28:55.278015: Training Step 319/354: batchLoss = 0.6265, diffLoss = 3.0603, kgLoss = 0.0180
2025-04-08 16:28:56.895378: Training Step 320/354: batchLoss = 0.5954, diffLoss = 2.9157, kgLoss = 0.0153
2025-04-08 16:28:58.515190: Training Step 321/354: batchLoss = 0.5405, diffLoss = 2.6501, kgLoss = 0.0131
2025-04-08 16:29:00.128298: Training Step 322/354: batchLoss = 0.4809, diffLoss = 2.3539, kgLoss = 0.0127
2025-04-08 16:29:01.748052: Training Step 323/354: batchLoss = 0.5252, diffLoss = 2.5712, kgLoss = 0.0137
2025-04-08 16:29:03.387246: Training Step 324/354: batchLoss = 0.7011, diffLoss = 3.4406, kgLoss = 0.0162
2025-04-08 16:29:05.012769: Training Step 325/354: batchLoss = 0.6055, diffLoss = 2.9593, kgLoss = 0.0171
2025-04-08 16:29:06.639431: Training Step 326/354: batchLoss = 0.5868, diffLoss = 2.8748, kgLoss = 0.0148
2025-04-08 16:29:08.268632: Training Step 327/354: batchLoss = 0.4996, diffLoss = 2.4432, kgLoss = 0.0137
2025-04-08 16:29:09.895744: Training Step 328/354: batchLoss = 0.5739, diffLoss = 2.8113, kgLoss = 0.0145
2025-04-08 16:29:11.515751: Training Step 329/354: batchLoss = 0.4723, diffLoss = 2.3168, kgLoss = 0.0112
2025-04-08 16:29:13.141441: Training Step 330/354: batchLoss = 0.5075, diffLoss = 2.4847, kgLoss = 0.0132
2025-04-08 16:29:14.751671: Training Step 331/354: batchLoss = 0.5699, diffLoss = 2.7896, kgLoss = 0.0149
2025-04-08 16:29:16.382576: Training Step 332/354: batchLoss = 0.5883, diffLoss = 2.8804, kgLoss = 0.0153
2025-04-08 16:29:18.005073: Training Step 333/354: batchLoss = 0.4943, diffLoss = 2.4177, kgLoss = 0.0135
2025-04-08 16:29:19.632058: Training Step 334/354: batchLoss = 0.5061, diffLoss = 2.4704, kgLoss = 0.0150
2025-04-08 16:29:21.256436: Training Step 335/354: batchLoss = 0.5624, diffLoss = 2.7554, kgLoss = 0.0142
2025-04-08 16:29:22.881840: Training Step 336/354: batchLoss = 0.4833, diffLoss = 2.3670, kgLoss = 0.0123
2025-04-08 16:29:24.505641: Training Step 337/354: batchLoss = 0.6088, diffLoss = 2.9857, kgLoss = 0.0146
2025-04-08 16:29:26.133858: Training Step 338/354: batchLoss = 0.6167, diffLoss = 3.0165, kgLoss = 0.0167
2025-04-08 16:29:27.756150: Training Step 339/354: batchLoss = 0.5931, diffLoss = 2.9089, kgLoss = 0.0141
2025-04-08 16:29:29.377347: Training Step 340/354: batchLoss = 0.5535, diffLoss = 2.7082, kgLoss = 0.0149
2025-04-08 16:29:30.984087: Training Step 341/354: batchLoss = 0.5418, diffLoss = 2.6559, kgLoss = 0.0133
2025-04-08 16:29:32.606815: Training Step 342/354: batchLoss = 0.6815, diffLoss = 3.3402, kgLoss = 0.0168
2025-04-08 16:29:34.220272: Training Step 343/354: batchLoss = 0.5238, diffLoss = 2.5614, kgLoss = 0.0144
2025-04-08 16:29:35.842316: Training Step 344/354: batchLoss = 0.6027, diffLoss = 2.9425, kgLoss = 0.0177
2025-04-08 16:29:37.467525: Training Step 345/354: batchLoss = 0.5325, diffLoss = 2.6033, kgLoss = 0.0148
2025-04-08 16:29:39.095909: Training Step 346/354: batchLoss = 0.6406, diffLoss = 3.1380, kgLoss = 0.0162
2025-04-08 16:29:40.723613: Training Step 347/354: batchLoss = 0.5368, diffLoss = 2.6275, kgLoss = 0.0141
2025-04-08 16:29:42.354679: Training Step 348/354: batchLoss = 0.5646, diffLoss = 2.7623, kgLoss = 0.0152
2025-04-08 16:29:43.982153: Training Step 349/354: batchLoss = 0.7759, diffLoss = 3.8053, kgLoss = 0.0185
2025-04-08 16:29:45.606618: Training Step 350/354: batchLoss = 0.4881, diffLoss = 2.3934, kgLoss = 0.0117
2025-04-08 16:29:47.219803: Training Step 351/354: batchLoss = 0.5157, diffLoss = 2.5305, kgLoss = 0.0120
2025-04-08 16:29:48.816226: Training Step 352/354: batchLoss = 0.5070, diffLoss = 2.4853, kgLoss = 0.0125
2025-04-08 16:29:50.231734: Training Step 353/354: batchLoss = 0.6897, diffLoss = 3.3825, kgLoss = 0.0165
2025-04-08 16:29:50.322712: 
2025-04-08 16:29:50.323332: Epoch 18/1000, Train: epLoss = 1.0180, epDfLoss = 4.9861, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 16:29:51.654308: Steps 0/138: batch_recall = 46.68, batch_ndcg = 26.60 
2025-04-08 16:29:53.008673: Steps 1/138: batch_recall = 46.07, batch_ndcg = 27.75 
2025-04-08 16:29:54.342567: Steps 2/138: batch_recall = 59.03, batch_ndcg = 36.62 
2025-04-08 16:29:55.668965: Steps 3/138: batch_recall = 60.63, batch_ndcg = 34.90 
2025-04-08 16:29:56.975449: Steps 4/138: batch_recall = 69.60, batch_ndcg = 41.23 
2025-04-08 16:29:58.297231: Steps 5/138: batch_recall = 60.48, batch_ndcg = 32.87 
2025-04-08 16:29:59.617669: Steps 6/138: batch_recall = 52.29, batch_ndcg = 31.20 
2025-04-08 16:30:00.922027: Steps 7/138: batch_recall = 64.83, batch_ndcg = 41.84 
2025-04-08 16:30:02.231305: Steps 8/138: batch_recall = 64.35, batch_ndcg = 40.59 
2025-04-08 16:30:03.530359: Steps 9/138: batch_recall = 57.50, batch_ndcg = 33.91 
2025-04-08 16:30:04.837377: Steps 10/138: batch_recall = 55.91, batch_ndcg = 31.32 
2025-04-08 16:30:06.132013: Steps 11/138: batch_recall = 57.96, batch_ndcg = 33.58 
2025-04-08 16:30:07.434151: Steps 12/138: batch_recall = 53.25, batch_ndcg = 29.43 
2025-04-08 16:30:08.740365: Steps 13/138: batch_recall = 52.62, batch_ndcg = 30.83 
2025-04-08 16:30:10.040430: Steps 14/138: batch_recall = 53.71, batch_ndcg = 31.45 
2025-04-08 16:30:11.340826: Steps 15/138: batch_recall = 50.66, batch_ndcg = 30.22 
2025-04-08 16:30:12.639716: Steps 16/138: batch_recall = 59.27, batch_ndcg = 33.26 
2025-04-08 16:30:13.924544: Steps 17/138: batch_recall = 57.90, batch_ndcg = 33.29 
2025-04-08 16:30:15.222481: Steps 18/138: batch_recall = 51.08, batch_ndcg = 32.33 
2025-04-08 16:30:16.530565: Steps 19/138: batch_recall = 56.56, batch_ndcg = 33.03 
2025-04-08 16:30:17.820399: Steps 20/138: batch_recall = 61.69, batch_ndcg = 36.53 
2025-04-08 16:30:19.134049: Steps 21/138: batch_recall = 68.39, batch_ndcg = 40.10 
2025-04-08 16:30:20.447147: Steps 22/138: batch_recall = 57.93, batch_ndcg = 33.31 
2025-04-08 16:30:21.760315: Steps 23/138: batch_recall = 51.24, batch_ndcg = 30.37 
2025-04-08 16:30:23.062695: Steps 24/138: batch_recall = 57.61, batch_ndcg = 31.31 
2025-04-08 16:30:24.372955: Steps 25/138: batch_recall = 61.28, batch_ndcg = 35.94 
2025-04-08 16:30:25.677612: Steps 26/138: batch_recall = 56.26, batch_ndcg = 32.95 
2025-04-08 16:30:26.988288: Steps 27/138: batch_recall = 62.44, batch_ndcg = 35.11 
2025-04-08 16:30:28.296881: Steps 28/138: batch_recall = 60.20, batch_ndcg = 33.70 
2025-04-08 16:30:29.593191: Steps 29/138: batch_recall = 60.78, batch_ndcg = 32.15 
2025-04-08 16:30:30.891035: Steps 30/138: batch_recall = 57.86, batch_ndcg = 34.38 
2025-04-08 16:30:32.196672: Steps 31/138: batch_recall = 41.68, batch_ndcg = 24.27 
2025-04-08 16:30:33.479893: Steps 32/138: batch_recall = 53.33, batch_ndcg = 31.34 
2025-04-08 16:30:34.779931: Steps 33/138: batch_recall = 60.97, batch_ndcg = 34.21 
2025-04-08 16:30:36.055730: Steps 34/138: batch_recall = 52.01, batch_ndcg = 27.81 
2025-04-08 16:30:37.363778: Steps 35/138: batch_recall = 52.89, batch_ndcg = 30.64 
2025-04-08 16:30:38.654782: Steps 36/138: batch_recall = 48.83, batch_ndcg = 28.10 
2025-04-08 16:30:39.946409: Steps 37/138: batch_recall = 60.14, batch_ndcg = 34.70 
2025-04-08 16:30:41.246923: Steps 38/138: batch_recall = 57.68, batch_ndcg = 32.64 
2025-04-08 16:30:42.546340: Steps 39/138: batch_recall = 66.29, batch_ndcg = 38.32 
2025-04-08 16:30:43.849978: Steps 40/138: batch_recall = 59.12, batch_ndcg = 30.60 
2025-04-08 16:30:45.128402: Steps 41/138: batch_recall = 60.64, batch_ndcg = 33.70 
2025-04-08 16:30:46.431453: Steps 42/138: batch_recall = 55.19, batch_ndcg = 30.04 
2025-04-08 16:30:47.728390: Steps 43/138: batch_recall = 55.57, batch_ndcg = 34.08 
2025-04-08 16:30:49.025128: Steps 44/138: batch_recall = 55.99, batch_ndcg = 30.10 
2025-04-08 16:30:50.321516: Steps 45/138: batch_recall = 62.09, batch_ndcg = 35.36 
2025-04-08 16:30:51.604894: Steps 46/138: batch_recall = 60.75, batch_ndcg = 35.38 
2025-04-08 16:30:52.898070: Steps 47/138: batch_recall = 53.23, batch_ndcg = 31.93 
2025-04-08 16:30:54.183550: Steps 48/138: batch_recall = 58.58, batch_ndcg = 34.37 
2025-04-08 16:30:55.475298: Steps 49/138: batch_recall = 65.91, batch_ndcg = 37.52 
2025-04-08 16:30:56.769725: Steps 50/138: batch_recall = 59.97, batch_ndcg = 31.75 
2025-04-08 16:30:58.053913: Steps 51/138: batch_recall = 58.44, batch_ndcg = 34.65 
2025-04-08 16:30:59.352404: Steps 52/138: batch_recall = 67.52, batch_ndcg = 43.01 
2025-04-08 16:31:00.647169: Steps 53/138: batch_recall = 70.10, batch_ndcg = 35.75 
2025-04-08 16:31:01.943258: Steps 54/138: batch_recall = 65.93, batch_ndcg = 38.49 
2025-04-08 16:31:03.237470: Steps 55/138: batch_recall = 63.86, batch_ndcg = 34.18 
2025-04-08 16:31:04.534238: Steps 56/138: batch_recall = 62.45, batch_ndcg = 34.66 
2025-04-08 16:31:05.825297: Steps 57/138: batch_recall = 60.02, batch_ndcg = 33.28 
2025-04-08 16:31:07.107531: Steps 58/138: batch_recall = 69.11, batch_ndcg = 37.64 
2025-04-08 16:31:08.391252: Steps 59/138: batch_recall = 71.76, batch_ndcg = 42.05 
2025-04-08 16:31:09.678319: Steps 60/138: batch_recall = 69.78, batch_ndcg = 37.41 
2025-04-08 16:31:10.966165: Steps 61/138: batch_recall = 63.44, batch_ndcg = 35.54 
2025-04-08 16:31:12.252530: Steps 62/138: batch_recall = 86.08, batch_ndcg = 44.09 
2025-04-08 16:31:13.543461: Steps 63/138: batch_recall = 72.86, batch_ndcg = 42.17 
2025-04-08 16:31:14.840028: Steps 64/138: batch_recall = 65.13, batch_ndcg = 34.13 
2025-04-08 16:31:16.122174: Steps 65/138: batch_recall = 87.33, batch_ndcg = 47.81 
2025-04-08 16:31:17.414065: Steps 66/138: batch_recall = 68.84, batch_ndcg = 40.07 
2025-04-08 16:31:18.715046: Steps 67/138: batch_recall = 78.93, batch_ndcg = 48.02 
2025-04-08 16:31:20.010503: Steps 68/138: batch_recall = 63.19, batch_ndcg = 33.89 
2025-04-08 16:31:21.301758: Steps 69/138: batch_recall = 88.42, batch_ndcg = 51.37 
2025-04-08 16:31:22.585231: Steps 70/138: batch_recall = 79.45, batch_ndcg = 46.16 
2025-04-08 16:31:23.865066: Steps 71/138: batch_recall = 92.23, batch_ndcg = 51.74 
2025-04-08 16:31:25.147798: Steps 72/138: batch_recall = 85.17, batch_ndcg = 48.88 
2025-04-08 16:31:26.434644: Steps 73/138: batch_recall = 86.79, batch_ndcg = 48.61 
2025-04-08 16:31:27.724855: Steps 74/138: batch_recall = 81.88, batch_ndcg = 48.78 
2025-04-08 16:31:29.022883: Steps 75/138: batch_recall = 87.12, batch_ndcg = 50.25 
2025-04-08 16:31:30.315695: Steps 76/138: batch_recall = 97.42, batch_ndcg = 55.41 
2025-04-08 16:31:31.603152: Steps 77/138: batch_recall = 89.84, batch_ndcg = 50.23 
2025-04-08 16:31:32.895762: Steps 78/138: batch_recall = 89.00, batch_ndcg = 47.51 
2025-04-08 16:31:34.192716: Steps 79/138: batch_recall = 93.24, batch_ndcg = 49.83 
2025-04-08 16:31:35.479190: Steps 80/138: batch_recall = 71.33, batch_ndcg = 38.17 
2025-04-08 16:31:36.773988: Steps 81/138: batch_recall = 78.77, batch_ndcg = 47.31 
2025-04-08 16:31:38.061456: Steps 82/138: batch_recall = 88.95, batch_ndcg = 51.54 
2025-04-08 16:31:39.354905: Steps 83/138: batch_recall = 80.86, batch_ndcg = 47.63 
2025-04-08 16:31:40.637755: Steps 84/138: batch_recall = 100.44, batch_ndcg = 56.90 
2025-04-08 16:31:41.929253: Steps 85/138: batch_recall = 104.49, batch_ndcg = 60.44 
2025-04-08 16:31:43.216562: Steps 86/138: batch_recall = 118.26, batch_ndcg = 70.11 
2025-04-08 16:31:44.507480: Steps 87/138: batch_recall = 106.25, batch_ndcg = 55.80 
2025-04-08 16:31:45.792352: Steps 88/138: batch_recall = 99.41, batch_ndcg = 56.35 
2025-04-08 16:31:47.077189: Steps 89/138: batch_recall = 116.22, batch_ndcg = 67.02 
2025-04-08 16:31:48.359952: Steps 90/138: batch_recall = 101.63, batch_ndcg = 58.27 
2025-04-08 16:31:49.645158: Steps 91/138: batch_recall = 118.31, batch_ndcg = 66.07 
2025-04-08 16:31:50.935764: Steps 92/138: batch_recall = 117.73, batch_ndcg = 63.72 
2025-04-08 16:31:52.220246: Steps 93/138: batch_recall = 120.60, batch_ndcg = 69.05 
2025-04-08 16:31:53.494624: Steps 94/138: batch_recall = 123.14, batch_ndcg = 64.44 
2025-04-08 16:31:54.767490: Steps 95/138: batch_recall = 114.77, batch_ndcg = 68.19 
2025-04-08 16:31:56.046616: Steps 96/138: batch_recall = 131.42, batch_ndcg = 75.70 
2025-04-08 16:31:57.317782: Steps 97/138: batch_recall = 145.05, batch_ndcg = 84.69 
2025-04-08 16:31:58.594678: Steps 98/138: batch_recall = 108.35, batch_ndcg = 62.51 
2025-04-08 16:31:59.872167: Steps 99/138: batch_recall = 125.97, batch_ndcg = 71.27 
2025-04-08 16:32:01.146460: Steps 100/138: batch_recall = 132.09, batch_ndcg = 73.49 
2025-04-08 16:32:02.410160: Steps 101/138: batch_recall = 125.42, batch_ndcg = 69.35 
2025-04-08 16:32:03.700327: Steps 102/138: batch_recall = 125.93, batch_ndcg = 73.91 
2025-04-08 16:32:04.971963: Steps 103/138: batch_recall = 142.18, batch_ndcg = 80.25 
2025-04-08 16:32:06.246504: Steps 104/138: batch_recall = 134.75, batch_ndcg = 77.41 
2025-04-08 16:32:07.531666: Steps 105/138: batch_recall = 116.20, batch_ndcg = 66.30 
2025-04-08 16:32:08.831258: Steps 106/138: batch_recall = 105.01, batch_ndcg = 59.24 
2025-04-08 16:32:10.114985: Steps 107/138: batch_recall = 114.45, batch_ndcg = 64.14 
2025-04-08 16:32:11.397902: Steps 108/138: batch_recall = 116.86, batch_ndcg = 70.18 
2025-04-08 16:32:12.664744: Steps 109/138: batch_recall = 137.03, batch_ndcg = 76.79 
2025-04-08 16:32:13.930612: Steps 110/138: batch_recall = 125.58, batch_ndcg = 64.99 
2025-04-08 16:32:15.211518: Steps 111/138: batch_recall = 140.92, batch_ndcg = 85.31 
2025-04-08 16:32:16.508425: Steps 112/138: batch_recall = 165.73, batch_ndcg = 91.78 
2025-04-08 16:32:17.788323: Steps 113/138: batch_recall = 128.64, batch_ndcg = 73.14 
2025-04-08 16:32:19.054145: Steps 114/138: batch_recall = 126.31, batch_ndcg = 71.76 
2025-04-08 16:32:20.342262: Steps 115/138: batch_recall = 121.01, batch_ndcg = 63.87 
2025-04-08 16:32:21.644587: Steps 116/138: batch_recall = 127.80, batch_ndcg = 66.92 
2025-04-08 16:32:22.935830: Steps 117/138: batch_recall = 118.68, batch_ndcg = 68.04 
2025-04-08 16:32:24.212312: Steps 118/138: batch_recall = 124.78, batch_ndcg = 70.62 
2025-04-08 16:32:25.480116: Steps 119/138: batch_recall = 136.95, batch_ndcg = 75.17 
2025-04-08 16:32:26.747041: Steps 120/138: batch_recall = 123.66, batch_ndcg = 70.01 
2025-04-08 16:32:28.022129: Steps 121/138: batch_recall = 151.56, batch_ndcg = 79.37 
2025-04-08 16:32:29.294374: Steps 122/138: batch_recall = 149.27, batch_ndcg = 81.60 
2025-04-08 16:32:30.580031: Steps 123/138: batch_recall = 130.42, batch_ndcg = 72.05 
2025-04-08 16:32:31.866091: Steps 124/138: batch_recall = 153.20, batch_ndcg = 93.73 
2025-04-08 16:32:33.143359: Steps 125/138: batch_recall = 131.75, batch_ndcg = 72.46 
2025-04-08 16:32:34.413822: Steps 126/138: batch_recall = 157.80, batch_ndcg = 89.18 
2025-04-08 16:32:35.694632: Steps 127/138: batch_recall = 141.76, batch_ndcg = 80.06 
2025-04-08 16:32:36.983485: Steps 128/138: batch_recall = 129.10, batch_ndcg = 71.24 
2025-04-08 16:32:38.263431: Steps 129/138: batch_recall = 156.49, batch_ndcg = 90.06 
2025-04-08 16:32:39.548925: Steps 130/138: batch_recall = 134.74, batch_ndcg = 70.30 
2025-04-08 16:32:40.824478: Steps 131/138: batch_recall = 151.47, batch_ndcg = 86.96 
2025-04-08 16:32:42.098743: Steps 132/138: batch_recall = 147.16, batch_ndcg = 85.51 
2025-04-08 16:32:43.367183: Steps 133/138: batch_recall = 147.10, batch_ndcg = 84.92 
2025-04-08 16:32:44.636326: Steps 134/138: batch_recall = 142.46, batch_ndcg = 81.54 
2025-04-08 16:32:45.904446: Steps 135/138: batch_recall = 166.03, batch_ndcg = 94.74 
2025-04-08 16:32:47.184166: Steps 136/138: batch_recall = 152.04, batch_ndcg = 78.60 
2025-04-08 16:32:48.457511: Steps 137/138: batch_recall = 140.97, batch_ndcg = 87.43 
2025-04-08 16:32:48.458109: Epoch 18/1000, Test: Recall = 0.1769, NDCG = 0.1003  

2025-04-08 16:32:50.232946: Training Step 0/354: batchLoss = 0.5124, diffLoss = 2.5103, kgLoss = 0.0129
2025-04-08 16:32:51.856868: Training Step 1/354: batchLoss = 0.5945, diffLoss = 2.9105, kgLoss = 0.0155
2025-04-08 16:32:53.486636: Training Step 2/354: batchLoss = 0.5485, diffLoss = 2.6802, kgLoss = 0.0155
2025-04-08 16:32:55.119623: Training Step 3/354: batchLoss = 0.6203, diffLoss = 3.0382, kgLoss = 0.0158
2025-04-08 16:32:56.749974: Training Step 4/354: batchLoss = 0.8035, diffLoss = 3.9356, kgLoss = 0.0204
2025-04-08 16:32:58.367374: Training Step 5/354: batchLoss = 0.5186, diffLoss = 2.5451, kgLoss = 0.0120
2025-04-08 16:32:59.981011: Training Step 6/354: batchLoss = 0.5964, diffLoss = 2.9220, kgLoss = 0.0150
2025-04-08 16:33:01.599632: Training Step 7/354: batchLoss = 0.5351, diffLoss = 2.6231, kgLoss = 0.0131
2025-04-08 16:33:03.228513: Training Step 8/354: batchLoss = 0.6149, diffLoss = 3.0062, kgLoss = 0.0171
2025-04-08 16:33:04.852844: Training Step 9/354: batchLoss = 0.5984, diffLoss = 2.9364, kgLoss = 0.0139
2025-04-08 16:33:06.477267: Training Step 10/354: batchLoss = 0.4154, diffLoss = 2.0249, kgLoss = 0.0130
2025-04-08 16:33:08.098096: Training Step 11/354: batchLoss = 0.6573, diffLoss = 3.2232, kgLoss = 0.0159
2025-04-08 16:33:09.725088: Training Step 12/354: batchLoss = 0.6156, diffLoss = 3.0035, kgLoss = 0.0186
2025-04-08 16:33:11.347464: Training Step 13/354: batchLoss = 0.6097, diffLoss = 2.9851, kgLoss = 0.0159
2025-04-08 16:33:12.969001: Training Step 14/354: batchLoss = 0.4094, diffLoss = 2.0007, kgLoss = 0.0116
2025-04-08 16:33:14.585601: Training Step 15/354: batchLoss = 0.5343, diffLoss = 2.6139, kgLoss = 0.0144
2025-04-08 16:33:16.202336: Training Step 16/354: batchLoss = 0.4561, diffLoss = 2.2255, kgLoss = 0.0138
2025-04-08 16:33:17.824323: Training Step 17/354: batchLoss = 0.5880, diffLoss = 2.8821, kgLoss = 0.0145
2025-04-08 16:33:19.448849: Training Step 18/354: batchLoss = 0.4059, diffLoss = 1.9845, kgLoss = 0.0112
2025-04-08 16:33:21.073527: Training Step 19/354: batchLoss = 0.8922, diffLoss = 4.3722, kgLoss = 0.0222
2025-04-08 16:33:22.697669: Training Step 20/354: batchLoss = 0.4839, diffLoss = 2.3717, kgLoss = 0.0120
2025-04-08 16:33:24.331554: Training Step 21/354: batchLoss = 0.4492, diffLoss = 2.1917, kgLoss = 0.0135
2025-04-08 16:33:25.962434: Training Step 22/354: batchLoss = 0.5599, diffLoss = 2.7411, kgLoss = 0.0146
2025-04-08 16:33:27.594871: Training Step 23/354: batchLoss = 0.4512, diffLoss = 2.1955, kgLoss = 0.0152
2025-04-08 16:33:29.212175: Training Step 24/354: batchLoss = 0.5635, diffLoss = 2.7570, kgLoss = 0.0152
2025-04-08 16:33:30.833754: Training Step 25/354: batchLoss = 0.5324, diffLoss = 2.6068, kgLoss = 0.0138
2025-04-08 16:33:32.447131: Training Step 26/354: batchLoss = 0.6687, diffLoss = 3.2712, kgLoss = 0.0181
2025-04-08 16:33:34.056167: Training Step 27/354: batchLoss = 0.5957, diffLoss = 2.9092, kgLoss = 0.0174
2025-04-08 16:33:35.675288: Training Step 28/354: batchLoss = 0.6877, diffLoss = 3.3725, kgLoss = 0.0165
2025-04-08 16:33:37.301773: Training Step 29/354: batchLoss = 0.5998, diffLoss = 2.9368, kgLoss = 0.0155
2025-04-08 16:33:38.932933: Training Step 30/354: batchLoss = 0.5912, diffLoss = 2.8920, kgLoss = 0.0160
2025-04-08 16:33:40.562283: Training Step 31/354: batchLoss = 0.6785, diffLoss = 3.3215, kgLoss = 0.0178
2025-04-08 16:33:42.197556: Training Step 32/354: batchLoss = 0.6183, diffLoss = 3.0337, kgLoss = 0.0145
2025-04-08 16:33:43.825914: Training Step 33/354: batchLoss = 0.5181, diffLoss = 2.5433, kgLoss = 0.0118
2025-04-08 16:33:45.444038: Training Step 34/354: batchLoss = 0.5579, diffLoss = 2.7319, kgLoss = 0.0144
2025-04-08 16:33:47.061746: Training Step 35/354: batchLoss = 0.5666, diffLoss = 2.7775, kgLoss = 0.0139
2025-04-08 16:33:48.673995: Training Step 36/354: batchLoss = 0.4967, diffLoss = 2.4345, kgLoss = 0.0122
2025-04-08 16:33:50.293023: Training Step 37/354: batchLoss = 0.6146, diffLoss = 3.0145, kgLoss = 0.0146
2025-04-08 16:33:51.909895: Training Step 38/354: batchLoss = 0.7551, diffLoss = 3.7049, kgLoss = 0.0176
2025-04-08 16:33:53.537768: Training Step 39/354: batchLoss = 0.6193, diffLoss = 3.0409, kgLoss = 0.0138
2025-04-08 16:33:55.163431: Training Step 40/354: batchLoss = 0.5198, diffLoss = 2.5480, kgLoss = 0.0128
2025-04-08 16:33:56.803805: Training Step 41/354: batchLoss = 0.5068, diffLoss = 2.4829, kgLoss = 0.0128
2025-04-08 16:33:58.440709: Training Step 42/354: batchLoss = 0.5142, diffLoss = 2.5207, kgLoss = 0.0126
2025-04-08 16:34:00.063985: Training Step 43/354: batchLoss = 0.5436, diffLoss = 2.6598, kgLoss = 0.0146
2025-04-08 16:34:01.684842: Training Step 44/354: batchLoss = 0.5125, diffLoss = 2.5172, kgLoss = 0.0113
2025-04-08 16:34:03.314522: Training Step 45/354: batchLoss = 0.5608, diffLoss = 2.7498, kgLoss = 0.0135
2025-04-08 16:34:04.931818: Training Step 46/354: batchLoss = 0.5627, diffLoss = 2.7584, kgLoss = 0.0138
2025-04-08 16:34:06.541899: Training Step 47/354: batchLoss = 0.4822, diffLoss = 2.3507, kgLoss = 0.0151
2025-04-08 16:34:08.168719: Training Step 48/354: batchLoss = 0.5738, diffLoss = 2.8114, kgLoss = 0.0144
2025-04-08 16:34:09.790618: Training Step 49/354: batchLoss = 0.5819, diffLoss = 2.8515, kgLoss = 0.0145
2025-04-08 16:34:11.419159: Training Step 50/354: batchLoss = 0.5959, diffLoss = 2.9094, kgLoss = 0.0176
2025-04-08 16:34:13.044504: Training Step 51/354: batchLoss = 0.5643, diffLoss = 2.7687, kgLoss = 0.0132
2025-04-08 16:34:14.679313: Training Step 52/354: batchLoss = 0.7013, diffLoss = 3.4353, kgLoss = 0.0178
2025-04-08 16:34:16.309172: Training Step 53/354: batchLoss = 0.6145, diffLoss = 3.0086, kgLoss = 0.0159
2025-04-08 16:34:17.922659: Training Step 54/354: batchLoss = 0.5479, diffLoss = 2.6912, kgLoss = 0.0121
2025-04-08 16:34:19.543865: Training Step 55/354: batchLoss = 0.6922, diffLoss = 3.3913, kgLoss = 0.0174
2025-04-08 16:34:21.158958: Training Step 56/354: batchLoss = 0.5377, diffLoss = 2.6344, kgLoss = 0.0135
2025-04-08 16:34:22.787568: Training Step 57/354: batchLoss = 0.4968, diffLoss = 2.4214, kgLoss = 0.0157
2025-04-08 16:34:24.409929: Training Step 58/354: batchLoss = 1.0577, diffLoss = 5.1885, kgLoss = 0.0250
2025-04-08 16:34:26.039112: Training Step 59/354: batchLoss = 0.5828, diffLoss = 2.8540, kgLoss = 0.0150
2025-04-08 16:34:27.669089: Training Step 60/354: batchLoss = 0.4970, diffLoss = 2.4277, kgLoss = 0.0143
2025-04-08 16:34:29.305701: Training Step 61/354: batchLoss = 0.5731, diffLoss = 2.8083, kgLoss = 0.0143
2025-04-08 16:34:30.929531: Training Step 62/354: batchLoss = 0.6975, diffLoss = 3.4206, kgLoss = 0.0167
2025-04-08 16:34:32.563235: Training Step 63/354: batchLoss = 0.6088, diffLoss = 2.9786, kgLoss = 0.0164
2025-04-08 16:34:34.182416: Training Step 64/354: batchLoss = 0.6593, diffLoss = 3.2339, kgLoss = 0.0156
2025-04-08 16:34:35.802445: Training Step 65/354: batchLoss = 0.4531, diffLoss = 2.2222, kgLoss = 0.0108
2025-04-08 16:34:37.426495: Training Step 66/354: batchLoss = 0.6270, diffLoss = 3.0710, kgLoss = 0.0159
2025-04-08 16:34:39.047022: Training Step 67/354: batchLoss = 0.6677, diffLoss = 3.2667, kgLoss = 0.0179
2025-04-08 16:34:40.677364: Training Step 68/354: batchLoss = 0.5657, diffLoss = 2.7690, kgLoss = 0.0149
2025-04-08 16:34:42.314744: Training Step 69/354: batchLoss = 0.5910, diffLoss = 2.8934, kgLoss = 0.0154
2025-04-08 16:34:43.937587: Training Step 70/354: batchLoss = 0.6388, diffLoss = 3.1275, kgLoss = 0.0166
2025-04-08 16:34:45.559587: Training Step 71/354: batchLoss = 0.5627, diffLoss = 2.7618, kgLoss = 0.0129
2025-04-08 16:34:47.192814: Training Step 72/354: batchLoss = 0.7165, diffLoss = 3.5081, kgLoss = 0.0186
2025-04-08 16:34:48.812455: Training Step 73/354: batchLoss = 0.6612, diffLoss = 3.2435, kgLoss = 0.0156
2025-04-08 16:34:50.436872: Training Step 74/354: batchLoss = 0.5964, diffLoss = 2.9240, kgLoss = 0.0145
2025-04-08 16:34:52.060028: Training Step 75/354: batchLoss = 0.6344, diffLoss = 3.1065, kgLoss = 0.0163
2025-04-08 16:34:53.676284: Training Step 76/354: batchLoss = 0.6558, diffLoss = 3.2070, kgLoss = 0.0181
2025-04-08 16:34:55.295672: Training Step 77/354: batchLoss = 0.6611, diffLoss = 3.2348, kgLoss = 0.0177
2025-04-08 16:34:56.920825: Training Step 78/354: batchLoss = 0.5138, diffLoss = 2.5161, kgLoss = 0.0133
2025-04-08 16:34:58.551454: Training Step 79/354: batchLoss = 0.4518, diffLoss = 2.2109, kgLoss = 0.0120
2025-04-08 16:35:00.180873: Training Step 80/354: batchLoss = 0.5358, diffLoss = 2.6169, kgLoss = 0.0156
2025-04-08 16:35:01.807306: Training Step 81/354: batchLoss = 0.4693, diffLoss = 2.2997, kgLoss = 0.0116
2025-04-08 16:35:03.430963: Training Step 82/354: batchLoss = 0.5619, diffLoss = 2.7559, kgLoss = 0.0134
2025-04-08 16:35:05.051309: Training Step 83/354: batchLoss = 0.6203, diffLoss = 3.0446, kgLoss = 0.0142
2025-04-08 16:35:06.675337: Training Step 84/354: batchLoss = 0.4640, diffLoss = 2.2680, kgLoss = 0.0129
2025-04-08 16:35:08.303746: Training Step 85/354: batchLoss = 0.6528, diffLoss = 3.2023, kgLoss = 0.0154
2025-04-08 16:35:09.924305: Training Step 86/354: batchLoss = 0.6412, diffLoss = 3.1459, kgLoss = 0.0151
2025-04-08 16:35:11.556399: Training Step 87/354: batchLoss = 0.6255, diffLoss = 3.0651, kgLoss = 0.0156
2025-04-08 16:35:13.184518: Training Step 88/354: batchLoss = 0.6240, diffLoss = 3.0585, kgLoss = 0.0154
2025-04-08 16:35:14.819249: Training Step 89/354: batchLoss = 0.4909, diffLoss = 2.4033, kgLoss = 0.0128
2025-04-08 16:35:16.451407: Training Step 90/354: batchLoss = 0.5030, diffLoss = 2.4535, kgLoss = 0.0153
2025-04-08 16:35:18.080304: Training Step 91/354: batchLoss = 0.6183, diffLoss = 3.0266, kgLoss = 0.0162
2025-04-08 16:35:19.709180: Training Step 92/354: batchLoss = 0.4818, diffLoss = 2.3556, kgLoss = 0.0134
2025-04-08 16:35:21.321347: Training Step 93/354: batchLoss = 0.5310, diffLoss = 2.6056, kgLoss = 0.0124
2025-04-08 16:35:22.951282: Training Step 94/354: batchLoss = 0.5267, diffLoss = 2.5748, kgLoss = 0.0147
2025-04-08 16:35:24.575001: Training Step 95/354: batchLoss = 0.4693, diffLoss = 2.2991, kgLoss = 0.0118
2025-04-08 16:35:26.199387: Training Step 96/354: batchLoss = 0.5752, diffLoss = 2.8059, kgLoss = 0.0175
2025-04-08 16:35:27.816259: Training Step 97/354: batchLoss = 0.5543, diffLoss = 2.7159, kgLoss = 0.0139
2025-04-08 16:35:29.449884: Training Step 98/354: batchLoss = 0.5474, diffLoss = 2.6851, kgLoss = 0.0130
2025-04-08 16:35:31.081549: Training Step 99/354: batchLoss = 0.8369, diffLoss = 4.1097, kgLoss = 0.0187
2025-04-08 16:35:32.703937: Training Step 100/354: batchLoss = 0.5026, diffLoss = 2.4582, kgLoss = 0.0137
2025-04-08 16:35:34.325151: Training Step 101/354: batchLoss = 0.5884, diffLoss = 2.8829, kgLoss = 0.0148
2025-04-08 16:35:35.948200: Training Step 102/354: batchLoss = 0.6954, diffLoss = 3.4099, kgLoss = 0.0167
2025-04-08 16:35:37.565930: Training Step 103/354: batchLoss = 0.5531, diffLoss = 2.7096, kgLoss = 0.0140
2025-04-08 16:35:39.184316: Training Step 104/354: batchLoss = 0.5144, diffLoss = 2.5141, kgLoss = 0.0144
2025-04-08 16:35:40.802832: Training Step 105/354: batchLoss = 0.6701, diffLoss = 3.2862, kgLoss = 0.0161
2025-04-08 16:35:42.429796: Training Step 106/354: batchLoss = 0.4300, diffLoss = 2.0921, kgLoss = 0.0145
2025-04-08 16:35:44.050657: Training Step 107/354: batchLoss = 0.4566, diffLoss = 2.2321, kgLoss = 0.0128
2025-04-08 16:35:45.687280: Training Step 108/354: batchLoss = 0.5225, diffLoss = 2.5556, kgLoss = 0.0142
2025-04-08 16:35:47.314708: Training Step 109/354: batchLoss = 0.6054, diffLoss = 2.9659, kgLoss = 0.0153
2025-04-08 16:35:48.937189: Training Step 110/354: batchLoss = 0.5384, diffLoss = 2.6320, kgLoss = 0.0150
2025-04-08 16:35:50.565643: Training Step 111/354: batchLoss = 0.7263, diffLoss = 3.5620, kgLoss = 0.0173
2025-04-08 16:35:52.190029: Training Step 112/354: batchLoss = 0.5001, diffLoss = 2.4468, kgLoss = 0.0135
2025-04-08 16:35:53.814125: Training Step 113/354: batchLoss = 0.6244, diffLoss = 3.0597, kgLoss = 0.0156
2025-04-08 16:35:55.425353: Training Step 114/354: batchLoss = 0.4918, diffLoss = 2.4119, kgLoss = 0.0118
2025-04-08 16:35:57.042847: Training Step 115/354: batchLoss = 0.5990, diffLoss = 2.9383, kgLoss = 0.0142
2025-04-08 16:35:58.659131: Training Step 116/354: batchLoss = 0.5458, diffLoss = 2.6708, kgLoss = 0.0146
2025-04-08 16:36:00.275292: Training Step 117/354: batchLoss = 0.4999, diffLoss = 2.4457, kgLoss = 0.0135
2025-04-08 16:36:01.896899: Training Step 118/354: batchLoss = 0.5731, diffLoss = 2.8101, kgLoss = 0.0138
2025-04-08 16:36:03.522785: Training Step 119/354: batchLoss = 0.6541, diffLoss = 3.2069, kgLoss = 0.0159
2025-04-08 16:36:05.145088: Training Step 120/354: batchLoss = 0.6029, diffLoss = 2.9453, kgLoss = 0.0173
2025-04-08 16:36:06.767617: Training Step 121/354: batchLoss = 0.5578, diffLoss = 2.7257, kgLoss = 0.0158
2025-04-08 16:36:08.395938: Training Step 122/354: batchLoss = 0.6139, diffLoss = 3.0041, kgLoss = 0.0164
2025-04-08 16:36:10.023620: Training Step 123/354: batchLoss = 0.5202, diffLoss = 2.5516, kgLoss = 0.0124
2025-04-08 16:36:11.643658: Training Step 124/354: batchLoss = 0.4977, diffLoss = 2.4372, kgLoss = 0.0128
2025-04-08 16:36:13.254913: Training Step 125/354: batchLoss = 0.4815, diffLoss = 2.3619, kgLoss = 0.0114
2025-04-08 16:36:14.875120: Training Step 126/354: batchLoss = 0.5629, diffLoss = 2.7573, kgLoss = 0.0142
2025-04-08 16:36:16.497005: Training Step 127/354: batchLoss = 0.4888, diffLoss = 2.3937, kgLoss = 0.0125
2025-04-08 16:36:18.127404: Training Step 128/354: batchLoss = 0.5260, diffLoss = 2.5740, kgLoss = 0.0140
2025-04-08 16:36:19.746300: Training Step 129/354: batchLoss = 0.5468, diffLoss = 2.6697, kgLoss = 0.0161
2025-04-08 16:36:21.377844: Training Step 130/354: batchLoss = 0.6958, diffLoss = 3.4057, kgLoss = 0.0183
2025-04-08 16:36:23.009334: Training Step 131/354: batchLoss = 0.6636, diffLoss = 3.2575, kgLoss = 0.0151
2025-04-08 16:36:24.638033: Training Step 132/354: batchLoss = 0.5001, diffLoss = 2.4439, kgLoss = 0.0141
2025-04-08 16:36:26.266161: Training Step 133/354: batchLoss = 0.5752, diffLoss = 2.8229, kgLoss = 0.0133
2025-04-08 16:36:27.886379: Training Step 134/354: batchLoss = 0.4786, diffLoss = 2.3459, kgLoss = 0.0118
2025-04-08 16:36:29.506635: Training Step 135/354: batchLoss = 0.5257, diffLoss = 2.5760, kgLoss = 0.0131
2025-04-08 16:36:31.127218: Training Step 136/354: batchLoss = 0.6075, diffLoss = 2.9769, kgLoss = 0.0152
2025-04-08 16:36:32.750268: Training Step 137/354: batchLoss = 0.6147, diffLoss = 3.0098, kgLoss = 0.0159
2025-04-08 16:36:34.367773: Training Step 138/354: batchLoss = 0.5782, diffLoss = 2.8276, kgLoss = 0.0158
2025-04-08 16:36:35.996899: Training Step 139/354: batchLoss = 0.5194, diffLoss = 2.5483, kgLoss = 0.0122
2025-04-08 16:36:37.617843: Training Step 140/354: batchLoss = 0.5173, diffLoss = 2.5333, kgLoss = 0.0133
2025-04-08 16:36:39.242218: Training Step 141/354: batchLoss = 0.4497, diffLoss = 2.1956, kgLoss = 0.0133
2025-04-08 16:36:40.867146: Training Step 142/354: batchLoss = 0.5113, diffLoss = 2.5053, kgLoss = 0.0128
2025-04-08 16:36:42.485298: Training Step 143/354: batchLoss = 0.6243, diffLoss = 3.0614, kgLoss = 0.0150
2025-04-08 16:36:44.111613: Training Step 144/354: batchLoss = 0.7360, diffLoss = 3.6111, kgLoss = 0.0172
2025-04-08 16:36:45.730346: Training Step 145/354: batchLoss = 0.4764, diffLoss = 2.3318, kgLoss = 0.0126
2025-04-08 16:36:47.350024: Training Step 146/354: batchLoss = 0.8138, diffLoss = 3.9823, kgLoss = 0.0217
2025-04-08 16:36:48.981136: Training Step 147/354: batchLoss = 0.5110, diffLoss = 2.5015, kgLoss = 0.0134
2025-04-08 16:36:50.607728: Training Step 148/354: batchLoss = 0.5124, diffLoss = 2.5037, kgLoss = 0.0145
2025-04-08 16:36:52.236150: Training Step 149/354: batchLoss = 0.5334, diffLoss = 2.6137, kgLoss = 0.0134
2025-04-08 16:36:53.859336: Training Step 150/354: batchLoss = 0.5072, diffLoss = 2.4730, kgLoss = 0.0157
2025-04-08 16:36:55.491327: Training Step 151/354: batchLoss = 0.6100, diffLoss = 2.9911, kgLoss = 0.0148
2025-04-08 16:36:57.127582: Training Step 152/354: batchLoss = 0.6751, diffLoss = 3.3104, kgLoss = 0.0163
2025-04-08 16:36:58.756372: Training Step 153/354: batchLoss = 0.5260, diffLoss = 2.5762, kgLoss = 0.0134
2025-04-08 16:37:00.382768: Training Step 154/354: batchLoss = 0.6052, diffLoss = 2.9651, kgLoss = 0.0153
2025-04-08 16:37:02.001018: Training Step 155/354: batchLoss = 0.6156, diffLoss = 3.0164, kgLoss = 0.0155
2025-04-08 16:37:03.622630: Training Step 156/354: batchLoss = 0.6906, diffLoss = 3.3834, kgLoss = 0.0175
2025-04-08 16:37:05.251617: Training Step 157/354: batchLoss = 0.5892, diffLoss = 2.8884, kgLoss = 0.0144
2025-04-08 16:37:06.888835: Training Step 158/354: batchLoss = 0.5596, diffLoss = 2.7363, kgLoss = 0.0154
2025-04-08 16:37:08.510450: Training Step 159/354: batchLoss = 0.5492, diffLoss = 2.6825, kgLoss = 0.0158
2025-04-08 16:37:10.140452: Training Step 160/354: batchLoss = 0.5137, diffLoss = 2.5187, kgLoss = 0.0125
2025-04-08 16:37:11.764516: Training Step 161/354: batchLoss = 0.5591, diffLoss = 2.7270, kgLoss = 0.0171
2025-04-08 16:37:13.387850: Training Step 162/354: batchLoss = 0.6450, diffLoss = 3.1622, kgLoss = 0.0157
2025-04-08 16:37:15.007242: Training Step 163/354: batchLoss = 0.5381, diffLoss = 2.6372, kgLoss = 0.0133
2025-04-08 16:37:16.621092: Training Step 164/354: batchLoss = 0.6425, diffLoss = 3.1431, kgLoss = 0.0173
2025-04-08 16:37:18.226923: Training Step 165/354: batchLoss = 0.5168, diffLoss = 2.5299, kgLoss = 0.0136
2025-04-08 16:37:19.837061: Training Step 166/354: batchLoss = 0.5246, diffLoss = 2.5705, kgLoss = 0.0131
2025-04-08 16:37:21.464797: Training Step 167/354: batchLoss = 0.5098, diffLoss = 2.5012, kgLoss = 0.0120
2025-04-08 16:37:23.092492: Training Step 168/354: batchLoss = 0.5649, diffLoss = 2.7626, kgLoss = 0.0155
2025-04-08 16:37:24.717712: Training Step 169/354: batchLoss = 0.6693, diffLoss = 3.2760, kgLoss = 0.0177
2025-04-08 16:37:26.340758: Training Step 170/354: batchLoss = 0.4592, diffLoss = 2.2441, kgLoss = 0.0129
2025-04-08 16:37:27.965308: Training Step 171/354: batchLoss = 0.5630, diffLoss = 2.7541, kgLoss = 0.0152
2025-04-08 16:37:29.600245: Training Step 172/354: batchLoss = 0.5718, diffLoss = 2.7941, kgLoss = 0.0163
2025-04-08 16:37:31.228972: Training Step 173/354: batchLoss = 0.6475, diffLoss = 3.1761, kgLoss = 0.0153
2025-04-08 16:37:32.839148: Training Step 174/354: batchLoss = 0.5918, diffLoss = 2.8992, kgLoss = 0.0150
2025-04-08 16:37:34.445408: Training Step 175/354: batchLoss = 0.5840, diffLoss = 2.8607, kgLoss = 0.0148
2025-04-08 16:37:36.061060: Training Step 176/354: batchLoss = 0.6351, diffLoss = 3.1092, kgLoss = 0.0166
2025-04-08 16:37:37.673489: Training Step 177/354: batchLoss = 0.5394, diffLoss = 2.6308, kgLoss = 0.0166
2025-04-08 16:37:39.296322: Training Step 178/354: batchLoss = 0.5375, diffLoss = 2.6275, kgLoss = 0.0150
2025-04-08 16:37:40.919127: Training Step 179/354: batchLoss = 0.5699, diffLoss = 2.7915, kgLoss = 0.0145
2025-04-08 16:37:42.541253: Training Step 180/354: batchLoss = 0.5183, diffLoss = 2.5340, kgLoss = 0.0144
2025-04-08 16:37:44.166782: Training Step 181/354: batchLoss = 0.5096, diffLoss = 2.4950, kgLoss = 0.0132
2025-04-08 16:37:45.793406: Training Step 182/354: batchLoss = 0.5734, diffLoss = 2.8015, kgLoss = 0.0163
2025-04-08 16:37:47.421672: Training Step 183/354: batchLoss = 0.5945, diffLoss = 2.9207, kgLoss = 0.0130
2025-04-08 16:37:49.043045: Training Step 184/354: batchLoss = 0.5142, diffLoss = 2.5206, kgLoss = 0.0126
2025-04-08 16:37:50.661460: Training Step 185/354: batchLoss = 0.5927, diffLoss = 2.9035, kgLoss = 0.0150
2025-04-08 16:37:52.282546: Training Step 186/354: batchLoss = 0.5034, diffLoss = 2.4702, kgLoss = 0.0117
2025-04-08 16:37:53.898848: Training Step 187/354: batchLoss = 0.5524, diffLoss = 2.7082, kgLoss = 0.0134
2025-04-08 16:37:55.520019: Training Step 188/354: batchLoss = 0.6386, diffLoss = 3.1295, kgLoss = 0.0159
2025-04-08 16:37:57.144841: Training Step 189/354: batchLoss = 0.5600, diffLoss = 2.7442, kgLoss = 0.0139
2025-04-08 16:37:58.770448: Training Step 190/354: batchLoss = 0.5624, diffLoss = 2.7590, kgLoss = 0.0133
2025-04-08 16:38:00.400433: Training Step 191/354: batchLoss = 0.5032, diffLoss = 2.4662, kgLoss = 0.0125
2025-04-08 16:38:02.021300: Training Step 192/354: batchLoss = 0.7030, diffLoss = 3.4498, kgLoss = 0.0163
2025-04-08 16:38:03.641252: Training Step 193/354: batchLoss = 0.5494, diffLoss = 2.6958, kgLoss = 0.0128
2025-04-08 16:38:05.271729: Training Step 194/354: batchLoss = 0.5652, diffLoss = 2.7683, kgLoss = 0.0144
2025-04-08 16:38:06.890460: Training Step 195/354: batchLoss = 0.4876, diffLoss = 2.3852, kgLoss = 0.0131
2025-04-08 16:38:08.513267: Training Step 196/354: batchLoss = 0.7140, diffLoss = 3.4856, kgLoss = 0.0211
2025-04-08 16:38:10.131421: Training Step 197/354: batchLoss = 0.6456, diffLoss = 3.1584, kgLoss = 0.0173
2025-04-08 16:38:11.768389: Training Step 198/354: batchLoss = 0.5386, diffLoss = 2.6368, kgLoss = 0.0141
2025-04-08 16:38:13.393922: Training Step 199/354: batchLoss = 0.5398, diffLoss = 2.6400, kgLoss = 0.0148
2025-04-08 16:38:15.017048: Training Step 200/354: batchLoss = 0.5097, diffLoss = 2.4957, kgLoss = 0.0132
2025-04-08 16:38:16.646948: Training Step 201/354: batchLoss = 0.5127, diffLoss = 2.5120, kgLoss = 0.0129
2025-04-08 16:38:18.267949: Training Step 202/354: batchLoss = 0.5693, diffLoss = 2.7911, kgLoss = 0.0139
2025-04-08 16:38:19.891282: Training Step 203/354: batchLoss = 0.5345, diffLoss = 2.6171, kgLoss = 0.0138
2025-04-08 16:38:21.513622: Training Step 204/354: batchLoss = 0.7858, diffLoss = 3.8593, kgLoss = 0.0174
2025-04-08 16:38:23.131093: Training Step 205/354: batchLoss = 0.5750, diffLoss = 2.8212, kgLoss = 0.0135
2025-04-08 16:38:24.751126: Training Step 206/354: batchLoss = 0.6115, diffLoss = 2.9901, kgLoss = 0.0169
2025-04-08 16:38:26.373875: Training Step 207/354: batchLoss = 0.5926, diffLoss = 2.8947, kgLoss = 0.0170
2025-04-08 16:38:27.995175: Training Step 208/354: batchLoss = 0.4715, diffLoss = 2.3045, kgLoss = 0.0133
2025-04-08 16:38:29.623317: Training Step 209/354: batchLoss = 0.4674, diffLoss = 2.2893, kgLoss = 0.0119
2025-04-08 16:38:31.257289: Training Step 210/354: batchLoss = 0.7140, diffLoss = 3.4990, kgLoss = 0.0177
2025-04-08 16:38:32.891303: Training Step 211/354: batchLoss = 0.4995, diffLoss = 2.4441, kgLoss = 0.0134
2025-04-08 16:38:34.519982: Training Step 212/354: batchLoss = 0.5106, diffLoss = 2.5031, kgLoss = 0.0125
2025-04-08 16:38:36.148779: Training Step 213/354: batchLoss = 0.4884, diffLoss = 2.3917, kgLoss = 0.0126
2025-04-08 16:38:37.776247: Training Step 214/354: batchLoss = 0.4819, diffLoss = 2.3581, kgLoss = 0.0129
2025-04-08 16:38:39.394963: Training Step 215/354: batchLoss = 0.4757, diffLoss = 2.3298, kgLoss = 0.0121
2025-04-08 16:38:41.022042: Training Step 216/354: batchLoss = 0.5532, diffLoss = 2.7063, kgLoss = 0.0149
2025-04-08 16:38:42.648033: Training Step 217/354: batchLoss = 0.5453, diffLoss = 2.6618, kgLoss = 0.0162
2025-04-08 16:38:44.269427: Training Step 218/354: batchLoss = 1.1037, diffLoss = 5.4098, kgLoss = 0.0272
2025-04-08 16:38:45.895246: Training Step 219/354: batchLoss = 0.4610, diffLoss = 2.2564, kgLoss = 0.0121
2025-04-08 16:38:47.523639: Training Step 220/354: batchLoss = 0.5354, diffLoss = 2.6158, kgLoss = 0.0154
2025-04-08 16:38:49.155166: Training Step 221/354: batchLoss = 0.5664, diffLoss = 2.7734, kgLoss = 0.0146
2025-04-08 16:38:50.784061: Training Step 222/354: batchLoss = 0.5544, diffLoss = 2.7174, kgLoss = 0.0137
2025-04-08 16:38:52.415767: Training Step 223/354: batchLoss = 0.5997, diffLoss = 2.9330, kgLoss = 0.0164
2025-04-08 16:38:54.043380: Training Step 224/354: batchLoss = 0.5316, diffLoss = 2.6079, kgLoss = 0.0126
2025-04-08 16:38:55.661169: Training Step 225/354: batchLoss = 0.4703, diffLoss = 2.3017, kgLoss = 0.0125
2025-04-08 16:38:57.293172: Training Step 226/354: batchLoss = 0.5554, diffLoss = 2.7267, kgLoss = 0.0125
2025-04-08 16:38:58.910319: Training Step 227/354: batchLoss = 0.4582, diffLoss = 2.2329, kgLoss = 0.0145
2025-04-08 16:39:00.540957: Training Step 228/354: batchLoss = 0.6486, diffLoss = 3.1755, kgLoss = 0.0168
2025-04-08 16:39:02.162893: Training Step 229/354: batchLoss = 0.5849, diffLoss = 2.8679, kgLoss = 0.0141
2025-04-08 16:39:03.798084: Training Step 230/354: batchLoss = 0.6080, diffLoss = 2.9804, kgLoss = 0.0149
2025-04-08 16:39:05.424395: Training Step 231/354: batchLoss = 0.6761, diffLoss = 3.3113, kgLoss = 0.0172
2025-04-08 16:39:07.049084: Training Step 232/354: batchLoss = 0.6211, diffLoss = 3.0490, kgLoss = 0.0142
2025-04-08 16:39:08.674183: Training Step 233/354: batchLoss = 0.4227, diffLoss = 2.0647, kgLoss = 0.0122
2025-04-08 16:39:10.294858: Training Step 234/354: batchLoss = 0.6326, diffLoss = 3.0922, kgLoss = 0.0177
2025-04-08 16:39:11.910217: Training Step 235/354: batchLoss = 0.4865, diffLoss = 2.3794, kgLoss = 0.0133
2025-04-08 16:39:13.530328: Training Step 236/354: batchLoss = 0.5945, diffLoss = 2.9137, kgLoss = 0.0147
2025-04-08 16:39:15.161840: Training Step 237/354: batchLoss = 0.3903, diffLoss = 1.8999, kgLoss = 0.0128
2025-04-08 16:39:16.792733: Training Step 238/354: batchLoss = 0.4397, diffLoss = 2.1529, kgLoss = 0.0115
2025-04-08 16:39:18.420206: Training Step 239/354: batchLoss = 0.5420, diffLoss = 2.6524, kgLoss = 0.0144
2025-04-08 16:39:20.047647: Training Step 240/354: batchLoss = 0.6182, diffLoss = 3.0325, kgLoss = 0.0146
2025-04-08 16:39:21.679332: Training Step 241/354: batchLoss = 0.5166, diffLoss = 2.5325, kgLoss = 0.0126
2025-04-08 16:39:23.304731: Training Step 242/354: batchLoss = 0.7575, diffLoss = 3.7171, kgLoss = 0.0176
2025-04-08 16:39:24.924485: Training Step 243/354: batchLoss = 0.4959, diffLoss = 2.4289, kgLoss = 0.0127
2025-04-08 16:39:26.546425: Training Step 244/354: batchLoss = 0.5836, diffLoss = 2.8598, kgLoss = 0.0145
2025-04-08 16:39:28.157556: Training Step 245/354: batchLoss = 0.6154, diffLoss = 3.0053, kgLoss = 0.0179
2025-04-08 16:39:29.771928: Training Step 246/354: batchLoss = 0.5722, diffLoss = 2.7920, kgLoss = 0.0173
2025-04-08 16:39:31.400363: Training Step 247/354: batchLoss = 0.4340, diffLoss = 2.1219, kgLoss = 0.0120
2025-04-08 16:39:33.026544: Training Step 248/354: batchLoss = 0.5343, diffLoss = 2.6119, kgLoss = 0.0149
2025-04-08 16:39:34.666792: Training Step 249/354: batchLoss = 0.5704, diffLoss = 2.7940, kgLoss = 0.0145
2025-04-08 16:39:36.289506: Training Step 250/354: batchLoss = 0.5227, diffLoss = 2.5611, kgLoss = 0.0131
2025-04-08 16:39:37.915961: Training Step 251/354: batchLoss = 0.5023, diffLoss = 2.4648, kgLoss = 0.0116
2025-04-08 16:39:39.544757: Training Step 252/354: batchLoss = 0.5411, diffLoss = 2.6488, kgLoss = 0.0142
2025-04-08 16:39:41.162042: Training Step 253/354: batchLoss = 0.7168, diffLoss = 3.5181, kgLoss = 0.0165
2025-04-08 16:39:42.772476: Training Step 254/354: batchLoss = 0.5479, diffLoss = 2.6835, kgLoss = 0.0140
2025-04-08 16:39:44.392188: Training Step 255/354: batchLoss = 0.4782, diffLoss = 2.3402, kgLoss = 0.0127
2025-04-08 16:39:46.006276: Training Step 256/354: batchLoss = 0.4618, diffLoss = 2.2523, kgLoss = 0.0141
2025-04-08 16:39:47.636284: Training Step 257/354: batchLoss = 0.4536, diffLoss = 2.2196, kgLoss = 0.0121
2025-04-08 16:39:49.270455: Training Step 258/354: batchLoss = 0.5192, diffLoss = 2.5441, kgLoss = 0.0130
2025-04-08 16:39:50.906896: Training Step 259/354: batchLoss = 0.5905, diffLoss = 2.8811, kgLoss = 0.0178
2025-04-08 16:39:52.533670: Training Step 260/354: batchLoss = 0.5604, diffLoss = 2.7398, kgLoss = 0.0156
2025-04-08 16:39:54.168330: Training Step 261/354: batchLoss = 0.6804, diffLoss = 3.3366, kgLoss = 0.0164
2025-04-08 16:39:55.802350: Training Step 262/354: batchLoss = 1.8653, diffLoss = 9.1523, kgLoss = 0.0435
2025-04-08 16:39:57.431340: Training Step 263/354: batchLoss = 0.4658, diffLoss = 2.2790, kgLoss = 0.0124
2025-04-08 16:39:59.049586: Training Step 264/354: batchLoss = 0.5315, diffLoss = 2.6088, kgLoss = 0.0121
2025-04-08 16:40:00.666261: Training Step 265/354: batchLoss = 0.5394, diffLoss = 2.6443, kgLoss = 0.0132
2025-04-08 16:40:02.293047: Training Step 266/354: batchLoss = 0.4420, diffLoss = 2.1639, kgLoss = 0.0116
2025-04-08 16:40:03.928035: Training Step 267/354: batchLoss = 0.5397, diffLoss = 2.6422, kgLoss = 0.0141
2025-04-08 16:40:05.558260: Training Step 268/354: batchLoss = 0.5385, diffLoss = 2.6362, kgLoss = 0.0141
2025-04-08 16:40:07.189897: Training Step 269/354: batchLoss = 0.4981, diffLoss = 2.4374, kgLoss = 0.0133
2025-04-08 16:40:08.820940: Training Step 270/354: batchLoss = 0.6771, diffLoss = 3.3198, kgLoss = 0.0165
2025-04-08 16:40:10.454225: Training Step 271/354: batchLoss = 0.6102, diffLoss = 2.9868, kgLoss = 0.0160
2025-04-08 16:40:12.088135: Training Step 272/354: batchLoss = 0.4938, diffLoss = 2.4196, kgLoss = 0.0123
2025-04-08 16:40:13.707255: Training Step 273/354: batchLoss = 0.7438, diffLoss = 3.6421, kgLoss = 0.0193
2025-04-08 16:40:15.319914: Training Step 274/354: batchLoss = 0.5329, diffLoss = 2.6105, kgLoss = 0.0135
2025-04-08 16:40:16.931641: Training Step 275/354: batchLoss = 0.5645, diffLoss = 2.7585, kgLoss = 0.0160
2025-04-08 16:40:18.546418: Training Step 276/354: batchLoss = 0.5385, diffLoss = 2.6342, kgLoss = 0.0146
2025-04-08 16:40:20.176130: Training Step 277/354: batchLoss = 0.6285, diffLoss = 3.0806, kgLoss = 0.0155
2025-04-08 16:40:21.804406: Training Step 278/354: batchLoss = 0.6123, diffLoss = 3.0049, kgLoss = 0.0142
2025-04-08 16:40:23.430567: Training Step 279/354: batchLoss = 0.6174, diffLoss = 3.0234, kgLoss = 0.0159
2025-04-08 16:40:25.057761: Training Step 280/354: batchLoss = 0.4717, diffLoss = 2.3010, kgLoss = 0.0144
2025-04-08 16:40:26.686035: Training Step 281/354: batchLoss = 0.6282, diffLoss = 3.0747, kgLoss = 0.0166
2025-04-08 16:40:28.314881: Training Step 282/354: batchLoss = 0.4966, diffLoss = 2.4229, kgLoss = 0.0150
2025-04-08 16:40:29.924434: Training Step 283/354: batchLoss = 0.5693, diffLoss = 2.7921, kgLoss = 0.0136
2025-04-08 16:40:31.530920: Training Step 284/354: batchLoss = 0.5566, diffLoss = 2.7293, kgLoss = 0.0134
2025-04-08 16:40:33.139112: Training Step 285/354: batchLoss = 0.5804, diffLoss = 2.8396, kgLoss = 0.0156
2025-04-08 16:40:34.744463: Training Step 286/354: batchLoss = 0.6306, diffLoss = 3.0940, kgLoss = 0.0148
2025-04-08 16:40:36.364298: Training Step 287/354: batchLoss = 0.6038, diffLoss = 2.9619, kgLoss = 0.0143
2025-04-08 16:40:38.001346: Training Step 288/354: batchLoss = 0.4156, diffLoss = 2.0368, kgLoss = 0.0103
2025-04-08 16:40:39.632616: Training Step 289/354: batchLoss = 0.5883, diffLoss = 2.8850, kgLoss = 0.0142
2025-04-08 16:40:41.255733: Training Step 290/354: batchLoss = 0.5301, diffLoss = 2.5990, kgLoss = 0.0129
2025-04-08 16:40:42.880061: Training Step 291/354: batchLoss = 0.7324, diffLoss = 3.5889, kgLoss = 0.0183
2025-04-08 16:40:44.504997: Training Step 292/354: batchLoss = 0.5847, diffLoss = 2.8689, kgLoss = 0.0136
2025-04-08 16:40:46.131462: Training Step 293/354: batchLoss = 0.5439, diffLoss = 2.6587, kgLoss = 0.0152
2025-04-08 16:40:47.742156: Training Step 294/354: batchLoss = 0.6038, diffLoss = 2.9547, kgLoss = 0.0161
2025-04-08 16:40:49.357284: Training Step 295/354: batchLoss = 0.5052, diffLoss = 2.4766, kgLoss = 0.0123
2025-04-08 16:40:50.975641: Training Step 296/354: batchLoss = 0.4912, diffLoss = 2.4001, kgLoss = 0.0140
2025-04-08 16:40:52.595670: Training Step 297/354: batchLoss = 0.5596, diffLoss = 2.7409, kgLoss = 0.0143
2025-04-08 16:40:54.220791: Training Step 298/354: batchLoss = 0.6012, diffLoss = 2.9398, kgLoss = 0.0166
2025-04-08 16:40:55.843064: Training Step 299/354: batchLoss = 0.5518, diffLoss = 2.7005, kgLoss = 0.0146
2025-04-08 16:40:57.466415: Training Step 300/354: batchLoss = 0.5043, diffLoss = 2.4705, kgLoss = 0.0127
2025-04-08 16:40:59.090399: Training Step 301/354: batchLoss = 0.6327, diffLoss = 3.0942, kgLoss = 0.0173
2025-04-08 16:41:00.722435: Training Step 302/354: batchLoss = 0.5611, diffLoss = 2.7489, kgLoss = 0.0142
2025-04-08 16:41:02.354708: Training Step 303/354: batchLoss = 0.4653, diffLoss = 2.2820, kgLoss = 0.0111
2025-04-08 16:41:03.976948: Training Step 304/354: batchLoss = 0.6242, diffLoss = 3.0602, kgLoss = 0.0152
2025-04-08 16:41:05.598198: Training Step 305/354: batchLoss = 0.6068, diffLoss = 2.9583, kgLoss = 0.0190
2025-04-08 16:41:07.217677: Training Step 306/354: batchLoss = 0.6043, diffLoss = 2.9582, kgLoss = 0.0159
2025-04-08 16:41:08.840312: Training Step 307/354: batchLoss = 0.5326, diffLoss = 2.6097, kgLoss = 0.0134
2025-04-08 16:41:10.464948: Training Step 308/354: batchLoss = 0.6036, diffLoss = 2.9617, kgLoss = 0.0141
2025-04-08 16:41:12.097793: Training Step 309/354: batchLoss = 0.6044, diffLoss = 2.9666, kgLoss = 0.0139
2025-04-08 16:41:13.730107: Training Step 310/354: batchLoss = 0.4873, diffLoss = 2.3863, kgLoss = 0.0126
2025-04-08 16:41:15.350418: Training Step 311/354: batchLoss = 0.5988, diffLoss = 2.9326, kgLoss = 0.0153
2025-04-08 16:41:16.976481: Training Step 312/354: batchLoss = 0.5155, diffLoss = 2.5301, kgLoss = 0.0119
2025-04-08 16:41:18.599817: Training Step 313/354: batchLoss = 0.5463, diffLoss = 2.6740, kgLoss = 0.0144
2025-04-08 16:41:20.225511: Training Step 314/354: batchLoss = 0.5096, diffLoss = 2.4991, kgLoss = 0.0122
2025-04-08 16:41:21.843501: Training Step 315/354: batchLoss = 0.6959, diffLoss = 3.4125, kgLoss = 0.0168
2025-04-08 16:41:23.466131: Training Step 316/354: batchLoss = 0.6729, diffLoss = 3.3022, kgLoss = 0.0155
2025-04-08 16:41:25.090557: Training Step 317/354: batchLoss = 0.5424, diffLoss = 2.6529, kgLoss = 0.0148
2025-04-08 16:41:26.720440: Training Step 318/354: batchLoss = 0.4551, diffLoss = 2.2293, kgLoss = 0.0116
2025-04-08 16:41:28.355099: Training Step 319/354: batchLoss = 0.5435, diffLoss = 2.6626, kgLoss = 0.0137
2025-04-08 16:41:29.982914: Training Step 320/354: batchLoss = 0.5694, diffLoss = 2.7913, kgLoss = 0.0139
2025-04-08 16:41:31.611555: Training Step 321/354: batchLoss = 0.5319, diffLoss = 2.6048, kgLoss = 0.0136
2025-04-08 16:41:33.238600: Training Step 322/354: batchLoss = 0.4195, diffLoss = 2.0535, kgLoss = 0.0110
2025-04-08 16:41:34.858276: Training Step 323/354: batchLoss = 0.6863, diffLoss = 3.3623, kgLoss = 0.0173
2025-04-08 16:41:36.548622: Training Step 324/354: batchLoss = 0.4774, diffLoss = 2.3384, kgLoss = 0.0121
2025-04-08 16:41:38.186792: Training Step 325/354: batchLoss = 0.5218, diffLoss = 2.5463, kgLoss = 0.0157
2025-04-08 16:41:39.807998: Training Step 326/354: batchLoss = 0.4893, diffLoss = 2.3945, kgLoss = 0.0130
2025-04-08 16:41:41.442378: Training Step 327/354: batchLoss = 0.6045, diffLoss = 2.9548, kgLoss = 0.0169
2025-04-08 16:41:43.071165: Training Step 328/354: batchLoss = 0.6108, diffLoss = 2.9986, kgLoss = 0.0138
2025-04-08 16:41:44.700007: Training Step 329/354: batchLoss = 0.4692, diffLoss = 2.2930, kgLoss = 0.0132
2025-04-08 16:41:46.320179: Training Step 330/354: batchLoss = 0.5184, diffLoss = 2.5353, kgLoss = 0.0142
2025-04-08 16:41:47.950033: Training Step 331/354: batchLoss = 0.6773, diffLoss = 3.3150, kgLoss = 0.0179
2025-04-08 16:41:49.578917: Training Step 332/354: batchLoss = 0.7295, diffLoss = 3.5761, kgLoss = 0.0179
2025-04-08 16:41:51.203898: Training Step 333/354: batchLoss = 0.4840, diffLoss = 2.3666, kgLoss = 0.0133
2025-04-08 16:41:52.809662: Training Step 334/354: batchLoss = 0.6113, diffLoss = 2.9954, kgLoss = 0.0153
2025-04-08 16:41:54.425407: Training Step 335/354: batchLoss = 0.4845, diffLoss = 2.3777, kgLoss = 0.0112
2025-04-08 16:41:56.041105: Training Step 336/354: batchLoss = 0.5284, diffLoss = 2.5884, kgLoss = 0.0134
2025-04-08 16:41:57.662201: Training Step 337/354: batchLoss = 0.4856, diffLoss = 2.3808, kgLoss = 0.0118
2025-04-08 16:41:59.293219: Training Step 338/354: batchLoss = 0.5999, diffLoss = 2.9390, kgLoss = 0.0152
2025-04-08 16:42:00.920805: Training Step 339/354: batchLoss = 0.6944, diffLoss = 3.4075, kgLoss = 0.0161
2025-04-08 16:42:02.549347: Training Step 340/354: batchLoss = 0.4944, diffLoss = 2.4184, kgLoss = 0.0135
2025-04-08 16:42:04.169910: Training Step 341/354: batchLoss = 0.6601, diffLoss = 3.2411, kgLoss = 0.0148
2025-04-08 16:42:05.803976: Training Step 342/354: batchLoss = 0.5948, diffLoss = 2.9147, kgLoss = 0.0149
2025-04-08 16:42:07.431802: Training Step 343/354: batchLoss = 0.5286, diffLoss = 2.5859, kgLoss = 0.0143
2025-04-08 16:42:09.057324: Training Step 344/354: batchLoss = 0.6010, diffLoss = 2.9473, kgLoss = 0.0144
2025-04-08 16:42:10.671406: Training Step 345/354: batchLoss = 0.4984, diffLoss = 2.4367, kgLoss = 0.0138
2025-04-08 16:42:12.289723: Training Step 346/354: batchLoss = 0.7230, diffLoss = 3.5488, kgLoss = 0.0165
2025-04-08 16:42:13.904114: Training Step 347/354: batchLoss = 0.7527, diffLoss = 3.6882, kgLoss = 0.0188
2025-04-08 16:42:15.533694: Training Step 348/354: batchLoss = 0.6296, diffLoss = 3.0856, kgLoss = 0.0156
2025-04-08 16:42:17.166033: Training Step 349/354: batchLoss = 0.8525, diffLoss = 4.1837, kgLoss = 0.0196
2025-04-08 16:42:18.795705: Training Step 350/354: batchLoss = 0.5910, diffLoss = 2.8971, kgLoss = 0.0144
2025-04-08 16:42:20.415659: Training Step 351/354: batchLoss = 0.5232, diffLoss = 2.5648, kgLoss = 0.0128
2025-04-08 16:42:22.026756: Training Step 352/354: batchLoss = 0.4974, diffLoss = 2.4332, kgLoss = 0.0134
2025-04-08 16:42:23.443523: Training Step 353/354: batchLoss = 0.5776, diffLoss = 2.8387, kgLoss = 0.0124
2025-04-08 16:42:23.539001: 
2025-04-08 16:42:23.539863: Epoch 19/1000, Train: epLoss = 1.0171, epDfLoss = 4.9810, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 16:42:24.860955: Steps 0/138: batch_recall = 45.96, batch_ndcg = 26.25 
2025-04-08 16:42:26.164853: Steps 1/138: batch_recall = 47.13, batch_ndcg = 27.76 
2025-04-08 16:42:27.450224: Steps 2/138: batch_recall = 59.05, batch_ndcg = 36.55 
2025-04-08 16:42:28.762952: Steps 3/138: batch_recall = 59.21, batch_ndcg = 34.42 
2025-04-08 16:42:30.056806: Steps 4/138: batch_recall = 69.17, batch_ndcg = 41.25 
2025-04-08 16:42:31.356680: Steps 5/138: batch_recall = 57.84, batch_ndcg = 31.93 
2025-04-08 16:42:32.650624: Steps 6/138: batch_recall = 52.93, batch_ndcg = 31.79 
2025-04-08 16:42:33.948401: Steps 7/138: batch_recall = 64.17, batch_ndcg = 42.15 
2025-04-08 16:42:35.240267: Steps 8/138: batch_recall = 63.91, batch_ndcg = 40.53 
2025-04-08 16:42:36.536563: Steps 9/138: batch_recall = 56.55, batch_ndcg = 33.37 
2025-04-08 16:42:37.828598: Steps 10/138: batch_recall = 56.09, batch_ndcg = 31.39 
2025-04-08 16:42:39.105451: Steps 11/138: batch_recall = 57.88, batch_ndcg = 32.96 
2025-04-08 16:42:40.390894: Steps 12/138: batch_recall = 52.38, batch_ndcg = 29.15 
2025-04-08 16:42:41.657241: Steps 13/138: batch_recall = 51.53, batch_ndcg = 30.80 
2025-04-08 16:42:42.924271: Steps 14/138: batch_recall = 53.40, batch_ndcg = 31.03 
2025-04-08 16:42:44.193739: Steps 15/138: batch_recall = 50.34, batch_ndcg = 29.99 
2025-04-08 16:42:45.473754: Steps 16/138: batch_recall = 60.40, batch_ndcg = 33.93 
2025-04-08 16:42:46.766372: Steps 17/138: batch_recall = 57.35, batch_ndcg = 32.58 
2025-04-08 16:42:48.074686: Steps 18/138: batch_recall = 52.59, batch_ndcg = 32.85 
2025-04-08 16:42:49.381063: Steps 19/138: batch_recall = 57.01, batch_ndcg = 33.37 
2025-04-08 16:42:50.664856: Steps 20/138: batch_recall = 62.11, batch_ndcg = 36.53 
2025-04-08 16:42:51.952699: Steps 21/138: batch_recall = 68.82, batch_ndcg = 40.41 
2025-04-08 16:42:53.240271: Steps 22/138: batch_recall = 57.47, batch_ndcg = 33.05 
2025-04-08 16:42:54.543658: Steps 23/138: batch_recall = 50.86, batch_ndcg = 29.86 
2025-04-08 16:42:55.833707: Steps 24/138: batch_recall = 58.61, batch_ndcg = 31.47 
2025-04-08 16:42:57.119885: Steps 25/138: batch_recall = 61.59, batch_ndcg = 35.80 
2025-04-08 16:42:58.413587: Steps 26/138: batch_recall = 56.99, batch_ndcg = 33.23 
2025-04-08 16:42:59.689444: Steps 27/138: batch_recall = 60.24, batch_ndcg = 34.63 
2025-04-08 16:43:00.965829: Steps 28/138: batch_recall = 59.12, batch_ndcg = 33.24 
2025-04-08 16:43:02.237024: Steps 29/138: batch_recall = 63.41, batch_ndcg = 32.92 
2025-04-08 16:43:03.526652: Steps 30/138: batch_recall = 57.12, batch_ndcg = 34.00 
2025-04-08 16:43:04.824189: Steps 31/138: batch_recall = 42.67, batch_ndcg = 24.72 
2025-04-08 16:43:06.093777: Steps 32/138: batch_recall = 53.24, batch_ndcg = 31.21 
2025-04-08 16:43:07.371840: Steps 33/138: batch_recall = 61.42, batch_ndcg = 33.87 
2025-04-08 16:43:08.654588: Steps 34/138: batch_recall = 54.25, batch_ndcg = 28.50 
2025-04-08 16:43:09.937857: Steps 35/138: batch_recall = 52.82, batch_ndcg = 30.68 
2025-04-08 16:43:11.215885: Steps 36/138: batch_recall = 48.06, batch_ndcg = 27.90 
2025-04-08 16:43:12.486651: Steps 37/138: batch_recall = 59.50, batch_ndcg = 34.38 
2025-04-08 16:43:13.756326: Steps 38/138: batch_recall = 58.55, batch_ndcg = 32.72 
2025-04-08 16:43:15.018000: Steps 39/138: batch_recall = 65.03, batch_ndcg = 38.09 
2025-04-08 16:43:16.291607: Steps 40/138: batch_recall = 60.05, batch_ndcg = 31.29 
2025-04-08 16:43:17.560945: Steps 41/138: batch_recall = 60.39, batch_ndcg = 33.37 
2025-04-08 16:43:18.836310: Steps 42/138: batch_recall = 52.10, batch_ndcg = 29.32 
2025-04-08 16:43:20.100722: Steps 43/138: batch_recall = 55.18, batch_ndcg = 34.42 
2025-04-08 16:43:21.381945: Steps 44/138: batch_recall = 55.04, batch_ndcg = 30.05 
2025-04-08 16:43:22.664181: Steps 45/138: batch_recall = 61.62, batch_ndcg = 35.28 
2025-04-08 16:43:23.928517: Steps 46/138: batch_recall = 62.46, batch_ndcg = 35.74 
2025-04-08 16:43:25.195355: Steps 47/138: batch_recall = 55.23, batch_ndcg = 32.46 
2025-04-08 16:43:26.471845: Steps 48/138: batch_recall = 59.75, batch_ndcg = 34.55 
2025-04-08 16:43:27.752076: Steps 49/138: batch_recall = 66.84, batch_ndcg = 37.42 
2025-04-08 16:43:29.017460: Steps 50/138: batch_recall = 60.97, batch_ndcg = 31.85 
2025-04-08 16:43:30.277093: Steps 51/138: batch_recall = 59.52, batch_ndcg = 35.12 
2025-04-08 16:43:31.543742: Steps 52/138: batch_recall = 65.85, batch_ndcg = 42.27 
2025-04-08 16:43:32.820322: Steps 53/138: batch_recall = 70.15, batch_ndcg = 35.74 
2025-04-08 16:43:34.091026: Steps 54/138: batch_recall = 67.31, batch_ndcg = 38.75 
2025-04-08 16:43:35.364550: Steps 55/138: batch_recall = 63.86, batch_ndcg = 33.58 
2025-04-08 16:43:36.650839: Steps 56/138: batch_recall = 61.23, batch_ndcg = 34.45 
2025-04-08 16:43:37.937299: Steps 57/138: batch_recall = 58.35, batch_ndcg = 32.67 
2025-04-08 16:43:39.214163: Steps 58/138: batch_recall = 69.86, batch_ndcg = 37.68 
2025-04-08 16:43:40.491102: Steps 59/138: batch_recall = 70.09, batch_ndcg = 41.34 
2025-04-08 16:43:41.776643: Steps 60/138: batch_recall = 66.85, batch_ndcg = 37.16 
2025-04-08 16:43:43.047146: Steps 61/138: batch_recall = 62.49, batch_ndcg = 35.63 
2025-04-08 16:43:44.315733: Steps 62/138: batch_recall = 85.21, batch_ndcg = 44.29 
2025-04-08 16:43:45.580733: Steps 63/138: batch_recall = 75.41, batch_ndcg = 42.86 
2025-04-08 16:43:46.842954: Steps 64/138: batch_recall = 61.80, batch_ndcg = 33.01 
2025-04-08 16:43:48.110312: Steps 65/138: batch_recall = 87.05, batch_ndcg = 47.53 
2025-04-08 16:43:49.370189: Steps 66/138: batch_recall = 70.32, batch_ndcg = 40.48 
2025-04-08 16:43:50.638787: Steps 67/138: batch_recall = 79.01, batch_ndcg = 48.00 
2025-04-08 16:43:51.908512: Steps 68/138: batch_recall = 64.42, batch_ndcg = 34.60 
2025-04-08 16:43:53.185144: Steps 69/138: batch_recall = 88.17, batch_ndcg = 51.69 
2025-04-08 16:43:54.459587: Steps 70/138: batch_recall = 78.93, batch_ndcg = 46.05 
2025-04-08 16:43:55.739495: Steps 71/138: batch_recall = 89.57, batch_ndcg = 51.30 
2025-04-08 16:43:57.022855: Steps 72/138: batch_recall = 84.80, batch_ndcg = 48.78 
2025-04-08 16:43:58.300456: Steps 73/138: batch_recall = 89.21, batch_ndcg = 48.88 
2025-04-08 16:43:59.577886: Steps 74/138: batch_recall = 81.99, batch_ndcg = 50.30 
2025-04-08 16:44:00.863407: Steps 75/138: batch_recall = 86.94, batch_ndcg = 50.31 
2025-04-08 16:44:02.135882: Steps 76/138: batch_recall = 96.37, batch_ndcg = 55.78 
2025-04-08 16:44:03.406172: Steps 77/138: batch_recall = 87.95, batch_ndcg = 49.73 
2025-04-08 16:44:04.670331: Steps 78/138: batch_recall = 91.02, batch_ndcg = 48.72 
2025-04-08 16:44:05.950646: Steps 79/138: batch_recall = 94.94, batch_ndcg = 49.95 
2025-04-08 16:44:07.222479: Steps 80/138: batch_recall = 74.10, batch_ndcg = 39.52 
2025-04-08 16:44:08.513703: Steps 81/138: batch_recall = 77.99, batch_ndcg = 46.81 
2025-04-08 16:44:09.782669: Steps 82/138: batch_recall = 89.75, batch_ndcg = 52.69 
2025-04-08 16:44:11.049669: Steps 83/138: batch_recall = 80.66, batch_ndcg = 47.24 
2025-04-08 16:44:12.311694: Steps 84/138: batch_recall = 101.05, batch_ndcg = 57.60 
2025-04-08 16:44:13.569622: Steps 85/138: batch_recall = 105.35, batch_ndcg = 60.69 
2025-04-08 16:44:14.824718: Steps 86/138: batch_recall = 121.05, batch_ndcg = 71.01 
2025-04-08 16:44:16.082470: Steps 87/138: batch_recall = 106.78, batch_ndcg = 56.02 
2025-04-08 16:44:17.349555: Steps 88/138: batch_recall = 100.83, batch_ndcg = 57.01 
2025-04-08 16:44:18.605958: Steps 89/138: batch_recall = 117.09, batch_ndcg = 67.33 
2025-04-08 16:44:19.858081: Steps 90/138: batch_recall = 104.20, batch_ndcg = 58.60 
2025-04-08 16:44:21.112610: Steps 91/138: batch_recall = 119.86, batch_ndcg = 65.68 
2025-04-08 16:44:22.367334: Steps 92/138: batch_recall = 121.69, batch_ndcg = 64.53 
2025-04-08 16:44:23.628461: Steps 93/138: batch_recall = 121.00, batch_ndcg = 68.60 
2025-04-08 16:44:24.875568: Steps 94/138: batch_recall = 123.45, batch_ndcg = 64.94 
2025-04-08 16:44:26.124359: Steps 95/138: batch_recall = 115.94, batch_ndcg = 68.39 
2025-04-08 16:44:27.369000: Steps 96/138: batch_recall = 135.78, batch_ndcg = 75.75 
2025-04-08 16:44:28.617261: Steps 97/138: batch_recall = 151.85, batch_ndcg = 83.81 
2025-04-08 16:44:29.868277: Steps 98/138: batch_recall = 111.43, batch_ndcg = 63.33 
2025-04-08 16:44:31.111182: Steps 99/138: batch_recall = 125.92, batch_ndcg = 70.88 
2025-04-08 16:44:32.363800: Steps 100/138: batch_recall = 130.11, batch_ndcg = 72.59 
2025-04-08 16:44:33.625272: Steps 101/138: batch_recall = 124.59, batch_ndcg = 69.59 
2025-04-08 16:44:34.900549: Steps 102/138: batch_recall = 125.35, batch_ndcg = 73.59 
2025-04-08 16:44:36.169420: Steps 103/138: batch_recall = 142.31, batch_ndcg = 79.70 
2025-04-08 16:44:37.420883: Steps 104/138: batch_recall = 134.68, batch_ndcg = 78.12 
2025-04-08 16:44:38.673383: Steps 105/138: batch_recall = 118.00, batch_ndcg = 67.25 
2025-04-08 16:44:39.931394: Steps 106/138: batch_recall = 105.23, batch_ndcg = 59.58 
2025-04-08 16:44:41.211291: Steps 107/138: batch_recall = 114.51, batch_ndcg = 64.97 
2025-04-08 16:44:42.483945: Steps 108/138: batch_recall = 117.44, batch_ndcg = 69.84 
2025-04-08 16:44:43.749857: Steps 109/138: batch_recall = 133.03, batch_ndcg = 75.61 
2025-04-08 16:44:45.019411: Steps 110/138: batch_recall = 126.44, batch_ndcg = 66.02 
2025-04-08 16:44:46.293153: Steps 111/138: batch_recall = 139.25, batch_ndcg = 84.06 
2025-04-08 16:44:47.554330: Steps 112/138: batch_recall = 170.68, batch_ndcg = 93.48 
2025-04-08 16:44:48.824672: Steps 113/138: batch_recall = 130.06, batch_ndcg = 73.77 
2025-04-08 16:44:50.081385: Steps 114/138: batch_recall = 124.42, batch_ndcg = 71.49 
2025-04-08 16:44:51.355408: Steps 115/138: batch_recall = 118.58, batch_ndcg = 62.77 
2025-04-08 16:44:52.622784: Steps 116/138: batch_recall = 129.69, batch_ndcg = 67.68 
2025-04-08 16:44:53.901980: Steps 117/138: batch_recall = 112.68, batch_ndcg = 66.32 
2025-04-08 16:44:55.188767: Steps 118/138: batch_recall = 122.08, batch_ndcg = 69.76 
2025-04-08 16:44:56.461694: Steps 119/138: batch_recall = 140.07, batch_ndcg = 75.39 
2025-04-08 16:44:57.736871: Steps 120/138: batch_recall = 126.16, batch_ndcg = 70.38 
2025-04-08 16:44:59.020039: Steps 121/138: batch_recall = 148.53, batch_ndcg = 78.93 
2025-04-08 16:45:00.278227: Steps 122/138: batch_recall = 154.23, batch_ndcg = 83.57 
2025-04-08 16:45:01.554330: Steps 123/138: batch_recall = 131.26, batch_ndcg = 73.39 
2025-04-08 16:45:02.838426: Steps 124/138: batch_recall = 153.36, batch_ndcg = 94.40 
2025-04-08 16:45:04.100799: Steps 125/138: batch_recall = 130.83, batch_ndcg = 72.42 
2025-04-08 16:45:05.359383: Steps 126/138: batch_recall = 159.30, batch_ndcg = 88.87 
2025-04-08 16:45:06.614640: Steps 127/138: batch_recall = 142.34, batch_ndcg = 81.42 
2025-04-08 16:45:07.864933: Steps 128/138: batch_recall = 130.43, batch_ndcg = 72.04 
2025-04-08 16:45:09.125237: Steps 129/138: batch_recall = 158.32, batch_ndcg = 91.32 
2025-04-08 16:45:10.403213: Steps 130/138: batch_recall = 131.66, batch_ndcg = 69.56 
2025-04-08 16:45:11.662921: Steps 131/138: batch_recall = 151.58, batch_ndcg = 86.14 
2025-04-08 16:45:12.934499: Steps 132/138: batch_recall = 146.50, batch_ndcg = 84.65 
2025-04-08 16:45:14.193582: Steps 133/138: batch_recall = 146.85, batch_ndcg = 84.93 
2025-04-08 16:45:15.464101: Steps 134/138: batch_recall = 140.29, batch_ndcg = 81.93 
2025-04-08 16:45:16.714093: Steps 135/138: batch_recall = 163.53, batch_ndcg = 95.70 
2025-04-08 16:45:17.968123: Steps 136/138: batch_recall = 149.34, batch_ndcg = 78.01 
2025-04-08 16:45:19.217186: Steps 137/138: batch_recall = 141.77, batch_ndcg = 86.92 
2025-04-08 16:45:19.217640: Epoch 19/1000, Test: Recall = 0.1772, NDCG = 0.1004  

2025-04-08 16:45:20.985128: Training Step 0/354: batchLoss = 0.4848, diffLoss = 2.3698, kgLoss = 0.0135
2025-04-08 16:45:22.599510: Training Step 1/354: batchLoss = 0.5515, diffLoss = 2.7041, kgLoss = 0.0134
2025-04-08 16:45:24.217711: Training Step 2/354: batchLoss = 0.4824, diffLoss = 2.3611, kgLoss = 0.0127
2025-04-08 16:45:25.833341: Training Step 3/354: batchLoss = 0.6210, diffLoss = 3.0487, kgLoss = 0.0141
2025-04-08 16:45:27.454968: Training Step 4/354: batchLoss = 0.5173, diffLoss = 2.5348, kgLoss = 0.0130
2025-04-08 16:45:29.075791: Training Step 5/354: batchLoss = 0.5572, diffLoss = 2.7263, kgLoss = 0.0149
2025-04-08 16:45:30.701236: Training Step 6/354: batchLoss = 0.7053, diffLoss = 3.4487, kgLoss = 0.0195
2025-04-08 16:45:32.325651: Training Step 7/354: batchLoss = 0.5618, diffLoss = 2.7484, kgLoss = 0.0151
2025-04-08 16:45:33.951431: Training Step 8/354: batchLoss = 0.5079, diffLoss = 2.4814, kgLoss = 0.0145
2025-04-08 16:45:35.571648: Training Step 9/354: batchLoss = 0.6118, diffLoss = 2.9952, kgLoss = 0.0159
2025-04-08 16:45:37.187961: Training Step 10/354: batchLoss = 0.4648, diffLoss = 2.2710, kgLoss = 0.0132
2025-04-08 16:45:38.800835: Training Step 11/354: batchLoss = 0.4490, diffLoss = 2.1998, kgLoss = 0.0113
2025-04-08 16:45:40.421454: Training Step 12/354: batchLoss = 0.4967, diffLoss = 2.4272, kgLoss = 0.0141
2025-04-08 16:45:42.047600: Training Step 13/354: batchLoss = 0.4771, diffLoss = 2.3322, kgLoss = 0.0133
2025-04-08 16:45:43.669147: Training Step 14/354: batchLoss = 0.5793, diffLoss = 2.8366, kgLoss = 0.0149
2025-04-08 16:45:45.291965: Training Step 15/354: batchLoss = 0.5148, diffLoss = 2.5242, kgLoss = 0.0124
2025-04-08 16:45:46.914173: Training Step 16/354: batchLoss = 0.5419, diffLoss = 2.6488, kgLoss = 0.0152
2025-04-08 16:45:48.535832: Training Step 17/354: batchLoss = 0.5443, diffLoss = 2.6672, kgLoss = 0.0136
2025-04-08 16:45:50.162269: Training Step 18/354: batchLoss = 0.5103, diffLoss = 2.4858, kgLoss = 0.0164
2025-04-08 16:45:51.781468: Training Step 19/354: batchLoss = 0.6008, diffLoss = 2.9434, kgLoss = 0.0152
2025-04-08 16:45:53.394813: Training Step 20/354: batchLoss = 0.4550, diffLoss = 2.2264, kgLoss = 0.0121
2025-04-08 16:45:55.011196: Training Step 21/354: batchLoss = 0.5567, diffLoss = 2.7213, kgLoss = 0.0156
2025-04-08 16:45:56.627921: Training Step 22/354: batchLoss = 0.7067, diffLoss = 3.4657, kgLoss = 0.0170
2025-04-08 16:45:58.244906: Training Step 23/354: batchLoss = 0.8138, diffLoss = 3.9869, kgLoss = 0.0205
2025-04-08 16:45:59.868231: Training Step 24/354: batchLoss = 0.5812, diffLoss = 2.8490, kgLoss = 0.0142
2025-04-08 16:46:01.480298: Training Step 25/354: batchLoss = 0.6345, diffLoss = 3.1131, kgLoss = 0.0148
2025-04-08 16:46:03.100687: Training Step 26/354: batchLoss = 0.5669, diffLoss = 2.7766, kgLoss = 0.0144
2025-04-08 16:46:04.717302: Training Step 27/354: batchLoss = 0.4589, diffLoss = 2.2487, kgLoss = 0.0114
2025-04-08 16:46:06.336033: Training Step 28/354: batchLoss = 0.5800, diffLoss = 2.8435, kgLoss = 0.0141
2025-04-08 16:46:07.955544: Training Step 29/354: batchLoss = 0.5032, diffLoss = 2.4610, kgLoss = 0.0137
2025-04-08 16:46:09.571805: Training Step 30/354: batchLoss = 0.4520, diffLoss = 2.2132, kgLoss = 0.0117
2025-04-08 16:46:11.189187: Training Step 31/354: batchLoss = 0.7254, diffLoss = 3.5458, kgLoss = 0.0203
2025-04-08 16:46:12.804336: Training Step 32/354: batchLoss = 0.4880, diffLoss = 2.3830, kgLoss = 0.0142
2025-04-08 16:46:14.425794: Training Step 33/354: batchLoss = 0.6891, diffLoss = 3.3811, kgLoss = 0.0161
2025-04-08 16:46:16.045023: Training Step 34/354: batchLoss = 0.5392, diffLoss = 2.6402, kgLoss = 0.0140
2025-04-08 16:46:17.665027: Training Step 35/354: batchLoss = 0.5413, diffLoss = 2.6530, kgLoss = 0.0134
2025-04-08 16:46:19.292297: Training Step 36/354: batchLoss = 0.6606, diffLoss = 3.2405, kgLoss = 0.0157
2025-04-08 16:46:20.913137: Training Step 37/354: batchLoss = 0.6438, diffLoss = 3.1453, kgLoss = 0.0185
2025-04-08 16:46:22.530023: Training Step 38/354: batchLoss = 0.5993, diffLoss = 2.9276, kgLoss = 0.0173
2025-04-08 16:46:24.144894: Training Step 39/354: batchLoss = 0.5171, diffLoss = 2.5348, kgLoss = 0.0127
2025-04-08 16:46:25.766888: Training Step 40/354: batchLoss = 0.4903, diffLoss = 2.3975, kgLoss = 0.0135
2025-04-08 16:46:27.377331: Training Step 41/354: batchLoss = 0.5365, diffLoss = 2.6347, kgLoss = 0.0119
2025-04-08 16:46:28.988689: Training Step 42/354: batchLoss = 0.6075, diffLoss = 2.9721, kgLoss = 0.0163
2025-04-08 16:46:30.607034: Training Step 43/354: batchLoss = 0.5617, diffLoss = 2.6418, kgLoss = 0.0416
2025-04-08 16:46:32.226520: Training Step 44/354: batchLoss = 0.6921, diffLoss = 3.3943, kgLoss = 0.0165
2025-04-08 16:46:33.853238: Training Step 45/354: batchLoss = 0.4281, diffLoss = 2.0876, kgLoss = 0.0132
2025-04-08 16:46:35.473063: Training Step 46/354: batchLoss = 0.5467, diffLoss = 2.6726, kgLoss = 0.0152
2025-04-08 16:46:37.096365: Training Step 47/354: batchLoss = 1.0486, diffLoss = 5.1456, kgLoss = 0.0243
2025-04-08 16:46:38.717936: Training Step 48/354: batchLoss = 0.5824, diffLoss = 2.8581, kgLoss = 0.0135
2025-04-08 16:46:40.334825: Training Step 49/354: batchLoss = 0.5375, diffLoss = 2.6304, kgLoss = 0.0143
2025-04-08 16:46:41.949201: Training Step 50/354: batchLoss = 0.4807, diffLoss = 2.3593, kgLoss = 0.0111
2025-04-08 16:46:43.569440: Training Step 51/354: batchLoss = 0.5299, diffLoss = 2.5948, kgLoss = 0.0136
2025-04-08 16:46:45.187681: Training Step 52/354: batchLoss = 0.5556, diffLoss = 2.7204, kgLoss = 0.0144
2025-04-08 16:46:46.808006: Training Step 53/354: batchLoss = 0.4958, diffLoss = 2.4277, kgLoss = 0.0128
2025-04-08 16:46:48.436493: Training Step 54/354: batchLoss = 0.5343, diffLoss = 2.6170, kgLoss = 0.0137
2025-04-08 16:46:50.072149: Training Step 55/354: batchLoss = 0.4420, diffLoss = 2.1608, kgLoss = 0.0123
2025-04-08 16:46:51.699628: Training Step 56/354: batchLoss = 0.7599, diffLoss = 3.7303, kgLoss = 0.0173
2025-04-08 16:46:53.326227: Training Step 57/354: batchLoss = 0.4280, diffLoss = 2.0949, kgLoss = 0.0112
2025-04-08 16:46:54.953833: Training Step 58/354: batchLoss = 0.7123, diffLoss = 3.4969, kgLoss = 0.0161
2025-04-08 16:46:56.574171: Training Step 59/354: batchLoss = 0.4816, diffLoss = 2.3554, kgLoss = 0.0131
2025-04-08 16:46:58.189684: Training Step 60/354: batchLoss = 0.4555, diffLoss = 2.2342, kgLoss = 0.0109
2025-04-08 16:46:59.811632: Training Step 61/354: batchLoss = 0.5802, diffLoss = 2.8378, kgLoss = 0.0158
2025-04-08 16:47:01.425049: Training Step 62/354: batchLoss = 0.6098, diffLoss = 2.9897, kgLoss = 0.0149
2025-04-08 16:47:03.047180: Training Step 63/354: batchLoss = 0.4907, diffLoss = 2.3925, kgLoss = 0.0152
2025-04-08 16:47:04.679095: Training Step 64/354: batchLoss = 0.6288, diffLoss = 3.0750, kgLoss = 0.0172
2025-04-08 16:47:06.307486: Training Step 65/354: batchLoss = 0.5494, diffLoss = 2.6827, kgLoss = 0.0161
2025-04-08 16:47:07.937819: Training Step 66/354: batchLoss = 0.6283, diffLoss = 3.0669, kgLoss = 0.0186
2025-04-08 16:47:09.552377: Training Step 67/354: batchLoss = 0.5457, diffLoss = 2.6703, kgLoss = 0.0145
2025-04-08 16:47:11.180512: Training Step 68/354: batchLoss = 0.5934, diffLoss = 2.9050, kgLoss = 0.0155
2025-04-08 16:47:12.800754: Training Step 69/354: batchLoss = 0.5169, diffLoss = 2.5275, kgLoss = 0.0143
2025-04-08 16:47:14.419622: Training Step 70/354: batchLoss = 0.5747, diffLoss = 2.8107, kgLoss = 0.0157
2025-04-08 16:47:16.040275: Training Step 71/354: batchLoss = 0.5831, diffLoss = 2.8576, kgLoss = 0.0145
2025-04-08 16:47:17.662977: Training Step 72/354: batchLoss = 0.6134, diffLoss = 3.0134, kgLoss = 0.0135
2025-04-08 16:47:19.291172: Training Step 73/354: batchLoss = 0.6642, diffLoss = 3.2505, kgLoss = 0.0176
2025-04-08 16:47:20.910533: Training Step 74/354: batchLoss = 0.5083, diffLoss = 2.4905, kgLoss = 0.0127
2025-04-08 16:47:22.539463: Training Step 75/354: batchLoss = 0.5790, diffLoss = 2.8406, kgLoss = 0.0136
2025-04-08 16:47:24.162890: Training Step 76/354: batchLoss = 0.4538, diffLoss = 2.2204, kgLoss = 0.0121
2025-04-08 16:47:25.788166: Training Step 77/354: batchLoss = 0.4664, diffLoss = 2.2810, kgLoss = 0.0128
2025-04-08 16:47:27.410861: Training Step 78/354: batchLoss = 0.5890, diffLoss = 2.8860, kgLoss = 0.0148
2025-04-08 16:47:29.027147: Training Step 79/354: batchLoss = 0.6787, diffLoss = 3.3281, kgLoss = 0.0164
2025-04-08 16:47:30.641865: Training Step 80/354: batchLoss = 0.5663, diffLoss = 2.7782, kgLoss = 0.0134
2025-04-08 16:47:32.259913: Training Step 81/354: batchLoss = 0.5317, diffLoss = 2.6051, kgLoss = 0.0133
2025-04-08 16:47:33.878772: Training Step 82/354: batchLoss = 0.6208, diffLoss = 3.0413, kgLoss = 0.0157
2025-04-08 16:47:35.504425: Training Step 83/354: batchLoss = 0.6755, diffLoss = 3.3120, kgLoss = 0.0164
2025-04-08 16:47:37.126567: Training Step 84/354: batchLoss = 0.4937, diffLoss = 2.4183, kgLoss = 0.0126
2025-04-08 16:47:38.752262: Training Step 85/354: batchLoss = 0.5210, diffLoss = 2.5482, kgLoss = 0.0141
2025-04-08 16:47:40.376295: Training Step 86/354: batchLoss = 0.5107, diffLoss = 2.5031, kgLoss = 0.0126
2025-04-08 16:47:41.994417: Training Step 87/354: batchLoss = 0.5504, diffLoss = 2.6924, kgLoss = 0.0148
2025-04-08 16:47:43.609945: Training Step 88/354: batchLoss = 0.5423, diffLoss = 2.6552, kgLoss = 0.0140
2025-04-08 16:47:45.224931: Training Step 89/354: batchLoss = 0.4699, diffLoss = 2.3027, kgLoss = 0.0117
2025-04-08 16:47:46.851380: Training Step 90/354: batchLoss = 0.6543, diffLoss = 3.2088, kgLoss = 0.0157
2025-04-08 16:47:48.468809: Training Step 91/354: batchLoss = 0.6281, diffLoss = 3.0790, kgLoss = 0.0154
2025-04-08 16:47:50.096289: Training Step 92/354: batchLoss = 0.5654, diffLoss = 2.7660, kgLoss = 0.0152
2025-04-08 16:47:51.725923: Training Step 93/354: batchLoss = 0.5325, diffLoss = 2.6131, kgLoss = 0.0124
2025-04-08 16:47:53.353527: Training Step 94/354: batchLoss = 0.5268, diffLoss = 2.5809, kgLoss = 0.0133
2025-04-08 16:47:54.979819: Training Step 95/354: batchLoss = 0.6638, diffLoss = 3.2524, kgLoss = 0.0166
2025-04-08 16:47:56.610893: Training Step 96/354: batchLoss = 0.6588, diffLoss = 3.2320, kgLoss = 0.0155
2025-04-08 16:47:58.239692: Training Step 97/354: batchLoss = 0.7223, diffLoss = 3.5456, kgLoss = 0.0165
2025-04-08 16:47:59.856966: Training Step 98/354: batchLoss = 0.4647, diffLoss = 2.2651, kgLoss = 0.0146
2025-04-08 16:48:01.472150: Training Step 99/354: batchLoss = 0.5644, diffLoss = 2.7662, kgLoss = 0.0139
2025-04-08 16:48:03.090648: Training Step 100/354: batchLoss = 0.5350, diffLoss = 2.6183, kgLoss = 0.0142
2025-04-08 16:48:04.711193: Training Step 101/354: batchLoss = 0.5674, diffLoss = 2.7757, kgLoss = 0.0154
2025-04-08 16:48:06.343192: Training Step 102/354: batchLoss = 0.5983, diffLoss = 2.9318, kgLoss = 0.0149
2025-04-08 16:48:07.968262: Training Step 103/354: batchLoss = 0.5331, diffLoss = 2.6053, kgLoss = 0.0151
2025-04-08 16:48:09.590896: Training Step 104/354: batchLoss = 0.5419, diffLoss = 2.6520, kgLoss = 0.0143
2025-04-08 16:48:11.210923: Training Step 105/354: batchLoss = 0.4691, diffLoss = 2.2968, kgLoss = 0.0122
2025-04-08 16:48:12.834630: Training Step 106/354: batchLoss = 0.5434, diffLoss = 2.6656, kgLoss = 0.0128
2025-04-08 16:48:14.460557: Training Step 107/354: batchLoss = 0.5239, diffLoss = 2.5662, kgLoss = 0.0134
2025-04-08 16:48:16.084432: Training Step 108/354: batchLoss = 0.5118, diffLoss = 2.4987, kgLoss = 0.0151
2025-04-08 16:48:17.704160: Training Step 109/354: batchLoss = 0.5521, diffLoss = 2.7054, kgLoss = 0.0137
2025-04-08 16:48:19.317161: Training Step 110/354: batchLoss = 0.7393, diffLoss = 3.6238, kgLoss = 0.0181
2025-04-08 16:48:20.941484: Training Step 111/354: batchLoss = 0.5036, diffLoss = 2.4656, kgLoss = 0.0131
2025-04-08 16:48:22.562869: Training Step 112/354: batchLoss = 0.6282, diffLoss = 3.0765, kgLoss = 0.0161
2025-04-08 16:48:24.197956: Training Step 113/354: batchLoss = 0.6145, diffLoss = 3.0149, kgLoss = 0.0144
2025-04-08 16:48:25.819046: Training Step 114/354: batchLoss = 0.5239, diffLoss = 2.5652, kgLoss = 0.0136
2025-04-08 16:48:27.446190: Training Step 115/354: batchLoss = 0.5848, diffLoss = 2.8630, kgLoss = 0.0153
2025-04-08 16:48:29.074822: Training Step 116/354: batchLoss = 0.5732, diffLoss = 2.7943, kgLoss = 0.0179
2025-04-08 16:48:30.695751: Training Step 117/354: batchLoss = 0.5185, diffLoss = 2.5323, kgLoss = 0.0150
2025-04-08 16:48:32.315191: Training Step 118/354: batchLoss = 0.6622, diffLoss = 3.2428, kgLoss = 0.0171
2025-04-08 16:48:33.931550: Training Step 119/354: batchLoss = 0.4803, diffLoss = 2.3514, kgLoss = 0.0126
2025-04-08 16:48:35.552554: Training Step 120/354: batchLoss = 0.6383, diffLoss = 3.1311, kgLoss = 0.0151
2025-04-08 16:48:37.177253: Training Step 121/354: batchLoss = 0.4893, diffLoss = 2.3942, kgLoss = 0.0131
2025-04-08 16:48:38.798278: Training Step 122/354: batchLoss = 0.5391, diffLoss = 2.6422, kgLoss = 0.0134
2025-04-08 16:48:40.427542: Training Step 123/354: batchLoss = 0.5207, diffLoss = 2.5524, kgLoss = 0.0127
2025-04-08 16:48:42.052435: Training Step 124/354: batchLoss = 0.6996, diffLoss = 3.4305, kgLoss = 0.0169
2025-04-08 16:48:43.678195: Training Step 125/354: batchLoss = 0.6955, diffLoss = 3.4091, kgLoss = 0.0172
2025-04-08 16:48:45.312329: Training Step 126/354: batchLoss = 0.5954, diffLoss = 2.9168, kgLoss = 0.0151
2025-04-08 16:48:46.932323: Training Step 127/354: batchLoss = 0.5030, diffLoss = 2.4651, kgLoss = 0.0125
2025-04-08 16:48:48.553911: Training Step 128/354: batchLoss = 0.5081, diffLoss = 2.4847, kgLoss = 0.0139
2025-04-08 16:48:50.171742: Training Step 129/354: batchLoss = 0.5233, diffLoss = 2.5619, kgLoss = 0.0136
2025-04-08 16:48:51.771479: Training Step 130/354: batchLoss = 0.6145, diffLoss = 2.9983, kgLoss = 0.0185
2025-04-08 16:48:53.383364: Training Step 131/354: batchLoss = 0.4323, diffLoss = 2.0960, kgLoss = 0.0164
2025-04-08 16:48:55.006203: Training Step 132/354: batchLoss = 0.5401, diffLoss = 2.6455, kgLoss = 0.0137
2025-04-08 16:48:56.628258: Training Step 133/354: batchLoss = 0.4630, diffLoss = 2.2604, kgLoss = 0.0136
2025-04-08 16:48:58.250637: Training Step 134/354: batchLoss = 0.5636, diffLoss = 2.7628, kgLoss = 0.0138
2025-04-08 16:48:59.882953: Training Step 135/354: batchLoss = 0.6542, diffLoss = 3.1996, kgLoss = 0.0179
2025-04-08 16:49:01.525662: Training Step 136/354: batchLoss = 0.5508, diffLoss = 2.6978, kgLoss = 0.0141
2025-04-08 16:49:03.145568: Training Step 137/354: batchLoss = 0.4565, diffLoss = 2.2346, kgLoss = 0.0120
2025-04-08 16:49:04.756794: Training Step 138/354: batchLoss = 0.4718, diffLoss = 2.3070, kgLoss = 0.0130
2025-04-08 16:49:06.375400: Training Step 139/354: batchLoss = 0.5613, diffLoss = 2.7410, kgLoss = 0.0163
2025-04-08 16:49:07.992712: Training Step 140/354: batchLoss = 0.7686, diffLoss = 3.7706, kgLoss = 0.0181
2025-04-08 16:49:09.618189: Training Step 141/354: batchLoss = 0.7105, diffLoss = 3.4785, kgLoss = 0.0185
2025-04-08 16:49:11.244141: Training Step 142/354: batchLoss = 0.5716, diffLoss = 2.8070, kgLoss = 0.0127
2025-04-08 16:49:12.867268: Training Step 143/354: batchLoss = 0.5731, diffLoss = 2.8066, kgLoss = 0.0147
2025-04-08 16:49:14.493567: Training Step 144/354: batchLoss = 0.5311, diffLoss = 2.6046, kgLoss = 0.0128
2025-04-08 16:49:16.119841: Training Step 145/354: batchLoss = 0.5167, diffLoss = 2.5243, kgLoss = 0.0148
2025-04-08 16:49:17.733948: Training Step 146/354: batchLoss = 0.5948, diffLoss = 2.9108, kgLoss = 0.0158
2025-04-08 16:49:19.357852: Training Step 147/354: batchLoss = 0.5827, diffLoss = 2.8544, kgLoss = 0.0148
2025-04-08 16:49:20.981683: Training Step 148/354: batchLoss = 0.5479, diffLoss = 2.6813, kgLoss = 0.0145
2025-04-08 16:49:22.598023: Training Step 149/354: batchLoss = 0.5482, diffLoss = 2.6830, kgLoss = 0.0144
2025-04-08 16:49:24.209160: Training Step 150/354: batchLoss = 0.5184, diffLoss = 2.5423, kgLoss = 0.0125
2025-04-08 16:49:25.831673: Training Step 151/354: batchLoss = 0.6382, diffLoss = 3.1212, kgLoss = 0.0174
2025-04-08 16:49:27.453886: Training Step 152/354: batchLoss = 0.5272, diffLoss = 2.5856, kgLoss = 0.0125
2025-04-08 16:49:29.074199: Training Step 153/354: batchLoss = 0.6794, diffLoss = 3.3280, kgLoss = 0.0172
2025-04-08 16:49:30.694891: Training Step 154/354: batchLoss = 0.5459, diffLoss = 2.6740, kgLoss = 0.0139
2025-04-08 16:49:32.319433: Training Step 155/354: batchLoss = 0.6379, diffLoss = 3.1320, kgLoss = 0.0144
2025-04-08 16:49:33.938345: Training Step 156/354: batchLoss = 0.5605, diffLoss = 2.7481, kgLoss = 0.0137
2025-04-08 16:49:35.559304: Training Step 157/354: batchLoss = 0.4489, diffLoss = 2.1977, kgLoss = 0.0117
2025-04-08 16:49:37.174089: Training Step 158/354: batchLoss = 0.4652, diffLoss = 2.2795, kgLoss = 0.0116
2025-04-08 16:49:38.789398: Training Step 159/354: batchLoss = 0.4465, diffLoss = 2.1923, kgLoss = 0.0100
2025-04-08 16:49:40.407943: Training Step 160/354: batchLoss = 0.7305, diffLoss = 3.5886, kgLoss = 0.0159
2025-04-08 16:49:42.030312: Training Step 161/354: batchLoss = 0.3930, diffLoss = 1.9174, kgLoss = 0.0119
2025-04-08 16:49:43.651774: Training Step 162/354: batchLoss = 0.7289, diffLoss = 3.5681, kgLoss = 0.0191
2025-04-08 16:49:45.268832: Training Step 163/354: batchLoss = 0.6865, diffLoss = 3.3580, kgLoss = 0.0187
2025-04-08 16:49:46.899279: Training Step 164/354: batchLoss = 0.4172, diffLoss = 2.0427, kgLoss = 0.0108
2025-04-08 16:49:48.523975: Training Step 165/354: batchLoss = 0.5923, diffLoss = 2.9067, kgLoss = 0.0137
2025-04-08 16:49:50.151139: Training Step 166/354: batchLoss = 0.5141, diffLoss = 2.5152, kgLoss = 0.0138
2025-04-08 16:49:51.766626: Training Step 167/354: batchLoss = 0.5648, diffLoss = 2.7631, kgLoss = 0.0152
2025-04-08 16:49:53.381444: Training Step 168/354: batchLoss = 0.5447, diffLoss = 2.6681, kgLoss = 0.0139
2025-04-08 16:49:55.004061: Training Step 169/354: batchLoss = 0.5328, diffLoss = 2.6073, kgLoss = 0.0142
2025-04-08 16:49:56.621770: Training Step 170/354: batchLoss = 0.7095, diffLoss = 3.4786, kgLoss = 0.0173
2025-04-08 16:49:58.243719: Training Step 171/354: batchLoss = 0.5078, diffLoss = 2.4790, kgLoss = 0.0150
2025-04-08 16:49:59.867580: Training Step 172/354: batchLoss = 0.5476, diffLoss = 2.6767, kgLoss = 0.0153
2025-04-08 16:50:01.504311: Training Step 173/354: batchLoss = 0.5718, diffLoss = 2.7982, kgLoss = 0.0152
2025-04-08 16:50:03.136674: Training Step 174/354: batchLoss = 0.4525, diffLoss = 2.2016, kgLoss = 0.0152
2025-04-08 16:50:04.759447: Training Step 175/354: batchLoss = 0.5236, diffLoss = 2.5593, kgLoss = 0.0146
2025-04-08 16:50:06.380179: Training Step 176/354: batchLoss = 0.5875, diffLoss = 2.8815, kgLoss = 0.0140
2025-04-08 16:50:08.009918: Training Step 177/354: batchLoss = 0.5945, diffLoss = 2.9124, kgLoss = 0.0150
2025-04-08 16:50:09.632709: Training Step 178/354: batchLoss = 0.8694, diffLoss = 4.2621, kgLoss = 0.0213
2025-04-08 16:50:11.258278: Training Step 179/354: batchLoss = 0.4796, diffLoss = 2.3393, kgLoss = 0.0146
2025-04-08 16:50:12.879091: Training Step 180/354: batchLoss = 0.5115, diffLoss = 2.5058, kgLoss = 0.0129
2025-04-08 16:50:14.500942: Training Step 181/354: batchLoss = 0.4673, diffLoss = 2.2859, kgLoss = 0.0127
2025-04-08 16:50:16.129646: Training Step 182/354: batchLoss = 0.5113, diffLoss = 2.4998, kgLoss = 0.0142
2025-04-08 16:50:17.754823: Training Step 183/354: batchLoss = 0.5522, diffLoss = 2.7070, kgLoss = 0.0135
2025-04-08 16:50:19.378983: Training Step 184/354: batchLoss = 0.6634, diffLoss = 3.2495, kgLoss = 0.0169
2025-04-08 16:50:21.009812: Training Step 185/354: batchLoss = 1.0157, diffLoss = 4.9815, kgLoss = 0.0243
2025-04-08 16:50:22.631642: Training Step 186/354: batchLoss = 0.7651, diffLoss = 3.7492, kgLoss = 0.0191
2025-04-08 16:50:24.254844: Training Step 187/354: batchLoss = 0.5534, diffLoss = 2.7097, kgLoss = 0.0143
2025-04-08 16:50:25.874478: Training Step 188/354: batchLoss = 0.5071, diffLoss = 2.4825, kgLoss = 0.0133
2025-04-08 16:50:27.497515: Training Step 189/354: batchLoss = 0.4498, diffLoss = 2.1998, kgLoss = 0.0123
2025-04-08 16:50:29.118346: Training Step 190/354: batchLoss = 0.5142, diffLoss = 2.5200, kgLoss = 0.0128
2025-04-08 16:50:30.744767: Training Step 191/354: batchLoss = 0.4700, diffLoss = 2.2988, kgLoss = 0.0129
2025-04-08 16:50:32.375807: Training Step 192/354: batchLoss = 0.5592, diffLoss = 2.7398, kgLoss = 0.0140
2025-04-08 16:50:34.008541: Training Step 193/354: batchLoss = 0.5332, diffLoss = 2.6085, kgLoss = 0.0144
2025-04-08 16:50:35.638407: Training Step 194/354: batchLoss = 0.6355, diffLoss = 3.1185, kgLoss = 0.0147
2025-04-08 16:50:37.266850: Training Step 195/354: batchLoss = 0.5416, diffLoss = 2.6540, kgLoss = 0.0134
2025-04-08 16:50:38.892753: Training Step 196/354: batchLoss = 0.6915, diffLoss = 3.3902, kgLoss = 0.0168
2025-04-08 16:50:40.511502: Training Step 197/354: batchLoss = 0.5672, diffLoss = 2.7709, kgLoss = 0.0162
2025-04-08 16:50:42.128192: Training Step 198/354: batchLoss = 0.6161, diffLoss = 3.0163, kgLoss = 0.0160
2025-04-08 16:50:43.753111: Training Step 199/354: batchLoss = 0.5963, diffLoss = 2.9201, kgLoss = 0.0153
2025-04-08 16:50:45.367818: Training Step 200/354: batchLoss = 0.4814, diffLoss = 2.3566, kgLoss = 0.0126
2025-04-08 16:50:46.993829: Training Step 201/354: batchLoss = 0.4618, diffLoss = 2.2613, kgLoss = 0.0119
2025-04-08 16:50:48.621247: Training Step 202/354: batchLoss = 0.6881, diffLoss = 3.3622, kgLoss = 0.0196
2025-04-08 16:50:50.246083: Training Step 203/354: batchLoss = 0.5382, diffLoss = 2.6368, kgLoss = 0.0136
2025-04-08 16:50:51.870034: Training Step 204/354: batchLoss = 0.6182, diffLoss = 3.0239, kgLoss = 0.0167
2025-04-08 16:50:53.494502: Training Step 205/354: batchLoss = 0.5324, diffLoss = 2.5996, kgLoss = 0.0157
2025-04-08 16:50:55.113704: Training Step 206/354: batchLoss = 0.4896, diffLoss = 2.4025, kgLoss = 0.0114
2025-04-08 16:50:56.730675: Training Step 207/354: batchLoss = 0.5756, diffLoss = 2.8246, kgLoss = 0.0134
2025-04-08 16:50:58.349577: Training Step 208/354: batchLoss = 0.4733, diffLoss = 2.3159, kgLoss = 0.0127
2025-04-08 16:50:59.978702: Training Step 209/354: batchLoss = 0.6985, diffLoss = 3.4203, kgLoss = 0.0181
2025-04-08 16:51:01.593817: Training Step 210/354: batchLoss = 0.5362, diffLoss = 2.6248, kgLoss = 0.0141
2025-04-08 16:51:03.211919: Training Step 211/354: batchLoss = 0.5632, diffLoss = 2.7549, kgLoss = 0.0153
2025-04-08 16:51:04.831944: Training Step 212/354: batchLoss = 0.5091, diffLoss = 2.4926, kgLoss = 0.0132
2025-04-08 16:51:06.460952: Training Step 213/354: batchLoss = 0.4768, diffLoss = 2.3388, kgLoss = 0.0113
2025-04-08 16:51:08.087781: Training Step 214/354: batchLoss = 0.6236, diffLoss = 3.0496, kgLoss = 0.0172
2025-04-08 16:51:09.715492: Training Step 215/354: batchLoss = 0.5568, diffLoss = 2.7267, kgLoss = 0.0143
2025-04-08 16:51:11.328594: Training Step 216/354: batchLoss = 0.6025, diffLoss = 2.9595, kgLoss = 0.0132
2025-04-08 16:51:12.945051: Training Step 217/354: batchLoss = 0.5305, diffLoss = 2.6047, kgLoss = 0.0119
2025-04-08 16:51:14.565525: Training Step 218/354: batchLoss = 0.4581, diffLoss = 2.2394, kgLoss = 0.0127
2025-04-08 16:51:16.178175: Training Step 219/354: batchLoss = 0.4767, diffLoss = 2.3256, kgLoss = 0.0145
2025-04-08 16:51:17.794403: Training Step 220/354: batchLoss = 0.4898, diffLoss = 2.3883, kgLoss = 0.0152
2025-04-08 16:51:19.424987: Training Step 221/354: batchLoss = 0.5334, diffLoss = 2.6072, kgLoss = 0.0150
2025-04-08 16:51:21.045491: Training Step 222/354: batchLoss = 0.5751, diffLoss = 2.8092, kgLoss = 0.0165
2025-04-08 16:51:22.676052: Training Step 223/354: batchLoss = 0.5309, diffLoss = 2.5996, kgLoss = 0.0137
2025-04-08 16:51:24.303702: Training Step 224/354: batchLoss = 0.6543, diffLoss = 3.2075, kgLoss = 0.0160
2025-04-08 16:51:25.929348: Training Step 225/354: batchLoss = 0.5440, diffLoss = 2.6637, kgLoss = 0.0141
2025-04-08 16:51:27.555609: Training Step 226/354: batchLoss = 0.4930, diffLoss = 2.4173, kgLoss = 0.0120
2025-04-08 16:51:29.180889: Training Step 227/354: batchLoss = 0.5415, diffLoss = 2.6530, kgLoss = 0.0136
2025-04-08 16:51:30.793740: Training Step 228/354: batchLoss = 0.5933, diffLoss = 2.9010, kgLoss = 0.0163
2025-04-08 16:51:32.414514: Training Step 229/354: batchLoss = 0.4474, diffLoss = 2.1917, kgLoss = 0.0113
2025-04-08 16:51:34.037428: Training Step 230/354: batchLoss = 0.5073, diffLoss = 2.4850, kgLoss = 0.0128
2025-04-08 16:51:35.664475: Training Step 231/354: batchLoss = 0.5630, diffLoss = 2.7535, kgLoss = 0.0154
2025-04-08 16:51:37.294349: Training Step 232/354: batchLoss = 0.5209, diffLoss = 2.5476, kgLoss = 0.0142
2025-04-08 16:51:38.920571: Training Step 233/354: batchLoss = 0.5959, diffLoss = 2.9147, kgLoss = 0.0162
2025-04-08 16:51:40.552166: Training Step 234/354: batchLoss = 0.5697, diffLoss = 2.7932, kgLoss = 0.0138
2025-04-08 16:51:42.176574: Training Step 235/354: batchLoss = 0.5682, diffLoss = 2.7815, kgLoss = 0.0148
2025-04-08 16:51:43.802511: Training Step 236/354: batchLoss = 0.5422, diffLoss = 2.6521, kgLoss = 0.0147
2025-04-08 16:51:45.417580: Training Step 237/354: batchLoss = 0.5615, diffLoss = 2.7500, kgLoss = 0.0144
2025-04-08 16:51:47.032320: Training Step 238/354: batchLoss = 0.5864, diffLoss = 2.8741, kgLoss = 0.0145
2025-04-08 16:51:48.650957: Training Step 239/354: batchLoss = 0.6830, diffLoss = 3.3453, kgLoss = 0.0174
2025-04-08 16:51:50.264712: Training Step 240/354: batchLoss = 0.5555, diffLoss = 2.7209, kgLoss = 0.0141
2025-04-08 16:51:51.890206: Training Step 241/354: batchLoss = 0.6093, diffLoss = 2.9831, kgLoss = 0.0159
2025-04-08 16:51:53.510150: Training Step 242/354: batchLoss = 0.5848, diffLoss = 2.8656, kgLoss = 0.0146
2025-04-08 16:51:55.155595: Training Step 243/354: batchLoss = 0.5822, diffLoss = 2.8593, kgLoss = 0.0129
2025-04-08 16:51:56.778597: Training Step 244/354: batchLoss = 0.5222, diffLoss = 2.5567, kgLoss = 0.0136
2025-04-08 16:51:58.398064: Training Step 245/354: batchLoss = 0.4802, diffLoss = 2.3529, kgLoss = 0.0121
2025-04-08 16:52:00.027685: Training Step 246/354: batchLoss = 0.6450, diffLoss = 3.1572, kgLoss = 0.0170
2025-04-08 16:52:01.652060: Training Step 247/354: batchLoss = 0.6551, diffLoss = 3.2115, kgLoss = 0.0159
2025-04-08 16:52:03.274621: Training Step 248/354: batchLoss = 0.5392, diffLoss = 2.6419, kgLoss = 0.0135
2025-04-08 16:52:04.896846: Training Step 249/354: batchLoss = 0.5964, diffLoss = 2.9177, kgLoss = 0.0160
2025-04-08 16:52:06.513182: Training Step 250/354: batchLoss = 0.5581, diffLoss = 2.7372, kgLoss = 0.0133
2025-04-08 16:52:08.141764: Training Step 251/354: batchLoss = 0.5920, diffLoss = 2.8998, kgLoss = 0.0150
2025-04-08 16:52:09.770547: Training Step 252/354: batchLoss = 0.6001, diffLoss = 2.9432, kgLoss = 0.0143
2025-04-08 16:52:11.395080: Training Step 253/354: batchLoss = 0.5878, diffLoss = 2.8773, kgLoss = 0.0155
2025-04-08 16:52:13.023316: Training Step 254/354: batchLoss = 0.4811, diffLoss = 2.3585, kgLoss = 0.0117
2025-04-08 16:52:14.652684: Training Step 255/354: batchLoss = 0.5098, diffLoss = 2.4895, kgLoss = 0.0148
2025-04-08 16:52:16.280035: Training Step 256/354: batchLoss = 0.6266, diffLoss = 3.0709, kgLoss = 0.0155
2025-04-08 16:52:17.897254: Training Step 257/354: batchLoss = 0.7471, diffLoss = 3.6572, kgLoss = 0.0196
2025-04-08 16:52:19.516960: Training Step 258/354: batchLoss = 0.6776, diffLoss = 3.3208, kgLoss = 0.0168
2025-04-08 16:52:21.134629: Training Step 259/354: batchLoss = 0.5505, diffLoss = 2.7008, kgLoss = 0.0129
2025-04-08 16:52:22.756696: Training Step 260/354: batchLoss = 0.5240, diffLoss = 2.5630, kgLoss = 0.0142
2025-04-08 16:52:24.381836: Training Step 261/354: batchLoss = 1.1574, diffLoss = 5.6663, kgLoss = 0.0302
2025-04-08 16:52:26.014735: Training Step 262/354: batchLoss = 0.6600, diffLoss = 3.2331, kgLoss = 0.0168
2025-04-08 16:52:27.643452: Training Step 263/354: batchLoss = 0.5599, diffLoss = 2.7365, kgLoss = 0.0158
2025-04-08 16:52:29.265809: Training Step 264/354: batchLoss = 0.6519, diffLoss = 3.1910, kgLoss = 0.0171
2025-04-08 16:52:30.890861: Training Step 265/354: batchLoss = 0.5102, diffLoss = 2.4879, kgLoss = 0.0157
2025-04-08 16:52:32.505604: Training Step 266/354: batchLoss = 0.6288, diffLoss = 3.0866, kgLoss = 0.0144
2025-04-08 16:52:34.125567: Training Step 267/354: batchLoss = 0.6819, diffLoss = 3.3501, kgLoss = 0.0149
2025-04-08 16:52:35.742614: Training Step 268/354: batchLoss = 0.6215, diffLoss = 3.0425, kgLoss = 0.0163
2025-04-08 16:52:37.362468: Training Step 269/354: batchLoss = 0.4662, diffLoss = 2.2757, kgLoss = 0.0138
2025-04-08 16:52:38.986987: Training Step 270/354: batchLoss = 0.6577, diffLoss = 3.2200, kgLoss = 0.0171
2025-04-08 16:52:40.607256: Training Step 271/354: batchLoss = 0.5367, diffLoss = 2.6331, kgLoss = 0.0126
2025-04-08 16:52:42.232082: Training Step 272/354: batchLoss = 0.5732, diffLoss = 2.8099, kgLoss = 0.0141
2025-04-08 16:52:43.858367: Training Step 273/354: batchLoss = 0.6204, diffLoss = 3.0373, kgLoss = 0.0161
2025-04-08 16:52:45.481186: Training Step 274/354: batchLoss = 0.5718, diffLoss = 2.8046, kgLoss = 0.0136
2025-04-08 16:52:47.104416: Training Step 275/354: batchLoss = 0.5899, diffLoss = 2.8925, kgLoss = 0.0142
2025-04-08 16:52:48.720390: Training Step 276/354: batchLoss = 0.5355, diffLoss = 2.6212, kgLoss = 0.0141
2025-04-08 16:52:50.330107: Training Step 277/354: batchLoss = 0.5338, diffLoss = 2.6189, kgLoss = 0.0125
2025-04-08 16:52:51.943550: Training Step 278/354: batchLoss = 0.5137, diffLoss = 2.5198, kgLoss = 0.0122
2025-04-08 16:52:53.561570: Training Step 279/354: batchLoss = 0.6111, diffLoss = 2.9885, kgLoss = 0.0168
2025-04-08 16:52:55.176710: Training Step 280/354: batchLoss = 0.5428, diffLoss = 2.6601, kgLoss = 0.0134
2025-04-08 16:52:56.796219: Training Step 281/354: batchLoss = 0.5144, diffLoss = 2.5188, kgLoss = 0.0133
2025-04-08 16:52:58.422062: Training Step 282/354: batchLoss = 0.5577, diffLoss = 2.7267, kgLoss = 0.0155
2025-04-08 16:53:00.049487: Training Step 283/354: batchLoss = 0.6478, diffLoss = 3.1760, kgLoss = 0.0158
2025-04-08 16:53:01.671684: Training Step 284/354: batchLoss = 0.5925, diffLoss = 2.9080, kgLoss = 0.0136
2025-04-08 16:53:03.300407: Training Step 285/354: batchLoss = 0.4692, diffLoss = 2.2943, kgLoss = 0.0130
2025-04-08 16:53:04.911256: Training Step 286/354: batchLoss = 0.5143, diffLoss = 2.5192, kgLoss = 0.0131
2025-04-08 16:53:06.526836: Training Step 287/354: batchLoss = 0.4631, diffLoss = 2.2472, kgLoss = 0.0170
2025-04-08 16:53:08.143543: Training Step 288/354: batchLoss = 0.4734, diffLoss = 2.3164, kgLoss = 0.0127
2025-04-08 16:53:09.765294: Training Step 289/354: batchLoss = 0.6238, diffLoss = 3.0565, kgLoss = 0.0157
2025-04-08 16:53:11.393871: Training Step 290/354: batchLoss = 0.8084, diffLoss = 3.9614, kgLoss = 0.0202
2025-04-08 16:53:13.019002: Training Step 291/354: batchLoss = 0.5315, diffLoss = 2.5987, kgLoss = 0.0147
2025-04-08 16:53:14.646347: Training Step 292/354: batchLoss = 0.5966, diffLoss = 2.9222, kgLoss = 0.0152
2025-04-08 16:53:16.276407: Training Step 293/354: batchLoss = 0.5529, diffLoss = 2.7058, kgLoss = 0.0146
2025-04-08 16:53:17.903980: Training Step 294/354: batchLoss = 0.6190, diffLoss = 3.0379, kgLoss = 0.0143
2025-04-08 16:53:19.531040: Training Step 295/354: batchLoss = 0.4692, diffLoss = 2.3005, kgLoss = 0.0114
2025-04-08 16:53:21.151662: Training Step 296/354: batchLoss = 0.6077, diffLoss = 2.9759, kgLoss = 0.0156
2025-04-08 16:53:22.771030: Training Step 297/354: batchLoss = 0.5258, diffLoss = 2.5760, kgLoss = 0.0133
2025-04-08 16:53:24.387016: Training Step 298/354: batchLoss = 0.7440, diffLoss = 3.6551, kgLoss = 0.0163
2025-04-08 16:53:26.014640: Training Step 299/354: batchLoss = 0.5027, diffLoss = 2.4690, kgLoss = 0.0111
2025-04-08 16:53:27.649993: Training Step 300/354: batchLoss = 0.5333, diffLoss = 2.6106, kgLoss = 0.0140
2025-04-08 16:53:29.267834: Training Step 301/354: batchLoss = 0.5181, diffLoss = 2.5378, kgLoss = 0.0132
2025-04-08 16:53:30.896453: Training Step 302/354: batchLoss = 0.5757, diffLoss = 2.8194, kgLoss = 0.0148
2025-04-08 16:53:32.534401: Training Step 303/354: batchLoss = 0.5441, diffLoss = 2.6636, kgLoss = 0.0143
2025-04-08 16:53:34.167367: Training Step 304/354: batchLoss = 0.4978, diffLoss = 2.4363, kgLoss = 0.0132
2025-04-08 16:53:35.790980: Training Step 305/354: batchLoss = 0.5560, diffLoss = 2.7216, kgLoss = 0.0146
2025-04-08 16:53:37.410583: Training Step 306/354: batchLoss = 0.5427, diffLoss = 2.6587, kgLoss = 0.0138
2025-04-08 16:53:39.036063: Training Step 307/354: batchLoss = 0.5623, diffLoss = 2.7567, kgLoss = 0.0137
2025-04-08 16:53:40.661337: Training Step 308/354: batchLoss = 0.5887, diffLoss = 2.8849, kgLoss = 0.0147
2025-04-08 16:53:42.288781: Training Step 309/354: batchLoss = 0.6093, diffLoss = 2.9901, kgLoss = 0.0141
2025-04-08 16:53:43.919672: Training Step 310/354: batchLoss = 0.5792, diffLoss = 2.8389, kgLoss = 0.0142
2025-04-08 16:53:45.547716: Training Step 311/354: batchLoss = 0.5012, diffLoss = 2.4541, kgLoss = 0.0130
2025-04-08 16:53:47.175868: Training Step 312/354: batchLoss = 0.5330, diffLoss = 2.6099, kgLoss = 0.0137
2025-04-08 16:53:48.801856: Training Step 313/354: batchLoss = 0.6034, diffLoss = 2.9508, kgLoss = 0.0165
2025-04-08 16:53:50.427541: Training Step 314/354: batchLoss = 0.7164, diffLoss = 3.5110, kgLoss = 0.0178
2025-04-08 16:53:52.052462: Training Step 315/354: batchLoss = 0.4687, diffLoss = 2.2892, kgLoss = 0.0136
2025-04-08 16:53:53.671456: Training Step 316/354: batchLoss = 0.5878, diffLoss = 2.8790, kgLoss = 0.0150
2025-04-08 16:53:55.289192: Training Step 317/354: batchLoss = 0.5745, diffLoss = 2.8133, kgLoss = 0.0148
2025-04-08 16:53:56.910281: Training Step 318/354: batchLoss = 0.4996, diffLoss = 2.4476, kgLoss = 0.0126
2025-04-08 16:53:58.539060: Training Step 319/354: batchLoss = 0.4915, diffLoss = 2.4130, kgLoss = 0.0111
2025-04-08 16:54:00.163388: Training Step 320/354: batchLoss = 0.5485, diffLoss = 2.6829, kgLoss = 0.0149
2025-04-08 16:54:01.783837: Training Step 321/354: batchLoss = 0.5304, diffLoss = 2.5971, kgLoss = 0.0137
2025-04-08 16:54:03.414637: Training Step 322/354: batchLoss = 0.5667, diffLoss = 2.7715, kgLoss = 0.0155
2025-04-08 16:54:05.044859: Training Step 323/354: batchLoss = 0.6688, diffLoss = 3.2839, kgLoss = 0.0151
2025-04-08 16:54:06.672477: Training Step 324/354: batchLoss = 0.5366, diffLoss = 2.6228, kgLoss = 0.0150
2025-04-08 16:54:08.289342: Training Step 325/354: batchLoss = 0.5428, diffLoss = 2.6652, kgLoss = 0.0122
2025-04-08 16:54:09.910166: Training Step 326/354: batchLoss = 0.6049, diffLoss = 2.9656, kgLoss = 0.0148
2025-04-08 16:54:11.527506: Training Step 327/354: batchLoss = 0.5095, diffLoss = 2.4900, kgLoss = 0.0144
2025-04-08 16:54:13.150085: Training Step 328/354: batchLoss = 0.6852, diffLoss = 3.3638, kgLoss = 0.0155
2025-04-08 16:54:14.776814: Training Step 329/354: batchLoss = 0.4789, diffLoss = 2.3368, kgLoss = 0.0144
2025-04-08 16:54:16.399998: Training Step 330/354: batchLoss = 0.5788, diffLoss = 2.8384, kgLoss = 0.0139
2025-04-08 16:54:18.023421: Training Step 331/354: batchLoss = 0.7675, diffLoss = 3.7664, kgLoss = 0.0178
2025-04-08 16:54:19.653545: Training Step 332/354: batchLoss = 0.6353, diffLoss = 3.1161, kgLoss = 0.0151
2025-04-08 16:54:21.282543: Training Step 333/354: batchLoss = 0.5506, diffLoss = 2.6978, kgLoss = 0.0138
2025-04-08 16:54:22.906884: Training Step 334/354: batchLoss = 0.5511, diffLoss = 2.6922, kgLoss = 0.0159
2025-04-08 16:54:24.528033: Training Step 335/354: batchLoss = 0.5145, diffLoss = 2.5205, kgLoss = 0.0130
2025-04-08 16:54:26.149827: Training Step 336/354: batchLoss = 0.6499, diffLoss = 3.1894, kgLoss = 0.0150
2025-04-08 16:54:27.759955: Training Step 337/354: batchLoss = 0.6267, diffLoss = 3.0697, kgLoss = 0.0160
2025-04-08 16:54:29.378569: Training Step 338/354: batchLoss = 0.6027, diffLoss = 2.9534, kgLoss = 0.0150
2025-04-08 16:54:30.997230: Training Step 339/354: batchLoss = 0.5706, diffLoss = 2.7971, kgLoss = 0.0140
2025-04-08 16:54:32.625770: Training Step 340/354: batchLoss = 0.5951, diffLoss = 2.9223, kgLoss = 0.0133
2025-04-08 16:54:34.255144: Training Step 341/354: batchLoss = 0.5257, diffLoss = 2.5719, kgLoss = 0.0141
2025-04-08 16:54:35.882346: Training Step 342/354: batchLoss = 0.7193, diffLoss = 3.5244, kgLoss = 0.0181
2025-04-08 16:54:37.502639: Training Step 343/354: batchLoss = 0.6628, diffLoss = 3.2534, kgLoss = 0.0152
2025-04-08 16:54:39.121259: Training Step 344/354: batchLoss = 0.5450, diffLoss = 2.6700, kgLoss = 0.0137
2025-04-08 16:54:40.736621: Training Step 345/354: batchLoss = 0.5339, diffLoss = 2.6128, kgLoss = 0.0142
2025-04-08 16:54:42.345722: Training Step 346/354: batchLoss = 0.6022, diffLoss = 2.9496, kgLoss = 0.0154
2025-04-08 16:54:43.957120: Training Step 347/354: batchLoss = 0.4680, diffLoss = 2.2917, kgLoss = 0.0121
2025-04-08 16:54:45.576185: Training Step 348/354: batchLoss = 0.5271, diffLoss = 2.5811, kgLoss = 0.0136
2025-04-08 16:54:47.201584: Training Step 349/354: batchLoss = 0.5229, diffLoss = 2.5569, kgLoss = 0.0144
2025-04-08 16:54:48.834000: Training Step 350/354: batchLoss = 0.5458, diffLoss = 2.6761, kgLoss = 0.0132
2025-04-08 16:54:50.455223: Training Step 351/354: batchLoss = 0.5573, diffLoss = 2.7238, kgLoss = 0.0156
2025-04-08 16:54:52.056772: Training Step 352/354: batchLoss = 0.5883, diffLoss = 2.8838, kgLoss = 0.0144
2025-04-08 16:54:53.468986: Training Step 353/354: batchLoss = 0.3781, diffLoss = 1.8359, kgLoss = 0.0137
2025-04-08 16:54:53.562758: 
2025-04-08 16:54:53.563409: Epoch 20/1000, Train: epLoss = 1.0073, epDfLoss = 4.9320, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 16:54:54.899741: Steps 0/138: batch_recall = 47.12, batch_ndcg = 26.80 
2025-04-08 16:54:56.229547: Steps 1/138: batch_recall = 47.11, batch_ndcg = 27.82 
2025-04-08 16:54:57.535191: Steps 2/138: batch_recall = 58.90, batch_ndcg = 36.28 
2025-04-08 16:54:58.854658: Steps 3/138: batch_recall = 59.90, batch_ndcg = 34.48 
2025-04-08 16:55:00.163626: Steps 4/138: batch_recall = 69.67, batch_ndcg = 41.42 
2025-04-08 16:55:01.483687: Steps 5/138: batch_recall = 58.77, batch_ndcg = 32.08 
2025-04-08 16:55:02.805338: Steps 6/138: batch_recall = 52.46, batch_ndcg = 31.83 
2025-04-08 16:55:04.125034: Steps 7/138: batch_recall = 63.85, batch_ndcg = 41.72 
2025-04-08 16:55:05.432965: Steps 8/138: batch_recall = 63.97, batch_ndcg = 40.28 
2025-04-08 16:55:06.748346: Steps 9/138: batch_recall = 55.95, batch_ndcg = 33.25 
2025-04-08 16:55:08.036837: Steps 10/138: batch_recall = 56.03, batch_ndcg = 30.99 
2025-04-08 16:55:09.334660: Steps 11/138: batch_recall = 57.71, batch_ndcg = 33.04 
2025-04-08 16:55:10.635976: Steps 12/138: batch_recall = 52.01, batch_ndcg = 28.91 
2025-04-08 16:55:11.930631: Steps 13/138: batch_recall = 53.75, batch_ndcg = 31.24 
2025-04-08 16:55:13.225714: Steps 14/138: batch_recall = 52.86, batch_ndcg = 31.00 
2025-04-08 16:55:14.525606: Steps 15/138: batch_recall = 48.70, batch_ndcg = 29.63 
2025-04-08 16:55:15.816947: Steps 16/138: batch_recall = 60.86, batch_ndcg = 33.74 
2025-04-08 16:55:17.099310: Steps 17/138: batch_recall = 57.59, batch_ndcg = 33.07 
2025-04-08 16:55:18.393365: Steps 18/138: batch_recall = 51.60, batch_ndcg = 32.39 
2025-04-08 16:55:19.704031: Steps 19/138: batch_recall = 54.89, batch_ndcg = 32.49 
2025-04-08 16:55:21.011328: Steps 20/138: batch_recall = 60.72, batch_ndcg = 36.23 
2025-04-08 16:55:22.316660: Steps 21/138: batch_recall = 71.52, batch_ndcg = 41.47 
2025-04-08 16:55:23.612874: Steps 22/138: batch_recall = 58.53, batch_ndcg = 33.95 
2025-04-08 16:55:24.917992: Steps 23/138: batch_recall = 51.23, batch_ndcg = 30.16 
2025-04-08 16:55:26.217988: Steps 24/138: batch_recall = 58.56, batch_ndcg = 31.59 
2025-04-08 16:55:27.523904: Steps 25/138: batch_recall = 61.78, batch_ndcg = 35.99 
2025-04-08 16:55:28.813465: Steps 26/138: batch_recall = 57.00, batch_ndcg = 32.98 
2025-04-08 16:55:30.103017: Steps 27/138: batch_recall = 63.13, batch_ndcg = 34.52 
2025-04-08 16:55:31.389388: Steps 28/138: batch_recall = 59.86, batch_ndcg = 33.15 
2025-04-08 16:55:32.669442: Steps 29/138: batch_recall = 63.12, batch_ndcg = 32.84 
2025-04-08 16:55:33.970713: Steps 30/138: batch_recall = 57.28, batch_ndcg = 34.20 
2025-04-08 16:55:35.264687: Steps 31/138: batch_recall = 41.50, batch_ndcg = 24.37 
2025-04-08 16:55:36.561076: Steps 32/138: batch_recall = 52.85, batch_ndcg = 31.04 
2025-04-08 16:55:37.849323: Steps 33/138: batch_recall = 60.45, batch_ndcg = 33.46 
2025-04-08 16:55:39.136831: Steps 34/138: batch_recall = 54.50, batch_ndcg = 28.74 
2025-04-08 16:55:40.438095: Steps 35/138: batch_recall = 51.35, batch_ndcg = 30.07 
2025-04-08 16:55:41.728304: Steps 36/138: batch_recall = 49.06, batch_ndcg = 27.89 
2025-04-08 16:55:43.027592: Steps 37/138: batch_recall = 59.67, batch_ndcg = 34.56 
2025-04-08 16:55:44.311945: Steps 38/138: batch_recall = 58.63, batch_ndcg = 32.50 
2025-04-08 16:55:45.586403: Steps 39/138: batch_recall = 67.42, batch_ndcg = 38.72 
2025-04-08 16:55:46.877787: Steps 40/138: batch_recall = 59.62, batch_ndcg = 31.24 
2025-04-08 16:55:48.154261: Steps 41/138: batch_recall = 60.48, batch_ndcg = 33.49 
2025-04-08 16:55:49.425818: Steps 42/138: batch_recall = 53.89, batch_ndcg = 29.89 
2025-04-08 16:55:50.704893: Steps 43/138: batch_recall = 56.52, batch_ndcg = 34.91 
2025-04-08 16:55:51.991475: Steps 44/138: batch_recall = 54.40, batch_ndcg = 30.01 
2025-04-08 16:55:53.276992: Steps 45/138: batch_recall = 61.28, batch_ndcg = 34.61 
2025-04-08 16:55:54.558421: Steps 46/138: batch_recall = 61.24, batch_ndcg = 35.77 
2025-04-08 16:55:55.843791: Steps 47/138: batch_recall = 55.31, batch_ndcg = 32.21 
2025-04-08 16:55:57.138059: Steps 48/138: batch_recall = 59.05, batch_ndcg = 34.40 
2025-04-08 16:55:58.429358: Steps 49/138: batch_recall = 67.00, batch_ndcg = 37.49 
2025-04-08 16:55:59.712784: Steps 50/138: batch_recall = 60.71, batch_ndcg = 31.77 
2025-04-08 16:56:00.997226: Steps 51/138: batch_recall = 62.86, batch_ndcg = 36.26 
2025-04-08 16:56:02.271713: Steps 52/138: batch_recall = 65.38, batch_ndcg = 42.28 
2025-04-08 16:56:03.549363: Steps 53/138: batch_recall = 69.15, batch_ndcg = 35.27 
2025-04-08 16:56:04.834589: Steps 54/138: batch_recall = 67.97, batch_ndcg = 39.35 
2025-04-08 16:56:06.116205: Steps 55/138: batch_recall = 62.24, batch_ndcg = 33.44 
2025-04-08 16:56:07.407681: Steps 56/138: batch_recall = 61.98, batch_ndcg = 34.78 
2025-04-08 16:56:08.713694: Steps 57/138: batch_recall = 58.07, batch_ndcg = 32.53 
2025-04-08 16:56:10.004636: Steps 58/138: batch_recall = 70.86, batch_ndcg = 37.66 
2025-04-08 16:56:11.299824: Steps 59/138: batch_recall = 70.05, batch_ndcg = 41.35 
2025-04-08 16:56:12.593968: Steps 60/138: batch_recall = 67.84, batch_ndcg = 37.45 
2025-04-08 16:56:13.889872: Steps 61/138: batch_recall = 61.49, batch_ndcg = 34.80 
2025-04-08 16:56:15.180199: Steps 62/138: batch_recall = 84.60, batch_ndcg = 43.66 
2025-04-08 16:56:16.468622: Steps 63/138: batch_recall = 75.05, batch_ndcg = 43.17 
2025-04-08 16:56:17.744015: Steps 64/138: batch_recall = 61.72, batch_ndcg = 33.28 
2025-04-08 16:56:19.033205: Steps 65/138: batch_recall = 88.08, batch_ndcg = 47.71 
2025-04-08 16:56:20.310240: Steps 66/138: batch_recall = 70.95, batch_ndcg = 40.97 
2025-04-08 16:56:21.585616: Steps 67/138: batch_recall = 76.99, batch_ndcg = 47.30 
2025-04-08 16:56:22.884321: Steps 68/138: batch_recall = 62.81, batch_ndcg = 33.77 
2025-04-08 16:56:24.168107: Steps 69/138: batch_recall = 88.88, batch_ndcg = 51.84 
2025-04-08 16:56:25.458187: Steps 70/138: batch_recall = 78.55, batch_ndcg = 45.65 
2025-04-08 16:56:26.744953: Steps 71/138: batch_recall = 89.82, batch_ndcg = 51.64 
2025-04-08 16:56:28.028164: Steps 72/138: batch_recall = 86.21, batch_ndcg = 49.55 
2025-04-08 16:56:29.317502: Steps 73/138: batch_recall = 86.85, batch_ndcg = 48.21 
2025-04-08 16:56:30.600387: Steps 74/138: batch_recall = 80.48, batch_ndcg = 50.12 
2025-04-08 16:56:31.878273: Steps 75/138: batch_recall = 87.89, batch_ndcg = 50.68 
2025-04-08 16:56:33.152940: Steps 76/138: batch_recall = 97.75, batch_ndcg = 56.17 
2025-04-08 16:56:34.422573: Steps 77/138: batch_recall = 87.59, batch_ndcg = 49.48 
2025-04-08 16:56:35.696645: Steps 78/138: batch_recall = 92.11, batch_ndcg = 49.27 
2025-04-08 16:56:36.968925: Steps 79/138: batch_recall = 94.04, batch_ndcg = 49.72 
2025-04-08 16:56:38.260651: Steps 80/138: batch_recall = 73.07, batch_ndcg = 39.21 
2025-04-08 16:56:39.538994: Steps 81/138: batch_recall = 79.67, batch_ndcg = 46.91 
2025-04-08 16:56:40.812997: Steps 82/138: batch_recall = 91.45, batch_ndcg = 53.09 
2025-04-08 16:56:42.095658: Steps 83/138: batch_recall = 84.05, batch_ndcg = 48.57 
2025-04-08 16:56:43.371004: Steps 84/138: batch_recall = 101.61, batch_ndcg = 58.04 
2025-04-08 16:56:44.660729: Steps 85/138: batch_recall = 105.41, batch_ndcg = 61.07 
2025-04-08 16:56:45.945638: Steps 86/138: batch_recall = 120.16, batch_ndcg = 71.48 
2025-04-08 16:56:47.225163: Steps 87/138: batch_recall = 108.55, batch_ndcg = 56.77 
2025-04-08 16:56:48.505362: Steps 88/138: batch_recall = 101.74, batch_ndcg = 57.35 
2025-04-08 16:56:49.776263: Steps 89/138: batch_recall = 119.09, batch_ndcg = 67.75 
2025-04-08 16:56:51.047544: Steps 90/138: batch_recall = 104.71, batch_ndcg = 58.52 
2025-04-08 16:56:52.320068: Steps 91/138: batch_recall = 118.06, batch_ndcg = 65.43 
2025-04-08 16:56:53.594847: Steps 92/138: batch_recall = 119.14, batch_ndcg = 63.90 
2025-04-08 16:56:54.883411: Steps 93/138: batch_recall = 121.02, batch_ndcg = 68.97 
2025-04-08 16:56:56.174528: Steps 94/138: batch_recall = 129.99, batch_ndcg = 67.22 
2025-04-08 16:56:57.480889: Steps 95/138: batch_recall = 113.98, batch_ndcg = 67.09 
2025-04-08 16:56:58.759773: Steps 96/138: batch_recall = 135.12, batch_ndcg = 80.57 
2025-04-08 16:57:00.047206: Steps 97/138: batch_recall = 150.57, batch_ndcg = 92.37 
2025-04-08 16:57:01.324818: Steps 98/138: batch_recall = 112.93, batch_ndcg = 64.44 
2025-04-08 16:57:02.596428: Steps 99/138: batch_recall = 128.22, batch_ndcg = 72.29 
2025-04-08 16:57:03.869987: Steps 100/138: batch_recall = 132.50, batch_ndcg = 72.97 
2025-04-08 16:57:05.132628: Steps 101/138: batch_recall = 127.22, batch_ndcg = 70.06 
2025-04-08 16:57:06.399732: Steps 102/138: batch_recall = 126.94, batch_ndcg = 73.57 
2025-04-08 16:57:07.667276: Steps 103/138: batch_recall = 143.43, batch_ndcg = 80.16 
2025-04-08 16:57:08.935495: Steps 104/138: batch_recall = 135.34, batch_ndcg = 78.32 
2025-04-08 16:57:10.211914: Steps 105/138: batch_recall = 119.58, batch_ndcg = 66.93 
2025-04-08 16:57:11.487812: Steps 106/138: batch_recall = 104.99, batch_ndcg = 59.46 
2025-04-08 16:57:12.764805: Steps 107/138: batch_recall = 117.87, batch_ndcg = 65.31 
2025-04-08 16:57:14.052227: Steps 108/138: batch_recall = 119.41, batch_ndcg = 69.96 
2025-04-08 16:57:15.332521: Steps 109/138: batch_recall = 136.61, batch_ndcg = 76.67 
2025-04-08 16:57:16.607036: Steps 110/138: batch_recall = 125.29, batch_ndcg = 65.52 
2025-04-08 16:57:17.878711: Steps 111/138: batch_recall = 140.58, batch_ndcg = 83.86 
2025-04-08 16:57:19.150997: Steps 112/138: batch_recall = 166.13, batch_ndcg = 92.10 
2025-04-08 16:57:20.405047: Steps 113/138: batch_recall = 131.64, batch_ndcg = 73.28 
2025-04-08 16:57:21.657625: Steps 114/138: batch_recall = 126.12, batch_ndcg = 71.72 
2025-04-08 16:57:22.913877: Steps 115/138: batch_recall = 118.79, batch_ndcg = 62.80 
2025-04-08 16:57:24.171234: Steps 116/138: batch_recall = 126.18, batch_ndcg = 66.90 
2025-04-08 16:57:25.447154: Steps 117/138: batch_recall = 117.56, batch_ndcg = 67.20 
2025-04-08 16:57:26.731766: Steps 118/138: batch_recall = 125.12, batch_ndcg = 70.24 
2025-04-08 16:57:28.008139: Steps 119/138: batch_recall = 138.36, batch_ndcg = 74.55 
2025-04-08 16:57:29.294023: Steps 120/138: batch_recall = 127.99, batch_ndcg = 70.88 
2025-04-08 16:57:30.579056: Steps 121/138: batch_recall = 152.17, batch_ndcg = 79.50 
2025-04-08 16:57:31.858082: Steps 122/138: batch_recall = 149.73, batch_ndcg = 82.50 
2025-04-08 16:57:33.147592: Steps 123/138: batch_recall = 132.26, batch_ndcg = 74.02 
2025-04-08 16:57:34.427393: Steps 124/138: batch_recall = 155.09, batch_ndcg = 94.15 
2025-04-08 16:57:35.691532: Steps 125/138: batch_recall = 131.75, batch_ndcg = 72.96 
2025-04-08 16:57:36.953466: Steps 126/138: batch_recall = 158.64, batch_ndcg = 90.53 
2025-04-08 16:57:38.227122: Steps 127/138: batch_recall = 143.34, batch_ndcg = 81.38 
2025-04-08 16:57:39.504354: Steps 128/138: batch_recall = 130.60, batch_ndcg = 72.10 
2025-04-08 16:57:40.773160: Steps 129/138: batch_recall = 161.66, batch_ndcg = 92.12 
2025-04-08 16:57:42.050401: Steps 130/138: batch_recall = 133.41, batch_ndcg = 70.76 
2025-04-08 16:57:43.327437: Steps 131/138: batch_recall = 150.80, batch_ndcg = 86.22 
2025-04-08 16:57:44.596283: Steps 132/138: batch_recall = 148.00, batch_ndcg = 85.43 
2025-04-08 16:57:45.872648: Steps 133/138: batch_recall = 148.35, batch_ndcg = 85.42 
2025-04-08 16:57:47.147639: Steps 134/138: batch_recall = 142.46, batch_ndcg = 82.18 
2025-04-08 16:57:48.429319: Steps 135/138: batch_recall = 169.86, batch_ndcg = 97.32 
2025-04-08 16:57:49.709854: Steps 136/138: batch_recall = 153.51, batch_ndcg = 78.34 
2025-04-08 16:57:50.977863: Steps 137/138: batch_recall = 142.12, batch_ndcg = 87.10 
2025-04-08 16:57:50.978431: Epoch 20/1000, Test: Recall = 0.1781, NDCG = 0.1009  

2025-04-08 16:57:52.748507: Training Step 0/354: batchLoss = 0.5860, diffLoss = 2.8750, kgLoss = 0.0137
2025-04-08 16:57:54.368237: Training Step 1/354: batchLoss = 0.6470, diffLoss = 3.1693, kgLoss = 0.0165
2025-04-08 16:57:55.992273: Training Step 2/354: batchLoss = 0.8802, diffLoss = 4.3005, kgLoss = 0.0251
2025-04-08 16:57:57.618379: Training Step 3/354: batchLoss = 0.5461, diffLoss = 2.6704, kgLoss = 0.0151
2025-04-08 16:57:59.247182: Training Step 4/354: batchLoss = 0.5702, diffLoss = 2.7866, kgLoss = 0.0161
2025-04-08 16:58:00.875745: Training Step 5/354: batchLoss = 0.6008, diffLoss = 2.9461, kgLoss = 0.0144
2025-04-08 16:58:02.514287: Training Step 6/354: batchLoss = 0.6923, diffLoss = 3.3996, kgLoss = 0.0154
2025-04-08 16:58:04.148070: Training Step 7/354: batchLoss = 0.6300, diffLoss = 3.0870, kgLoss = 0.0158
2025-04-08 16:58:05.775894: Training Step 8/354: batchLoss = 0.5470, diffLoss = 2.6796, kgLoss = 0.0138
2025-04-08 16:58:07.397232: Training Step 9/354: batchLoss = 0.6513, diffLoss = 3.1976, kgLoss = 0.0147
2025-04-08 16:58:09.020821: Training Step 10/354: batchLoss = 0.4907, diffLoss = 2.3976, kgLoss = 0.0140
2025-04-08 16:58:10.640505: Training Step 11/354: batchLoss = 0.5192, diffLoss = 2.5430, kgLoss = 0.0133
2025-04-08 16:58:12.260061: Training Step 12/354: batchLoss = 0.4867, diffLoss = 2.3854, kgLoss = 0.0120
2025-04-08 16:58:13.885334: Training Step 13/354: batchLoss = 0.6235, diffLoss = 3.0573, kgLoss = 0.0151
2025-04-08 16:58:15.514354: Training Step 14/354: batchLoss = 0.6802, diffLoss = 3.3374, kgLoss = 0.0159
2025-04-08 16:58:17.141274: Training Step 15/354: batchLoss = 0.5179, diffLoss = 2.5339, kgLoss = 0.0139
2025-04-08 16:58:18.766857: Training Step 16/354: batchLoss = 0.6781, diffLoss = 3.3216, kgLoss = 0.0172
2025-04-08 16:58:20.396414: Training Step 17/354: batchLoss = 0.4680, diffLoss = 2.2859, kgLoss = 0.0135
2025-04-08 16:58:22.016174: Training Step 18/354: batchLoss = 0.7174, diffLoss = 3.5194, kgLoss = 0.0170
2025-04-08 16:58:23.633623: Training Step 19/354: batchLoss = 0.5412, diffLoss = 2.6503, kgLoss = 0.0140
2025-04-08 16:58:25.256464: Training Step 20/354: batchLoss = 0.4789, diffLoss = 2.3486, kgLoss = 0.0115
2025-04-08 16:58:26.869936: Training Step 21/354: batchLoss = 0.5005, diffLoss = 2.4429, kgLoss = 0.0149
2025-04-08 16:58:28.492279: Training Step 22/354: batchLoss = 0.6039, diffLoss = 2.9247, kgLoss = 0.0237
2025-04-08 16:58:30.119175: Training Step 23/354: batchLoss = 0.5541, diffLoss = 2.7080, kgLoss = 0.0156
2025-04-08 16:58:31.751705: Training Step 24/354: batchLoss = 0.4770, diffLoss = 2.3370, kgLoss = 0.0119
2025-04-08 16:58:33.379484: Training Step 25/354: batchLoss = 0.5116, diffLoss = 2.5024, kgLoss = 0.0139
2025-04-08 16:58:35.004698: Training Step 26/354: batchLoss = 0.5003, diffLoss = 2.4465, kgLoss = 0.0138
2025-04-08 16:58:36.618792: Training Step 27/354: batchLoss = 0.5724, diffLoss = 2.8069, kgLoss = 0.0138
2025-04-08 16:58:38.237323: Training Step 28/354: batchLoss = 0.4362, diffLoss = 2.1375, kgLoss = 0.0109
2025-04-08 16:58:39.854375: Training Step 29/354: batchLoss = 0.5290, diffLoss = 2.5878, kgLoss = 0.0143
2025-04-08 16:58:41.470216: Training Step 30/354: batchLoss = 0.6902, diffLoss = 3.3817, kgLoss = 0.0174
2025-04-08 16:58:43.086185: Training Step 31/354: batchLoss = 0.5778, diffLoss = 2.8334, kgLoss = 0.0139
2025-04-08 16:58:44.704428: Training Step 32/354: batchLoss = 0.5336, diffLoss = 2.6072, kgLoss = 0.0152
2025-04-08 16:58:46.321322: Training Step 33/354: batchLoss = 0.5903, diffLoss = 2.8938, kgLoss = 0.0144
2025-04-08 16:58:47.938419: Training Step 34/354: batchLoss = 0.4910, diffLoss = 2.4087, kgLoss = 0.0116
2025-04-08 16:58:49.557870: Training Step 35/354: batchLoss = 0.5899, diffLoss = 2.8824, kgLoss = 0.0168
2025-04-08 16:58:51.183758: Training Step 36/354: batchLoss = 0.5259, diffLoss = 2.5772, kgLoss = 0.0131
2025-04-08 16:58:52.812095: Training Step 37/354: batchLoss = 0.6202, diffLoss = 3.0369, kgLoss = 0.0160
2025-04-08 16:58:54.438055: Training Step 38/354: batchLoss = 0.6100, diffLoss = 2.9902, kgLoss = 0.0149
2025-04-08 16:58:56.051345: Training Step 39/354: batchLoss = 0.5392, diffLoss = 2.6316, kgLoss = 0.0161
2025-04-08 16:58:57.671489: Training Step 40/354: batchLoss = 0.5461, diffLoss = 2.6719, kgLoss = 0.0146
2025-04-08 16:58:59.288502: Training Step 41/354: batchLoss = 0.5193, diffLoss = 2.5481, kgLoss = 0.0121
2025-04-08 16:59:00.908879: Training Step 42/354: batchLoss = 0.4475, diffLoss = 2.1908, kgLoss = 0.0117
2025-04-08 16:59:02.531500: Training Step 43/354: batchLoss = 0.4876, diffLoss = 2.3855, kgLoss = 0.0131
2025-04-08 16:59:04.166714: Training Step 44/354: batchLoss = 0.5418, diffLoss = 2.6548, kgLoss = 0.0136
2025-04-08 16:59:05.793304: Training Step 45/354: batchLoss = 0.5177, diffLoss = 2.5337, kgLoss = 0.0137
2025-04-08 16:59:07.418259: Training Step 46/354: batchLoss = 0.5962, diffLoss = 2.9264, kgLoss = 0.0137
2025-04-08 16:59:09.044783: Training Step 47/354: batchLoss = 0.4297, diffLoss = 2.1059, kgLoss = 0.0106
2025-04-08 16:59:10.663687: Training Step 48/354: batchLoss = 0.5560, diffLoss = 2.7230, kgLoss = 0.0142
2025-04-08 16:59:12.287039: Training Step 49/354: batchLoss = 0.5662, diffLoss = 2.7783, kgLoss = 0.0132
2025-04-08 16:59:13.906927: Training Step 50/354: batchLoss = 0.6003, diffLoss = 2.9448, kgLoss = 0.0141
2025-04-08 16:59:15.521152: Training Step 51/354: batchLoss = 0.5191, diffLoss = 2.5408, kgLoss = 0.0137
2025-04-08 16:59:17.139787: Training Step 52/354: batchLoss = 0.5834, diffLoss = 2.8607, kgLoss = 0.0140
2025-04-08 16:59:18.765029: Training Step 53/354: batchLoss = 0.6160, diffLoss = 3.0218, kgLoss = 0.0146
2025-04-08 16:59:20.385618: Training Step 54/354: batchLoss = 0.4880, diffLoss = 2.3926, kgLoss = 0.0119
2025-04-08 16:59:22.006915: Training Step 55/354: batchLoss = 0.6224, diffLoss = 3.0547, kgLoss = 0.0143
2025-04-08 16:59:23.628605: Training Step 56/354: batchLoss = 0.8131, diffLoss = 3.9871, kgLoss = 0.0196
2025-04-08 16:59:25.250086: Training Step 57/354: batchLoss = 0.4895, diffLoss = 2.3920, kgLoss = 0.0139
2025-04-08 16:59:26.871801: Training Step 58/354: batchLoss = 0.6632, diffLoss = 3.2424, kgLoss = 0.0184
2025-04-08 16:59:28.481386: Training Step 59/354: batchLoss = 0.4917, diffLoss = 2.4065, kgLoss = 0.0130
2025-04-08 16:59:30.097224: Training Step 60/354: batchLoss = 0.6002, diffLoss = 2.9360, kgLoss = 0.0162
2025-04-08 16:59:31.706450: Training Step 61/354: batchLoss = 0.5010, diffLoss = 2.4559, kgLoss = 0.0123
2025-04-08 16:59:33.327892: Training Step 62/354: batchLoss = 0.5348, diffLoss = 2.6139, kgLoss = 0.0151
2025-04-08 16:59:34.954764: Training Step 63/354: batchLoss = 0.6134, diffLoss = 3.0055, kgLoss = 0.0153
2025-04-08 16:59:36.579687: Training Step 64/354: batchLoss = 0.4769, diffLoss = 2.3374, kgLoss = 0.0118
2025-04-08 16:59:38.196436: Training Step 65/354: batchLoss = 0.5955, diffLoss = 2.9174, kgLoss = 0.0150
2025-04-08 16:59:39.814665: Training Step 66/354: batchLoss = 0.5513, diffLoss = 2.7010, kgLoss = 0.0139
2025-04-08 16:59:41.451148: Training Step 67/354: batchLoss = 0.5663, diffLoss = 2.7715, kgLoss = 0.0150
2025-04-08 16:59:43.060261: Training Step 68/354: batchLoss = 0.5133, diffLoss = 2.5203, kgLoss = 0.0116
2025-04-08 16:59:44.681273: Training Step 69/354: batchLoss = 0.4719, diffLoss = 2.3082, kgLoss = 0.0128
2025-04-08 16:59:46.297756: Training Step 70/354: batchLoss = 0.4952, diffLoss = 2.4270, kgLoss = 0.0122
2025-04-08 16:59:47.909058: Training Step 71/354: batchLoss = 0.5652, diffLoss = 2.7579, kgLoss = 0.0170
2025-04-08 16:59:49.526171: Training Step 72/354: batchLoss = 0.5916, diffLoss = 2.8990, kgLoss = 0.0148
2025-04-08 16:59:51.134901: Training Step 73/354: batchLoss = 0.5441, diffLoss = 2.6662, kgLoss = 0.0136
2025-04-08 16:59:52.753173: Training Step 74/354: batchLoss = 0.5789, diffLoss = 2.8318, kgLoss = 0.0157
2025-04-08 16:59:54.382700: Training Step 75/354: batchLoss = 0.4644, diffLoss = 2.2748, kgLoss = 0.0119
2025-04-08 16:59:56.000036: Training Step 76/354: batchLoss = 0.4580, diffLoss = 2.2406, kgLoss = 0.0123
2025-04-08 16:59:57.613903: Training Step 77/354: batchLoss = 0.6195, diffLoss = 3.0278, kgLoss = 0.0174
2025-04-08 16:59:59.238329: Training Step 78/354: batchLoss = 0.5318, diffLoss = 2.6091, kgLoss = 0.0125
2025-04-08 17:00:00.862668: Training Step 79/354: batchLoss = 0.6684, diffLoss = 3.2775, kgLoss = 0.0161
2025-04-08 17:00:02.473494: Training Step 80/354: batchLoss = 0.6143, diffLoss = 3.0048, kgLoss = 0.0166
2025-04-08 17:00:04.075781: Training Step 81/354: batchLoss = 0.5589, diffLoss = 2.7323, kgLoss = 0.0155
2025-04-08 17:00:05.692822: Training Step 82/354: batchLoss = 0.6219, diffLoss = 3.0515, kgLoss = 0.0145
2025-04-08 17:00:07.321434: Training Step 83/354: batchLoss = 0.6902, diffLoss = 3.3839, kgLoss = 0.0168
2025-04-08 17:00:08.942659: Training Step 84/354: batchLoss = 0.6667, diffLoss = 3.2673, kgLoss = 0.0166
2025-04-08 17:00:10.561959: Training Step 85/354: batchLoss = 0.5673, diffLoss = 2.7755, kgLoss = 0.0153
2025-04-08 17:00:12.167583: Training Step 86/354: batchLoss = 0.5454, diffLoss = 2.6667, kgLoss = 0.0150
2025-04-08 17:00:13.776146: Training Step 87/354: batchLoss = 0.6272, diffLoss = 3.0684, kgLoss = 0.0169
2025-04-08 17:00:15.403948: Training Step 88/354: batchLoss = 0.6047, diffLoss = 2.9590, kgLoss = 0.0162
2025-04-08 17:00:17.023573: Training Step 89/354: batchLoss = 0.4986, diffLoss = 2.4420, kgLoss = 0.0128
2025-04-08 17:00:18.647857: Training Step 90/354: batchLoss = 0.6323, diffLoss = 3.1047, kgLoss = 0.0142
2025-04-08 17:00:20.266641: Training Step 91/354: batchLoss = 0.6296, diffLoss = 3.0905, kgLoss = 0.0144
2025-04-08 17:00:21.878728: Training Step 92/354: batchLoss = 0.6053, diffLoss = 2.9689, kgLoss = 0.0144
2025-04-08 17:00:23.491315: Training Step 93/354: batchLoss = 0.5209, diffLoss = 2.5485, kgLoss = 0.0140
2025-04-08 17:00:25.107728: Training Step 94/354: batchLoss = 0.5563, diffLoss = 2.7159, kgLoss = 0.0164
2025-04-08 17:00:26.743154: Training Step 95/354: batchLoss = 0.5751, diffLoss = 2.8103, kgLoss = 0.0163
2025-04-08 17:00:28.362814: Training Step 96/354: batchLoss = 0.5692, diffLoss = 2.7899, kgLoss = 0.0140
2025-04-08 17:00:29.985927: Training Step 97/354: batchLoss = 0.5794, diffLoss = 2.8351, kgLoss = 0.0154
2025-04-08 17:00:31.604494: Training Step 98/354: batchLoss = 0.5272, diffLoss = 2.5771, kgLoss = 0.0147
2025-04-08 17:00:33.219878: Training Step 99/354: batchLoss = 0.6204, diffLoss = 3.0410, kgLoss = 0.0152
2025-04-08 17:00:34.821997: Training Step 100/354: batchLoss = 0.6135, diffLoss = 3.0119, kgLoss = 0.0139
2025-04-08 17:00:36.421621: Training Step 101/354: batchLoss = 0.6683, diffLoss = 3.2775, kgLoss = 0.0161
2025-04-08 17:00:38.033390: Training Step 102/354: batchLoss = 0.5429, diffLoss = 2.6520, kgLoss = 0.0156
2025-04-08 17:00:39.655412: Training Step 103/354: batchLoss = 0.5250, diffLoss = 2.5679, kgLoss = 0.0142
2025-04-08 17:00:41.284357: Training Step 104/354: batchLoss = 0.4888, diffLoss = 2.3867, kgLoss = 0.0144
2025-04-08 17:00:42.906052: Training Step 105/354: batchLoss = 0.5156, diffLoss = 2.5182, kgLoss = 0.0150
2025-04-08 17:00:44.534277: Training Step 106/354: batchLoss = 0.5942, diffLoss = 2.9150, kgLoss = 0.0139
2025-04-08 17:00:46.164794: Training Step 107/354: batchLoss = 0.4891, diffLoss = 2.3932, kgLoss = 0.0130
2025-04-08 17:00:47.781295: Training Step 108/354: batchLoss = 0.5604, diffLoss = 2.7397, kgLoss = 0.0155
2025-04-08 17:00:49.403929: Training Step 109/354: batchLoss = 0.5681, diffLoss = 2.7786, kgLoss = 0.0155
2025-04-08 17:00:51.006036: Training Step 110/354: batchLoss = 0.5270, diffLoss = 2.5805, kgLoss = 0.0136
2025-04-08 17:00:52.607737: Training Step 111/354: batchLoss = 0.6545, diffLoss = 3.2080, kgLoss = 0.0162
2025-04-08 17:00:54.215846: Training Step 112/354: batchLoss = 0.5158, diffLoss = 2.5271, kgLoss = 0.0129
2025-04-08 17:00:55.828706: Training Step 113/354: batchLoss = 0.5469, diffLoss = 2.6784, kgLoss = 0.0140
2025-04-08 17:00:57.452775: Training Step 114/354: batchLoss = 0.4253, diffLoss = 2.0780, kgLoss = 0.0121
2025-04-08 17:00:59.077753: Training Step 115/354: batchLoss = 0.6206, diffLoss = 3.0427, kgLoss = 0.0150
2025-04-08 17:01:00.704685: Training Step 116/354: batchLoss = 0.6589, diffLoss = 3.2308, kgLoss = 0.0160
2025-04-08 17:01:02.323675: Training Step 117/354: batchLoss = 0.4243, diffLoss = 2.0771, kgLoss = 0.0111
2025-04-08 17:01:03.948130: Training Step 118/354: batchLoss = 0.5877, diffLoss = 2.8759, kgLoss = 0.0157
2025-04-08 17:01:05.577905: Training Step 119/354: batchLoss = 0.4757, diffLoss = 2.3321, kgLoss = 0.0116
2025-04-08 17:01:07.201822: Training Step 120/354: batchLoss = 0.6366, diffLoss = 3.1273, kgLoss = 0.0139
2025-04-08 17:01:08.830628: Training Step 121/354: batchLoss = 0.6160, diffLoss = 3.0181, kgLoss = 0.0155
2025-04-08 17:01:10.449181: Training Step 122/354: batchLoss = 0.6777, diffLoss = 3.3200, kgLoss = 0.0171
2025-04-08 17:01:12.060499: Training Step 123/354: batchLoss = 0.6871, diffLoss = 3.3632, kgLoss = 0.0181
2025-04-08 17:01:13.665089: Training Step 124/354: batchLoss = 0.6424, diffLoss = 3.1446, kgLoss = 0.0168
2025-04-08 17:01:15.279529: Training Step 125/354: batchLoss = 0.4799, diffLoss = 2.3520, kgLoss = 0.0119
2025-04-08 17:01:16.889341: Training Step 126/354: batchLoss = 0.5020, diffLoss = 2.4542, kgLoss = 0.0140
2025-04-08 17:01:18.507348: Training Step 127/354: batchLoss = 0.5035, diffLoss = 2.4643, kgLoss = 0.0133
2025-04-08 17:01:20.138258: Training Step 128/354: batchLoss = 0.4835, diffLoss = 2.3640, kgLoss = 0.0134
2025-04-08 17:01:21.754764: Training Step 129/354: batchLoss = 0.5100, diffLoss = 2.4954, kgLoss = 0.0136
2025-04-08 17:01:23.377454: Training Step 130/354: batchLoss = 0.5329, diffLoss = 2.6075, kgLoss = 0.0143
2025-04-08 17:01:25.006726: Training Step 131/354: batchLoss = 0.5269, diffLoss = 2.5795, kgLoss = 0.0137
2025-04-08 17:01:26.630276: Training Step 132/354: batchLoss = 0.6175, diffLoss = 3.0248, kgLoss = 0.0156
2025-04-08 17:01:28.259543: Training Step 133/354: batchLoss = 0.4893, diffLoss = 2.3932, kgLoss = 0.0133
2025-04-08 17:01:29.893811: Training Step 134/354: batchLoss = 0.6435, diffLoss = 3.1541, kgLoss = 0.0159
2025-04-08 17:01:31.512640: Training Step 135/354: batchLoss = 0.5867, diffLoss = 2.8726, kgLoss = 0.0152
2025-04-08 17:01:33.147819: Training Step 136/354: batchLoss = 0.5608, diffLoss = 2.7509, kgLoss = 0.0132
2025-04-08 17:01:34.762241: Training Step 137/354: batchLoss = 0.5950, diffLoss = 2.9129, kgLoss = 0.0156
2025-04-08 17:01:36.367546: Training Step 138/354: batchLoss = 0.5256, diffLoss = 2.5651, kgLoss = 0.0158
2025-04-08 17:01:37.967649: Training Step 139/354: batchLoss = 0.5818, diffLoss = 2.8465, kgLoss = 0.0156
2025-04-08 17:01:39.577163: Training Step 140/354: batchLoss = 0.4851, diffLoss = 2.3710, kgLoss = 0.0136
2025-04-08 17:01:41.183871: Training Step 141/354: batchLoss = 0.6374, diffLoss = 3.1249, kgLoss = 0.0155
2025-04-08 17:01:42.788364: Training Step 142/354: batchLoss = 0.5757, diffLoss = 2.8132, kgLoss = 0.0163
2025-04-08 17:01:44.394708: Training Step 143/354: batchLoss = 0.5557, diffLoss = 2.7200, kgLoss = 0.0146
2025-04-08 17:01:46.021892: Training Step 144/354: batchLoss = 0.4960, diffLoss = 2.4225, kgLoss = 0.0144
2025-04-08 17:01:47.644493: Training Step 145/354: batchLoss = 0.7036, diffLoss = 3.4511, kgLoss = 0.0167
2025-04-08 17:01:49.275288: Training Step 146/354: batchLoss = 0.5598, diffLoss = 2.7473, kgLoss = 0.0129
2025-04-08 17:01:50.893798: Training Step 147/354: batchLoss = 0.5051, diffLoss = 2.4737, kgLoss = 0.0130
2025-04-08 17:01:52.511472: Training Step 148/354: batchLoss = 0.5952, diffLoss = 2.9112, kgLoss = 0.0162
2025-04-08 17:01:54.135140: Training Step 149/354: batchLoss = 0.5044, diffLoss = 2.4703, kgLoss = 0.0130
2025-04-08 17:01:55.755277: Training Step 150/354: batchLoss = 0.5246, diffLoss = 2.5645, kgLoss = 0.0147
2025-04-08 17:01:57.383222: Training Step 151/354: batchLoss = 0.5265, diffLoss = 2.5799, kgLoss = 0.0132
2025-04-08 17:01:59.086233: Training Step 152/354: batchLoss = 0.8222, diffLoss = 4.0340, kgLoss = 0.0192
2025-04-08 17:02:00.719528: Training Step 153/354: batchLoss = 0.6101, diffLoss = 2.9878, kgLoss = 0.0157
2025-04-08 17:02:02.347766: Training Step 154/354: batchLoss = 0.5496, diffLoss = 2.6895, kgLoss = 0.0146
2025-04-08 17:02:03.974755: Training Step 155/354: batchLoss = 0.5528, diffLoss = 2.7019, kgLoss = 0.0156
2025-04-08 17:02:05.593255: Training Step 156/354: batchLoss = 0.5209, diffLoss = 2.5485, kgLoss = 0.0140
2025-04-08 17:02:07.192869: Training Step 157/354: batchLoss = 0.6085, diffLoss = 2.9768, kgLoss = 0.0165
2025-04-08 17:02:08.793614: Training Step 158/354: batchLoss = 0.4158, diffLoss = 2.0313, kgLoss = 0.0119
2025-04-08 17:02:10.401043: Training Step 159/354: batchLoss = 0.5745, diffLoss = 2.8103, kgLoss = 0.0156
2025-04-08 17:02:12.004187: Training Step 160/354: batchLoss = 0.6688, diffLoss = 3.2703, kgLoss = 0.0185
2025-04-08 17:02:13.607455: Training Step 161/354: batchLoss = 0.6201, diffLoss = 3.0291, kgLoss = 0.0179
2025-04-08 17:02:15.214945: Training Step 162/354: batchLoss = 0.6076, diffLoss = 2.9783, kgLoss = 0.0149
2025-04-08 17:02:16.824477: Training Step 163/354: batchLoss = 0.5454, diffLoss = 2.6741, kgLoss = 0.0132
2025-04-08 17:02:18.431894: Training Step 164/354: batchLoss = 0.6047, diffLoss = 2.9621, kgLoss = 0.0154
2025-04-08 17:02:20.051273: Training Step 165/354: batchLoss = 0.5617, diffLoss = 2.7449, kgLoss = 0.0159
2025-04-08 17:02:21.670858: Training Step 166/354: batchLoss = 0.5421, diffLoss = 2.6558, kgLoss = 0.0136
2025-04-08 17:02:23.284889: Training Step 167/354: batchLoss = 0.5469, diffLoss = 2.6751, kgLoss = 0.0148
2025-04-08 17:02:24.899395: Training Step 168/354: batchLoss = 0.5739, diffLoss = 2.8074, kgLoss = 0.0155
2025-04-08 17:02:26.514137: Training Step 169/354: batchLoss = 0.4754, diffLoss = 2.3301, kgLoss = 0.0118
2025-04-08 17:02:28.134285: Training Step 170/354: batchLoss = 0.4825, diffLoss = 2.3605, kgLoss = 0.0130
2025-04-08 17:02:29.759616: Training Step 171/354: batchLoss = 0.6144, diffLoss = 3.0086, kgLoss = 0.0159
2025-04-08 17:02:31.382652: Training Step 172/354: batchLoss = 0.5641, diffLoss = 2.7649, kgLoss = 0.0139
2025-04-08 17:02:33.001687: Training Step 173/354: batchLoss = 0.4820, diffLoss = 2.3568, kgLoss = 0.0133
2025-04-08 17:02:34.625574: Training Step 174/354: batchLoss = 0.5798, diffLoss = 2.8403, kgLoss = 0.0146
2025-04-08 17:02:36.248033: Training Step 175/354: batchLoss = 0.4932, diffLoss = 2.4053, kgLoss = 0.0152
2025-04-08 17:02:37.869596: Training Step 176/354: batchLoss = 0.4914, diffLoss = 2.4032, kgLoss = 0.0135
2025-04-08 17:02:39.480510: Training Step 177/354: batchLoss = 0.6236, diffLoss = 3.0561, kgLoss = 0.0155
2025-04-08 17:02:41.100759: Training Step 178/354: batchLoss = 0.5163, diffLoss = 2.5248, kgLoss = 0.0142
2025-04-08 17:02:42.722300: Training Step 179/354: batchLoss = 0.5825, diffLoss = 2.8444, kgLoss = 0.0170
2025-04-08 17:02:44.337185: Training Step 180/354: batchLoss = 0.6056, diffLoss = 2.9696, kgLoss = 0.0146
2025-04-08 17:02:45.941987: Training Step 181/354: batchLoss = 0.4063, diffLoss = 1.9832, kgLoss = 0.0120
2025-04-08 17:02:47.551187: Training Step 182/354: batchLoss = 0.5642, diffLoss = 2.7654, kgLoss = 0.0140
2025-04-08 17:02:49.160483: Training Step 183/354: batchLoss = 0.5254, diffLoss = 2.5725, kgLoss = 0.0136
2025-04-08 17:02:50.766816: Training Step 184/354: batchLoss = 0.5944, diffLoss = 2.9094, kgLoss = 0.0156
2025-04-08 17:02:52.372441: Training Step 185/354: batchLoss = 0.5095, diffLoss = 2.4950, kgLoss = 0.0132
2025-04-08 17:02:53.979308: Training Step 186/354: batchLoss = 0.5448, diffLoss = 2.6721, kgLoss = 0.0129
2025-04-08 17:02:55.577589: Training Step 187/354: batchLoss = 0.5360, diffLoss = 2.6236, kgLoss = 0.0141
2025-04-08 17:02:57.183862: Training Step 188/354: batchLoss = 0.9757, diffLoss = 4.7771, kgLoss = 0.0254
2025-04-08 17:02:58.783544: Training Step 189/354: batchLoss = 0.4780, diffLoss = 2.3381, kgLoss = 0.0129
2025-04-08 17:03:00.391672: Training Step 190/354: batchLoss = 0.6353, diffLoss = 3.1152, kgLoss = 0.0154
2025-04-08 17:03:02.017127: Training Step 191/354: batchLoss = 0.5453, diffLoss = 2.6675, kgLoss = 0.0148
2025-04-08 17:03:03.652443: Training Step 192/354: batchLoss = 0.5253, diffLoss = 2.5702, kgLoss = 0.0141
2025-04-08 17:03:05.283403: Training Step 193/354: batchLoss = 0.5944, diffLoss = 2.9161, kgLoss = 0.0140
2025-04-08 17:03:06.912836: Training Step 194/354: batchLoss = 0.4893, diffLoss = 2.3989, kgLoss = 0.0119
2025-04-08 17:03:08.535371: Training Step 195/354: batchLoss = 0.5582, diffLoss = 2.7248, kgLoss = 0.0165
2025-04-08 17:03:10.153905: Training Step 196/354: batchLoss = 0.6074, diffLoss = 2.9828, kgLoss = 0.0136
2025-04-08 17:03:11.770891: Training Step 197/354: batchLoss = 0.5549, diffLoss = 2.7184, kgLoss = 0.0140
2025-04-08 17:03:13.394912: Training Step 198/354: batchLoss = 0.5149, diffLoss = 2.5244, kgLoss = 0.0125
2025-04-08 17:03:15.007519: Training Step 199/354: batchLoss = 0.5887, diffLoss = 2.8882, kgLoss = 0.0139
2025-04-08 17:03:16.635998: Training Step 200/354: batchLoss = 0.5360, diffLoss = 2.6270, kgLoss = 0.0133
2025-04-08 17:03:18.262988: Training Step 201/354: batchLoss = 0.5954, diffLoss = 2.9137, kgLoss = 0.0158
2025-04-08 17:03:19.889531: Training Step 202/354: batchLoss = 0.5990, diffLoss = 2.9302, kgLoss = 0.0163
2025-04-08 17:03:21.510230: Training Step 203/354: batchLoss = 0.6303, diffLoss = 3.0848, kgLoss = 0.0167
2025-04-08 17:03:23.133172: Training Step 204/354: batchLoss = 0.5758, diffLoss = 2.8188, kgLoss = 0.0150
2025-04-08 17:03:24.760084: Training Step 205/354: batchLoss = 0.4912, diffLoss = 2.3965, kgLoss = 0.0149
2025-04-08 17:03:26.389030: Training Step 206/354: batchLoss = 0.5706, diffLoss = 2.7975, kgLoss = 0.0139
2025-04-08 17:03:28.005998: Training Step 207/354: batchLoss = 0.5819, diffLoss = 2.8541, kgLoss = 0.0138
2025-04-08 17:03:29.633866: Training Step 208/354: batchLoss = 0.4972, diffLoss = 2.4290, kgLoss = 0.0143
2025-04-08 17:03:31.244848: Training Step 209/354: batchLoss = 0.5384, diffLoss = 2.6305, kgLoss = 0.0154
2025-04-08 17:03:32.848210: Training Step 210/354: batchLoss = 0.5028, diffLoss = 2.4660, kgLoss = 0.0120
2025-04-08 17:03:34.455276: Training Step 211/354: batchLoss = 0.5285, diffLoss = 2.5833, kgLoss = 0.0148
2025-04-08 17:03:36.068512: Training Step 212/354: batchLoss = 0.5510, diffLoss = 2.7016, kgLoss = 0.0133
2025-04-08 17:03:37.675108: Training Step 213/354: batchLoss = 0.6345, diffLoss = 3.1076, kgLoss = 0.0162
2025-04-08 17:03:39.285304: Training Step 214/354: batchLoss = 0.6548, diffLoss = 3.2078, kgLoss = 0.0165
2025-04-08 17:03:40.904904: Training Step 215/354: batchLoss = 0.5489, diffLoss = 2.6857, kgLoss = 0.0147
2025-04-08 17:03:42.509741: Training Step 216/354: batchLoss = 0.5497, diffLoss = 2.6596, kgLoss = 0.0223
2025-04-08 17:03:44.111653: Training Step 217/354: batchLoss = 0.5413, diffLoss = 2.6537, kgLoss = 0.0132
2025-04-08 17:03:45.713117: Training Step 218/354: batchLoss = 0.5634, diffLoss = 2.7620, kgLoss = 0.0138
2025-04-08 17:03:47.312259: Training Step 219/354: batchLoss = 0.6624, diffLoss = 3.2416, kgLoss = 0.0176
2025-04-08 17:03:48.918615: Training Step 220/354: batchLoss = 0.5272, diffLoss = 2.5854, kgLoss = 0.0126
2025-04-08 17:03:50.521991: Training Step 221/354: batchLoss = 0.5258, diffLoss = 2.5741, kgLoss = 0.0137
2025-04-08 17:03:52.126817: Training Step 222/354: batchLoss = 0.6001, diffLoss = 2.9417, kgLoss = 0.0147
2025-04-08 17:03:53.732469: Training Step 223/354: batchLoss = 0.4952, diffLoss = 2.4210, kgLoss = 0.0137
2025-04-08 17:03:55.342970: Training Step 224/354: batchLoss = 0.5388, diffLoss = 2.6434, kgLoss = 0.0126
2025-04-08 17:03:56.947211: Training Step 225/354: batchLoss = 0.5010, diffLoss = 2.4570, kgLoss = 0.0120
2025-04-08 17:03:58.550970: Training Step 226/354: batchLoss = 0.5316, diffLoss = 2.6042, kgLoss = 0.0134
2025-04-08 17:04:00.147756: Training Step 227/354: batchLoss = 0.4463, diffLoss = 2.1783, kgLoss = 0.0133
2025-04-08 17:04:01.745445: Training Step 228/354: batchLoss = 0.6194, diffLoss = 3.0340, kgLoss = 0.0157
2025-04-08 17:04:03.347249: Training Step 229/354: batchLoss = 0.5799, diffLoss = 2.8484, kgLoss = 0.0128
2025-04-08 17:04:04.958575: Training Step 230/354: batchLoss = 0.6170, diffLoss = 3.0147, kgLoss = 0.0176
2025-04-08 17:04:06.568843: Training Step 231/354: batchLoss = 0.5100, diffLoss = 2.4950, kgLoss = 0.0138
2025-04-08 17:04:08.184617: Training Step 232/354: batchLoss = 0.5974, diffLoss = 2.9291, kgLoss = 0.0145
2025-04-08 17:04:09.805196: Training Step 233/354: batchLoss = 0.5559, diffLoss = 2.7241, kgLoss = 0.0138
2025-04-08 17:04:11.422630: Training Step 234/354: batchLoss = 1.9286, diffLoss = 9.4701, kgLoss = 0.0433
2025-04-08 17:04:13.041628: Training Step 235/354: batchLoss = 0.4328, diffLoss = 2.1187, kgLoss = 0.0113
2025-04-08 17:04:14.657789: Training Step 236/354: batchLoss = 0.7714, diffLoss = 3.7806, kgLoss = 0.0191
2025-04-08 17:04:16.267181: Training Step 237/354: batchLoss = 0.5890, diffLoss = 2.8858, kgLoss = 0.0148
2025-04-08 17:04:17.876068: Training Step 238/354: batchLoss = 0.5771, diffLoss = 2.8258, kgLoss = 0.0150
2025-04-08 17:04:19.484199: Training Step 239/354: batchLoss = 0.5332, diffLoss = 2.6107, kgLoss = 0.0138
2025-04-08 17:04:21.096569: Training Step 240/354: batchLoss = 0.5784, diffLoss = 2.8313, kgLoss = 0.0151
2025-04-08 17:04:22.709042: Training Step 241/354: batchLoss = 0.4467, diffLoss = 2.1768, kgLoss = 0.0142
2025-04-08 17:04:24.328202: Training Step 242/354: batchLoss = 0.6668, diffLoss = 3.2624, kgLoss = 0.0179
2025-04-08 17:04:25.946339: Training Step 243/354: batchLoss = 0.5089, diffLoss = 2.4952, kgLoss = 0.0124
2025-04-08 17:04:27.570458: Training Step 244/354: batchLoss = 0.5706, diffLoss = 2.7921, kgLoss = 0.0152
2025-04-08 17:04:29.197961: Training Step 245/354: batchLoss = 0.5201, diffLoss = 2.5406, kgLoss = 0.0150
2025-04-08 17:04:30.822356: Training Step 246/354: batchLoss = 0.4990, diffLoss = 2.4453, kgLoss = 0.0124
2025-04-08 17:04:32.439928: Training Step 247/354: batchLoss = 0.6174, diffLoss = 3.0282, kgLoss = 0.0147
2025-04-08 17:04:34.054983: Training Step 248/354: batchLoss = 0.5353, diffLoss = 2.6220, kgLoss = 0.0136
2025-04-08 17:04:35.663029: Training Step 249/354: batchLoss = 0.5670, diffLoss = 2.7815, kgLoss = 0.0133
2025-04-08 17:04:37.282200: Training Step 250/354: batchLoss = 0.4647, diffLoss = 2.2686, kgLoss = 0.0138
2025-04-08 17:04:38.904828: Training Step 251/354: batchLoss = 0.5249, diffLoss = 2.5668, kgLoss = 0.0145
2025-04-08 17:04:40.525138: Training Step 252/354: batchLoss = 0.5564, diffLoss = 2.7275, kgLoss = 0.0137
2025-04-08 17:04:42.149528: Training Step 253/354: batchLoss = 0.5709, diffLoss = 2.7980, kgLoss = 0.0141
2025-04-08 17:04:43.777195: Training Step 254/354: batchLoss = 0.5020, diffLoss = 2.4543, kgLoss = 0.0139
2025-04-08 17:04:45.400286: Training Step 255/354: batchLoss = 0.5266, diffLoss = 2.5837, kgLoss = 0.0124
2025-04-08 17:04:47.016271: Training Step 256/354: batchLoss = 0.5223, diffLoss = 2.5548, kgLoss = 0.0141
2025-04-08 17:04:48.632189: Training Step 257/354: batchLoss = 0.5936, diffLoss = 2.9000, kgLoss = 0.0171
2025-04-08 17:04:50.245220: Training Step 258/354: batchLoss = 0.5001, diffLoss = 2.4363, kgLoss = 0.0160
2025-04-08 17:04:51.865587: Training Step 259/354: batchLoss = 0.5358, diffLoss = 2.6176, kgLoss = 0.0153
2025-04-08 17:04:53.493001: Training Step 260/354: batchLoss = 0.5943, diffLoss = 2.9127, kgLoss = 0.0146
2025-04-08 17:04:55.120267: Training Step 261/354: batchLoss = 0.5822, diffLoss = 2.8482, kgLoss = 0.0156
2025-04-08 17:04:56.745442: Training Step 262/354: batchLoss = 0.4990, diffLoss = 2.4467, kgLoss = 0.0121
2025-04-08 17:04:58.367451: Training Step 263/354: batchLoss = 0.5051, diffLoss = 2.4706, kgLoss = 0.0138
2025-04-08 17:04:59.995152: Training Step 264/354: batchLoss = 0.5594, diffLoss = 2.7366, kgLoss = 0.0151
2025-04-08 17:05:01.618254: Training Step 265/354: batchLoss = 0.7271, diffLoss = 3.5575, kgLoss = 0.0195
2025-04-08 17:05:03.247563: Training Step 266/354: batchLoss = 0.6723, diffLoss = 3.2964, kgLoss = 0.0163
2025-04-08 17:05:04.859817: Training Step 267/354: batchLoss = 0.4443, diffLoss = 2.1678, kgLoss = 0.0134
2025-04-08 17:05:06.474525: Training Step 268/354: batchLoss = 0.7035, diffLoss = 3.4481, kgLoss = 0.0173
2025-04-08 17:05:08.100405: Training Step 269/354: batchLoss = 0.5752, diffLoss = 2.8224, kgLoss = 0.0134
2025-04-08 17:05:09.734390: Training Step 270/354: batchLoss = 0.5913, diffLoss = 2.9013, kgLoss = 0.0138
2025-04-08 17:05:11.350646: Training Step 271/354: batchLoss = 0.5049, diffLoss = 2.4741, kgLoss = 0.0126
2025-04-08 17:05:12.969765: Training Step 272/354: batchLoss = 0.6437, diffLoss = 3.1546, kgLoss = 0.0160
2025-04-08 17:05:14.579707: Training Step 273/354: batchLoss = 0.6233, diffLoss = 3.0421, kgLoss = 0.0186
2025-04-08 17:05:16.201526: Training Step 274/354: batchLoss = 0.5950, diffLoss = 2.9195, kgLoss = 0.0139
2025-04-08 17:05:17.814196: Training Step 275/354: batchLoss = 0.4542, diffLoss = 2.2179, kgLoss = 0.0132
2025-04-08 17:05:19.415378: Training Step 276/354: batchLoss = 0.7919, diffLoss = 3.8825, kgLoss = 0.0193
2025-04-08 17:05:21.023474: Training Step 277/354: batchLoss = 0.6154, diffLoss = 3.0173, kgLoss = 0.0150
2025-04-08 17:05:22.629525: Training Step 278/354: batchLoss = 0.5550, diffLoss = 2.7186, kgLoss = 0.0140
2025-04-08 17:05:24.241548: Training Step 279/354: batchLoss = 0.4760, diffLoss = 2.3343, kgLoss = 0.0115
2025-04-08 17:05:25.853626: Training Step 280/354: batchLoss = 0.6437, diffLoss = 3.1579, kgLoss = 0.0152
2025-04-08 17:05:27.461909: Training Step 281/354: batchLoss = 0.6297, diffLoss = 3.0857, kgLoss = 0.0157
2025-04-08 17:05:29.067514: Training Step 282/354: batchLoss = 0.5356, diffLoss = 2.6226, kgLoss = 0.0138
2025-04-08 17:05:30.679488: Training Step 283/354: batchLoss = 0.5449, diffLoss = 2.6790, kgLoss = 0.0113
2025-04-08 17:05:32.287774: Training Step 284/354: batchLoss = 0.5020, diffLoss = 2.4599, kgLoss = 0.0125
2025-04-08 17:05:33.893197: Training Step 285/354: batchLoss = 0.6553, diffLoss = 3.2141, kgLoss = 0.0155
2025-04-08 17:05:35.496040: Training Step 286/354: batchLoss = 0.5445, diffLoss = 2.6629, kgLoss = 0.0149
2025-04-08 17:05:37.092038: Training Step 287/354: batchLoss = 0.5704, diffLoss = 2.7904, kgLoss = 0.0153
2025-04-08 17:05:38.695753: Training Step 288/354: batchLoss = 0.5842, diffLoss = 2.8665, kgLoss = 0.0137
2025-04-08 17:05:40.311161: Training Step 289/354: batchLoss = 0.5539, diffLoss = 2.7172, kgLoss = 0.0130
2025-04-08 17:05:41.918027: Training Step 290/354: batchLoss = 0.4582, diffLoss = 2.2446, kgLoss = 0.0116
2025-04-08 17:05:43.527586: Training Step 291/354: batchLoss = 0.5563, diffLoss = 2.7194, kgLoss = 0.0155
2025-04-08 17:05:45.134834: Training Step 292/354: batchLoss = 0.4655, diffLoss = 2.2782, kgLoss = 0.0123
2025-04-08 17:05:46.741566: Training Step 293/354: batchLoss = 0.6167, diffLoss = 3.0213, kgLoss = 0.0155
2025-04-08 17:05:48.354841: Training Step 294/354: batchLoss = 0.5186, diffLoss = 2.5435, kgLoss = 0.0124
2025-04-08 17:05:49.960788: Training Step 295/354: batchLoss = 0.5343, diffLoss = 2.6154, kgLoss = 0.0140
2025-04-08 17:05:51.569200: Training Step 296/354: batchLoss = 0.4901, diffLoss = 2.3970, kgLoss = 0.0134
2025-04-08 17:05:53.178455: Training Step 297/354: batchLoss = 0.7645, diffLoss = 3.7458, kgLoss = 0.0192
2025-04-08 17:05:54.794422: Training Step 298/354: batchLoss = 0.6069, diffLoss = 2.9725, kgLoss = 0.0155
2025-04-08 17:05:56.416291: Training Step 299/354: batchLoss = 0.5047, diffLoss = 2.4632, kgLoss = 0.0151
2025-04-08 17:05:58.031221: Training Step 300/354: batchLoss = 0.4868, diffLoss = 2.3728, kgLoss = 0.0153
2025-04-08 17:05:59.657249: Training Step 301/354: batchLoss = 0.5990, diffLoss = 2.9317, kgLoss = 0.0159
2025-04-08 17:06:01.278267: Training Step 302/354: batchLoss = 0.6037, diffLoss = 2.9595, kgLoss = 0.0147
2025-04-08 17:06:02.885962: Training Step 303/354: batchLoss = 0.5171, diffLoss = 2.5394, kgLoss = 0.0116
2025-04-08 17:06:04.496253: Training Step 304/354: batchLoss = 0.5413, diffLoss = 2.6490, kgLoss = 0.0143
2025-04-08 17:06:06.106329: Training Step 305/354: batchLoss = 0.4599, diffLoss = 2.2483, kgLoss = 0.0128
2025-04-08 17:06:07.707481: Training Step 306/354: batchLoss = 0.5488, diffLoss = 2.6780, kgLoss = 0.0165
2025-04-08 17:06:09.318187: Training Step 307/354: batchLoss = 0.6400, diffLoss = 3.1326, kgLoss = 0.0168
2025-04-08 17:06:10.941054: Training Step 308/354: batchLoss = 0.5028, diffLoss = 2.4656, kgLoss = 0.0121
2025-04-08 17:06:12.557675: Training Step 309/354: batchLoss = 0.4663, diffLoss = 2.2822, kgLoss = 0.0123
2025-04-08 17:06:14.170060: Training Step 310/354: batchLoss = 0.5390, diffLoss = 2.6421, kgLoss = 0.0132
2025-04-08 17:06:15.795177: Training Step 311/354: batchLoss = 0.5080, diffLoss = 2.4865, kgLoss = 0.0134
2025-04-08 17:06:17.412031: Training Step 312/354: batchLoss = 0.5166, diffLoss = 2.5295, kgLoss = 0.0133
2025-04-08 17:06:19.029038: Training Step 313/354: batchLoss = 0.5712, diffLoss = 2.7973, kgLoss = 0.0146
2025-04-08 17:06:20.649000: Training Step 314/354: batchLoss = 0.6295, diffLoss = 3.0839, kgLoss = 0.0159
2025-04-08 17:06:22.260127: Training Step 315/354: batchLoss = 0.5970, diffLoss = 2.9207, kgLoss = 0.0161
2025-04-08 17:06:23.878653: Training Step 316/354: batchLoss = 0.4432, diffLoss = 2.1621, kgLoss = 0.0135
2025-04-08 17:06:25.494070: Training Step 317/354: batchLoss = 0.6057, diffLoss = 2.9625, kgLoss = 0.0164
2025-04-08 17:06:27.111393: Training Step 318/354: batchLoss = 0.7075, diffLoss = 3.4712, kgLoss = 0.0165
2025-04-08 17:06:28.741604: Training Step 319/354: batchLoss = 0.7239, diffLoss = 3.5440, kgLoss = 0.0188
2025-04-08 17:06:30.368852: Training Step 320/354: batchLoss = 0.5277, diffLoss = 2.5779, kgLoss = 0.0151
2025-04-08 17:06:31.996625: Training Step 321/354: batchLoss = 0.4485, diffLoss = 2.1949, kgLoss = 0.0119
2025-04-08 17:06:33.618623: Training Step 322/354: batchLoss = 0.6050, diffLoss = 2.9616, kgLoss = 0.0159
2025-04-08 17:06:35.246902: Training Step 323/354: batchLoss = 0.6778, diffLoss = 3.3208, kgLoss = 0.0170
2025-04-08 17:06:36.872636: Training Step 324/354: batchLoss = 0.5351, diffLoss = 2.6207, kgLoss = 0.0137
2025-04-08 17:06:38.488799: Training Step 325/354: batchLoss = 0.4791, diffLoss = 2.3435, kgLoss = 0.0131
2025-04-08 17:06:40.108672: Training Step 326/354: batchLoss = 0.6088, diffLoss = 2.9803, kgLoss = 0.0159
2025-04-08 17:06:41.710221: Training Step 327/354: batchLoss = 0.6219, diffLoss = 3.0432, kgLoss = 0.0165
2025-04-08 17:06:43.318749: Training Step 328/354: batchLoss = 0.7147, diffLoss = 3.5100, kgLoss = 0.0158
2025-04-08 17:06:44.950214: Training Step 329/354: batchLoss = 0.4633, diffLoss = 2.2620, kgLoss = 0.0136
2025-04-08 17:06:46.577769: Training Step 330/354: batchLoss = 0.6149, diffLoss = 3.0176, kgLoss = 0.0142
2025-04-08 17:06:48.205543: Training Step 331/354: batchLoss = 0.5885, diffLoss = 2.8891, kgLoss = 0.0133
2025-04-08 17:06:49.827051: Training Step 332/354: batchLoss = 0.4278, diffLoss = 2.0872, kgLoss = 0.0129
2025-04-08 17:06:51.451155: Training Step 333/354: batchLoss = 0.7253, diffLoss = 3.5622, kgLoss = 0.0161
2025-04-08 17:06:53.085664: Training Step 334/354: batchLoss = 0.6148, diffLoss = 2.9975, kgLoss = 0.0192
2025-04-08 17:06:54.715508: Training Step 335/354: batchLoss = 0.4521, diffLoss = 2.2141, kgLoss = 0.0116
2025-04-08 17:06:56.338989: Training Step 336/354: batchLoss = 0.5598, diffLoss = 2.7463, kgLoss = 0.0132
2025-04-08 17:06:57.963647: Training Step 337/354: batchLoss = 0.4708, diffLoss = 2.3035, kgLoss = 0.0126
2025-04-08 17:06:59.599190: Training Step 338/354: batchLoss = 0.5017, diffLoss = 2.4595, kgLoss = 0.0122
2025-04-08 17:07:01.229351: Training Step 339/354: batchLoss = 0.5388, diffLoss = 2.6366, kgLoss = 0.0144
2025-04-08 17:07:02.862706: Training Step 340/354: batchLoss = 0.8151, diffLoss = 3.9971, kgLoss = 0.0196
2025-04-08 17:07:04.480911: Training Step 341/354: batchLoss = 0.5182, diffLoss = 2.5406, kgLoss = 0.0126
2025-04-08 17:07:06.106077: Training Step 342/354: batchLoss = 0.5143, diffLoss = 2.5116, kgLoss = 0.0149
2025-04-08 17:07:07.728992: Training Step 343/354: batchLoss = 0.5535, diffLoss = 2.7145, kgLoss = 0.0133
2025-04-08 17:07:09.345683: Training Step 344/354: batchLoss = 0.7451, diffLoss = 3.6505, kgLoss = 0.0187
2025-04-08 17:07:10.969148: Training Step 345/354: batchLoss = 0.4867, diffLoss = 2.3798, kgLoss = 0.0134
2025-04-08 17:07:12.587347: Training Step 346/354: batchLoss = 0.5643, diffLoss = 2.7617, kgLoss = 0.0150
2025-04-08 17:07:14.211664: Training Step 347/354: batchLoss = 0.5274, diffLoss = 2.5844, kgLoss = 0.0131
2025-04-08 17:07:15.830860: Training Step 348/354: batchLoss = 0.6990, diffLoss = 3.4263, kgLoss = 0.0172
2025-04-08 17:07:17.453058: Training Step 349/354: batchLoss = 0.6693, diffLoss = 3.2766, kgLoss = 0.0175
2025-04-08 17:07:19.068857: Training Step 350/354: batchLoss = 0.6045, diffLoss = 2.9561, kgLoss = 0.0166
2025-04-08 17:07:20.678758: Training Step 351/354: batchLoss = 0.5252, diffLoss = 2.5744, kgLoss = 0.0129
2025-04-08 17:07:22.275479: Training Step 352/354: batchLoss = 0.5739, diffLoss = 2.8100, kgLoss = 0.0149
2025-04-08 17:07:23.675221: Training Step 353/354: batchLoss = 0.4786, diffLoss = 2.3430, kgLoss = 0.0125
2025-04-08 17:07:23.762227: 
2025-04-08 17:07:23.763081: Epoch 21/1000, Train: epLoss = 1.0070, epDfLoss = 4.9307, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 17:07:25.068203: Steps 0/138: batch_recall = 47.02, batch_ndcg = 26.54 
2025-04-08 17:07:26.378927: Steps 1/138: batch_recall = 48.99, batch_ndcg = 28.40 
2025-04-08 17:07:27.665604: Steps 2/138: batch_recall = 60.20, batch_ndcg = 36.42 
2025-04-08 17:07:28.967172: Steps 3/138: batch_recall = 60.41, batch_ndcg = 34.74 
2025-04-08 17:07:30.268608: Steps 4/138: batch_recall = 68.55, batch_ndcg = 40.82 
2025-04-08 17:07:31.569930: Steps 5/138: batch_recall = 58.09, batch_ndcg = 31.84 
2025-04-08 17:07:32.869137: Steps 6/138: batch_recall = 52.68, batch_ndcg = 31.89 
2025-04-08 17:07:34.160188: Steps 7/138: batch_recall = 63.50, batch_ndcg = 41.96 
2025-04-08 17:07:35.450158: Steps 8/138: batch_recall = 61.97, batch_ndcg = 39.96 
2025-04-08 17:07:36.747176: Steps 9/138: batch_recall = 56.95, batch_ndcg = 33.41 
2025-04-08 17:07:38.032629: Steps 10/138: batch_recall = 56.69, batch_ndcg = 31.90 
2025-04-08 17:07:39.319173: Steps 11/138: batch_recall = 56.63, batch_ndcg = 32.63 
2025-04-08 17:07:40.598919: Steps 12/138: batch_recall = 53.29, batch_ndcg = 29.29 
2025-04-08 17:07:41.878039: Steps 13/138: batch_recall = 55.25, batch_ndcg = 31.98 
2025-04-08 17:07:43.155451: Steps 14/138: batch_recall = 52.12, batch_ndcg = 31.09 
2025-04-08 17:07:44.431787: Steps 15/138: batch_recall = 50.35, batch_ndcg = 30.07 
2025-04-08 17:07:45.708007: Steps 16/138: batch_recall = 59.75, batch_ndcg = 33.57 
2025-04-08 17:07:46.989496: Steps 17/138: batch_recall = 57.25, batch_ndcg = 32.73 
2025-04-08 17:07:48.279335: Steps 18/138: batch_recall = 51.38, batch_ndcg = 32.44 
2025-04-08 17:07:49.577985: Steps 19/138: batch_recall = 54.45, batch_ndcg = 32.71 
2025-04-08 17:07:50.864217: Steps 20/138: batch_recall = 61.22, batch_ndcg = 36.47 
2025-04-08 17:07:52.148937: Steps 21/138: batch_recall = 68.99, batch_ndcg = 40.50 
2025-04-08 17:07:53.425270: Steps 22/138: batch_recall = 58.00, batch_ndcg = 33.54 
2025-04-08 17:07:54.711604: Steps 23/138: batch_recall = 51.32, batch_ndcg = 30.06 
2025-04-08 17:07:55.994176: Steps 24/138: batch_recall = 57.35, batch_ndcg = 31.45 
2025-04-08 17:07:57.267752: Steps 25/138: batch_recall = 61.69, batch_ndcg = 35.85 
2025-04-08 17:07:58.543101: Steps 26/138: batch_recall = 58.11, batch_ndcg = 33.35 
2025-04-08 17:07:59.824113: Steps 27/138: batch_recall = 61.16, batch_ndcg = 34.10 
2025-04-08 17:08:01.093381: Steps 28/138: batch_recall = 60.03, batch_ndcg = 33.44 
2025-04-08 17:08:02.374250: Steps 29/138: batch_recall = 61.35, batch_ndcg = 32.49 
2025-04-08 17:08:03.657722: Steps 30/138: batch_recall = 57.95, batch_ndcg = 34.23 
2025-04-08 17:08:04.937476: Steps 31/138: batch_recall = 43.47, batch_ndcg = 24.85 
2025-04-08 17:08:06.219761: Steps 32/138: batch_recall = 53.34, batch_ndcg = 31.00 
2025-04-08 17:08:07.491688: Steps 33/138: batch_recall = 59.29, batch_ndcg = 33.26 
2025-04-08 17:08:08.774859: Steps 34/138: batch_recall = 53.75, batch_ndcg = 28.76 
2025-04-08 17:08:10.060253: Steps 35/138: batch_recall = 51.62, batch_ndcg = 29.85 
2025-04-08 17:08:11.339378: Steps 36/138: batch_recall = 48.72, batch_ndcg = 27.89 
2025-04-08 17:08:12.611908: Steps 37/138: batch_recall = 58.16, batch_ndcg = 33.72 
2025-04-08 17:08:13.888825: Steps 38/138: batch_recall = 58.50, batch_ndcg = 32.58 
2025-04-08 17:08:15.165754: Steps 39/138: batch_recall = 66.61, batch_ndcg = 38.70 
2025-04-08 17:08:16.437880: Steps 40/138: batch_recall = 58.85, batch_ndcg = 30.78 
2025-04-08 17:08:17.712008: Steps 41/138: batch_recall = 60.31, batch_ndcg = 33.90 
2025-04-08 17:08:18.989477: Steps 42/138: batch_recall = 52.68, batch_ndcg = 30.03 
2025-04-08 17:08:20.268262: Steps 43/138: batch_recall = 56.05, batch_ndcg = 35.04 
2025-04-08 17:08:21.549084: Steps 44/138: batch_recall = 54.81, batch_ndcg = 30.75 
2025-04-08 17:08:22.833261: Steps 45/138: batch_recall = 62.34, batch_ndcg = 35.63 
2025-04-08 17:08:24.105250: Steps 46/138: batch_recall = 60.32, batch_ndcg = 35.52 
2025-04-08 17:08:25.382506: Steps 47/138: batch_recall = 53.00, batch_ndcg = 32.04 
2025-04-08 17:08:26.667930: Steps 48/138: batch_recall = 60.55, batch_ndcg = 34.88 
2025-04-08 17:08:27.940205: Steps 49/138: batch_recall = 67.60, batch_ndcg = 37.73 
2025-04-08 17:08:29.202999: Steps 50/138: batch_recall = 60.83, batch_ndcg = 31.54 
2025-04-08 17:08:30.471260: Steps 51/138: batch_recall = 60.86, batch_ndcg = 35.52 
2025-04-08 17:08:31.739163: Steps 52/138: batch_recall = 65.00, batch_ndcg = 42.21 
2025-04-08 17:08:33.010551: Steps 53/138: batch_recall = 67.31, batch_ndcg = 34.74 
2025-04-08 17:08:34.282342: Steps 54/138: batch_recall = 67.72, batch_ndcg = 39.13 
2025-04-08 17:08:35.557117: Steps 55/138: batch_recall = 59.30, batch_ndcg = 32.73 
2025-04-08 17:08:36.842176: Steps 56/138: batch_recall = 60.46, batch_ndcg = 34.84 
2025-04-08 17:08:38.120482: Steps 57/138: batch_recall = 58.58, batch_ndcg = 32.74 
2025-04-08 17:08:39.402448: Steps 58/138: batch_recall = 72.20, batch_ndcg = 37.50 
2025-04-08 17:08:40.690752: Steps 59/138: batch_recall = 69.20, batch_ndcg = 41.04 
2025-04-08 17:08:41.987077: Steps 60/138: batch_recall = 65.93, batch_ndcg = 37.27 
2025-04-08 17:08:43.267749: Steps 61/138: batch_recall = 62.35, batch_ndcg = 35.03 
2025-04-08 17:08:44.548055: Steps 62/138: batch_recall = 82.18, batch_ndcg = 43.25 
2025-04-08 17:08:45.823686: Steps 63/138: batch_recall = 74.19, batch_ndcg = 42.49 
2025-04-08 17:08:47.102532: Steps 64/138: batch_recall = 62.47, batch_ndcg = 33.35 
2025-04-08 17:08:48.376421: Steps 65/138: batch_recall = 87.75, batch_ndcg = 47.79 
2025-04-08 17:08:49.649458: Steps 66/138: batch_recall = 67.87, batch_ndcg = 40.31 
2025-04-08 17:08:50.926530: Steps 67/138: batch_recall = 76.09, batch_ndcg = 46.86 
2025-04-08 17:08:52.206073: Steps 68/138: batch_recall = 62.73, batch_ndcg = 34.27 
2025-04-08 17:08:53.494292: Steps 69/138: batch_recall = 88.53, batch_ndcg = 51.89 
2025-04-08 17:08:54.772461: Steps 70/138: batch_recall = 79.78, batch_ndcg = 45.73 
2025-04-08 17:08:56.060206: Steps 71/138: batch_recall = 90.07, batch_ndcg = 50.99 
2025-04-08 17:08:57.341227: Steps 72/138: batch_recall = 87.31, batch_ndcg = 49.99 
2025-04-08 17:08:58.621021: Steps 73/138: batch_recall = 86.13, batch_ndcg = 47.85 
2025-04-08 17:08:59.903559: Steps 74/138: batch_recall = 80.38, batch_ndcg = 49.14 
2025-04-08 17:09:01.181127: Steps 75/138: batch_recall = 85.95, batch_ndcg = 49.72 
2025-04-08 17:09:02.465159: Steps 76/138: batch_recall = 96.21, batch_ndcg = 55.10 
2025-04-08 17:09:03.742688: Steps 77/138: batch_recall = 89.84, batch_ndcg = 50.63 
2025-04-08 17:09:05.024066: Steps 78/138: batch_recall = 91.41, batch_ndcg = 48.87 
2025-04-08 17:09:06.307051: Steps 79/138: batch_recall = 91.09, batch_ndcg = 49.60 
2025-04-08 17:09:07.600292: Steps 80/138: batch_recall = 72.44, batch_ndcg = 38.91 
2025-04-08 17:09:08.879471: Steps 81/138: batch_recall = 80.06, batch_ndcg = 47.02 
2025-04-08 17:09:10.164661: Steps 82/138: batch_recall = 89.45, batch_ndcg = 52.57 
2025-04-08 17:09:11.438882: Steps 83/138: batch_recall = 82.44, batch_ndcg = 48.02 
2025-04-08 17:09:12.699151: Steps 84/138: batch_recall = 98.27, batch_ndcg = 57.78 
2025-04-08 17:09:13.968843: Steps 85/138: batch_recall = 105.74, batch_ndcg = 61.06 
2025-04-08 17:09:15.241144: Steps 86/138: batch_recall = 115.59, batch_ndcg = 70.55 
2025-04-08 17:09:16.516242: Steps 87/138: batch_recall = 106.18, batch_ndcg = 56.71 
2025-04-08 17:09:17.786881: Steps 88/138: batch_recall = 101.37, batch_ndcg = 57.40 
2025-04-08 17:09:19.059612: Steps 89/138: batch_recall = 119.73, batch_ndcg = 67.98 
2025-04-08 17:09:20.328677: Steps 90/138: batch_recall = 103.25, batch_ndcg = 58.20 
2025-04-08 17:09:21.603602: Steps 91/138: batch_recall = 117.72, batch_ndcg = 64.86 
2025-04-08 17:09:22.880878: Steps 92/138: batch_recall = 118.02, batch_ndcg = 64.29 
2025-04-08 17:09:24.154956: Steps 93/138: batch_recall = 121.02, batch_ndcg = 68.64 
2025-04-08 17:09:25.433896: Steps 94/138: batch_recall = 125.96, batch_ndcg = 66.13 
2025-04-08 17:09:26.703436: Steps 95/138: batch_recall = 114.55, batch_ndcg = 67.16 
2025-04-08 17:09:27.974472: Steps 96/138: batch_recall = 127.81, batch_ndcg = 77.83 
2025-04-08 17:09:29.243589: Steps 97/138: batch_recall = 139.19, batch_ndcg = 87.74 
2025-04-08 17:09:30.511638: Steps 98/138: batch_recall = 109.60, batch_ndcg = 63.34 
2025-04-08 17:09:31.777733: Steps 99/138: batch_recall = 124.97, batch_ndcg = 71.29 
2025-04-08 17:09:33.046874: Steps 100/138: batch_recall = 131.95, batch_ndcg = 73.27 
2025-04-08 17:09:34.321124: Steps 101/138: batch_recall = 127.47, batch_ndcg = 70.72 
2025-04-08 17:09:35.587345: Steps 102/138: batch_recall = 122.10, batch_ndcg = 72.00 
2025-04-08 17:09:36.862018: Steps 103/138: batch_recall = 143.98, batch_ndcg = 80.63 
2025-04-08 17:09:38.129741: Steps 104/138: batch_recall = 133.35, batch_ndcg = 77.38 
2025-04-08 17:09:39.402785: Steps 105/138: batch_recall = 118.75, batch_ndcg = 66.89 
2025-04-08 17:09:40.678557: Steps 106/138: batch_recall = 105.21, batch_ndcg = 58.80 
2025-04-08 17:09:41.958874: Steps 107/138: batch_recall = 113.34, batch_ndcg = 63.98 
2025-04-08 17:09:43.239222: Steps 108/138: batch_recall = 117.98, batch_ndcg = 69.39 
2025-04-08 17:09:44.509704: Steps 109/138: batch_recall = 136.78, batch_ndcg = 76.61 
2025-04-08 17:09:45.771328: Steps 110/138: batch_recall = 123.44, batch_ndcg = 64.54 
2025-04-08 17:09:47.043369: Steps 111/138: batch_recall = 140.25, batch_ndcg = 84.30 
2025-04-08 17:09:48.315957: Steps 112/138: batch_recall = 154.35, batch_ndcg = 88.75 
2025-04-08 17:09:49.582232: Steps 113/138: batch_recall = 127.82, batch_ndcg = 71.72 
2025-04-08 17:09:50.850746: Steps 114/138: batch_recall = 125.62, batch_ndcg = 71.72 
2025-04-08 17:09:52.120465: Steps 115/138: batch_recall = 119.27, batch_ndcg = 63.57 
2025-04-08 17:09:53.385732: Steps 116/138: batch_recall = 123.91, batch_ndcg = 65.86 
2025-04-08 17:09:54.657456: Steps 117/138: batch_recall = 114.39, batch_ndcg = 66.53 
2025-04-08 17:09:55.940388: Steps 118/138: batch_recall = 127.95, batch_ndcg = 70.31 
2025-04-08 17:09:57.205368: Steps 119/138: batch_recall = 137.61, batch_ndcg = 74.64 
2025-04-08 17:09:58.478714: Steps 120/138: batch_recall = 127.16, batch_ndcg = 70.72 
2025-04-08 17:09:59.743310: Steps 121/138: batch_recall = 152.66, batch_ndcg = 80.37 
2025-04-08 17:10:01.003164: Steps 122/138: batch_recall = 153.22, batch_ndcg = 83.84 
2025-04-08 17:10:02.263396: Steps 123/138: batch_recall = 128.76, batch_ndcg = 72.72 
2025-04-08 17:10:03.536736: Steps 124/138: batch_recall = 152.55, batch_ndcg = 92.82 
2025-04-08 17:10:04.793265: Steps 125/138: batch_recall = 132.00, batch_ndcg = 73.01 
2025-04-08 17:10:06.052072: Steps 126/138: batch_recall = 158.19, batch_ndcg = 88.69 
2025-04-08 17:10:07.305090: Steps 127/138: batch_recall = 143.76, batch_ndcg = 80.85 
2025-04-08 17:10:08.559959: Steps 128/138: batch_recall = 128.10, batch_ndcg = 71.27 
2025-04-08 17:10:09.820342: Steps 129/138: batch_recall = 160.66, batch_ndcg = 91.80 
2025-04-08 17:10:11.084498: Steps 130/138: batch_recall = 131.91, batch_ndcg = 69.90 
2025-04-08 17:10:12.348574: Steps 131/138: batch_recall = 151.46, batch_ndcg = 86.99 
2025-04-08 17:10:13.612289: Steps 132/138: batch_recall = 147.33, batch_ndcg = 83.90 
2025-04-08 17:10:14.869497: Steps 133/138: batch_recall = 151.18, batch_ndcg = 86.03 
2025-04-08 17:10:16.127996: Steps 134/138: batch_recall = 138.79, batch_ndcg = 80.91 
2025-04-08 17:10:17.386579: Steps 135/138: batch_recall = 171.86, batch_ndcg = 97.87 
2025-04-08 17:10:18.644145: Steps 136/138: batch_recall = 154.34, batch_ndcg = 78.97 
2025-04-08 17:10:19.899440: Steps 137/138: batch_recall = 141.12, batch_ndcg = 86.32 
2025-04-08 17:10:19.899963: Epoch 21/1000, Test: Recall = 0.1765, NDCG = 0.1004  

2025-04-08 17:10:21.642484: Training Step 0/354: batchLoss = 0.6037, diffLoss = 2.9550, kgLoss = 0.0159
2025-04-08 17:10:23.246582: Training Step 1/354: batchLoss = 0.4964, diffLoss = 2.4304, kgLoss = 0.0130
2025-04-08 17:10:24.852067: Training Step 2/354: batchLoss = 0.4808, diffLoss = 2.3427, kgLoss = 0.0153
2025-04-08 17:10:26.454408: Training Step 3/354: batchLoss = 0.5309, diffLoss = 2.5980, kgLoss = 0.0142
2025-04-08 17:10:28.065493: Training Step 4/354: batchLoss = 0.6350, diffLoss = 3.1093, kgLoss = 0.0164
2025-04-08 17:10:29.674080: Training Step 5/354: batchLoss = 0.4380, diffLoss = 2.1408, kgLoss = 0.0123
2025-04-08 17:10:31.281258: Training Step 6/354: batchLoss = 0.6144, diffLoss = 3.0067, kgLoss = 0.0163
2025-04-08 17:10:32.894038: Training Step 7/354: batchLoss = 0.6468, diffLoss = 3.1677, kgLoss = 0.0165
2025-04-08 17:10:34.499978: Training Step 8/354: batchLoss = 0.4262, diffLoss = 2.0757, kgLoss = 0.0138
2025-04-08 17:10:36.104704: Training Step 9/354: batchLoss = 0.5329, diffLoss = 2.6085, kgLoss = 0.0140
2025-04-08 17:10:37.708074: Training Step 10/354: batchLoss = 0.5241, diffLoss = 2.5582, kgLoss = 0.0156
2025-04-08 17:10:39.307290: Training Step 11/354: batchLoss = 0.6526, diffLoss = 3.1995, kgLoss = 0.0159
2025-04-08 17:10:40.908360: Training Step 12/354: batchLoss = 0.5454, diffLoss = 2.6788, kgLoss = 0.0120
2025-04-08 17:10:42.515714: Training Step 13/354: batchLoss = 0.5814, diffLoss = 2.8445, kgLoss = 0.0156
2025-04-08 17:10:44.124251: Training Step 14/354: batchLoss = 0.5657, diffLoss = 2.7676, kgLoss = 0.0152
2025-04-08 17:10:45.732769: Training Step 15/354: batchLoss = 0.4060, diffLoss = 1.9773, kgLoss = 0.0131
2025-04-08 17:10:47.349767: Training Step 16/354: batchLoss = 0.5513, diffLoss = 2.7006, kgLoss = 0.0139
2025-04-08 17:10:48.960485: Training Step 17/354: batchLoss = 0.6336, diffLoss = 3.1031, kgLoss = 0.0163
2025-04-08 17:10:50.562385: Training Step 18/354: batchLoss = 0.4946, diffLoss = 2.4206, kgLoss = 0.0131
2025-04-08 17:10:52.164867: Training Step 19/354: batchLoss = 0.4445, diffLoss = 2.1741, kgLoss = 0.0121
2025-04-08 17:10:53.771558: Training Step 20/354: batchLoss = 0.6822, diffLoss = 3.3418, kgLoss = 0.0173
2025-04-08 17:10:55.373355: Training Step 21/354: batchLoss = 0.5078, diffLoss = 2.4812, kgLoss = 0.0144
2025-04-08 17:10:56.975981: Training Step 22/354: batchLoss = 0.5467, diffLoss = 2.6784, kgLoss = 0.0138
2025-04-08 17:10:58.583909: Training Step 23/354: batchLoss = 0.5189, diffLoss = 2.5386, kgLoss = 0.0139
2025-04-08 17:11:00.187933: Training Step 24/354: batchLoss = 0.5840, diffLoss = 2.8628, kgLoss = 0.0143
2025-04-08 17:11:01.797767: Training Step 25/354: batchLoss = 0.5312, diffLoss = 2.6004, kgLoss = 0.0140
2025-04-08 17:11:03.409662: Training Step 26/354: batchLoss = 0.5155, diffLoss = 2.5206, kgLoss = 0.0142
2025-04-08 17:11:05.024367: Training Step 27/354: batchLoss = 0.5066, diffLoss = 2.4860, kgLoss = 0.0117
2025-04-08 17:11:06.650543: Training Step 28/354: batchLoss = 0.7804, diffLoss = 3.8278, kgLoss = 0.0185
2025-04-08 17:11:08.259643: Training Step 29/354: batchLoss = 0.5525, diffLoss = 2.7110, kgLoss = 0.0129
2025-04-08 17:11:09.874313: Training Step 30/354: batchLoss = 0.5515, diffLoss = 2.6835, kgLoss = 0.0185
2025-04-08 17:11:11.486676: Training Step 31/354: batchLoss = 0.5177, diffLoss = 2.5317, kgLoss = 0.0142
2025-04-08 17:11:13.097431: Training Step 32/354: batchLoss = 0.6183, diffLoss = 3.0255, kgLoss = 0.0165
2025-04-08 17:11:14.713819: Training Step 33/354: batchLoss = 0.5116, diffLoss = 2.4985, kgLoss = 0.0149
2025-04-08 17:11:16.330337: Training Step 34/354: batchLoss = 0.6361, diffLoss = 3.1201, kgLoss = 0.0151
2025-04-08 17:11:17.939064: Training Step 35/354: batchLoss = 0.5992, diffLoss = 2.9378, kgLoss = 0.0145
2025-04-08 17:11:19.558089: Training Step 36/354: batchLoss = 0.5763, diffLoss = 2.8232, kgLoss = 0.0146
2025-04-08 17:11:21.175264: Training Step 37/354: batchLoss = 0.5922, diffLoss = 2.8977, kgLoss = 0.0159
2025-04-08 17:11:22.789139: Training Step 38/354: batchLoss = 0.5176, diffLoss = 2.5383, kgLoss = 0.0124
2025-04-08 17:11:24.389676: Training Step 39/354: batchLoss = 0.6263, diffLoss = 3.0683, kgLoss = 0.0158
2025-04-08 17:11:26.001636: Training Step 40/354: batchLoss = 0.5905, diffLoss = 2.8850, kgLoss = 0.0169
2025-04-08 17:11:27.614481: Training Step 41/354: batchLoss = 0.5010, diffLoss = 2.4497, kgLoss = 0.0138
2025-04-08 17:11:29.226953: Training Step 42/354: batchLoss = 0.5780, diffLoss = 2.8236, kgLoss = 0.0166
2025-04-08 17:11:30.847484: Training Step 43/354: batchLoss = 0.5234, diffLoss = 2.5599, kgLoss = 0.0143
2025-04-08 17:11:32.467104: Training Step 44/354: batchLoss = 0.4947, diffLoss = 2.4197, kgLoss = 0.0134
2025-04-08 17:11:34.089069: Training Step 45/354: batchLoss = 0.5543, diffLoss = 2.7133, kgLoss = 0.0145
2025-04-08 17:11:35.716675: Training Step 46/354: batchLoss = 0.6615, diffLoss = 3.2479, kgLoss = 0.0149
2025-04-08 17:11:37.337238: Training Step 47/354: batchLoss = 0.6081, diffLoss = 2.9814, kgLoss = 0.0148
2025-04-08 17:11:38.960064: Training Step 48/354: batchLoss = 0.5558, diffLoss = 2.7153, kgLoss = 0.0159
2025-04-08 17:11:40.570118: Training Step 49/354: batchLoss = 0.6732, diffLoss = 3.2937, kgLoss = 0.0181
2025-04-08 17:11:42.192870: Training Step 50/354: batchLoss = 0.4222, diffLoss = 2.0607, kgLoss = 0.0126
2025-04-08 17:11:43.811793: Training Step 51/354: batchLoss = 0.6568, diffLoss = 3.2140, kgLoss = 0.0175
2025-04-08 17:11:45.433866: Training Step 52/354: batchLoss = 0.6725, diffLoss = 3.3021, kgLoss = 0.0150
2025-04-08 17:11:47.061064: Training Step 53/354: batchLoss = 0.4864, diffLoss = 2.3778, kgLoss = 0.0135
2025-04-08 17:11:48.688444: Training Step 54/354: batchLoss = 0.5119, diffLoss = 2.5111, kgLoss = 0.0121
2025-04-08 17:11:50.312047: Training Step 55/354: batchLoss = 0.5785, diffLoss = 2.8328, kgLoss = 0.0149
2025-04-08 17:11:51.932565: Training Step 56/354: batchLoss = 0.6506, diffLoss = 3.1884, kgLoss = 0.0161
2025-04-08 17:11:53.552685: Training Step 57/354: batchLoss = 0.6016, diffLoss = 2.9471, kgLoss = 0.0152
2025-04-08 17:11:55.172380: Training Step 58/354: batchLoss = 0.5377, diffLoss = 2.6265, kgLoss = 0.0155
2025-04-08 17:11:56.785919: Training Step 59/354: batchLoss = 0.6088, diffLoss = 2.9831, kgLoss = 0.0152
2025-04-08 17:11:58.403148: Training Step 60/354: batchLoss = 0.6219, diffLoss = 3.0521, kgLoss = 0.0144
2025-04-08 17:12:00.016629: Training Step 61/354: batchLoss = 0.5764, diffLoss = 2.8171, kgLoss = 0.0163
2025-04-08 17:12:01.635167: Training Step 62/354: batchLoss = 0.5473, diffLoss = 2.6717, kgLoss = 0.0162
2025-04-08 17:12:03.256371: Training Step 63/354: batchLoss = 0.6272, diffLoss = 3.0660, kgLoss = 0.0175
2025-04-08 17:12:04.876495: Training Step 64/354: batchLoss = 0.5420, diffLoss = 2.6555, kgLoss = 0.0136
2025-04-08 17:12:06.504553: Training Step 65/354: batchLoss = 0.5628, diffLoss = 2.7577, kgLoss = 0.0141
2025-04-08 17:12:08.124305: Training Step 66/354: batchLoss = 0.5495, diffLoss = 2.6940, kgLoss = 0.0133
2025-04-08 17:12:09.750209: Training Step 67/354: batchLoss = 0.6039, diffLoss = 2.9533, kgLoss = 0.0165
2025-04-08 17:12:11.371999: Training Step 68/354: batchLoss = 0.5715, diffLoss = 2.8048, kgLoss = 0.0132
2025-04-08 17:12:12.990392: Training Step 69/354: batchLoss = 0.5485, diffLoss = 2.6876, kgLoss = 0.0137
2025-04-08 17:12:14.612309: Training Step 70/354: batchLoss = 0.5701, diffLoss = 2.7997, kgLoss = 0.0127
2025-04-08 17:12:16.228226: Training Step 71/354: batchLoss = 0.5181, diffLoss = 2.5405, kgLoss = 0.0125
2025-04-08 17:12:17.848073: Training Step 72/354: batchLoss = 0.6167, diffLoss = 3.0239, kgLoss = 0.0149
2025-04-08 17:12:19.475781: Training Step 73/354: batchLoss = 0.6052, diffLoss = 2.9614, kgLoss = 0.0162
2025-04-08 17:12:21.106183: Training Step 74/354: batchLoss = 0.6721, diffLoss = 3.2956, kgLoss = 0.0162
2025-04-08 17:12:22.725212: Training Step 75/354: batchLoss = 0.6987, diffLoss = 3.4201, kgLoss = 0.0183
2025-04-08 17:12:24.346764: Training Step 76/354: batchLoss = 0.6588, diffLoss = 3.2231, kgLoss = 0.0177
2025-04-08 17:12:25.962366: Training Step 77/354: batchLoss = 0.6026, diffLoss = 2.9529, kgLoss = 0.0150
2025-04-08 17:12:27.576266: Training Step 78/354: batchLoss = 0.6096, diffLoss = 2.9835, kgLoss = 0.0161
2025-04-08 17:12:29.191864: Training Step 79/354: batchLoss = 0.4653, diffLoss = 2.2834, kgLoss = 0.0108
2025-04-08 17:12:30.809885: Training Step 80/354: batchLoss = 0.8027, diffLoss = 3.9336, kgLoss = 0.0200
2025-04-08 17:12:32.425061: Training Step 81/354: batchLoss = 0.6876, diffLoss = 3.3715, kgLoss = 0.0167
2025-04-08 17:12:34.038169: Training Step 82/354: batchLoss = 0.6437, diffLoss = 3.1537, kgLoss = 0.0162
2025-04-08 17:12:35.653480: Training Step 83/354: batchLoss = 0.6736, diffLoss = 3.3029, kgLoss = 0.0162
2025-04-08 17:12:37.272590: Training Step 84/354: batchLoss = 0.7033, diffLoss = 3.4441, kgLoss = 0.0182
2025-04-08 17:12:38.892839: Training Step 85/354: batchLoss = 0.4752, diffLoss = 2.3223, kgLoss = 0.0135
2025-04-08 17:12:40.500378: Training Step 86/354: batchLoss = 0.5774, diffLoss = 2.8295, kgLoss = 0.0144
2025-04-08 17:12:42.115515: Training Step 87/354: batchLoss = 0.6404, diffLoss = 3.1316, kgLoss = 0.0177
2025-04-08 17:12:43.728176: Training Step 88/354: batchLoss = 0.6151, diffLoss = 3.0085, kgLoss = 0.0168
2025-04-08 17:12:45.338856: Training Step 89/354: batchLoss = 0.5615, diffLoss = 2.7431, kgLoss = 0.0161
2025-04-08 17:12:46.955876: Training Step 90/354: batchLoss = 0.4248, diffLoss = 2.0807, kgLoss = 0.0108
2025-04-08 17:12:48.568477: Training Step 91/354: batchLoss = 0.6385, diffLoss = 3.1173, kgLoss = 0.0188
2025-04-08 17:12:50.187356: Training Step 92/354: batchLoss = 0.4443, diffLoss = 2.1692, kgLoss = 0.0131
2025-04-08 17:12:51.808465: Training Step 93/354: batchLoss = 0.5475, diffLoss = 2.6842, kgLoss = 0.0133
2025-04-08 17:12:53.427625: Training Step 94/354: batchLoss = 0.4957, diffLoss = 2.4261, kgLoss = 0.0131
2025-04-08 17:12:55.047155: Training Step 95/354: batchLoss = 0.4304, diffLoss = 2.1034, kgLoss = 0.0121
2025-04-08 17:12:56.667795: Training Step 96/354: batchLoss = 0.6993, diffLoss = 3.4258, kgLoss = 0.0177
2025-04-08 17:12:58.288399: Training Step 97/354: batchLoss = 0.6808, diffLoss = 3.3359, kgLoss = 0.0170
2025-04-08 17:12:59.905076: Training Step 98/354: batchLoss = 0.6863, diffLoss = 3.3684, kgLoss = 0.0158
2025-04-08 17:13:01.513173: Training Step 99/354: batchLoss = 0.5703, diffLoss = 2.7873, kgLoss = 0.0160
2025-04-08 17:13:03.115067: Training Step 100/354: batchLoss = 0.4419, diffLoss = 2.1487, kgLoss = 0.0152
2025-04-08 17:13:04.726686: Training Step 101/354: batchLoss = 0.5184, diffLoss = 2.5375, kgLoss = 0.0136
2025-04-08 17:13:06.339307: Training Step 102/354: batchLoss = 0.5941, diffLoss = 2.9131, kgLoss = 0.0144
2025-04-08 17:13:07.953671: Training Step 103/354: batchLoss = 0.5436, diffLoss = 2.6597, kgLoss = 0.0146
2025-04-08 17:13:09.571810: Training Step 104/354: batchLoss = 0.5079, diffLoss = 2.4823, kgLoss = 0.0143
2025-04-08 17:13:11.201289: Training Step 105/354: batchLoss = 0.5554, diffLoss = 2.7250, kgLoss = 0.0130
2025-04-08 17:13:12.818545: Training Step 106/354: batchLoss = 0.7551, diffLoss = 3.7019, kgLoss = 0.0184
2025-04-08 17:13:14.444556: Training Step 107/354: batchLoss = 0.5152, diffLoss = 2.5185, kgLoss = 0.0144
2025-04-08 17:13:16.065695: Training Step 108/354: batchLoss = 0.5674, diffLoss = 2.7809, kgLoss = 0.0141
2025-04-08 17:13:17.679052: Training Step 109/354: batchLoss = 0.4715, diffLoss = 2.3068, kgLoss = 0.0127
2025-04-08 17:13:19.303446: Training Step 110/354: batchLoss = 0.5645, diffLoss = 2.7604, kgLoss = 0.0155
2025-04-08 17:13:20.921245: Training Step 111/354: batchLoss = 0.5394, diffLoss = 2.6355, kgLoss = 0.0154
2025-04-08 17:13:22.534476: Training Step 112/354: batchLoss = 0.4742, diffLoss = 2.3048, kgLoss = 0.0165
2025-04-08 17:13:24.151034: Training Step 113/354: batchLoss = 0.6410, diffLoss = 3.1393, kgLoss = 0.0165
2025-04-08 17:13:25.770891: Training Step 114/354: batchLoss = 0.5802, diffLoss = 2.8447, kgLoss = 0.0141
2025-04-08 17:13:27.395247: Training Step 115/354: batchLoss = 0.5326, diffLoss = 2.6082, kgLoss = 0.0138
2025-04-08 17:13:29.015635: Training Step 116/354: batchLoss = 0.5351, diffLoss = 2.6181, kgLoss = 0.0143
2025-04-08 17:13:30.632416: Training Step 117/354: batchLoss = 0.4828, diffLoss = 2.3662, kgLoss = 0.0119
2025-04-08 17:13:32.251116: Training Step 118/354: batchLoss = 0.6067, diffLoss = 2.9730, kgLoss = 0.0151
2025-04-08 17:13:33.873052: Training Step 119/354: batchLoss = 0.4346, diffLoss = 2.1307, kgLoss = 0.0106
2025-04-08 17:13:35.483668: Training Step 120/354: batchLoss = 0.6420, diffLoss = 3.1460, kgLoss = 0.0161
2025-04-08 17:13:37.096848: Training Step 121/354: batchLoss = 0.7407, diffLoss = 3.6350, kgLoss = 0.0171
2025-04-08 17:13:38.707734: Training Step 122/354: batchLoss = 0.8683, diffLoss = 4.2534, kgLoss = 0.0220
2025-04-08 17:13:40.323006: Training Step 123/354: batchLoss = 0.5674, diffLoss = 2.7847, kgLoss = 0.0131
2025-04-08 17:13:41.944652: Training Step 124/354: batchLoss = 0.6224, diffLoss = 3.0516, kgLoss = 0.0151
2025-04-08 17:13:43.561816: Training Step 125/354: batchLoss = 0.5910, diffLoss = 2.8990, kgLoss = 0.0139
2025-04-08 17:13:45.179666: Training Step 126/354: batchLoss = 0.5206, diffLoss = 2.5559, kgLoss = 0.0117
2025-04-08 17:13:46.795957: Training Step 127/354: batchLoss = 0.5566, diffLoss = 2.7253, kgLoss = 0.0144
2025-04-08 17:13:48.409275: Training Step 128/354: batchLoss = 0.6369, diffLoss = 3.1261, kgLoss = 0.0147
2025-04-08 17:13:50.025680: Training Step 129/354: batchLoss = 0.5351, diffLoss = 2.6051, kgLoss = 0.0176
2025-04-08 17:13:51.641162: Training Step 130/354: batchLoss = 0.3868, diffLoss = 1.8901, kgLoss = 0.0110
2025-04-08 17:13:53.246647: Training Step 131/354: batchLoss = 0.5259, diffLoss = 2.5778, kgLoss = 0.0129
2025-04-08 17:13:54.860191: Training Step 132/354: batchLoss = 0.6195, diffLoss = 3.0238, kgLoss = 0.0185
2025-04-08 17:13:56.485885: Training Step 133/354: batchLoss = 0.6348, diffLoss = 3.1089, kgLoss = 0.0163
2025-04-08 17:13:58.096759: Training Step 134/354: batchLoss = 0.5774, diffLoss = 2.8353, kgLoss = 0.0129
2025-04-08 17:13:59.711038: Training Step 135/354: batchLoss = 0.5423, diffLoss = 2.6518, kgLoss = 0.0149
2025-04-08 17:14:01.324068: Training Step 136/354: batchLoss = 0.6542, diffLoss = 3.2069, kgLoss = 0.0161
2025-04-08 17:14:02.942497: Training Step 137/354: batchLoss = 0.4868, diffLoss = 2.3824, kgLoss = 0.0129
2025-04-08 17:14:04.552625: Training Step 138/354: batchLoss = 0.5863, diffLoss = 2.8785, kgLoss = 0.0132
2025-04-08 17:14:06.165463: Training Step 139/354: batchLoss = 0.5077, diffLoss = 2.4857, kgLoss = 0.0132
2025-04-08 17:14:07.773801: Training Step 140/354: batchLoss = 0.4952, diffLoss = 2.4203, kgLoss = 0.0140
2025-04-08 17:14:09.388157: Training Step 141/354: batchLoss = 0.5914, diffLoss = 2.9007, kgLoss = 0.0140
2025-04-08 17:14:11.003496: Training Step 142/354: batchLoss = 0.5774, diffLoss = 2.8278, kgLoss = 0.0149
2025-04-08 17:14:12.620642: Training Step 143/354: batchLoss = 0.5548, diffLoss = 2.7140, kgLoss = 0.0150
2025-04-08 17:14:14.237470: Training Step 144/354: batchLoss = 0.4763, diffLoss = 2.3335, kgLoss = 0.0120
2025-04-08 17:14:15.842859: Training Step 145/354: batchLoss = 0.5723, diffLoss = 2.8029, kgLoss = 0.0146
2025-04-08 17:14:17.457223: Training Step 146/354: batchLoss = 0.5817, diffLoss = 2.8435, kgLoss = 0.0162
2025-04-08 17:14:19.063738: Training Step 147/354: batchLoss = 0.5290, diffLoss = 2.5912, kgLoss = 0.0135
2025-04-08 17:14:20.670049: Training Step 148/354: batchLoss = 0.6209, diffLoss = 3.0423, kgLoss = 0.0155
2025-04-08 17:14:22.269419: Training Step 149/354: batchLoss = 0.5694, diffLoss = 2.7893, kgLoss = 0.0144
2025-04-08 17:14:23.871638: Training Step 150/354: batchLoss = 0.5040, diffLoss = 2.4712, kgLoss = 0.0122
2025-04-08 17:14:25.474989: Training Step 151/354: batchLoss = 0.6625, diffLoss = 3.2449, kgLoss = 0.0168
2025-04-08 17:14:27.076352: Training Step 152/354: batchLoss = 0.5255, diffLoss = 2.5734, kgLoss = 0.0135
2025-04-08 17:14:28.687231: Training Step 153/354: batchLoss = 0.4588, diffLoss = 2.2481, kgLoss = 0.0115
2025-04-08 17:14:30.294524: Training Step 154/354: batchLoss = 0.6105, diffLoss = 2.9787, kgLoss = 0.0185
2025-04-08 17:14:31.905435: Training Step 155/354: batchLoss = 0.5646, diffLoss = 2.7665, kgLoss = 0.0141
2025-04-08 17:14:33.518295: Training Step 156/354: batchLoss = 0.7061, diffLoss = 3.4598, kgLoss = 0.0177
2025-04-08 17:14:35.122033: Training Step 157/354: batchLoss = 0.5432, diffLoss = 2.6615, kgLoss = 0.0136
2025-04-08 17:14:36.729524: Training Step 158/354: batchLoss = 0.4674, diffLoss = 2.2869, kgLoss = 0.0125
2025-04-08 17:14:38.337271: Training Step 159/354: batchLoss = 0.4758, diffLoss = 2.3199, kgLoss = 0.0148
2025-04-08 17:14:39.955478: Training Step 160/354: batchLoss = 0.6466, diffLoss = 3.1740, kgLoss = 0.0147
2025-04-08 17:14:41.575330: Training Step 161/354: batchLoss = 0.4332, diffLoss = 2.1206, kgLoss = 0.0114
2025-04-08 17:14:43.193459: Training Step 162/354: batchLoss = 0.6465, diffLoss = 3.1564, kgLoss = 0.0190
2025-04-08 17:14:44.820732: Training Step 163/354: batchLoss = 0.5078, diffLoss = 2.4812, kgLoss = 0.0145
2025-04-08 17:14:46.445009: Training Step 164/354: batchLoss = 0.6235, diffLoss = 3.0573, kgLoss = 0.0150
2025-04-08 17:14:48.062218: Training Step 165/354: batchLoss = 0.5660, diffLoss = 2.7746, kgLoss = 0.0139
2025-04-08 17:14:49.694057: Training Step 166/354: batchLoss = 0.5172, diffLoss = 2.5320, kgLoss = 0.0135
2025-04-08 17:14:51.303200: Training Step 167/354: batchLoss = 0.5459, diffLoss = 2.6757, kgLoss = 0.0135
2025-04-08 17:14:52.918500: Training Step 168/354: batchLoss = 0.7949, diffLoss = 3.8988, kgLoss = 0.0189
2025-04-08 17:14:54.536594: Training Step 169/354: batchLoss = 0.4375, diffLoss = 2.1385, kgLoss = 0.0123
2025-04-08 17:14:56.153695: Training Step 170/354: batchLoss = 0.5843, diffLoss = 2.8524, kgLoss = 0.0173
2025-04-08 17:14:57.776211: Training Step 171/354: batchLoss = 0.4716, diffLoss = 2.3071, kgLoss = 0.0127
2025-04-08 17:14:59.398639: Training Step 172/354: batchLoss = 0.5954, diffLoss = 2.9153, kgLoss = 0.0155
2025-04-08 17:15:01.026316: Training Step 173/354: batchLoss = 0.4491, diffLoss = 2.1917, kgLoss = 0.0135
2025-04-08 17:15:02.647084: Training Step 174/354: batchLoss = 0.4849, diffLoss = 2.3759, kgLoss = 0.0121
2025-04-08 17:15:04.271156: Training Step 175/354: batchLoss = 0.5607, diffLoss = 2.7435, kgLoss = 0.0150
2025-04-08 17:15:05.901231: Training Step 176/354: batchLoss = 0.5551, diffLoss = 2.7116, kgLoss = 0.0159
2025-04-08 17:15:07.517892: Training Step 177/354: batchLoss = 0.7096, diffLoss = 3.4822, kgLoss = 0.0165
2025-04-08 17:15:09.142282: Training Step 178/354: batchLoss = 0.6237, diffLoss = 3.0505, kgLoss = 0.0170
2025-04-08 17:15:10.774770: Training Step 179/354: batchLoss = 0.4038, diffLoss = 1.9750, kgLoss = 0.0109
2025-04-08 17:15:12.397172: Training Step 180/354: batchLoss = 0.6961, diffLoss = 3.4138, kgLoss = 0.0167
2025-04-08 17:15:14.021519: Training Step 181/354: batchLoss = 0.5369, diffLoss = 2.6297, kgLoss = 0.0137
2025-04-08 17:15:15.642874: Training Step 182/354: batchLoss = 0.6185, diffLoss = 3.0342, kgLoss = 0.0146
2025-04-08 17:15:17.279481: Training Step 183/354: batchLoss = 0.5509, diffLoss = 2.6984, kgLoss = 0.0140
2025-04-08 17:15:18.900181: Training Step 184/354: batchLoss = 0.4952, diffLoss = 2.4217, kgLoss = 0.0135
2025-04-08 17:15:20.519497: Training Step 185/354: batchLoss = 0.4161, diffLoss = 2.0333, kgLoss = 0.0117
2025-04-08 17:15:22.145410: Training Step 186/354: batchLoss = 0.5105, diffLoss = 2.4938, kgLoss = 0.0147
2025-04-08 17:15:23.777853: Training Step 187/354: batchLoss = 0.5918, diffLoss = 2.8955, kgLoss = 0.0159
2025-04-08 17:15:25.396023: Training Step 188/354: batchLoss = 0.5626, diffLoss = 2.7552, kgLoss = 0.0144
2025-04-08 17:15:27.017756: Training Step 189/354: batchLoss = 0.5610, diffLoss = 2.7433, kgLoss = 0.0154
2025-04-08 17:15:28.636039: Training Step 190/354: batchLoss = 0.4966, diffLoss = 2.4296, kgLoss = 0.0133
2025-04-08 17:15:30.267798: Training Step 191/354: batchLoss = 0.5131, diffLoss = 2.5213, kgLoss = 0.0110
2025-04-08 17:15:31.898016: Training Step 192/354: batchLoss = 0.5440, diffLoss = 2.6685, kgLoss = 0.0129
2025-04-08 17:15:33.521252: Training Step 193/354: batchLoss = 0.5875, diffLoss = 2.8766, kgLoss = 0.0152
2025-04-08 17:15:35.148034: Training Step 194/354: batchLoss = 0.7263, diffLoss = 3.5610, kgLoss = 0.0176
2025-04-08 17:15:36.772889: Training Step 195/354: batchLoss = 0.6457, diffLoss = 3.1673, kgLoss = 0.0154
2025-04-08 17:15:38.396928: Training Step 196/354: batchLoss = 0.6547, diffLoss = 3.2054, kgLoss = 0.0170
2025-04-08 17:15:40.022707: Training Step 197/354: batchLoss = 0.5218, diffLoss = 2.5560, kgLoss = 0.0132
2025-04-08 17:15:41.636251: Training Step 198/354: batchLoss = 0.6352, diffLoss = 3.1095, kgLoss = 0.0167
2025-04-08 17:15:43.250143: Training Step 199/354: batchLoss = 0.4922, diffLoss = 2.4069, kgLoss = 0.0135
2025-04-08 17:15:44.871558: Training Step 200/354: batchLoss = 0.5264, diffLoss = 2.5797, kgLoss = 0.0131
2025-04-08 17:15:46.491312: Training Step 201/354: batchLoss = 0.4966, diffLoss = 2.4350, kgLoss = 0.0121
2025-04-08 17:15:48.115439: Training Step 202/354: batchLoss = 0.4923, diffLoss = 2.4131, kgLoss = 0.0121
2025-04-08 17:15:49.729885: Training Step 203/354: batchLoss = 0.5520, diffLoss = 2.7036, kgLoss = 0.0141
2025-04-08 17:15:51.356662: Training Step 204/354: batchLoss = 0.6262, diffLoss = 3.0631, kgLoss = 0.0170
2025-04-08 17:15:52.985427: Training Step 205/354: batchLoss = 0.5436, diffLoss = 2.6593, kgLoss = 0.0147
2025-04-08 17:15:54.609130: Training Step 206/354: batchLoss = 0.5308, diffLoss = 2.6005, kgLoss = 0.0134
2025-04-08 17:15:56.229836: Training Step 207/354: batchLoss = 0.4713, diffLoss = 2.3076, kgLoss = 0.0122
2025-04-08 17:15:57.850442: Training Step 208/354: batchLoss = 0.7126, diffLoss = 3.4944, kgLoss = 0.0171
2025-04-08 17:15:59.474190: Training Step 209/354: batchLoss = 0.6731, diffLoss = 3.2963, kgLoss = 0.0173
2025-04-08 17:16:01.083375: Training Step 210/354: batchLoss = 1.6755, diffLoss = 8.2197, kgLoss = 0.0395
2025-04-08 17:16:02.709579: Training Step 211/354: batchLoss = 0.6001, diffLoss = 2.9323, kgLoss = 0.0170
2025-04-08 17:16:04.334731: Training Step 212/354: batchLoss = 0.7219, diffLoss = 3.5383, kgLoss = 0.0179
2025-04-08 17:16:05.971514: Training Step 213/354: batchLoss = 0.5767, diffLoss = 2.8262, kgLoss = 0.0144
2025-04-08 17:16:07.594781: Training Step 214/354: batchLoss = 0.4650, diffLoss = 2.2703, kgLoss = 0.0137
2025-04-08 17:16:09.225480: Training Step 215/354: batchLoss = 0.5312, diffLoss = 2.5943, kgLoss = 0.0154
2025-04-08 17:16:10.850294: Training Step 216/354: batchLoss = 0.5609, diffLoss = 2.7489, kgLoss = 0.0139
2025-04-08 17:16:12.469293: Training Step 217/354: batchLoss = 0.5643, diffLoss = 2.7673, kgLoss = 0.0135
2025-04-08 17:16:14.087244: Training Step 218/354: batchLoss = 0.5304, diffLoss = 2.5904, kgLoss = 0.0154
2025-04-08 17:16:15.707163: Training Step 219/354: batchLoss = 0.4499, diffLoss = 2.1984, kgLoss = 0.0128
2025-04-08 17:16:17.333777: Training Step 220/354: batchLoss = 0.5062, diffLoss = 2.4743, kgLoss = 0.0141
2025-04-08 17:16:18.962298: Training Step 221/354: batchLoss = 0.4804, diffLoss = 2.3380, kgLoss = 0.0161
2025-04-08 17:16:20.592996: Training Step 222/354: batchLoss = 0.5236, diffLoss = 2.5609, kgLoss = 0.0142
2025-04-08 17:16:22.228583: Training Step 223/354: batchLoss = 0.4998, diffLoss = 2.4430, kgLoss = 0.0140
2025-04-08 17:16:23.855239: Training Step 224/354: batchLoss = 0.4524, diffLoss = 2.2159, kgLoss = 0.0116
2025-04-08 17:16:25.491472: Training Step 225/354: batchLoss = 0.7107, diffLoss = 3.4886, kgLoss = 0.0163
2025-04-08 17:16:27.121684: Training Step 226/354: batchLoss = 0.5255, diffLoss = 2.5797, kgLoss = 0.0119
2025-04-08 17:16:28.736136: Training Step 227/354: batchLoss = 0.5592, diffLoss = 2.7357, kgLoss = 0.0150
2025-04-08 17:16:30.351033: Training Step 228/354: batchLoss = 0.5478, diffLoss = 2.6849, kgLoss = 0.0135
2025-04-08 17:16:31.976104: Training Step 229/354: batchLoss = 0.5464, diffLoss = 2.6823, kgLoss = 0.0124
2025-04-08 17:16:33.598694: Training Step 230/354: batchLoss = 0.4616, diffLoss = 2.2540, kgLoss = 0.0135
2025-04-08 17:16:35.226731: Training Step 231/354: batchLoss = 0.6222, diffLoss = 3.0352, kgLoss = 0.0189
2025-04-08 17:16:36.858389: Training Step 232/354: batchLoss = 0.4800, diffLoss = 2.3383, kgLoss = 0.0154
2025-04-08 17:16:38.481774: Training Step 233/354: batchLoss = 0.5897, diffLoss = 2.8914, kgLoss = 0.0142
2025-04-08 17:16:40.109190: Training Step 234/354: batchLoss = 0.5005, diffLoss = 2.4519, kgLoss = 0.0127
2025-04-08 17:16:41.740385: Training Step 235/354: batchLoss = 0.4757, diffLoss = 2.3261, kgLoss = 0.0131
2025-04-08 17:16:43.360828: Training Step 236/354: batchLoss = 0.5136, diffLoss = 2.5068, kgLoss = 0.0153
2025-04-08 17:16:44.975016: Training Step 237/354: batchLoss = 0.7038, diffLoss = 3.4517, kgLoss = 0.0169
2025-04-08 17:16:46.590444: Training Step 238/354: batchLoss = 0.5355, diffLoss = 2.6210, kgLoss = 0.0141
2025-04-08 17:16:48.207064: Training Step 239/354: batchLoss = 0.6791, diffLoss = 3.3279, kgLoss = 0.0168
2025-04-08 17:16:49.837701: Training Step 240/354: batchLoss = 0.5914, diffLoss = 2.8993, kgLoss = 0.0144
2025-04-08 17:16:51.457458: Training Step 241/354: batchLoss = 0.4693, diffLoss = 2.3003, kgLoss = 0.0115
2025-04-08 17:16:53.077527: Training Step 242/354: batchLoss = 0.5081, diffLoss = 2.4862, kgLoss = 0.0135
2025-04-08 17:16:54.707824: Training Step 243/354: batchLoss = 0.5850, diffLoss = 2.8581, kgLoss = 0.0168
2025-04-08 17:16:56.336814: Training Step 244/354: batchLoss = 0.4079, diffLoss = 1.9865, kgLoss = 0.0133
2025-04-08 17:16:57.967390: Training Step 245/354: batchLoss = 0.6561, diffLoss = 3.2056, kgLoss = 0.0187
2025-04-08 17:16:59.588658: Training Step 246/354: batchLoss = 0.5973, diffLoss = 2.9248, kgLoss = 0.0154
2025-04-08 17:17:01.219247: Training Step 247/354: batchLoss = 0.5411, diffLoss = 2.6474, kgLoss = 0.0145
2025-04-08 17:17:02.839731: Training Step 248/354: batchLoss = 0.5318, diffLoss = 2.6043, kgLoss = 0.0137
2025-04-08 17:17:04.463276: Training Step 249/354: batchLoss = 0.5484, diffLoss = 2.6841, kgLoss = 0.0145
2025-04-08 17:17:06.085849: Training Step 250/354: batchLoss = 0.4797, diffLoss = 2.3439, kgLoss = 0.0137
2025-04-08 17:17:07.720777: Training Step 251/354: batchLoss = 0.5283, diffLoss = 2.5883, kgLoss = 0.0133
2025-04-08 17:17:09.347053: Training Step 252/354: batchLoss = 0.5337, diffLoss = 2.5960, kgLoss = 0.0181
2025-04-08 17:17:10.979885: Training Step 253/354: batchLoss = 0.5714, diffLoss = 2.7917, kgLoss = 0.0163
2025-04-08 17:17:12.604356: Training Step 254/354: batchLoss = 0.7325, diffLoss = 3.5923, kgLoss = 0.0176
2025-04-08 17:17:14.226695: Training Step 255/354: batchLoss = 0.6490, diffLoss = 3.1842, kgLoss = 0.0152
2025-04-08 17:17:15.838864: Training Step 256/354: batchLoss = 0.5317, diffLoss = 2.6029, kgLoss = 0.0139
2025-04-08 17:17:17.464128: Training Step 257/354: batchLoss = 0.5288, diffLoss = 2.5861, kgLoss = 0.0145
2025-04-08 17:17:19.073060: Training Step 258/354: batchLoss = 0.5636, diffLoss = 2.7598, kgLoss = 0.0145
2025-04-08 17:17:20.688720: Training Step 259/354: batchLoss = 0.5346, diffLoss = 2.6112, kgLoss = 0.0154
2025-04-08 17:17:22.313013: Training Step 260/354: batchLoss = 0.5213, diffLoss = 2.5520, kgLoss = 0.0136
2025-04-08 17:17:23.936426: Training Step 261/354: batchLoss = 0.5249, diffLoss = 2.5489, kgLoss = 0.0189
2025-04-08 17:17:25.560970: Training Step 262/354: batchLoss = 0.5069, diffLoss = 2.4838, kgLoss = 0.0126
2025-04-08 17:17:27.187822: Training Step 263/354: batchLoss = 0.5164, diffLoss = 2.5332, kgLoss = 0.0122
2025-04-08 17:17:28.810726: Training Step 264/354: batchLoss = 0.5370, diffLoss = 2.6334, kgLoss = 0.0129
2025-04-08 17:17:30.431446: Training Step 265/354: batchLoss = 0.6839, diffLoss = 3.3579, kgLoss = 0.0155
2025-04-08 17:17:32.049554: Training Step 266/354: batchLoss = 0.6871, diffLoss = 3.3612, kgLoss = 0.0186
2025-04-08 17:17:33.673842: Training Step 267/354: batchLoss = 0.4869, diffLoss = 2.3834, kgLoss = 0.0128
2025-04-08 17:17:35.283535: Training Step 268/354: batchLoss = 0.4152, diffLoss = 2.0280, kgLoss = 0.0120
2025-04-08 17:17:36.900442: Training Step 269/354: batchLoss = 0.4766, diffLoss = 2.3316, kgLoss = 0.0129
2025-04-08 17:17:38.521786: Training Step 270/354: batchLoss = 0.5476, diffLoss = 2.6804, kgLoss = 0.0143
2025-04-08 17:17:40.149005: Training Step 271/354: batchLoss = 0.5117, diffLoss = 2.4983, kgLoss = 0.0150
2025-04-08 17:17:41.765601: Training Step 272/354: batchLoss = 0.4763, diffLoss = 2.3296, kgLoss = 0.0130
2025-04-08 17:17:43.389664: Training Step 273/354: batchLoss = 0.6131, diffLoss = 3.0037, kgLoss = 0.0154
2025-04-08 17:17:45.018675: Training Step 274/354: batchLoss = 0.5380, diffLoss = 2.6325, kgLoss = 0.0144
2025-04-08 17:17:46.642661: Training Step 275/354: batchLoss = 0.5638, diffLoss = 2.7614, kgLoss = 0.0144
2025-04-08 17:17:48.257295: Training Step 276/354: batchLoss = 0.4999, diffLoss = 2.4461, kgLoss = 0.0133
2025-04-08 17:17:49.875846: Training Step 277/354: batchLoss = 0.6463, diffLoss = 3.1686, kgLoss = 0.0157
2025-04-08 17:17:51.494517: Training Step 278/354: batchLoss = 0.4628, diffLoss = 2.2519, kgLoss = 0.0155
2025-04-08 17:17:53.109583: Training Step 279/354: batchLoss = 0.5846, diffLoss = 2.8712, kgLoss = 0.0129
2025-04-08 17:17:54.731219: Training Step 280/354: batchLoss = 0.4860, diffLoss = 2.3777, kgLoss = 0.0130
2025-04-08 17:17:56.358069: Training Step 281/354: batchLoss = 0.6517, diffLoss = 3.1898, kgLoss = 0.0171
2025-04-08 17:17:57.984920: Training Step 282/354: batchLoss = 0.6180, diffLoss = 3.0289, kgLoss = 0.0152
2025-04-08 17:17:59.615197: Training Step 283/354: batchLoss = 0.6705, diffLoss = 3.2884, kgLoss = 0.0161
2025-04-08 17:18:01.239172: Training Step 284/354: batchLoss = 0.5171, diffLoss = 2.5342, kgLoss = 0.0128
2025-04-08 17:18:02.863779: Training Step 285/354: batchLoss = 0.4239, diffLoss = 2.0674, kgLoss = 0.0130
2025-04-08 17:18:04.477205: Training Step 286/354: batchLoss = 0.7075, diffLoss = 3.4695, kgLoss = 0.0170
2025-04-08 17:18:06.099335: Training Step 287/354: batchLoss = 0.4588, diffLoss = 2.2445, kgLoss = 0.0124
2025-04-08 17:18:07.719149: Training Step 288/354: batchLoss = 0.4993, diffLoss = 2.4429, kgLoss = 0.0134
2025-04-08 17:18:09.334200: Training Step 289/354: batchLoss = 0.6782, diffLoss = 3.3218, kgLoss = 0.0173
2025-04-08 17:18:10.955669: Training Step 290/354: batchLoss = 0.5513, diffLoss = 2.6973, kgLoss = 0.0148
2025-04-08 17:18:12.579299: Training Step 291/354: batchLoss = 0.5538, diffLoss = 2.7159, kgLoss = 0.0133
2025-04-08 17:18:14.204137: Training Step 292/354: batchLoss = 0.5232, diffLoss = 2.5672, kgLoss = 0.0122
2025-04-08 17:18:15.833805: Training Step 293/354: batchLoss = 0.5572, diffLoss = 2.7261, kgLoss = 0.0149
2025-04-08 17:18:17.472450: Training Step 294/354: batchLoss = 0.5079, diffLoss = 2.4911, kgLoss = 0.0120
2025-04-08 17:18:19.097072: Training Step 295/354: batchLoss = 0.5208, diffLoss = 2.5502, kgLoss = 0.0134
2025-04-08 17:18:20.711496: Training Step 296/354: batchLoss = 0.5058, diffLoss = 2.4733, kgLoss = 0.0140
2025-04-08 17:18:22.332849: Training Step 297/354: batchLoss = 0.7190, diffLoss = 3.5205, kgLoss = 0.0186
2025-04-08 17:18:23.945471: Training Step 298/354: batchLoss = 0.5329, diffLoss = 2.6091, kgLoss = 0.0139
2025-04-08 17:18:25.564061: Training Step 299/354: batchLoss = 0.4829, diffLoss = 2.3627, kgLoss = 0.0129
2025-04-08 17:18:27.192313: Training Step 300/354: batchLoss = 0.5449, diffLoss = 2.6681, kgLoss = 0.0141
2025-04-08 17:18:28.819115: Training Step 301/354: batchLoss = 0.5813, diffLoss = 2.8524, kgLoss = 0.0135
2025-04-08 17:18:30.437996: Training Step 302/354: batchLoss = 0.4174, diffLoss = 2.0382, kgLoss = 0.0123
2025-04-08 17:18:32.060331: Training Step 303/354: batchLoss = 0.5370, diffLoss = 2.6306, kgLoss = 0.0136
2025-04-08 17:18:33.683276: Training Step 304/354: batchLoss = 0.5784, diffLoss = 2.8326, kgLoss = 0.0149
2025-04-08 17:18:35.302859: Training Step 305/354: batchLoss = 0.5875, diffLoss = 2.8779, kgLoss = 0.0149
2025-04-08 17:18:36.913162: Training Step 306/354: batchLoss = 0.5653, diffLoss = 2.7713, kgLoss = 0.0138
2025-04-08 17:18:38.523026: Training Step 307/354: batchLoss = 0.6038, diffLoss = 2.9613, kgLoss = 0.0145
2025-04-08 17:18:40.146643: Training Step 308/354: batchLoss = 0.5346, diffLoss = 2.6214, kgLoss = 0.0129
2025-04-08 17:18:41.770648: Training Step 309/354: batchLoss = 0.5980, diffLoss = 2.9313, kgLoss = 0.0146
2025-04-08 17:18:43.398023: Training Step 310/354: batchLoss = 0.5155, diffLoss = 2.5212, kgLoss = 0.0141
2025-04-08 17:18:45.019830: Training Step 311/354: batchLoss = 0.6165, diffLoss = 3.0246, kgLoss = 0.0145
2025-04-08 17:18:46.646648: Training Step 312/354: batchLoss = 0.5530, diffLoss = 2.7116, kgLoss = 0.0134
2025-04-08 17:18:48.274596: Training Step 313/354: batchLoss = 0.5068, diffLoss = 2.4883, kgLoss = 0.0115
2025-04-08 17:18:49.899522: Training Step 314/354: batchLoss = 0.4949, diffLoss = 2.4219, kgLoss = 0.0132
2025-04-08 17:18:51.519090: Training Step 315/354: batchLoss = 0.6067, diffLoss = 2.9763, kgLoss = 0.0143
2025-04-08 17:18:53.133372: Training Step 316/354: batchLoss = 0.5211, diffLoss = 2.5463, kgLoss = 0.0147
2025-04-08 17:18:54.750109: Training Step 317/354: batchLoss = 0.4741, diffLoss = 2.3206, kgLoss = 0.0125
2025-04-08 17:18:56.369247: Training Step 318/354: batchLoss = 0.4777, diffLoss = 2.3326, kgLoss = 0.0140
2025-04-08 17:18:57.993544: Training Step 319/354: batchLoss = 0.7264, diffLoss = 3.5585, kgLoss = 0.0184
2025-04-08 17:18:59.627091: Training Step 320/354: batchLoss = 0.5757, diffLoss = 2.8145, kgLoss = 0.0160
2025-04-08 17:19:01.256153: Training Step 321/354: batchLoss = 0.6555, diffLoss = 3.2157, kgLoss = 0.0154
2025-04-08 17:19:02.885597: Training Step 322/354: batchLoss = 0.6043, diffLoss = 2.9624, kgLoss = 0.0148
2025-04-08 17:19:04.511939: Training Step 323/354: batchLoss = 0.5467, diffLoss = 2.6709, kgLoss = 0.0157
2025-04-08 17:19:06.142038: Training Step 324/354: batchLoss = 0.6899, diffLoss = 3.3831, kgLoss = 0.0166
2025-04-08 17:19:07.764252: Training Step 325/354: batchLoss = 0.5731, diffLoss = 2.8041, kgLoss = 0.0154
2025-04-08 17:19:09.381999: Training Step 326/354: batchLoss = 0.5375, diffLoss = 2.6212, kgLoss = 0.0165
2025-04-08 17:19:10.996346: Training Step 327/354: batchLoss = 0.4672, diffLoss = 2.2859, kgLoss = 0.0125
2025-04-08 17:19:12.611436: Training Step 328/354: batchLoss = 0.6093, diffLoss = 2.9891, kgLoss = 0.0143
2025-04-08 17:19:14.229916: Training Step 329/354: batchLoss = 0.4647, diffLoss = 2.2734, kgLoss = 0.0125
2025-04-08 17:19:15.854395: Training Step 330/354: batchLoss = 0.4614, diffLoss = 2.2531, kgLoss = 0.0134
2025-04-08 17:19:17.481588: Training Step 331/354: batchLoss = 0.4679, diffLoss = 2.2915, kgLoss = 0.0121
2025-04-08 17:19:19.103725: Training Step 332/354: batchLoss = 0.5160, diffLoss = 2.5251, kgLoss = 0.0137
2025-04-08 17:19:20.729755: Training Step 333/354: batchLoss = 0.6326, diffLoss = 3.0970, kgLoss = 0.0166
2025-04-08 17:19:22.349184: Training Step 334/354: batchLoss = 1.0268, diffLoss = 5.0282, kgLoss = 0.0264
2025-04-08 17:19:23.967774: Training Step 335/354: batchLoss = 0.5570, diffLoss = 2.7294, kgLoss = 0.0139
2025-04-08 17:19:25.586862: Training Step 336/354: batchLoss = 0.4779, diffLoss = 2.3375, kgLoss = 0.0130
2025-04-08 17:19:27.210269: Training Step 337/354: batchLoss = 0.6863, diffLoss = 3.3669, kgLoss = 0.0161
2025-04-08 17:19:28.834512: Training Step 338/354: batchLoss = 0.6057, diffLoss = 2.9718, kgLoss = 0.0142
2025-04-08 17:19:30.467613: Training Step 339/354: batchLoss = 0.5906, diffLoss = 2.8900, kgLoss = 0.0158
2025-04-08 17:19:32.091751: Training Step 340/354: batchLoss = 0.5710, diffLoss = 2.7983, kgLoss = 0.0142
2025-04-08 17:19:33.713412: Training Step 341/354: batchLoss = 0.6549, diffLoss = 3.2094, kgLoss = 0.0163
2025-04-08 17:19:35.338926: Training Step 342/354: batchLoss = 0.6472, diffLoss = 3.1784, kgLoss = 0.0145
2025-04-08 17:19:36.962407: Training Step 343/354: batchLoss = 0.6628, diffLoss = 3.2517, kgLoss = 0.0156
2025-04-08 17:19:38.586563: Training Step 344/354: batchLoss = 0.5302, diffLoss = 2.5941, kgLoss = 0.0142
2025-04-08 17:19:40.207993: Training Step 345/354: batchLoss = 0.5083, diffLoss = 2.4901, kgLoss = 0.0129
2025-04-08 17:19:41.898957: Training Step 346/354: batchLoss = 0.5241, diffLoss = 2.5708, kgLoss = 0.0124
2025-04-08 17:19:43.516562: Training Step 347/354: batchLoss = 0.5499, diffLoss = 2.6881, kgLoss = 0.0154
2025-04-08 17:19:45.138876: Training Step 348/354: batchLoss = 0.6198, diffLoss = 3.0379, kgLoss = 0.0152
2025-04-08 17:19:46.767656: Training Step 349/354: batchLoss = 0.6352, diffLoss = 3.1139, kgLoss = 0.0155
2025-04-08 17:19:48.395132: Training Step 350/354: batchLoss = 0.5640, diffLoss = 2.7588, kgLoss = 0.0154
2025-04-08 17:19:50.012033: Training Step 351/354: batchLoss = 0.4726, diffLoss = 2.3187, kgLoss = 0.0111
2025-04-08 17:19:51.618627: Training Step 352/354: batchLoss = 0.4506, diffLoss = 2.2055, kgLoss = 0.0118
2025-04-08 17:19:53.024855: Training Step 353/354: batchLoss = 0.6400, diffLoss = 3.1336, kgLoss = 0.0166
2025-04-08 17:19:53.120433: 
2025-04-08 17:19:53.121302: Epoch 22/1000, Train: epLoss = 1.0043, epDfLoss = 4.9171, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 17:19:54.442104: Steps 0/138: batch_recall = 46.38, batch_ndcg = 26.40 
2025-04-08 17:19:55.770166: Steps 1/138: batch_recall = 48.22, batch_ndcg = 28.26 
2025-04-08 17:19:57.078353: Steps 2/138: batch_recall = 59.88, batch_ndcg = 36.27 
2025-04-08 17:19:58.402898: Steps 3/138: batch_recall = 58.95, batch_ndcg = 34.49 
2025-04-08 17:19:59.704154: Steps 4/138: batch_recall = 66.11, batch_ndcg = 40.27 
2025-04-08 17:20:01.015594: Steps 5/138: batch_recall = 59.63, batch_ndcg = 32.47 
2025-04-08 17:20:02.326720: Steps 6/138: batch_recall = 52.20, batch_ndcg = 31.87 
2025-04-08 17:20:03.627522: Steps 7/138: batch_recall = 64.33, batch_ndcg = 41.82 
2025-04-08 17:20:04.935607: Steps 8/138: batch_recall = 62.84, batch_ndcg = 40.19 
2025-04-08 17:20:06.234227: Steps 9/138: batch_recall = 57.95, batch_ndcg = 33.54 
2025-04-08 17:20:07.532908: Steps 10/138: batch_recall = 56.00, batch_ndcg = 31.89 
2025-04-08 17:20:08.838726: Steps 11/138: batch_recall = 57.14, batch_ndcg = 32.78 
2025-04-08 17:20:10.147092: Steps 12/138: batch_recall = 52.63, batch_ndcg = 29.06 
2025-04-08 17:20:11.449746: Steps 13/138: batch_recall = 53.71, batch_ndcg = 31.74 
2025-04-08 17:20:12.749600: Steps 14/138: batch_recall = 52.50, batch_ndcg = 31.05 
2025-04-08 17:20:14.044118: Steps 15/138: batch_recall = 48.87, batch_ndcg = 29.83 
2025-04-08 17:20:15.347757: Steps 16/138: batch_recall = 59.62, batch_ndcg = 33.62 
2025-04-08 17:20:16.632155: Steps 17/138: batch_recall = 57.25, batch_ndcg = 32.79 
2025-04-08 17:20:17.934359: Steps 18/138: batch_recall = 53.15, batch_ndcg = 32.77 
2025-04-08 17:20:19.241146: Steps 19/138: batch_recall = 55.11, batch_ndcg = 33.29 
2025-04-08 17:20:20.544409: Steps 20/138: batch_recall = 61.40, batch_ndcg = 36.47 
2025-04-08 17:20:21.843183: Steps 21/138: batch_recall = 70.30, batch_ndcg = 41.01 
2025-04-08 17:20:23.146061: Steps 22/138: batch_recall = 57.78, batch_ndcg = 33.68 
2025-04-08 17:20:24.448636: Steps 23/138: batch_recall = 50.83, batch_ndcg = 29.76 
2025-04-08 17:20:25.747965: Steps 24/138: batch_recall = 58.88, batch_ndcg = 31.97 
2025-04-08 17:20:27.054001: Steps 25/138: batch_recall = 61.72, batch_ndcg = 35.90 
2025-04-08 17:20:28.352805: Steps 26/138: batch_recall = 57.42, batch_ndcg = 32.98 
2025-04-08 17:20:29.638151: Steps 27/138: batch_recall = 58.19, batch_ndcg = 33.33 
2025-04-08 17:20:30.912655: Steps 28/138: batch_recall = 60.36, batch_ndcg = 33.19 
2025-04-08 17:20:32.199155: Steps 29/138: batch_recall = 62.81, batch_ndcg = 32.64 
2025-04-08 17:20:33.491439: Steps 30/138: batch_recall = 57.28, batch_ndcg = 34.11 
2025-04-08 17:20:34.780650: Steps 31/138: batch_recall = 44.83, batch_ndcg = 25.55 
2025-04-08 17:20:36.070372: Steps 32/138: batch_recall = 52.38, batch_ndcg = 30.76 
2025-04-08 17:20:37.353360: Steps 33/138: batch_recall = 60.42, batch_ndcg = 33.21 
2025-04-08 17:20:38.638932: Steps 34/138: batch_recall = 55.25, batch_ndcg = 29.63 
2025-04-08 17:20:39.936437: Steps 35/138: batch_recall = 54.54, batch_ndcg = 30.62 
2025-04-08 17:20:41.230473: Steps 36/138: batch_recall = 48.22, batch_ndcg = 27.54 
2025-04-08 17:20:42.521799: Steps 37/138: batch_recall = 59.54, batch_ndcg = 34.38 
2025-04-08 17:20:43.806337: Steps 38/138: batch_recall = 58.33, batch_ndcg = 32.36 
2025-04-08 17:20:45.085470: Steps 39/138: batch_recall = 67.88, batch_ndcg = 39.16 
2025-04-08 17:20:46.364179: Steps 40/138: batch_recall = 59.85, batch_ndcg = 31.14 
2025-04-08 17:20:47.643079: Steps 41/138: batch_recall = 60.98, batch_ndcg = 34.16 
2025-04-08 17:20:48.930414: Steps 42/138: batch_recall = 53.46, batch_ndcg = 29.84 
2025-04-08 17:20:50.217122: Steps 43/138: batch_recall = 56.72, batch_ndcg = 35.36 
2025-04-08 17:20:51.509227: Steps 44/138: batch_recall = 57.48, batch_ndcg = 31.16 
2025-04-08 17:20:52.805865: Steps 45/138: batch_recall = 62.76, batch_ndcg = 35.44 
2025-04-08 17:20:54.102623: Steps 46/138: batch_recall = 60.52, batch_ndcg = 35.45 
2025-04-08 17:20:55.395353: Steps 47/138: batch_recall = 51.93, batch_ndcg = 31.47 
2025-04-08 17:20:56.695345: Steps 48/138: batch_recall = 60.58, batch_ndcg = 34.86 
2025-04-08 17:20:57.994640: Steps 49/138: batch_recall = 67.38, batch_ndcg = 37.68 
2025-04-08 17:20:59.281915: Steps 50/138: batch_recall = 59.92, batch_ndcg = 30.56 
2025-04-08 17:21:00.559319: Steps 51/138: batch_recall = 64.31, batch_ndcg = 36.53 
2025-04-08 17:21:01.834621: Steps 52/138: batch_recall = 65.75, batch_ndcg = 42.71 
2025-04-08 17:21:03.118029: Steps 53/138: batch_recall = 67.73, batch_ndcg = 35.36 
2025-04-08 17:21:04.401546: Steps 54/138: batch_recall = 70.24, batch_ndcg = 39.38 
2025-04-08 17:21:05.690091: Steps 55/138: batch_recall = 59.84, batch_ndcg = 32.91 
2025-04-08 17:21:06.977665: Steps 56/138: batch_recall = 59.94, batch_ndcg = 35.17 
2025-04-08 17:21:08.269601: Steps 57/138: batch_recall = 58.97, batch_ndcg = 33.19 
2025-04-08 17:21:09.556261: Steps 58/138: batch_recall = 73.50, batch_ndcg = 38.03 
2025-04-08 17:21:10.854908: Steps 59/138: batch_recall = 68.36, batch_ndcg = 40.16 
2025-04-08 17:21:12.150498: Steps 60/138: batch_recall = 66.39, batch_ndcg = 37.92 
2025-04-08 17:21:13.436946: Steps 61/138: batch_recall = 61.87, batch_ndcg = 35.22 
2025-04-08 17:21:14.730434: Steps 62/138: batch_recall = 82.71, batch_ndcg = 43.34 
2025-04-08 17:21:16.014015: Steps 63/138: batch_recall = 74.27, batch_ndcg = 42.72 
2025-04-08 17:21:17.305242: Steps 64/138: batch_recall = 59.22, batch_ndcg = 32.44 
2025-04-08 17:21:18.582579: Steps 65/138: batch_recall = 86.28, batch_ndcg = 47.40 
2025-04-08 17:21:19.863382: Steps 66/138: batch_recall = 68.12, batch_ndcg = 40.34 
2025-04-08 17:21:21.142462: Steps 67/138: batch_recall = 75.94, batch_ndcg = 46.71 
2025-04-08 17:21:22.425514: Steps 68/138: batch_recall = 62.23, batch_ndcg = 33.98 
2025-04-08 17:21:23.705743: Steps 69/138: batch_recall = 89.98, batch_ndcg = 52.51 
2025-04-08 17:21:24.984645: Steps 70/138: batch_recall = 80.72, batch_ndcg = 45.68 
2025-04-08 17:21:26.261239: Steps 71/138: batch_recall = 88.19, batch_ndcg = 50.69 
2025-04-08 17:21:27.548396: Steps 72/138: batch_recall = 86.67, batch_ndcg = 49.41 
2025-04-08 17:21:28.824010: Steps 73/138: batch_recall = 83.88, batch_ndcg = 47.47 
2025-04-08 17:21:30.106446: Steps 74/138: batch_recall = 78.84, batch_ndcg = 48.90 
2025-04-08 17:21:31.392644: Steps 75/138: batch_recall = 83.24, batch_ndcg = 49.09 
2025-04-08 17:21:32.662431: Steps 76/138: batch_recall = 96.59, batch_ndcg = 55.86 
2025-04-08 17:21:33.945015: Steps 77/138: batch_recall = 88.14, batch_ndcg = 50.25 
2025-04-08 17:21:35.223467: Steps 78/138: batch_recall = 91.11, batch_ndcg = 48.90 
2025-04-08 17:21:36.508867: Steps 79/138: batch_recall = 92.46, batch_ndcg = 50.45 
2025-04-08 17:21:37.779439: Steps 80/138: batch_recall = 73.06, batch_ndcg = 39.19 
2025-04-08 17:21:39.065331: Steps 81/138: batch_recall = 79.89, batch_ndcg = 47.28 
2025-04-08 17:21:40.343415: Steps 82/138: batch_recall = 90.44, batch_ndcg = 52.85 
2025-04-08 17:21:41.621504: Steps 83/138: batch_recall = 81.73, batch_ndcg = 47.64 
2025-04-08 17:21:42.896833: Steps 84/138: batch_recall = 98.39, batch_ndcg = 57.05 
2025-04-08 17:21:44.172969: Steps 85/138: batch_recall = 105.34, batch_ndcg = 60.96 
2025-04-08 17:21:45.450568: Steps 86/138: batch_recall = 119.66, batch_ndcg = 71.07 
2025-04-08 17:21:46.738400: Steps 87/138: batch_recall = 109.01, batch_ndcg = 56.68 
2025-04-08 17:21:48.010340: Steps 88/138: batch_recall = 104.43, batch_ndcg = 59.11 
2025-04-08 17:21:49.286686: Steps 89/138: batch_recall = 118.16, batch_ndcg = 67.39 
2025-04-08 17:21:50.558716: Steps 90/138: batch_recall = 105.42, batch_ndcg = 58.65 
2025-04-08 17:21:51.830570: Steps 91/138: batch_recall = 116.22, batch_ndcg = 64.52 
2025-04-08 17:21:53.100784: Steps 92/138: batch_recall = 120.54, batch_ndcg = 64.76 
2025-04-08 17:21:54.374047: Steps 93/138: batch_recall = 122.32, batch_ndcg = 69.18 
2025-04-08 17:21:55.656813: Steps 94/138: batch_recall = 122.81, batch_ndcg = 64.89 
2025-04-08 17:21:56.929720: Steps 95/138: batch_recall = 113.06, batch_ndcg = 66.85 
2025-04-08 17:21:58.197338: Steps 96/138: batch_recall = 126.65, batch_ndcg = 78.52 
2025-04-08 17:21:59.463511: Steps 97/138: batch_recall = 136.35, batch_ndcg = 87.02 
2025-04-08 17:22:00.740647: Steps 98/138: batch_recall = 108.38, batch_ndcg = 63.66 
2025-04-08 17:22:02.011326: Steps 99/138: batch_recall = 125.14, batch_ndcg = 71.86 
2025-04-08 17:22:03.290108: Steps 100/138: batch_recall = 130.53, batch_ndcg = 73.19 
2025-04-08 17:22:04.565266: Steps 101/138: batch_recall = 126.97, batch_ndcg = 69.83 
2025-04-08 17:22:05.845191: Steps 102/138: batch_recall = 121.79, batch_ndcg = 71.73 
2025-04-08 17:22:07.122396: Steps 103/138: batch_recall = 142.06, batch_ndcg = 80.64 
2025-04-08 17:22:08.395173: Steps 104/138: batch_recall = 134.51, batch_ndcg = 78.79 
2025-04-08 17:22:09.674397: Steps 105/138: batch_recall = 119.58, batch_ndcg = 66.66 
2025-04-08 17:22:10.943997: Steps 106/138: batch_recall = 104.30, batch_ndcg = 59.21 
2025-04-08 17:22:12.227836: Steps 107/138: batch_recall = 113.34, batch_ndcg = 63.79 
2025-04-08 17:22:13.498843: Steps 108/138: batch_recall = 118.64, batch_ndcg = 71.05 
2025-04-08 17:22:14.770738: Steps 109/138: batch_recall = 138.31, batch_ndcg = 77.53 
2025-04-08 17:22:16.053558: Steps 110/138: batch_recall = 124.24, batch_ndcg = 64.43 
2025-04-08 17:22:17.330002: Steps 111/138: batch_recall = 140.08, batch_ndcg = 84.71 
2025-04-08 17:22:18.605772: Steps 112/138: batch_recall = 167.88, batch_ndcg = 92.39 
2025-04-08 17:22:19.884120: Steps 113/138: batch_recall = 129.47, batch_ndcg = 71.65 
2025-04-08 17:22:21.156803: Steps 114/138: batch_recall = 124.37, batch_ndcg = 71.17 
2025-04-08 17:22:22.428389: Steps 115/138: batch_recall = 121.08, batch_ndcg = 63.79 
2025-04-08 17:22:23.700286: Steps 116/138: batch_recall = 124.45, batch_ndcg = 65.88 
2025-04-08 17:22:24.974422: Steps 117/138: batch_recall = 111.89, batch_ndcg = 65.72 
2025-04-08 17:22:26.244305: Steps 118/138: batch_recall = 124.45, batch_ndcg = 69.61 
2025-04-08 17:22:27.515069: Steps 119/138: batch_recall = 139.99, batch_ndcg = 75.55 
2025-04-08 17:22:28.795758: Steps 120/138: batch_recall = 128.58, batch_ndcg = 70.65 
2025-04-08 17:22:30.074424: Steps 121/138: batch_recall = 149.19, batch_ndcg = 78.99 
2025-04-08 17:22:31.342690: Steps 122/138: batch_recall = 152.26, batch_ndcg = 83.43 
2025-04-08 17:22:32.621719: Steps 123/138: batch_recall = 130.92, batch_ndcg = 73.29 
2025-04-08 17:22:33.896185: Steps 124/138: batch_recall = 154.96, batch_ndcg = 93.17 
2025-04-08 17:22:35.175983: Steps 125/138: batch_recall = 134.33, batch_ndcg = 73.61 
2025-04-08 17:22:36.449379: Steps 126/138: batch_recall = 160.69, batch_ndcg = 89.45 
2025-04-08 17:22:37.714861: Steps 127/138: batch_recall = 146.01, batch_ndcg = 82.35 
2025-04-08 17:22:38.989664: Steps 128/138: batch_recall = 129.33, batch_ndcg = 71.95 
2025-04-08 17:22:40.257682: Steps 129/138: batch_recall = 157.52, batch_ndcg = 91.44 
2025-04-08 17:22:41.541896: Steps 130/138: batch_recall = 132.58, batch_ndcg = 70.14 
2025-04-08 17:22:42.816200: Steps 131/138: batch_recall = 152.21, batch_ndcg = 86.54 
2025-04-08 17:22:44.097774: Steps 132/138: batch_recall = 147.83, batch_ndcg = 84.33 
2025-04-08 17:22:45.375582: Steps 133/138: batch_recall = 149.43, batch_ndcg = 85.57 
2025-04-08 17:22:46.656529: Steps 134/138: batch_recall = 143.63, batch_ndcg = 82.63 
2025-04-08 17:22:47.942996: Steps 135/138: batch_recall = 169.53, batch_ndcg = 96.93 
2025-04-08 17:22:49.225724: Steps 136/138: batch_recall = 151.84, batch_ndcg = 78.33 
2025-04-08 17:22:50.497677: Steps 137/138: batch_recall = 141.26, batch_ndcg = 86.97 
2025-04-08 17:22:50.498198: Epoch 22/1000, Test: Recall = 0.1769, NDCG = 0.1006  

2025-04-08 17:22:52.264883: Training Step 0/354: batchLoss = 0.7306, diffLoss = 3.5810, kgLoss = 0.0180
2025-04-08 17:22:53.884531: Training Step 1/354: batchLoss = 0.5488, diffLoss = 2.6931, kgLoss = 0.0128
2025-04-08 17:22:55.502177: Training Step 2/354: batchLoss = 0.5739, diffLoss = 2.8083, kgLoss = 0.0153
2025-04-08 17:22:57.121061: Training Step 3/354: batchLoss = 0.5739, diffLoss = 2.7989, kgLoss = 0.0177
2025-04-08 17:22:58.741333: Training Step 4/354: batchLoss = 0.5643, diffLoss = 2.7652, kgLoss = 0.0140
2025-04-08 17:23:00.368416: Training Step 5/354: batchLoss = 0.5482, diffLoss = 2.6807, kgLoss = 0.0151
2025-04-08 17:23:01.988584: Training Step 6/354: batchLoss = 0.4435, diffLoss = 2.1682, kgLoss = 0.0123
2025-04-08 17:23:03.604313: Training Step 7/354: batchLoss = 0.5135, diffLoss = 2.5119, kgLoss = 0.0138
2025-04-08 17:23:05.228356: Training Step 8/354: batchLoss = 0.5260, diffLoss = 2.5759, kgLoss = 0.0135
2025-04-08 17:23:06.851680: Training Step 9/354: batchLoss = 0.5445, diffLoss = 2.6681, kgLoss = 0.0136
2025-04-08 17:23:08.472564: Training Step 10/354: batchLoss = 0.6069, diffLoss = 2.9704, kgLoss = 0.0160
2025-04-08 17:23:10.086285: Training Step 11/354: batchLoss = 0.4976, diffLoss = 2.4389, kgLoss = 0.0123
2025-04-08 17:23:11.696990: Training Step 12/354: batchLoss = 0.5090, diffLoss = 2.4947, kgLoss = 0.0126
2025-04-08 17:23:13.310179: Training Step 13/354: batchLoss = 0.5268, diffLoss = 2.5794, kgLoss = 0.0137
2025-04-08 17:23:14.939014: Training Step 14/354: batchLoss = 0.4798, diffLoss = 2.3497, kgLoss = 0.0124
2025-04-08 17:23:16.568668: Training Step 15/354: batchLoss = 0.5247, diffLoss = 2.5705, kgLoss = 0.0133
2025-04-08 17:23:18.199166: Training Step 16/354: batchLoss = 0.5223, diffLoss = 2.5561, kgLoss = 0.0138
2025-04-08 17:23:19.829566: Training Step 17/354: batchLoss = 0.6023, diffLoss = 2.9450, kgLoss = 0.0166
2025-04-08 17:23:21.459483: Training Step 18/354: batchLoss = 0.4694, diffLoss = 2.2944, kgLoss = 0.0132
2025-04-08 17:23:23.082156: Training Step 19/354: batchLoss = 0.6800, diffLoss = 3.3379, kgLoss = 0.0155
2025-04-08 17:23:24.703819: Training Step 20/354: batchLoss = 0.5669, diffLoss = 2.7728, kgLoss = 0.0154
2025-04-08 17:23:26.331370: Training Step 21/354: batchLoss = 0.5178, diffLoss = 2.5388, kgLoss = 0.0125
2025-04-08 17:23:27.943381: Training Step 22/354: batchLoss = 0.5173, diffLoss = 2.5321, kgLoss = 0.0136
2025-04-08 17:23:29.565344: Training Step 23/354: batchLoss = 0.6364, diffLoss = 3.1240, kgLoss = 0.0145
2025-04-08 17:23:31.192153: Training Step 24/354: batchLoss = 0.5750, diffLoss = 2.8182, kgLoss = 0.0142
2025-04-08 17:23:32.816592: Training Step 25/354: batchLoss = 0.5865, diffLoss = 2.8752, kgLoss = 0.0143
2025-04-08 17:23:34.443288: Training Step 26/354: batchLoss = 0.7156, diffLoss = 3.5066, kgLoss = 0.0179
2025-04-08 17:23:36.059139: Training Step 27/354: batchLoss = 0.4365, diffLoss = 2.1300, kgLoss = 0.0131
2025-04-08 17:23:37.689208: Training Step 28/354: batchLoss = 0.5520, diffLoss = 2.7025, kgLoss = 0.0144
2025-04-08 17:23:39.308282: Training Step 29/354: batchLoss = 0.5280, diffLoss = 2.5814, kgLoss = 0.0146
2025-04-08 17:23:40.926034: Training Step 30/354: batchLoss = 0.4626, diffLoss = 2.2628, kgLoss = 0.0125
2025-04-08 17:23:42.533276: Training Step 31/354: batchLoss = 0.6063, diffLoss = 2.9641, kgLoss = 0.0169
2025-04-08 17:23:44.143989: Training Step 32/354: batchLoss = 1.1399, diffLoss = 5.5834, kgLoss = 0.0290
2025-04-08 17:23:45.759298: Training Step 33/354: batchLoss = 0.5670, diffLoss = 2.7746, kgLoss = 0.0151
2025-04-08 17:23:47.383816: Training Step 34/354: batchLoss = 0.5653, diffLoss = 2.7713, kgLoss = 0.0138
2025-04-08 17:23:49.004973: Training Step 35/354: batchLoss = 0.5247, diffLoss = 2.5588, kgLoss = 0.0162
2025-04-08 17:23:50.633872: Training Step 36/354: batchLoss = 0.5955, diffLoss = 2.9113, kgLoss = 0.0166
2025-04-08 17:23:52.257014: Training Step 37/354: batchLoss = 0.5940, diffLoss = 2.9081, kgLoss = 0.0155
2025-04-08 17:23:53.885148: Training Step 38/354: batchLoss = 0.5446, diffLoss = 2.6667, kgLoss = 0.0141
2025-04-08 17:23:55.513398: Training Step 39/354: batchLoss = 0.5260, diffLoss = 2.5728, kgLoss = 0.0143
2025-04-08 17:23:57.130945: Training Step 40/354: batchLoss = 0.5116, diffLoss = 2.5026, kgLoss = 0.0139
2025-04-08 17:23:58.751388: Training Step 41/354: batchLoss = 0.6152, diffLoss = 3.0119, kgLoss = 0.0160
2025-04-08 17:24:00.370989: Training Step 42/354: batchLoss = 0.5364, diffLoss = 2.6221, kgLoss = 0.0150
2025-04-08 17:24:01.990631: Training Step 43/354: batchLoss = 0.4970, diffLoss = 2.4251, kgLoss = 0.0150
2025-04-08 17:24:03.619974: Training Step 44/354: batchLoss = 0.7179, diffLoss = 3.5182, kgLoss = 0.0178
2025-04-08 17:24:05.252120: Training Step 45/354: batchLoss = 0.5700, diffLoss = 2.7936, kgLoss = 0.0141
2025-04-08 17:24:06.875408: Training Step 46/354: batchLoss = 0.5154, diffLoss = 2.5228, kgLoss = 0.0135
2025-04-08 17:24:08.501286: Training Step 47/354: batchLoss = 0.5358, diffLoss = 2.6262, kgLoss = 0.0132
2025-04-08 17:24:10.130114: Training Step 48/354: batchLoss = 0.6060, diffLoss = 2.9636, kgLoss = 0.0166
2025-04-08 17:24:11.750352: Training Step 49/354: batchLoss = 0.5158, diffLoss = 2.5254, kgLoss = 0.0134
2025-04-08 17:24:13.373719: Training Step 50/354: batchLoss = 0.5832, diffLoss = 2.8556, kgLoss = 0.0151
2025-04-08 17:24:14.989503: Training Step 51/354: batchLoss = 0.5490, diffLoss = 2.6905, kgLoss = 0.0137
2025-04-08 17:24:16.610257: Training Step 52/354: batchLoss = 0.6193, diffLoss = 3.0363, kgLoss = 0.0151
2025-04-08 17:24:18.235946: Training Step 53/354: batchLoss = 0.4808, diffLoss = 2.3498, kgLoss = 0.0135
2025-04-08 17:24:19.854950: Training Step 54/354: batchLoss = 0.5193, diffLoss = 2.5455, kgLoss = 0.0127
2025-04-08 17:24:21.467560: Training Step 55/354: batchLoss = 0.5567, diffLoss = 2.7240, kgLoss = 0.0149
2025-04-08 17:24:23.096804: Training Step 56/354: batchLoss = 0.5009, diffLoss = 2.4471, kgLoss = 0.0143
2025-04-08 17:24:24.725363: Training Step 57/354: batchLoss = 0.5347, diffLoss = 2.6222, kgLoss = 0.0128
2025-04-08 17:24:26.343593: Training Step 58/354: batchLoss = 0.5536, diffLoss = 2.7015, kgLoss = 0.0166
2025-04-08 17:24:27.965188: Training Step 59/354: batchLoss = 0.9319, diffLoss = 4.5600, kgLoss = 0.0249
2025-04-08 17:24:29.578883: Training Step 60/354: batchLoss = 0.5860, diffLoss = 2.8710, kgLoss = 0.0148
2025-04-08 17:24:31.196679: Training Step 61/354: batchLoss = 0.6104, diffLoss = 2.9812, kgLoss = 0.0177
2025-04-08 17:24:32.814495: Training Step 62/354: batchLoss = 0.5313, diffLoss = 2.6055, kgLoss = 0.0127
2025-04-08 17:24:34.441623: Training Step 63/354: batchLoss = 0.6521, diffLoss = 3.1924, kgLoss = 0.0170
2025-04-08 17:24:36.065809: Training Step 64/354: batchLoss = 0.5517, diffLoss = 2.7008, kgLoss = 0.0144
2025-04-08 17:24:37.689594: Training Step 65/354: batchLoss = 0.6202, diffLoss = 3.0373, kgLoss = 0.0160
2025-04-08 17:24:39.319040: Training Step 66/354: batchLoss = 0.4296, diffLoss = 2.0993, kgLoss = 0.0122
2025-04-08 17:24:40.946197: Training Step 67/354: batchLoss = 0.6431, diffLoss = 3.1437, kgLoss = 0.0180
2025-04-08 17:24:42.571031: Training Step 68/354: batchLoss = 0.5026, diffLoss = 2.4588, kgLoss = 0.0135
2025-04-08 17:24:44.185762: Training Step 69/354: batchLoss = 0.6758, diffLoss = 3.3144, kgLoss = 0.0162
2025-04-08 17:24:45.803146: Training Step 70/354: batchLoss = 0.5340, diffLoss = 2.6137, kgLoss = 0.0141
2025-04-08 17:24:47.418780: Training Step 71/354: batchLoss = 0.5308, diffLoss = 2.5963, kgLoss = 0.0145
2025-04-08 17:24:49.025926: Training Step 72/354: batchLoss = 0.5461, diffLoss = 2.6743, kgLoss = 0.0141
2025-04-08 17:24:50.651705: Training Step 73/354: batchLoss = 0.5355, diffLoss = 2.6217, kgLoss = 0.0139
2025-04-08 17:24:52.279632: Training Step 74/354: batchLoss = 0.5346, diffLoss = 2.6145, kgLoss = 0.0146
2025-04-08 17:24:53.910248: Training Step 75/354: batchLoss = 0.7065, diffLoss = 3.4650, kgLoss = 0.0168
2025-04-08 17:24:55.539932: Training Step 76/354: batchLoss = 0.6085, diffLoss = 2.9863, kgLoss = 0.0141
2025-04-08 17:24:57.178803: Training Step 77/354: batchLoss = 0.6479, diffLoss = 3.1718, kgLoss = 0.0169
2025-04-08 17:24:58.809477: Training Step 78/354: batchLoss = 0.6781, diffLoss = 3.3168, kgLoss = 0.0184
2025-04-08 17:25:00.433970: Training Step 79/354: batchLoss = 0.4954, diffLoss = 2.4250, kgLoss = 0.0130
2025-04-08 17:25:02.048984: Training Step 80/354: batchLoss = 0.5983, diffLoss = 2.9283, kgLoss = 0.0158
2025-04-08 17:25:03.674868: Training Step 81/354: batchLoss = 0.5382, diffLoss = 2.6411, kgLoss = 0.0125
2025-04-08 17:25:05.292313: Training Step 82/354: batchLoss = 0.5608, diffLoss = 2.7502, kgLoss = 0.0135
2025-04-08 17:25:06.908621: Training Step 83/354: batchLoss = 0.6156, diffLoss = 3.0188, kgLoss = 0.0148
2025-04-08 17:25:08.538602: Training Step 84/354: batchLoss = 0.5433, diffLoss = 2.6618, kgLoss = 0.0136
2025-04-08 17:25:10.167347: Training Step 85/354: batchLoss = 0.5914, diffLoss = 2.8997, kgLoss = 0.0143
2025-04-08 17:25:11.798638: Training Step 86/354: batchLoss = 0.5898, diffLoss = 2.8846, kgLoss = 0.0161
2025-04-08 17:25:13.418804: Training Step 87/354: batchLoss = 0.6627, diffLoss = 3.2437, kgLoss = 0.0174
2025-04-08 17:25:15.040110: Training Step 88/354: batchLoss = 0.6511, diffLoss = 3.1931, kgLoss = 0.0156
2025-04-08 17:25:16.656921: Training Step 89/354: batchLoss = 0.5103, diffLoss = 2.4893, kgLoss = 0.0156
2025-04-08 17:25:18.278658: Training Step 90/354: batchLoss = 0.5645, diffLoss = 2.7667, kgLoss = 0.0139
2025-04-08 17:25:19.899253: Training Step 91/354: batchLoss = 0.4846, diffLoss = 2.3658, kgLoss = 0.0143
2025-04-08 17:25:21.523035: Training Step 92/354: batchLoss = 0.5904, diffLoss = 2.8926, kgLoss = 0.0149
2025-04-08 17:25:23.158199: Training Step 93/354: batchLoss = 0.4961, diffLoss = 2.4255, kgLoss = 0.0138
2025-04-08 17:25:24.785776: Training Step 94/354: batchLoss = 0.5355, diffLoss = 2.6231, kgLoss = 0.0136
2025-04-08 17:25:26.409689: Training Step 95/354: batchLoss = 0.5837, diffLoss = 2.8600, kgLoss = 0.0146
2025-04-08 17:25:28.039434: Training Step 96/354: batchLoss = 0.5551, diffLoss = 2.7163, kgLoss = 0.0148
2025-04-08 17:25:29.669353: Training Step 97/354: batchLoss = 0.4925, diffLoss = 2.4168, kgLoss = 0.0115
2025-04-08 17:25:31.295602: Training Step 98/354: batchLoss = 0.5887, diffLoss = 2.8766, kgLoss = 0.0167
2025-04-08 17:25:32.919731: Training Step 99/354: batchLoss = 0.5062, diffLoss = 2.4730, kgLoss = 0.0145
2025-04-08 17:25:34.544716: Training Step 100/354: batchLoss = 0.4247, diffLoss = 2.0769, kgLoss = 0.0117
2025-04-08 17:25:36.161262: Training Step 101/354: batchLoss = 0.4607, diffLoss = 2.2480, kgLoss = 0.0139
2025-04-08 17:25:37.787028: Training Step 102/354: batchLoss = 0.6047, diffLoss = 2.9668, kgLoss = 0.0142
2025-04-08 17:25:39.413127: Training Step 103/354: batchLoss = 0.5366, diffLoss = 2.6330, kgLoss = 0.0126
2025-04-08 17:25:41.040530: Training Step 104/354: batchLoss = 0.4606, diffLoss = 2.2497, kgLoss = 0.0133
2025-04-08 17:25:42.667468: Training Step 105/354: batchLoss = 0.5311, diffLoss = 2.6012, kgLoss = 0.0136
2025-04-08 17:25:44.291106: Training Step 106/354: batchLoss = 0.5874, diffLoss = 2.8813, kgLoss = 0.0139
2025-04-08 17:25:45.919242: Training Step 107/354: batchLoss = 0.6761, diffLoss = 3.3172, kgLoss = 0.0158
2025-04-08 17:25:47.543368: Training Step 108/354: batchLoss = 0.5099, diffLoss = 2.4950, kgLoss = 0.0136
2025-04-08 17:25:49.165051: Training Step 109/354: batchLoss = 0.4880, diffLoss = 2.3919, kgLoss = 0.0121
2025-04-08 17:25:50.802097: Training Step 110/354: batchLoss = 0.5483, diffLoss = 2.6820, kgLoss = 0.0148
2025-04-08 17:25:52.422061: Training Step 111/354: batchLoss = 0.4951, diffLoss = 2.4235, kgLoss = 0.0130
2025-04-08 17:25:54.046974: Training Step 112/354: batchLoss = 0.6539, diffLoss = 3.2001, kgLoss = 0.0174
2025-04-08 17:25:55.673633: Training Step 113/354: batchLoss = 0.5298, diffLoss = 2.5974, kgLoss = 0.0129
2025-04-08 17:25:57.299145: Training Step 114/354: batchLoss = 0.6170, diffLoss = 3.0244, kgLoss = 0.0152
2025-04-08 17:25:58.925751: Training Step 115/354: batchLoss = 0.6225, diffLoss = 3.0512, kgLoss = 0.0154
2025-04-08 17:26:00.552981: Training Step 116/354: batchLoss = 0.6273, diffLoss = 3.0711, kgLoss = 0.0163
2025-04-08 17:26:02.183255: Training Step 117/354: batchLoss = 0.5361, diffLoss = 2.6258, kgLoss = 0.0137
2025-04-08 17:26:03.805969: Training Step 118/354: batchLoss = 0.6135, diffLoss = 3.0103, kgLoss = 0.0143
2025-04-08 17:26:05.429577: Training Step 119/354: batchLoss = 0.5106, diffLoss = 2.4989, kgLoss = 0.0136
2025-04-08 17:26:07.037949: Training Step 120/354: batchLoss = 0.5603, diffLoss = 2.7483, kgLoss = 0.0132
2025-04-08 17:26:08.643417: Training Step 121/354: batchLoss = 0.8057, diffLoss = 3.9580, kgLoss = 0.0176
2025-04-08 17:26:10.249098: Training Step 122/354: batchLoss = 0.6040, diffLoss = 2.9564, kgLoss = 0.0158
2025-04-08 17:26:11.851802: Training Step 123/354: batchLoss = 0.5064, diffLoss = 2.4723, kgLoss = 0.0149
2025-04-08 17:26:13.453231: Training Step 124/354: batchLoss = 0.4989, diffLoss = 2.4445, kgLoss = 0.0125
2025-04-08 17:26:15.070248: Training Step 125/354: batchLoss = 0.5380, diffLoss = 2.6275, kgLoss = 0.0156
2025-04-08 17:26:16.691226: Training Step 126/354: batchLoss = 0.5332, diffLoss = 2.6111, kgLoss = 0.0138
2025-04-08 17:26:18.306118: Training Step 127/354: batchLoss = 0.5697, diffLoss = 2.7965, kgLoss = 0.0131
2025-04-08 17:26:19.920585: Training Step 128/354: batchLoss = 0.7119, diffLoss = 3.4921, kgLoss = 0.0169
2025-04-08 17:26:21.539870: Training Step 129/354: batchLoss = 0.6416, diffLoss = 3.1409, kgLoss = 0.0167
2025-04-08 17:26:23.155557: Training Step 130/354: batchLoss = 0.6393, diffLoss = 3.1230, kgLoss = 0.0184
2025-04-08 17:26:24.768478: Training Step 131/354: batchLoss = 0.6481, diffLoss = 3.1832, kgLoss = 0.0143
2025-04-08 17:26:26.381010: Training Step 132/354: batchLoss = 0.5466, diffLoss = 2.6786, kgLoss = 0.0136
2025-04-08 17:26:28.000295: Training Step 133/354: batchLoss = 0.5577, diffLoss = 2.7296, kgLoss = 0.0147
2025-04-08 17:26:29.617429: Training Step 134/354: batchLoss = 0.5325, diffLoss = 2.6120, kgLoss = 0.0126
2025-04-08 17:26:31.236983: Training Step 135/354: batchLoss = 0.6130, diffLoss = 2.9997, kgLoss = 0.0164
2025-04-08 17:26:32.859870: Training Step 136/354: batchLoss = 0.5304, diffLoss = 2.5998, kgLoss = 0.0131
2025-04-08 17:26:34.481404: Training Step 137/354: batchLoss = 0.4674, diffLoss = 2.2935, kgLoss = 0.0109
2025-04-08 17:26:36.101729: Training Step 138/354: batchLoss = 0.5705, diffLoss = 2.7950, kgLoss = 0.0144
2025-04-08 17:26:37.720581: Training Step 139/354: batchLoss = 0.4494, diffLoss = 2.2035, kgLoss = 0.0108
2025-04-08 17:26:39.350868: Training Step 140/354: batchLoss = 0.6477, diffLoss = 3.1759, kgLoss = 0.0157
2025-04-08 17:26:40.972163: Training Step 141/354: batchLoss = 0.4667, diffLoss = 2.2826, kgLoss = 0.0128
2025-04-08 17:26:42.588888: Training Step 142/354: batchLoss = 0.5462, diffLoss = 2.6738, kgLoss = 0.0143
2025-04-08 17:26:44.204197: Training Step 143/354: batchLoss = 0.6011, diffLoss = 2.9372, kgLoss = 0.0170
2025-04-08 17:26:45.828598: Training Step 144/354: batchLoss = 0.5172, diffLoss = 2.5248, kgLoss = 0.0153
2025-04-08 17:26:47.456997: Training Step 145/354: batchLoss = 0.5250, diffLoss = 2.5656, kgLoss = 0.0149
2025-04-08 17:26:49.073148: Training Step 146/354: batchLoss = 0.4855, diffLoss = 2.3718, kgLoss = 0.0139
2025-04-08 17:26:50.688243: Training Step 147/354: batchLoss = 0.5742, diffLoss = 2.8145, kgLoss = 0.0141
2025-04-08 17:26:52.302963: Training Step 148/354: batchLoss = 0.4959, diffLoss = 2.4289, kgLoss = 0.0126
2025-04-08 17:26:53.929135: Training Step 149/354: batchLoss = 0.5102, diffLoss = 2.4982, kgLoss = 0.0132
2025-04-08 17:26:55.548964: Training Step 150/354: batchLoss = 0.5803, diffLoss = 2.8419, kgLoss = 0.0149
2025-04-08 17:26:57.161186: Training Step 151/354: batchLoss = 0.7137, diffLoss = 3.5036, kgLoss = 0.0162
2025-04-08 17:26:58.770837: Training Step 152/354: batchLoss = 0.7436, diffLoss = 3.6425, kgLoss = 0.0189
2025-04-08 17:27:00.385258: Training Step 153/354: batchLoss = 0.5609, diffLoss = 2.7427, kgLoss = 0.0154
2025-04-08 17:27:02.005070: Training Step 154/354: batchLoss = 0.5235, diffLoss = 2.5602, kgLoss = 0.0143
2025-04-08 17:27:03.625472: Training Step 155/354: batchLoss = 0.4373, diffLoss = 2.1419, kgLoss = 0.0111
2025-04-08 17:27:05.247870: Training Step 156/354: batchLoss = 0.6108, diffLoss = 2.9905, kgLoss = 0.0159
2025-04-08 17:27:06.869956: Training Step 157/354: batchLoss = 0.4676, diffLoss = 2.2882, kgLoss = 0.0125
2025-04-08 17:27:08.486690: Training Step 158/354: batchLoss = 0.5924, diffLoss = 2.9065, kgLoss = 0.0139
2025-04-08 17:27:10.102246: Training Step 159/354: batchLoss = 0.5620, diffLoss = 2.7573, kgLoss = 0.0131
2025-04-08 17:27:11.718118: Training Step 160/354: batchLoss = 0.5500, diffLoss = 2.6936, kgLoss = 0.0141
2025-04-08 17:27:13.332241: Training Step 161/354: batchLoss = 0.6329, diffLoss = 3.0922, kgLoss = 0.0181
2025-04-08 17:27:14.952124: Training Step 162/354: batchLoss = 0.5265, diffLoss = 2.5723, kgLoss = 0.0150
2025-04-08 17:27:16.571982: Training Step 163/354: batchLoss = 0.4546, diffLoss = 2.2186, kgLoss = 0.0136
2025-04-08 17:27:18.193320: Training Step 164/354: batchLoss = 0.5743, diffLoss = 2.8092, kgLoss = 0.0156
2025-04-08 17:27:19.815181: Training Step 165/354: batchLoss = 0.6244, diffLoss = 3.0635, kgLoss = 0.0146
2025-04-08 17:27:21.436341: Training Step 166/354: batchLoss = 0.5324, diffLoss = 2.5962, kgLoss = 0.0165
2025-04-08 17:27:23.057375: Training Step 167/354: batchLoss = 0.6118, diffLoss = 3.0018, kgLoss = 0.0143
2025-04-08 17:27:24.680892: Training Step 168/354: batchLoss = 0.5347, diffLoss = 2.6215, kgLoss = 0.0130
2025-04-08 17:27:26.297567: Training Step 169/354: batchLoss = 0.4793, diffLoss = 2.3440, kgLoss = 0.0131
2025-04-08 17:27:27.913074: Training Step 170/354: batchLoss = 0.6389, diffLoss = 3.1334, kgLoss = 0.0152
2025-04-08 17:27:29.537671: Training Step 171/354: batchLoss = 0.4441, diffLoss = 2.1638, kgLoss = 0.0142
2025-04-08 17:27:31.154988: Training Step 172/354: batchLoss = 0.5567, diffLoss = 2.7259, kgLoss = 0.0144
2025-04-08 17:27:32.776549: Training Step 173/354: batchLoss = 0.4935, diffLoss = 2.4128, kgLoss = 0.0137
2025-04-08 17:27:34.398648: Training Step 174/354: batchLoss = 0.4743, diffLoss = 2.3204, kgLoss = 0.0128
2025-04-08 17:27:36.023470: Training Step 175/354: batchLoss = 0.5440, diffLoss = 2.6600, kgLoss = 0.0150
2025-04-08 17:27:37.637816: Training Step 176/354: batchLoss = 0.5854, diffLoss = 2.8637, kgLoss = 0.0158
2025-04-08 17:27:39.256307: Training Step 177/354: batchLoss = 0.5499, diffLoss = 2.6996, kgLoss = 0.0125
2025-04-08 17:27:40.872305: Training Step 178/354: batchLoss = 0.6010, diffLoss = 2.9504, kgLoss = 0.0137
2025-04-08 17:27:42.487310: Training Step 179/354: batchLoss = 0.6182, diffLoss = 3.0285, kgLoss = 0.0156
2025-04-08 17:27:44.098053: Training Step 180/354: batchLoss = 0.5616, diffLoss = 2.7473, kgLoss = 0.0152
2025-04-08 17:27:45.712010: Training Step 181/354: batchLoss = 0.5632, diffLoss = 2.7562, kgLoss = 0.0149
2025-04-08 17:27:47.334932: Training Step 182/354: batchLoss = 0.6113, diffLoss = 2.9894, kgLoss = 0.0168
2025-04-08 17:27:48.949569: Training Step 183/354: batchLoss = 0.5396, diffLoss = 2.6408, kgLoss = 0.0143
2025-04-08 17:27:50.567512: Training Step 184/354: batchLoss = 0.6164, diffLoss = 3.0235, kgLoss = 0.0146
2025-04-08 17:27:52.187544: Training Step 185/354: batchLoss = 0.5999, diffLoss = 2.9390, kgLoss = 0.0151
2025-04-08 17:27:53.813037: Training Step 186/354: batchLoss = 0.6157, diffLoss = 3.0184, kgLoss = 0.0150
2025-04-08 17:27:55.434809: Training Step 187/354: batchLoss = 0.5691, diffLoss = 2.7910, kgLoss = 0.0137
2025-04-08 17:27:57.051072: Training Step 188/354: batchLoss = 0.6012, diffLoss = 2.9490, kgLoss = 0.0142
2025-04-08 17:27:58.662723: Training Step 189/354: batchLoss = 0.6323, diffLoss = 3.0963, kgLoss = 0.0163
2025-04-08 17:28:00.278813: Training Step 190/354: batchLoss = 0.6309, diffLoss = 3.0925, kgLoss = 0.0154
2025-04-08 17:28:01.898520: Training Step 191/354: batchLoss = 0.4769, diffLoss = 2.3372, kgLoss = 0.0119
2025-04-08 17:28:03.515023: Training Step 192/354: batchLoss = 0.4934, diffLoss = 2.4069, kgLoss = 0.0150
2025-04-08 17:28:05.136234: Training Step 193/354: batchLoss = 0.5873, diffLoss = 2.8760, kgLoss = 0.0151
2025-04-08 17:28:06.760655: Training Step 194/354: batchLoss = 1.6868, diffLoss = 8.2632, kgLoss = 0.0426
2025-04-08 17:28:08.382719: Training Step 195/354: batchLoss = 0.5155, diffLoss = 2.5168, kgLoss = 0.0152
2025-04-08 17:28:10.004887: Training Step 196/354: batchLoss = 0.6196, diffLoss = 3.0431, kgLoss = 0.0137
2025-04-08 17:28:11.623397: Training Step 197/354: batchLoss = 0.5886, diffLoss = 2.8856, kgLoss = 0.0144
2025-04-08 17:28:13.241116: Training Step 198/354: batchLoss = 0.5302, diffLoss = 2.5963, kgLoss = 0.0137
2025-04-08 17:28:14.860495: Training Step 199/354: batchLoss = 0.5541, diffLoss = 2.7135, kgLoss = 0.0143
2025-04-08 17:28:16.478803: Training Step 200/354: batchLoss = 0.5276, diffLoss = 2.5863, kgLoss = 0.0129
2025-04-08 17:28:18.102311: Training Step 201/354: batchLoss = 0.5381, diffLoss = 2.6384, kgLoss = 0.0131
2025-04-08 17:28:19.727870: Training Step 202/354: batchLoss = 0.4715, diffLoss = 2.3023, kgLoss = 0.0138
2025-04-08 17:28:21.343682: Training Step 203/354: batchLoss = 0.6501, diffLoss = 3.1800, kgLoss = 0.0176
2025-04-08 17:28:22.957288: Training Step 204/354: batchLoss = 0.5704, diffLoss = 2.7913, kgLoss = 0.0151
2025-04-08 17:28:24.577569: Training Step 205/354: batchLoss = 0.5075, diffLoss = 2.4847, kgLoss = 0.0132
2025-04-08 17:28:26.194104: Training Step 206/354: batchLoss = 0.4634, diffLoss = 2.2711, kgLoss = 0.0115
2025-04-08 17:28:27.811762: Training Step 207/354: batchLoss = 0.4161, diffLoss = 2.0340, kgLoss = 0.0117
2025-04-08 17:28:29.430081: Training Step 208/354: batchLoss = 0.8585, diffLoss = 4.2080, kgLoss = 0.0211
2025-04-08 17:28:31.047270: Training Step 209/354: batchLoss = 0.7394, diffLoss = 3.6254, kgLoss = 0.0179
2025-04-08 17:28:32.658219: Training Step 210/354: batchLoss = 0.5834, diffLoss = 2.8624, kgLoss = 0.0137
2025-04-08 17:28:34.277110: Training Step 211/354: batchLoss = 0.5937, diffLoss = 2.9118, kgLoss = 0.0141
2025-04-08 17:28:35.899321: Training Step 212/354: batchLoss = 0.5170, diffLoss = 2.5303, kgLoss = 0.0137
2025-04-08 17:28:37.518371: Training Step 213/354: batchLoss = 0.6497, diffLoss = 3.1862, kgLoss = 0.0156
2025-04-08 17:28:39.135012: Training Step 214/354: batchLoss = 0.4949, diffLoss = 2.4273, kgLoss = 0.0118
2025-04-08 17:28:40.762408: Training Step 215/354: batchLoss = 0.5358, diffLoss = 2.6170, kgLoss = 0.0156
2025-04-08 17:28:42.386417: Training Step 216/354: batchLoss = 0.5533, diffLoss = 2.7032, kgLoss = 0.0158
2025-04-08 17:28:44.006697: Training Step 217/354: batchLoss = 0.6293, diffLoss = 3.0787, kgLoss = 0.0169
2025-04-08 17:28:45.626441: Training Step 218/354: batchLoss = 0.5723, diffLoss = 2.7990, kgLoss = 0.0156
2025-04-08 17:28:47.255062: Training Step 219/354: batchLoss = 0.4832, diffLoss = 2.3610, kgLoss = 0.0137
2025-04-08 17:28:48.871995: Training Step 220/354: batchLoss = 0.5972, diffLoss = 2.9259, kgLoss = 0.0150
2025-04-08 17:28:50.495764: Training Step 221/354: batchLoss = 0.4642, diffLoss = 2.2671, kgLoss = 0.0135
2025-04-08 17:28:52.120895: Training Step 222/354: batchLoss = 0.5015, diffLoss = 2.4532, kgLoss = 0.0136
2025-04-08 17:28:53.749765: Training Step 223/354: batchLoss = 0.7172, diffLoss = 3.5213, kgLoss = 0.0162
2025-04-08 17:28:55.376729: Training Step 224/354: batchLoss = 0.5981, diffLoss = 2.9261, kgLoss = 0.0161
2025-04-08 17:28:56.998854: Training Step 225/354: batchLoss = 0.5038, diffLoss = 2.4659, kgLoss = 0.0133
2025-04-08 17:28:58.616600: Training Step 226/354: batchLoss = 0.5843, diffLoss = 2.8480, kgLoss = 0.0183
2025-04-08 17:29:00.234365: Training Step 227/354: batchLoss = 0.7666, diffLoss = 3.7577, kgLoss = 0.0188
2025-04-08 17:29:01.851144: Training Step 228/354: batchLoss = 0.5292, diffLoss = 2.5986, kgLoss = 0.0119
2025-04-08 17:29:03.474171: Training Step 229/354: batchLoss = 0.4684, diffLoss = 2.2906, kgLoss = 0.0128
2025-04-08 17:29:05.082888: Training Step 230/354: batchLoss = 0.5626, diffLoss = 2.7586, kgLoss = 0.0136
2025-04-08 17:29:06.700219: Training Step 231/354: batchLoss = 0.6984, diffLoss = 3.4268, kgLoss = 0.0163
2025-04-08 17:29:08.318667: Training Step 232/354: batchLoss = 0.4831, diffLoss = 2.3622, kgLoss = 0.0134
2025-04-08 17:29:09.944400: Training Step 233/354: batchLoss = 0.5985, diffLoss = 2.9338, kgLoss = 0.0147
2025-04-08 17:29:11.566749: Training Step 234/354: batchLoss = 0.5405, diffLoss = 2.6445, kgLoss = 0.0145
2025-04-08 17:29:13.186741: Training Step 235/354: batchLoss = 0.5028, diffLoss = 2.4683, kgLoss = 0.0115
2025-04-08 17:29:14.811612: Training Step 236/354: batchLoss = 0.5296, diffLoss = 2.5934, kgLoss = 0.0136
2025-04-08 17:29:16.427614: Training Step 237/354: batchLoss = 0.5098, diffLoss = 2.4945, kgLoss = 0.0136
2025-04-08 17:29:18.041086: Training Step 238/354: batchLoss = 0.6197, diffLoss = 3.0376, kgLoss = 0.0152
2025-04-08 17:29:19.652191: Training Step 239/354: batchLoss = 0.5995, diffLoss = 2.9345, kgLoss = 0.0158
2025-04-08 17:29:21.267880: Training Step 240/354: batchLoss = 0.6074, diffLoss = 2.9747, kgLoss = 0.0156
2025-04-08 17:29:22.885026: Training Step 241/354: batchLoss = 0.5189, diffLoss = 2.5419, kgLoss = 0.0132
2025-04-08 17:29:24.507432: Training Step 242/354: batchLoss = 0.5609, diffLoss = 2.7454, kgLoss = 0.0147
2025-04-08 17:29:26.130779: Training Step 243/354: batchLoss = 0.5605, diffLoss = 2.7397, kgLoss = 0.0157
2025-04-08 17:29:27.750976: Training Step 244/354: batchLoss = 0.5074, diffLoss = 2.4775, kgLoss = 0.0148
2025-04-08 17:29:29.378903: Training Step 245/354: batchLoss = 0.5087, diffLoss = 2.4892, kgLoss = 0.0135
2025-04-08 17:29:31.001284: Training Step 246/354: batchLoss = 0.4897, diffLoss = 2.3974, kgLoss = 0.0128
2025-04-08 17:29:32.617242: Training Step 247/354: batchLoss = 0.5625, diffLoss = 2.7546, kgLoss = 0.0145
2025-04-08 17:29:34.234434: Training Step 248/354: batchLoss = 0.5811, diffLoss = 2.8516, kgLoss = 0.0135
2025-04-08 17:29:35.849652: Training Step 249/354: batchLoss = 0.6718, diffLoss = 3.2926, kgLoss = 0.0166
2025-04-08 17:29:37.467742: Training Step 250/354: batchLoss = 0.5594, diffLoss = 2.7348, kgLoss = 0.0155
2025-04-08 17:29:39.085207: Training Step 251/354: batchLoss = 0.4940, diffLoss = 2.4030, kgLoss = 0.0167
2025-04-08 17:29:40.705238: Training Step 252/354: batchLoss = 0.5424, diffLoss = 2.6492, kgLoss = 0.0157
2025-04-08 17:29:42.320598: Training Step 253/354: batchLoss = 0.5506, diffLoss = 2.7019, kgLoss = 0.0127
2025-04-08 17:29:43.942837: Training Step 254/354: batchLoss = 0.7139, diffLoss = 3.5069, kgLoss = 0.0157
2025-04-08 17:29:45.567434: Training Step 255/354: batchLoss = 0.6292, diffLoss = 3.0806, kgLoss = 0.0164
2025-04-08 17:29:47.186353: Training Step 256/354: batchLoss = 0.5276, diffLoss = 2.5860, kgLoss = 0.0130
2025-04-08 17:29:48.802611: Training Step 257/354: batchLoss = 0.4397, diffLoss = 2.1568, kgLoss = 0.0104
2025-04-08 17:29:50.420227: Training Step 258/354: batchLoss = 0.6061, diffLoss = 2.9701, kgLoss = 0.0151
2025-04-08 17:29:52.039891: Training Step 259/354: batchLoss = 0.6270, diffLoss = 3.0711, kgLoss = 0.0159
2025-04-08 17:29:53.659070: Training Step 260/354: batchLoss = 0.7074, diffLoss = 3.4664, kgLoss = 0.0176
2025-04-08 17:29:55.278882: Training Step 261/354: batchLoss = 0.9002, diffLoss = 4.4180, kgLoss = 0.0207
2025-04-08 17:29:56.900046: Training Step 262/354: batchLoss = 0.4990, diffLoss = 2.4445, kgLoss = 0.0127
2025-04-08 17:29:58.524819: Training Step 263/354: batchLoss = 0.5555, diffLoss = 2.7179, kgLoss = 0.0148
2025-04-08 17:30:00.149936: Training Step 264/354: batchLoss = 0.5588, diffLoss = 2.7384, kgLoss = 0.0140
2025-04-08 17:30:01.781707: Training Step 265/354: batchLoss = 0.5482, diffLoss = 2.6841, kgLoss = 0.0143
2025-04-08 17:30:03.402884: Training Step 266/354: batchLoss = 0.5704, diffLoss = 2.7925, kgLoss = 0.0149
2025-04-08 17:30:05.019749: Training Step 267/354: batchLoss = 0.5973, diffLoss = 2.9236, kgLoss = 0.0157
2025-04-08 17:30:06.635089: Training Step 268/354: batchLoss = 0.4465, diffLoss = 2.1870, kgLoss = 0.0114
2025-04-08 17:30:08.249830: Training Step 269/354: batchLoss = 0.6628, diffLoss = 3.2487, kgLoss = 0.0163
2025-04-08 17:30:09.869002: Training Step 270/354: batchLoss = 0.6389, diffLoss = 3.1266, kgLoss = 0.0170
2025-04-08 17:30:11.493605: Training Step 271/354: batchLoss = 0.5613, diffLoss = 2.7512, kgLoss = 0.0138
2025-04-08 17:30:13.117581: Training Step 272/354: batchLoss = 0.5488, diffLoss = 2.6894, kgLoss = 0.0137
2025-04-08 17:30:14.744174: Training Step 273/354: batchLoss = 0.6380, diffLoss = 3.1177, kgLoss = 0.0180
2025-04-08 17:30:16.362133: Training Step 274/354: batchLoss = 0.5342, diffLoss = 2.6142, kgLoss = 0.0142
2025-04-08 17:30:17.989174: Training Step 275/354: batchLoss = 0.6694, diffLoss = 3.2704, kgLoss = 0.0191
2025-04-08 17:30:19.603910: Training Step 276/354: batchLoss = 0.6117, diffLoss = 2.9941, kgLoss = 0.0161
2025-04-08 17:30:21.222566: Training Step 277/354: batchLoss = 0.6094, diffLoss = 2.9862, kgLoss = 0.0152
2025-04-08 17:30:22.847686: Training Step 278/354: batchLoss = 0.5705, diffLoss = 2.7898, kgLoss = 0.0157
2025-04-08 17:30:24.466056: Training Step 279/354: batchLoss = 0.4808, diffLoss = 2.3541, kgLoss = 0.0125
2025-04-08 17:30:26.086887: Training Step 280/354: batchLoss = 0.6021, diffLoss = 2.9538, kgLoss = 0.0142
2025-04-08 17:30:27.708208: Training Step 281/354: batchLoss = 0.9065, diffLoss = 4.4458, kgLoss = 0.0217
2025-04-08 17:30:29.330415: Training Step 282/354: batchLoss = 0.5283, diffLoss = 2.5914, kgLoss = 0.0126
2025-04-08 17:30:30.953947: Training Step 283/354: batchLoss = 0.4729, diffLoss = 2.3174, kgLoss = 0.0118
2025-04-08 17:30:32.576441: Training Step 284/354: batchLoss = 0.5040, diffLoss = 2.4641, kgLoss = 0.0140
2025-04-08 17:30:34.202161: Training Step 285/354: batchLoss = 0.5090, diffLoss = 2.4892, kgLoss = 0.0140
2025-04-08 17:30:35.823004: Training Step 286/354: batchLoss = 0.5679, diffLoss = 2.7805, kgLoss = 0.0148
2025-04-08 17:30:37.437567: Training Step 287/354: batchLoss = 0.4411, diffLoss = 2.1589, kgLoss = 0.0117
2025-04-08 17:30:39.062551: Training Step 288/354: batchLoss = 0.5586, diffLoss = 2.7346, kgLoss = 0.0146
2025-04-08 17:30:40.684271: Training Step 289/354: batchLoss = 0.5305, diffLoss = 2.5998, kgLoss = 0.0132
2025-04-08 17:30:42.306907: Training Step 290/354: batchLoss = 0.4653, diffLoss = 2.2789, kgLoss = 0.0119
2025-04-08 17:30:43.934315: Training Step 291/354: batchLoss = 0.5290, diffLoss = 2.5904, kgLoss = 0.0137
2025-04-08 17:30:45.556066: Training Step 292/354: batchLoss = 0.5289, diffLoss = 2.5954, kgLoss = 0.0123
2025-04-08 17:30:47.179693: Training Step 293/354: batchLoss = 0.6977, diffLoss = 3.4170, kgLoss = 0.0179
2025-04-08 17:30:48.798296: Training Step 294/354: batchLoss = 0.5573, diffLoss = 2.7298, kgLoss = 0.0142
2025-04-08 17:30:50.419143: Training Step 295/354: batchLoss = 0.4421, diffLoss = 2.1597, kgLoss = 0.0127
2025-04-08 17:30:52.042237: Training Step 296/354: batchLoss = 0.4679, diffLoss = 2.2939, kgLoss = 0.0114
2025-04-08 17:30:53.656958: Training Step 297/354: batchLoss = 0.6119, diffLoss = 3.0016, kgLoss = 0.0145
2025-04-08 17:30:55.273597: Training Step 298/354: batchLoss = 0.5327, diffLoss = 2.6113, kgLoss = 0.0130
2025-04-08 17:30:56.886648: Training Step 299/354: batchLoss = 0.4397, diffLoss = 2.1510, kgLoss = 0.0119
2025-04-08 17:30:58.503903: Training Step 300/354: batchLoss = 0.4825, diffLoss = 2.3610, kgLoss = 0.0129
2025-04-08 17:31:00.123921: Training Step 301/354: batchLoss = 0.5131, diffLoss = 2.5075, kgLoss = 0.0145
2025-04-08 17:31:01.744640: Training Step 302/354: batchLoss = 0.5087, diffLoss = 2.4903, kgLoss = 0.0133
2025-04-08 17:31:03.371507: Training Step 303/354: batchLoss = 0.5659, diffLoss = 2.7748, kgLoss = 0.0137
2025-04-08 17:31:04.997805: Training Step 304/354: batchLoss = 0.4533, diffLoss = 2.2110, kgLoss = 0.0138
2025-04-08 17:31:06.617627: Training Step 305/354: batchLoss = 0.5155, diffLoss = 2.5232, kgLoss = 0.0136
2025-04-08 17:31:08.239039: Training Step 306/354: batchLoss = 0.4982, diffLoss = 2.4396, kgLoss = 0.0128
2025-04-08 17:31:09.854723: Training Step 307/354: batchLoss = 0.5684, diffLoss = 2.7683, kgLoss = 0.0184
2025-04-08 17:31:11.479159: Training Step 308/354: batchLoss = 0.5330, diffLoss = 2.6101, kgLoss = 0.0138
2025-04-08 17:31:13.094014: Training Step 309/354: batchLoss = 0.4260, diffLoss = 2.0867, kgLoss = 0.0108
2025-04-08 17:31:14.707349: Training Step 310/354: batchLoss = 0.5398, diffLoss = 2.6453, kgLoss = 0.0134
2025-04-08 17:31:16.323452: Training Step 311/354: batchLoss = 0.4824, diffLoss = 2.3573, kgLoss = 0.0136
2025-04-08 17:31:17.948533: Training Step 312/354: batchLoss = 0.6224, diffLoss = 3.0412, kgLoss = 0.0177
2025-04-08 17:31:19.572815: Training Step 313/354: batchLoss = 0.5722, diffLoss = 2.7991, kgLoss = 0.0155
2025-04-08 17:31:21.192595: Training Step 314/354: batchLoss = 0.5596, diffLoss = 2.7494, kgLoss = 0.0121
2025-04-08 17:31:22.820576: Training Step 315/354: batchLoss = 0.5060, diffLoss = 2.4731, kgLoss = 0.0142
2025-04-08 17:31:24.441029: Training Step 316/354: batchLoss = 0.6831, diffLoss = 3.3450, kgLoss = 0.0176
2025-04-08 17:31:26.057811: Training Step 317/354: batchLoss = 0.6785, diffLoss = 3.3284, kgLoss = 0.0160
2025-04-08 17:31:27.674520: Training Step 318/354: batchLoss = 0.5073, diffLoss = 2.4818, kgLoss = 0.0137
2025-04-08 17:31:29.293350: Training Step 319/354: batchLoss = 0.5824, diffLoss = 2.8474, kgLoss = 0.0162
2025-04-08 17:31:30.913703: Training Step 320/354: batchLoss = 0.5271, diffLoss = 2.5827, kgLoss = 0.0132
2025-04-08 17:31:32.537415: Training Step 321/354: batchLoss = 0.6128, diffLoss = 3.0029, kgLoss = 0.0153
2025-04-08 17:31:34.152968: Training Step 322/354: batchLoss = 0.5288, diffLoss = 2.5784, kgLoss = 0.0164
2025-04-08 17:31:35.767631: Training Step 323/354: batchLoss = 0.4879, diffLoss = 2.3863, kgLoss = 0.0133
2025-04-08 17:31:37.389392: Training Step 324/354: batchLoss = 0.5389, diffLoss = 2.6404, kgLoss = 0.0136
2025-04-08 17:31:39.011128: Training Step 325/354: batchLoss = 0.5590, diffLoss = 2.7364, kgLoss = 0.0146
2025-04-08 17:31:40.627223: Training Step 326/354: batchLoss = 0.5607, diffLoss = 2.7399, kgLoss = 0.0159
2025-04-08 17:31:42.241787: Training Step 327/354: batchLoss = 0.5520, diffLoss = 2.7000, kgLoss = 0.0150
2025-04-08 17:31:43.860892: Training Step 328/354: batchLoss = 0.5667, diffLoss = 2.7776, kgLoss = 0.0140
2025-04-08 17:31:45.473556: Training Step 329/354: batchLoss = 0.5525, diffLoss = 2.7070, kgLoss = 0.0139
2025-04-08 17:31:47.111125: Training Step 330/354: batchLoss = 0.9404, diffLoss = 4.6069, kgLoss = 0.0238
2025-04-08 17:31:48.729111: Training Step 331/354: batchLoss = 0.5261, diffLoss = 2.5718, kgLoss = 0.0147
2025-04-08 17:31:50.353979: Training Step 332/354: batchLoss = 0.5459, diffLoss = 2.6780, kgLoss = 0.0129
2025-04-08 17:31:51.975353: Training Step 333/354: batchLoss = 0.5301, diffLoss = 2.5928, kgLoss = 0.0144
2025-04-08 17:31:53.597782: Training Step 334/354: batchLoss = 0.7288, diffLoss = 3.5719, kgLoss = 0.0181
2025-04-08 17:31:55.217313: Training Step 335/354: batchLoss = 0.6532, diffLoss = 3.1979, kgLoss = 0.0171
2025-04-08 17:31:56.832663: Training Step 336/354: batchLoss = 0.4911, diffLoss = 2.4059, kgLoss = 0.0124
2025-04-08 17:31:58.452511: Training Step 337/354: batchLoss = 0.6026, diffLoss = 2.9443, kgLoss = 0.0171
2025-04-08 17:32:00.071164: Training Step 338/354: batchLoss = 0.5100, diffLoss = 2.4968, kgLoss = 0.0133
2025-04-08 17:32:01.687169: Training Step 339/354: batchLoss = 0.5264, diffLoss = 2.5741, kgLoss = 0.0145
2025-04-08 17:32:03.312478: Training Step 340/354: batchLoss = 0.4741, diffLoss = 2.3231, kgLoss = 0.0118
2025-04-08 17:32:04.933073: Training Step 341/354: batchLoss = 0.6267, diffLoss = 3.0675, kgLoss = 0.0165
2025-04-08 17:32:06.555556: Training Step 342/354: batchLoss = 0.4697, diffLoss = 2.2952, kgLoss = 0.0133
2025-04-08 17:32:08.179504: Training Step 343/354: batchLoss = 0.5094, diffLoss = 2.4914, kgLoss = 0.0139
2025-04-08 17:32:09.813049: Training Step 344/354: batchLoss = 0.6083, diffLoss = 2.9875, kgLoss = 0.0135
2025-04-08 17:32:11.434352: Training Step 345/354: batchLoss = 0.4301, diffLoss = 2.0912, kgLoss = 0.0149
2025-04-08 17:32:13.052350: Training Step 346/354: batchLoss = 0.5743, diffLoss = 2.8141, kgLoss = 0.0143
2025-04-08 17:32:14.666951: Training Step 347/354: batchLoss = 0.6347, diffLoss = 3.1141, kgLoss = 0.0148
2025-04-08 17:32:16.279436: Training Step 348/354: batchLoss = 0.6676, diffLoss = 3.2734, kgLoss = 0.0161
2025-04-08 17:32:17.908041: Training Step 349/354: batchLoss = 0.6427, diffLoss = 3.1524, kgLoss = 0.0152
2025-04-08 17:32:19.527063: Training Step 350/354: batchLoss = 0.5810, diffLoss = 2.8313, kgLoss = 0.0184
2025-04-08 17:32:21.144435: Training Step 351/354: batchLoss = 0.5163, diffLoss = 2.5269, kgLoss = 0.0137
2025-04-08 17:32:22.745767: Training Step 352/354: batchLoss = 0.5171, diffLoss = 2.5273, kgLoss = 0.0145
2025-04-08 17:32:24.161762: Training Step 353/354: batchLoss = 0.5409, diffLoss = 2.6446, kgLoss = 0.0150
2025-04-08 17:32:24.250853: 
2025-04-08 17:32:24.251480: Epoch 23/1000, Train: epLoss = 1.0067, epDfLoss = 4.9290, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 17:32:25.569939: Steps 0/138: batch_recall = 45.89, batch_ndcg = 26.52 
2025-04-08 17:32:26.898146: Steps 1/138: batch_recall = 49.30, batch_ndcg = 28.93 
2025-04-08 17:32:28.211474: Steps 2/138: batch_recall = 60.05, batch_ndcg = 36.52 
2025-04-08 17:32:29.537996: Steps 3/138: batch_recall = 58.98, batch_ndcg = 34.41 
2025-04-08 17:32:30.837692: Steps 4/138: batch_recall = 67.90, batch_ndcg = 41.32 
2025-04-08 17:32:32.142484: Steps 5/138: batch_recall = 58.76, batch_ndcg = 32.22 
2025-04-08 17:32:33.439973: Steps 6/138: batch_recall = 53.05, batch_ndcg = 32.01 
2025-04-08 17:32:34.743954: Steps 7/138: batch_recall = 63.87, batch_ndcg = 41.85 
2025-04-08 17:32:36.045498: Steps 8/138: batch_recall = 61.36, batch_ndcg = 39.60 
2025-04-08 17:32:37.349063: Steps 9/138: batch_recall = 58.66, batch_ndcg = 33.78 
2025-04-08 17:32:38.647839: Steps 10/138: batch_recall = 57.08, batch_ndcg = 32.19 
2025-04-08 17:32:39.950448: Steps 11/138: batch_recall = 56.99, batch_ndcg = 32.61 
2025-04-08 17:32:41.253949: Steps 12/138: batch_recall = 53.51, batch_ndcg = 29.44 
2025-04-08 17:32:42.545582: Steps 13/138: batch_recall = 53.87, batch_ndcg = 31.48 
2025-04-08 17:32:43.838541: Steps 14/138: batch_recall = 53.33, batch_ndcg = 31.11 
2025-04-08 17:32:45.134056: Steps 15/138: batch_recall = 48.51, batch_ndcg = 29.76 
2025-04-08 17:32:46.430370: Steps 16/138: batch_recall = 61.19, batch_ndcg = 34.22 
2025-04-08 17:32:47.711915: Steps 17/138: batch_recall = 56.87, batch_ndcg = 33.26 
2025-04-08 17:32:49.014882: Steps 18/138: batch_recall = 52.82, batch_ndcg = 32.63 
2025-04-08 17:32:50.317234: Steps 19/138: batch_recall = 56.50, batch_ndcg = 33.88 
2025-04-08 17:32:51.614579: Steps 20/138: batch_recall = 60.88, batch_ndcg = 36.17 
2025-04-08 17:32:52.918791: Steps 21/138: batch_recall = 71.07, batch_ndcg = 41.39 
2025-04-08 17:32:54.209985: Steps 22/138: batch_recall = 56.20, batch_ndcg = 33.23 
2025-04-08 17:32:55.503756: Steps 23/138: batch_recall = 51.23, batch_ndcg = 30.02 
2025-04-08 17:32:56.803952: Steps 24/138: batch_recall = 57.69, batch_ndcg = 31.62 
2025-04-08 17:32:58.102675: Steps 25/138: batch_recall = 61.62, batch_ndcg = 35.72 
2025-04-08 17:32:59.398186: Steps 26/138: batch_recall = 57.34, batch_ndcg = 33.03 
2025-04-08 17:33:00.684393: Steps 27/138: batch_recall = 59.29, batch_ndcg = 33.30 
2025-04-08 17:33:01.962383: Steps 28/138: batch_recall = 61.00, batch_ndcg = 33.37 
2025-04-08 17:33:03.252386: Steps 29/138: batch_recall = 63.14, batch_ndcg = 32.53 
2025-04-08 17:33:04.543487: Steps 30/138: batch_recall = 57.56, batch_ndcg = 34.02 
2025-04-08 17:33:05.835857: Steps 31/138: batch_recall = 45.03, batch_ndcg = 25.63 
2025-04-08 17:33:07.126838: Steps 32/138: batch_recall = 54.22, batch_ndcg = 31.46 
2025-04-08 17:33:08.425865: Steps 33/138: batch_recall = 61.09, batch_ndcg = 33.61 
2025-04-08 17:33:09.721212: Steps 34/138: batch_recall = 54.75, batch_ndcg = 29.38 
2025-04-08 17:33:11.022281: Steps 35/138: batch_recall = 51.54, batch_ndcg = 29.57 
2025-04-08 17:33:12.320688: Steps 36/138: batch_recall = 48.06, batch_ndcg = 27.23 
2025-04-08 17:33:13.608075: Steps 37/138: batch_recall = 59.37, batch_ndcg = 34.33 
2025-04-08 17:33:14.890494: Steps 38/138: batch_recall = 60.38, batch_ndcg = 33.09 
2025-04-08 17:33:16.175316: Steps 39/138: batch_recall = 65.68, batch_ndcg = 38.58 
2025-04-08 17:33:17.455603: Steps 40/138: batch_recall = 61.19, batch_ndcg = 30.72 
2025-04-08 17:33:18.737149: Steps 41/138: batch_recall = 60.98, batch_ndcg = 33.96 
2025-04-08 17:33:20.016649: Steps 42/138: batch_recall = 54.50, batch_ndcg = 30.29 
2025-04-08 17:33:21.304819: Steps 43/138: batch_recall = 56.87, batch_ndcg = 35.63 
2025-04-08 17:33:22.594953: Steps 44/138: batch_recall = 57.56, batch_ndcg = 31.32 
2025-04-08 17:33:23.883073: Steps 45/138: batch_recall = 61.26, batch_ndcg = 35.18 
2025-04-08 17:33:25.158484: Steps 46/138: batch_recall = 61.85, batch_ndcg = 35.55 
2025-04-08 17:33:26.445654: Steps 47/138: batch_recall = 50.86, batch_ndcg = 31.66 
2025-04-08 17:33:27.738647: Steps 48/138: batch_recall = 60.25, batch_ndcg = 34.80 
2025-04-08 17:33:29.019551: Steps 49/138: batch_recall = 67.43, batch_ndcg = 37.86 
2025-04-08 17:33:30.309383: Steps 50/138: batch_recall = 58.17, batch_ndcg = 30.54 
2025-04-08 17:33:31.584739: Steps 51/138: batch_recall = 61.62, batch_ndcg = 35.86 
2025-04-08 17:33:32.870251: Steps 52/138: batch_recall = 66.08, batch_ndcg = 42.56 
2025-04-08 17:33:34.151768: Steps 53/138: batch_recall = 67.36, batch_ndcg = 35.07 
2025-04-08 17:33:35.428376: Steps 54/138: batch_recall = 68.04, batch_ndcg = 38.66 
2025-04-08 17:33:36.708959: Steps 55/138: batch_recall = 60.98, batch_ndcg = 33.27 
2025-04-08 17:33:37.997573: Steps 56/138: batch_recall = 62.78, batch_ndcg = 36.35 
2025-04-08 17:33:39.292322: Steps 57/138: batch_recall = 58.94, batch_ndcg = 33.04 
2025-04-08 17:33:40.588139: Steps 58/138: batch_recall = 69.66, batch_ndcg = 37.18 
2025-04-08 17:33:41.877918: Steps 59/138: batch_recall = 66.00, batch_ndcg = 39.43 
2025-04-08 17:33:43.166855: Steps 60/138: batch_recall = 66.77, batch_ndcg = 37.60 
2025-04-08 17:33:44.454805: Steps 61/138: batch_recall = 64.23, batch_ndcg = 35.98 
2025-04-08 17:33:45.750906: Steps 62/138: batch_recall = 82.71, batch_ndcg = 43.45 
2025-04-08 17:33:47.031911: Steps 63/138: batch_recall = 74.97, batch_ndcg = 42.80 
2025-04-08 17:33:48.313448: Steps 64/138: batch_recall = 59.50, batch_ndcg = 32.70 
2025-04-08 17:33:49.591989: Steps 65/138: batch_recall = 86.28, batch_ndcg = 47.38 
2025-04-08 17:33:50.868777: Steps 66/138: batch_recall = 68.12, batch_ndcg = 40.61 
2025-04-08 17:33:52.152269: Steps 67/138: batch_recall = 76.33, batch_ndcg = 46.63 
2025-04-08 17:33:53.440237: Steps 68/138: batch_recall = 60.97, batch_ndcg = 33.44 
2025-04-08 17:33:54.722111: Steps 69/138: batch_recall = 88.06, batch_ndcg = 51.76 
2025-04-08 17:33:56.006732: Steps 70/138: batch_recall = 79.38, batch_ndcg = 45.21 
2025-04-08 17:33:57.289895: Steps 71/138: batch_recall = 86.82, batch_ndcg = 50.83 
2025-04-08 17:33:58.571574: Steps 72/138: batch_recall = 86.77, batch_ndcg = 49.91 
2025-04-08 17:33:59.847959: Steps 73/138: batch_recall = 83.90, batch_ndcg = 47.31 
2025-04-08 17:34:01.129786: Steps 74/138: batch_recall = 77.58, batch_ndcg = 48.78 
2025-04-08 17:34:02.408824: Steps 75/138: batch_recall = 84.74, batch_ndcg = 49.53 
2025-04-08 17:34:03.688160: Steps 76/138: batch_recall = 95.80, batch_ndcg = 55.34 
2025-04-08 17:34:04.964506: Steps 77/138: batch_recall = 90.09, batch_ndcg = 50.57 
2025-04-08 17:34:06.239549: Steps 78/138: batch_recall = 90.41, batch_ndcg = 48.74 
2025-04-08 17:34:07.527909: Steps 79/138: batch_recall = 89.59, batch_ndcg = 49.99 
2025-04-08 17:34:08.819490: Steps 80/138: batch_recall = 71.69, batch_ndcg = 38.71 
2025-04-08 17:34:10.107575: Steps 81/138: batch_recall = 81.58, batch_ndcg = 47.63 
2025-04-08 17:34:11.393654: Steps 82/138: batch_recall = 88.68, batch_ndcg = 52.46 
2025-04-08 17:34:12.674061: Steps 83/138: batch_recall = 79.98, batch_ndcg = 47.06 
2025-04-08 17:34:13.957263: Steps 84/138: batch_recall = 98.00, batch_ndcg = 56.92 
2025-04-08 17:34:15.226569: Steps 85/138: batch_recall = 104.88, batch_ndcg = 61.29 
2025-04-08 17:34:16.500417: Steps 86/138: batch_recall = 119.91, batch_ndcg = 70.72 
2025-04-08 17:34:17.779011: Steps 87/138: batch_recall = 104.93, batch_ndcg = 55.06 
2025-04-08 17:34:19.049812: Steps 88/138: batch_recall = 103.35, batch_ndcg = 59.28 
2025-04-08 17:34:20.325624: Steps 89/138: batch_recall = 118.74, batch_ndcg = 68.19 
2025-04-08 17:34:21.602618: Steps 90/138: batch_recall = 105.42, batch_ndcg = 59.03 
2025-04-08 17:34:22.873713: Steps 91/138: batch_recall = 118.39, batch_ndcg = 65.43 
2025-04-08 17:34:24.143901: Steps 92/138: batch_recall = 118.62, batch_ndcg = 64.25 
2025-04-08 17:34:25.419591: Steps 93/138: batch_recall = 121.22, batch_ndcg = 68.58 
2025-04-08 17:34:26.695879: Steps 94/138: batch_recall = 124.08, batch_ndcg = 66.26 
2025-04-08 17:34:27.974488: Steps 95/138: batch_recall = 113.81, batch_ndcg = 66.53 
2025-04-08 17:34:29.253397: Steps 96/138: batch_recall = 134.90, batch_ndcg = 80.43 
2025-04-08 17:34:30.520140: Steps 97/138: batch_recall = 150.52, batch_ndcg = 90.83 
2025-04-08 17:34:31.796085: Steps 98/138: batch_recall = 107.30, batch_ndcg = 63.62 
2025-04-08 17:34:33.066074: Steps 99/138: batch_recall = 126.14, batch_ndcg = 71.95 
2025-04-08 17:34:34.336289: Steps 100/138: batch_recall = 130.10, batch_ndcg = 72.83 
2025-04-08 17:34:35.600858: Steps 101/138: batch_recall = 127.51, batch_ndcg = 70.38 
2025-04-08 17:34:36.874329: Steps 102/138: batch_recall = 122.62, batch_ndcg = 71.96 
2025-04-08 17:34:38.136557: Steps 103/138: batch_recall = 144.52, batch_ndcg = 80.67 
2025-04-08 17:34:39.403870: Steps 104/138: batch_recall = 133.81, batch_ndcg = 78.46 
2025-04-08 17:34:40.680892: Steps 105/138: batch_recall = 118.25, batch_ndcg = 66.97 
2025-04-08 17:34:41.954355: Steps 106/138: batch_recall = 103.93, batch_ndcg = 58.74 
2025-04-08 17:34:43.227998: Steps 107/138: batch_recall = 112.31, batch_ndcg = 64.01 
2025-04-08 17:34:44.499968: Steps 108/138: batch_recall = 117.14, batch_ndcg = 70.24 
2025-04-08 17:34:45.774352: Steps 109/138: batch_recall = 136.23, batch_ndcg = 77.25 
2025-04-08 17:34:47.052943: Steps 110/138: batch_recall = 122.41, batch_ndcg = 64.07 
2025-04-08 17:34:48.323521: Steps 111/138: batch_recall = 138.58, batch_ndcg = 82.59 
2025-04-08 17:34:49.605292: Steps 112/138: batch_recall = 169.13, batch_ndcg = 93.36 
2025-04-08 17:34:50.871406: Steps 113/138: batch_recall = 125.91, batch_ndcg = 70.36 
2025-04-08 17:34:52.128883: Steps 114/138: batch_recall = 121.78, batch_ndcg = 70.88 
2025-04-08 17:34:53.392877: Steps 115/138: batch_recall = 122.42, batch_ndcg = 63.79 
2025-04-08 17:34:54.659764: Steps 116/138: batch_recall = 123.57, batch_ndcg = 66.15 
2025-04-08 17:34:55.921428: Steps 117/138: batch_recall = 113.14, batch_ndcg = 65.77 
2025-04-08 17:34:57.184495: Steps 118/138: batch_recall = 127.98, batch_ndcg = 70.51 
2025-04-08 17:34:58.459699: Steps 119/138: batch_recall = 139.16, batch_ndcg = 75.33 
2025-04-08 17:34:59.735982: Steps 120/138: batch_recall = 124.24, batch_ndcg = 70.37 
2025-04-08 17:35:01.004154: Steps 121/138: batch_recall = 150.03, batch_ndcg = 80.01 
2025-04-08 17:35:02.282007: Steps 122/138: batch_recall = 150.64, batch_ndcg = 83.05 
2025-04-08 17:35:03.558360: Steps 123/138: batch_recall = 132.34, batch_ndcg = 73.62 
2025-04-08 17:35:04.834596: Steps 124/138: batch_recall = 153.30, batch_ndcg = 93.27 
2025-04-08 17:35:06.110787: Steps 125/138: batch_recall = 132.75, batch_ndcg = 72.96 
2025-04-08 17:35:07.377762: Steps 126/138: batch_recall = 158.10, batch_ndcg = 88.63 
2025-04-08 17:35:08.634697: Steps 127/138: batch_recall = 145.51, batch_ndcg = 82.59 
2025-04-08 17:35:09.901660: Steps 128/138: batch_recall = 130.50, batch_ndcg = 73.26 
2025-04-08 17:35:11.177072: Steps 129/138: batch_recall = 162.52, batch_ndcg = 92.29 
2025-04-08 17:35:12.448809: Steps 130/138: batch_recall = 133.77, batch_ndcg = 70.05 
2025-04-08 17:35:13.719971: Steps 131/138: batch_recall = 153.05, batch_ndcg = 87.24 
2025-04-08 17:35:14.996872: Steps 132/138: batch_recall = 147.00, batch_ndcg = 84.52 
2025-04-08 17:35:16.287088: Steps 133/138: batch_recall = 147.60, batch_ndcg = 84.66 
2025-04-08 17:35:17.562254: Steps 134/138: batch_recall = 146.46, batch_ndcg = 82.73 
2025-04-08 17:35:18.836288: Steps 135/138: batch_recall = 168.99, batch_ndcg = 95.86 
2025-04-08 17:35:20.109486: Steps 136/138: batch_recall = 152.68, batch_ndcg = 78.29 
2025-04-08 17:35:21.378087: Steps 137/138: batch_recall = 142.45, batch_ndcg = 86.97 
2025-04-08 17:35:21.378634: Epoch 23/1000, Test: Recall = 0.1769, NDCG = 0.1006  

2025-04-08 17:35:23.137406: Training Step 0/354: batchLoss = 0.5595, diffLoss = 2.7469, kgLoss = 0.0126
2025-04-08 17:35:24.754901: Training Step 1/354: batchLoss = 0.6057, diffLoss = 2.9740, kgLoss = 0.0137
2025-04-08 17:35:26.376942: Training Step 2/354: batchLoss = 0.6194, diffLoss = 3.0371, kgLoss = 0.0149
2025-04-08 17:35:27.999402: Training Step 3/354: batchLoss = 0.4594, diffLoss = 2.2491, kgLoss = 0.0119
2025-04-08 17:35:29.608361: Training Step 4/354: batchLoss = 0.5446, diffLoss = 2.6660, kgLoss = 0.0143
2025-04-08 17:35:31.224651: Training Step 5/354: batchLoss = 0.6002, diffLoss = 2.9184, kgLoss = 0.0207
2025-04-08 17:35:32.851055: Training Step 6/354: batchLoss = 0.5755, diffLoss = 2.8199, kgLoss = 0.0144
2025-04-08 17:35:34.474943: Training Step 7/354: batchLoss = 0.6115, diffLoss = 2.9941, kgLoss = 0.0158
2025-04-08 17:35:36.102659: Training Step 8/354: batchLoss = 0.6098, diffLoss = 2.9866, kgLoss = 0.0156
2025-04-08 17:35:37.721993: Training Step 9/354: batchLoss = 0.5959, diffLoss = 2.9165, kgLoss = 0.0157
2025-04-08 17:35:39.344805: Training Step 10/354: batchLoss = 0.5386, diffLoss = 2.6357, kgLoss = 0.0143
2025-04-08 17:35:40.959948: Training Step 11/354: batchLoss = 0.4942, diffLoss = 2.4208, kgLoss = 0.0126
2025-04-08 17:35:42.579366: Training Step 12/354: batchLoss = 0.5905, diffLoss = 2.8941, kgLoss = 0.0147
2025-04-08 17:35:44.198604: Training Step 13/354: batchLoss = 0.5499, diffLoss = 2.6976, kgLoss = 0.0130
2025-04-08 17:35:45.818136: Training Step 14/354: batchLoss = 0.5703, diffLoss = 2.7838, kgLoss = 0.0169
2025-04-08 17:35:47.445566: Training Step 15/354: batchLoss = 0.5615, diffLoss = 2.7501, kgLoss = 0.0143
2025-04-08 17:35:49.062038: Training Step 16/354: batchLoss = 0.5010, diffLoss = 2.4544, kgLoss = 0.0126
2025-04-08 17:35:50.679664: Training Step 17/354: batchLoss = 0.6097, diffLoss = 2.9779, kgLoss = 0.0177
2025-04-08 17:35:52.298901: Training Step 18/354: batchLoss = 0.4778, diffLoss = 2.3366, kgLoss = 0.0131
2025-04-08 17:35:53.917342: Training Step 19/354: batchLoss = 0.7009, diffLoss = 3.4429, kgLoss = 0.0154
2025-04-08 17:35:55.541061: Training Step 20/354: batchLoss = 0.5001, diffLoss = 2.4509, kgLoss = 0.0124
2025-04-08 17:35:57.158209: Training Step 21/354: batchLoss = 0.6125, diffLoss = 3.0003, kgLoss = 0.0156
2025-04-08 17:35:58.774671: Training Step 22/354: batchLoss = 0.6013, diffLoss = 2.9458, kgLoss = 0.0151
2025-04-08 17:36:00.389062: Training Step 23/354: batchLoss = 0.5546, diffLoss = 2.7175, kgLoss = 0.0139
2025-04-08 17:36:02.007122: Training Step 24/354: batchLoss = 0.7354, diffLoss = 3.6009, kgLoss = 0.0190
2025-04-08 17:36:03.626505: Training Step 25/354: batchLoss = 0.5135, diffLoss = 2.5174, kgLoss = 0.0126
2025-04-08 17:36:05.249265: Training Step 26/354: batchLoss = 0.5666, diffLoss = 2.7800, kgLoss = 0.0132
2025-04-08 17:36:06.875905: Training Step 27/354: batchLoss = 0.5198, diffLoss = 2.5426, kgLoss = 0.0141
2025-04-08 17:36:08.496238: Training Step 28/354: batchLoss = 0.5004, diffLoss = 2.4500, kgLoss = 0.0129
2025-04-08 17:36:10.113755: Training Step 29/354: batchLoss = 0.5157, diffLoss = 2.5225, kgLoss = 0.0139
2025-04-08 17:36:11.727511: Training Step 30/354: batchLoss = 0.5745, diffLoss = 2.8172, kgLoss = 0.0138
2025-04-08 17:36:13.344854: Training Step 31/354: batchLoss = 0.5727, diffLoss = 2.8073, kgLoss = 0.0140
2025-04-08 17:36:14.960358: Training Step 32/354: batchLoss = 0.5648, diffLoss = 2.7661, kgLoss = 0.0144
2025-04-08 17:36:16.576052: Training Step 33/354: batchLoss = 0.5916, diffLoss = 2.8950, kgLoss = 0.0158
2025-04-08 17:36:18.197839: Training Step 34/354: batchLoss = 0.5157, diffLoss = 2.5232, kgLoss = 0.0138
2025-04-08 17:36:19.816307: Training Step 35/354: batchLoss = 0.6046, diffLoss = 2.9588, kgLoss = 0.0161
2025-04-08 17:36:21.438161: Training Step 36/354: batchLoss = 0.4752, diffLoss = 2.3262, kgLoss = 0.0124
2025-04-08 17:36:23.058280: Training Step 37/354: batchLoss = 0.5273, diffLoss = 2.5818, kgLoss = 0.0137
2025-04-08 17:36:24.679807: Training Step 38/354: batchLoss = 0.6082, diffLoss = 2.9805, kgLoss = 0.0151
2025-04-08 17:36:26.299194: Training Step 39/354: batchLoss = 0.6861, diffLoss = 3.3633, kgLoss = 0.0168
2025-04-08 17:36:27.915431: Training Step 40/354: batchLoss = 0.7100, diffLoss = 3.4837, kgLoss = 0.0165
2025-04-08 17:36:29.530398: Training Step 41/354: batchLoss = 0.4804, diffLoss = 2.3505, kgLoss = 0.0129
2025-04-08 17:36:31.142424: Training Step 42/354: batchLoss = 0.6414, diffLoss = 3.1459, kgLoss = 0.0153
2025-04-08 17:36:32.762183: Training Step 43/354: batchLoss = 0.6849, diffLoss = 3.3538, kgLoss = 0.0176
2025-04-08 17:36:34.382305: Training Step 44/354: batchLoss = 0.6118, diffLoss = 2.9991, kgLoss = 0.0149
2025-04-08 17:36:36.004903: Training Step 45/354: batchLoss = 0.5405, diffLoss = 2.6457, kgLoss = 0.0142
2025-04-08 17:36:37.623856: Training Step 46/354: batchLoss = 0.5654, diffLoss = 2.7651, kgLoss = 0.0154
2025-04-08 17:36:39.245968: Training Step 47/354: batchLoss = 0.5707, diffLoss = 2.7949, kgLoss = 0.0146
2025-04-08 17:36:40.874115: Training Step 48/354: batchLoss = 0.5929, diffLoss = 2.9058, kgLoss = 0.0147
2025-04-08 17:36:42.489619: Training Step 49/354: batchLoss = 0.5578, diffLoss = 2.7335, kgLoss = 0.0139
2025-04-08 17:36:44.110017: Training Step 50/354: batchLoss = 0.4589, diffLoss = 2.2408, kgLoss = 0.0135
2025-04-08 17:36:45.726081: Training Step 51/354: batchLoss = 0.5699, diffLoss = 2.7859, kgLoss = 0.0159
2025-04-08 17:36:47.346842: Training Step 52/354: batchLoss = 0.5647, diffLoss = 2.7670, kgLoss = 0.0141
2025-04-08 17:36:48.961370: Training Step 53/354: batchLoss = 0.7070, diffLoss = 3.4654, kgLoss = 0.0173
2025-04-08 17:36:50.582107: Training Step 54/354: batchLoss = 0.5337, diffLoss = 2.6155, kgLoss = 0.0133
2025-04-08 17:36:52.207584: Training Step 55/354: batchLoss = 0.6070, diffLoss = 2.9695, kgLoss = 0.0164
2025-04-08 17:36:53.832185: Training Step 56/354: batchLoss = 0.5092, diffLoss = 2.4929, kgLoss = 0.0133
2025-04-08 17:36:55.457517: Training Step 57/354: batchLoss = 0.5757, diffLoss = 2.8177, kgLoss = 0.0152
2025-04-08 17:36:57.076033: Training Step 58/354: batchLoss = 0.4954, diffLoss = 2.4133, kgLoss = 0.0159
2025-04-08 17:36:58.696120: Training Step 59/354: batchLoss = 0.5457, diffLoss = 2.6598, kgLoss = 0.0172
2025-04-08 17:37:00.320089: Training Step 60/354: batchLoss = 0.4839, diffLoss = 2.3694, kgLoss = 0.0125
2025-04-08 17:37:01.933131: Training Step 61/354: batchLoss = 0.5583, diffLoss = 2.7373, kgLoss = 0.0136
2025-04-08 17:37:03.547999: Training Step 62/354: batchLoss = 0.5294, diffLoss = 2.5940, kgLoss = 0.0133
2025-04-08 17:37:05.167129: Training Step 63/354: batchLoss = 0.5028, diffLoss = 2.4616, kgLoss = 0.0131
2025-04-08 17:37:06.786463: Training Step 64/354: batchLoss = 0.5120, diffLoss = 2.5083, kgLoss = 0.0129
2025-04-08 17:37:08.404496: Training Step 65/354: batchLoss = 0.5463, diffLoss = 2.6727, kgLoss = 0.0147
2025-04-08 17:37:10.024570: Training Step 66/354: batchLoss = 0.5635, diffLoss = 2.7645, kgLoss = 0.0133
2025-04-08 17:37:11.640916: Training Step 67/354: batchLoss = 0.5956, diffLoss = 2.9008, kgLoss = 0.0193
2025-04-08 17:37:13.259975: Training Step 68/354: batchLoss = 0.6248, diffLoss = 3.0605, kgLoss = 0.0159
2025-04-08 17:37:14.880855: Training Step 69/354: batchLoss = 0.5517, diffLoss = 2.7049, kgLoss = 0.0134
2025-04-08 17:37:16.508802: Training Step 70/354: batchLoss = 0.6516, diffLoss = 3.1929, kgLoss = 0.0163
2025-04-08 17:37:18.125141: Training Step 71/354: batchLoss = 0.6580, diffLoss = 3.2189, kgLoss = 0.0177
2025-04-08 17:37:19.744314: Training Step 72/354: batchLoss = 0.6593, diffLoss = 3.2250, kgLoss = 0.0179
2025-04-08 17:37:21.362524: Training Step 73/354: batchLoss = 0.4997, diffLoss = 2.4463, kgLoss = 0.0131
2025-04-08 17:37:22.975125: Training Step 74/354: batchLoss = 0.5863, diffLoss = 2.8749, kgLoss = 0.0142
2025-04-08 17:37:24.596888: Training Step 75/354: batchLoss = 0.4747, diffLoss = 2.3220, kgLoss = 0.0129
2025-04-08 17:37:26.216459: Training Step 76/354: batchLoss = 0.4938, diffLoss = 2.4190, kgLoss = 0.0125
2025-04-08 17:37:27.831592: Training Step 77/354: batchLoss = 0.6224, diffLoss = 3.0439, kgLoss = 0.0170
2025-04-08 17:37:29.454836: Training Step 78/354: batchLoss = 0.4874, diffLoss = 2.3783, kgLoss = 0.0147
2025-04-08 17:37:31.077598: Training Step 79/354: batchLoss = 0.5211, diffLoss = 2.5613, kgLoss = 0.0111
2025-04-08 17:37:32.702157: Training Step 80/354: batchLoss = 0.5339, diffLoss = 2.6215, kgLoss = 0.0120
2025-04-08 17:37:34.323451: Training Step 81/354: batchLoss = 0.5172, diffLoss = 2.5291, kgLoss = 0.0142
2025-04-08 17:37:35.946305: Training Step 82/354: batchLoss = 0.6032, diffLoss = 2.9539, kgLoss = 0.0155
2025-04-08 17:37:37.565258: Training Step 83/354: batchLoss = 0.5881, diffLoss = 2.8813, kgLoss = 0.0148
2025-04-08 17:37:39.181052: Training Step 84/354: batchLoss = 0.6942, diffLoss = 3.3938, kgLoss = 0.0192
2025-04-08 17:37:40.797999: Training Step 85/354: batchLoss = 0.5453, diffLoss = 2.6643, kgLoss = 0.0156
2025-04-08 17:37:42.413286: Training Step 86/354: batchLoss = 0.4855, diffLoss = 2.3772, kgLoss = 0.0126
2025-04-08 17:37:44.030740: Training Step 87/354: batchLoss = 0.4372, diffLoss = 2.1325, kgLoss = 0.0133
2025-04-08 17:37:45.655811: Training Step 88/354: batchLoss = 0.6035, diffLoss = 2.9518, kgLoss = 0.0165
2025-04-08 17:37:47.279517: Training Step 89/354: batchLoss = 0.4224, diffLoss = 2.0628, kgLoss = 0.0123
2025-04-08 17:37:48.907440: Training Step 90/354: batchLoss = 0.5257, diffLoss = 2.5756, kgLoss = 0.0132
2025-04-08 17:37:50.528947: Training Step 91/354: batchLoss = 0.5208, diffLoss = 2.5513, kgLoss = 0.0132
2025-04-08 17:37:52.150810: Training Step 92/354: batchLoss = 0.5374, diffLoss = 2.6277, kgLoss = 0.0148
2025-04-08 17:37:53.766969: Training Step 93/354: batchLoss = 0.5616, diffLoss = 2.7485, kgLoss = 0.0149
2025-04-08 17:37:55.388053: Training Step 94/354: batchLoss = 0.5852, diffLoss = 2.8694, kgLoss = 0.0141
2025-04-08 17:37:56.999234: Training Step 95/354: batchLoss = 0.4739, diffLoss = 2.3166, kgLoss = 0.0132
2025-04-08 17:37:58.614630: Training Step 96/354: batchLoss = 0.5542, diffLoss = 2.7123, kgLoss = 0.0147
2025-04-08 17:38:00.229546: Training Step 97/354: batchLoss = 0.6130, diffLoss = 3.0069, kgLoss = 0.0145
2025-04-08 17:38:01.850189: Training Step 98/354: batchLoss = 0.4999, diffLoss = 2.4457, kgLoss = 0.0134
2025-04-08 17:38:03.465878: Training Step 99/354: batchLoss = 0.5363, diffLoss = 2.6229, kgLoss = 0.0146
2025-04-08 17:38:05.082183: Training Step 100/354: batchLoss = 0.3930, diffLoss = 1.9119, kgLoss = 0.0132
2025-04-08 17:38:06.702330: Training Step 101/354: batchLoss = 0.5323, diffLoss = 2.6055, kgLoss = 0.0140
2025-04-08 17:38:08.322021: Training Step 102/354: batchLoss = 0.5576, diffLoss = 2.7308, kgLoss = 0.0143
2025-04-08 17:38:09.938584: Training Step 103/354: batchLoss = 0.4117, diffLoss = 2.0156, kgLoss = 0.0107
2025-04-08 17:38:11.552772: Training Step 104/354: batchLoss = 0.5913, diffLoss = 2.9008, kgLoss = 0.0139
2025-04-08 17:38:13.165396: Training Step 105/354: batchLoss = 0.6193, diffLoss = 3.0294, kgLoss = 0.0168
2025-04-08 17:38:14.787761: Training Step 106/354: batchLoss = 0.5659, diffLoss = 2.7670, kgLoss = 0.0157
2025-04-08 17:38:16.412478: Training Step 107/354: batchLoss = 0.4561, diffLoss = 2.2283, kgLoss = 0.0130
2025-04-08 17:38:18.030776: Training Step 108/354: batchLoss = 0.5239, diffLoss = 2.5620, kgLoss = 0.0143
2025-04-08 17:38:19.663903: Training Step 109/354: batchLoss = 0.5623, diffLoss = 2.7522, kgLoss = 0.0148
2025-04-08 17:38:21.288956: Training Step 110/354: batchLoss = 0.5620, diffLoss = 2.7570, kgLoss = 0.0133
2025-04-08 17:38:22.913427: Training Step 111/354: batchLoss = 0.5514, diffLoss = 2.7005, kgLoss = 0.0141
2025-04-08 17:38:24.536075: Training Step 112/354: batchLoss = 0.5839, diffLoss = 2.8646, kgLoss = 0.0137
2025-04-08 17:38:26.160603: Training Step 113/354: batchLoss = 0.5639, diffLoss = 2.7669, kgLoss = 0.0132
2025-04-08 17:38:27.780403: Training Step 114/354: batchLoss = 0.6806, diffLoss = 3.3368, kgLoss = 0.0165
2025-04-08 17:38:29.398713: Training Step 115/354: batchLoss = 0.6171, diffLoss = 3.0146, kgLoss = 0.0178
2025-04-08 17:38:31.012600: Training Step 116/354: batchLoss = 0.5318, diffLoss = 2.5994, kgLoss = 0.0149
2025-04-08 17:38:32.640281: Training Step 117/354: batchLoss = 0.5616, diffLoss = 2.7407, kgLoss = 0.0168
2025-04-08 17:38:34.267053: Training Step 118/354: batchLoss = 0.5072, diffLoss = 2.4837, kgLoss = 0.0131
2025-04-08 17:38:35.886582: Training Step 119/354: batchLoss = 0.6878, diffLoss = 3.3733, kgLoss = 0.0164
2025-04-08 17:38:37.513342: Training Step 120/354: batchLoss = 0.4662, diffLoss = 2.2817, kgLoss = 0.0124
2025-04-08 17:38:39.137520: Training Step 121/354: batchLoss = 0.4915, diffLoss = 2.3985, kgLoss = 0.0147
2025-04-08 17:38:40.759169: Training Step 122/354: batchLoss = 0.5519, diffLoss = 2.7058, kgLoss = 0.0134
2025-04-08 17:38:42.377503: Training Step 123/354: batchLoss = 0.5425, diffLoss = 2.6587, kgLoss = 0.0134
2025-04-08 17:38:43.996107: Training Step 124/354: batchLoss = 0.4996, diffLoss = 2.4488, kgLoss = 0.0123
2025-04-08 17:38:45.609584: Training Step 125/354: batchLoss = 0.6010, diffLoss = 2.9432, kgLoss = 0.0154
2025-04-08 17:38:47.226331: Training Step 126/354: batchLoss = 0.6804, diffLoss = 3.3404, kgLoss = 0.0154
2025-04-08 17:38:48.852265: Training Step 127/354: batchLoss = 0.5222, diffLoss = 2.5598, kgLoss = 0.0128
2025-04-08 17:38:50.473317: Training Step 128/354: batchLoss = 0.6453, diffLoss = 3.1650, kgLoss = 0.0153
2025-04-08 17:38:52.096644: Training Step 129/354: batchLoss = 0.6480, diffLoss = 3.1752, kgLoss = 0.0163
2025-04-08 17:38:53.720344: Training Step 130/354: batchLoss = 0.4552, diffLoss = 2.2247, kgLoss = 0.0129
2025-04-08 17:38:55.343296: Training Step 131/354: batchLoss = 0.5269, diffLoss = 2.5793, kgLoss = 0.0138
2025-04-08 17:38:56.966853: Training Step 132/354: batchLoss = 0.6544, diffLoss = 3.2032, kgLoss = 0.0172
2025-04-08 17:38:58.583134: Training Step 133/354: batchLoss = 0.4692, diffLoss = 2.2909, kgLoss = 0.0138
2025-04-08 17:39:00.197425: Training Step 134/354: batchLoss = 0.5696, diffLoss = 2.7939, kgLoss = 0.0135
2025-04-08 17:39:01.811828: Training Step 135/354: batchLoss = 0.4132, diffLoss = 2.0148, kgLoss = 0.0128
2025-04-08 17:39:03.430746: Training Step 136/354: batchLoss = 0.5731, diffLoss = 2.8070, kgLoss = 0.0146
2025-04-08 17:39:05.051327: Training Step 137/354: batchLoss = 0.5746, diffLoss = 2.8170, kgLoss = 0.0140
2025-04-08 17:39:06.676828: Training Step 138/354: batchLoss = 0.5804, diffLoss = 2.8419, kgLoss = 0.0151
2025-04-08 17:39:08.301945: Training Step 139/354: batchLoss = 0.5668, diffLoss = 2.7809, kgLoss = 0.0133
2025-04-08 17:39:09.924955: Training Step 140/354: batchLoss = 0.4759, diffLoss = 2.3308, kgLoss = 0.0121
2025-04-08 17:39:11.541965: Training Step 141/354: batchLoss = 0.5498, diffLoss = 2.6843, kgLoss = 0.0162
2025-04-08 17:39:13.162869: Training Step 142/354: batchLoss = 0.5137, diffLoss = 2.5088, kgLoss = 0.0149
2025-04-08 17:39:14.779763: Training Step 143/354: batchLoss = 0.5437, diffLoss = 2.6494, kgLoss = 0.0173
2025-04-08 17:39:16.398299: Training Step 144/354: batchLoss = 0.4303, diffLoss = 2.1067, kgLoss = 0.0112
2025-04-08 17:39:18.011514: Training Step 145/354: batchLoss = 0.4242, diffLoss = 2.0704, kgLoss = 0.0126
2025-04-08 17:39:19.633446: Training Step 146/354: batchLoss = 0.4554, diffLoss = 2.2233, kgLoss = 0.0135
2025-04-08 17:39:21.258763: Training Step 147/354: batchLoss = 0.4896, diffLoss = 2.3971, kgLoss = 0.0128
2025-04-08 17:39:22.877610: Training Step 148/354: batchLoss = 0.7569, diffLoss = 3.7133, kgLoss = 0.0178
2025-04-08 17:39:24.494840: Training Step 149/354: batchLoss = 0.4980, diffLoss = 2.4366, kgLoss = 0.0134
2025-04-08 17:39:26.114332: Training Step 150/354: batchLoss = 0.6108, diffLoss = 2.9866, kgLoss = 0.0168
2025-04-08 17:39:27.736938: Training Step 151/354: batchLoss = 0.4611, diffLoss = 2.2585, kgLoss = 0.0118
2025-04-08 17:39:29.356010: Training Step 152/354: batchLoss = 0.4414, diffLoss = 2.1526, kgLoss = 0.0135
2025-04-08 17:39:30.975385: Training Step 153/354: batchLoss = 0.5059, diffLoss = 2.4750, kgLoss = 0.0136
2025-04-08 17:39:32.591433: Training Step 154/354: batchLoss = 0.6109, diffLoss = 2.9920, kgLoss = 0.0156
2025-04-08 17:39:34.206625: Training Step 155/354: batchLoss = 0.5585, diffLoss = 2.7399, kgLoss = 0.0131
2025-04-08 17:39:35.827134: Training Step 156/354: batchLoss = 0.4583, diffLoss = 2.2389, kgLoss = 0.0131
2025-04-08 17:39:37.445454: Training Step 157/354: batchLoss = 0.6931, diffLoss = 3.3935, kgLoss = 0.0180
2025-04-08 17:39:39.070228: Training Step 158/354: batchLoss = 0.5645, diffLoss = 2.7634, kgLoss = 0.0148
2025-04-08 17:39:40.689584: Training Step 159/354: batchLoss = 0.5132, diffLoss = 2.5094, kgLoss = 0.0141
2025-04-08 17:39:42.309662: Training Step 160/354: batchLoss = 0.5807, diffLoss = 2.8415, kgLoss = 0.0155
2025-04-08 17:39:43.931853: Training Step 161/354: batchLoss = 0.5115, diffLoss = 2.4968, kgLoss = 0.0152
2025-04-08 17:39:45.544554: Training Step 162/354: batchLoss = 0.6472, diffLoss = 3.1738, kgLoss = 0.0155
2025-04-08 17:39:47.162858: Training Step 163/354: batchLoss = 0.5256, diffLoss = 2.5666, kgLoss = 0.0153
2025-04-08 17:39:48.784488: Training Step 164/354: batchLoss = 0.6835, diffLoss = 3.3547, kgLoss = 0.0157
2025-04-08 17:39:50.399734: Training Step 165/354: batchLoss = 0.7156, diffLoss = 3.5130, kgLoss = 0.0162
2025-04-08 17:39:52.017396: Training Step 166/354: batchLoss = 0.5986, diffLoss = 2.9342, kgLoss = 0.0147
2025-04-08 17:39:53.638926: Training Step 167/354: batchLoss = 0.4900, diffLoss = 2.3985, kgLoss = 0.0128
2025-04-08 17:39:55.258974: Training Step 168/354: batchLoss = 0.6420, diffLoss = 3.1425, kgLoss = 0.0169
2025-04-08 17:39:56.877343: Training Step 169/354: batchLoss = 0.8824, diffLoss = 4.3140, kgLoss = 0.0245
2025-04-08 17:39:58.494100: Training Step 170/354: batchLoss = 0.5811, diffLoss = 2.8425, kgLoss = 0.0158
2025-04-08 17:40:00.112675: Training Step 171/354: batchLoss = 0.5704, diffLoss = 2.7959, kgLoss = 0.0141
2025-04-08 17:40:01.728445: Training Step 172/354: batchLoss = 0.5730, diffLoss = 2.8044, kgLoss = 0.0151
2025-04-08 17:40:03.349704: Training Step 173/354: batchLoss = 0.7048, diffLoss = 3.4525, kgLoss = 0.0179
2025-04-08 17:40:04.965091: Training Step 174/354: batchLoss = 0.4676, diffLoss = 2.2850, kgLoss = 0.0132
2025-04-08 17:40:06.585639: Training Step 175/354: batchLoss = 0.5415, diffLoss = 2.6571, kgLoss = 0.0126
2025-04-08 17:40:08.204796: Training Step 176/354: batchLoss = 0.6665, diffLoss = 3.2715, kgLoss = 0.0152
2025-04-08 17:40:09.832752: Training Step 177/354: batchLoss = 0.6245, diffLoss = 3.0574, kgLoss = 0.0162
2025-04-08 17:40:11.458142: Training Step 178/354: batchLoss = 0.3952, diffLoss = 1.9338, kgLoss = 0.0106
2025-04-08 17:40:13.095522: Training Step 179/354: batchLoss = 0.7540, diffLoss = 3.6927, kgLoss = 0.0193
2025-04-08 17:40:14.719707: Training Step 180/354: batchLoss = 0.4574, diffLoss = 2.2376, kgLoss = 0.0123
2025-04-08 17:40:16.348893: Training Step 181/354: batchLoss = 0.6595, diffLoss = 3.2377, kgLoss = 0.0150
2025-04-08 17:40:17.973619: Training Step 182/354: batchLoss = 0.4544, diffLoss = 2.2235, kgLoss = 0.0121
2025-04-08 17:40:19.591908: Training Step 183/354: batchLoss = 0.5259, diffLoss = 2.5720, kgLoss = 0.0144
2025-04-08 17:40:21.210147: Training Step 184/354: batchLoss = 0.5613, diffLoss = 2.7537, kgLoss = 0.0133
2025-04-08 17:40:22.825094: Training Step 185/354: batchLoss = 0.5237, diffLoss = 2.5626, kgLoss = 0.0140
2025-04-08 17:40:24.516763: Training Step 186/354: batchLoss = 0.4393, diffLoss = 2.1521, kgLoss = 0.0111
2025-04-08 17:40:26.141245: Training Step 187/354: batchLoss = 0.5865, diffLoss = 2.8701, kgLoss = 0.0156
2025-04-08 17:40:27.764477: Training Step 188/354: batchLoss = 0.5926, diffLoss = 2.9018, kgLoss = 0.0154
2025-04-08 17:40:29.387660: Training Step 189/354: batchLoss = 0.5657, diffLoss = 2.7751, kgLoss = 0.0133
2025-04-08 17:40:31.010625: Training Step 190/354: batchLoss = 0.4457, diffLoss = 2.1733, kgLoss = 0.0138
2025-04-08 17:40:32.625792: Training Step 191/354: batchLoss = 1.0338, diffLoss = 5.0702, kgLoss = 0.0247
2025-04-08 17:40:34.239429: Training Step 192/354: batchLoss = 0.4961, diffLoss = 2.4322, kgLoss = 0.0120
2025-04-08 17:40:35.851828: Training Step 193/354: batchLoss = 0.5505, diffLoss = 2.6962, kgLoss = 0.0140
2025-04-08 17:40:37.467992: Training Step 194/354: batchLoss = 0.5494, diffLoss = 2.6874, kgLoss = 0.0149
2025-04-08 17:40:39.091007: Training Step 195/354: batchLoss = 0.4999, diffLoss = 2.4399, kgLoss = 0.0148
2025-04-08 17:40:40.712583: Training Step 196/354: batchLoss = 0.5410, diffLoss = 2.6504, kgLoss = 0.0136
2025-04-08 17:40:42.332846: Training Step 197/354: batchLoss = 0.5495, diffLoss = 2.6872, kgLoss = 0.0151
2025-04-08 17:40:43.957556: Training Step 198/354: batchLoss = 0.5934, diffLoss = 2.8987, kgLoss = 0.0171
2025-04-08 17:40:45.579333: Training Step 199/354: batchLoss = 0.5518, diffLoss = 2.6991, kgLoss = 0.0150
2025-04-08 17:40:47.193629: Training Step 200/354: batchLoss = 0.6498, diffLoss = 3.1837, kgLoss = 0.0163
2025-04-08 17:40:48.809677: Training Step 201/354: batchLoss = 0.5629, diffLoss = 2.7590, kgLoss = 0.0138
2025-04-08 17:40:50.424230: Training Step 202/354: batchLoss = 0.5312, diffLoss = 2.5874, kgLoss = 0.0172
2025-04-08 17:40:52.042683: Training Step 203/354: batchLoss = 0.7159, diffLoss = 3.5064, kgLoss = 0.0183
2025-04-08 17:40:53.658301: Training Step 204/354: batchLoss = 0.8257, diffLoss = 4.0531, kgLoss = 0.0188
2025-04-08 17:40:55.274326: Training Step 205/354: batchLoss = 0.5334, diffLoss = 2.6171, kgLoss = 0.0124
2025-04-08 17:40:56.890999: Training Step 206/354: batchLoss = 0.6626, diffLoss = 3.2429, kgLoss = 0.0176
2025-04-08 17:40:58.509370: Training Step 207/354: batchLoss = 0.5330, diffLoss = 2.6041, kgLoss = 0.0152
2025-04-08 17:41:00.129920: Training Step 208/354: batchLoss = 0.7573, diffLoss = 3.7068, kgLoss = 0.0199
2025-04-08 17:41:01.745805: Training Step 209/354: batchLoss = 0.4409, diffLoss = 2.1574, kgLoss = 0.0118
2025-04-08 17:41:03.364451: Training Step 210/354: batchLoss = 0.5610, diffLoss = 2.7479, kgLoss = 0.0143
2025-04-08 17:41:04.980089: Training Step 211/354: batchLoss = 0.5669, diffLoss = 2.7740, kgLoss = 0.0151
2025-04-08 17:41:06.608716: Training Step 212/354: batchLoss = 0.5864, diffLoss = 2.8715, kgLoss = 0.0152
2025-04-08 17:41:08.227217: Training Step 213/354: batchLoss = 0.5774, diffLoss = 2.8324, kgLoss = 0.0136
2025-04-08 17:41:09.836826: Training Step 214/354: batchLoss = 0.6571, diffLoss = 3.2142, kgLoss = 0.0178
2025-04-08 17:41:11.457072: Training Step 215/354: batchLoss = 0.6466, diffLoss = 3.1669, kgLoss = 0.0165
2025-04-08 17:41:13.087837: Training Step 216/354: batchLoss = 0.4757, diffLoss = 2.3301, kgLoss = 0.0120
2025-04-08 17:41:14.703671: Training Step 217/354: batchLoss = 0.6484, diffLoss = 3.1772, kgLoss = 0.0162
2025-04-08 17:41:16.326474: Training Step 218/354: batchLoss = 0.5582, diffLoss = 2.7351, kgLoss = 0.0140
2025-04-08 17:41:17.949414: Training Step 219/354: batchLoss = 0.6669, diffLoss = 3.2511, kgLoss = 0.0208
2025-04-08 17:41:19.573085: Training Step 220/354: batchLoss = 0.4605, diffLoss = 2.2532, kgLoss = 0.0123
2025-04-08 17:41:21.183820: Training Step 221/354: batchLoss = 0.5741, diffLoss = 2.8084, kgLoss = 0.0156
2025-04-08 17:41:22.801734: Training Step 222/354: batchLoss = 0.4750, diffLoss = 2.3176, kgLoss = 0.0144
2025-04-08 17:41:24.418451: Training Step 223/354: batchLoss = 0.5144, diffLoss = 2.5202, kgLoss = 0.0129
2025-04-08 17:41:26.036087: Training Step 224/354: batchLoss = 0.5135, diffLoss = 2.5111, kgLoss = 0.0141
2025-04-08 17:41:27.654463: Training Step 225/354: batchLoss = 0.5066, diffLoss = 2.4665, kgLoss = 0.0166
2025-04-08 17:41:29.278474: Training Step 226/354: batchLoss = 0.5898, diffLoss = 2.8897, kgLoss = 0.0148
2025-04-08 17:41:30.896086: Training Step 227/354: batchLoss = 0.7101, diffLoss = 3.4725, kgLoss = 0.0195
2025-04-08 17:41:32.522672: Training Step 228/354: batchLoss = 0.5465, diffLoss = 2.6746, kgLoss = 0.0144
2025-04-08 17:41:34.141470: Training Step 229/354: batchLoss = 0.5979, diffLoss = 2.9252, kgLoss = 0.0161
2025-04-08 17:41:35.765829: Training Step 230/354: batchLoss = 0.6705, diffLoss = 3.2891, kgLoss = 0.0158
2025-04-08 17:41:37.384449: Training Step 231/354: batchLoss = 1.7645, diffLoss = 8.6462, kgLoss = 0.0441
2025-04-08 17:41:38.997183: Training Step 232/354: batchLoss = 0.5217, diffLoss = 2.5606, kgLoss = 0.0119
2025-04-08 17:41:40.610555: Training Step 233/354: batchLoss = 0.4457, diffLoss = 2.1819, kgLoss = 0.0116
2025-04-08 17:41:42.233135: Training Step 234/354: batchLoss = 0.5374, diffLoss = 2.6293, kgLoss = 0.0145
2025-04-08 17:41:43.851952: Training Step 235/354: batchLoss = 0.5066, diffLoss = 2.4784, kgLoss = 0.0137
2025-04-08 17:41:45.466464: Training Step 236/354: batchLoss = 0.6864, diffLoss = 3.3528, kgLoss = 0.0198
2025-04-08 17:41:47.090542: Training Step 237/354: batchLoss = 0.5615, diffLoss = 2.7575, kgLoss = 0.0125
2025-04-08 17:41:48.713082: Training Step 238/354: batchLoss = 0.4819, diffLoss = 2.3564, kgLoss = 0.0133
2025-04-08 17:41:50.331064: Training Step 239/354: batchLoss = 0.5916, diffLoss = 2.9024, kgLoss = 0.0139
2025-04-08 17:41:51.950926: Training Step 240/354: batchLoss = 0.7644, diffLoss = 3.7515, kgLoss = 0.0176
2025-04-08 17:41:53.567419: Training Step 241/354: batchLoss = 0.5556, diffLoss = 2.7180, kgLoss = 0.0150
2025-04-08 17:41:55.195753: Training Step 242/354: batchLoss = 0.5982, diffLoss = 2.9313, kgLoss = 0.0149
2025-04-08 17:41:56.814699: Training Step 243/354: batchLoss = 0.5498, diffLoss = 2.6859, kgLoss = 0.0157
2025-04-08 17:41:58.427320: Training Step 244/354: batchLoss = 0.5021, diffLoss = 2.4612, kgLoss = 0.0124
2025-04-08 17:42:00.047080: Training Step 245/354: batchLoss = 0.4319, diffLoss = 2.1079, kgLoss = 0.0129
2025-04-08 17:42:01.681356: Training Step 246/354: batchLoss = 0.4650, diffLoss = 2.2760, kgLoss = 0.0123
2025-04-08 17:42:03.308215: Training Step 247/354: batchLoss = 0.5488, diffLoss = 2.6764, kgLoss = 0.0169
2025-04-08 17:42:04.924910: Training Step 248/354: batchLoss = 0.4163, diffLoss = 2.0366, kgLoss = 0.0112
2025-04-08 17:42:06.541416: Training Step 249/354: batchLoss = 0.5239, diffLoss = 2.5687, kgLoss = 0.0128
2025-04-08 17:42:08.160484: Training Step 250/354: batchLoss = 0.6256, diffLoss = 3.0650, kgLoss = 0.0157
2025-04-08 17:42:09.781097: Training Step 251/354: batchLoss = 0.6450, diffLoss = 3.1566, kgLoss = 0.0171
2025-04-08 17:42:11.397310: Training Step 252/354: batchLoss = 0.6280, diffLoss = 3.0720, kgLoss = 0.0170
2025-04-08 17:42:13.011940: Training Step 253/354: batchLoss = 0.4582, diffLoss = 2.2382, kgLoss = 0.0132
2025-04-08 17:42:14.631943: Training Step 254/354: batchLoss = 0.5984, diffLoss = 2.9292, kgLoss = 0.0158
2025-04-08 17:42:16.257376: Training Step 255/354: batchLoss = 0.4869, diffLoss = 2.3745, kgLoss = 0.0150
2025-04-08 17:42:17.879987: Training Step 256/354: batchLoss = 0.6405, diffLoss = 3.1438, kgLoss = 0.0146
2025-04-08 17:42:19.499987: Training Step 257/354: batchLoss = 0.5033, diffLoss = 2.4650, kgLoss = 0.0128
2025-04-08 17:42:21.120641: Training Step 258/354: batchLoss = 0.4509, diffLoss = 2.2065, kgLoss = 0.0120
2025-04-08 17:42:22.742641: Training Step 259/354: batchLoss = 0.5459, diffLoss = 2.6758, kgLoss = 0.0134
2025-04-08 17:42:24.359509: Training Step 260/354: batchLoss = 0.4705, diffLoss = 2.3057, kgLoss = 0.0117
2025-04-08 17:42:25.976131: Training Step 261/354: batchLoss = 0.5559, diffLoss = 2.7155, kgLoss = 0.0160
2025-04-08 17:42:27.593293: Training Step 262/354: batchLoss = 0.4761, diffLoss = 2.3240, kgLoss = 0.0141
2025-04-08 17:42:29.210453: Training Step 263/354: batchLoss = 0.7715, diffLoss = 3.7820, kgLoss = 0.0189
2025-04-08 17:42:30.834815: Training Step 264/354: batchLoss = 0.6572, diffLoss = 3.2174, kgLoss = 0.0172
2025-04-08 17:42:32.456809: Training Step 265/354: batchLoss = 0.5032, diffLoss = 2.4569, kgLoss = 0.0148
2025-04-08 17:42:34.080125: Training Step 266/354: batchLoss = 0.7018, diffLoss = 3.4356, kgLoss = 0.0184
2025-04-08 17:42:35.694824: Training Step 267/354: batchLoss = 0.6586, diffLoss = 3.2270, kgLoss = 0.0164
2025-04-08 17:42:37.317953: Training Step 268/354: batchLoss = 0.5310, diffLoss = 2.5980, kgLoss = 0.0143
2025-04-08 17:42:38.944888: Training Step 269/354: batchLoss = 0.6374, diffLoss = 3.1219, kgLoss = 0.0162
2025-04-08 17:42:40.563855: Training Step 270/354: batchLoss = 0.5781, diffLoss = 2.8293, kgLoss = 0.0153
2025-04-08 17:42:42.180608: Training Step 271/354: batchLoss = 0.4538, diffLoss = 2.2192, kgLoss = 0.0125
2025-04-08 17:42:43.794544: Training Step 272/354: batchLoss = 0.4445, diffLoss = 2.1754, kgLoss = 0.0118
2025-04-08 17:42:45.410930: Training Step 273/354: batchLoss = 0.5712, diffLoss = 2.7962, kgLoss = 0.0149
2025-04-08 17:42:47.032562: Training Step 274/354: batchLoss = 0.4801, diffLoss = 2.3485, kgLoss = 0.0131
2025-04-08 17:42:48.654972: Training Step 275/354: batchLoss = 0.5727, diffLoss = 2.8103, kgLoss = 0.0132
2025-04-08 17:42:50.279201: Training Step 276/354: batchLoss = 0.5126, diffLoss = 2.5104, kgLoss = 0.0132
2025-04-08 17:42:51.903089: Training Step 277/354: batchLoss = 0.7484, diffLoss = 3.6650, kgLoss = 0.0192
2025-04-08 17:42:53.526633: Training Step 278/354: batchLoss = 0.5473, diffLoss = 2.6811, kgLoss = 0.0138
2025-04-08 17:42:55.147753: Training Step 279/354: batchLoss = 0.4723, diffLoss = 2.3157, kgLoss = 0.0114
2025-04-08 17:42:56.774611: Training Step 280/354: batchLoss = 0.5160, diffLoss = 2.5242, kgLoss = 0.0140
2025-04-08 17:42:58.392322: Training Step 281/354: batchLoss = 0.6595, diffLoss = 3.2267, kgLoss = 0.0177
2025-04-08 17:43:00.006287: Training Step 282/354: batchLoss = 0.5070, diffLoss = 2.4845, kgLoss = 0.0126
2025-04-08 17:43:01.622176: Training Step 283/354: batchLoss = 0.6315, diffLoss = 3.1005, kgLoss = 0.0142
2025-04-08 17:43:03.241858: Training Step 284/354: batchLoss = 0.5872, diffLoss = 2.8759, kgLoss = 0.0150
2025-04-08 17:43:04.869342: Training Step 285/354: batchLoss = 0.5753, diffLoss = 2.8178, kgLoss = 0.0147
2025-04-08 17:43:06.488278: Training Step 286/354: batchLoss = 0.4735, diffLoss = 2.3175, kgLoss = 0.0125
2025-04-08 17:43:08.109695: Training Step 287/354: batchLoss = 0.6049, diffLoss = 2.9642, kgLoss = 0.0150
2025-04-08 17:43:09.740084: Training Step 288/354: batchLoss = 0.6162, diffLoss = 3.0208, kgLoss = 0.0151
2025-04-08 17:43:11.363800: Training Step 289/354: batchLoss = 0.4838, diffLoss = 2.3738, kgLoss = 0.0113
2025-04-08 17:43:12.977269: Training Step 290/354: batchLoss = 0.5793, diffLoss = 2.8395, kgLoss = 0.0142
2025-04-08 17:43:14.587475: Training Step 291/354: batchLoss = 0.5287, diffLoss = 2.5838, kgLoss = 0.0150
2025-04-08 17:43:16.203230: Training Step 292/354: batchLoss = 0.5159, diffLoss = 2.5245, kgLoss = 0.0137
2025-04-08 17:43:17.820793: Training Step 293/354: batchLoss = 0.4752, diffLoss = 2.3281, kgLoss = 0.0120
2025-04-08 17:43:19.439062: Training Step 294/354: batchLoss = 0.5402, diffLoss = 2.6464, kgLoss = 0.0137
2025-04-08 17:43:21.063846: Training Step 295/354: batchLoss = 0.5167, diffLoss = 2.5264, kgLoss = 0.0143
2025-04-08 17:43:22.684666: Training Step 296/354: batchLoss = 0.8222, diffLoss = 4.0331, kgLoss = 0.0195
2025-04-08 17:43:24.307189: Training Step 297/354: batchLoss = 0.5625, diffLoss = 2.7551, kgLoss = 0.0144
2025-04-08 17:43:25.932817: Training Step 298/354: batchLoss = 0.6121, diffLoss = 2.9942, kgLoss = 0.0165
2025-04-08 17:43:27.551329: Training Step 299/354: batchLoss = 0.6509, diffLoss = 3.1894, kgLoss = 0.0163
2025-04-08 17:43:29.164583: Training Step 300/354: batchLoss = 0.6222, diffLoss = 3.0427, kgLoss = 0.0171
2025-04-08 17:43:30.775323: Training Step 301/354: batchLoss = 0.4375, diffLoss = 2.1397, kgLoss = 0.0119
2025-04-08 17:43:32.392173: Training Step 302/354: batchLoss = 0.5394, diffLoss = 2.6408, kgLoss = 0.0141
2025-04-08 17:43:34.010179: Training Step 303/354: batchLoss = 0.6058, diffLoss = 2.9620, kgLoss = 0.0168
2025-04-08 17:43:35.631992: Training Step 304/354: batchLoss = 0.5396, diffLoss = 2.6441, kgLoss = 0.0135
2025-04-08 17:43:37.248169: Training Step 305/354: batchLoss = 0.4670, diffLoss = 2.2862, kgLoss = 0.0122
2025-04-08 17:43:38.866774: Training Step 306/354: batchLoss = 0.5180, diffLoss = 2.5323, kgLoss = 0.0145
2025-04-08 17:43:40.485527: Training Step 307/354: batchLoss = 0.4970, diffLoss = 2.4311, kgLoss = 0.0135
2025-04-08 17:43:42.111385: Training Step 308/354: batchLoss = 0.5742, diffLoss = 2.8039, kgLoss = 0.0168
2025-04-08 17:43:43.729011: Training Step 309/354: batchLoss = 0.5030, diffLoss = 2.4632, kgLoss = 0.0130
2025-04-08 17:43:45.347195: Training Step 310/354: batchLoss = 0.4548, diffLoss = 2.2243, kgLoss = 0.0124
2025-04-08 17:43:46.962115: Training Step 311/354: batchLoss = 0.5647, diffLoss = 2.7644, kgLoss = 0.0148
2025-04-08 17:43:48.588161: Training Step 312/354: batchLoss = 0.5179, diffLoss = 2.5378, kgLoss = 0.0130
2025-04-08 17:43:50.206736: Training Step 313/354: batchLoss = 0.4684, diffLoss = 2.2908, kgLoss = 0.0128
2025-04-08 17:43:51.828760: Training Step 314/354: batchLoss = 0.5183, diffLoss = 2.5362, kgLoss = 0.0138
2025-04-08 17:43:53.450035: Training Step 315/354: batchLoss = 0.8940, diffLoss = 4.3831, kgLoss = 0.0218
2025-04-08 17:43:55.075958: Training Step 316/354: batchLoss = 0.6101, diffLoss = 2.9877, kgLoss = 0.0157
2025-04-08 17:43:56.701123: Training Step 317/354: batchLoss = 0.4883, diffLoss = 2.3923, kgLoss = 0.0122
2025-04-08 17:43:58.323408: Training Step 318/354: batchLoss = 0.5417, diffLoss = 2.6532, kgLoss = 0.0139
2025-04-08 17:43:59.951157: Training Step 319/354: batchLoss = 0.6585, diffLoss = 3.2276, kgLoss = 0.0162
2025-04-08 17:44:01.570752: Training Step 320/354: batchLoss = 0.4785, diffLoss = 2.3435, kgLoss = 0.0123
2025-04-08 17:44:03.185625: Training Step 321/354: batchLoss = 0.5932, diffLoss = 2.8977, kgLoss = 0.0171
2025-04-08 17:44:04.799401: Training Step 322/354: batchLoss = 0.5358, diffLoss = 2.6217, kgLoss = 0.0143
2025-04-08 17:44:06.419788: Training Step 323/354: batchLoss = 0.4697, diffLoss = 2.3032, kgLoss = 0.0113
2025-04-08 17:44:08.042047: Training Step 324/354: batchLoss = 0.6374, diffLoss = 3.1221, kgLoss = 0.0162
2025-04-08 17:44:09.668724: Training Step 325/354: batchLoss = 0.5482, diffLoss = 2.6831, kgLoss = 0.0144
2025-04-08 17:44:11.285012: Training Step 326/354: batchLoss = 0.6616, diffLoss = 3.2402, kgLoss = 0.0169
2025-04-08 17:44:12.910040: Training Step 327/354: batchLoss = 0.6037, diffLoss = 2.9560, kgLoss = 0.0156
2025-04-08 17:44:14.538353: Training Step 328/354: batchLoss = 0.5791, diffLoss = 2.8431, kgLoss = 0.0132
2025-04-08 17:44:16.152812: Training Step 329/354: batchLoss = 0.5637, diffLoss = 2.7584, kgLoss = 0.0151
2025-04-08 17:44:17.770528: Training Step 330/354: batchLoss = 0.5736, diffLoss = 2.8056, kgLoss = 0.0155
2025-04-08 17:44:19.386222: Training Step 331/354: batchLoss = 0.6319, diffLoss = 3.0892, kgLoss = 0.0176
2025-04-08 17:44:21.004289: Training Step 332/354: batchLoss = 0.5051, diffLoss = 2.4694, kgLoss = 0.0140
2025-04-08 17:44:22.624436: Training Step 333/354: batchLoss = 0.6012, diffLoss = 2.9412, kgLoss = 0.0162
2025-04-08 17:44:24.249336: Training Step 334/354: batchLoss = 0.4667, diffLoss = 2.2811, kgLoss = 0.0131
2025-04-08 17:44:25.868138: Training Step 335/354: batchLoss = 0.6126, diffLoss = 3.0071, kgLoss = 0.0140
2025-04-08 17:44:27.490639: Training Step 336/354: batchLoss = 0.5905, diffLoss = 2.8959, kgLoss = 0.0142
2025-04-08 17:44:29.111007: Training Step 337/354: batchLoss = 0.6829, diffLoss = 3.3471, kgLoss = 0.0168
2025-04-08 17:44:30.732547: Training Step 338/354: batchLoss = 0.4682, diffLoss = 2.2934, kgLoss = 0.0118
2025-04-08 17:44:32.353732: Training Step 339/354: batchLoss = 0.5813, diffLoss = 2.8417, kgLoss = 0.0162
2025-04-08 17:44:33.975754: Training Step 340/354: batchLoss = 0.5739, diffLoss = 2.8116, kgLoss = 0.0145
2025-04-08 17:44:35.592061: Training Step 341/354: batchLoss = 0.4898, diffLoss = 2.3945, kgLoss = 0.0136
2025-04-08 17:44:37.208297: Training Step 342/354: batchLoss = 0.4629, diffLoss = 2.2542, kgLoss = 0.0151
2025-04-08 17:44:38.827439: Training Step 343/354: batchLoss = 0.6091, diffLoss = 2.9820, kgLoss = 0.0159
2025-04-08 17:44:40.447152: Training Step 344/354: batchLoss = 0.5516, diffLoss = 2.7020, kgLoss = 0.0140
2025-04-08 17:44:42.068227: Training Step 345/354: batchLoss = 0.6189, diffLoss = 3.0326, kgLoss = 0.0155
2025-04-08 17:44:43.683445: Training Step 346/354: batchLoss = 0.5075, diffLoss = 2.4861, kgLoss = 0.0129
2025-04-08 17:44:45.302739: Training Step 347/354: batchLoss = 0.6120, diffLoss = 2.9992, kgLoss = 0.0152
2025-04-08 17:44:46.924735: Training Step 348/354: batchLoss = 0.6515, diffLoss = 3.1879, kgLoss = 0.0173
2025-04-08 17:44:48.543820: Training Step 349/354: batchLoss = 0.4190, diffLoss = 2.0438, kgLoss = 0.0128
2025-04-08 17:44:50.159495: Training Step 350/354: batchLoss = 0.5275, diffLoss = 2.5856, kgLoss = 0.0130
2025-04-08 17:44:51.773139: Training Step 351/354: batchLoss = 0.5753, diffLoss = 2.8134, kgLoss = 0.0158
2025-04-08 17:44:53.364631: Training Step 352/354: batchLoss = 0.6212, diffLoss = 3.0447, kgLoss = 0.0153
2025-04-08 17:44:54.759414: Training Step 353/354: batchLoss = 0.5384, diffLoss = 2.6403, kgLoss = 0.0130
2025-04-08 17:44:54.850153: 
2025-04-08 17:44:54.850799: Epoch 24/1000, Train: epLoss = 1.0025, epDfLoss = 4.9079, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 17:44:56.191638: Steps 0/138: batch_recall = 47.08, batch_ndcg = 26.78 
2025-04-08 17:44:57.514437: Steps 1/138: batch_recall = 49.61, batch_ndcg = 28.70 
2025-04-08 17:44:58.820249: Steps 2/138: batch_recall = 59.00, batch_ndcg = 35.86 
2025-04-08 17:45:00.154383: Steps 3/138: batch_recall = 58.96, batch_ndcg = 34.25 
2025-04-08 17:45:01.456693: Steps 4/138: batch_recall = 67.40, batch_ndcg = 40.91 
2025-04-08 17:45:02.774185: Steps 5/138: batch_recall = 58.72, batch_ndcg = 32.11 
2025-04-08 17:45:04.078292: Steps 6/138: batch_recall = 53.57, batch_ndcg = 32.35 
2025-04-08 17:45:05.382801: Steps 7/138: batch_recall = 63.16, batch_ndcg = 41.63 
2025-04-08 17:45:06.682048: Steps 8/138: batch_recall = 61.30, batch_ndcg = 39.23 
2025-04-08 17:45:07.977550: Steps 9/138: batch_recall = 58.89, batch_ndcg = 33.85 
2025-04-08 17:45:09.275127: Steps 10/138: batch_recall = 56.21, batch_ndcg = 32.06 
2025-04-08 17:45:10.568007: Steps 11/138: batch_recall = 56.47, batch_ndcg = 32.40 
2025-04-08 17:45:11.873793: Steps 12/138: batch_recall = 52.26, batch_ndcg = 28.82 
2025-04-08 17:45:13.179507: Steps 13/138: batch_recall = 53.84, batch_ndcg = 31.63 
2025-04-08 17:45:14.477503: Steps 14/138: batch_recall = 53.72, batch_ndcg = 31.36 
2025-04-08 17:45:15.774651: Steps 15/138: batch_recall = 48.21, batch_ndcg = 29.94 
2025-04-08 17:45:17.084117: Steps 16/138: batch_recall = 58.63, batch_ndcg = 34.04 
2025-04-08 17:45:18.371726: Steps 17/138: batch_recall = 58.82, batch_ndcg = 34.21 
2025-04-08 17:45:19.675669: Steps 18/138: batch_recall = 52.33, batch_ndcg = 32.36 
2025-04-08 17:45:20.978966: Steps 19/138: batch_recall = 55.59, batch_ndcg = 33.47 
2025-04-08 17:45:22.273604: Steps 20/138: batch_recall = 62.95, batch_ndcg = 36.50 
2025-04-08 17:45:23.570817: Steps 21/138: batch_recall = 69.79, batch_ndcg = 41.25 
2025-04-08 17:45:24.856720: Steps 22/138: batch_recall = 57.95, batch_ndcg = 33.28 
2025-04-08 17:45:26.151404: Steps 23/138: batch_recall = 51.97, batch_ndcg = 30.23 
2025-04-08 17:45:27.460481: Steps 24/138: batch_recall = 57.13, batch_ndcg = 31.64 
2025-04-08 17:45:28.756935: Steps 25/138: batch_recall = 59.71, batch_ndcg = 35.38 
2025-04-08 17:45:30.063970: Steps 26/138: batch_recall = 58.28, batch_ndcg = 33.49 
2025-04-08 17:45:31.364561: Steps 27/138: batch_recall = 56.38, batch_ndcg = 32.35 
2025-04-08 17:45:32.656001: Steps 28/138: batch_recall = 58.70, batch_ndcg = 33.05 
2025-04-08 17:45:33.949051: Steps 29/138: batch_recall = 62.81, batch_ndcg = 32.48 
2025-04-08 17:45:35.249044: Steps 30/138: batch_recall = 58.23, batch_ndcg = 34.17 
2025-04-08 17:45:36.534709: Steps 31/138: batch_recall = 45.98, batch_ndcg = 26.23 
2025-04-08 17:45:37.815790: Steps 32/138: batch_recall = 54.32, batch_ndcg = 31.83 
2025-04-08 17:45:39.104570: Steps 33/138: batch_recall = 64.51, batch_ndcg = 34.74 
2025-04-08 17:45:40.384967: Steps 34/138: batch_recall = 56.05, batch_ndcg = 29.91 
2025-04-08 17:45:41.677865: Steps 35/138: batch_recall = 51.49, batch_ndcg = 29.24 
2025-04-08 17:45:42.970094: Steps 36/138: batch_recall = 47.67, batch_ndcg = 27.21 
2025-04-08 17:45:44.258004: Steps 37/138: batch_recall = 59.57, batch_ndcg = 34.60 
2025-04-08 17:45:45.552118: Steps 38/138: batch_recall = 61.21, batch_ndcg = 33.39 
2025-04-08 17:45:46.840771: Steps 39/138: batch_recall = 67.61, batch_ndcg = 38.95 
2025-04-08 17:45:48.133481: Steps 40/138: batch_recall = 58.41, batch_ndcg = 30.35 
2025-04-08 17:45:49.419467: Steps 41/138: batch_recall = 60.61, batch_ndcg = 34.03 
2025-04-08 17:45:50.706365: Steps 42/138: batch_recall = 54.58, batch_ndcg = 30.31 
2025-04-08 17:45:51.996523: Steps 43/138: batch_recall = 55.94, batch_ndcg = 35.54 
2025-04-08 17:45:53.272023: Steps 44/138: batch_recall = 56.71, batch_ndcg = 31.38 
2025-04-08 17:45:54.555442: Steps 45/138: batch_recall = 61.96, batch_ndcg = 35.13 
2025-04-08 17:45:55.837058: Steps 46/138: batch_recall = 62.19, batch_ndcg = 35.42 
2025-04-08 17:45:57.114715: Steps 47/138: batch_recall = 52.81, batch_ndcg = 31.85 
2025-04-08 17:45:58.396865: Steps 48/138: batch_recall = 62.25, batch_ndcg = 35.61 
2025-04-08 17:45:59.695218: Steps 49/138: batch_recall = 67.30, batch_ndcg = 37.74 
2025-04-08 17:46:00.981644: Steps 50/138: batch_recall = 60.50, batch_ndcg = 30.97 
2025-04-08 17:46:02.274770: Steps 51/138: batch_recall = 63.06, batch_ndcg = 35.91 
2025-04-08 17:46:03.560214: Steps 52/138: batch_recall = 65.30, batch_ndcg = 42.40 
2025-04-08 17:46:04.849388: Steps 53/138: batch_recall = 66.70, batch_ndcg = 34.76 
2025-04-08 17:46:06.138560: Steps 54/138: batch_recall = 67.29, batch_ndcg = 38.58 
2025-04-08 17:46:07.425762: Steps 55/138: batch_recall = 62.63, batch_ndcg = 34.09 
2025-04-08 17:46:08.710566: Steps 56/138: batch_recall = 63.31, batch_ndcg = 36.61 
2025-04-08 17:46:09.994555: Steps 57/138: batch_recall = 60.44, batch_ndcg = 33.55 
2025-04-08 17:46:11.275751: Steps 58/138: batch_recall = 71.19, batch_ndcg = 37.78 
2025-04-08 17:46:12.554191: Steps 59/138: batch_recall = 66.93, batch_ndcg = 39.80 
2025-04-08 17:46:13.841577: Steps 60/138: batch_recall = 66.91, batch_ndcg = 37.36 
2025-04-08 17:46:15.121392: Steps 61/138: batch_recall = 63.30, batch_ndcg = 35.20 
2025-04-08 17:46:16.410309: Steps 62/138: batch_recall = 84.88, batch_ndcg = 44.08 
2025-04-08 17:46:17.694246: Steps 63/138: batch_recall = 74.47, batch_ndcg = 42.50 
2025-04-08 17:46:19.002524: Steps 64/138: batch_recall = 61.00, batch_ndcg = 32.63 
2025-04-08 17:46:20.286255: Steps 65/138: batch_recall = 88.41, batch_ndcg = 47.88 
2025-04-08 17:46:21.577805: Steps 66/138: batch_recall = 68.65, batch_ndcg = 40.98 
2025-04-08 17:46:22.870068: Steps 67/138: batch_recall = 76.83, batch_ndcg = 46.50 
2025-04-08 17:46:24.156680: Steps 68/138: batch_recall = 61.31, batch_ndcg = 33.94 
2025-04-08 17:46:25.445735: Steps 69/138: batch_recall = 85.63, batch_ndcg = 51.33 
2025-04-08 17:46:26.725669: Steps 70/138: batch_recall = 77.88, batch_ndcg = 44.73 
2025-04-08 17:46:28.008204: Steps 71/138: batch_recall = 88.74, batch_ndcg = 51.41 
2025-04-08 17:46:29.290175: Steps 72/138: batch_recall = 86.84, batch_ndcg = 49.91 
2025-04-08 17:46:30.560553: Steps 73/138: batch_recall = 84.21, batch_ndcg = 47.36 
2025-04-08 17:46:31.845954: Steps 74/138: batch_recall = 77.74, batch_ndcg = 49.30 
2025-04-08 17:46:33.132006: Steps 75/138: batch_recall = 82.41, batch_ndcg = 48.84 
2025-04-08 17:46:34.417283: Steps 76/138: batch_recall = 95.26, batch_ndcg = 55.08 
2025-04-08 17:46:35.699104: Steps 77/138: batch_recall = 88.57, batch_ndcg = 50.42 
2025-04-08 17:46:36.985152: Steps 78/138: batch_recall = 91.01, batch_ndcg = 49.00 
2025-04-08 17:46:38.275462: Steps 79/138: batch_recall = 88.93, batch_ndcg = 49.39 
2025-04-08 17:46:39.561083: Steps 80/138: batch_recall = 70.25, batch_ndcg = 38.28 
2025-04-08 17:46:40.842481: Steps 81/138: batch_recall = 80.54, batch_ndcg = 47.37 
2025-04-08 17:46:42.120795: Steps 82/138: batch_recall = 90.84, batch_ndcg = 53.85 
2025-04-08 17:46:43.395833: Steps 83/138: batch_recall = 82.69, batch_ndcg = 48.05 
2025-04-08 17:46:44.666853: Steps 84/138: batch_recall = 98.31, batch_ndcg = 57.19 
2025-04-08 17:46:45.947801: Steps 85/138: batch_recall = 104.71, batch_ndcg = 61.22 
2025-04-08 17:46:47.226924: Steps 86/138: batch_recall = 119.64, batch_ndcg = 71.03 
2025-04-08 17:46:48.522652: Steps 87/138: batch_recall = 101.76, batch_ndcg = 55.02 
2025-04-08 17:46:49.811086: Steps 88/138: batch_recall = 101.82, batch_ndcg = 59.16 
2025-04-08 17:46:51.099184: Steps 89/138: batch_recall = 119.16, batch_ndcg = 68.97 
2025-04-08 17:46:52.383907: Steps 90/138: batch_recall = 104.34, batch_ndcg = 58.12 
2025-04-08 17:46:53.664464: Steps 91/138: batch_recall = 117.20, batch_ndcg = 65.01 
2025-04-08 17:46:54.951542: Steps 92/138: batch_recall = 120.81, batch_ndcg = 64.95 
2025-04-08 17:46:56.210466: Steps 93/138: batch_recall = 121.54, batch_ndcg = 68.89 
2025-04-08 17:46:57.487611: Steps 94/138: batch_recall = 125.94, batch_ndcg = 66.38 
2025-04-08 17:46:58.756470: Steps 95/138: batch_recall = 112.74, batch_ndcg = 66.10 
2025-04-08 17:47:00.027654: Steps 96/138: batch_recall = 135.77, batch_ndcg = 80.38 
2025-04-08 17:47:01.300736: Steps 97/138: batch_recall = 147.44, batch_ndcg = 90.02 
2025-04-08 17:47:02.571738: Steps 98/138: batch_recall = 108.22, batch_ndcg = 63.85 
2025-04-08 17:47:03.848310: Steps 99/138: batch_recall = 126.89, batch_ndcg = 72.70 
2025-04-08 17:47:05.129286: Steps 100/138: batch_recall = 132.79, batch_ndcg = 74.17 
2025-04-08 17:47:06.406157: Steps 101/138: batch_recall = 126.81, batch_ndcg = 69.71 
2025-04-08 17:47:07.682653: Steps 102/138: batch_recall = 124.34, batch_ndcg = 72.58 
2025-04-08 17:47:08.959196: Steps 103/138: batch_recall = 141.02, batch_ndcg = 80.08 
2025-04-08 17:47:10.237991: Steps 104/138: batch_recall = 134.18, batch_ndcg = 78.64 
2025-04-08 17:47:11.517198: Steps 105/138: batch_recall = 119.17, batch_ndcg = 66.36 
2025-04-08 17:47:12.796606: Steps 106/138: batch_recall = 104.76, batch_ndcg = 59.91 
2025-04-08 17:47:14.064238: Steps 107/138: batch_recall = 115.68, batch_ndcg = 64.82 
2025-04-08 17:47:15.333691: Steps 108/138: batch_recall = 116.64, batch_ndcg = 70.01 
2025-04-08 17:47:16.605142: Steps 109/138: batch_recall = 136.65, batch_ndcg = 76.57 
2025-04-08 17:47:17.879811: Steps 110/138: batch_recall = 121.21, batch_ndcg = 64.11 
2025-04-08 17:47:19.155225: Steps 111/138: batch_recall = 137.25, batch_ndcg = 83.44 
2025-04-08 17:47:20.444174: Steps 112/138: batch_recall = 152.72, batch_ndcg = 88.60 
2025-04-08 17:47:21.717076: Steps 113/138: batch_recall = 126.49, batch_ndcg = 70.31 
2025-04-08 17:47:22.996664: Steps 114/138: batch_recall = 124.62, batch_ndcg = 71.72 
2025-04-08 17:47:24.275718: Steps 115/138: batch_recall = 119.56, batch_ndcg = 63.30 
2025-04-08 17:47:25.553031: Steps 116/138: batch_recall = 125.79, batch_ndcg = 67.03 
2025-04-08 17:47:26.826463: Steps 117/138: batch_recall = 114.06, batch_ndcg = 66.10 
2025-04-08 17:47:28.104789: Steps 118/138: batch_recall = 129.08, batch_ndcg = 71.07 
2025-04-08 17:47:29.375216: Steps 119/138: batch_recall = 139.66, batch_ndcg = 75.84 
2025-04-08 17:47:30.641908: Steps 120/138: batch_recall = 127.24, batch_ndcg = 70.95 
2025-04-08 17:47:31.909882: Steps 121/138: batch_recall = 144.85, batch_ndcg = 77.57 
2025-04-08 17:47:33.180907: Steps 122/138: batch_recall = 145.99, batch_ndcg = 81.65 
2025-04-08 17:47:34.448804: Steps 123/138: batch_recall = 129.55, batch_ndcg = 73.40 
2025-04-08 17:47:35.723481: Steps 124/138: batch_recall = 153.66, batch_ndcg = 93.04 
2025-04-08 17:47:36.994444: Steps 125/138: batch_recall = 133.13, batch_ndcg = 72.84 
2025-04-08 17:47:38.270286: Steps 126/138: batch_recall = 158.10, batch_ndcg = 87.87 
2025-04-08 17:47:39.543238: Steps 127/138: batch_recall = 145.42, batch_ndcg = 82.60 
2025-04-08 17:47:40.817256: Steps 128/138: batch_recall = 125.64, batch_ndcg = 72.04 
2025-04-08 17:47:42.090202: Steps 129/138: batch_recall = 162.52, batch_ndcg = 92.40 
2025-04-08 17:47:43.365000: Steps 130/138: batch_recall = 132.27, batch_ndcg = 69.70 
2025-04-08 17:47:44.632901: Steps 131/138: batch_recall = 155.38, batch_ndcg = 87.53 
2025-04-08 17:47:45.902502: Steps 132/138: batch_recall = 150.33, batch_ndcg = 84.61 
2025-04-08 17:47:47.174971: Steps 133/138: batch_recall = 149.93, batch_ndcg = 85.63 
2025-04-08 17:47:48.440002: Steps 134/138: batch_recall = 146.63, batch_ndcg = 83.57 
2025-04-08 17:47:49.719172: Steps 135/138: batch_recall = 172.36, batch_ndcg = 96.69 
2025-04-08 17:47:50.987933: Steps 136/138: batch_recall = 153.48, batch_ndcg = 78.38 
2025-04-08 17:47:52.269187: Steps 137/138: batch_recall = 139.62, batch_ndcg = 86.88 
2025-04-08 17:47:52.269719: Epoch 24/1000, Test: Recall = 0.1768, NDCG = 0.1006  

2025-04-08 17:47:54.038722: Training Step 0/354: batchLoss = 0.5431, diffLoss = 2.6593, kgLoss = 0.0141
2025-04-08 17:47:55.662901: Training Step 1/354: batchLoss = 0.6067, diffLoss = 2.9648, kgLoss = 0.0172
2025-04-08 17:47:57.282175: Training Step 2/354: batchLoss = 0.4868, diffLoss = 2.3919, kgLoss = 0.0105
2025-04-08 17:47:58.906777: Training Step 3/354: batchLoss = 0.6356, diffLoss = 3.1100, kgLoss = 0.0170
2025-04-08 17:48:00.528269: Training Step 4/354: batchLoss = 0.5923, diffLoss = 2.9033, kgLoss = 0.0146
2025-04-08 17:48:02.146449: Training Step 5/354: batchLoss = 0.5997, diffLoss = 2.9371, kgLoss = 0.0153
2025-04-08 17:48:03.761618: Training Step 6/354: batchLoss = 0.4740, diffLoss = 2.3168, kgLoss = 0.0133
2025-04-08 17:48:05.370284: Training Step 7/354: batchLoss = 0.6116, diffLoss = 2.9964, kgLoss = 0.0154
2025-04-08 17:48:06.986300: Training Step 8/354: batchLoss = 0.5524, diffLoss = 2.7026, kgLoss = 0.0149
2025-04-08 17:48:08.611491: Training Step 9/354: batchLoss = 0.5456, diffLoss = 2.6541, kgLoss = 0.0185
2025-04-08 17:48:10.234245: Training Step 10/354: batchLoss = 0.4805, diffLoss = 2.3558, kgLoss = 0.0117
2025-04-08 17:48:11.850116: Training Step 11/354: batchLoss = 0.4488, diffLoss = 2.1929, kgLoss = 0.0128
2025-04-08 17:48:13.472894: Training Step 12/354: batchLoss = 0.5230, diffLoss = 2.5523, kgLoss = 0.0157
2025-04-08 17:48:15.092955: Training Step 13/354: batchLoss = 0.6082, diffLoss = 2.9769, kgLoss = 0.0161
2025-04-08 17:48:16.711866: Training Step 14/354: batchLoss = 0.5567, diffLoss = 2.7225, kgLoss = 0.0153
2025-04-08 17:48:18.323593: Training Step 15/354: batchLoss = 0.4932, diffLoss = 2.4164, kgLoss = 0.0124
2025-04-08 17:48:19.937452: Training Step 16/354: batchLoss = 0.6670, diffLoss = 3.2656, kgLoss = 0.0173
2025-04-08 17:48:21.549689: Training Step 17/354: batchLoss = 0.5611, diffLoss = 2.7479, kgLoss = 0.0143
2025-04-08 17:48:23.169400: Training Step 18/354: batchLoss = 0.5627, diffLoss = 2.7592, kgLoss = 0.0135
2025-04-08 17:48:24.799069: Training Step 19/354: batchLoss = 0.4500, diffLoss = 2.2069, kgLoss = 0.0108
2025-04-08 17:48:26.424630: Training Step 20/354: batchLoss = 0.6496, diffLoss = 3.1701, kgLoss = 0.0195
2025-04-08 17:48:28.044387: Training Step 21/354: batchLoss = 0.4643, diffLoss = 2.2739, kgLoss = 0.0119
2025-04-08 17:48:29.666088: Training Step 22/354: batchLoss = 0.7787, diffLoss = 3.8185, kgLoss = 0.0187
2025-04-08 17:48:31.296327: Training Step 23/354: batchLoss = 0.5200, diffLoss = 2.5370, kgLoss = 0.0158
2025-04-08 17:48:32.921789: Training Step 24/354: batchLoss = 0.5051, diffLoss = 2.4684, kgLoss = 0.0143
2025-04-08 17:48:34.538428: Training Step 25/354: batchLoss = 0.5019, diffLoss = 2.4612, kgLoss = 0.0121
2025-04-08 17:48:36.156242: Training Step 26/354: batchLoss = 0.5099, diffLoss = 2.4923, kgLoss = 0.0143
2025-04-08 17:48:37.775875: Training Step 27/354: batchLoss = 0.5142, diffLoss = 2.5149, kgLoss = 0.0140
2025-04-08 17:48:39.394144: Training Step 28/354: batchLoss = 0.5771, diffLoss = 2.8222, kgLoss = 0.0158
2025-04-08 17:48:41.016336: Training Step 29/354: batchLoss = 0.5336, diffLoss = 2.6152, kgLoss = 0.0133
2025-04-08 17:48:42.642714: Training Step 30/354: batchLoss = 0.5665, diffLoss = 2.7732, kgLoss = 0.0149
2025-04-08 17:48:44.258643: Training Step 31/354: batchLoss = 0.4862, diffLoss = 2.3820, kgLoss = 0.0123
2025-04-08 17:48:45.879207: Training Step 32/354: batchLoss = 0.4451, diffLoss = 2.1792, kgLoss = 0.0116
2025-04-08 17:48:47.497324: Training Step 33/354: batchLoss = 0.5594, diffLoss = 2.7460, kgLoss = 0.0128
2025-04-08 17:48:49.110711: Training Step 34/354: batchLoss = 0.4641, diffLoss = 2.2688, kgLoss = 0.0129
2025-04-08 17:48:50.726878: Training Step 35/354: batchLoss = 0.6781, diffLoss = 3.3183, kgLoss = 0.0180
2025-04-08 17:48:52.341490: Training Step 36/354: batchLoss = 0.4893, diffLoss = 2.3909, kgLoss = 0.0139
2025-04-08 17:48:53.959895: Training Step 37/354: batchLoss = 0.6809, diffLoss = 3.3337, kgLoss = 0.0177
2025-04-08 17:48:55.582144: Training Step 38/354: batchLoss = 0.5811, diffLoss = 2.8500, kgLoss = 0.0139
2025-04-08 17:48:57.204013: Training Step 39/354: batchLoss = 0.4737, diffLoss = 2.3164, kgLoss = 0.0131
2025-04-08 17:48:58.822872: Training Step 40/354: batchLoss = 0.5762, diffLoss = 2.8139, kgLoss = 0.0168
2025-04-08 17:49:00.445334: Training Step 41/354: batchLoss = 0.5448, diffLoss = 2.6616, kgLoss = 0.0155
2025-04-08 17:49:02.066498: Training Step 42/354: batchLoss = 0.4918, diffLoss = 2.4091, kgLoss = 0.0124
2025-04-08 17:49:03.687344: Training Step 43/354: batchLoss = 0.5136, diffLoss = 2.5168, kgLoss = 0.0128
2025-04-08 17:49:05.303270: Training Step 44/354: batchLoss = 0.5163, diffLoss = 2.5326, kgLoss = 0.0122
2025-04-08 17:49:06.921383: Training Step 45/354: batchLoss = 0.5681, diffLoss = 2.7806, kgLoss = 0.0150
2025-04-08 17:49:08.539621: Training Step 46/354: batchLoss = 0.5458, diffLoss = 2.6734, kgLoss = 0.0139
2025-04-08 17:49:10.152998: Training Step 47/354: batchLoss = 0.4785, diffLoss = 2.3439, kgLoss = 0.0122
2025-04-08 17:49:11.777096: Training Step 48/354: batchLoss = 0.5886, diffLoss = 2.8783, kgLoss = 0.0161
2025-04-08 17:49:13.398123: Training Step 49/354: batchLoss = 0.6145, diffLoss = 3.0069, kgLoss = 0.0164
2025-04-08 17:49:15.019541: Training Step 50/354: batchLoss = 0.6177, diffLoss = 3.0255, kgLoss = 0.0157
2025-04-08 17:49:16.637209: Training Step 51/354: batchLoss = 0.5131, diffLoss = 2.5157, kgLoss = 0.0124
2025-04-08 17:49:18.255152: Training Step 52/354: batchLoss = 0.6950, diffLoss = 3.4000, kgLoss = 0.0188
2025-04-08 17:49:19.875215: Training Step 53/354: batchLoss = 0.4615, diffLoss = 2.2546, kgLoss = 0.0133
2025-04-08 17:49:21.491033: Training Step 54/354: batchLoss = 0.5195, diffLoss = 2.5437, kgLoss = 0.0135
2025-04-08 17:49:23.106059: Training Step 55/354: batchLoss = 0.5797, diffLoss = 2.8419, kgLoss = 0.0141
2025-04-08 17:49:24.722600: Training Step 56/354: batchLoss = 0.6123, diffLoss = 3.0056, kgLoss = 0.0140
2025-04-08 17:49:26.338626: Training Step 57/354: batchLoss = 0.4613, diffLoss = 2.2622, kgLoss = 0.0111
2025-04-08 17:49:27.953886: Training Step 58/354: batchLoss = 0.7223, diffLoss = 3.5406, kgLoss = 0.0178
2025-04-08 17:49:29.570681: Training Step 59/354: batchLoss = 0.5402, diffLoss = 2.6411, kgLoss = 0.0149
2025-04-08 17:49:31.196316: Training Step 60/354: batchLoss = 0.4698, diffLoss = 2.2994, kgLoss = 0.0123
2025-04-08 17:49:32.817209: Training Step 61/354: batchLoss = 0.5599, diffLoss = 2.7441, kgLoss = 0.0138
2025-04-08 17:49:34.441036: Training Step 62/354: batchLoss = 0.5488, diffLoss = 2.6806, kgLoss = 0.0158
2025-04-08 17:49:36.062086: Training Step 63/354: batchLoss = 0.5992, diffLoss = 2.9290, kgLoss = 0.0167
2025-04-08 17:49:37.679109: Training Step 64/354: batchLoss = 0.6276, diffLoss = 3.0636, kgLoss = 0.0186
2025-04-08 17:49:39.298658: Training Step 65/354: batchLoss = 0.6071, diffLoss = 2.9691, kgLoss = 0.0166
2025-04-08 17:49:40.910264: Training Step 66/354: batchLoss = 0.6176, diffLoss = 3.0265, kgLoss = 0.0154
2025-04-08 17:49:42.527605: Training Step 67/354: batchLoss = 0.4772, diffLoss = 2.3307, kgLoss = 0.0139
2025-04-08 17:49:44.143435: Training Step 68/354: batchLoss = 0.5337, diffLoss = 2.6157, kgLoss = 0.0133
2025-04-08 17:49:45.761349: Training Step 69/354: batchLoss = 0.5150, diffLoss = 2.5199, kgLoss = 0.0138
2025-04-08 17:49:47.375021: Training Step 70/354: batchLoss = 0.5870, diffLoss = 2.8748, kgLoss = 0.0151
2025-04-08 17:49:48.997097: Training Step 71/354: batchLoss = 0.4863, diffLoss = 2.3755, kgLoss = 0.0140
2025-04-08 17:49:50.614977: Training Step 72/354: batchLoss = 0.5255, diffLoss = 2.5701, kgLoss = 0.0143
2025-04-08 17:49:52.236150: Training Step 73/354: batchLoss = 0.5067, diffLoss = 2.4835, kgLoss = 0.0125
2025-04-08 17:49:53.847776: Training Step 74/354: batchLoss = 0.5604, diffLoss = 2.7398, kgLoss = 0.0156
2025-04-08 17:49:55.460943: Training Step 75/354: batchLoss = 0.5452, diffLoss = 2.6748, kgLoss = 0.0128
2025-04-08 17:49:57.068897: Training Step 76/354: batchLoss = 0.5784, diffLoss = 2.8275, kgLoss = 0.0161
2025-04-08 17:49:58.686031: Training Step 77/354: batchLoss = 0.5399, diffLoss = 2.6469, kgLoss = 0.0132
2025-04-08 17:50:00.312165: Training Step 78/354: batchLoss = 0.4445, diffLoss = 2.1759, kgLoss = 0.0116
2025-04-08 17:50:01.932397: Training Step 79/354: batchLoss = 0.6232, diffLoss = 3.0555, kgLoss = 0.0151
2025-04-08 17:50:03.552772: Training Step 80/354: batchLoss = 0.5647, diffLoss = 2.7678, kgLoss = 0.0139
2025-04-08 17:50:05.172231: Training Step 81/354: batchLoss = 0.5341, diffLoss = 2.6110, kgLoss = 0.0149
2025-04-08 17:50:06.792552: Training Step 82/354: batchLoss = 0.6219, diffLoss = 3.0466, kgLoss = 0.0157
2025-04-08 17:50:08.416861: Training Step 83/354: batchLoss = 0.6032, diffLoss = 2.9513, kgLoss = 0.0161
2025-04-08 17:50:10.040533: Training Step 84/354: batchLoss = 0.5532, diffLoss = 2.7091, kgLoss = 0.0143
2025-04-08 17:50:11.656451: Training Step 85/354: batchLoss = 0.5867, diffLoss = 2.8713, kgLoss = 0.0155
2025-04-08 17:50:13.275475: Training Step 86/354: batchLoss = 0.5366, diffLoss = 2.6263, kgLoss = 0.0141
2025-04-08 17:50:14.896480: Training Step 87/354: batchLoss = 0.5626, diffLoss = 2.7609, kgLoss = 0.0130
2025-04-08 17:50:16.522117: Training Step 88/354: batchLoss = 0.5281, diffLoss = 2.5880, kgLoss = 0.0131
2025-04-08 17:50:18.142741: Training Step 89/354: batchLoss = 0.5698, diffLoss = 2.7910, kgLoss = 0.0145
2025-04-08 17:50:19.763602: Training Step 90/354: batchLoss = 0.5617, diffLoss = 2.7545, kgLoss = 0.0135
2025-04-08 17:50:21.380446: Training Step 91/354: batchLoss = 0.5971, diffLoss = 2.9232, kgLoss = 0.0156
2025-04-08 17:50:23.004423: Training Step 92/354: batchLoss = 0.5230, diffLoss = 2.5533, kgLoss = 0.0155
2025-04-08 17:50:24.621131: Training Step 93/354: batchLoss = 0.4556, diffLoss = 2.2293, kgLoss = 0.0122
2025-04-08 17:50:26.239770: Training Step 94/354: batchLoss = 0.4797, diffLoss = 2.3472, kgLoss = 0.0129
2025-04-08 17:50:27.858164: Training Step 95/354: batchLoss = 0.4895, diffLoss = 2.3944, kgLoss = 0.0133
2025-04-08 17:50:29.474164: Training Step 96/354: batchLoss = 0.4691, diffLoss = 2.2916, kgLoss = 0.0135
2025-04-08 17:50:31.088929: Training Step 97/354: batchLoss = 0.4974, diffLoss = 2.4361, kgLoss = 0.0127
2025-04-08 17:50:32.702024: Training Step 98/354: batchLoss = 0.5917, diffLoss = 2.8956, kgLoss = 0.0158
2025-04-08 17:50:34.320821: Training Step 99/354: batchLoss = 0.5034, diffLoss = 2.4697, kgLoss = 0.0118
2025-04-08 17:50:35.946180: Training Step 100/354: batchLoss = 0.5111, diffLoss = 2.4936, kgLoss = 0.0155
2025-04-08 17:50:37.571168: Training Step 101/354: batchLoss = 0.5455, diffLoss = 2.6715, kgLoss = 0.0140
2025-04-08 17:50:39.194021: Training Step 102/354: batchLoss = 0.5648, diffLoss = 2.7732, kgLoss = 0.0128
2025-04-08 17:50:40.810738: Training Step 103/354: batchLoss = 0.4855, diffLoss = 2.3719, kgLoss = 0.0139
2025-04-08 17:50:42.427682: Training Step 104/354: batchLoss = 0.5012, diffLoss = 2.4550, kgLoss = 0.0127
2025-04-08 17:50:44.044205: Training Step 105/354: batchLoss = 0.6390, diffLoss = 3.1284, kgLoss = 0.0166
2025-04-08 17:50:45.659601: Training Step 106/354: batchLoss = 0.6256, diffLoss = 3.0636, kgLoss = 0.0161
2025-04-08 17:50:47.279516: Training Step 107/354: batchLoss = 0.5817, diffLoss = 2.8548, kgLoss = 0.0134
2025-04-08 17:50:48.900869: Training Step 108/354: batchLoss = 0.4982, diffLoss = 2.4399, kgLoss = 0.0128
2025-04-08 17:50:50.523396: Training Step 109/354: batchLoss = 0.5303, diffLoss = 2.5959, kgLoss = 0.0139
2025-04-08 17:50:52.149048: Training Step 110/354: batchLoss = 0.5475, diffLoss = 2.6842, kgLoss = 0.0133
2025-04-08 17:50:53.770914: Training Step 111/354: batchLoss = 0.5074, diffLoss = 2.4746, kgLoss = 0.0157
2025-04-08 17:50:55.391068: Training Step 112/354: batchLoss = 0.5393, diffLoss = 2.6459, kgLoss = 0.0126
2025-04-08 17:50:57.002468: Training Step 113/354: batchLoss = 0.4873, diffLoss = 2.3808, kgLoss = 0.0139
2025-04-08 17:50:58.618371: Training Step 114/354: batchLoss = 0.5479, diffLoss = 2.6851, kgLoss = 0.0136
2025-04-08 17:51:00.225164: Training Step 115/354: batchLoss = 0.6101, diffLoss = 2.9909, kgLoss = 0.0149
2025-04-08 17:51:01.840400: Training Step 116/354: batchLoss = 0.4982, diffLoss = 2.4370, kgLoss = 0.0135
2025-04-08 17:51:03.461190: Training Step 117/354: batchLoss = 0.4560, diffLoss = 2.2348, kgLoss = 0.0113
2025-04-08 17:51:05.075313: Training Step 118/354: batchLoss = 0.5659, diffLoss = 2.7699, kgLoss = 0.0149
2025-04-08 17:51:06.687147: Training Step 119/354: batchLoss = 0.6450, diffLoss = 3.1631, kgLoss = 0.0155
2025-04-08 17:51:08.307062: Training Step 120/354: batchLoss = 0.5957, diffLoss = 2.9197, kgLoss = 0.0146
2025-04-08 17:51:09.922830: Training Step 121/354: batchLoss = 0.4871, diffLoss = 2.3811, kgLoss = 0.0136
2025-04-08 17:51:11.545328: Training Step 122/354: batchLoss = 0.5854, diffLoss = 2.8660, kgLoss = 0.0153
2025-04-08 17:51:13.159953: Training Step 123/354: batchLoss = 0.5469, diffLoss = 2.6764, kgLoss = 0.0145
2025-04-08 17:51:14.774997: Training Step 124/354: batchLoss = 0.4930, diffLoss = 2.4064, kgLoss = 0.0147
2025-04-08 17:51:16.394711: Training Step 125/354: batchLoss = 0.4950, diffLoss = 2.4232, kgLoss = 0.0130
2025-04-08 17:51:17.999178: Training Step 126/354: batchLoss = 0.5836, diffLoss = 2.8554, kgLoss = 0.0156
2025-04-08 17:51:19.620152: Training Step 127/354: batchLoss = 0.5875, diffLoss = 2.8708, kgLoss = 0.0167
2025-04-08 17:51:21.241766: Training Step 128/354: batchLoss = 0.5328, diffLoss = 2.6022, kgLoss = 0.0154
2025-04-08 17:51:22.870490: Training Step 129/354: batchLoss = 0.5457, diffLoss = 2.6664, kgLoss = 0.0156
2025-04-08 17:51:24.501872: Training Step 130/354: batchLoss = 0.5306, diffLoss = 2.5974, kgLoss = 0.0139
2025-04-08 17:51:26.122663: Training Step 131/354: batchLoss = 0.5598, diffLoss = 2.7392, kgLoss = 0.0150
2025-04-08 17:51:27.744770: Training Step 132/354: batchLoss = 0.7200, diffLoss = 3.5211, kgLoss = 0.0197
2025-04-08 17:51:29.358611: Training Step 133/354: batchLoss = 0.5908, diffLoss = 2.8919, kgLoss = 0.0156
2025-04-08 17:51:30.976579: Training Step 134/354: batchLoss = 0.8595, diffLoss = 4.2038, kgLoss = 0.0234
2025-04-08 17:51:32.589332: Training Step 135/354: batchLoss = 0.6541, diffLoss = 3.2028, kgLoss = 0.0169
2025-04-08 17:51:34.205958: Training Step 136/354: batchLoss = 0.5587, diffLoss = 2.7367, kgLoss = 0.0142
2025-04-08 17:51:35.825656: Training Step 137/354: batchLoss = 0.5008, diffLoss = 2.4540, kgLoss = 0.0125
2025-04-08 17:51:37.434599: Training Step 138/354: batchLoss = 0.6068, diffLoss = 2.9696, kgLoss = 0.0161
2025-04-08 17:51:39.052107: Training Step 139/354: batchLoss = 0.6111, diffLoss = 2.9952, kgLoss = 0.0151
2025-04-08 17:51:40.672896: Training Step 140/354: batchLoss = 0.4796, diffLoss = 2.3347, kgLoss = 0.0159
2025-04-08 17:51:42.296766: Training Step 141/354: batchLoss = 0.6162, diffLoss = 3.0195, kgLoss = 0.0154
2025-04-08 17:51:43.918381: Training Step 142/354: batchLoss = 0.5054, diffLoss = 2.4606, kgLoss = 0.0166
2025-04-08 17:51:45.537157: Training Step 143/354: batchLoss = 0.5520, diffLoss = 2.7015, kgLoss = 0.0147
2025-04-08 17:51:47.153127: Training Step 144/354: batchLoss = 0.6045, diffLoss = 2.9595, kgLoss = 0.0157
2025-04-08 17:51:48.769202: Training Step 145/354: batchLoss = 0.5251, diffLoss = 2.5694, kgLoss = 0.0140
2025-04-08 17:51:50.384402: Training Step 146/354: batchLoss = 0.6203, diffLoss = 3.0397, kgLoss = 0.0154
2025-04-08 17:51:52.009142: Training Step 147/354: batchLoss = 0.5195, diffLoss = 2.5460, kgLoss = 0.0128
2025-04-08 17:51:53.629379: Training Step 148/354: batchLoss = 0.5777, diffLoss = 2.8262, kgLoss = 0.0155
2025-04-08 17:51:55.254889: Training Step 149/354: batchLoss = 0.4715, diffLoss = 2.3040, kgLoss = 0.0134
2025-04-08 17:51:56.877723: Training Step 150/354: batchLoss = 0.4499, diffLoss = 2.2006, kgLoss = 0.0122
2025-04-08 17:51:58.501543: Training Step 151/354: batchLoss = 0.5388, diffLoss = 2.6352, kgLoss = 0.0148
2025-04-08 17:52:00.119809: Training Step 152/354: batchLoss = 0.4811, diffLoss = 2.3554, kgLoss = 0.0126
2025-04-08 17:52:01.742145: Training Step 153/354: batchLoss = 0.5711, diffLoss = 2.7880, kgLoss = 0.0169
2025-04-08 17:52:03.354248: Training Step 154/354: batchLoss = 0.5679, diffLoss = 2.7803, kgLoss = 0.0148
2025-04-08 17:52:04.963492: Training Step 155/354: batchLoss = 0.5594, diffLoss = 2.7443, kgLoss = 0.0132
2025-04-08 17:52:06.583120: Training Step 156/354: batchLoss = 0.6154, diffLoss = 3.0132, kgLoss = 0.0159
2025-04-08 17:52:08.205366: Training Step 157/354: batchLoss = 0.5497, diffLoss = 2.6918, kgLoss = 0.0142
2025-04-08 17:52:09.823088: Training Step 158/354: batchLoss = 0.5134, diffLoss = 2.5057, kgLoss = 0.0153
2025-04-08 17:52:11.440349: Training Step 159/354: batchLoss = 0.5428, diffLoss = 2.6619, kgLoss = 0.0130
2025-04-08 17:52:13.056249: Training Step 160/354: batchLoss = 0.5683, diffLoss = 2.7854, kgLoss = 0.0140
2025-04-08 17:52:14.674443: Training Step 161/354: batchLoss = 0.4986, diffLoss = 2.4390, kgLoss = 0.0135
2025-04-08 17:52:16.289187: Training Step 162/354: batchLoss = 0.7022, diffLoss = 3.4388, kgLoss = 0.0180
2025-04-08 17:52:17.905201: Training Step 163/354: batchLoss = 0.5111, diffLoss = 2.5002, kgLoss = 0.0139
2025-04-08 17:52:19.533698: Training Step 164/354: batchLoss = 0.4818, diffLoss = 2.3607, kgLoss = 0.0121
2025-04-08 17:52:21.154405: Training Step 165/354: batchLoss = 0.5634, diffLoss = 2.7572, kgLoss = 0.0150
2025-04-08 17:52:22.771274: Training Step 166/354: batchLoss = 0.6579, diffLoss = 3.2215, kgLoss = 0.0169
2025-04-08 17:52:24.396143: Training Step 167/354: batchLoss = 0.4697, diffLoss = 2.2963, kgLoss = 0.0130
2025-04-08 17:52:26.017646: Training Step 168/354: batchLoss = 0.6911, diffLoss = 3.3895, kgLoss = 0.0165
2025-04-08 17:52:27.642072: Training Step 169/354: batchLoss = 0.6016, diffLoss = 2.9452, kgLoss = 0.0157
2025-04-08 17:52:29.268938: Training Step 170/354: batchLoss = 0.5208, diffLoss = 2.5527, kgLoss = 0.0129
2025-04-08 17:52:30.887708: Training Step 171/354: batchLoss = 0.4670, diffLoss = 2.2829, kgLoss = 0.0131
2025-04-08 17:52:32.504738: Training Step 172/354: batchLoss = 0.5525, diffLoss = 2.7022, kgLoss = 0.0151
2025-04-08 17:52:34.121305: Training Step 173/354: batchLoss = 0.5011, diffLoss = 2.4516, kgLoss = 0.0135
2025-04-08 17:52:35.735157: Training Step 174/354: batchLoss = 0.4533, diffLoss = 2.2171, kgLoss = 0.0124
2025-04-08 17:52:37.352120: Training Step 175/354: batchLoss = 0.6088, diffLoss = 2.9870, kgLoss = 0.0143
2025-04-08 17:52:38.969310: Training Step 176/354: batchLoss = 0.6195, diffLoss = 3.0369, kgLoss = 0.0151
2025-04-08 17:52:40.589405: Training Step 177/354: batchLoss = 0.6410, diffLoss = 3.1446, kgLoss = 0.0151
2025-04-08 17:52:42.215943: Training Step 178/354: batchLoss = 0.6098, diffLoss = 2.9867, kgLoss = 0.0156
2025-04-08 17:52:43.831870: Training Step 179/354: batchLoss = 0.6665, diffLoss = 3.2661, kgLoss = 0.0166
2025-04-08 17:52:45.456131: Training Step 180/354: batchLoss = 0.5052, diffLoss = 2.4732, kgLoss = 0.0132
2025-04-08 17:52:47.076919: Training Step 181/354: batchLoss = 0.4603, diffLoss = 2.2508, kgLoss = 0.0127
2025-04-08 17:52:48.695379: Training Step 182/354: batchLoss = 0.6457, diffLoss = 3.1639, kgLoss = 0.0161
2025-04-08 17:52:50.309975: Training Step 183/354: batchLoss = 0.6251, diffLoss = 3.0657, kgLoss = 0.0149
2025-04-08 17:52:51.925067: Training Step 184/354: batchLoss = 0.7434, diffLoss = 3.6401, kgLoss = 0.0193
2025-04-08 17:52:53.533301: Training Step 185/354: batchLoss = 0.5548, diffLoss = 2.7142, kgLoss = 0.0149
2025-04-08 17:52:55.150897: Training Step 186/354: batchLoss = 0.5543, diffLoss = 2.7106, kgLoss = 0.0152
2025-04-08 17:52:56.767205: Training Step 187/354: batchLoss = 0.5724, diffLoss = 2.7966, kgLoss = 0.0164
2025-04-08 17:52:58.380840: Training Step 188/354: batchLoss = 0.5756, diffLoss = 2.8192, kgLoss = 0.0147
2025-04-08 17:52:59.999427: Training Step 189/354: batchLoss = 0.5509, diffLoss = 2.6970, kgLoss = 0.0144
2025-04-08 17:53:01.615423: Training Step 190/354: batchLoss = 0.5871, diffLoss = 2.8714, kgLoss = 0.0161
2025-04-08 17:53:03.238685: Training Step 191/354: batchLoss = 0.6394, diffLoss = 3.1331, kgLoss = 0.0160
2025-04-08 17:53:04.851288: Training Step 192/354: batchLoss = 0.4913, diffLoss = 2.4033, kgLoss = 0.0132
2025-04-08 17:53:06.463733: Training Step 193/354: batchLoss = 0.4280, diffLoss = 2.0984, kgLoss = 0.0104
2025-04-08 17:53:08.075578: Training Step 194/354: batchLoss = 0.5257, diffLoss = 2.5775, kgLoss = 0.0128
2025-04-08 17:53:09.684084: Training Step 195/354: batchLoss = 0.4970, diffLoss = 2.4261, kgLoss = 0.0147
2025-04-08 17:53:11.311836: Training Step 196/354: batchLoss = 0.6345, diffLoss = 3.1023, kgLoss = 0.0176
2025-04-08 17:53:12.923861: Training Step 197/354: batchLoss = 0.5182, diffLoss = 2.5341, kgLoss = 0.0142
2025-04-08 17:53:14.542790: Training Step 198/354: batchLoss = 0.6037, diffLoss = 2.9534, kgLoss = 0.0163
2025-04-08 17:53:16.169383: Training Step 199/354: batchLoss = 0.4683, diffLoss = 2.2891, kgLoss = 0.0131
2025-04-08 17:53:17.787675: Training Step 200/354: batchLoss = 0.4446, diffLoss = 2.1743, kgLoss = 0.0122
2025-04-08 17:53:19.398312: Training Step 201/354: batchLoss = 0.5628, diffLoss = 2.7538, kgLoss = 0.0150
2025-04-08 17:53:21.011215: Training Step 202/354: batchLoss = 0.4778, diffLoss = 2.3409, kgLoss = 0.0120
2025-04-08 17:53:22.625248: Training Step 203/354: batchLoss = 0.6198, diffLoss = 3.0245, kgLoss = 0.0186
2025-04-08 17:53:24.242903: Training Step 204/354: batchLoss = 0.6106, diffLoss = 2.9924, kgLoss = 0.0151
2025-04-08 17:53:25.863328: Training Step 205/354: batchLoss = 0.5690, diffLoss = 2.7891, kgLoss = 0.0140
2025-04-08 17:53:27.482731: Training Step 206/354: batchLoss = 0.6190, diffLoss = 3.0314, kgLoss = 0.0159
2025-04-08 17:53:29.103394: Training Step 207/354: batchLoss = 0.6394, diffLoss = 3.1265, kgLoss = 0.0176
2025-04-08 17:53:30.727822: Training Step 208/354: batchLoss = 0.5401, diffLoss = 2.6421, kgLoss = 0.0146
2025-04-08 17:53:32.351489: Training Step 209/354: batchLoss = 0.5512, diffLoss = 2.6969, kgLoss = 0.0147
2025-04-08 17:53:33.974391: Training Step 210/354: batchLoss = 0.5400, diffLoss = 2.6399, kgLoss = 0.0150
2025-04-08 17:53:35.587049: Training Step 211/354: batchLoss = 0.6351, diffLoss = 3.0987, kgLoss = 0.0192
2025-04-08 17:53:37.205386: Training Step 212/354: batchLoss = 0.5727, diffLoss = 2.8027, kgLoss = 0.0152
2025-04-08 17:53:38.820554: Training Step 213/354: batchLoss = 0.5288, diffLoss = 2.5906, kgLoss = 0.0133
2025-04-08 17:53:40.445000: Training Step 214/354: batchLoss = 0.5400, diffLoss = 2.6366, kgLoss = 0.0159
2025-04-08 17:53:42.063488: Training Step 215/354: batchLoss = 0.5603, diffLoss = 2.7366, kgLoss = 0.0162
2025-04-08 17:53:43.674306: Training Step 216/354: batchLoss = 0.7503, diffLoss = 3.6735, kgLoss = 0.0194
2025-04-08 17:53:45.294609: Training Step 217/354: batchLoss = 0.6771, diffLoss = 3.3176, kgLoss = 0.0170
2025-04-08 17:53:46.920382: Training Step 218/354: batchLoss = 0.5725, diffLoss = 2.8042, kgLoss = 0.0145
2025-04-08 17:53:48.543557: Training Step 219/354: batchLoss = 0.5515, diffLoss = 2.7001, kgLoss = 0.0144
2025-04-08 17:53:50.164723: Training Step 220/354: batchLoss = 0.6565, diffLoss = 3.1993, kgLoss = 0.0208
2025-04-08 17:53:51.784904: Training Step 221/354: batchLoss = 0.4750, diffLoss = 2.3300, kgLoss = 0.0112
2025-04-08 17:53:53.399526: Training Step 222/354: batchLoss = 0.5128, diffLoss = 2.5025, kgLoss = 0.0153
2025-04-08 17:53:55.013092: Training Step 223/354: batchLoss = 0.5727, diffLoss = 2.8070, kgLoss = 0.0141
2025-04-08 17:53:56.631116: Training Step 224/354: batchLoss = 0.5447, diffLoss = 2.6584, kgLoss = 0.0162
2025-04-08 17:53:58.250208: Training Step 225/354: batchLoss = 0.4321, diffLoss = 2.1163, kgLoss = 0.0111
2025-04-08 17:53:59.870435: Training Step 226/354: batchLoss = 0.5808, diffLoss = 2.8459, kgLoss = 0.0146
2025-04-08 17:54:01.489908: Training Step 227/354: batchLoss = 0.6112, diffLoss = 2.9926, kgLoss = 0.0158
2025-04-08 17:54:03.113317: Training Step 228/354: batchLoss = 0.5296, diffLoss = 2.4808, kgLoss = 0.0418
2025-04-08 17:54:04.730238: Training Step 229/354: batchLoss = 1.0172, diffLoss = 4.9768, kgLoss = 0.0273
2025-04-08 17:54:06.351806: Training Step 230/354: batchLoss = 0.5416, diffLoss = 2.6414, kgLoss = 0.0167
2025-04-08 17:54:07.966162: Training Step 231/354: batchLoss = 0.4962, diffLoss = 2.4317, kgLoss = 0.0123
2025-04-08 17:54:09.580417: Training Step 232/354: batchLoss = 0.4655, diffLoss = 2.2744, kgLoss = 0.0133
2025-04-08 17:54:11.196322: Training Step 233/354: batchLoss = 0.6081, diffLoss = 2.9825, kgLoss = 0.0145
2025-04-08 17:54:12.811364: Training Step 234/354: batchLoss = 0.5585, diffLoss = 2.7367, kgLoss = 0.0139
2025-04-08 17:54:14.433056: Training Step 235/354: batchLoss = 0.5226, diffLoss = 2.5575, kgLoss = 0.0139
2025-04-08 17:54:16.051328: Training Step 236/354: batchLoss = 0.6404, diffLoss = 3.1398, kgLoss = 0.0156
2025-04-08 17:54:17.674531: Training Step 237/354: batchLoss = 0.4817, diffLoss = 2.3546, kgLoss = 0.0135
2025-04-08 17:54:19.293664: Training Step 238/354: batchLoss = 0.5777, diffLoss = 2.8263, kgLoss = 0.0155
2025-04-08 17:54:20.915951: Training Step 239/354: batchLoss = 0.5903, diffLoss = 2.8912, kgLoss = 0.0150
2025-04-08 17:54:22.534864: Training Step 240/354: batchLoss = 0.6234, diffLoss = 3.0464, kgLoss = 0.0177
2025-04-08 17:54:24.151143: Training Step 241/354: batchLoss = 0.6093, diffLoss = 2.9806, kgLoss = 0.0164
2025-04-08 17:54:25.761640: Training Step 242/354: batchLoss = 0.6082, diffLoss = 2.9706, kgLoss = 0.0176
2025-04-08 17:54:27.371999: Training Step 243/354: batchLoss = 0.5195, diffLoss = 2.5460, kgLoss = 0.0129
2025-04-08 17:54:28.987263: Training Step 244/354: batchLoss = 0.6574, diffLoss = 3.2215, kgLoss = 0.0163
2025-04-08 17:54:30.605081: Training Step 245/354: batchLoss = 0.5041, diffLoss = 2.4602, kgLoss = 0.0150
2025-04-08 17:54:32.229955: Training Step 246/354: batchLoss = 0.5560, diffLoss = 2.7251, kgLoss = 0.0137
2025-04-08 17:54:33.850206: Training Step 247/354: batchLoss = 0.8057, diffLoss = 3.9420, kgLoss = 0.0216
2025-04-08 17:54:35.467452: Training Step 248/354: batchLoss = 0.5851, diffLoss = 2.8645, kgLoss = 0.0153
2025-04-08 17:54:37.083331: Training Step 249/354: batchLoss = 0.4853, diffLoss = 2.3800, kgLoss = 0.0117
2025-04-08 17:54:38.707504: Training Step 250/354: batchLoss = 0.6775, diffLoss = 3.3217, kgLoss = 0.0165
2025-04-08 17:54:40.324521: Training Step 251/354: batchLoss = 0.6030, diffLoss = 2.9526, kgLoss = 0.0157
2025-04-08 17:54:41.939520: Training Step 252/354: batchLoss = 0.4653, diffLoss = 2.2704, kgLoss = 0.0140
2025-04-08 17:54:43.555082: Training Step 253/354: batchLoss = 0.5615, diffLoss = 2.7468, kgLoss = 0.0152
2025-04-08 17:54:45.172601: Training Step 254/354: batchLoss = 0.6032, diffLoss = 2.9370, kgLoss = 0.0197
2025-04-08 17:54:46.795905: Training Step 255/354: batchLoss = 0.5138, diffLoss = 2.5180, kgLoss = 0.0128
2025-04-08 17:54:48.453647: Training Step 256/354: batchLoss = 0.5804, diffLoss = 2.8424, kgLoss = 0.0150
2025-04-08 17:54:50.076300: Training Step 257/354: batchLoss = 0.5410, diffLoss = 2.6514, kgLoss = 0.0134
2025-04-08 17:54:51.696762: Training Step 258/354: batchLoss = 0.5783, diffLoss = 2.8360, kgLoss = 0.0138
2025-04-08 17:54:53.308555: Training Step 259/354: batchLoss = 0.6594, diffLoss = 3.2308, kgLoss = 0.0166
2025-04-08 17:54:54.931399: Training Step 260/354: batchLoss = 0.6737, diffLoss = 3.3030, kgLoss = 0.0164
2025-04-08 17:54:56.545510: Training Step 261/354: batchLoss = 0.5907, diffLoss = 2.8956, kgLoss = 0.0145
2025-04-08 17:54:58.155593: Training Step 262/354: batchLoss = 0.4830, diffLoss = 2.3538, kgLoss = 0.0153
2025-04-08 17:54:59.772530: Training Step 263/354: batchLoss = 0.6274, diffLoss = 3.0779, kgLoss = 0.0148
2025-04-08 17:55:01.384645: Training Step 264/354: batchLoss = 0.6173, diffLoss = 3.0273, kgLoss = 0.0148
2025-04-08 17:55:03.005029: Training Step 265/354: batchLoss = 0.5322, diffLoss = 2.6044, kgLoss = 0.0141
2025-04-08 17:55:04.630169: Training Step 266/354: batchLoss = 0.5257, diffLoss = 2.5732, kgLoss = 0.0138
2025-04-08 17:55:06.249120: Training Step 267/354: batchLoss = 0.5221, diffLoss = 2.5531, kgLoss = 0.0143
2025-04-08 17:55:07.865800: Training Step 268/354: batchLoss = 0.6201, diffLoss = 3.0414, kgLoss = 0.0148
2025-04-08 17:55:09.487994: Training Step 269/354: batchLoss = 0.5365, diffLoss = 2.6230, kgLoss = 0.0148
2025-04-08 17:55:11.109231: Training Step 270/354: batchLoss = 0.4823, diffLoss = 2.3624, kgLoss = 0.0122
2025-04-08 17:55:12.724337: Training Step 271/354: batchLoss = 0.7952, diffLoss = 3.8899, kgLoss = 0.0216
2025-04-08 17:55:14.344303: Training Step 272/354: batchLoss = 0.5381, diffLoss = 2.6322, kgLoss = 0.0146
2025-04-08 17:55:15.955799: Training Step 273/354: batchLoss = 0.5135, diffLoss = 2.5122, kgLoss = 0.0138
2025-04-08 17:55:17.572867: Training Step 274/354: batchLoss = 0.5272, diffLoss = 2.5776, kgLoss = 0.0145
2025-04-08 17:55:19.196535: Training Step 275/354: batchLoss = 0.5352, diffLoss = 2.6174, kgLoss = 0.0146
2025-04-08 17:55:20.812085: Training Step 276/354: batchLoss = 0.4276, diffLoss = 2.0817, kgLoss = 0.0141
2025-04-08 17:55:22.428749: Training Step 277/354: batchLoss = 0.4329, diffLoss = 2.1200, kgLoss = 0.0111
2025-04-08 17:55:24.046565: Training Step 278/354: batchLoss = 0.5806, diffLoss = 2.8470, kgLoss = 0.0140
2025-04-08 17:55:25.673829: Training Step 279/354: batchLoss = 0.4965, diffLoss = 2.4313, kgLoss = 0.0128
2025-04-08 17:55:27.292494: Training Step 280/354: batchLoss = 0.5007, diffLoss = 2.4377, kgLoss = 0.0165
2025-04-08 17:55:28.908683: Training Step 281/354: batchLoss = 0.5298, diffLoss = 2.5821, kgLoss = 0.0167
2025-04-08 17:55:30.521933: Training Step 282/354: batchLoss = 0.4690, diffLoss = 2.3004, kgLoss = 0.0112
2025-04-08 17:55:32.141714: Training Step 283/354: batchLoss = 0.5627, diffLoss = 2.7483, kgLoss = 0.0163
2025-04-08 17:55:33.757867: Training Step 284/354: batchLoss = 0.4784, diffLoss = 2.3379, kgLoss = 0.0135
2025-04-08 17:55:35.384237: Training Step 285/354: batchLoss = 0.6200, diffLoss = 3.0401, kgLoss = 0.0150
2025-04-08 17:55:37.005184: Training Step 286/354: batchLoss = 0.5797, diffLoss = 2.8407, kgLoss = 0.0144
2025-04-08 17:55:38.624391: Training Step 287/354: batchLoss = 0.4909, diffLoss = 2.4047, kgLoss = 0.0125
2025-04-08 17:55:40.244964: Training Step 288/354: batchLoss = 0.5028, diffLoss = 2.4623, kgLoss = 0.0129
2025-04-08 17:55:41.868716: Training Step 289/354: batchLoss = 0.6346, diffLoss = 3.1097, kgLoss = 0.0158
2025-04-08 17:55:43.489038: Training Step 290/354: batchLoss = 0.5640, diffLoss = 2.7618, kgLoss = 0.0146
2025-04-08 17:55:45.108725: Training Step 291/354: batchLoss = 0.5183, diffLoss = 2.5359, kgLoss = 0.0139
2025-04-08 17:55:46.728582: Training Step 292/354: batchLoss = 0.5287, diffLoss = 2.5870, kgLoss = 0.0142
2025-04-08 17:55:48.346756: Training Step 293/354: batchLoss = 0.6656, diffLoss = 3.2679, kgLoss = 0.0150
2025-04-08 17:55:49.967226: Training Step 294/354: batchLoss = 0.6103, diffLoss = 2.9886, kgLoss = 0.0157
2025-04-08 17:55:51.589431: Training Step 295/354: batchLoss = 0.5859, diffLoss = 2.8658, kgLoss = 0.0159
2025-04-08 17:55:53.203526: Training Step 296/354: batchLoss = 0.5321, diffLoss = 2.6122, kgLoss = 0.0121
2025-04-08 17:55:54.824585: Training Step 297/354: batchLoss = 0.5323, diffLoss = 2.6062, kgLoss = 0.0138
2025-04-08 17:55:56.442238: Training Step 298/354: batchLoss = 0.5217, diffLoss = 2.5531, kgLoss = 0.0138
2025-04-08 17:55:58.064184: Training Step 299/354: batchLoss = 0.6361, diffLoss = 3.1122, kgLoss = 0.0171
2025-04-08 17:55:59.688926: Training Step 300/354: batchLoss = 0.5916, diffLoss = 2.8955, kgLoss = 0.0157
2025-04-08 17:56:01.304424: Training Step 301/354: batchLoss = 0.4906, diffLoss = 2.4010, kgLoss = 0.0130
2025-04-08 17:56:02.917302: Training Step 302/354: batchLoss = 0.4720, diffLoss = 2.3099, kgLoss = 0.0125
2025-04-08 17:56:04.531749: Training Step 303/354: batchLoss = 0.5707, diffLoss = 2.7921, kgLoss = 0.0154
2025-04-08 17:56:06.152993: Training Step 304/354: batchLoss = 0.5802, diffLoss = 2.8418, kgLoss = 0.0147
2025-04-08 17:56:07.769724: Training Step 305/354: batchLoss = 0.5626, diffLoss = 2.7591, kgLoss = 0.0135
2025-04-08 17:56:09.389829: Training Step 306/354: batchLoss = 0.5325, diffLoss = 2.6044, kgLoss = 0.0145
2025-04-08 17:56:11.014994: Training Step 307/354: batchLoss = 0.5282, diffLoss = 2.5815, kgLoss = 0.0149
2025-04-08 17:56:12.638290: Training Step 308/354: batchLoss = 0.6396, diffLoss = 3.1347, kgLoss = 0.0158
2025-04-08 17:56:14.256132: Training Step 309/354: batchLoss = 0.5884, diffLoss = 2.8837, kgLoss = 0.0145
2025-04-08 17:56:15.875375: Training Step 310/354: batchLoss = 0.4256, diffLoss = 2.0825, kgLoss = 0.0114
2025-04-08 17:56:17.495231: Training Step 311/354: batchLoss = 0.4847, diffLoss = 2.3703, kgLoss = 0.0133
2025-04-08 17:56:19.110957: Training Step 312/354: batchLoss = 0.5588, diffLoss = 2.7423, kgLoss = 0.0129
2025-04-08 17:56:20.728966: Training Step 313/354: batchLoss = 0.4764, diffLoss = 2.3353, kgLoss = 0.0117
2025-04-08 17:56:22.342372: Training Step 314/354: batchLoss = 0.6397, diffLoss = 3.1294, kgLoss = 0.0172
2025-04-08 17:56:23.960222: Training Step 315/354: batchLoss = 0.5758, diffLoss = 2.8189, kgLoss = 0.0150
2025-04-08 17:56:25.585848: Training Step 316/354: batchLoss = 0.6699, diffLoss = 3.2811, kgLoss = 0.0171
2025-04-08 17:56:27.201865: Training Step 317/354: batchLoss = 0.5070, diffLoss = 2.4838, kgLoss = 0.0129
2025-04-08 17:56:28.819742: Training Step 318/354: batchLoss = 0.5218, diffLoss = 2.5534, kgLoss = 0.0139
2025-04-08 17:56:30.438619: Training Step 319/354: batchLoss = 0.4693, diffLoss = 2.2984, kgLoss = 0.0120
2025-04-08 17:56:32.058663: Training Step 320/354: batchLoss = 0.5941, diffLoss = 2.9118, kgLoss = 0.0147
2025-04-08 17:56:33.670108: Training Step 321/354: batchLoss = 0.4975, diffLoss = 2.4316, kgLoss = 0.0140
2025-04-08 17:56:35.280623: Training Step 322/354: batchLoss = 0.4553, diffLoss = 2.2249, kgLoss = 0.0129
2025-04-08 17:56:36.892462: Training Step 323/354: batchLoss = 0.4974, diffLoss = 2.4365, kgLoss = 0.0126
2025-04-08 17:56:38.508672: Training Step 324/354: batchLoss = 0.4978, diffLoss = 2.4351, kgLoss = 0.0135
2025-04-08 17:56:40.125309: Training Step 325/354: batchLoss = 0.5795, diffLoss = 2.8370, kgLoss = 0.0151
2025-04-08 17:56:41.743868: Training Step 326/354: batchLoss = 0.5043, diffLoss = 2.4663, kgLoss = 0.0138
2025-04-08 17:56:43.355632: Training Step 327/354: batchLoss = 0.5941, diffLoss = 2.9047, kgLoss = 0.0164
2025-04-08 17:56:44.971839: Training Step 328/354: batchLoss = 0.5670, diffLoss = 2.7678, kgLoss = 0.0168
2025-04-08 17:56:46.589149: Training Step 329/354: batchLoss = 0.5251, diffLoss = 2.5622, kgLoss = 0.0158
2025-04-08 17:56:48.203444: Training Step 330/354: batchLoss = 0.5137, diffLoss = 2.5161, kgLoss = 0.0131
2025-04-08 17:56:49.818088: Training Step 331/354: batchLoss = 0.5585, diffLoss = 2.7377, kgLoss = 0.0137
2025-04-08 17:56:51.433982: Training Step 332/354: batchLoss = 0.5201, diffLoss = 2.5421, kgLoss = 0.0146
2025-04-08 17:56:53.048911: Training Step 333/354: batchLoss = 0.6527, diffLoss = 3.1986, kgLoss = 0.0163
2025-04-08 17:56:54.664124: Training Step 334/354: batchLoss = 0.5451, diffLoss = 2.6685, kgLoss = 0.0143
2025-04-08 17:56:56.283364: Training Step 335/354: batchLoss = 0.4622, diffLoss = 2.2626, kgLoss = 0.0121
2025-04-08 17:56:57.900024: Training Step 336/354: batchLoss = 0.5131, diffLoss = 2.5031, kgLoss = 0.0156
2025-04-08 17:56:59.517690: Training Step 337/354: batchLoss = 0.4444, diffLoss = 2.1717, kgLoss = 0.0126
2025-04-08 17:57:01.136748: Training Step 338/354: batchLoss = 0.5506, diffLoss = 2.7007, kgLoss = 0.0131
2025-04-08 17:57:02.749437: Training Step 339/354: batchLoss = 0.5103, diffLoss = 2.4970, kgLoss = 0.0136
2025-04-08 17:57:04.370353: Training Step 340/354: batchLoss = 0.5361, diffLoss = 2.6232, kgLoss = 0.0143
2025-04-08 17:57:05.989840: Training Step 341/354: batchLoss = 0.4748, diffLoss = 2.3233, kgLoss = 0.0127
2025-04-08 17:57:07.602555: Training Step 342/354: batchLoss = 0.5671, diffLoss = 2.7730, kgLoss = 0.0156
2025-04-08 17:57:09.215577: Training Step 343/354: batchLoss = 0.5866, diffLoss = 2.8716, kgLoss = 0.0153
2025-04-08 17:57:10.836438: Training Step 344/354: batchLoss = 0.5291, diffLoss = 2.5855, kgLoss = 0.0150
2025-04-08 17:57:12.455146: Training Step 345/354: batchLoss = 0.5490, diffLoss = 2.6828, kgLoss = 0.0155
2025-04-08 17:57:14.072649: Training Step 346/354: batchLoss = 0.5735, diffLoss = 2.8122, kgLoss = 0.0138
2025-04-08 17:57:15.689198: Training Step 347/354: batchLoss = 0.5670, diffLoss = 2.7792, kgLoss = 0.0140
2025-04-08 17:57:17.307473: Training Step 348/354: batchLoss = 0.6475, diffLoss = 3.1737, kgLoss = 0.0160
2025-04-08 17:57:18.924971: Training Step 349/354: batchLoss = 0.5384, diffLoss = 2.6370, kgLoss = 0.0138
2025-04-08 17:57:20.542684: Training Step 350/354: batchLoss = 0.5667, diffLoss = 2.7697, kgLoss = 0.0160
2025-04-08 17:57:22.150007: Training Step 351/354: batchLoss = 0.4738, diffLoss = 2.3185, kgLoss = 0.0126
2025-04-08 17:57:23.753296: Training Step 352/354: batchLoss = 0.5759, diffLoss = 2.8245, kgLoss = 0.0138
2025-04-08 17:57:25.155905: Training Step 353/354: batchLoss = 0.5003, diffLoss = 2.4395, kgLoss = 0.0155
2025-04-08 17:57:25.245859: 
2025-04-08 17:57:25.246500: Epoch 25/1000, Train: epLoss = 0.9844, epDfLoss = 4.8174, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 17:57:26.554107: Steps 0/138: batch_recall = 46.34, batch_ndcg = 26.14 
2025-04-08 17:57:27.881303: Steps 1/138: batch_recall = 49.17, batch_ndcg = 28.66 
2025-04-08 17:57:29.188770: Steps 2/138: batch_recall = 60.01, batch_ndcg = 35.82 
2025-04-08 17:57:30.503001: Steps 3/138: batch_recall = 57.94, batch_ndcg = 34.09 
2025-04-08 17:57:31.813162: Steps 4/138: batch_recall = 66.63, batch_ndcg = 40.74 
2025-04-08 17:57:33.125932: Steps 5/138: batch_recall = 59.97, batch_ndcg = 32.38 
2025-04-08 17:57:34.432515: Steps 6/138: batch_recall = 54.17, batch_ndcg = 32.38 
2025-04-08 17:57:35.735267: Steps 7/138: batch_recall = 61.95, batch_ndcg = 41.40 
2025-04-08 17:57:37.035985: Steps 8/138: batch_recall = 62.53, batch_ndcg = 39.25 
2025-04-08 17:57:38.326331: Steps 9/138: batch_recall = 59.15, batch_ndcg = 34.19 
2025-04-08 17:57:39.612452: Steps 10/138: batch_recall = 56.58, batch_ndcg = 32.07 
2025-04-08 17:57:40.909345: Steps 11/138: batch_recall = 56.56, batch_ndcg = 32.60 
2025-04-08 17:57:42.204875: Steps 12/138: batch_recall = 52.71, batch_ndcg = 28.80 
2025-04-08 17:57:43.495974: Steps 13/138: batch_recall = 53.89, batch_ndcg = 32.22 
2025-04-08 17:57:44.809797: Steps 14/138: batch_recall = 55.53, batch_ndcg = 31.85 
2025-04-08 17:57:46.106946: Steps 15/138: batch_recall = 48.03, batch_ndcg = 30.46 
2025-04-08 17:57:47.417343: Steps 16/138: batch_recall = 61.11, batch_ndcg = 34.32 
2025-04-08 17:57:48.703837: Steps 17/138: batch_recall = 56.99, batch_ndcg = 33.75 
2025-04-08 17:57:50.007177: Steps 18/138: batch_recall = 52.22, batch_ndcg = 32.67 
2025-04-08 17:57:51.308807: Steps 19/138: batch_recall = 54.53, batch_ndcg = 32.90 
2025-04-08 17:57:52.593797: Steps 20/138: batch_recall = 61.36, batch_ndcg = 36.21 
2025-04-08 17:57:53.886227: Steps 21/138: batch_recall = 69.61, batch_ndcg = 41.12 
2025-04-08 17:57:55.173021: Steps 22/138: batch_recall = 55.70, batch_ndcg = 32.88 
2025-04-08 17:57:56.464480: Steps 23/138: batch_recall = 50.71, batch_ndcg = 30.19 
2025-04-08 17:57:57.759366: Steps 24/138: batch_recall = 55.55, batch_ndcg = 30.90 
2025-04-08 17:57:59.057530: Steps 25/138: batch_recall = 61.17, batch_ndcg = 35.56 
2025-04-08 17:58:00.362814: Steps 26/138: batch_recall = 57.23, batch_ndcg = 33.18 
2025-04-08 17:58:01.666583: Steps 27/138: batch_recall = 57.55, batch_ndcg = 32.47 
2025-04-08 17:58:02.969095: Steps 28/138: batch_recall = 59.05, batch_ndcg = 33.25 
2025-04-08 17:58:04.262753: Steps 29/138: batch_recall = 61.73, batch_ndcg = 32.96 
2025-04-08 17:58:05.570487: Steps 30/138: batch_recall = 58.03, batch_ndcg = 34.05 
2025-04-08 17:58:06.871518: Steps 31/138: batch_recall = 48.73, batch_ndcg = 26.97 
2025-04-08 17:58:08.162872: Steps 32/138: batch_recall = 54.39, batch_ndcg = 31.63 
2025-04-08 17:58:09.438142: Steps 33/138: batch_recall = 63.01, batch_ndcg = 34.18 
2025-04-08 17:58:10.729581: Steps 34/138: batch_recall = 56.23, batch_ndcg = 30.00 
2025-04-08 17:58:12.015523: Steps 35/138: batch_recall = 50.82, batch_ndcg = 29.39 
2025-04-08 17:58:13.298080: Steps 36/138: batch_recall = 46.17, batch_ndcg = 26.79 
2025-04-08 17:58:14.583487: Steps 37/138: batch_recall = 57.62, batch_ndcg = 34.32 
2025-04-08 17:58:15.877767: Steps 38/138: batch_recall = 59.88, batch_ndcg = 32.95 
2025-04-08 17:58:17.158514: Steps 39/138: batch_recall = 65.69, batch_ndcg = 38.49 
2025-04-08 17:58:18.450839: Steps 40/138: batch_recall = 62.54, batch_ndcg = 31.27 
2025-04-08 17:58:19.739826: Steps 41/138: batch_recall = 60.16, batch_ndcg = 33.87 
2025-04-08 17:58:21.023156: Steps 42/138: batch_recall = 55.86, batch_ndcg = 30.65 
2025-04-08 17:58:22.312103: Steps 43/138: batch_recall = 57.59, batch_ndcg = 36.34 
2025-04-08 17:58:23.594361: Steps 44/138: batch_recall = 57.84, batch_ndcg = 31.39 
2025-04-08 17:58:24.875891: Steps 45/138: batch_recall = 61.24, batch_ndcg = 35.18 
2025-04-08 17:58:26.152699: Steps 46/138: batch_recall = 62.62, batch_ndcg = 35.59 
2025-04-08 17:58:27.436476: Steps 47/138: batch_recall = 53.03, batch_ndcg = 32.27 
2025-04-08 17:58:28.721215: Steps 48/138: batch_recall = 61.60, batch_ndcg = 35.14 
2025-04-08 17:58:29.999928: Steps 49/138: batch_recall = 66.20, batch_ndcg = 37.82 
2025-04-08 17:58:31.290587: Steps 50/138: batch_recall = 58.96, batch_ndcg = 30.90 
2025-04-08 17:58:32.579940: Steps 51/138: batch_recall = 60.97, batch_ndcg = 35.49 
2025-04-08 17:58:33.873129: Steps 52/138: batch_recall = 64.97, batch_ndcg = 41.66 
2025-04-08 17:58:35.170746: Steps 53/138: batch_recall = 65.61, batch_ndcg = 34.32 
2025-04-08 17:58:36.459929: Steps 54/138: batch_recall = 69.72, batch_ndcg = 39.25 
2025-04-08 17:58:37.738414: Steps 55/138: batch_recall = 63.09, batch_ndcg = 34.24 
2025-04-08 17:58:39.023184: Steps 56/138: batch_recall = 62.77, batch_ndcg = 36.31 
2025-04-08 17:58:40.310632: Steps 57/138: batch_recall = 58.87, batch_ndcg = 32.88 
2025-04-08 17:58:41.591645: Steps 58/138: batch_recall = 72.90, batch_ndcg = 37.85 
2025-04-08 17:58:42.864160: Steps 59/138: batch_recall = 65.59, batch_ndcg = 39.21 
2025-04-08 17:58:44.155054: Steps 60/138: batch_recall = 64.18, batch_ndcg = 36.85 
2025-04-08 17:58:45.438509: Steps 61/138: batch_recall = 63.38, batch_ndcg = 35.11 
2025-04-08 17:58:46.721768: Steps 62/138: batch_recall = 85.44, batch_ndcg = 44.79 
2025-04-08 17:58:48.010448: Steps 63/138: batch_recall = 74.72, batch_ndcg = 42.46 
2025-04-08 17:58:49.293340: Steps 64/138: batch_recall = 60.55, batch_ndcg = 32.81 
2025-04-08 17:58:50.574634: Steps 65/138: batch_recall = 87.25, batch_ndcg = 47.54 
2025-04-08 17:58:51.858886: Steps 66/138: batch_recall = 69.99, batch_ndcg = 41.53 
2025-04-08 17:58:53.141839: Steps 67/138: batch_recall = 75.33, batch_ndcg = 46.25 
2025-04-08 17:58:54.427844: Steps 68/138: batch_recall = 63.06, batch_ndcg = 33.99 
2025-04-08 17:58:55.701015: Steps 69/138: batch_recall = 88.28, batch_ndcg = 52.47 
2025-04-08 17:58:56.977246: Steps 70/138: batch_recall = 78.80, batch_ndcg = 44.96 
2025-04-08 17:58:58.255886: Steps 71/138: batch_recall = 87.53, batch_ndcg = 51.10 
2025-04-08 17:58:59.534205: Steps 72/138: batch_recall = 84.84, batch_ndcg = 49.67 
2025-04-08 17:59:00.817160: Steps 73/138: batch_recall = 85.13, batch_ndcg = 47.30 
2025-04-08 17:59:02.091400: Steps 74/138: batch_recall = 78.82, batch_ndcg = 49.59 
2025-04-08 17:59:03.381383: Steps 75/138: batch_recall = 83.79, batch_ndcg = 49.36 
2025-04-08 17:59:04.676433: Steps 76/138: batch_recall = 93.56, batch_ndcg = 54.93 
2025-04-08 17:59:05.964096: Steps 77/138: batch_recall = 92.19, batch_ndcg = 51.62 
2025-04-08 17:59:07.243440: Steps 78/138: batch_recall = 90.59, batch_ndcg = 48.66 
2025-04-08 17:59:08.528644: Steps 79/138: batch_recall = 91.44, batch_ndcg = 49.96 
2025-04-08 17:59:09.810214: Steps 80/138: batch_recall = 73.73, batch_ndcg = 38.90 
2025-04-08 17:59:11.097609: Steps 81/138: batch_recall = 81.87, batch_ndcg = 48.52 
2025-04-08 17:59:12.392210: Steps 82/138: batch_recall = 89.30, batch_ndcg = 53.59 
2025-04-08 17:59:13.666662: Steps 83/138: batch_recall = 81.08, batch_ndcg = 47.56 
2025-04-08 17:59:14.936180: Steps 84/138: batch_recall = 99.17, batch_ndcg = 56.86 
2025-04-08 17:59:16.203385: Steps 85/138: batch_recall = 105.28, batch_ndcg = 60.85 
2025-04-08 17:59:17.482123: Steps 86/138: batch_recall = 119.87, batch_ndcg = 71.10 
2025-04-08 17:59:18.763029: Steps 87/138: batch_recall = 105.35, batch_ndcg = 55.76 
2025-04-08 17:59:20.049703: Steps 88/138: batch_recall = 102.87, batch_ndcg = 59.40 
2025-04-08 17:59:21.324980: Steps 89/138: batch_recall = 118.16, batch_ndcg = 68.75 
2025-04-08 17:59:22.597507: Steps 90/138: batch_recall = 102.17, batch_ndcg = 57.62 
2025-04-08 17:59:23.866304: Steps 91/138: batch_recall = 116.22, batch_ndcg = 65.33 
2025-04-08 17:59:25.144135: Steps 92/138: batch_recall = 122.26, batch_ndcg = 65.54 
2025-04-08 17:59:26.414769: Steps 93/138: batch_recall = 120.20, batch_ndcg = 68.67 
2025-04-08 17:59:27.684551: Steps 94/138: batch_recall = 124.94, batch_ndcg = 66.40 
2025-04-08 17:59:28.950887: Steps 95/138: batch_recall = 116.96, batch_ndcg = 67.51 
2025-04-08 17:59:30.217851: Steps 96/138: batch_recall = 134.98, batch_ndcg = 79.89 
2025-04-08 17:59:31.496839: Steps 97/138: batch_recall = 152.64, batch_ndcg = 91.87 
2025-04-08 17:59:32.771148: Steps 98/138: batch_recall = 107.27, batch_ndcg = 63.46 
2025-04-08 17:59:34.037961: Steps 99/138: batch_recall = 127.61, batch_ndcg = 72.64 
2025-04-08 17:59:35.308596: Steps 100/138: batch_recall = 128.91, batch_ndcg = 72.17 
2025-04-08 17:59:36.586455: Steps 101/138: batch_recall = 128.38, batch_ndcg = 70.50 
2025-04-08 17:59:37.869077: Steps 102/138: batch_recall = 124.30, batch_ndcg = 71.69 
2025-04-08 17:59:39.147543: Steps 103/138: batch_recall = 143.82, batch_ndcg = 80.86 
2025-04-08 17:59:40.424912: Steps 104/138: batch_recall = 134.17, batch_ndcg = 78.72 
2025-04-08 17:59:41.711404: Steps 105/138: batch_recall = 122.67, batch_ndcg = 67.48 
2025-04-08 17:59:42.990002: Steps 106/138: batch_recall = 105.51, batch_ndcg = 59.67 
2025-04-08 17:59:44.265703: Steps 107/138: batch_recall = 113.46, batch_ndcg = 64.49 
2025-04-08 17:59:45.537560: Steps 108/138: batch_recall = 115.14, batch_ndcg = 69.98 
2025-04-08 17:59:46.807403: Steps 109/138: batch_recall = 138.50, batch_ndcg = 78.72 
2025-04-08 17:59:48.082183: Steps 110/138: batch_recall = 126.62, batch_ndcg = 65.75 
2025-04-08 17:59:49.343204: Steps 111/138: batch_recall = 135.75, batch_ndcg = 83.28 
2025-04-08 17:59:50.619299: Steps 112/138: batch_recall = 168.97, batch_ndcg = 93.91 
2025-04-08 17:59:51.896657: Steps 113/138: batch_recall = 125.70, batch_ndcg = 70.30 
2025-04-08 17:59:53.169207: Steps 114/138: batch_recall = 123.78, batch_ndcg = 72.30 
2025-04-08 17:59:54.445385: Steps 115/138: batch_recall = 121.55, batch_ndcg = 63.82 
2025-04-08 17:59:55.720242: Steps 116/138: batch_recall = 124.39, batch_ndcg = 66.57 
2025-04-08 17:59:56.992197: Steps 117/138: batch_recall = 112.73, batch_ndcg = 65.32 
2025-04-08 17:59:58.264337: Steps 118/138: batch_recall = 126.48, batch_ndcg = 69.95 
2025-04-08 17:59:59.540236: Steps 119/138: batch_recall = 136.16, batch_ndcg = 75.31 
2025-04-08 18:00:00.815067: Steps 120/138: batch_recall = 123.74, batch_ndcg = 70.27 
2025-04-08 18:00:02.093818: Steps 121/138: batch_recall = 150.47, batch_ndcg = 79.18 
2025-04-08 18:00:03.360917: Steps 122/138: batch_recall = 150.79, batch_ndcg = 82.57 
2025-04-08 18:00:04.635275: Steps 123/138: batch_recall = 129.51, batch_ndcg = 73.48 
2025-04-08 18:00:05.904285: Steps 124/138: batch_recall = 153.41, batch_ndcg = 93.14 
2025-04-08 18:00:07.189183: Steps 125/138: batch_recall = 131.55, batch_ndcg = 72.86 
2025-04-08 18:00:08.465238: Steps 126/138: batch_recall = 159.44, batch_ndcg = 88.97 
2025-04-08 18:00:09.739523: Steps 127/138: batch_recall = 145.59, batch_ndcg = 82.71 
2025-04-08 18:00:11.014931: Steps 128/138: batch_recall = 131.31, batch_ndcg = 73.18 
2025-04-08 18:00:12.284059: Steps 129/138: batch_recall = 162.02, batch_ndcg = 92.21 
2025-04-08 18:00:13.570487: Steps 130/138: batch_recall = 132.93, batch_ndcg = 70.13 
2025-04-08 18:00:14.851984: Steps 131/138: batch_recall = 155.21, batch_ndcg = 88.58 
2025-04-08 18:00:16.126015: Steps 132/138: batch_recall = 150.16, batch_ndcg = 84.71 
2025-04-08 18:00:17.391410: Steps 133/138: batch_recall = 151.60, batch_ndcg = 85.46 
2025-04-08 18:00:18.661490: Steps 134/138: batch_recall = 147.38, batch_ndcg = 84.06 
2025-04-08 18:00:19.929038: Steps 135/138: batch_recall = 168.53, batch_ndcg = 95.85 
2025-04-08 18:00:21.196068: Steps 136/138: batch_recall = 155.73, batch_ndcg = 79.45 
2025-04-08 18:00:22.463992: Steps 137/138: batch_recall = 140.20, batch_ndcg = 87.01 
2025-04-08 18:00:22.464523: Epoch 25/1000, Test: Recall = 0.1773, NDCG = 0.1009  

2025-04-08 18:00:24.298664: Training Step 0/354: batchLoss = 0.5326, diffLoss = 2.6117, kgLoss = 0.0129
2025-04-08 18:00:25.917931: Training Step 1/354: batchLoss = 0.5235, diffLoss = 2.5685, kgLoss = 0.0123
2025-04-08 18:00:27.536432: Training Step 2/354: batchLoss = 0.5592, diffLoss = 2.7414, kgLoss = 0.0137
2025-04-08 18:00:29.155603: Training Step 3/354: batchLoss = 0.5578, diffLoss = 2.7350, kgLoss = 0.0136
2025-04-08 18:00:30.775071: Training Step 4/354: batchLoss = 0.4991, diffLoss = 2.4455, kgLoss = 0.0126
2025-04-08 18:00:32.401342: Training Step 5/354: batchLoss = 0.4792, diffLoss = 2.3508, kgLoss = 0.0113
2025-04-08 18:00:34.015590: Training Step 6/354: batchLoss = 0.5144, diffLoss = 2.5180, kgLoss = 0.0135
2025-04-08 18:00:35.635626: Training Step 7/354: batchLoss = 0.5943, diffLoss = 2.9061, kgLoss = 0.0164
2025-04-08 18:00:37.250306: Training Step 8/354: batchLoss = 0.4992, diffLoss = 2.4460, kgLoss = 0.0125
2025-04-08 18:00:38.865099: Training Step 9/354: batchLoss = 0.6415, diffLoss = 3.1494, kgLoss = 0.0145
2025-04-08 18:00:40.488601: Training Step 10/354: batchLoss = 0.4842, diffLoss = 2.3764, kgLoss = 0.0112
2025-04-08 18:00:42.109406: Training Step 11/354: batchLoss = 0.5908, diffLoss = 2.8934, kgLoss = 0.0152
2025-04-08 18:00:43.730673: Training Step 12/354: batchLoss = 0.4636, diffLoss = 2.2720, kgLoss = 0.0115
2025-04-08 18:00:45.355869: Training Step 13/354: batchLoss = 0.4473, diffLoss = 2.1889, kgLoss = 0.0119
2025-04-08 18:00:46.974620: Training Step 14/354: batchLoss = 0.5042, diffLoss = 2.4547, kgLoss = 0.0166
2025-04-08 18:00:48.591763: Training Step 15/354: batchLoss = 0.4377, diffLoss = 2.1368, kgLoss = 0.0129
2025-04-08 18:00:50.210577: Training Step 16/354: batchLoss = 0.5919, diffLoss = 2.9006, kgLoss = 0.0147
2025-04-08 18:00:51.825025: Training Step 17/354: batchLoss = 0.5478, diffLoss = 2.6770, kgLoss = 0.0155
2025-04-08 18:00:53.445328: Training Step 18/354: batchLoss = 0.5559, diffLoss = 2.7206, kgLoss = 0.0147
2025-04-08 18:00:55.060918: Training Step 19/354: batchLoss = 0.5778, diffLoss = 2.8313, kgLoss = 0.0145
2025-04-08 18:00:56.677969: Training Step 20/354: batchLoss = 0.5023, diffLoss = 2.4520, kgLoss = 0.0148
2025-04-08 18:00:58.297756: Training Step 21/354: batchLoss = 0.6188, diffLoss = 3.0376, kgLoss = 0.0141
2025-04-08 18:00:59.918669: Training Step 22/354: batchLoss = 0.6179, diffLoss = 3.0252, kgLoss = 0.0161
2025-04-08 18:01:01.544505: Training Step 23/354: batchLoss = 0.7475, diffLoss = 3.6572, kgLoss = 0.0200
2025-04-08 18:01:03.170693: Training Step 24/354: batchLoss = 0.5723, diffLoss = 2.8022, kgLoss = 0.0148
2025-04-08 18:01:04.788287: Training Step 25/354: batchLoss = 0.4970, diffLoss = 2.4320, kgLoss = 0.0133
2025-04-08 18:01:06.471323: Training Step 26/354: batchLoss = 0.4888, diffLoss = 2.3929, kgLoss = 0.0127
2025-04-08 18:01:08.085775: Training Step 27/354: batchLoss = 0.5785, diffLoss = 2.8284, kgLoss = 0.0160
2025-04-08 18:01:09.699913: Training Step 28/354: batchLoss = 0.5844, diffLoss = 2.8623, kgLoss = 0.0149
2025-04-08 18:01:11.315728: Training Step 29/354: batchLoss = 0.6268, diffLoss = 3.0732, kgLoss = 0.0152
2025-04-08 18:01:12.933930: Training Step 30/354: batchLoss = 0.5355, diffLoss = 2.6222, kgLoss = 0.0138
2025-04-08 18:01:14.553400: Training Step 31/354: batchLoss = 0.4902, diffLoss = 2.4010, kgLoss = 0.0125
2025-04-08 18:01:16.168791: Training Step 32/354: batchLoss = 0.5791, diffLoss = 2.8305, kgLoss = 0.0163
2025-04-08 18:01:17.794307: Training Step 33/354: batchLoss = 0.5071, diffLoss = 2.4897, kgLoss = 0.0115
2025-04-08 18:01:19.451037: Training Step 34/354: batchLoss = 0.5597, diffLoss = 2.7417, kgLoss = 0.0141
2025-04-08 18:01:21.070965: Training Step 35/354: batchLoss = 0.6088, diffLoss = 2.9812, kgLoss = 0.0158
2025-04-08 18:01:22.684624: Training Step 36/354: batchLoss = 0.5130, diffLoss = 2.5142, kgLoss = 0.0127
2025-04-08 18:01:24.303428: Training Step 37/354: batchLoss = 0.5116, diffLoss = 2.5069, kgLoss = 0.0128
2025-04-08 18:01:25.928976: Training Step 38/354: batchLoss = 0.5034, diffLoss = 2.4559, kgLoss = 0.0153
2025-04-08 18:01:27.545054: Training Step 39/354: batchLoss = 0.4980, diffLoss = 2.4319, kgLoss = 0.0145
2025-04-08 18:01:29.161119: Training Step 40/354: batchLoss = 0.6191, diffLoss = 3.0243, kgLoss = 0.0179
2025-04-08 18:01:30.786145: Training Step 41/354: batchLoss = 0.5954, diffLoss = 2.9207, kgLoss = 0.0141
2025-04-08 18:01:32.413547: Training Step 42/354: batchLoss = 0.7295, diffLoss = 3.5738, kgLoss = 0.0184
2025-04-08 18:01:34.028805: Training Step 43/354: batchLoss = 0.5370, diffLoss = 2.6259, kgLoss = 0.0147
2025-04-08 18:01:35.646287: Training Step 44/354: batchLoss = 0.5085, diffLoss = 2.4947, kgLoss = 0.0120
2025-04-08 18:01:37.262267: Training Step 45/354: batchLoss = 0.7685, diffLoss = 3.7638, kgLoss = 0.0197
2025-04-08 18:01:38.877392: Training Step 46/354: batchLoss = 0.7191, diffLoss = 3.5198, kgLoss = 0.0189
2025-04-08 18:01:40.500657: Training Step 47/354: batchLoss = 0.5171, diffLoss = 2.5319, kgLoss = 0.0134
2025-04-08 18:01:42.117837: Training Step 48/354: batchLoss = 0.6630, diffLoss = 3.2489, kgLoss = 0.0166
2025-04-08 18:01:43.734791: Training Step 49/354: batchLoss = 0.5022, diffLoss = 2.4605, kgLoss = 0.0127
2025-04-08 18:01:45.352868: Training Step 50/354: batchLoss = 0.4246, diffLoss = 2.0760, kgLoss = 0.0118
2025-04-08 18:01:46.975341: Training Step 51/354: batchLoss = 0.4836, diffLoss = 2.3646, kgLoss = 0.0134
2025-04-08 18:01:48.595449: Training Step 52/354: batchLoss = 0.6821, diffLoss = 3.3451, kgLoss = 0.0164
2025-04-08 18:01:50.220257: Training Step 53/354: batchLoss = 0.5339, diffLoss = 2.6169, kgLoss = 0.0131
2025-04-08 18:01:51.840364: Training Step 54/354: batchLoss = 0.5587, diffLoss = 2.7362, kgLoss = 0.0144
2025-04-08 18:01:53.466276: Training Step 55/354: batchLoss = 0.4301, diffLoss = 2.1036, kgLoss = 0.0117
2025-04-08 18:01:55.077616: Training Step 56/354: batchLoss = 0.6672, diffLoss = 3.2696, kgLoss = 0.0166
2025-04-08 18:01:56.694761: Training Step 57/354: batchLoss = 0.4815, diffLoss = 2.3583, kgLoss = 0.0123
2025-04-08 18:01:58.306380: Training Step 58/354: batchLoss = 0.6017, diffLoss = 2.9473, kgLoss = 0.0153
2025-04-08 18:01:59.930559: Training Step 59/354: batchLoss = 0.7070, diffLoss = 3.4647, kgLoss = 0.0176
2025-04-08 18:02:01.557100: Training Step 60/354: batchLoss = 0.4803, diffLoss = 2.3511, kgLoss = 0.0126
2025-04-08 18:02:03.176310: Training Step 61/354: batchLoss = 0.4648, diffLoss = 2.2768, kgLoss = 0.0119
2025-04-08 18:02:04.795245: Training Step 62/354: batchLoss = 0.5650, diffLoss = 2.7665, kgLoss = 0.0147
2025-04-08 18:02:06.417261: Training Step 63/354: batchLoss = 0.4713, diffLoss = 2.3022, kgLoss = 0.0136
2025-04-08 18:02:08.040750: Training Step 64/354: batchLoss = 0.5476, diffLoss = 2.6756, kgLoss = 0.0156
2025-04-08 18:02:09.651926: Training Step 65/354: batchLoss = 0.5326, diffLoss = 2.6073, kgLoss = 0.0139
2025-04-08 18:02:11.273046: Training Step 66/354: batchLoss = 0.4836, diffLoss = 2.3669, kgLoss = 0.0127
2025-04-08 18:02:12.882700: Training Step 67/354: batchLoss = 0.9261, diffLoss = 4.5395, kgLoss = 0.0228
2025-04-08 18:02:14.495949: Training Step 68/354: batchLoss = 0.4978, diffLoss = 2.4383, kgLoss = 0.0126
2025-04-08 18:02:16.115909: Training Step 69/354: batchLoss = 0.4921, diffLoss = 2.4088, kgLoss = 0.0129
2025-04-08 18:02:17.735084: Training Step 70/354: batchLoss = 0.5092, diffLoss = 2.4899, kgLoss = 0.0140
2025-04-08 18:02:19.357568: Training Step 71/354: batchLoss = 0.4890, diffLoss = 2.3987, kgLoss = 0.0115
2025-04-08 18:02:20.980139: Training Step 72/354: batchLoss = 0.5744, diffLoss = 2.8155, kgLoss = 0.0141
2025-04-08 18:02:22.603719: Training Step 73/354: batchLoss = 0.5839, diffLoss = 2.8627, kgLoss = 0.0142
2025-04-08 18:02:24.227555: Training Step 74/354: batchLoss = 0.4986, diffLoss = 2.4390, kgLoss = 0.0135
2025-04-08 18:02:25.843271: Training Step 75/354: batchLoss = 0.6410, diffLoss = 3.1459, kgLoss = 0.0148
2025-04-08 18:02:27.463225: Training Step 76/354: batchLoss = 0.6753, diffLoss = 3.3069, kgLoss = 0.0174
2025-04-08 18:02:29.082783: Training Step 77/354: batchLoss = 0.5216, diffLoss = 2.5484, kgLoss = 0.0149
2025-04-08 18:02:30.700230: Training Step 78/354: batchLoss = 0.5457, diffLoss = 2.6700, kgLoss = 0.0147
2025-04-08 18:02:32.321096: Training Step 79/354: batchLoss = 0.5984, diffLoss = 2.9335, kgLoss = 0.0146
2025-04-08 18:02:33.939715: Training Step 80/354: batchLoss = 0.5530, diffLoss = 2.6976, kgLoss = 0.0169
2025-04-08 18:02:35.559070: Training Step 81/354: batchLoss = 0.5512, diffLoss = 2.6910, kgLoss = 0.0163
2025-04-08 18:02:37.183217: Training Step 82/354: batchLoss = 0.6653, diffLoss = 3.2612, kgLoss = 0.0164
2025-04-08 18:02:38.805601: Training Step 83/354: batchLoss = 0.5316, diffLoss = 2.6026, kgLoss = 0.0138
2025-04-08 18:02:40.426221: Training Step 84/354: batchLoss = 0.4949, diffLoss = 2.4140, kgLoss = 0.0151
2025-04-08 18:02:42.040524: Training Step 85/354: batchLoss = 0.6007, diffLoss = 2.9420, kgLoss = 0.0154
2025-04-08 18:02:43.659172: Training Step 86/354: batchLoss = 0.5150, diffLoss = 2.5200, kgLoss = 0.0138
2025-04-08 18:02:45.274040: Training Step 87/354: batchLoss = 0.5023, diffLoss = 2.4607, kgLoss = 0.0128
2025-04-08 18:02:46.889249: Training Step 88/354: batchLoss = 0.6207, diffLoss = 3.0469, kgLoss = 0.0141
2025-04-08 18:02:48.508188: Training Step 89/354: batchLoss = 0.6161, diffLoss = 3.0169, kgLoss = 0.0159
2025-04-08 18:02:50.128239: Training Step 90/354: batchLoss = 0.5353, diffLoss = 2.6194, kgLoss = 0.0142
2025-04-08 18:02:51.745249: Training Step 91/354: batchLoss = 0.4986, diffLoss = 2.4393, kgLoss = 0.0134
2025-04-08 18:02:53.366425: Training Step 92/354: batchLoss = 0.5052, diffLoss = 2.4753, kgLoss = 0.0127
2025-04-08 18:02:54.986209: Training Step 93/354: batchLoss = 0.5790, diffLoss = 2.8346, kgLoss = 0.0151
2025-04-08 18:02:56.596585: Training Step 94/354: batchLoss = 0.6513, diffLoss = 3.1955, kgLoss = 0.0153
2025-04-08 18:02:58.211675: Training Step 95/354: batchLoss = 0.5391, diffLoss = 2.6453, kgLoss = 0.0126
2025-04-08 18:02:59.829385: Training Step 96/354: batchLoss = 0.5142, diffLoss = 2.5152, kgLoss = 0.0140
2025-04-08 18:03:01.448059: Training Step 97/354: batchLoss = 0.4737, diffLoss = 2.3170, kgLoss = 0.0129
2025-04-08 18:03:03.066725: Training Step 98/354: batchLoss = 0.5961, diffLoss = 2.9191, kgLoss = 0.0153
2025-04-08 18:03:04.686734: Training Step 99/354: batchLoss = 0.7753, diffLoss = 3.8037, kgLoss = 0.0182
2025-04-08 18:03:06.312829: Training Step 100/354: batchLoss = 0.4904, diffLoss = 2.4023, kgLoss = 0.0124
2025-04-08 18:03:07.928740: Training Step 101/354: batchLoss = 0.5893, diffLoss = 2.8835, kgLoss = 0.0157
2025-04-08 18:03:09.548345: Training Step 102/354: batchLoss = 0.5748, diffLoss = 2.8123, kgLoss = 0.0155
2025-04-08 18:03:11.165117: Training Step 103/354: batchLoss = 0.4672, diffLoss = 2.2837, kgLoss = 0.0130
2025-04-08 18:03:12.779358: Training Step 104/354: batchLoss = 0.5488, diffLoss = 2.6918, kgLoss = 0.0130
2025-04-08 18:03:14.394965: Training Step 105/354: batchLoss = 1.0171, diffLoss = 4.9886, kgLoss = 0.0242
2025-04-08 18:03:16.009556: Training Step 106/354: batchLoss = 0.6024, diffLoss = 2.9508, kgLoss = 0.0153
2025-04-08 18:03:17.633534: Training Step 107/354: batchLoss = 0.5960, diffLoss = 2.9140, kgLoss = 0.0165
2025-04-08 18:03:19.248132: Training Step 108/354: batchLoss = 0.5162, diffLoss = 2.5287, kgLoss = 0.0131
2025-04-08 18:03:20.864822: Training Step 109/354: batchLoss = 0.5856, diffLoss = 2.8662, kgLoss = 0.0154
2025-04-08 18:03:22.487472: Training Step 110/354: batchLoss = 0.5936, diffLoss = 2.9079, kgLoss = 0.0150
2025-04-08 18:03:24.115556: Training Step 111/354: batchLoss = 0.6181, diffLoss = 3.0337, kgLoss = 0.0142
2025-04-08 18:03:25.736628: Training Step 112/354: batchLoss = 0.5214, diffLoss = 2.5550, kgLoss = 0.0130
2025-04-08 18:03:27.358604: Training Step 113/354: batchLoss = 0.5253, diffLoss = 2.5744, kgLoss = 0.0131
2025-04-08 18:03:28.974081: Training Step 114/354: batchLoss = 0.4475, diffLoss = 2.1905, kgLoss = 0.0117
2025-04-08 18:03:30.586716: Training Step 115/354: batchLoss = 0.5670, diffLoss = 2.7768, kgLoss = 0.0145
2025-04-08 18:03:32.209025: Training Step 116/354: batchLoss = 0.6031, diffLoss = 2.9590, kgLoss = 0.0142
2025-04-08 18:03:33.823579: Training Step 117/354: batchLoss = 0.5187, diffLoss = 2.5427, kgLoss = 0.0127
2025-04-08 18:03:35.452647: Training Step 118/354: batchLoss = 0.6646, diffLoss = 3.2616, kgLoss = 0.0154
2025-04-08 18:03:37.072173: Training Step 119/354: batchLoss = 0.5605, diffLoss = 2.7429, kgLoss = 0.0149
2025-04-08 18:03:38.695737: Training Step 120/354: batchLoss = 0.6344, diffLoss = 3.1035, kgLoss = 0.0171
2025-04-08 18:03:40.314337: Training Step 121/354: batchLoss = 0.6972, diffLoss = 3.4236, kgLoss = 0.0157
2025-04-08 18:03:41.936334: Training Step 122/354: batchLoss = 0.4954, diffLoss = 2.4333, kgLoss = 0.0109
2025-04-08 18:03:43.550934: Training Step 123/354: batchLoss = 0.5769, diffLoss = 2.8303, kgLoss = 0.0135
2025-04-08 18:03:45.161285: Training Step 124/354: batchLoss = 0.5159, diffLoss = 2.5238, kgLoss = 0.0140
2025-04-08 18:03:46.779213: Training Step 125/354: batchLoss = 0.5694, diffLoss = 2.7888, kgLoss = 0.0145
2025-04-08 18:03:48.393314: Training Step 126/354: batchLoss = 0.5983, diffLoss = 2.9315, kgLoss = 0.0150
2025-04-08 18:03:50.011377: Training Step 127/354: batchLoss = 0.5404, diffLoss = 2.6483, kgLoss = 0.0134
2025-04-08 18:03:51.631393: Training Step 128/354: batchLoss = 0.5551, diffLoss = 2.7232, kgLoss = 0.0131
2025-04-08 18:03:53.251575: Training Step 129/354: batchLoss = 0.5689, diffLoss = 2.7899, kgLoss = 0.0137
2025-04-08 18:03:54.873960: Training Step 130/354: batchLoss = 0.5030, diffLoss = 2.4607, kgLoss = 0.0136
2025-04-08 18:03:56.497950: Training Step 131/354: batchLoss = 0.7415, diffLoss = 3.6309, kgLoss = 0.0191
2025-04-08 18:03:58.124133: Training Step 132/354: batchLoss = 0.4983, diffLoss = 2.4385, kgLoss = 0.0133
2025-04-08 18:03:59.743530: Training Step 133/354: batchLoss = 0.6556, diffLoss = 3.2119, kgLoss = 0.0165
2025-04-08 18:04:01.361743: Training Step 134/354: batchLoss = 0.4775, diffLoss = 2.3366, kgLoss = 0.0127
2025-04-08 18:04:02.978035: Training Step 135/354: batchLoss = 0.5078, diffLoss = 2.4893, kgLoss = 0.0125
2025-04-08 18:04:04.597199: Training Step 136/354: batchLoss = 0.6343, diffLoss = 3.1055, kgLoss = 0.0165
2025-04-08 18:04:06.211337: Training Step 137/354: batchLoss = 0.7475, diffLoss = 3.6640, kgLoss = 0.0184
2025-04-08 18:04:07.835316: Training Step 138/354: batchLoss = 0.6158, diffLoss = 3.0121, kgLoss = 0.0168
2025-04-08 18:04:09.458838: Training Step 139/354: batchLoss = 0.6161, diffLoss = 3.0145, kgLoss = 0.0165
2025-04-08 18:04:11.084155: Training Step 140/354: batchLoss = 0.5468, diffLoss = 2.6686, kgLoss = 0.0163
2025-04-08 18:04:12.704857: Training Step 141/354: batchLoss = 0.6286, diffLoss = 3.0767, kgLoss = 0.0166
2025-04-08 18:04:14.331697: Training Step 142/354: batchLoss = 0.4807, diffLoss = 2.3456, kgLoss = 0.0145
2025-04-08 18:04:15.952524: Training Step 143/354: batchLoss = 0.4604, diffLoss = 2.2537, kgLoss = 0.0121
2025-04-08 18:04:17.567364: Training Step 144/354: batchLoss = 0.4954, diffLoss = 2.4231, kgLoss = 0.0134
2025-04-08 18:04:19.178402: Training Step 145/354: batchLoss = 0.5238, diffLoss = 2.5671, kgLoss = 0.0129
2025-04-08 18:04:20.790380: Training Step 146/354: batchLoss = 0.4420, diffLoss = 2.1570, kgLoss = 0.0132
2025-04-08 18:04:22.409535: Training Step 147/354: batchLoss = 0.6702, diffLoss = 3.2890, kgLoss = 0.0155
2025-04-08 18:04:24.036571: Training Step 148/354: batchLoss = 0.6138, diffLoss = 3.0110, kgLoss = 0.0145
2025-04-08 18:04:25.658965: Training Step 149/354: batchLoss = 0.5536, diffLoss = 2.7096, kgLoss = 0.0145
2025-04-08 18:04:27.274993: Training Step 150/354: batchLoss = 0.5753, diffLoss = 2.8137, kgLoss = 0.0157
2025-04-08 18:04:28.898955: Training Step 151/354: batchLoss = 0.5396, diffLoss = 2.6466, kgLoss = 0.0129
2025-04-08 18:04:30.517170: Training Step 152/354: batchLoss = 0.5260, diffLoss = 2.5662, kgLoss = 0.0160
2025-04-08 18:04:32.132194: Training Step 153/354: batchLoss = 0.4934, diffLoss = 2.4160, kgLoss = 0.0127
2025-04-08 18:04:33.747598: Training Step 154/354: batchLoss = 0.5013, diffLoss = 2.4525, kgLoss = 0.0135
2025-04-08 18:04:35.362524: Training Step 155/354: batchLoss = 0.5353, diffLoss = 2.6198, kgLoss = 0.0142
2025-04-08 18:04:36.976487: Training Step 156/354: batchLoss = 0.7527, diffLoss = 3.6927, kgLoss = 0.0177
2025-04-08 18:04:38.587711: Training Step 157/354: batchLoss = 0.5953, diffLoss = 2.9187, kgLoss = 0.0144
2025-04-08 18:04:40.205201: Training Step 158/354: batchLoss = 0.5290, diffLoss = 2.5859, kgLoss = 0.0148
2025-04-08 18:04:41.822734: Training Step 159/354: batchLoss = 0.6487, diffLoss = 3.1681, kgLoss = 0.0189
2025-04-08 18:04:43.437627: Training Step 160/354: batchLoss = 0.5337, diffLoss = 2.6147, kgLoss = 0.0135
2025-04-08 18:04:45.057713: Training Step 161/354: batchLoss = 0.6202, diffLoss = 3.0337, kgLoss = 0.0168
2025-04-08 18:04:46.674608: Training Step 162/354: batchLoss = 0.6101, diffLoss = 2.9913, kgLoss = 0.0148
2025-04-08 18:04:48.292432: Training Step 163/354: batchLoss = 0.5737, diffLoss = 2.8118, kgLoss = 0.0141
2025-04-08 18:04:49.911785: Training Step 164/354: batchLoss = 0.5561, diffLoss = 2.7269, kgLoss = 0.0134
2025-04-08 18:04:51.527100: Training Step 165/354: batchLoss = 0.5276, diffLoss = 2.5882, kgLoss = 0.0125
2025-04-08 18:04:53.147148: Training Step 166/354: batchLoss = 0.5630, diffLoss = 2.7574, kgLoss = 0.0145
2025-04-08 18:04:54.766104: Training Step 167/354: batchLoss = 0.7063, diffLoss = 3.4557, kgLoss = 0.0190
2025-04-08 18:04:56.386706: Training Step 168/354: batchLoss = 0.5658, diffLoss = 2.7701, kgLoss = 0.0147
2025-04-08 18:04:58.002683: Training Step 169/354: batchLoss = 0.4856, diffLoss = 2.3792, kgLoss = 0.0123
2025-04-08 18:04:59.626291: Training Step 170/354: batchLoss = 0.5583, diffLoss = 2.7319, kgLoss = 0.0149
2025-04-08 18:05:01.245402: Training Step 171/354: batchLoss = 0.5315, diffLoss = 2.6050, kgLoss = 0.0132
2025-04-08 18:05:02.868874: Training Step 172/354: batchLoss = 0.4520, diffLoss = 2.2080, kgLoss = 0.0130
2025-04-08 18:05:04.485897: Training Step 173/354: batchLoss = 0.5216, diffLoss = 2.5575, kgLoss = 0.0126
2025-04-08 18:05:06.094975: Training Step 174/354: batchLoss = 0.6106, diffLoss = 2.9901, kgLoss = 0.0157
2025-04-08 18:05:07.714031: Training Step 175/354: batchLoss = 0.5195, diffLoss = 2.5442, kgLoss = 0.0134
2025-04-08 18:05:09.341889: Training Step 176/354: batchLoss = 0.6200, diffLoss = 3.0353, kgLoss = 0.0161
2025-04-08 18:05:10.968944: Training Step 177/354: batchLoss = 0.5286, diffLoss = 2.5910, kgLoss = 0.0130
2025-04-08 18:05:12.587768: Training Step 178/354: batchLoss = 0.4463, diffLoss = 2.1818, kgLoss = 0.0125
2025-04-08 18:05:14.206407: Training Step 179/354: batchLoss = 0.7109, diffLoss = 3.4668, kgLoss = 0.0220
2025-04-08 18:05:15.833335: Training Step 180/354: batchLoss = 0.4738, diffLoss = 2.3228, kgLoss = 0.0116
2025-04-08 18:05:17.449744: Training Step 181/354: batchLoss = 0.5023, diffLoss = 2.4558, kgLoss = 0.0139
2025-04-08 18:05:19.068838: Training Step 182/354: batchLoss = 0.6261, diffLoss = 3.0760, kgLoss = 0.0136
2025-04-08 18:05:20.680848: Training Step 183/354: batchLoss = 0.5340, diffLoss = 2.6071, kgLoss = 0.0157
2025-04-08 18:05:22.302701: Training Step 184/354: batchLoss = 0.4389, diffLoss = 2.1464, kgLoss = 0.0121
2025-04-08 18:05:23.921516: Training Step 185/354: batchLoss = 0.3987, diffLoss = 1.9480, kgLoss = 0.0113
2025-04-08 18:05:25.537260: Training Step 186/354: batchLoss = 0.6819, diffLoss = 3.3404, kgLoss = 0.0172
2025-04-08 18:05:27.152574: Training Step 187/354: batchLoss = 0.5725, diffLoss = 2.8049, kgLoss = 0.0144
2025-04-08 18:05:28.771002: Training Step 188/354: batchLoss = 0.5591, diffLoss = 2.7339, kgLoss = 0.0155
2025-04-08 18:05:30.394396: Training Step 189/354: batchLoss = 0.4296, diffLoss = 2.0920, kgLoss = 0.0140
2025-04-08 18:05:32.011031: Training Step 190/354: batchLoss = 0.4777, diffLoss = 2.3392, kgLoss = 0.0124
2025-04-08 18:05:33.625436: Training Step 191/354: batchLoss = 0.5234, diffLoss = 2.5618, kgLoss = 0.0139
2025-04-08 18:05:35.248879: Training Step 192/354: batchLoss = 0.5142, diffLoss = 2.5127, kgLoss = 0.0145
2025-04-08 18:05:36.872837: Training Step 193/354: batchLoss = 0.5262, diffLoss = 2.5703, kgLoss = 0.0151
2025-04-08 18:05:38.491135: Training Step 194/354: batchLoss = 0.4562, diffLoss = 2.2260, kgLoss = 0.0137
2025-04-08 18:05:40.103383: Training Step 195/354: batchLoss = 0.5595, diffLoss = 2.7425, kgLoss = 0.0137
2025-04-08 18:05:41.723226: Training Step 196/354: batchLoss = 0.7090, diffLoss = 3.4694, kgLoss = 0.0189
2025-04-08 18:05:43.347242: Training Step 197/354: batchLoss = 0.4763, diffLoss = 2.3288, kgLoss = 0.0132
2025-04-08 18:05:44.967949: Training Step 198/354: batchLoss = 0.6312, diffLoss = 3.0853, kgLoss = 0.0177
2025-04-08 18:05:46.590680: Training Step 199/354: batchLoss = 0.4461, diffLoss = 2.1794, kgLoss = 0.0128
2025-04-08 18:05:48.209276: Training Step 200/354: batchLoss = 0.4988, diffLoss = 2.4280, kgLoss = 0.0165
2025-04-08 18:05:49.823193: Training Step 201/354: batchLoss = 0.5474, diffLoss = 2.6826, kgLoss = 0.0136
2025-04-08 18:05:51.445764: Training Step 202/354: batchLoss = 0.6319, diffLoss = 3.0956, kgLoss = 0.0160
2025-04-08 18:05:53.060571: Training Step 203/354: batchLoss = 0.4187, diffLoss = 2.0493, kgLoss = 0.0110
2025-04-08 18:05:54.677582: Training Step 204/354: batchLoss = 0.5746, diffLoss = 2.8132, kgLoss = 0.0149
2025-04-08 18:05:56.293093: Training Step 205/354: batchLoss = 0.5337, diffLoss = 2.6129, kgLoss = 0.0140
2025-04-08 18:05:57.911244: Training Step 206/354: batchLoss = 0.6350, diffLoss = 3.1119, kgLoss = 0.0157
2025-04-08 18:05:59.530718: Training Step 207/354: batchLoss = 0.6741, diffLoss = 3.3031, kgLoss = 0.0169
2025-04-08 18:06:01.140389: Training Step 208/354: batchLoss = 0.4972, diffLoss = 2.4346, kgLoss = 0.0128
2025-04-08 18:06:02.752644: Training Step 209/354: batchLoss = 0.6045, diffLoss = 2.9591, kgLoss = 0.0159
2025-04-08 18:06:04.370939: Training Step 210/354: batchLoss = 0.5398, diffLoss = 2.6444, kgLoss = 0.0136
2025-04-08 18:06:05.992718: Training Step 211/354: batchLoss = 0.5199, diffLoss = 2.5429, kgLoss = 0.0141
2025-04-08 18:06:07.608737: Training Step 212/354: batchLoss = 0.5728, diffLoss = 2.8005, kgLoss = 0.0158
2025-04-08 18:06:09.241610: Training Step 213/354: batchLoss = 0.6109, diffLoss = 2.9972, kgLoss = 0.0144
2025-04-08 18:06:10.853742: Training Step 214/354: batchLoss = 0.4575, diffLoss = 2.2406, kgLoss = 0.0117
2025-04-08 18:06:12.470476: Training Step 215/354: batchLoss = 0.6942, diffLoss = 3.4034, kgLoss = 0.0169
2025-04-08 18:06:14.089260: Training Step 216/354: batchLoss = 0.4800, diffLoss = 2.3499, kgLoss = 0.0125
2025-04-08 18:06:15.707930: Training Step 217/354: batchLoss = 0.5946, diffLoss = 2.9158, kgLoss = 0.0142
2025-04-08 18:06:17.329006: Training Step 218/354: batchLoss = 0.4835, diffLoss = 2.3578, kgLoss = 0.0149
2025-04-08 18:06:18.944700: Training Step 219/354: batchLoss = 0.5177, diffLoss = 2.5333, kgLoss = 0.0138
2025-04-08 18:06:20.562231: Training Step 220/354: batchLoss = 0.6613, diffLoss = 3.2342, kgLoss = 0.0181
2025-04-08 18:06:22.182049: Training Step 221/354: batchLoss = 0.6236, diffLoss = 3.0598, kgLoss = 0.0145
2025-04-08 18:06:23.798752: Training Step 222/354: batchLoss = 0.5358, diffLoss = 2.6241, kgLoss = 0.0137
2025-04-08 18:06:25.412651: Training Step 223/354: batchLoss = 0.5799, diffLoss = 2.8452, kgLoss = 0.0136
2025-04-08 18:06:27.033556: Training Step 224/354: batchLoss = 0.6460, diffLoss = 3.1620, kgLoss = 0.0170
2025-04-08 18:06:28.649030: Training Step 225/354: batchLoss = 0.5355, diffLoss = 2.6237, kgLoss = 0.0134
2025-04-08 18:06:30.267024: Training Step 226/354: batchLoss = 0.5402, diffLoss = 2.6413, kgLoss = 0.0149
2025-04-08 18:06:31.888905: Training Step 227/354: batchLoss = 0.5796, diffLoss = 2.8454, kgLoss = 0.0132
2025-04-08 18:06:33.512544: Training Step 228/354: batchLoss = 0.4964, diffLoss = 2.4275, kgLoss = 0.0136
2025-04-08 18:06:35.135550: Training Step 229/354: batchLoss = 0.5876, diffLoss = 2.8836, kgLoss = 0.0136
2025-04-08 18:06:36.758794: Training Step 230/354: batchLoss = 0.6998, diffLoss = 3.4299, kgLoss = 0.0173
2025-04-08 18:06:38.380995: Training Step 231/354: batchLoss = 0.5179, diffLoss = 2.5340, kgLoss = 0.0138
2025-04-08 18:06:39.996445: Training Step 232/354: batchLoss = 0.4812, diffLoss = 2.3552, kgLoss = 0.0128
2025-04-08 18:06:41.616106: Training Step 233/354: batchLoss = 0.5304, diffLoss = 2.5940, kgLoss = 0.0145
2025-04-08 18:06:43.235965: Training Step 234/354: batchLoss = 0.5233, diffLoss = 2.5638, kgLoss = 0.0132
2025-04-08 18:06:44.850652: Training Step 235/354: batchLoss = 0.5925, diffLoss = 2.9004, kgLoss = 0.0155
2025-04-08 18:06:46.467205: Training Step 236/354: batchLoss = 0.5387, diffLoss = 2.6373, kgLoss = 0.0141
2025-04-08 18:06:48.090683: Training Step 237/354: batchLoss = 0.5879, diffLoss = 2.8830, kgLoss = 0.0141
2025-04-08 18:06:49.709795: Training Step 238/354: batchLoss = 0.5659, diffLoss = 2.7719, kgLoss = 0.0144
2025-04-08 18:06:51.326267: Training Step 239/354: batchLoss = 0.5372, diffLoss = 2.6346, kgLoss = 0.0129
2025-04-08 18:06:52.948868: Training Step 240/354: batchLoss = 0.5348, diffLoss = 2.6212, kgLoss = 0.0132
2025-04-08 18:06:54.568698: Training Step 241/354: batchLoss = 0.5000, diffLoss = 2.4534, kgLoss = 0.0116
2025-04-08 18:06:56.185255: Training Step 242/354: batchLoss = 0.6092, diffLoss = 2.9854, kgLoss = 0.0151
2025-04-08 18:06:57.800993: Training Step 243/354: batchLoss = 0.6062, diffLoss = 2.9698, kgLoss = 0.0152
2025-04-08 18:06:59.424916: Training Step 244/354: batchLoss = 0.4226, diffLoss = 2.0626, kgLoss = 0.0127
2025-04-08 18:07:01.045971: Training Step 245/354: batchLoss = 0.4654, diffLoss = 2.2726, kgLoss = 0.0136
2025-04-08 18:07:02.680103: Training Step 246/354: batchLoss = 0.8666, diffLoss = 4.2487, kgLoss = 0.0210
2025-04-08 18:07:04.300797: Training Step 247/354: batchLoss = 0.4226, diffLoss = 2.0687, kgLoss = 0.0111
2025-04-08 18:07:05.922442: Training Step 248/354: batchLoss = 0.5992, diffLoss = 2.9238, kgLoss = 0.0181
2025-04-08 18:07:07.539589: Training Step 249/354: batchLoss = 0.6332, diffLoss = 3.0990, kgLoss = 0.0168
2025-04-08 18:07:09.162663: Training Step 250/354: batchLoss = 0.5338, diffLoss = 2.6132, kgLoss = 0.0139
2025-04-08 18:07:10.786430: Training Step 251/354: batchLoss = 0.4538, diffLoss = 2.2098, kgLoss = 0.0148
2025-04-08 18:07:12.401912: Training Step 252/354: batchLoss = 0.6038, diffLoss = 2.9581, kgLoss = 0.0153
2025-04-08 18:07:14.023885: Training Step 253/354: batchLoss = 0.6986, diffLoss = 3.4156, kgLoss = 0.0193
2025-04-08 18:07:15.642203: Training Step 254/354: batchLoss = 0.5968, diffLoss = 2.9280, kgLoss = 0.0140
2025-04-08 18:07:17.263014: Training Step 255/354: batchLoss = 0.4765, diffLoss = 2.3307, kgLoss = 0.0129
2025-04-08 18:07:18.881727: Training Step 256/354: batchLoss = 0.5142, diffLoss = 2.5130, kgLoss = 0.0145
2025-04-08 18:07:20.504345: Training Step 257/354: batchLoss = 0.5858, diffLoss = 2.8661, kgLoss = 0.0157
2025-04-08 18:07:22.129833: Training Step 258/354: batchLoss = 0.4968, diffLoss = 2.4294, kgLoss = 0.0137
2025-04-08 18:07:23.754767: Training Step 259/354: batchLoss = 0.5228, diffLoss = 2.5595, kgLoss = 0.0136
2025-04-08 18:07:25.377417: Training Step 260/354: batchLoss = 0.6833, diffLoss = 3.3352, kgLoss = 0.0203
2025-04-08 18:07:27.000279: Training Step 261/354: batchLoss = 0.4864, diffLoss = 2.3763, kgLoss = 0.0139
2025-04-08 18:07:28.615829: Training Step 262/354: batchLoss = 0.5393, diffLoss = 2.6445, kgLoss = 0.0130
2025-04-08 18:07:30.231439: Training Step 263/354: batchLoss = 0.4870, diffLoss = 2.3730, kgLoss = 0.0155
2025-04-08 18:07:31.856091: Training Step 264/354: batchLoss = 0.6879, diffLoss = 3.3637, kgLoss = 0.0190
2025-04-08 18:07:33.468414: Training Step 265/354: batchLoss = 0.4585, diffLoss = 2.2432, kgLoss = 0.0123
2025-04-08 18:07:35.087710: Training Step 266/354: batchLoss = 0.5265, diffLoss = 2.5831, kgLoss = 0.0124
2025-04-08 18:07:36.706406: Training Step 267/354: batchLoss = 0.4662, diffLoss = 2.2827, kgLoss = 0.0120
2025-04-08 18:07:38.328232: Training Step 268/354: batchLoss = 0.4466, diffLoss = 2.1879, kgLoss = 0.0113
2025-04-08 18:07:39.953274: Training Step 269/354: batchLoss = 0.6155, diffLoss = 3.0164, kgLoss = 0.0152
2025-04-08 18:07:41.574040: Training Step 270/354: batchLoss = 0.6529, diffLoss = 3.1973, kgLoss = 0.0168
2025-04-08 18:07:43.195744: Training Step 271/354: batchLoss = 0.5331, diffLoss = 2.6086, kgLoss = 0.0142
2025-04-08 18:07:44.812080: Training Step 272/354: batchLoss = 0.5937, diffLoss = 2.9053, kgLoss = 0.0158
2025-04-08 18:07:46.423371: Training Step 273/354: batchLoss = 0.5800, diffLoss = 2.8257, kgLoss = 0.0185
2025-04-08 18:07:48.036878: Training Step 274/354: batchLoss = 0.5011, diffLoss = 2.4533, kgLoss = 0.0130
2025-04-08 18:07:49.656501: Training Step 275/354: batchLoss = 0.5822, diffLoss = 2.8532, kgLoss = 0.0145
2025-04-08 18:07:51.269619: Training Step 276/354: batchLoss = 0.5962, diffLoss = 2.9195, kgLoss = 0.0154
2025-04-08 18:07:52.885604: Training Step 277/354: batchLoss = 0.4882, diffLoss = 2.3902, kgLoss = 0.0127
2025-04-08 18:07:54.505455: Training Step 278/354: batchLoss = 0.5015, diffLoss = 2.4546, kgLoss = 0.0133
2025-04-08 18:07:56.125929: Training Step 279/354: batchLoss = 0.5693, diffLoss = 2.7873, kgLoss = 0.0148
2025-04-08 18:07:57.750445: Training Step 280/354: batchLoss = 0.7101, diffLoss = 3.4860, kgLoss = 0.0162
2025-04-08 18:07:59.364901: Training Step 281/354: batchLoss = 0.5057, diffLoss = 2.4743, kgLoss = 0.0136
2025-04-08 18:08:00.984913: Training Step 282/354: batchLoss = 0.5165, diffLoss = 2.5252, kgLoss = 0.0143
2025-04-08 18:08:02.601600: Training Step 283/354: batchLoss = 0.4896, diffLoss = 2.3964, kgLoss = 0.0129
2025-04-08 18:08:04.218496: Training Step 284/354: batchLoss = 0.4675, diffLoss = 2.2841, kgLoss = 0.0133
2025-04-08 18:08:05.831098: Training Step 285/354: batchLoss = 0.5597, diffLoss = 2.7366, kgLoss = 0.0155
2025-04-08 18:08:07.445834: Training Step 286/354: batchLoss = 0.6519, diffLoss = 3.1982, kgLoss = 0.0153
2025-04-08 18:08:09.061175: Training Step 287/354: batchLoss = 0.6665, diffLoss = 3.2620, kgLoss = 0.0176
2025-04-08 18:08:10.678079: Training Step 288/354: batchLoss = 1.7520, diffLoss = 8.5961, kgLoss = 0.0410
2025-04-08 18:08:12.300205: Training Step 289/354: batchLoss = 0.6907, diffLoss = 3.3823, kgLoss = 0.0178
2025-04-08 18:08:13.915905: Training Step 290/354: batchLoss = 0.5237, diffLoss = 2.5607, kgLoss = 0.0145
2025-04-08 18:08:15.538412: Training Step 291/354: batchLoss = 0.5979, diffLoss = 2.9276, kgLoss = 0.0155
2025-04-08 18:08:17.152967: Training Step 292/354: batchLoss = 0.5913, diffLoss = 2.8931, kgLoss = 0.0158
2025-04-08 18:08:18.766822: Training Step 293/354: batchLoss = 0.5566, diffLoss = 2.7223, kgLoss = 0.0152
2025-04-08 18:08:20.377302: Training Step 294/354: batchLoss = 0.5890, diffLoss = 2.8902, kgLoss = 0.0137
2025-04-08 18:08:21.990040: Training Step 295/354: batchLoss = 0.6333, diffLoss = 3.1029, kgLoss = 0.0159
2025-04-08 18:08:23.603878: Training Step 296/354: batchLoss = 0.5879, diffLoss = 2.8853, kgLoss = 0.0135
2025-04-08 18:08:25.225650: Training Step 297/354: batchLoss = 0.6183, diffLoss = 3.0267, kgLoss = 0.0162
2025-04-08 18:08:26.851029: Training Step 298/354: batchLoss = 0.5294, diffLoss = 2.5968, kgLoss = 0.0125
2025-04-08 18:08:28.471968: Training Step 299/354: batchLoss = 0.5628, diffLoss = 2.7627, kgLoss = 0.0128
2025-04-08 18:08:30.099113: Training Step 300/354: batchLoss = 0.7483, diffLoss = 3.6664, kgLoss = 0.0187
2025-04-08 18:08:31.719522: Training Step 301/354: batchLoss = 0.7321, diffLoss = 3.5910, kgLoss = 0.0174
2025-04-08 18:08:33.338872: Training Step 302/354: batchLoss = 0.6294, diffLoss = 3.0777, kgLoss = 0.0173
2025-04-08 18:08:34.946524: Training Step 303/354: batchLoss = 0.4821, diffLoss = 2.3433, kgLoss = 0.0168
2025-04-08 18:08:36.569271: Training Step 304/354: batchLoss = 0.4651, diffLoss = 2.2743, kgLoss = 0.0127
2025-04-08 18:08:38.178917: Training Step 305/354: batchLoss = 0.5445, diffLoss = 2.6619, kgLoss = 0.0151
2025-04-08 18:08:39.802736: Training Step 306/354: batchLoss = 0.5953, diffLoss = 2.9083, kgLoss = 0.0170
2025-04-08 18:08:41.420861: Training Step 307/354: batchLoss = 0.5710, diffLoss = 2.7984, kgLoss = 0.0142
2025-04-08 18:08:43.041318: Training Step 308/354: batchLoss = 0.5064, diffLoss = 2.4725, kgLoss = 0.0148
2025-04-08 18:08:44.659624: Training Step 309/354: batchLoss = 0.5511, diffLoss = 2.6898, kgLoss = 0.0164
2025-04-08 18:08:46.286197: Training Step 310/354: batchLoss = 0.5470, diffLoss = 2.6800, kgLoss = 0.0138
2025-04-08 18:08:47.900310: Training Step 311/354: batchLoss = 0.5767, diffLoss = 2.8212, kgLoss = 0.0156
2025-04-08 18:08:49.516319: Training Step 312/354: batchLoss = 0.5149, diffLoss = 2.5169, kgLoss = 0.0144
2025-04-08 18:08:51.129406: Training Step 313/354: batchLoss = 0.4668, diffLoss = 2.2800, kgLoss = 0.0135
2025-04-08 18:08:52.748677: Training Step 314/354: batchLoss = 0.6266, diffLoss = 3.0682, kgLoss = 0.0162
2025-04-08 18:08:54.365051: Training Step 315/354: batchLoss = 0.6169, diffLoss = 3.0226, kgLoss = 0.0155
2025-04-08 18:08:55.982470: Training Step 316/354: batchLoss = 0.4868, diffLoss = 2.3822, kgLoss = 0.0129
2025-04-08 18:08:57.602915: Training Step 317/354: batchLoss = 0.5043, diffLoss = 2.4696, kgLoss = 0.0129
2025-04-08 18:08:59.228075: Training Step 318/354: batchLoss = 0.5462, diffLoss = 2.6769, kgLoss = 0.0135
2025-04-08 18:09:00.849629: Training Step 319/354: batchLoss = 0.6667, diffLoss = 3.2649, kgLoss = 0.0171
2025-04-08 18:09:02.474360: Training Step 320/354: batchLoss = 0.4861, diffLoss = 2.3777, kgLoss = 0.0131
2025-04-08 18:09:04.092199: Training Step 321/354: batchLoss = 0.5588, diffLoss = 2.7270, kgLoss = 0.0168
2025-04-08 18:09:05.708658: Training Step 322/354: batchLoss = 0.7094, diffLoss = 3.4664, kgLoss = 0.0201
2025-04-08 18:09:07.324362: Training Step 323/354: batchLoss = 0.5331, diffLoss = 2.6146, kgLoss = 0.0128
2025-04-08 18:09:08.943349: Training Step 324/354: batchLoss = 0.4546, diffLoss = 2.2244, kgLoss = 0.0121
2025-04-08 18:09:10.558476: Training Step 325/354: batchLoss = 0.8075, diffLoss = 3.9614, kgLoss = 0.0190
2025-04-08 18:09:12.181234: Training Step 326/354: batchLoss = 0.5057, diffLoss = 2.4750, kgLoss = 0.0133
2025-04-08 18:09:13.800203: Training Step 327/354: batchLoss = 0.5241, diffLoss = 2.5696, kgLoss = 0.0127
2025-04-08 18:09:15.422758: Training Step 328/354: batchLoss = 0.6510, diffLoss = 3.1899, kgLoss = 0.0163
2025-04-08 18:09:17.042667: Training Step 329/354: batchLoss = 0.5628, diffLoss = 2.7567, kgLoss = 0.0143
2025-04-08 18:09:18.664769: Training Step 330/354: batchLoss = 0.6139, diffLoss = 3.0078, kgLoss = 0.0154
2025-04-08 18:09:20.282044: Training Step 331/354: batchLoss = 0.4652, diffLoss = 2.2778, kgLoss = 0.0120
2025-04-08 18:09:21.898902: Training Step 332/354: batchLoss = 0.4388, diffLoss = 2.1438, kgLoss = 0.0126
2025-04-08 18:09:23.509642: Training Step 333/354: batchLoss = 0.5485, diffLoss = 2.6933, kgLoss = 0.0124
2025-04-08 18:09:25.121304: Training Step 334/354: batchLoss = 0.4208, diffLoss = 2.0565, kgLoss = 0.0118
2025-04-08 18:09:26.742848: Training Step 335/354: batchLoss = 0.5122, diffLoss = 2.5080, kgLoss = 0.0133
2025-04-08 18:09:28.361643: Training Step 336/354: batchLoss = 0.6083, diffLoss = 2.9794, kgLoss = 0.0155
2025-04-08 18:09:29.984318: Training Step 337/354: batchLoss = 0.5422, diffLoss = 2.6481, kgLoss = 0.0157
2025-04-08 18:09:31.603715: Training Step 338/354: batchLoss = 0.5788, diffLoss = 2.8362, kgLoss = 0.0145
2025-04-08 18:09:33.226085: Training Step 339/354: batchLoss = 0.5278, diffLoss = 2.5791, kgLoss = 0.0150
2025-04-08 18:09:34.842184: Training Step 340/354: batchLoss = 0.9909, diffLoss = 4.8549, kgLoss = 0.0249
2025-04-08 18:09:36.455699: Training Step 341/354: batchLoss = 0.6882, diffLoss = 3.3695, kgLoss = 0.0179
2025-04-08 18:09:38.068830: Training Step 342/354: batchLoss = 0.4781, diffLoss = 2.3358, kgLoss = 0.0137
2025-04-08 18:09:39.682781: Training Step 343/354: batchLoss = 0.8616, diffLoss = 4.2271, kgLoss = 0.0202
2025-04-08 18:09:41.300979: Training Step 344/354: batchLoss = 0.6296, diffLoss = 3.0764, kgLoss = 0.0179
2025-04-08 18:09:42.922333: Training Step 345/354: batchLoss = 0.5192, diffLoss = 2.5416, kgLoss = 0.0136
2025-04-08 18:09:44.535197: Training Step 346/354: batchLoss = 0.5288, diffLoss = 2.5835, kgLoss = 0.0151
2025-04-08 18:09:46.157651: Training Step 347/354: batchLoss = 0.5958, diffLoss = 2.9180, kgLoss = 0.0153
2025-04-08 18:09:47.779069: Training Step 348/354: batchLoss = 0.6235, diffLoss = 3.0543, kgLoss = 0.0158
2025-04-08 18:09:49.401325: Training Step 349/354: batchLoss = 0.4521, diffLoss = 2.2125, kgLoss = 0.0120
2025-04-08 18:09:51.019229: Training Step 350/354: batchLoss = 0.5633, diffLoss = 2.7608, kgLoss = 0.0139
2025-04-08 18:09:52.632079: Training Step 351/354: batchLoss = 0.5323, diffLoss = 2.6141, kgLoss = 0.0118
2025-04-08 18:09:54.223231: Training Step 352/354: batchLoss = 0.5919, diffLoss = 2.9053, kgLoss = 0.0136
2025-04-08 18:09:55.622215: Training Step 353/354: batchLoss = 0.5289, diffLoss = 2.5860, kgLoss = 0.0146
2025-04-08 18:09:55.715126: 
2025-04-08 18:09:55.716248: Epoch 26/1000, Train: epLoss = 1.0027, epDfLoss = 4.9091, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 18:09:57.017015: Steps 0/138: batch_recall = 46.08, batch_ndcg = 26.27 
2025-04-08 18:09:58.336751: Steps 1/138: batch_recall = 49.27, batch_ndcg = 28.60 
2025-04-08 18:09:59.650762: Steps 2/138: batch_recall = 60.45, batch_ndcg = 36.23 
2025-04-08 18:10:00.976877: Steps 3/138: batch_recall = 58.10, batch_ndcg = 33.96 
2025-04-08 18:10:02.291549: Steps 4/138: batch_recall = 67.00, batch_ndcg = 41.02 
2025-04-08 18:10:03.614077: Steps 5/138: batch_recall = 58.40, batch_ndcg = 32.14 
2025-04-08 18:10:04.927119: Steps 6/138: batch_recall = 54.11, batch_ndcg = 32.47 
2025-04-08 18:10:06.233433: Steps 7/138: batch_recall = 62.00, batch_ndcg = 41.56 
2025-04-08 18:10:07.536227: Steps 8/138: batch_recall = 63.05, batch_ndcg = 40.15 
2025-04-08 18:10:08.840222: Steps 9/138: batch_recall = 58.75, batch_ndcg = 34.37 
2025-04-08 18:10:10.131455: Steps 10/138: batch_recall = 56.92, batch_ndcg = 32.31 
2025-04-08 18:10:11.426030: Steps 11/138: batch_recall = 56.99, batch_ndcg = 32.60 
2025-04-08 18:10:12.726305: Steps 12/138: batch_recall = 52.84, batch_ndcg = 29.17 
2025-04-08 18:10:14.016702: Steps 13/138: batch_recall = 52.99, batch_ndcg = 31.23 
2025-04-08 18:10:15.312171: Steps 14/138: batch_recall = 52.55, batch_ndcg = 31.15 
2025-04-08 18:10:16.611712: Steps 15/138: batch_recall = 48.37, batch_ndcg = 30.18 
2025-04-08 18:10:17.906819: Steps 16/138: batch_recall = 60.25, batch_ndcg = 34.11 
2025-04-08 18:10:19.191106: Steps 17/138: batch_recall = 56.61, batch_ndcg = 33.91 
2025-04-08 18:10:20.481848: Steps 18/138: batch_recall = 52.52, batch_ndcg = 32.94 
2025-04-08 18:10:21.775790: Steps 19/138: batch_recall = 55.53, batch_ndcg = 33.36 
2025-04-08 18:10:23.065563: Steps 20/138: batch_recall = 61.78, batch_ndcg = 36.25 
2025-04-08 18:10:24.350189: Steps 21/138: batch_recall = 69.44, batch_ndcg = 40.99 
2025-04-08 18:10:25.640488: Steps 22/138: batch_recall = 55.20, batch_ndcg = 32.73 
2025-04-08 18:10:26.931722: Steps 23/138: batch_recall = 51.44, batch_ndcg = 30.33 
2025-04-08 18:10:28.218548: Steps 24/138: batch_recall = 55.97, batch_ndcg = 30.76 
2025-04-08 18:10:29.506977: Steps 25/138: batch_recall = 61.43, batch_ndcg = 35.69 
2025-04-08 18:10:30.802514: Steps 26/138: batch_recall = 56.77, batch_ndcg = 32.87 
2025-04-08 18:10:32.098018: Steps 27/138: batch_recall = 54.55, batch_ndcg = 31.88 
2025-04-08 18:10:33.385833: Steps 28/138: batch_recall = 59.25, batch_ndcg = 33.46 
2025-04-08 18:10:34.670325: Steps 29/138: batch_recall = 62.40, batch_ndcg = 33.00 
2025-04-08 18:10:35.963633: Steps 30/138: batch_recall = 58.65, batch_ndcg = 34.02 
2025-04-08 18:10:37.251684: Steps 31/138: batch_recall = 46.93, batch_ndcg = 26.46 
2025-04-08 18:10:38.533689: Steps 32/138: batch_recall = 54.92, batch_ndcg = 32.06 
2025-04-08 18:10:39.812198: Steps 33/138: batch_recall = 61.51, batch_ndcg = 34.03 
2025-04-08 18:10:41.096891: Steps 34/138: batch_recall = 54.80, batch_ndcg = 29.62 
2025-04-08 18:10:42.386030: Steps 35/138: batch_recall = 52.17, batch_ndcg = 29.88 
2025-04-08 18:10:43.676136: Steps 36/138: batch_recall = 46.77, batch_ndcg = 26.78 
2025-04-08 18:10:44.965974: Steps 37/138: batch_recall = 59.32, batch_ndcg = 34.62 
2025-04-08 18:10:46.250231: Steps 38/138: batch_recall = 59.62, batch_ndcg = 32.99 
2025-04-08 18:10:47.537476: Steps 39/138: batch_recall = 67.82, batch_ndcg = 39.10 
2025-04-08 18:10:48.826252: Steps 40/138: batch_recall = 62.66, batch_ndcg = 31.56 
2025-04-08 18:10:50.112717: Steps 41/138: batch_recall = 59.91, batch_ndcg = 34.14 
2025-04-08 18:10:51.394586: Steps 42/138: batch_recall = 53.50, batch_ndcg = 30.08 
2025-04-08 18:10:52.694231: Steps 43/138: batch_recall = 58.74, batch_ndcg = 36.74 
2025-04-08 18:10:53.981021: Steps 44/138: batch_recall = 57.65, batch_ndcg = 31.28 
2025-04-08 18:10:55.269297: Steps 45/138: batch_recall = 61.99, batch_ndcg = 35.63 
2025-04-08 18:10:56.541224: Steps 46/138: batch_recall = 62.45, batch_ndcg = 36.09 
2025-04-08 18:10:57.818086: Steps 47/138: batch_recall = 51.50, batch_ndcg = 31.69 
2025-04-08 18:10:59.095743: Steps 48/138: batch_recall = 64.05, batch_ndcg = 36.02 
2025-04-08 18:11:00.373310: Steps 49/138: batch_recall = 65.53, batch_ndcg = 37.43 
2025-04-08 18:11:01.653901: Steps 50/138: batch_recall = 59.17, batch_ndcg = 31.35 
2025-04-08 18:11:02.934886: Steps 51/138: batch_recall = 62.18, batch_ndcg = 35.95 
2025-04-08 18:11:04.220095: Steps 52/138: batch_recall = 64.27, batch_ndcg = 41.76 
2025-04-08 18:11:05.503039: Steps 53/138: batch_recall = 66.88, batch_ndcg = 34.85 
2025-04-08 18:11:06.785148: Steps 54/138: batch_recall = 65.96, batch_ndcg = 38.17 
2025-04-08 18:11:08.063419: Steps 55/138: batch_recall = 65.06, batch_ndcg = 35.57 
2025-04-08 18:11:09.351345: Steps 56/138: batch_recall = 61.61, batch_ndcg = 36.12 
2025-04-08 18:11:10.642559: Steps 57/138: batch_recall = 59.33, batch_ndcg = 33.11 
2025-04-08 18:11:11.925608: Steps 58/138: batch_recall = 72.98, batch_ndcg = 38.27 
2025-04-08 18:11:13.200465: Steps 59/138: batch_recall = 67.73, batch_ndcg = 39.80 
2025-04-08 18:11:14.480852: Steps 60/138: batch_recall = 67.37, batch_ndcg = 37.53 
2025-04-08 18:11:15.759255: Steps 61/138: batch_recall = 63.84, batch_ndcg = 35.57 
2025-04-08 18:11:17.041542: Steps 62/138: batch_recall = 85.46, batch_ndcg = 44.72 
2025-04-08 18:11:18.330329: Steps 63/138: batch_recall = 73.55, batch_ndcg = 42.44 
2025-04-08 18:11:19.615437: Steps 64/138: batch_recall = 64.50, batch_ndcg = 33.44 
2025-04-08 18:11:20.895316: Steps 65/138: batch_recall = 88.08, batch_ndcg = 47.77 
2025-04-08 18:11:22.176102: Steps 66/138: batch_recall = 69.32, batch_ndcg = 41.48 
2025-04-08 18:11:23.462031: Steps 67/138: batch_recall = 75.14, batch_ndcg = 46.70 
2025-04-08 18:11:24.744406: Steps 68/138: batch_recall = 61.62, batch_ndcg = 33.66 
2025-04-08 18:11:26.020785: Steps 69/138: batch_recall = 89.65, batch_ndcg = 52.55 
2025-04-08 18:11:27.299220: Steps 70/138: batch_recall = 78.05, batch_ndcg = 45.11 
2025-04-08 18:11:28.584024: Steps 71/138: batch_recall = 86.79, batch_ndcg = 50.96 
2025-04-08 18:11:29.867526: Steps 72/138: batch_recall = 85.59, batch_ndcg = 49.52 
2025-04-08 18:11:31.140040: Steps 73/138: batch_recall = 86.09, batch_ndcg = 47.57 
2025-04-08 18:11:32.415293: Steps 74/138: batch_recall = 77.74, batch_ndcg = 48.98 
2025-04-08 18:11:33.689625: Steps 75/138: batch_recall = 82.37, batch_ndcg = 49.05 
2025-04-08 18:11:34.972282: Steps 76/138: batch_recall = 91.60, batch_ndcg = 54.63 
2025-04-08 18:11:36.248983: Steps 77/138: batch_recall = 91.14, batch_ndcg = 51.14 
2025-04-08 18:11:37.536904: Steps 78/138: batch_recall = 89.34, batch_ndcg = 47.91 
2025-04-08 18:11:38.821605: Steps 79/138: batch_recall = 92.36, batch_ndcg = 50.43 
2025-04-08 18:11:40.108014: Steps 80/138: batch_recall = 74.23, batch_ndcg = 39.11 
2025-04-08 18:11:41.391970: Steps 81/138: batch_recall = 83.60, batch_ndcg = 48.50 
2025-04-08 18:11:42.669626: Steps 82/138: batch_recall = 87.84, batch_ndcg = 53.06 
2025-04-08 18:11:43.951072: Steps 83/138: batch_recall = 80.06, batch_ndcg = 47.24 
2025-04-08 18:11:45.212979: Steps 84/138: batch_recall = 101.31, batch_ndcg = 57.93 
2025-04-08 18:11:46.487729: Steps 85/138: batch_recall = 105.10, batch_ndcg = 61.23 
2025-04-08 18:11:47.761958: Steps 86/138: batch_recall = 120.77, batch_ndcg = 71.13 
2025-04-08 18:11:49.032135: Steps 87/138: batch_recall = 103.76, batch_ndcg = 54.82 
2025-04-08 18:11:50.310109: Steps 88/138: batch_recall = 100.95, batch_ndcg = 59.21 
2025-04-08 18:11:51.585999: Steps 89/138: batch_recall = 118.73, batch_ndcg = 68.54 
2025-04-08 18:11:52.866900: Steps 90/138: batch_recall = 101.40, batch_ndcg = 57.74 
2025-04-08 18:11:54.144813: Steps 91/138: batch_recall = 117.87, batch_ndcg = 66.46 
2025-04-08 18:11:55.417192: Steps 92/138: batch_recall = 121.35, batch_ndcg = 64.82 
2025-04-08 18:11:56.697528: Steps 93/138: batch_recall = 121.74, batch_ndcg = 69.11 
2025-04-08 18:11:57.970727: Steps 94/138: batch_recall = 127.19, batch_ndcg = 66.88 
2025-04-08 18:11:59.250780: Steps 95/138: batch_recall = 113.79, batch_ndcg = 66.45 
2025-04-08 18:12:00.521975: Steps 96/138: batch_recall = 129.57, batch_ndcg = 78.35 
2025-04-08 18:12:01.792800: Steps 97/138: batch_recall = 138.08, batch_ndcg = 88.49 
2025-04-08 18:12:03.076439: Steps 98/138: batch_recall = 109.26, batch_ndcg = 63.46 
2025-04-08 18:12:04.347150: Steps 99/138: batch_recall = 125.31, batch_ndcg = 72.62 
2025-04-08 18:12:05.625245: Steps 100/138: batch_recall = 128.41, batch_ndcg = 72.44 
2025-04-08 18:12:06.904608: Steps 101/138: batch_recall = 126.72, batch_ndcg = 70.00 
2025-04-08 18:12:08.183803: Steps 102/138: batch_recall = 123.75, batch_ndcg = 71.38 
2025-04-08 18:12:09.461223: Steps 103/138: batch_recall = 143.28, batch_ndcg = 81.39 
2025-04-08 18:12:10.738527: Steps 104/138: batch_recall = 134.88, batch_ndcg = 79.06 
2025-04-08 18:12:12.015426: Steps 105/138: batch_recall = 122.92, batch_ndcg = 67.43 
2025-04-08 18:12:13.293797: Steps 106/138: batch_recall = 105.03, batch_ndcg = 59.71 
2025-04-08 18:12:14.564781: Steps 107/138: batch_recall = 113.42, batch_ndcg = 64.57 
2025-04-08 18:12:15.838415: Steps 108/138: batch_recall = 117.50, batch_ndcg = 70.69 
2025-04-08 18:12:17.106075: Steps 109/138: batch_recall = 139.33, batch_ndcg = 79.25 
2025-04-08 18:12:18.374845: Steps 110/138: batch_recall = 126.45, batch_ndcg = 66.00 
2025-04-08 18:12:19.644450: Steps 111/138: batch_recall = 138.48, batch_ndcg = 84.07 
2025-04-08 18:12:20.914299: Steps 112/138: batch_recall = 149.38, batch_ndcg = 87.73 
2025-04-08 18:12:22.190357: Steps 113/138: batch_recall = 124.49, batch_ndcg = 69.01 
2025-04-08 18:12:23.472343: Steps 114/138: batch_recall = 124.20, batch_ndcg = 72.85 
2025-04-08 18:12:24.745004: Steps 115/138: batch_recall = 119.77, batch_ndcg = 63.08 
2025-04-08 18:12:26.022222: Steps 116/138: batch_recall = 124.70, batch_ndcg = 67.04 
2025-04-08 18:12:27.297173: Steps 117/138: batch_recall = 113.31, batch_ndcg = 66.21 
2025-04-08 18:12:28.568756: Steps 118/138: batch_recall = 123.83, batch_ndcg = 68.64 
2025-04-08 18:12:29.842040: Steps 119/138: batch_recall = 139.82, batch_ndcg = 76.12 
2025-04-08 18:12:31.116035: Steps 120/138: batch_recall = 124.83, batch_ndcg = 70.26 
2025-04-08 18:12:32.386853: Steps 121/138: batch_recall = 143.36, batch_ndcg = 77.12 
2025-04-08 18:12:33.651426: Steps 122/138: batch_recall = 146.60, batch_ndcg = 81.44 
2025-04-08 18:12:34.926459: Steps 123/138: batch_recall = 127.76, batch_ndcg = 72.90 
2025-04-08 18:12:36.199040: Steps 124/138: batch_recall = 154.29, batch_ndcg = 93.49 
2025-04-08 18:12:37.465576: Steps 125/138: batch_recall = 134.63, batch_ndcg = 73.74 
2025-04-08 18:12:38.742624: Steps 126/138: batch_recall = 158.44, batch_ndcg = 87.79 
2025-04-08 18:12:40.014912: Steps 127/138: batch_recall = 146.42, batch_ndcg = 82.41 
2025-04-08 18:12:41.289396: Steps 128/138: batch_recall = 130.36, batch_ndcg = 73.28 
2025-04-08 18:12:42.559041: Steps 129/138: batch_recall = 159.64, batch_ndcg = 91.42 
2025-04-08 18:12:43.830483: Steps 130/138: batch_recall = 135.43, batch_ndcg = 70.41 
2025-04-08 18:12:45.108515: Steps 131/138: batch_recall = 154.55, batch_ndcg = 87.98 
2025-04-08 18:12:46.373094: Steps 132/138: batch_recall = 148.33, batch_ndcg = 84.42 
2025-04-08 18:12:47.635805: Steps 133/138: batch_recall = 151.02, batch_ndcg = 85.19 
2025-04-08 18:12:48.906718: Steps 134/138: batch_recall = 147.38, batch_ndcg = 83.60 
2025-04-08 18:12:50.167959: Steps 135/138: batch_recall = 169.19, batch_ndcg = 97.07 
2025-04-08 18:12:51.431521: Steps 136/138: batch_recall = 155.14, batch_ndcg = 79.87 
2025-04-08 18:12:52.700071: Steps 137/138: batch_recall = 139.20, batch_ndcg = 87.28 
2025-04-08 18:12:52.700816: Epoch 26/1000, Test: Recall = 0.1766, NDCG = 0.1008  

2025-04-08 18:12:54.475394: Training Step 0/354: batchLoss = 0.6189, diffLoss = 3.0286, kgLoss = 0.0164
2025-04-08 18:12:56.096983: Training Step 1/354: batchLoss = 0.5487, diffLoss = 2.6870, kgLoss = 0.0141
2025-04-08 18:12:57.723948: Training Step 2/354: batchLoss = 0.6628, diffLoss = 3.2446, kgLoss = 0.0174
2025-04-08 18:12:59.348431: Training Step 3/354: batchLoss = 0.6124, diffLoss = 3.0011, kgLoss = 0.0152
2025-04-08 18:13:00.972583: Training Step 4/354: batchLoss = 0.4300, diffLoss = 2.0992, kgLoss = 0.0126
2025-04-08 18:13:02.604404: Training Step 5/354: batchLoss = 0.5531, diffLoss = 2.7122, kgLoss = 0.0133
2025-04-08 18:13:04.224482: Training Step 6/354: batchLoss = 0.5947, diffLoss = 2.9091, kgLoss = 0.0161
2025-04-08 18:13:05.842286: Training Step 7/354: batchLoss = 0.7358, diffLoss = 3.5931, kgLoss = 0.0215
2025-04-08 18:13:07.461319: Training Step 8/354: batchLoss = 0.5374, diffLoss = 2.6258, kgLoss = 0.0152
2025-04-08 18:13:09.090171: Training Step 9/354: batchLoss = 0.5569, diffLoss = 2.7283, kgLoss = 0.0140
2025-04-08 18:13:10.712747: Training Step 10/354: batchLoss = 0.4414, diffLoss = 2.1597, kgLoss = 0.0119
2025-04-08 18:13:12.328271: Training Step 11/354: batchLoss = 0.5287, diffLoss = 2.5907, kgLoss = 0.0132
2025-04-08 18:13:13.945982: Training Step 12/354: batchLoss = 0.6495, diffLoss = 3.1848, kgLoss = 0.0157
2025-04-08 18:13:15.562781: Training Step 13/354: batchLoss = 0.5275, diffLoss = 2.5824, kgLoss = 0.0138
2025-04-08 18:13:17.186843: Training Step 14/354: batchLoss = 0.4994, diffLoss = 2.4404, kgLoss = 0.0142
2025-04-08 18:13:18.812875: Training Step 15/354: batchLoss = 0.4625, diffLoss = 2.2636, kgLoss = 0.0123
2025-04-08 18:13:20.431869: Training Step 16/354: batchLoss = 0.5351, diffLoss = 2.6235, kgLoss = 0.0130
2025-04-08 18:13:22.050365: Training Step 17/354: batchLoss = 0.5818, diffLoss = 2.8561, kgLoss = 0.0133
2025-04-08 18:13:23.670218: Training Step 18/354: batchLoss = 0.7140, diffLoss = 3.4990, kgLoss = 0.0177
2025-04-08 18:13:25.286906: Training Step 19/354: batchLoss = 0.5360, diffLoss = 2.6221, kgLoss = 0.0145
2025-04-08 18:13:26.906108: Training Step 20/354: batchLoss = 0.4386, diffLoss = 2.1444, kgLoss = 0.0121
2025-04-08 18:13:28.524784: Training Step 21/354: batchLoss = 0.7554, diffLoss = 3.7050, kgLoss = 0.0180
2025-04-08 18:13:30.150211: Training Step 22/354: batchLoss = 0.5872, diffLoss = 2.8758, kgLoss = 0.0151
2025-04-08 18:13:31.769822: Training Step 23/354: batchLoss = 0.6399, diffLoss = 3.1272, kgLoss = 0.0181
2025-04-08 18:13:33.393147: Training Step 24/354: batchLoss = 0.7139, diffLoss = 3.4996, kgLoss = 0.0175
2025-04-08 18:13:35.012878: Training Step 25/354: batchLoss = 0.4482, diffLoss = 2.1888, kgLoss = 0.0131
2025-04-08 18:13:36.629930: Training Step 26/354: batchLoss = 0.6194, diffLoss = 3.0386, kgLoss = 0.0146
2025-04-08 18:13:38.244823: Training Step 27/354: batchLoss = 0.7129, diffLoss = 3.4943, kgLoss = 0.0176
2025-04-08 18:13:39.860827: Training Step 28/354: batchLoss = 0.6041, diffLoss = 2.9525, kgLoss = 0.0171
2025-04-08 18:13:41.481647: Training Step 29/354: batchLoss = 0.4294, diffLoss = 2.1029, kgLoss = 0.0111
2025-04-08 18:13:43.105400: Training Step 30/354: batchLoss = 0.6757, diffLoss = 3.3145, kgLoss = 0.0160
2025-04-08 18:13:44.727636: Training Step 31/354: batchLoss = 0.6366, diffLoss = 3.1183, kgLoss = 0.0162
2025-04-08 18:13:46.352842: Training Step 32/354: batchLoss = 0.6231, diffLoss = 3.0549, kgLoss = 0.0151
2025-04-08 18:13:47.970901: Training Step 33/354: batchLoss = 0.4972, diffLoss = 2.4372, kgLoss = 0.0122
2025-04-08 18:13:49.602327: Training Step 34/354: batchLoss = 0.5162, diffLoss = 2.5273, kgLoss = 0.0135
2025-04-08 18:13:51.223753: Training Step 35/354: batchLoss = 0.5657, diffLoss = 2.7641, kgLoss = 0.0160
2025-04-08 18:13:52.842080: Training Step 36/354: batchLoss = 0.5356, diffLoss = 2.6161, kgLoss = 0.0155
2025-04-08 18:13:54.454390: Training Step 37/354: batchLoss = 0.4820, diffLoss = 2.3552, kgLoss = 0.0138
2025-04-08 18:13:56.067967: Training Step 38/354: batchLoss = 0.4784, diffLoss = 2.3375, kgLoss = 0.0136
2025-04-08 18:13:57.682766: Training Step 39/354: batchLoss = 0.6428, diffLoss = 3.1548, kgLoss = 0.0148
2025-04-08 18:13:59.307230: Training Step 40/354: batchLoss = 0.5373, diffLoss = 2.6265, kgLoss = 0.0151
2025-04-08 18:14:00.928235: Training Step 41/354: batchLoss = 0.5045, diffLoss = 2.4675, kgLoss = 0.0138
2025-04-08 18:14:02.551831: Training Step 42/354: batchLoss = 0.4925, diffLoss = 2.4076, kgLoss = 0.0137
2025-04-08 18:14:04.171385: Training Step 43/354: batchLoss = 0.4917, diffLoss = 2.4062, kgLoss = 0.0131
2025-04-08 18:14:05.791077: Training Step 44/354: batchLoss = 0.5127, diffLoss = 2.5108, kgLoss = 0.0132
2025-04-08 18:14:07.408766: Training Step 45/354: batchLoss = 0.5677, diffLoss = 2.7836, kgLoss = 0.0137
2025-04-08 18:14:09.030726: Training Step 46/354: batchLoss = 0.5356, diffLoss = 2.6214, kgLoss = 0.0141
2025-04-08 18:14:10.646960: Training Step 47/354: batchLoss = 0.5309, diffLoss = 2.6004, kgLoss = 0.0135
2025-04-08 18:14:12.263266: Training Step 48/354: batchLoss = 0.6336, diffLoss = 3.1035, kgLoss = 0.0161
2025-04-08 18:14:13.881808: Training Step 49/354: batchLoss = 0.5020, diffLoss = 2.4552, kgLoss = 0.0137
2025-04-08 18:14:15.494691: Training Step 50/354: batchLoss = 0.4788, diffLoss = 2.3432, kgLoss = 0.0126
2025-04-08 18:14:17.113578: Training Step 51/354: batchLoss = 0.5319, diffLoss = 2.6041, kgLoss = 0.0138
2025-04-08 18:14:18.738847: Training Step 52/354: batchLoss = 0.6319, diffLoss = 3.0961, kgLoss = 0.0159
2025-04-08 18:14:20.363917: Training Step 53/354: batchLoss = 0.5337, diffLoss = 2.6075, kgLoss = 0.0152
2025-04-08 18:14:21.983255: Training Step 54/354: batchLoss = 0.5397, diffLoss = 2.6433, kgLoss = 0.0139
2025-04-08 18:14:23.598002: Training Step 55/354: batchLoss = 0.4417, diffLoss = 2.1678, kgLoss = 0.0102
2025-04-08 18:14:25.212016: Training Step 56/354: batchLoss = 0.4806, diffLoss = 2.3485, kgLoss = 0.0136
2025-04-08 18:14:26.826279: Training Step 57/354: batchLoss = 0.4979, diffLoss = 2.4399, kgLoss = 0.0124
2025-04-08 18:14:28.432685: Training Step 58/354: batchLoss = 0.5236, diffLoss = 2.5658, kgLoss = 0.0130
2025-04-08 18:14:30.046407: Training Step 59/354: batchLoss = 0.5927, diffLoss = 2.9080, kgLoss = 0.0138
2025-04-08 18:14:31.663247: Training Step 60/354: batchLoss = 0.5331, diffLoss = 2.6140, kgLoss = 0.0129
2025-04-08 18:14:33.282596: Training Step 61/354: batchLoss = 0.5617, diffLoss = 2.7542, kgLoss = 0.0136
2025-04-08 18:14:34.905623: Training Step 62/354: batchLoss = 0.4795, diffLoss = 2.3486, kgLoss = 0.0122
2025-04-08 18:14:36.530398: Training Step 63/354: batchLoss = 0.5731, diffLoss = 2.8108, kgLoss = 0.0137
2025-04-08 18:14:38.151236: Training Step 64/354: batchLoss = 0.5523, diffLoss = 2.7080, kgLoss = 0.0133
2025-04-08 18:14:39.771386: Training Step 65/354: batchLoss = 0.6501, diffLoss = 3.1861, kgLoss = 0.0161
2025-04-08 18:14:41.389359: Training Step 66/354: batchLoss = 0.4960, diffLoss = 2.4248, kgLoss = 0.0137
2025-04-08 18:14:43.008657: Training Step 67/354: batchLoss = 0.5400, diffLoss = 2.6342, kgLoss = 0.0165
2025-04-08 18:14:44.624568: Training Step 68/354: batchLoss = 0.6153, diffLoss = 3.0153, kgLoss = 0.0153
2025-04-08 18:14:46.247644: Training Step 69/354: batchLoss = 0.7655, diffLoss = 3.7563, kgLoss = 0.0178
2025-04-08 18:14:47.867852: Training Step 70/354: batchLoss = 0.4841, diffLoss = 2.3693, kgLoss = 0.0128
2025-04-08 18:14:49.486971: Training Step 71/354: batchLoss = 0.6674, diffLoss = 3.2728, kgLoss = 0.0160
2025-04-08 18:14:51.103135: Training Step 72/354: batchLoss = 0.6766, diffLoss = 3.3121, kgLoss = 0.0177
2025-04-08 18:14:52.721980: Training Step 73/354: batchLoss = 0.5422, diffLoss = 2.6558, kgLoss = 0.0138
2025-04-08 18:14:54.356220: Training Step 74/354: batchLoss = 0.4916, diffLoss = 2.4029, kgLoss = 0.0138
2025-04-08 18:14:55.978783: Training Step 75/354: batchLoss = 0.6298, diffLoss = 3.0818, kgLoss = 0.0168
2025-04-08 18:14:57.596056: Training Step 76/354: batchLoss = 0.6026, diffLoss = 2.9510, kgLoss = 0.0155
2025-04-08 18:14:59.216290: Training Step 77/354: batchLoss = 0.6036, diffLoss = 2.9561, kgLoss = 0.0155
2025-04-08 18:15:00.833357: Training Step 78/354: batchLoss = 0.6327, diffLoss = 3.0995, kgLoss = 0.0161
2025-04-08 18:15:02.452453: Training Step 79/354: batchLoss = 0.4551, diffLoss = 2.2236, kgLoss = 0.0130
2025-04-08 18:15:04.071260: Training Step 80/354: batchLoss = 0.5306, diffLoss = 2.5996, kgLoss = 0.0134
2025-04-08 18:15:05.697043: Training Step 81/354: batchLoss = 0.5594, diffLoss = 2.7319, kgLoss = 0.0163
2025-04-08 18:15:07.327091: Training Step 82/354: batchLoss = 0.5383, diffLoss = 2.6296, kgLoss = 0.0155
2025-04-08 18:15:08.945663: Training Step 83/354: batchLoss = 0.4978, diffLoss = 2.4322, kgLoss = 0.0142
2025-04-08 18:15:10.567394: Training Step 84/354: batchLoss = 0.4730, diffLoss = 2.3166, kgLoss = 0.0121
2025-04-08 18:15:12.183320: Training Step 85/354: batchLoss = 0.5574, diffLoss = 2.7248, kgLoss = 0.0155
2025-04-08 18:15:13.796106: Training Step 86/354: batchLoss = 0.5391, diffLoss = 2.6381, kgLoss = 0.0143
2025-04-08 18:15:15.410891: Training Step 87/354: batchLoss = 0.4914, diffLoss = 2.4086, kgLoss = 0.0121
2025-04-08 18:15:17.031755: Training Step 88/354: batchLoss = 0.5750, diffLoss = 2.8104, kgLoss = 0.0161
2025-04-08 18:15:18.652057: Training Step 89/354: batchLoss = 0.6472, diffLoss = 3.1728, kgLoss = 0.0158
2025-04-08 18:15:20.265375: Training Step 90/354: batchLoss = 0.6807, diffLoss = 3.3320, kgLoss = 0.0178
2025-04-08 18:15:21.891250: Training Step 91/354: batchLoss = 0.6495, diffLoss = 3.1863, kgLoss = 0.0154
2025-04-08 18:15:23.514247: Training Step 92/354: batchLoss = 0.5145, diffLoss = 2.5016, kgLoss = 0.0178
2025-04-08 18:15:25.137349: Training Step 93/354: batchLoss = 0.5025, diffLoss = 2.4585, kgLoss = 0.0135
2025-04-08 18:15:26.757439: Training Step 94/354: batchLoss = 0.6883, diffLoss = 3.3741, kgLoss = 0.0169
2025-04-08 18:15:28.385327: Training Step 95/354: batchLoss = 0.6139, diffLoss = 3.0046, kgLoss = 0.0162
2025-04-08 18:15:30.006503: Training Step 96/354: batchLoss = 0.4965, diffLoss = 2.4269, kgLoss = 0.0139
2025-04-08 18:15:31.620242: Training Step 97/354: batchLoss = 0.5447, diffLoss = 2.6666, kgLoss = 0.0142
2025-04-08 18:15:33.236253: Training Step 98/354: batchLoss = 0.7320, diffLoss = 3.5860, kgLoss = 0.0185
2025-04-08 18:15:34.853118: Training Step 99/354: batchLoss = 0.5752, diffLoss = 2.8162, kgLoss = 0.0150
2025-04-08 18:15:36.478811: Training Step 100/354: batchLoss = 0.5865, diffLoss = 2.8702, kgLoss = 0.0155
2025-04-08 18:15:38.101616: Training Step 101/354: batchLoss = 0.7806, diffLoss = 3.8244, kgLoss = 0.0197
2025-04-08 18:15:39.728859: Training Step 102/354: batchLoss = 0.4867, diffLoss = 2.3834, kgLoss = 0.0125
2025-04-08 18:15:41.349849: Training Step 103/354: batchLoss = 0.6210, diffLoss = 2.9988, kgLoss = 0.0265
2025-04-08 18:15:42.976412: Training Step 104/354: batchLoss = 0.6432, diffLoss = 3.1475, kgLoss = 0.0171
2025-04-08 18:15:44.592390: Training Step 105/354: batchLoss = 0.5202, diffLoss = 2.5454, kgLoss = 0.0140
2025-04-08 18:15:46.206102: Training Step 106/354: batchLoss = 0.5083, diffLoss = 2.4861, kgLoss = 0.0138
2025-04-08 18:15:47.825269: Training Step 107/354: batchLoss = 0.5483, diffLoss = 2.6869, kgLoss = 0.0137
2025-04-08 18:15:49.439636: Training Step 108/354: batchLoss = 0.5776, diffLoss = 2.8296, kgLoss = 0.0147
2025-04-08 18:15:51.059461: Training Step 109/354: batchLoss = 0.5074, diffLoss = 2.4831, kgLoss = 0.0134
2025-04-08 18:15:52.675035: Training Step 110/354: batchLoss = 0.5772, diffLoss = 2.8265, kgLoss = 0.0149
2025-04-08 18:15:54.298267: Training Step 111/354: batchLoss = 0.4810, diffLoss = 2.3548, kgLoss = 0.0125
2025-04-08 18:15:55.924784: Training Step 112/354: batchLoss = 0.5209, diffLoss = 2.5521, kgLoss = 0.0131
2025-04-08 18:15:57.547808: Training Step 113/354: batchLoss = 0.4923, diffLoss = 2.4097, kgLoss = 0.0129
2025-04-08 18:15:59.167847: Training Step 114/354: batchLoss = 0.6309, diffLoss = 3.0905, kgLoss = 0.0160
2025-04-08 18:16:00.779771: Training Step 115/354: batchLoss = 0.6170, diffLoss = 3.0157, kgLoss = 0.0173
2025-04-08 18:16:02.394169: Training Step 116/354: batchLoss = 0.6397, diffLoss = 3.1428, kgLoss = 0.0139
2025-04-08 18:16:04.008506: Training Step 117/354: batchLoss = 0.6155, diffLoss = 3.0182, kgLoss = 0.0149
2025-04-08 18:16:05.627773: Training Step 118/354: batchLoss = 0.6521, diffLoss = 3.1987, kgLoss = 0.0154
2025-04-08 18:16:07.254611: Training Step 119/354: batchLoss = 0.5183, diffLoss = 2.5286, kgLoss = 0.0158
2025-04-08 18:16:08.872492: Training Step 120/354: batchLoss = 0.4572, diffLoss = 2.2323, kgLoss = 0.0134
2025-04-08 18:16:10.488608: Training Step 121/354: batchLoss = 0.4823, diffLoss = 2.3510, kgLoss = 0.0152
2025-04-08 18:16:12.108035: Training Step 122/354: batchLoss = 0.5205, diffLoss = 2.5473, kgLoss = 0.0138
2025-04-08 18:16:13.727039: Training Step 123/354: batchLoss = 0.5222, diffLoss = 2.5575, kgLoss = 0.0134
2025-04-08 18:16:15.346213: Training Step 124/354: batchLoss = 0.4870, diffLoss = 2.3793, kgLoss = 0.0139
2025-04-08 18:16:16.974350: Training Step 125/354: batchLoss = 0.6236, diffLoss = 3.0549, kgLoss = 0.0157
2025-04-08 18:16:18.589641: Training Step 126/354: batchLoss = 0.6766, diffLoss = 3.3099, kgLoss = 0.0183
2025-04-08 18:16:20.207922: Training Step 127/354: batchLoss = 0.6807, diffLoss = 3.3387, kgLoss = 0.0162
2025-04-08 18:16:21.825349: Training Step 128/354: batchLoss = 0.4583, diffLoss = 2.2395, kgLoss = 0.0130
2025-04-08 18:16:23.449191: Training Step 129/354: batchLoss = 0.5016, diffLoss = 2.4440, kgLoss = 0.0160
2025-04-08 18:16:25.071397: Training Step 130/354: batchLoss = 0.5316, diffLoss = 2.6017, kgLoss = 0.0141
2025-04-08 18:16:26.692232: Training Step 131/354: batchLoss = 0.5159, diffLoss = 2.5183, kgLoss = 0.0153
2025-04-08 18:16:28.313886: Training Step 132/354: batchLoss = 0.6571, diffLoss = 3.2217, kgLoss = 0.0159
2025-04-08 18:16:29.930953: Training Step 133/354: batchLoss = 0.5525, diffLoss = 2.7095, kgLoss = 0.0133
2025-04-08 18:16:31.551296: Training Step 134/354: batchLoss = 0.5479, diffLoss = 2.6746, kgLoss = 0.0162
2025-04-08 18:16:33.167794: Training Step 135/354: batchLoss = 0.4373, diffLoss = 2.1421, kgLoss = 0.0111
2025-04-08 18:16:34.781883: Training Step 136/354: batchLoss = 0.5233, diffLoss = 2.5577, kgLoss = 0.0147
2025-04-08 18:16:36.398505: Training Step 137/354: batchLoss = 0.4765, diffLoss = 2.3340, kgLoss = 0.0121
2025-04-08 18:16:38.011909: Training Step 138/354: batchLoss = 0.5319, diffLoss = 2.6080, kgLoss = 0.0129
2025-04-08 18:16:39.640568: Training Step 139/354: batchLoss = 0.4905, diffLoss = 2.4031, kgLoss = 0.0124
2025-04-08 18:16:41.266190: Training Step 140/354: batchLoss = 0.5760, diffLoss = 2.8249, kgLoss = 0.0137
2025-04-08 18:16:42.887087: Training Step 141/354: batchLoss = 0.6465, diffLoss = 3.1737, kgLoss = 0.0147
2025-04-08 18:16:44.517173: Training Step 142/354: batchLoss = 0.4946, diffLoss = 2.4283, kgLoss = 0.0112
2025-04-08 18:16:46.134262: Training Step 143/354: batchLoss = 0.5078, diffLoss = 2.4797, kgLoss = 0.0148
2025-04-08 18:16:47.749867: Training Step 144/354: batchLoss = 0.5168, diffLoss = 2.5279, kgLoss = 0.0141
2025-04-08 18:16:49.362494: Training Step 145/354: batchLoss = 0.6294, diffLoss = 3.0850, kgLoss = 0.0155
2025-04-08 18:16:50.976916: Training Step 146/354: batchLoss = 0.4664, diffLoss = 2.2861, kgLoss = 0.0115
2025-04-08 18:16:52.594041: Training Step 147/354: batchLoss = 0.4567, diffLoss = 2.2257, kgLoss = 0.0144
2025-04-08 18:16:54.209153: Training Step 148/354: batchLoss = 0.5699, diffLoss = 2.7839, kgLoss = 0.0164
2025-04-08 18:16:55.829808: Training Step 149/354: batchLoss = 0.5380, diffLoss = 2.6392, kgLoss = 0.0127
2025-04-08 18:16:57.449129: Training Step 150/354: batchLoss = 0.6682, diffLoss = 3.2715, kgLoss = 0.0174
2025-04-08 18:16:59.071202: Training Step 151/354: batchLoss = 0.5708, diffLoss = 2.7973, kgLoss = 0.0142
2025-04-08 18:17:00.688765: Training Step 152/354: batchLoss = 0.5176, diffLoss = 2.4913, kgLoss = 0.0242
2025-04-08 18:17:02.310566: Training Step 153/354: batchLoss = 0.6065, diffLoss = 2.9745, kgLoss = 0.0145
2025-04-08 18:17:03.934117: Training Step 154/354: batchLoss = 0.5243, diffLoss = 2.5623, kgLoss = 0.0148
2025-04-08 18:17:05.553207: Training Step 155/354: batchLoss = 0.6387, diffLoss = 3.1264, kgLoss = 0.0168
2025-04-08 18:17:07.165731: Training Step 156/354: batchLoss = 0.5360, diffLoss = 2.6219, kgLoss = 0.0145
2025-04-08 18:17:08.782729: Training Step 157/354: batchLoss = 0.6012, diffLoss = 2.9408, kgLoss = 0.0163
2025-04-08 18:17:10.406210: Training Step 158/354: batchLoss = 0.5533, diffLoss = 2.7050, kgLoss = 0.0154
2025-04-08 18:17:12.025286: Training Step 159/354: batchLoss = 0.5851, diffLoss = 2.8642, kgLoss = 0.0154
2025-04-08 18:17:13.641802: Training Step 160/354: batchLoss = 0.5634, diffLoss = 2.7596, kgLoss = 0.0144
2025-04-08 18:17:15.259433: Training Step 161/354: batchLoss = 0.5217, diffLoss = 2.5550, kgLoss = 0.0134
2025-04-08 18:17:16.878619: Training Step 162/354: batchLoss = 0.4361, diffLoss = 2.1283, kgLoss = 0.0130
2025-04-08 18:17:18.497387: Training Step 163/354: batchLoss = 0.6139, diffLoss = 3.0067, kgLoss = 0.0157
2025-04-08 18:17:20.112423: Training Step 164/354: batchLoss = 0.5916, diffLoss = 2.8968, kgLoss = 0.0153
2025-04-08 18:17:21.726114: Training Step 165/354: batchLoss = 0.5846, diffLoss = 2.8655, kgLoss = 0.0143
2025-04-08 18:17:23.334578: Training Step 166/354: batchLoss = 0.5089, diffLoss = 2.4867, kgLoss = 0.0145
2025-04-08 18:17:24.946013: Training Step 167/354: batchLoss = 0.6015, diffLoss = 2.9503, kgLoss = 0.0143
2025-04-08 18:17:26.566585: Training Step 168/354: batchLoss = 0.5135, diffLoss = 2.5177, kgLoss = 0.0125
2025-04-08 18:17:28.187384: Training Step 169/354: batchLoss = 0.5758, diffLoss = 2.8256, kgLoss = 0.0133
2025-04-08 18:17:29.807091: Training Step 170/354: batchLoss = 0.6056, diffLoss = 2.9639, kgLoss = 0.0161
2025-04-08 18:17:31.427688: Training Step 171/354: batchLoss = 0.4626, diffLoss = 2.2660, kgLoss = 0.0117
2025-04-08 18:17:33.050841: Training Step 172/354: batchLoss = 0.6297, diffLoss = 3.0892, kgLoss = 0.0148
2025-04-08 18:17:34.670257: Training Step 173/354: batchLoss = 0.4331, diffLoss = 2.1151, kgLoss = 0.0126
2025-04-08 18:17:36.285978: Training Step 174/354: batchLoss = 0.6096, diffLoss = 2.9907, kgLoss = 0.0143
2025-04-08 18:17:37.904163: Training Step 175/354: batchLoss = 0.5221, diffLoss = 2.5541, kgLoss = 0.0141
2025-04-08 18:17:39.520582: Training Step 176/354: batchLoss = 0.5858, diffLoss = 2.8656, kgLoss = 0.0158
2025-04-08 18:17:41.138080: Training Step 177/354: batchLoss = 0.5660, diffLoss = 2.7670, kgLoss = 0.0158
2025-04-08 18:17:42.759432: Training Step 178/354: batchLoss = 0.5592, diffLoss = 2.7333, kgLoss = 0.0156
2025-04-08 18:17:44.380967: Training Step 179/354: batchLoss = 0.4308, diffLoss = 2.1075, kgLoss = 0.0116
2025-04-08 18:17:46.003219: Training Step 180/354: batchLoss = 0.5358, diffLoss = 2.6270, kgLoss = 0.0130
2025-04-08 18:17:47.623439: Training Step 181/354: batchLoss = 0.5110, diffLoss = 2.5016, kgLoss = 0.0133
2025-04-08 18:17:49.245758: Training Step 182/354: batchLoss = 0.6044, diffLoss = 2.9461, kgLoss = 0.0190
2025-04-08 18:17:50.865430: Training Step 183/354: batchLoss = 0.6976, diffLoss = 3.4197, kgLoss = 0.0171
2025-04-08 18:17:52.483862: Training Step 184/354: batchLoss = 0.6418, diffLoss = 3.1483, kgLoss = 0.0152
2025-04-08 18:17:54.105387: Training Step 185/354: batchLoss = 0.5137, diffLoss = 2.5045, kgLoss = 0.0160
2025-04-08 18:17:55.718503: Training Step 186/354: batchLoss = 0.5229, diffLoss = 2.5629, kgLoss = 0.0129
2025-04-08 18:17:57.334084: Training Step 187/354: batchLoss = 0.4802, diffLoss = 2.3538, kgLoss = 0.0118
2025-04-08 18:17:58.951697: Training Step 188/354: batchLoss = 0.6774, diffLoss = 3.3243, kgLoss = 0.0157
2025-04-08 18:18:00.569458: Training Step 189/354: batchLoss = 0.5217, diffLoss = 2.5550, kgLoss = 0.0133
2025-04-08 18:18:02.190117: Training Step 190/354: batchLoss = 0.6219, diffLoss = 3.0478, kgLoss = 0.0154
2025-04-08 18:18:03.812313: Training Step 191/354: batchLoss = 0.7308, diffLoss = 3.5784, kgLoss = 0.0189
2025-04-08 18:18:05.432477: Training Step 192/354: batchLoss = 0.5573, diffLoss = 2.7226, kgLoss = 0.0159
2025-04-08 18:18:07.053518: Training Step 193/354: batchLoss = 0.5533, diffLoss = 2.7045, kgLoss = 0.0154
2025-04-08 18:18:08.669490: Training Step 194/354: batchLoss = 0.5565, diffLoss = 2.7229, kgLoss = 0.0149
2025-04-08 18:18:10.281824: Training Step 195/354: batchLoss = 0.5888, diffLoss = 2.8863, kgLoss = 0.0144
2025-04-08 18:18:11.895693: Training Step 196/354: batchLoss = 0.6767, diffLoss = 3.3201, kgLoss = 0.0158
2025-04-08 18:18:13.517270: Training Step 197/354: batchLoss = 0.4549, diffLoss = 2.2277, kgLoss = 0.0117
2025-04-08 18:18:15.136384: Training Step 198/354: batchLoss = 0.4482, diffLoss = 2.1952, kgLoss = 0.0114
2025-04-08 18:18:16.753092: Training Step 199/354: batchLoss = 0.6074, diffLoss = 2.9765, kgLoss = 0.0151
2025-04-08 18:18:18.375994: Training Step 200/354: batchLoss = 0.5540, diffLoss = 2.7058, kgLoss = 0.0161
2025-04-08 18:18:20.002502: Training Step 201/354: batchLoss = 0.5377, diffLoss = 2.6286, kgLoss = 0.0150
2025-04-08 18:18:21.625067: Training Step 202/354: batchLoss = 0.4747, diffLoss = 2.3190, kgLoss = 0.0136
2025-04-08 18:18:23.255158: Training Step 203/354: batchLoss = 0.5919, diffLoss = 2.9004, kgLoss = 0.0148
2025-04-08 18:18:24.871477: Training Step 204/354: batchLoss = 0.5062, diffLoss = 2.4755, kgLoss = 0.0139
2025-04-08 18:18:26.497836: Training Step 205/354: batchLoss = 0.5981, diffLoss = 2.9310, kgLoss = 0.0149
2025-04-08 18:18:28.116529: Training Step 206/354: batchLoss = 0.5442, diffLoss = 2.6630, kgLoss = 0.0145
2025-04-08 18:18:29.732871: Training Step 207/354: batchLoss = 0.5659, diffLoss = 2.7743, kgLoss = 0.0138
2025-04-08 18:18:31.358166: Training Step 208/354: batchLoss = 0.6050, diffLoss = 2.9637, kgLoss = 0.0154
2025-04-08 18:18:32.980078: Training Step 209/354: batchLoss = 0.5805, diffLoss = 2.8328, kgLoss = 0.0175
2025-04-08 18:18:34.594369: Training Step 210/354: batchLoss = 0.6545, diffLoss = 3.2134, kgLoss = 0.0147
2025-04-08 18:18:36.218925: Training Step 211/354: batchLoss = 0.5770, diffLoss = 2.8211, kgLoss = 0.0160
2025-04-08 18:18:37.848665: Training Step 212/354: batchLoss = 0.5482, diffLoss = 2.6886, kgLoss = 0.0131
2025-04-08 18:18:39.469216: Training Step 213/354: batchLoss = 0.5314, diffLoss = 2.5999, kgLoss = 0.0143
2025-04-08 18:18:41.084851: Training Step 214/354: batchLoss = 0.5599, diffLoss = 2.7414, kgLoss = 0.0145
2025-04-08 18:18:42.703091: Training Step 215/354: batchLoss = 0.5000, diffLoss = 2.4476, kgLoss = 0.0131
2025-04-08 18:18:44.317115: Training Step 216/354: batchLoss = 0.5116, diffLoss = 2.5012, kgLoss = 0.0141
2025-04-08 18:18:45.930016: Training Step 217/354: batchLoss = 0.6526, diffLoss = 3.1938, kgLoss = 0.0173
2025-04-08 18:18:47.550524: Training Step 218/354: batchLoss = 0.6609, diffLoss = 3.2417, kgLoss = 0.0157
2025-04-08 18:18:49.172963: Training Step 219/354: batchLoss = 0.4375, diffLoss = 2.1335, kgLoss = 0.0135
2025-04-08 18:18:50.865511: Training Step 220/354: batchLoss = 0.4980, diffLoss = 2.4396, kgLoss = 0.0127
2025-04-08 18:18:52.487546: Training Step 221/354: batchLoss = 0.5897, diffLoss = 2.8913, kgLoss = 0.0143
2025-04-08 18:18:54.110098: Training Step 222/354: batchLoss = 0.5292, diffLoss = 2.5929, kgLoss = 0.0133
2025-04-08 18:18:55.730400: Training Step 223/354: batchLoss = 0.5417, diffLoss = 2.6575, kgLoss = 0.0127
2025-04-08 18:18:57.348412: Training Step 224/354: batchLoss = 0.6250, diffLoss = 3.0586, kgLoss = 0.0166
2025-04-08 18:18:58.949192: Training Step 225/354: batchLoss = 0.5448, diffLoss = 2.6615, kgLoss = 0.0157
2025-04-08 18:19:00.553196: Training Step 226/354: batchLoss = 0.4863, diffLoss = 2.3660, kgLoss = 0.0164
2025-04-08 18:19:02.159206: Training Step 227/354: batchLoss = 0.6971, diffLoss = 3.4172, kgLoss = 0.0170
2025-04-08 18:19:03.776809: Training Step 228/354: batchLoss = 0.5401, diffLoss = 2.6401, kgLoss = 0.0151
2025-04-08 18:19:05.392823: Training Step 229/354: batchLoss = 0.5451, diffLoss = 2.6731, kgLoss = 0.0131
2025-04-08 18:19:07.010963: Training Step 230/354: batchLoss = 0.4757, diffLoss = 2.3221, kgLoss = 0.0142
2025-04-08 18:19:08.633136: Training Step 231/354: batchLoss = 0.4983, diffLoss = 2.4395, kgLoss = 0.0130
2025-04-08 18:19:10.254179: Training Step 232/354: batchLoss = 0.5492, diffLoss = 2.6874, kgLoss = 0.0147
2025-04-08 18:19:11.875939: Training Step 233/354: batchLoss = 0.5862, diffLoss = 2.8756, kgLoss = 0.0138
2025-04-08 18:19:13.492739: Training Step 234/354: batchLoss = 0.6279, diffLoss = 3.0760, kgLoss = 0.0158
2025-04-08 18:19:15.107445: Training Step 235/354: batchLoss = 0.7297, diffLoss = 3.5737, kgLoss = 0.0187
2025-04-08 18:19:16.723914: Training Step 236/354: batchLoss = 0.6423, diffLoss = 3.1412, kgLoss = 0.0176
2025-04-08 18:19:18.339212: Training Step 237/354: batchLoss = 0.5566, diffLoss = 2.7257, kgLoss = 0.0144
2025-04-08 18:19:19.962763: Training Step 238/354: batchLoss = 0.4577, diffLoss = 2.2363, kgLoss = 0.0131
2025-04-08 18:19:21.586244: Training Step 239/354: batchLoss = 0.4344, diffLoss = 2.1283, kgLoss = 0.0109
2025-04-08 18:19:23.205642: Training Step 240/354: batchLoss = 0.5162, diffLoss = 2.5213, kgLoss = 0.0150
2025-04-08 18:19:24.829479: Training Step 241/354: batchLoss = 0.4937, diffLoss = 2.4175, kgLoss = 0.0127
2025-04-08 18:19:26.446926: Training Step 242/354: batchLoss = 0.4697, diffLoss = 2.2989, kgLoss = 0.0124
2025-04-08 18:19:28.067411: Training Step 243/354: batchLoss = 0.6427, diffLoss = 3.1487, kgLoss = 0.0163
2025-04-08 18:19:29.688148: Training Step 244/354: batchLoss = 0.7135, diffLoss = 3.4915, kgLoss = 0.0190
2025-04-08 18:19:31.301982: Training Step 245/354: batchLoss = 0.9211, diffLoss = 4.5127, kgLoss = 0.0232
2025-04-08 18:19:32.913356: Training Step 246/354: batchLoss = 0.5113, diffLoss = 2.4973, kgLoss = 0.0148
2025-04-08 18:19:34.528217: Training Step 247/354: batchLoss = 0.5108, diffLoss = 2.5039, kgLoss = 0.0126
2025-04-08 18:19:36.144008: Training Step 248/354: batchLoss = 0.5005, diffLoss = 2.4520, kgLoss = 0.0126
2025-04-08 18:19:37.762972: Training Step 249/354: batchLoss = 0.4537, diffLoss = 2.2182, kgLoss = 0.0126
2025-04-08 18:19:39.383267: Training Step 250/354: batchLoss = 0.5722, diffLoss = 2.8064, kgLoss = 0.0136
2025-04-08 18:19:41.003935: Training Step 251/354: batchLoss = 0.6670, diffLoss = 3.2740, kgLoss = 0.0152
2025-04-08 18:19:42.628933: Training Step 252/354: batchLoss = 0.5095, diffLoss = 2.4946, kgLoss = 0.0132
2025-04-08 18:19:44.244884: Training Step 253/354: batchLoss = 0.5767, diffLoss = 2.8195, kgLoss = 0.0159
2025-04-08 18:19:45.863623: Training Step 254/354: batchLoss = 0.4767, diffLoss = 2.3336, kgLoss = 0.0125
2025-04-08 18:19:47.480050: Training Step 255/354: batchLoss = 0.4423, diffLoss = 2.1625, kgLoss = 0.0123
2025-04-08 18:19:49.097308: Training Step 256/354: batchLoss = 0.5036, diffLoss = 2.4685, kgLoss = 0.0124
2025-04-08 18:19:50.719053: Training Step 257/354: batchLoss = 0.6256, diffLoss = 3.0709, kgLoss = 0.0143
2025-04-08 18:19:52.338010: Training Step 258/354: batchLoss = 0.5063, diffLoss = 2.4701, kgLoss = 0.0154
2025-04-08 18:19:53.958842: Training Step 259/354: batchLoss = 0.5081, diffLoss = 2.4889, kgLoss = 0.0129
2025-04-08 18:19:55.576755: Training Step 260/354: batchLoss = 0.4967, diffLoss = 2.4323, kgLoss = 0.0128
2025-04-08 18:19:57.193948: Training Step 261/354: batchLoss = 0.4678, diffLoss = 2.2855, kgLoss = 0.0134
2025-04-08 18:19:58.813527: Training Step 262/354: batchLoss = 0.6310, diffLoss = 3.0889, kgLoss = 0.0165
2025-04-08 18:20:00.429584: Training Step 263/354: batchLoss = 0.6978, diffLoss = 3.4163, kgLoss = 0.0182
2025-04-08 18:20:02.048231: Training Step 264/354: batchLoss = 0.5056, diffLoss = 2.4699, kgLoss = 0.0145
2025-04-08 18:20:03.668225: Training Step 265/354: batchLoss = 0.5607, diffLoss = 2.7461, kgLoss = 0.0144
2025-04-08 18:20:05.290556: Training Step 266/354: batchLoss = 0.6571, diffLoss = 3.2216, kgLoss = 0.0160
2025-04-08 18:20:06.905736: Training Step 267/354: batchLoss = 0.5366, diffLoss = 2.6283, kgLoss = 0.0137
2025-04-08 18:20:08.533876: Training Step 268/354: batchLoss = 0.4819, diffLoss = 2.3572, kgLoss = 0.0130
2025-04-08 18:20:10.152233: Training Step 269/354: batchLoss = 0.5280, diffLoss = 2.5868, kgLoss = 0.0133
2025-04-08 18:20:11.769158: Training Step 270/354: batchLoss = 0.5388, diffLoss = 2.6336, kgLoss = 0.0151
2025-04-08 18:20:13.390494: Training Step 271/354: batchLoss = 0.6059, diffLoss = 2.9672, kgLoss = 0.0155
2025-04-08 18:20:15.014794: Training Step 272/354: batchLoss = 0.5536, diffLoss = 2.7123, kgLoss = 0.0140
2025-04-08 18:20:16.632490: Training Step 273/354: batchLoss = 0.5326, diffLoss = 2.6016, kgLoss = 0.0154
2025-04-08 18:20:18.248029: Training Step 274/354: batchLoss = 0.5532, diffLoss = 2.7151, kgLoss = 0.0127
2025-04-08 18:20:19.868977: Training Step 275/354: batchLoss = 0.5107, diffLoss = 2.4981, kgLoss = 0.0139
2025-04-08 18:20:21.489765: Training Step 276/354: batchLoss = 0.5671, diffLoss = 2.7743, kgLoss = 0.0153
2025-04-08 18:20:23.113260: Training Step 277/354: batchLoss = 0.5480, diffLoss = 2.6849, kgLoss = 0.0138
2025-04-08 18:20:24.733089: Training Step 278/354: batchLoss = 0.5952, diffLoss = 2.9098, kgLoss = 0.0166
2025-04-08 18:20:26.356245: Training Step 279/354: batchLoss = 0.4620, diffLoss = 2.2555, kgLoss = 0.0136
2025-04-08 18:20:27.973212: Training Step 280/354: batchLoss = 0.7056, diffLoss = 3.4610, kgLoss = 0.0168
2025-04-08 18:20:29.597783: Training Step 281/354: batchLoss = 0.5868, diffLoss = 2.8749, kgLoss = 0.0147
2025-04-08 18:20:31.219781: Training Step 282/354: batchLoss = 0.5185, diffLoss = 2.5337, kgLoss = 0.0147
2025-04-08 18:20:32.831946: Training Step 283/354: batchLoss = 0.5053, diffLoss = 2.4775, kgLoss = 0.0123
2025-04-08 18:20:34.447719: Training Step 284/354: batchLoss = 0.6697, diffLoss = 3.2829, kgLoss = 0.0165
2025-04-08 18:20:36.069744: Training Step 285/354: batchLoss = 0.5920, diffLoss = 2.8884, kgLoss = 0.0178
2025-04-08 18:20:37.687453: Training Step 286/354: batchLoss = 0.5032, diffLoss = 2.4647, kgLoss = 0.0129
2025-04-08 18:20:39.305770: Training Step 287/354: batchLoss = 0.5478, diffLoss = 2.6851, kgLoss = 0.0135
2025-04-08 18:20:40.931811: Training Step 288/354: batchLoss = 0.5065, diffLoss = 2.4751, kgLoss = 0.0143
2025-04-08 18:20:42.553003: Training Step 289/354: batchLoss = 0.6134, diffLoss = 3.0053, kgLoss = 0.0155
2025-04-08 18:20:44.170666: Training Step 290/354: batchLoss = 0.5995, diffLoss = 2.9331, kgLoss = 0.0161
2025-04-08 18:20:45.788046: Training Step 291/354: batchLoss = 0.5294, diffLoss = 2.5885, kgLoss = 0.0146
2025-04-08 18:20:47.415111: Training Step 292/354: batchLoss = 0.4988, diffLoss = 2.4348, kgLoss = 0.0148
2025-04-08 18:20:49.028969: Training Step 293/354: batchLoss = 0.5509, diffLoss = 2.6935, kgLoss = 0.0153
2025-04-08 18:20:50.646379: Training Step 294/354: batchLoss = 0.5343, diffLoss = 2.6142, kgLoss = 0.0143
2025-04-08 18:20:52.264476: Training Step 295/354: batchLoss = 0.5275, diffLoss = 2.5825, kgLoss = 0.0138
2025-04-08 18:20:53.882889: Training Step 296/354: batchLoss = 0.5393, diffLoss = 2.6280, kgLoss = 0.0171
2025-04-08 18:20:55.499890: Training Step 297/354: batchLoss = 0.5532, diffLoss = 2.7111, kgLoss = 0.0137
2025-04-08 18:20:57.119130: Training Step 298/354: batchLoss = 0.5410, diffLoss = 2.6510, kgLoss = 0.0135
2025-04-08 18:20:58.741409: Training Step 299/354: batchLoss = 0.4830, diffLoss = 2.3642, kgLoss = 0.0127
2025-04-08 18:21:00.361246: Training Step 300/354: batchLoss = 0.6313, diffLoss = 3.0944, kgLoss = 0.0155
2025-04-08 18:21:01.981808: Training Step 301/354: batchLoss = 0.4705, diffLoss = 2.3085, kgLoss = 0.0110
2025-04-08 18:21:03.605249: Training Step 302/354: batchLoss = 0.5540, diffLoss = 2.7147, kgLoss = 0.0138
2025-04-08 18:21:05.225832: Training Step 303/354: batchLoss = 0.6003, diffLoss = 2.9341, kgLoss = 0.0169
2025-04-08 18:21:06.843823: Training Step 304/354: batchLoss = 0.7290, diffLoss = 3.5769, kgLoss = 0.0170
2025-04-08 18:21:08.459903: Training Step 305/354: batchLoss = 0.5511, diffLoss = 2.7019, kgLoss = 0.0134
2025-04-08 18:21:10.072188: Training Step 306/354: batchLoss = 0.5748, diffLoss = 2.8182, kgLoss = 0.0139
2025-04-08 18:21:11.689937: Training Step 307/354: batchLoss = 0.4731, diffLoss = 2.3104, kgLoss = 0.0138
2025-04-08 18:21:13.310649: Training Step 308/354: batchLoss = 0.5651, diffLoss = 2.7649, kgLoss = 0.0152
2025-04-08 18:21:14.932380: Training Step 309/354: batchLoss = 0.4609, diffLoss = 2.2583, kgLoss = 0.0116
2025-04-08 18:21:16.550642: Training Step 310/354: batchLoss = 0.5838, diffLoss = 2.8632, kgLoss = 0.0140
2025-04-08 18:21:18.170840: Training Step 311/354: batchLoss = 0.5848, diffLoss = 2.8581, kgLoss = 0.0165
2025-04-08 18:21:19.792112: Training Step 312/354: batchLoss = 0.4399, diffLoss = 2.1525, kgLoss = 0.0118
2025-04-08 18:21:21.406710: Training Step 313/354: batchLoss = 0.6624, diffLoss = 3.2475, kgLoss = 0.0162
2025-04-08 18:21:23.019906: Training Step 314/354: batchLoss = 0.4760, diffLoss = 2.3249, kgLoss = 0.0138
2025-04-08 18:21:24.635137: Training Step 315/354: batchLoss = 0.5495, diffLoss = 2.6910, kgLoss = 0.0142
2025-04-08 18:21:26.253722: Training Step 316/354: batchLoss = 0.5323, diffLoss = 2.6086, kgLoss = 0.0132
2025-04-08 18:21:27.876485: Training Step 317/354: batchLoss = 0.6493, diffLoss = 3.1842, kgLoss = 0.0156
2025-04-08 18:21:29.495715: Training Step 318/354: batchLoss = 0.5776, diffLoss = 2.8275, kgLoss = 0.0151
2025-04-08 18:21:31.116449: Training Step 319/354: batchLoss = 0.5879, diffLoss = 2.8713, kgLoss = 0.0170
2025-04-08 18:21:32.744258: Training Step 320/354: batchLoss = 0.5142, diffLoss = 2.5235, kgLoss = 0.0119
2025-04-08 18:21:34.366076: Training Step 321/354: batchLoss = 0.6412, diffLoss = 3.1401, kgLoss = 0.0165
2025-04-08 18:21:35.980411: Training Step 322/354: batchLoss = 0.6238, diffLoss = 3.0537, kgLoss = 0.0163
2025-04-08 18:21:37.598792: Training Step 323/354: batchLoss = 0.7698, diffLoss = 3.7730, kgLoss = 0.0190
2025-04-08 18:21:39.215278: Training Step 324/354: batchLoss = 0.5891, diffLoss = 2.8824, kgLoss = 0.0158
2025-04-08 18:21:40.827585: Training Step 325/354: batchLoss = 0.4319, diffLoss = 2.1162, kgLoss = 0.0109
2025-04-08 18:21:42.445768: Training Step 326/354: batchLoss = 0.5099, diffLoss = 2.4951, kgLoss = 0.0136
2025-04-08 18:21:44.061359: Training Step 327/354: batchLoss = 0.5719, diffLoss = 2.7910, kgLoss = 0.0171
2025-04-08 18:21:45.699425: Training Step 328/354: batchLoss = 0.5473, diffLoss = 2.6884, kgLoss = 0.0120
2025-04-08 18:21:47.318508: Training Step 329/354: batchLoss = 0.6601, diffLoss = 3.2338, kgLoss = 0.0167
2025-04-08 18:21:48.934482: Training Step 330/354: batchLoss = 0.5468, diffLoss = 2.6792, kgLoss = 0.0137
2025-04-08 18:21:50.552166: Training Step 331/354: batchLoss = 0.5522, diffLoss = 2.6970, kgLoss = 0.0160
2025-04-08 18:21:52.176661: Training Step 332/354: batchLoss = 0.5664, diffLoss = 2.7783, kgLoss = 0.0134
2025-04-08 18:21:53.788740: Training Step 333/354: batchLoss = 0.5405, diffLoss = 2.6466, kgLoss = 0.0139
2025-04-08 18:21:55.400119: Training Step 334/354: batchLoss = 0.5451, diffLoss = 2.6639, kgLoss = 0.0154
2025-04-08 18:21:57.018605: Training Step 335/354: batchLoss = 0.5261, diffLoss = 2.5810, kgLoss = 0.0124
2025-04-08 18:21:58.636084: Training Step 336/354: batchLoss = 0.5518, diffLoss = 2.6965, kgLoss = 0.0157
2025-04-08 18:22:00.264651: Training Step 337/354: batchLoss = 0.4889, diffLoss = 2.3944, kgLoss = 0.0125
2025-04-08 18:22:01.883883: Training Step 338/354: batchLoss = 0.4989, diffLoss = 2.4418, kgLoss = 0.0131
2025-04-08 18:22:03.507044: Training Step 339/354: batchLoss = 0.4903, diffLoss = 2.3958, kgLoss = 0.0140
2025-04-08 18:22:05.129441: Training Step 340/354: batchLoss = 0.4226, diffLoss = 2.0663, kgLoss = 0.0117
2025-04-08 18:22:06.756733: Training Step 341/354: batchLoss = 0.6371, diffLoss = 3.1188, kgLoss = 0.0167
2025-04-08 18:22:08.372661: Training Step 342/354: batchLoss = 0.4641, diffLoss = 2.2677, kgLoss = 0.0132
2025-04-08 18:22:09.991626: Training Step 343/354: batchLoss = 0.4732, diffLoss = 2.3154, kgLoss = 0.0127
2025-04-08 18:22:11.606436: Training Step 344/354: batchLoss = 0.5629, diffLoss = 2.7591, kgLoss = 0.0138
2025-04-08 18:22:13.225109: Training Step 345/354: batchLoss = 0.4945, diffLoss = 2.4153, kgLoss = 0.0143
2025-04-08 18:22:14.847145: Training Step 346/354: batchLoss = 0.4657, diffLoss = 2.2643, kgLoss = 0.0160
2025-04-08 18:22:16.464899: Training Step 347/354: batchLoss = 1.7111, diffLoss = 8.3942, kgLoss = 0.0403
2025-04-08 18:22:18.087139: Training Step 348/354: batchLoss = 0.5835, diffLoss = 2.8590, kgLoss = 0.0146
2025-04-08 18:22:19.704826: Training Step 349/354: batchLoss = 0.5843, diffLoss = 2.8605, kgLoss = 0.0153
2025-04-08 18:22:21.329341: Training Step 350/354: batchLoss = 0.5259, diffLoss = 2.5745, kgLoss = 0.0137
2025-04-08 18:22:22.951928: Training Step 351/354: batchLoss = 0.5207, diffLoss = 2.5511, kgLoss = 0.0131
2025-04-08 18:22:24.547153: Training Step 352/354: batchLoss = 0.6107, diffLoss = 2.9893, kgLoss = 0.0160
2025-04-08 18:22:25.949698: Training Step 353/354: batchLoss = 0.4519, diffLoss = 2.2045, kgLoss = 0.0138
2025-04-08 18:22:26.035788: 
2025-04-08 18:22:26.036418: Epoch 27/1000, Train: epLoss = 0.9940, epDfLoss = 4.8657, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 18:22:27.339454: Steps 0/138: batch_recall = 46.66, batch_ndcg = 26.42 
2025-04-08 18:22:28.652061: Steps 1/138: batch_recall = 49.19, batch_ndcg = 28.47 
2025-04-08 18:22:29.961064: Steps 2/138: batch_recall = 60.45, batch_ndcg = 36.06 
2025-04-08 18:22:31.284020: Steps 3/138: batch_recall = 58.10, batch_ndcg = 34.07 
2025-04-08 18:22:32.592636: Steps 4/138: batch_recall = 67.23, batch_ndcg = 40.93 
2025-04-08 18:22:33.908596: Steps 5/138: batch_recall = 58.85, batch_ndcg = 31.94 
2025-04-08 18:22:35.219565: Steps 6/138: batch_recall = 52.24, batch_ndcg = 32.11 
2025-04-08 18:22:36.521216: Steps 7/138: batch_recall = 61.94, batch_ndcg = 41.05 
2025-04-08 18:22:37.811629: Steps 8/138: batch_recall = 61.84, batch_ndcg = 39.13 
2025-04-08 18:22:39.115753: Steps 9/138: batch_recall = 59.02, batch_ndcg = 34.58 
2025-04-08 18:22:40.401523: Steps 10/138: batch_recall = 59.46, batch_ndcg = 33.10 
2025-04-08 18:22:41.685227: Steps 11/138: batch_recall = 55.89, batch_ndcg = 32.39 
2025-04-08 18:22:42.976057: Steps 12/138: batch_recall = 51.97, batch_ndcg = 29.13 
2025-04-08 18:22:44.255712: Steps 13/138: batch_recall = 53.83, batch_ndcg = 31.72 
2025-04-08 18:22:45.553784: Steps 14/138: batch_recall = 53.98, batch_ndcg = 31.38 
2025-04-08 18:22:46.852793: Steps 15/138: batch_recall = 48.51, batch_ndcg = 30.13 
2025-04-08 18:22:48.160854: Steps 16/138: batch_recall = 60.62, batch_ndcg = 34.47 
2025-04-08 18:22:49.453421: Steps 17/138: batch_recall = 56.74, batch_ndcg = 33.47 
2025-04-08 18:22:50.754237: Steps 18/138: batch_recall = 52.63, batch_ndcg = 32.83 
2025-04-08 18:22:52.060085: Steps 19/138: batch_recall = 55.27, batch_ndcg = 33.11 
2025-04-08 18:22:53.356356: Steps 20/138: batch_recall = 61.43, batch_ndcg = 36.12 
2025-04-08 18:22:54.654727: Steps 21/138: batch_recall = 69.19, batch_ndcg = 40.54 
2025-04-08 18:22:55.943786: Steps 22/138: batch_recall = 54.51, batch_ndcg = 32.46 
2025-04-08 18:22:57.232999: Steps 23/138: batch_recall = 50.26, batch_ndcg = 30.18 
2025-04-08 18:22:58.522403: Steps 24/138: batch_recall = 56.96, batch_ndcg = 31.08 
2025-04-08 18:22:59.812103: Steps 25/138: batch_recall = 62.09, batch_ndcg = 35.65 
2025-04-08 18:23:01.107621: Steps 26/138: batch_recall = 56.63, batch_ndcg = 32.52 
2025-04-08 18:23:02.401737: Steps 27/138: batch_recall = 55.03, batch_ndcg = 31.91 
2025-04-08 18:23:03.691020: Steps 28/138: batch_recall = 59.88, batch_ndcg = 33.52 
2025-04-08 18:23:04.982685: Steps 29/138: batch_recall = 61.73, batch_ndcg = 32.30 
2025-04-08 18:23:06.282356: Steps 30/138: batch_recall = 59.21, batch_ndcg = 34.30 
2025-04-08 18:23:07.580602: Steps 31/138: batch_recall = 48.76, batch_ndcg = 27.44 
2025-04-08 18:23:08.879191: Steps 32/138: batch_recall = 54.50, batch_ndcg = 31.62 
2025-04-08 18:23:10.160584: Steps 33/138: batch_recall = 60.84, batch_ndcg = 33.67 
2025-04-08 18:23:11.450052: Steps 34/138: batch_recall = 55.08, batch_ndcg = 30.06 
2025-04-08 18:23:12.739908: Steps 35/138: batch_recall = 51.69, batch_ndcg = 29.77 
2025-04-08 18:23:14.024945: Steps 36/138: batch_recall = 46.25, batch_ndcg = 26.50 
2025-04-08 18:23:15.311889: Steps 37/138: batch_recall = 57.49, batch_ndcg = 33.84 
2025-04-08 18:23:16.591157: Steps 38/138: batch_recall = 60.26, batch_ndcg = 32.95 
2025-04-08 18:23:17.874405: Steps 39/138: batch_recall = 67.59, batch_ndcg = 38.71 
2025-04-08 18:23:19.169401: Steps 40/138: batch_recall = 61.46, batch_ndcg = 31.04 
2025-04-08 18:23:20.450911: Steps 41/138: batch_recall = 60.11, batch_ndcg = 33.92 
2025-04-08 18:23:21.736571: Steps 42/138: batch_recall = 54.42, batch_ndcg = 30.02 
2025-04-08 18:23:23.025926: Steps 43/138: batch_recall = 57.75, batch_ndcg = 36.58 
2025-04-08 18:23:24.311306: Steps 44/138: batch_recall = 57.47, batch_ndcg = 31.14 
2025-04-08 18:23:25.599673: Steps 45/138: batch_recall = 61.45, batch_ndcg = 35.20 
2025-04-08 18:23:26.871836: Steps 46/138: batch_recall = 62.20, batch_ndcg = 36.33 
2025-04-08 18:23:28.153430: Steps 47/138: batch_recall = 52.80, batch_ndcg = 32.01 
2025-04-08 18:23:29.425693: Steps 48/138: batch_recall = 62.97, batch_ndcg = 35.58 
2025-04-08 18:23:30.701809: Steps 49/138: batch_recall = 63.78, batch_ndcg = 36.88 
2025-04-08 18:23:31.984093: Steps 50/138: batch_recall = 59.71, batch_ndcg = 31.41 
2025-04-08 18:23:33.262590: Steps 51/138: batch_recall = 62.52, batch_ndcg = 35.91 
2025-04-08 18:23:34.552613: Steps 52/138: batch_recall = 65.02, batch_ndcg = 41.73 
2025-04-08 18:23:35.844580: Steps 53/138: batch_recall = 64.58, batch_ndcg = 33.96 
2025-04-08 18:23:37.134386: Steps 54/138: batch_recall = 68.48, batch_ndcg = 38.63 
2025-04-08 18:23:38.406708: Steps 55/138: batch_recall = 61.30, batch_ndcg = 34.73 
2025-04-08 18:23:39.698759: Steps 56/138: batch_recall = 61.44, batch_ndcg = 35.79 
2025-04-08 18:23:40.981043: Steps 57/138: batch_recall = 58.54, batch_ndcg = 32.89 
2025-04-08 18:23:42.261288: Steps 58/138: batch_recall = 71.36, batch_ndcg = 36.99 
2025-04-08 18:23:43.531953: Steps 59/138: batch_recall = 65.96, batch_ndcg = 38.86 
2025-04-08 18:23:44.821243: Steps 60/138: batch_recall = 66.85, batch_ndcg = 37.15 
2025-04-08 18:23:46.098533: Steps 61/138: batch_recall = 66.18, batch_ndcg = 35.45 
2025-04-08 18:23:47.377135: Steps 62/138: batch_recall = 87.29, batch_ndcg = 45.52 
2025-04-08 18:23:48.658487: Steps 63/138: batch_recall = 75.60, batch_ndcg = 42.81 
2025-04-08 18:23:49.939713: Steps 64/138: batch_recall = 61.55, batch_ndcg = 32.66 
2025-04-08 18:23:51.222115: Steps 65/138: batch_recall = 86.08, batch_ndcg = 47.07 
2025-04-08 18:23:52.510805: Steps 66/138: batch_recall = 69.44, batch_ndcg = 41.22 
2025-04-08 18:23:53.791221: Steps 67/138: batch_recall = 76.55, batch_ndcg = 46.46 
2025-04-08 18:23:55.065744: Steps 68/138: batch_recall = 60.96, batch_ndcg = 33.39 
2025-04-08 18:23:56.348968: Steps 69/138: batch_recall = 86.31, batch_ndcg = 50.81 
2025-04-08 18:23:57.629034: Steps 70/138: batch_recall = 76.95, batch_ndcg = 44.73 
2025-04-08 18:23:58.902821: Steps 71/138: batch_recall = 87.45, batch_ndcg = 50.78 
2025-04-08 18:24:00.171947: Steps 72/138: batch_recall = 84.51, batch_ndcg = 48.97 
2025-04-08 18:24:01.448099: Steps 73/138: batch_recall = 84.63, batch_ndcg = 47.15 
2025-04-08 18:24:02.731052: Steps 74/138: batch_recall = 76.49, batch_ndcg = 48.03 
2025-04-08 18:24:04.004015: Steps 75/138: batch_recall = 84.52, batch_ndcg = 49.34 
2025-04-08 18:24:05.285761: Steps 76/138: batch_recall = 92.22, batch_ndcg = 54.71 
2025-04-08 18:24:06.581860: Steps 77/138: batch_recall = 88.64, batch_ndcg = 50.56 
2025-04-08 18:24:07.874036: Steps 78/138: batch_recall = 92.70, batch_ndcg = 48.93 
2025-04-08 18:24:09.162333: Steps 79/138: batch_recall = 91.19, batch_ndcg = 49.68 
2025-04-08 18:24:10.455364: Steps 80/138: batch_recall = 75.58, batch_ndcg = 39.66 
2025-04-08 18:24:11.734727: Steps 81/138: batch_recall = 82.43, batch_ndcg = 48.03 
2025-04-08 18:24:13.008274: Steps 82/138: batch_recall = 86.80, batch_ndcg = 53.18 
2025-04-08 18:24:14.301281: Steps 83/138: batch_recall = 79.60, batch_ndcg = 46.94 
2025-04-08 18:24:15.579266: Steps 84/138: batch_recall = 98.24, batch_ndcg = 57.19 
2025-04-08 18:24:16.845040: Steps 85/138: batch_recall = 104.21, batch_ndcg = 61.01 
2025-04-08 18:24:18.113947: Steps 86/138: batch_recall = 121.04, batch_ndcg = 71.31 
2025-04-08 18:24:19.385543: Steps 87/138: batch_recall = 107.01, batch_ndcg = 55.68 
2025-04-08 18:24:20.660856: Steps 88/138: batch_recall = 104.70, batch_ndcg = 59.77 
2025-04-08 18:24:21.933007: Steps 89/138: batch_recall = 121.04, batch_ndcg = 69.10 
2025-04-08 18:24:23.212842: Steps 90/138: batch_recall = 98.59, batch_ndcg = 57.16 
2025-04-08 18:24:24.484475: Steps 91/138: batch_recall = 115.29, batch_ndcg = 64.92 
2025-04-08 18:24:25.752709: Steps 92/138: batch_recall = 124.79, batch_ndcg = 65.66 
2025-04-08 18:24:27.026489: Steps 93/138: batch_recall = 120.24, batch_ndcg = 68.83 
2025-04-08 18:24:28.300952: Steps 94/138: batch_recall = 126.54, batch_ndcg = 66.73 
2025-04-08 18:24:29.569970: Steps 95/138: batch_recall = 114.84, batch_ndcg = 67.03 
2025-04-08 18:24:30.840950: Steps 96/138: batch_recall = 133.62, batch_ndcg = 79.68 
2025-04-08 18:24:32.106976: Steps 97/138: batch_recall = 153.38, batch_ndcg = 92.76 
2025-04-08 18:24:33.378817: Steps 98/138: batch_recall = 110.04, batch_ndcg = 63.56 
2025-04-08 18:24:34.638108: Steps 99/138: batch_recall = 123.77, batch_ndcg = 71.59 
2025-04-08 18:24:35.905140: Steps 100/138: batch_recall = 129.24, batch_ndcg = 72.20 
2025-04-08 18:24:37.170673: Steps 101/138: batch_recall = 127.72, batch_ndcg = 70.25 
2025-04-08 18:24:38.445647: Steps 102/138: batch_recall = 121.38, batch_ndcg = 69.92 
2025-04-08 18:24:39.721681: Steps 103/138: batch_recall = 141.68, batch_ndcg = 81.30 
2025-04-08 18:24:40.998630: Steps 104/138: batch_recall = 138.04, batch_ndcg = 79.19 
2025-04-08 18:24:42.276824: Steps 105/138: batch_recall = 123.81, batch_ndcg = 67.96 
2025-04-08 18:24:43.552437: Steps 106/138: batch_recall = 107.74, batch_ndcg = 60.58 
2025-04-08 18:24:44.824600: Steps 107/138: batch_recall = 114.88, batch_ndcg = 65.23 
2025-04-08 18:24:46.099963: Steps 108/138: batch_recall = 117.41, batch_ndcg = 70.98 
2025-04-08 18:24:47.373224: Steps 109/138: batch_recall = 138.25, batch_ndcg = 77.44 
2025-04-08 18:24:48.643277: Steps 110/138: batch_recall = 124.62, batch_ndcg = 65.40 
2025-04-08 18:24:49.907335: Steps 111/138: batch_recall = 135.56, batch_ndcg = 84.08 
2025-04-08 18:24:51.168632: Steps 112/138: batch_recall = 168.25, batch_ndcg = 93.68 
2025-04-08 18:24:52.437644: Steps 113/138: batch_recall = 126.33, batch_ndcg = 70.18 
2025-04-08 18:24:53.715643: Steps 114/138: batch_recall = 124.87, batch_ndcg = 72.72 
2025-04-08 18:24:54.990140: Steps 115/138: batch_recall = 120.44, batch_ndcg = 63.00 
2025-04-08 18:24:56.259875: Steps 116/138: batch_recall = 126.89, batch_ndcg = 67.85 
2025-04-08 18:24:57.534998: Steps 117/138: batch_recall = 113.81, batch_ndcg = 66.24 
2025-04-08 18:24:58.806090: Steps 118/138: batch_recall = 125.78, batch_ndcg = 68.83 
2025-04-08 18:25:00.080099: Steps 119/138: batch_recall = 142.57, batch_ndcg = 77.05 
2025-04-08 18:25:01.356668: Steps 120/138: batch_recall = 126.24, batch_ndcg = 71.10 
2025-04-08 18:25:02.637092: Steps 121/138: batch_recall = 151.69, batch_ndcg = 79.81 
2025-04-08 18:25:03.905806: Steps 122/138: batch_recall = 148.99, batch_ndcg = 81.72 
2025-04-08 18:25:05.177644: Steps 123/138: batch_recall = 129.46, batch_ndcg = 73.50 
2025-04-08 18:25:06.443797: Steps 124/138: batch_recall = 152.91, batch_ndcg = 93.32 
2025-04-08 18:25:07.711150: Steps 125/138: batch_recall = 133.63, batch_ndcg = 73.68 
2025-04-08 18:25:08.980380: Steps 126/138: batch_recall = 159.44, batch_ndcg = 88.19 
2025-04-08 18:25:10.256383: Steps 127/138: batch_recall = 144.26, batch_ndcg = 81.87 
2025-04-08 18:25:11.532689: Steps 128/138: batch_recall = 129.16, batch_ndcg = 71.40 
2025-04-08 18:25:12.800037: Steps 129/138: batch_recall = 164.50, batch_ndcg = 92.78 
2025-04-08 18:25:14.072741: Steps 130/138: batch_recall = 133.27, batch_ndcg = 70.07 
2025-04-08 18:25:15.334435: Steps 131/138: batch_recall = 153.88, batch_ndcg = 88.80 
2025-04-08 18:25:16.599915: Steps 132/138: batch_recall = 152.30, batch_ndcg = 85.94 
2025-04-08 18:25:17.861740: Steps 133/138: batch_recall = 146.43, batch_ndcg = 84.33 
2025-04-08 18:25:19.131215: Steps 134/138: batch_recall = 144.88, batch_ndcg = 83.91 
2025-04-08 18:25:20.396842: Steps 135/138: batch_recall = 166.36, batch_ndcg = 95.18 
2025-04-08 18:25:21.669237: Steps 136/138: batch_recall = 154.69, batch_ndcg = 79.39 
2025-04-08 18:25:22.937608: Steps 137/138: batch_recall = 138.95, batch_ndcg = 86.74 
2025-04-08 18:25:22.938163: Epoch 27/1000, Test: Recall = 0.1772, NDCG = 0.1008  

2025-04-08 18:25:24.697318: Training Step 0/354: batchLoss = 0.5763, diffLoss = 2.8212, kgLoss = 0.0151
2025-04-08 18:25:26.314462: Training Step 1/354: batchLoss = 0.5473, diffLoss = 2.6825, kgLoss = 0.0135
2025-04-08 18:25:27.931082: Training Step 2/354: batchLoss = 0.4843, diffLoss = 2.3686, kgLoss = 0.0132
2025-04-08 18:25:29.546238: Training Step 3/354: batchLoss = 0.5007, diffLoss = 2.4510, kgLoss = 0.0132
2025-04-08 18:25:31.168486: Training Step 4/354: batchLoss = 0.5920, diffLoss = 2.9005, kgLoss = 0.0149
2025-04-08 18:25:32.787430: Training Step 5/354: batchLoss = 0.4872, diffLoss = 2.3870, kgLoss = 0.0122
2025-04-08 18:25:34.409941: Training Step 6/354: batchLoss = 0.5520, diffLoss = 2.6906, kgLoss = 0.0174
2025-04-08 18:25:36.028027: Training Step 7/354: batchLoss = 0.5061, diffLoss = 2.4766, kgLoss = 0.0135
2025-04-08 18:25:37.645354: Training Step 8/354: batchLoss = 0.5950, diffLoss = 2.9152, kgLoss = 0.0149
2025-04-08 18:25:39.262624: Training Step 9/354: batchLoss = 0.4911, diffLoss = 2.3978, kgLoss = 0.0145
2025-04-08 18:25:40.877420: Training Step 10/354: batchLoss = 0.4538, diffLoss = 2.2190, kgLoss = 0.0126
2025-04-08 18:25:42.498890: Training Step 11/354: batchLoss = 0.4790, diffLoss = 2.3448, kgLoss = 0.0126
2025-04-08 18:25:44.114876: Training Step 12/354: batchLoss = 0.5718, diffLoss = 2.7976, kgLoss = 0.0154
2025-04-08 18:25:45.731084: Training Step 13/354: batchLoss = 0.5972, diffLoss = 2.9231, kgLoss = 0.0157
2025-04-08 18:25:47.355248: Training Step 14/354: batchLoss = 0.5094, diffLoss = 2.4975, kgLoss = 0.0124
2025-04-08 18:25:48.977726: Training Step 15/354: batchLoss = 0.4781, diffLoss = 2.3391, kgLoss = 0.0129
2025-04-08 18:25:50.597027: Training Step 16/354: batchLoss = 0.6060, diffLoss = 2.9659, kgLoss = 0.0160
2025-04-08 18:25:52.218397: Training Step 17/354: batchLoss = 0.6158, diffLoss = 3.0138, kgLoss = 0.0163
2025-04-08 18:25:53.841214: Training Step 18/354: batchLoss = 0.6243, diffLoss = 3.0582, kgLoss = 0.0158
2025-04-08 18:25:55.458633: Training Step 19/354: batchLoss = 0.5021, diffLoss = 2.4556, kgLoss = 0.0137
2025-04-08 18:25:57.077506: Training Step 20/354: batchLoss = 0.5077, diffLoss = 2.4871, kgLoss = 0.0128
2025-04-08 18:25:58.691688: Training Step 21/354: batchLoss = 0.6156, diffLoss = 3.0242, kgLoss = 0.0135
2025-04-08 18:26:00.306272: Training Step 22/354: batchLoss = 0.4866, diffLoss = 2.3560, kgLoss = 0.0193
2025-04-08 18:26:01.921133: Training Step 23/354: batchLoss = 0.6418, diffLoss = 3.1449, kgLoss = 0.0160
2025-04-08 18:26:03.540126: Training Step 24/354: batchLoss = 0.5657, diffLoss = 2.7704, kgLoss = 0.0145
2025-04-08 18:26:05.159210: Training Step 25/354: batchLoss = 0.5783, diffLoss = 2.8349, kgLoss = 0.0142
2025-04-08 18:26:06.776710: Training Step 26/354: batchLoss = 0.5728, diffLoss = 2.7993, kgLoss = 0.0161
2025-04-08 18:26:08.402144: Training Step 27/354: batchLoss = 0.7320, diffLoss = 3.5828, kgLoss = 0.0193
2025-04-08 18:26:10.026710: Training Step 28/354: batchLoss = 0.7243, diffLoss = 3.5541, kgLoss = 0.0168
2025-04-08 18:26:11.650149: Training Step 29/354: batchLoss = 0.6148, diffLoss = 3.0050, kgLoss = 0.0172
2025-04-08 18:26:13.268404: Training Step 30/354: batchLoss = 0.5380, diffLoss = 2.6358, kgLoss = 0.0135
2025-04-08 18:26:14.886525: Training Step 31/354: batchLoss = 0.6175, diffLoss = 3.0264, kgLoss = 0.0153
2025-04-08 18:26:16.500632: Training Step 32/354: batchLoss = 0.6019, diffLoss = 2.9540, kgLoss = 0.0139
2025-04-08 18:26:18.115199: Training Step 33/354: batchLoss = 0.4821, diffLoss = 2.3634, kgLoss = 0.0118
2025-04-08 18:26:19.733671: Training Step 34/354: batchLoss = 0.7637, diffLoss = 3.7494, kgLoss = 0.0172
2025-04-08 18:26:21.356500: Training Step 35/354: batchLoss = 0.7295, diffLoss = 3.5612, kgLoss = 0.0216
2025-04-08 18:26:22.979047: Training Step 36/354: batchLoss = 0.6330, diffLoss = 3.1042, kgLoss = 0.0152
2025-04-08 18:26:24.601888: Training Step 37/354: batchLoss = 0.6116, diffLoss = 3.0032, kgLoss = 0.0137
2025-04-08 18:26:26.225261: Training Step 38/354: batchLoss = 0.4456, diffLoss = 2.1812, kgLoss = 0.0117
2025-04-08 18:26:27.855943: Training Step 39/354: batchLoss = 0.5405, diffLoss = 2.6358, kgLoss = 0.0167
2025-04-08 18:26:29.474285: Training Step 40/354: batchLoss = 0.5958, diffLoss = 2.9177, kgLoss = 0.0153
2025-04-08 18:26:31.084158: Training Step 41/354: batchLoss = 0.4801, diffLoss = 2.3437, kgLoss = 0.0142
2025-04-08 18:26:32.693241: Training Step 42/354: batchLoss = 0.5481, diffLoss = 2.6821, kgLoss = 0.0145
2025-04-08 18:26:34.306970: Training Step 43/354: batchLoss = 0.5919, diffLoss = 2.8973, kgLoss = 0.0155
2025-04-08 18:26:35.919614: Training Step 44/354: batchLoss = 0.6525, diffLoss = 3.1901, kgLoss = 0.0181
2025-04-08 18:26:37.534941: Training Step 45/354: batchLoss = 0.5243, diffLoss = 2.5702, kgLoss = 0.0128
2025-04-08 18:26:39.157248: Training Step 46/354: batchLoss = 0.5238, diffLoss = 2.5676, kgLoss = 0.0128
2025-04-08 18:26:40.788310: Training Step 47/354: batchLoss = 0.4690, diffLoss = 2.2946, kgLoss = 0.0126
2025-04-08 18:26:42.413093: Training Step 48/354: batchLoss = 0.7009, diffLoss = 3.4403, kgLoss = 0.0160
2025-04-08 18:26:44.038795: Training Step 49/354: batchLoss = 0.7732, diffLoss = 3.7835, kgLoss = 0.0206
2025-04-08 18:26:45.650255: Training Step 50/354: batchLoss = 0.5484, diffLoss = 2.6924, kgLoss = 0.0124
2025-04-08 18:26:47.265088: Training Step 51/354: batchLoss = 0.4819, diffLoss = 2.3534, kgLoss = 0.0140
2025-04-08 18:26:48.881232: Training Step 52/354: batchLoss = 0.4938, diffLoss = 2.4130, kgLoss = 0.0140
2025-04-08 18:26:50.495688: Training Step 53/354: batchLoss = 0.6864, diffLoss = 3.3606, kgLoss = 0.0179
2025-04-08 18:26:52.114688: Training Step 54/354: batchLoss = 0.5294, diffLoss = 2.5945, kgLoss = 0.0132
2025-04-08 18:26:53.733774: Training Step 55/354: batchLoss = 0.5623, diffLoss = 2.7562, kgLoss = 0.0138
2025-04-08 18:26:55.358755: Training Step 56/354: batchLoss = 0.5973, diffLoss = 2.9276, kgLoss = 0.0147
2025-04-08 18:26:56.982733: Training Step 57/354: batchLoss = 0.6336, diffLoss = 3.1142, kgLoss = 0.0134
2025-04-08 18:26:58.604444: Training Step 58/354: batchLoss = 0.5939, diffLoss = 2.9106, kgLoss = 0.0148
2025-04-08 18:27:00.224030: Training Step 59/354: batchLoss = 0.5822, diffLoss = 2.8598, kgLoss = 0.0128
2025-04-08 18:27:01.839979: Training Step 60/354: batchLoss = 0.5655, diffLoss = 2.7720, kgLoss = 0.0138
2025-04-08 18:27:03.455039: Training Step 61/354: batchLoss = 0.6448, diffLoss = 3.1521, kgLoss = 0.0180
2025-04-08 18:27:05.078106: Training Step 62/354: batchLoss = 0.4870, diffLoss = 2.3869, kgLoss = 0.0120
2025-04-08 18:27:06.691660: Training Step 63/354: batchLoss = 0.5799, diffLoss = 2.8444, kgLoss = 0.0138
2025-04-08 18:27:08.311714: Training Step 64/354: batchLoss = 0.6368, diffLoss = 3.1205, kgLoss = 0.0158
2025-04-08 18:27:09.936211: Training Step 65/354: batchLoss = 0.5782, diffLoss = 2.8318, kgLoss = 0.0148
2025-04-08 18:27:11.558164: Training Step 66/354: batchLoss = 0.4502, diffLoss = 2.2021, kgLoss = 0.0122
2025-04-08 18:27:13.180647: Training Step 67/354: batchLoss = 0.5193, diffLoss = 2.5406, kgLoss = 0.0139
2025-04-08 18:27:14.807318: Training Step 68/354: batchLoss = 0.4747, diffLoss = 2.3258, kgLoss = 0.0120
2025-04-08 18:27:16.426180: Training Step 69/354: batchLoss = 0.5763, diffLoss = 2.8242, kgLoss = 0.0143
2025-04-08 18:27:18.046441: Training Step 70/354: batchLoss = 0.5849, diffLoss = 2.8666, kgLoss = 0.0144
2025-04-08 18:27:19.664829: Training Step 71/354: batchLoss = 0.4730, diffLoss = 2.3124, kgLoss = 0.0131
2025-04-08 18:27:21.284821: Training Step 72/354: batchLoss = 0.5469, diffLoss = 2.6773, kgLoss = 0.0143
2025-04-08 18:27:22.898194: Training Step 73/354: batchLoss = 0.4551, diffLoss = 2.2225, kgLoss = 0.0133
2025-04-08 18:27:24.516801: Training Step 74/354: batchLoss = 0.4826, diffLoss = 2.3596, kgLoss = 0.0133
2025-04-08 18:27:26.140433: Training Step 75/354: batchLoss = 0.4980, diffLoss = 2.4275, kgLoss = 0.0156
2025-04-08 18:27:27.765389: Training Step 76/354: batchLoss = 0.6223, diffLoss = 3.0499, kgLoss = 0.0154
2025-04-08 18:27:29.391472: Training Step 77/354: batchLoss = 0.6130, diffLoss = 3.0065, kgLoss = 0.0146
2025-04-08 18:27:31.009659: Training Step 78/354: batchLoss = 0.4860, diffLoss = 2.3781, kgLoss = 0.0129
2025-04-08 18:27:32.627931: Training Step 79/354: batchLoss = 0.5316, diffLoss = 2.5999, kgLoss = 0.0145
2025-04-08 18:27:34.242777: Training Step 80/354: batchLoss = 0.5654, diffLoss = 2.7724, kgLoss = 0.0137
2025-04-08 18:27:35.850299: Training Step 81/354: batchLoss = 0.5171, diffLoss = 2.5157, kgLoss = 0.0174
2025-04-08 18:27:37.463656: Training Step 82/354: batchLoss = 0.5795, diffLoss = 2.8367, kgLoss = 0.0153
2025-04-08 18:27:39.077063: Training Step 83/354: batchLoss = 0.5077, diffLoss = 2.4784, kgLoss = 0.0150
2025-04-08 18:27:40.690570: Training Step 84/354: batchLoss = 0.5930, diffLoss = 2.9045, kgLoss = 0.0151
2025-04-08 18:27:42.312353: Training Step 85/354: batchLoss = 0.4594, diffLoss = 2.2419, kgLoss = 0.0137
2025-04-08 18:27:43.933179: Training Step 86/354: batchLoss = 0.6395, diffLoss = 3.1366, kgLoss = 0.0152
2025-04-08 18:27:45.555752: Training Step 87/354: batchLoss = 0.5258, diffLoss = 2.5754, kgLoss = 0.0133
2025-04-08 18:27:47.173547: Training Step 88/354: batchLoss = 0.5059, diffLoss = 2.4742, kgLoss = 0.0139
2025-04-08 18:27:48.796471: Training Step 89/354: batchLoss = 0.7931, diffLoss = 3.8899, kgLoss = 0.0189
2025-04-08 18:27:50.416141: Training Step 90/354: batchLoss = 0.5868, diffLoss = 2.8775, kgLoss = 0.0141
2025-04-08 18:27:52.032127: Training Step 91/354: batchLoss = 0.5205, diffLoss = 2.5449, kgLoss = 0.0144
2025-04-08 18:27:53.648865: Training Step 92/354: batchLoss = 0.5153, diffLoss = 2.5197, kgLoss = 0.0142
2025-04-08 18:27:55.267810: Training Step 93/354: batchLoss = 0.6574, diffLoss = 3.2240, kgLoss = 0.0157
2025-04-08 18:27:56.887196: Training Step 94/354: batchLoss = 0.6124, diffLoss = 3.0048, kgLoss = 0.0143
2025-04-08 18:27:58.513034: Training Step 95/354: batchLoss = 0.5569, diffLoss = 2.7341, kgLoss = 0.0126
2025-04-08 18:28:00.139427: Training Step 96/354: batchLoss = 0.5492, diffLoss = 2.6892, kgLoss = 0.0141
2025-04-08 18:28:01.759578: Training Step 97/354: batchLoss = 0.6080, diffLoss = 2.9801, kgLoss = 0.0150
2025-04-08 18:28:03.383618: Training Step 98/354: batchLoss = 0.4606, diffLoss = 2.2533, kgLoss = 0.0125
2025-04-08 18:28:05.001507: Training Step 99/354: batchLoss = 0.6168, diffLoss = 3.0194, kgLoss = 0.0161
2025-04-08 18:28:06.618596: Training Step 100/354: batchLoss = 0.5160, diffLoss = 2.5303, kgLoss = 0.0124
2025-04-08 18:28:08.233699: Training Step 101/354: batchLoss = 0.5525, diffLoss = 2.7013, kgLoss = 0.0153
2025-04-08 18:28:09.844475: Training Step 102/354: batchLoss = 0.7188, diffLoss = 3.5248, kgLoss = 0.0173
2025-04-08 18:28:11.457358: Training Step 103/354: batchLoss = 0.5302, diffLoss = 2.5927, kgLoss = 0.0146
2025-04-08 18:28:13.074313: Training Step 104/354: batchLoss = 0.5034, diffLoss = 2.4598, kgLoss = 0.0143
2025-04-08 18:28:14.698324: Training Step 105/354: batchLoss = 0.5502, diffLoss = 2.6903, kgLoss = 0.0152
2025-04-08 18:28:16.318051: Training Step 106/354: batchLoss = 0.6814, diffLoss = 3.3427, kgLoss = 0.0161
2025-04-08 18:28:17.941997: Training Step 107/354: batchLoss = 0.4694, diffLoss = 2.2950, kgLoss = 0.0130
2025-04-08 18:28:19.568191: Training Step 108/354: batchLoss = 0.5221, diffLoss = 2.5591, kgLoss = 0.0129
2025-04-08 18:28:21.185959: Training Step 109/354: batchLoss = 0.5440, diffLoss = 2.6601, kgLoss = 0.0150
2025-04-08 18:28:22.804668: Training Step 110/354: batchLoss = 0.4640, diffLoss = 2.2704, kgLoss = 0.0124
2025-04-08 18:28:24.418057: Training Step 111/354: batchLoss = 0.6209, diffLoss = 3.0385, kgLoss = 0.0165
2025-04-08 18:28:26.033797: Training Step 112/354: batchLoss = 0.5474, diffLoss = 2.6804, kgLoss = 0.0141
2025-04-08 18:28:27.652650: Training Step 113/354: batchLoss = 0.5043, diffLoss = 2.4690, kgLoss = 0.0132
2025-04-08 18:28:29.268255: Training Step 114/354: batchLoss = 0.4919, diffLoss = 2.4021, kgLoss = 0.0144
2025-04-08 18:28:30.893345: Training Step 115/354: batchLoss = 0.5862, diffLoss = 2.8725, kgLoss = 0.0146
2025-04-08 18:28:32.514336: Training Step 116/354: batchLoss = 0.6101, diffLoss = 2.9905, kgLoss = 0.0151
2025-04-08 18:28:34.140586: Training Step 117/354: batchLoss = 0.5395, diffLoss = 2.6342, kgLoss = 0.0158
2025-04-08 18:28:35.760618: Training Step 118/354: batchLoss = 1.6181, diffLoss = 7.9186, kgLoss = 0.0430
2025-04-08 18:28:37.382317: Training Step 119/354: batchLoss = 0.6898, diffLoss = 3.3753, kgLoss = 0.0184
2025-04-08 18:28:38.996925: Training Step 120/354: batchLoss = 0.5889, diffLoss = 2.8837, kgLoss = 0.0152
2025-04-08 18:28:40.612429: Training Step 121/354: batchLoss = 0.5870, diffLoss = 2.8726, kgLoss = 0.0156
2025-04-08 18:28:42.226402: Training Step 122/354: batchLoss = 0.6194, diffLoss = 3.0356, kgLoss = 0.0153
2025-04-08 18:28:43.845134: Training Step 123/354: batchLoss = 0.5844, diffLoss = 2.8625, kgLoss = 0.0148
2025-04-08 18:28:45.463915: Training Step 124/354: batchLoss = 0.4903, diffLoss = 2.3945, kgLoss = 0.0142
2025-04-08 18:28:47.085552: Training Step 125/354: batchLoss = 0.5488, diffLoss = 2.6897, kgLoss = 0.0135
2025-04-08 18:28:48.705995: Training Step 126/354: batchLoss = 0.7011, diffLoss = 3.4407, kgLoss = 0.0162
2025-04-08 18:28:50.327253: Training Step 127/354: batchLoss = 0.5870, diffLoss = 2.8735, kgLoss = 0.0154
2025-04-08 18:28:51.943257: Training Step 128/354: batchLoss = 0.6206, diffLoss = 3.0296, kgLoss = 0.0184
2025-04-08 18:28:53.563163: Training Step 129/354: batchLoss = 0.4975, diffLoss = 2.4341, kgLoss = 0.0134
2025-04-08 18:28:55.181583: Training Step 130/354: batchLoss = 0.4471, diffLoss = 2.1860, kgLoss = 0.0124
2025-04-08 18:28:56.797217: Training Step 131/354: batchLoss = 0.4353, diffLoss = 2.1324, kgLoss = 0.0110
2025-04-08 18:28:58.416119: Training Step 132/354: batchLoss = 0.5297, diffLoss = 2.5919, kgLoss = 0.0142
2025-04-08 18:29:00.030255: Training Step 133/354: batchLoss = 0.6531, diffLoss = 3.1970, kgLoss = 0.0171
2025-04-08 18:29:01.653326: Training Step 134/354: batchLoss = 0.5753, diffLoss = 2.8120, kgLoss = 0.0161
2025-04-08 18:29:03.275245: Training Step 135/354: batchLoss = 0.5966, diffLoss = 2.9248, kgLoss = 0.0145
2025-04-08 18:29:04.896483: Training Step 136/354: batchLoss = 0.5343, diffLoss = 2.6165, kgLoss = 0.0137
2025-04-08 18:29:06.518357: Training Step 137/354: batchLoss = 0.4788, diffLoss = 2.3493, kgLoss = 0.0112
2025-04-08 18:29:08.138083: Training Step 138/354: batchLoss = 0.5879, diffLoss = 2.8813, kgLoss = 0.0145
2025-04-08 18:29:09.756337: Training Step 139/354: batchLoss = 0.5164, diffLoss = 2.5299, kgLoss = 0.0131
2025-04-08 18:29:11.375091: Training Step 140/354: batchLoss = 0.5661, diffLoss = 2.7755, kgLoss = 0.0137
2025-04-08 18:29:12.988458: Training Step 141/354: batchLoss = 0.6301, diffLoss = 3.0884, kgLoss = 0.0155
2025-04-08 18:29:14.607533: Training Step 142/354: batchLoss = 0.5040, diffLoss = 2.4594, kgLoss = 0.0152
2025-04-08 18:29:16.227288: Training Step 143/354: batchLoss = 0.7458, diffLoss = 3.6513, kgLoss = 0.0194
2025-04-08 18:29:17.851870: Training Step 144/354: batchLoss = 0.5819, diffLoss = 2.8293, kgLoss = 0.0200
2025-04-08 18:29:19.475595: Training Step 145/354: batchLoss = 0.4900, diffLoss = 2.3929, kgLoss = 0.0143
2025-04-08 18:29:21.099694: Training Step 146/354: batchLoss = 0.5439, diffLoss = 2.6649, kgLoss = 0.0136
2025-04-08 18:29:22.719726: Training Step 147/354: batchLoss = 0.5070, diffLoss = 2.4774, kgLoss = 0.0143
2025-04-08 18:29:24.334035: Training Step 148/354: batchLoss = 0.5792, diffLoss = 2.8354, kgLoss = 0.0151
2025-04-08 18:29:25.950054: Training Step 149/354: batchLoss = 0.5758, diffLoss = 2.8266, kgLoss = 0.0132
2025-04-08 18:29:27.560363: Training Step 150/354: batchLoss = 0.6557, diffLoss = 3.2139, kgLoss = 0.0161
2025-04-08 18:29:29.173681: Training Step 151/354: batchLoss = 0.5176, diffLoss = 2.5318, kgLoss = 0.0140
2025-04-08 18:29:30.785331: Training Step 152/354: batchLoss = 0.5492, diffLoss = 2.6865, kgLoss = 0.0149
2025-04-08 18:29:32.405535: Training Step 153/354: batchLoss = 0.6406, diffLoss = 3.1424, kgLoss = 0.0152
2025-04-08 18:29:34.024941: Training Step 154/354: batchLoss = 0.7187, diffLoss = 3.5220, kgLoss = 0.0178
2025-04-08 18:29:35.643607: Training Step 155/354: batchLoss = 0.5185, diffLoss = 2.5418, kgLoss = 0.0127
2025-04-08 18:29:37.262106: Training Step 156/354: batchLoss = 0.5103, diffLoss = 2.5092, kgLoss = 0.0105
2025-04-08 18:29:38.879275: Training Step 157/354: batchLoss = 0.6374, diffLoss = 3.1243, kgLoss = 0.0156
2025-04-08 18:29:40.504990: Training Step 158/354: batchLoss = 0.5928, diffLoss = 2.8972, kgLoss = 0.0167
2025-04-08 18:29:42.124379: Training Step 159/354: batchLoss = 0.5163, diffLoss = 2.5277, kgLoss = 0.0135
2025-04-08 18:29:43.741226: Training Step 160/354: batchLoss = 0.6208, diffLoss = 3.0368, kgLoss = 0.0168
2025-04-08 18:29:45.359376: Training Step 161/354: batchLoss = 0.5866, diffLoss = 2.8775, kgLoss = 0.0138
2025-04-08 18:29:46.969776: Training Step 162/354: batchLoss = 0.5489, diffLoss = 2.6874, kgLoss = 0.0143
2025-04-08 18:29:48.595613: Training Step 163/354: batchLoss = 0.5931, diffLoss = 2.9035, kgLoss = 0.0154
2025-04-08 18:29:50.218901: Training Step 164/354: batchLoss = 0.4899, diffLoss = 2.3987, kgLoss = 0.0127
2025-04-08 18:29:51.840761: Training Step 165/354: batchLoss = 0.5113, diffLoss = 2.4954, kgLoss = 0.0153
2025-04-08 18:29:53.457557: Training Step 166/354: batchLoss = 0.5744, diffLoss = 2.8141, kgLoss = 0.0145
2025-04-08 18:29:55.082227: Training Step 167/354: batchLoss = 0.8729, diffLoss = 4.2695, kgLoss = 0.0237
2025-04-08 18:29:56.709289: Training Step 168/354: batchLoss = 0.4762, diffLoss = 2.3193, kgLoss = 0.0155
2025-04-08 18:29:58.329535: Training Step 169/354: batchLoss = 0.5360, diffLoss = 2.6256, kgLoss = 0.0136
2025-04-08 18:29:59.950691: Training Step 170/354: batchLoss = 0.5014, diffLoss = 2.4507, kgLoss = 0.0140
2025-04-08 18:30:01.569585: Training Step 171/354: batchLoss = 0.6240, diffLoss = 3.0567, kgLoss = 0.0158
2025-04-08 18:30:03.187417: Training Step 172/354: batchLoss = 0.5477, diffLoss = 2.6823, kgLoss = 0.0140
2025-04-08 18:30:04.810323: Training Step 173/354: batchLoss = 0.7296, diffLoss = 3.5711, kgLoss = 0.0192
2025-04-08 18:30:06.446461: Training Step 174/354: batchLoss = 0.5645, diffLoss = 2.7708, kgLoss = 0.0129
2025-04-08 18:30:08.064422: Training Step 175/354: batchLoss = 0.5173, diffLoss = 2.5333, kgLoss = 0.0132
2025-04-08 18:30:09.689020: Training Step 176/354: batchLoss = 0.5917, diffLoss = 2.8985, kgLoss = 0.0150
2025-04-08 18:30:11.310825: Training Step 177/354: batchLoss = 0.6055, diffLoss = 2.9657, kgLoss = 0.0154
2025-04-08 18:30:12.935217: Training Step 178/354: batchLoss = 0.5694, diffLoss = 2.7883, kgLoss = 0.0146
2025-04-08 18:30:14.555822: Training Step 179/354: batchLoss = 0.9643, diffLoss = 4.7238, kgLoss = 0.0244
2025-04-08 18:30:16.170913: Training Step 180/354: batchLoss = 0.5898, diffLoss = 2.8924, kgLoss = 0.0141
2025-04-08 18:30:17.789406: Training Step 181/354: batchLoss = 0.6280, diffLoss = 3.0827, kgLoss = 0.0143
2025-04-08 18:30:19.414934: Training Step 182/354: batchLoss = 0.5577, diffLoss = 2.7307, kgLoss = 0.0144
2025-04-08 18:30:21.040787: Training Step 183/354: batchLoss = 0.4793, diffLoss = 2.3438, kgLoss = 0.0132
2025-04-08 18:30:22.661043: Training Step 184/354: batchLoss = 0.6254, diffLoss = 3.0587, kgLoss = 0.0171
2025-04-08 18:30:24.286732: Training Step 185/354: batchLoss = 0.4845, diffLoss = 2.3719, kgLoss = 0.0127
2025-04-08 18:30:25.906999: Training Step 186/354: batchLoss = 0.5231, diffLoss = 2.5615, kgLoss = 0.0135
2025-04-08 18:30:27.530601: Training Step 187/354: batchLoss = 0.6123, diffLoss = 3.0043, kgLoss = 0.0143
2025-04-08 18:30:29.156166: Training Step 188/354: batchLoss = 0.5287, diffLoss = 2.5888, kgLoss = 0.0137
2025-04-08 18:30:30.770440: Training Step 189/354: batchLoss = 0.4673, diffLoss = 2.2821, kgLoss = 0.0136
2025-04-08 18:30:32.387552: Training Step 190/354: batchLoss = 0.5704, diffLoss = 2.7968, kgLoss = 0.0138
2025-04-08 18:30:34.007492: Training Step 191/354: batchLoss = 0.6201, diffLoss = 3.0376, kgLoss = 0.0158
2025-04-08 18:30:35.625273: Training Step 192/354: batchLoss = 0.5332, diffLoss = 2.6067, kgLoss = 0.0148
2025-04-08 18:30:37.244414: Training Step 193/354: batchLoss = 0.4657, diffLoss = 2.2768, kgLoss = 0.0130
2025-04-08 18:30:38.863600: Training Step 194/354: batchLoss = 0.5455, diffLoss = 2.6670, kgLoss = 0.0151
2025-04-08 18:30:40.479886: Training Step 195/354: batchLoss = 0.6362, diffLoss = 3.1213, kgLoss = 0.0149
2025-04-08 18:30:42.106489: Training Step 196/354: batchLoss = 0.6301, diffLoss = 3.0849, kgLoss = 0.0164
2025-04-08 18:30:43.727328: Training Step 197/354: batchLoss = 0.5064, diffLoss = 2.4816, kgLoss = 0.0126
2025-04-08 18:30:45.347294: Training Step 198/354: batchLoss = 0.5674, diffLoss = 2.7780, kgLoss = 0.0147
2025-04-08 18:30:46.963626: Training Step 199/354: batchLoss = 0.5675, diffLoss = 2.7784, kgLoss = 0.0148
2025-04-08 18:30:48.577972: Training Step 200/354: batchLoss = 0.5314, diffLoss = 2.6053, kgLoss = 0.0130
2025-04-08 18:30:50.187501: Training Step 201/354: batchLoss = 0.4496, diffLoss = 2.1960, kgLoss = 0.0130
2025-04-08 18:30:51.802004: Training Step 202/354: batchLoss = 0.7648, diffLoss = 3.7395, kgLoss = 0.0211
2025-04-08 18:30:53.423809: Training Step 203/354: batchLoss = 0.4696, diffLoss = 2.2994, kgLoss = 0.0122
2025-04-08 18:30:55.039151: Training Step 204/354: batchLoss = 0.5197, diffLoss = 2.5443, kgLoss = 0.0135
2025-04-08 18:30:56.653780: Training Step 205/354: batchLoss = 0.5305, diffLoss = 2.6024, kgLoss = 0.0125
2025-04-08 18:30:58.272022: Training Step 206/354: batchLoss = 0.6394, diffLoss = 3.1383, kgLoss = 0.0147
2025-04-08 18:30:59.893082: Training Step 207/354: batchLoss = 0.5794, diffLoss = 2.8430, kgLoss = 0.0135
2025-04-08 18:31:01.509044: Training Step 208/354: batchLoss = 0.4688, diffLoss = 2.2863, kgLoss = 0.0145
2025-04-08 18:31:03.124622: Training Step 209/354: batchLoss = 0.5691, diffLoss = 2.7871, kgLoss = 0.0146
2025-04-08 18:31:04.752943: Training Step 210/354: batchLoss = 0.5885, diffLoss = 2.8847, kgLoss = 0.0144
2025-04-08 18:31:06.364711: Training Step 211/354: batchLoss = 0.4691, diffLoss = 2.2947, kgLoss = 0.0127
2025-04-08 18:31:07.981837: Training Step 212/354: batchLoss = 0.5244, diffLoss = 2.5670, kgLoss = 0.0137
2025-04-08 18:31:09.594705: Training Step 213/354: batchLoss = 0.6230, diffLoss = 3.0496, kgLoss = 0.0163
2025-04-08 18:31:11.214270: Training Step 214/354: batchLoss = 0.4803, diffLoss = 2.3554, kgLoss = 0.0115
2025-04-08 18:31:12.834327: Training Step 215/354: batchLoss = 0.5567, diffLoss = 2.7236, kgLoss = 0.0149
2025-04-08 18:31:14.452209: Training Step 216/354: batchLoss = 0.4982, diffLoss = 2.4400, kgLoss = 0.0127
2025-04-08 18:31:16.070794: Training Step 217/354: batchLoss = 0.5061, diffLoss = 2.4754, kgLoss = 0.0137
2025-04-08 18:31:17.688101: Training Step 218/354: batchLoss = 0.6080, diffLoss = 2.9700, kgLoss = 0.0175
2025-04-08 18:31:19.303671: Training Step 219/354: batchLoss = 0.5097, diffLoss = 2.4889, kgLoss = 0.0149
2025-04-08 18:31:20.923132: Training Step 220/354: batchLoss = 0.5537, diffLoss = 2.7149, kgLoss = 0.0134
2025-04-08 18:31:22.542289: Training Step 221/354: batchLoss = 0.5508, diffLoss = 2.7011, kgLoss = 0.0132
2025-04-08 18:31:24.159923: Training Step 222/354: batchLoss = 0.5847, diffLoss = 2.8632, kgLoss = 0.0151
2025-04-08 18:31:25.777456: Training Step 223/354: batchLoss = 0.5234, diffLoss = 2.5609, kgLoss = 0.0140
2025-04-08 18:31:27.401513: Training Step 224/354: batchLoss = 0.7292, diffLoss = 3.5775, kgLoss = 0.0171
2025-04-08 18:31:29.025005: Training Step 225/354: batchLoss = 0.4911, diffLoss = 2.3962, kgLoss = 0.0149
2025-04-08 18:31:30.636871: Training Step 226/354: batchLoss = 0.5046, diffLoss = 2.4653, kgLoss = 0.0144
2025-04-08 18:31:32.257014: Training Step 227/354: batchLoss = 0.6583, diffLoss = 3.2225, kgLoss = 0.0172
2025-04-08 18:31:33.874735: Training Step 228/354: batchLoss = 0.5612, diffLoss = 2.7520, kgLoss = 0.0135
2025-04-08 18:31:35.490163: Training Step 229/354: batchLoss = 0.5867, diffLoss = 2.8772, kgLoss = 0.0141
2025-04-08 18:31:37.109785: Training Step 230/354: batchLoss = 0.4988, diffLoss = 2.4429, kgLoss = 0.0128
2025-04-08 18:31:38.728444: Training Step 231/354: batchLoss = 0.4354, diffLoss = 2.1251, kgLoss = 0.0130
2025-04-08 18:31:40.348093: Training Step 232/354: batchLoss = 0.6061, diffLoss = 2.9702, kgLoss = 0.0151
2025-04-08 18:31:41.958732: Training Step 233/354: batchLoss = 0.5636, diffLoss = 2.7652, kgLoss = 0.0132
2025-04-08 18:31:43.573877: Training Step 234/354: batchLoss = 0.5540, diffLoss = 2.7130, kgLoss = 0.0142
2025-04-08 18:31:45.194243: Training Step 235/354: batchLoss = 0.5491, diffLoss = 2.6910, kgLoss = 0.0136
2025-04-08 18:31:46.812208: Training Step 236/354: batchLoss = 0.4671, diffLoss = 2.2875, kgLoss = 0.0120
2025-04-08 18:31:48.433628: Training Step 237/354: batchLoss = 0.7072, diffLoss = 3.4659, kgLoss = 0.0176
2025-04-08 18:31:50.045360: Training Step 238/354: batchLoss = 0.4736, diffLoss = 2.3085, kgLoss = 0.0148
2025-04-08 18:31:51.659948: Training Step 239/354: batchLoss = 0.6622, diffLoss = 3.2418, kgLoss = 0.0173
2025-04-08 18:31:53.272068: Training Step 240/354: batchLoss = 0.5396, diffLoss = 2.6432, kgLoss = 0.0136
2025-04-08 18:31:54.885345: Training Step 241/354: batchLoss = 0.5695, diffLoss = 2.7938, kgLoss = 0.0135
2025-04-08 18:31:56.503780: Training Step 242/354: batchLoss = 0.5383, diffLoss = 2.6328, kgLoss = 0.0147
2025-04-08 18:31:58.126960: Training Step 243/354: batchLoss = 0.5289, diffLoss = 2.5912, kgLoss = 0.0133
2025-04-08 18:31:59.758056: Training Step 244/354: batchLoss = 0.4573, diffLoss = 2.2419, kgLoss = 0.0112
2025-04-08 18:32:01.376911: Training Step 245/354: batchLoss = 0.5307, diffLoss = 2.6008, kgLoss = 0.0132
2025-04-08 18:32:02.997759: Training Step 246/354: batchLoss = 0.5371, diffLoss = 2.6250, kgLoss = 0.0151
2025-04-08 18:32:04.613314: Training Step 247/354: batchLoss = 0.5013, diffLoss = 2.4570, kgLoss = 0.0124
2025-04-08 18:32:06.229861: Training Step 248/354: batchLoss = 0.5595, diffLoss = 2.7409, kgLoss = 0.0142
2025-04-08 18:32:07.846660: Training Step 249/354: batchLoss = 0.5128, diffLoss = 2.5125, kgLoss = 0.0129
2025-04-08 18:32:09.463152: Training Step 250/354: batchLoss = 0.4952, diffLoss = 2.4173, kgLoss = 0.0147
2025-04-08 18:32:11.080781: Training Step 251/354: batchLoss = 0.5383, diffLoss = 2.6353, kgLoss = 0.0141
2025-04-08 18:32:12.704017: Training Step 252/354: batchLoss = 0.5790, diffLoss = 2.8325, kgLoss = 0.0156
2025-04-08 18:32:14.318821: Training Step 253/354: batchLoss = 0.6167, diffLoss = 3.0305, kgLoss = 0.0132
2025-04-08 18:32:15.936969: Training Step 254/354: batchLoss = 0.6536, diffLoss = 3.2038, kgLoss = 0.0160
2025-04-08 18:32:17.552382: Training Step 255/354: batchLoss = 0.4876, diffLoss = 2.3904, kgLoss = 0.0119
2025-04-08 18:32:19.171420: Training Step 256/354: batchLoss = 0.5688, diffLoss = 2.7892, kgLoss = 0.0137
2025-04-08 18:32:20.789938: Training Step 257/354: batchLoss = 0.5715, diffLoss = 2.7998, kgLoss = 0.0144
2025-04-08 18:32:22.406942: Training Step 258/354: batchLoss = 0.5768, diffLoss = 2.8266, kgLoss = 0.0143
2025-04-08 18:32:24.024448: Training Step 259/354: batchLoss = 0.5305, diffLoss = 2.6033, kgLoss = 0.0124
2025-04-08 18:32:25.643321: Training Step 260/354: batchLoss = 0.5459, diffLoss = 2.6733, kgLoss = 0.0141
2025-04-08 18:32:27.253495: Training Step 261/354: batchLoss = 0.5316, diffLoss = 2.6016, kgLoss = 0.0141
2025-04-08 18:32:28.875002: Training Step 262/354: batchLoss = 0.6335, diffLoss = 3.1000, kgLoss = 0.0169
2025-04-08 18:32:30.493817: Training Step 263/354: batchLoss = 0.5901, diffLoss = 2.8863, kgLoss = 0.0160
2025-04-08 18:32:32.110289: Training Step 264/354: batchLoss = 0.6595, diffLoss = 3.2323, kgLoss = 0.0163
2025-04-08 18:32:33.733636: Training Step 265/354: batchLoss = 0.5177, diffLoss = 2.5311, kgLoss = 0.0144
2025-04-08 18:32:35.348320: Training Step 266/354: batchLoss = 0.4944, diffLoss = 2.4193, kgLoss = 0.0131
2025-04-08 18:32:36.969322: Training Step 267/354: batchLoss = 0.5497, diffLoss = 2.6886, kgLoss = 0.0149
2025-04-08 18:32:38.585723: Training Step 268/354: batchLoss = 0.5087, diffLoss = 2.4880, kgLoss = 0.0139
2025-04-08 18:32:40.202501: Training Step 269/354: batchLoss = 0.5427, diffLoss = 2.6612, kgLoss = 0.0130
2025-04-08 18:32:41.815179: Training Step 270/354: batchLoss = 0.5843, diffLoss = 2.8661, kgLoss = 0.0139
2025-04-08 18:32:43.432986: Training Step 271/354: batchLoss = 0.4935, diffLoss = 2.4132, kgLoss = 0.0136
2025-04-08 18:32:45.059965: Training Step 272/354: batchLoss = 0.4603, diffLoss = 2.2446, kgLoss = 0.0143
2025-04-08 18:32:46.676599: Training Step 273/354: batchLoss = 0.5957, diffLoss = 2.9166, kgLoss = 0.0155
2025-04-08 18:32:48.300565: Training Step 274/354: batchLoss = 0.5179, diffLoss = 2.5298, kgLoss = 0.0150
2025-04-08 18:32:49.932723: Training Step 275/354: batchLoss = 0.5560, diffLoss = 2.7272, kgLoss = 0.0132
2025-04-08 18:32:51.556345: Training Step 276/354: batchLoss = 0.6017, diffLoss = 2.9455, kgLoss = 0.0157
2025-04-08 18:32:53.175626: Training Step 277/354: batchLoss = 0.5688, diffLoss = 2.7819, kgLoss = 0.0156
2025-04-08 18:32:54.793423: Training Step 278/354: batchLoss = 0.5236, diffLoss = 2.5648, kgLoss = 0.0133
2025-04-08 18:32:56.404499: Training Step 279/354: batchLoss = 0.4899, diffLoss = 2.3999, kgLoss = 0.0124
2025-04-08 18:32:58.021640: Training Step 280/354: batchLoss = 0.5155, diffLoss = 2.5205, kgLoss = 0.0143
2025-04-08 18:32:59.638020: Training Step 281/354: batchLoss = 0.5670, diffLoss = 2.7746, kgLoss = 0.0151
2025-04-08 18:33:01.259430: Training Step 282/354: batchLoss = 0.5710, diffLoss = 2.8075, kgLoss = 0.0119
2025-04-08 18:33:02.882729: Training Step 283/354: batchLoss = 0.5609, diffLoss = 2.7433, kgLoss = 0.0153
2025-04-08 18:33:04.512232: Training Step 284/354: batchLoss = 0.6628, diffLoss = 3.2474, kgLoss = 0.0166
2025-04-08 18:33:06.139867: Training Step 285/354: batchLoss = 0.5472, diffLoss = 2.6785, kgLoss = 0.0143
2025-04-08 18:33:07.761905: Training Step 286/354: batchLoss = 0.6274, diffLoss = 3.0689, kgLoss = 0.0170
2025-04-08 18:33:09.385353: Training Step 287/354: batchLoss = 0.6288, diffLoss = 3.0769, kgLoss = 0.0168
2025-04-08 18:33:10.999306: Training Step 288/354: batchLoss = 0.5438, diffLoss = 2.6561, kgLoss = 0.0157
2025-04-08 18:33:12.613819: Training Step 289/354: batchLoss = 0.4646, diffLoss = 2.2745, kgLoss = 0.0121
2025-04-08 18:33:14.225950: Training Step 290/354: batchLoss = 0.4552, diffLoss = 2.2245, kgLoss = 0.0129
2025-04-08 18:33:15.845311: Training Step 291/354: batchLoss = 0.4555, diffLoss = 2.2191, kgLoss = 0.0146
2025-04-08 18:33:17.469663: Training Step 292/354: batchLoss = 0.4829, diffLoss = 2.3596, kgLoss = 0.0137
2025-04-08 18:33:19.091376: Training Step 293/354: batchLoss = 0.5177, diffLoss = 2.5386, kgLoss = 0.0124
2025-04-08 18:33:20.718808: Training Step 294/354: batchLoss = 0.7351, diffLoss = 3.6022, kgLoss = 0.0183
2025-04-08 18:33:22.343835: Training Step 295/354: batchLoss = 0.5516, diffLoss = 2.7041, kgLoss = 0.0134
2025-04-08 18:33:23.966369: Training Step 296/354: batchLoss = 0.4745, diffLoss = 2.3255, kgLoss = 0.0118
2025-04-08 18:33:25.587980: Training Step 297/354: batchLoss = 0.5756, diffLoss = 2.8203, kgLoss = 0.0144
2025-04-08 18:33:27.208204: Training Step 298/354: batchLoss = 0.7294, diffLoss = 3.5727, kgLoss = 0.0185
2025-04-08 18:33:28.819996: Training Step 299/354: batchLoss = 0.5524, diffLoss = 2.7046, kgLoss = 0.0143
2025-04-08 18:33:30.439725: Training Step 300/354: batchLoss = 0.6584, diffLoss = 3.2187, kgLoss = 0.0184
2025-04-08 18:33:32.059273: Training Step 301/354: batchLoss = 0.4864, diffLoss = 2.3779, kgLoss = 0.0135
2025-04-08 18:33:33.673836: Training Step 302/354: batchLoss = 0.4962, diffLoss = 2.4227, kgLoss = 0.0146
2025-04-08 18:33:35.302055: Training Step 303/354: batchLoss = 0.5367, diffLoss = 2.6214, kgLoss = 0.0155
2025-04-08 18:33:36.919723: Training Step 304/354: batchLoss = 0.5203, diffLoss = 2.5505, kgLoss = 0.0127
2025-04-08 18:33:38.543340: Training Step 305/354: batchLoss = 0.4912, diffLoss = 2.4082, kgLoss = 0.0119
2025-04-08 18:33:40.166792: Training Step 306/354: batchLoss = 0.4945, diffLoss = 2.4210, kgLoss = 0.0129
2025-04-08 18:33:41.790157: Training Step 307/354: batchLoss = 0.7206, diffLoss = 3.5304, kgLoss = 0.0182
2025-04-08 18:33:43.405931: Training Step 308/354: batchLoss = 0.5227, diffLoss = 2.5597, kgLoss = 0.0134
2025-04-08 18:33:45.020375: Training Step 309/354: batchLoss = 0.5282, diffLoss = 2.5857, kgLoss = 0.0138
2025-04-08 18:33:46.636869: Training Step 310/354: batchLoss = 0.5581, diffLoss = 2.7358, kgLoss = 0.0137
2025-04-08 18:33:48.259362: Training Step 311/354: batchLoss = 0.5755, diffLoss = 2.8092, kgLoss = 0.0171
2025-04-08 18:33:49.889739: Training Step 312/354: batchLoss = 0.4734, diffLoss = 2.3206, kgLoss = 0.0116
2025-04-08 18:33:51.511354: Training Step 313/354: batchLoss = 0.5878, diffLoss = 2.8772, kgLoss = 0.0155
2025-04-08 18:33:53.137222: Training Step 314/354: batchLoss = 0.5315, diffLoss = 2.6071, kgLoss = 0.0126
2025-04-08 18:33:54.767101: Training Step 315/354: batchLoss = 0.5260, diffLoss = 2.5788, kgLoss = 0.0129
2025-04-08 18:33:56.392483: Training Step 316/354: batchLoss = 0.4865, diffLoss = 2.3828, kgLoss = 0.0125
2025-04-08 18:33:58.012957: Training Step 317/354: batchLoss = 0.4301, diffLoss = 2.0997, kgLoss = 0.0127
2025-04-08 18:33:59.628002: Training Step 318/354: batchLoss = 0.6389, diffLoss = 3.1230, kgLoss = 0.0178
2025-04-08 18:34:01.246084: Training Step 319/354: batchLoss = 0.9458, diffLoss = 4.6275, kgLoss = 0.0253
2025-04-08 18:34:02.865234: Training Step 320/354: batchLoss = 0.5439, diffLoss = 2.6581, kgLoss = 0.0153
2025-04-08 18:34:04.480835: Training Step 321/354: batchLoss = 0.6187, diffLoss = 3.0334, kgLoss = 0.0150
2025-04-08 18:34:06.097155: Training Step 322/354: batchLoss = 0.4718, diffLoss = 2.3072, kgLoss = 0.0130
2025-04-08 18:34:07.720728: Training Step 323/354: batchLoss = 0.5137, diffLoss = 2.5157, kgLoss = 0.0132
2025-04-08 18:34:09.344038: Training Step 324/354: batchLoss = 0.5578, diffLoss = 2.7159, kgLoss = 0.0182
2025-04-08 18:34:10.965668: Training Step 325/354: batchLoss = 0.5260, diffLoss = 2.5664, kgLoss = 0.0159
2025-04-08 18:34:12.589254: Training Step 326/354: batchLoss = 0.5248, diffLoss = 2.5645, kgLoss = 0.0149
2025-04-08 18:34:14.208956: Training Step 327/354: batchLoss = 0.5075, diffLoss = 2.4835, kgLoss = 0.0135
2025-04-08 18:34:15.830461: Training Step 328/354: batchLoss = 0.5139, diffLoss = 2.5183, kgLoss = 0.0128
2025-04-08 18:34:17.445692: Training Step 329/354: batchLoss = 0.5043, diffLoss = 2.4591, kgLoss = 0.0156
2025-04-08 18:34:19.063646: Training Step 330/354: batchLoss = 0.7507, diffLoss = 3.6775, kgLoss = 0.0190
2025-04-08 18:34:20.685840: Training Step 331/354: batchLoss = 0.6130, diffLoss = 2.9946, kgLoss = 0.0176
2025-04-08 18:34:22.302049: Training Step 332/354: batchLoss = 0.4960, diffLoss = 2.4251, kgLoss = 0.0138
2025-04-08 18:34:23.930827: Training Step 333/354: batchLoss = 0.5524, diffLoss = 2.7050, kgLoss = 0.0143
2025-04-08 18:34:25.550209: Training Step 334/354: batchLoss = 0.4339, diffLoss = 2.1260, kgLoss = 0.0109
2025-04-08 18:34:27.172040: Training Step 335/354: batchLoss = 0.5886, diffLoss = 2.8912, kgLoss = 0.0130
2025-04-08 18:34:28.796305: Training Step 336/354: batchLoss = 0.4976, diffLoss = 2.4341, kgLoss = 0.0135
2025-04-08 18:34:30.407958: Training Step 337/354: batchLoss = 0.7206, diffLoss = 3.5312, kgLoss = 0.0180
2025-04-08 18:34:32.021203: Training Step 338/354: batchLoss = 0.6182, diffLoss = 3.0250, kgLoss = 0.0164
2025-04-08 18:34:33.640044: Training Step 339/354: batchLoss = 0.6244, diffLoss = 3.0553, kgLoss = 0.0167
2025-04-08 18:34:35.256323: Training Step 340/354: batchLoss = 0.5449, diffLoss = 2.6660, kgLoss = 0.0146
2025-04-08 18:34:36.877043: Training Step 341/354: batchLoss = 0.5233, diffLoss = 2.5624, kgLoss = 0.0135
2025-04-08 18:34:38.498064: Training Step 342/354: batchLoss = 0.4499, diffLoss = 2.2053, kgLoss = 0.0111
2025-04-08 18:34:40.116704: Training Step 343/354: batchLoss = 0.4850, diffLoss = 2.3731, kgLoss = 0.0130
2025-04-08 18:34:41.741370: Training Step 344/354: batchLoss = 0.6152, diffLoss = 3.0086, kgLoss = 0.0168
2025-04-08 18:34:43.367191: Training Step 345/354: batchLoss = 0.4683, diffLoss = 2.2896, kgLoss = 0.0130
2025-04-08 18:34:44.985235: Training Step 346/354: batchLoss = 0.6000, diffLoss = 2.9421, kgLoss = 0.0145
2025-04-08 18:34:46.598945: Training Step 347/354: batchLoss = 0.5436, diffLoss = 2.6604, kgLoss = 0.0144
2025-04-08 18:34:48.214394: Training Step 348/354: batchLoss = 0.6416, diffLoss = 3.1399, kgLoss = 0.0170
2025-04-08 18:34:49.831785: Training Step 349/354: batchLoss = 0.4692, diffLoss = 2.2945, kgLoss = 0.0129
2025-04-08 18:34:51.452525: Training Step 350/354: batchLoss = 0.5148, diffLoss = 2.5259, kgLoss = 0.0120
2025-04-08 18:34:53.067943: Training Step 351/354: batchLoss = 0.6223, diffLoss = 3.0467, kgLoss = 0.0162
2025-04-08 18:34:54.668211: Training Step 352/354: batchLoss = 0.5553, diffLoss = 2.7145, kgLoss = 0.0155
2025-04-08 18:34:56.076429: Training Step 353/354: batchLoss = 0.5485, diffLoss = 2.6795, kgLoss = 0.0158
2025-04-08 18:34:56.162797: 
2025-04-08 18:34:56.163446: Epoch 28/1000, Train: epLoss = 0.9995, epDfLoss = 4.8931, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 18:34:57.469084: Steps 0/138: batch_recall = 48.00, batch_ndcg = 26.69 
2025-04-08 18:34:58.827976: Steps 1/138: batch_recall = 48.19, batch_ndcg = 28.06 
2025-04-08 18:35:00.155783: Steps 2/138: batch_recall = 58.62, batch_ndcg = 35.49 
2025-04-08 18:35:01.481831: Steps 3/138: batch_recall = 58.61, batch_ndcg = 34.05 
2025-04-08 18:35:02.780982: Steps 4/138: batch_recall = 67.34, batch_ndcg = 40.99 
2025-04-08 18:35:04.106203: Steps 5/138: batch_recall = 57.52, batch_ndcg = 31.51 
2025-04-08 18:35:05.429103: Steps 6/138: batch_recall = 52.86, batch_ndcg = 32.14 
2025-04-08 18:35:06.765498: Steps 7/138: batch_recall = 62.53, batch_ndcg = 41.34 
2025-04-08 18:35:08.081755: Steps 8/138: batch_recall = 62.78, batch_ndcg = 39.93 
2025-04-08 18:35:09.396470: Steps 9/138: batch_recall = 59.02, batch_ndcg = 34.66 
2025-04-08 18:35:10.696639: Steps 10/138: batch_recall = 58.59, batch_ndcg = 32.90 
2025-04-08 18:35:11.988954: Steps 11/138: batch_recall = 56.56, batch_ndcg = 32.87 
2025-04-08 18:35:13.297376: Steps 12/138: batch_recall = 51.69, batch_ndcg = 28.99 
2025-04-08 18:35:14.594874: Steps 13/138: batch_recall = 53.48, batch_ndcg = 31.96 
2025-04-08 18:35:15.887632: Steps 14/138: batch_recall = 53.63, batch_ndcg = 31.46 
2025-04-08 18:35:17.188051: Steps 15/138: batch_recall = 48.07, batch_ndcg = 29.85 
2025-04-08 18:35:18.484232: Steps 16/138: batch_recall = 60.85, batch_ndcg = 34.49 
2025-04-08 18:35:19.763615: Steps 17/138: batch_recall = 57.75, batch_ndcg = 33.92 
2025-04-08 18:35:21.062932: Steps 18/138: batch_recall = 52.92, batch_ndcg = 33.08 
2025-04-08 18:35:22.360368: Steps 19/138: batch_recall = 52.99, batch_ndcg = 32.69 
2025-04-08 18:35:23.667135: Steps 20/138: batch_recall = 62.37, batch_ndcg = 36.47 
2025-04-08 18:35:24.966881: Steps 21/138: batch_recall = 69.80, batch_ndcg = 41.16 
2025-04-08 18:35:26.264806: Steps 22/138: batch_recall = 55.10, batch_ndcg = 32.64 
2025-04-08 18:35:27.561449: Steps 23/138: batch_recall = 50.13, batch_ndcg = 30.39 
2025-04-08 18:35:28.857568: Steps 24/138: batch_recall = 56.65, batch_ndcg = 31.09 
2025-04-08 18:35:30.158471: Steps 25/138: batch_recall = 61.38, batch_ndcg = 35.31 
2025-04-08 18:35:31.457858: Steps 26/138: batch_recall = 56.50, batch_ndcg = 32.86 
2025-04-08 18:35:32.753395: Steps 27/138: batch_recall = 55.95, batch_ndcg = 32.23 
2025-04-08 18:35:34.040155: Steps 28/138: batch_recall = 58.78, batch_ndcg = 33.35 
2025-04-08 18:35:35.328499: Steps 29/138: batch_recall = 61.98, batch_ndcg = 32.78 
2025-04-08 18:35:36.622309: Steps 30/138: batch_recall = 59.18, batch_ndcg = 34.32 
2025-04-08 18:35:37.904350: Steps 31/138: batch_recall = 48.56, batch_ndcg = 27.14 
2025-04-08 18:35:39.194012: Steps 32/138: batch_recall = 53.41, batch_ndcg = 31.23 
2025-04-08 18:35:40.489923: Steps 33/138: batch_recall = 62.51, batch_ndcg = 33.96 
2025-04-08 18:35:41.781664: Steps 34/138: batch_recall = 56.71, batch_ndcg = 30.58 
2025-04-08 18:35:43.077455: Steps 35/138: batch_recall = 53.05, batch_ndcg = 30.37 
2025-04-08 18:35:44.365314: Steps 36/138: batch_recall = 46.47, batch_ndcg = 26.55 
2025-04-08 18:35:45.656682: Steps 37/138: batch_recall = 57.32, batch_ndcg = 34.13 
2025-04-08 18:35:46.955427: Steps 38/138: batch_recall = 59.88, batch_ndcg = 33.01 
2025-04-08 18:35:48.239331: Steps 39/138: batch_recall = 67.62, batch_ndcg = 38.95 
2025-04-08 18:35:49.537681: Steps 40/138: batch_recall = 60.54, batch_ndcg = 30.58 
2025-04-08 18:35:50.815991: Steps 41/138: batch_recall = 61.02, batch_ndcg = 34.10 
2025-04-08 18:35:52.091213: Steps 42/138: batch_recall = 54.02, batch_ndcg = 30.23 
2025-04-08 18:35:53.376661: Steps 43/138: batch_recall = 57.74, batch_ndcg = 36.24 
2025-04-08 18:35:54.663178: Steps 44/138: batch_recall = 56.88, batch_ndcg = 31.14 
2025-04-08 18:35:55.960990: Steps 45/138: batch_recall = 63.74, batch_ndcg = 35.92 
2025-04-08 18:35:57.239896: Steps 46/138: batch_recall = 61.95, batch_ndcg = 36.63 
2025-04-08 18:35:58.521746: Steps 47/138: batch_recall = 53.12, batch_ndcg = 32.06 
2025-04-08 18:35:59.807691: Steps 48/138: batch_recall = 62.74, batch_ndcg = 35.96 
2025-04-08 18:36:01.087042: Steps 49/138: batch_recall = 65.09, batch_ndcg = 37.32 
2025-04-08 18:36:02.372603: Steps 50/138: batch_recall = 61.50, batch_ndcg = 31.82 
2025-04-08 18:36:03.648924: Steps 51/138: batch_recall = 61.34, batch_ndcg = 35.87 
2025-04-08 18:36:04.929254: Steps 52/138: batch_recall = 65.02, batch_ndcg = 41.77 
2025-04-08 18:36:06.210962: Steps 53/138: batch_recall = 63.78, batch_ndcg = 33.77 
2025-04-08 18:36:07.484454: Steps 54/138: batch_recall = 66.38, batch_ndcg = 37.56 
2025-04-08 18:36:08.758399: Steps 55/138: batch_recall = 60.50, batch_ndcg = 34.31 
2025-04-08 18:36:10.047766: Steps 56/138: batch_recall = 60.99, batch_ndcg = 35.53 
2025-04-08 18:36:11.336489: Steps 57/138: batch_recall = 57.12, batch_ndcg = 32.35 
2025-04-08 18:36:12.625380: Steps 58/138: batch_recall = 70.95, batch_ndcg = 36.67 
2025-04-08 18:36:13.906191: Steps 59/138: batch_recall = 65.66, batch_ndcg = 39.15 
2025-04-08 18:36:15.201297: Steps 60/138: batch_recall = 66.74, batch_ndcg = 37.41 
2025-04-08 18:36:16.487429: Steps 61/138: batch_recall = 64.36, batch_ndcg = 35.18 
2025-04-08 18:36:17.772517: Steps 62/138: batch_recall = 87.29, batch_ndcg = 45.46 
2025-04-08 18:36:19.059695: Steps 63/138: batch_recall = 76.10, batch_ndcg = 42.99 
2025-04-08 18:36:20.348478: Steps 64/138: batch_recall = 61.38, batch_ndcg = 32.26 
2025-04-08 18:36:21.626431: Steps 65/138: batch_recall = 86.03, batch_ndcg = 46.91 
2025-04-08 18:36:22.905108: Steps 66/138: batch_recall = 69.99, batch_ndcg = 41.39 
2025-04-08 18:36:24.185224: Steps 67/138: batch_recall = 75.24, batch_ndcg = 46.04 
2025-04-08 18:36:25.464279: Steps 68/138: batch_recall = 60.73, batch_ndcg = 33.43 
2025-04-08 18:36:26.740894: Steps 69/138: batch_recall = 88.72, batch_ndcg = 52.11 
2025-04-08 18:36:28.019178: Steps 70/138: batch_recall = 79.53, batch_ndcg = 45.11 
2025-04-08 18:36:29.295937: Steps 71/138: batch_recall = 87.20, batch_ndcg = 51.33 
2025-04-08 18:36:30.576147: Steps 72/138: batch_recall = 84.51, batch_ndcg = 49.27 
2025-04-08 18:36:31.852172: Steps 73/138: batch_recall = 82.93, batch_ndcg = 46.59 
2025-04-08 18:36:33.131421: Steps 74/138: batch_recall = 76.24, batch_ndcg = 48.15 
2025-04-08 18:36:34.412670: Steps 75/138: batch_recall = 82.11, batch_ndcg = 48.75 
2025-04-08 18:36:35.693947: Steps 76/138: batch_recall = 93.60, batch_ndcg = 55.36 
2025-04-08 18:36:36.965759: Steps 77/138: batch_recall = 89.74, batch_ndcg = 51.16 
2025-04-08 18:36:38.237618: Steps 78/138: batch_recall = 90.59, batch_ndcg = 48.42 
2025-04-08 18:36:39.512842: Steps 79/138: batch_recall = 91.07, batch_ndcg = 50.23 
2025-04-08 18:36:40.785108: Steps 80/138: batch_recall = 74.29, batch_ndcg = 39.47 
2025-04-08 18:36:42.063732: Steps 81/138: batch_recall = 81.93, batch_ndcg = 48.27 
2025-04-08 18:36:43.340029: Steps 82/138: batch_recall = 87.55, batch_ndcg = 53.92 
2025-04-08 18:36:44.620712: Steps 83/138: batch_recall = 79.92, batch_ndcg = 47.16 
2025-04-08 18:36:45.884938: Steps 84/138: batch_recall = 100.99, batch_ndcg = 57.61 
2025-04-08 18:36:47.166112: Steps 85/138: batch_recall = 107.71, batch_ndcg = 62.79 
2025-04-08 18:36:48.444772: Steps 86/138: batch_recall = 123.16, batch_ndcg = 72.07 
2025-04-08 18:36:49.722673: Steps 87/138: batch_recall = 103.85, batch_ndcg = 54.87 
2025-04-08 18:36:51.006568: Steps 88/138: batch_recall = 98.48, batch_ndcg = 58.19 
2025-04-08 18:36:52.286271: Steps 89/138: batch_recall = 121.01, batch_ndcg = 69.86 
2025-04-08 18:36:53.562560: Steps 90/138: batch_recall = 99.93, batch_ndcg = 57.23 
2025-04-08 18:36:54.833923: Steps 91/138: batch_recall = 117.45, batch_ndcg = 65.52 
2025-04-08 18:36:56.099008: Steps 92/138: batch_recall = 122.87, batch_ndcg = 65.96 
2025-04-08 18:36:57.368994: Steps 93/138: batch_recall = 120.79, batch_ndcg = 68.66 
2025-04-08 18:36:58.644107: Steps 94/138: batch_recall = 126.61, batch_ndcg = 66.80 
2025-04-08 18:36:59.919446: Steps 95/138: batch_recall = 113.84, batch_ndcg = 66.85 
2025-04-08 18:37:01.187462: Steps 96/138: batch_recall = 126.71, batch_ndcg = 77.71 
2025-04-08 18:37:02.465940: Steps 97/138: batch_recall = 135.44, batch_ndcg = 87.32 
2025-04-08 18:37:03.742028: Steps 98/138: batch_recall = 110.04, batch_ndcg = 64.21 
2025-04-08 18:37:05.014878: Steps 99/138: batch_recall = 122.94, batch_ndcg = 71.55 
2025-04-08 18:37:06.291168: Steps 100/138: batch_recall = 129.44, batch_ndcg = 73.07 
2025-04-08 18:37:07.561963: Steps 101/138: batch_recall = 128.22, batch_ndcg = 70.40 
2025-04-08 18:37:08.841415: Steps 102/138: batch_recall = 121.46, batch_ndcg = 70.19 
2025-04-08 18:37:10.122508: Steps 103/138: batch_recall = 141.76, batch_ndcg = 80.93 
2025-04-08 18:37:11.383387: Steps 104/138: batch_recall = 136.81, batch_ndcg = 79.25 
2025-04-08 18:37:12.657558: Steps 105/138: batch_recall = 121.93, batch_ndcg = 68.19 
2025-04-08 18:37:13.929452: Steps 106/138: batch_recall = 105.68, batch_ndcg = 60.42 
2025-04-08 18:37:15.195421: Steps 107/138: batch_recall = 116.10, batch_ndcg = 65.29 
2025-04-08 18:37:16.474338: Steps 108/138: batch_recall = 117.79, batch_ndcg = 71.76 
2025-04-08 18:37:17.745848: Steps 109/138: batch_recall = 137.33, batch_ndcg = 77.71 
2025-04-08 18:37:19.039904: Steps 110/138: batch_recall = 123.45, batch_ndcg = 65.40 
2025-04-08 18:37:20.313825: Steps 111/138: batch_recall = 137.92, batch_ndcg = 84.10 
2025-04-08 18:37:21.588954: Steps 112/138: batch_recall = 154.69, batch_ndcg = 88.88 
2025-04-08 18:37:22.856086: Steps 113/138: batch_recall = 121.62, batch_ndcg = 69.33 
2025-04-08 18:37:24.123796: Steps 114/138: batch_recall = 125.03, batch_ndcg = 72.49 
2025-04-08 18:37:25.396932: Steps 115/138: batch_recall = 120.76, batch_ndcg = 63.59 
2025-04-08 18:37:26.667443: Steps 116/138: batch_recall = 125.26, batch_ndcg = 67.35 
2025-04-08 18:37:27.926793: Steps 117/138: batch_recall = 113.17, batch_ndcg = 66.31 
2025-04-08 18:37:29.194404: Steps 118/138: batch_recall = 124.95, batch_ndcg = 68.70 
2025-04-08 18:37:30.459601: Steps 119/138: batch_recall = 143.16, batch_ndcg = 77.57 
2025-04-08 18:37:31.737482: Steps 120/138: batch_recall = 123.24, batch_ndcg = 71.44 
2025-04-08 18:37:33.009359: Steps 121/138: batch_recall = 148.03, batch_ndcg = 78.42 
2025-04-08 18:37:34.287925: Steps 122/138: batch_recall = 150.63, batch_ndcg = 81.39 
2025-04-08 18:37:35.565444: Steps 123/138: batch_recall = 132.96, batch_ndcg = 74.57 
2025-04-08 18:37:36.841828: Steps 124/138: batch_recall = 153.50, batch_ndcg = 93.38 
2025-04-08 18:37:38.114691: Steps 125/138: batch_recall = 133.38, batch_ndcg = 73.65 
2025-04-08 18:37:39.398705: Steps 126/138: batch_recall = 161.44, batch_ndcg = 88.23 
2025-04-08 18:37:40.662990: Steps 127/138: batch_recall = 144.51, batch_ndcg = 82.27 
2025-04-08 18:37:41.937520: Steps 128/138: batch_recall = 128.86, batch_ndcg = 72.48 
2025-04-08 18:37:43.201002: Steps 129/138: batch_recall = 162.52, batch_ndcg = 92.47 
2025-04-08 18:37:44.467776: Steps 130/138: batch_recall = 134.52, batch_ndcg = 70.57 
2025-04-08 18:37:45.736192: Steps 131/138: batch_recall = 155.30, batch_ndcg = 88.54 
2025-04-08 18:37:46.998876: Steps 132/138: batch_recall = 151.13, batch_ndcg = 84.43 
2025-04-08 18:37:48.267221: Steps 133/138: batch_recall = 150.60, batch_ndcg = 86.38 
2025-04-08 18:37:49.538910: Steps 134/138: batch_recall = 144.88, batch_ndcg = 84.13 
2025-04-08 18:37:50.811317: Steps 135/138: batch_recall = 168.53, batch_ndcg = 96.16 
2025-04-08 18:37:52.079011: Steps 136/138: batch_recall = 153.19, batch_ndcg = 79.71 
2025-04-08 18:37:53.335527: Steps 137/138: batch_recall = 138.95, batch_ndcg = 86.53 
2025-04-08 18:37:53.336072: Epoch 28/1000, Test: Recall = 0.1765, NDCG = 0.1008  

2025-04-08 18:37:55.099108: Training Step 0/354: batchLoss = 0.4459, diffLoss = 2.1758, kgLoss = 0.0134
2025-04-08 18:37:56.722451: Training Step 1/354: batchLoss = 0.6221, diffLoss = 3.0458, kgLoss = 0.0162
2025-04-08 18:37:58.339552: Training Step 2/354: batchLoss = 0.5475, diffLoss = 2.6770, kgLoss = 0.0152
2025-04-08 18:37:59.954649: Training Step 3/354: batchLoss = 0.4844, diffLoss = 2.3739, kgLoss = 0.0121
2025-04-08 18:38:01.571832: Training Step 4/354: batchLoss = 0.4282, diffLoss = 2.0822, kgLoss = 0.0147
2025-04-08 18:38:03.194326: Training Step 5/354: batchLoss = 0.5623, diffLoss = 2.7527, kgLoss = 0.0147
2025-04-08 18:38:04.830203: Training Step 6/354: batchLoss = 0.5976, diffLoss = 2.9281, kgLoss = 0.0150
2025-04-08 18:38:06.458301: Training Step 7/354: batchLoss = 0.4896, diffLoss = 2.3957, kgLoss = 0.0131
2025-04-08 18:38:08.083213: Training Step 8/354: batchLoss = 0.5923, diffLoss = 2.8980, kgLoss = 0.0159
2025-04-08 18:38:09.706262: Training Step 9/354: batchLoss = 0.5619, diffLoss = 2.7507, kgLoss = 0.0146
2025-04-08 18:38:11.327787: Training Step 10/354: batchLoss = 0.5973, diffLoss = 2.9273, kgLoss = 0.0147
2025-04-08 18:38:12.950014: Training Step 11/354: batchLoss = 0.4400, diffLoss = 2.1399, kgLoss = 0.0151
2025-04-08 18:38:14.565543: Training Step 12/354: batchLoss = 0.5751, diffLoss = 2.8193, kgLoss = 0.0141
2025-04-08 18:38:16.188607: Training Step 13/354: batchLoss = 0.5496, diffLoss = 2.6819, kgLoss = 0.0166
2025-04-08 18:38:17.807927: Training Step 14/354: batchLoss = 0.5974, diffLoss = 2.9311, kgLoss = 0.0140
2025-04-08 18:38:19.425481: Training Step 15/354: batchLoss = 0.5857, diffLoss = 2.8682, kgLoss = 0.0151
2025-04-08 18:38:21.051905: Training Step 16/354: batchLoss = 0.5516, diffLoss = 2.6988, kgLoss = 0.0149
2025-04-08 18:38:22.670541: Training Step 17/354: batchLoss = 0.6381, diffLoss = 3.1225, kgLoss = 0.0170
2025-04-08 18:38:24.298131: Training Step 18/354: batchLoss = 0.4800, diffLoss = 2.3548, kgLoss = 0.0113
2025-04-08 18:38:25.918791: Training Step 19/354: batchLoss = 0.4722, diffLoss = 2.3101, kgLoss = 0.0127
2025-04-08 18:38:27.539606: Training Step 20/354: batchLoss = 0.6832, diffLoss = 3.3544, kgLoss = 0.0154
2025-04-08 18:38:29.174579: Training Step 21/354: batchLoss = 0.6065, diffLoss = 2.9688, kgLoss = 0.0159
2025-04-08 18:38:30.794515: Training Step 22/354: batchLoss = 0.5109, diffLoss = 2.4958, kgLoss = 0.0146
2025-04-08 18:38:32.413013: Training Step 23/354: batchLoss = 0.5117, diffLoss = 2.5074, kgLoss = 0.0128
2025-04-08 18:38:34.026986: Training Step 24/354: batchLoss = 0.6228, diffLoss = 3.0532, kgLoss = 0.0152
2025-04-08 18:38:35.648382: Training Step 25/354: batchLoss = 0.4900, diffLoss = 2.4009, kgLoss = 0.0123
2025-04-08 18:38:37.268722: Training Step 26/354: batchLoss = 0.5300, diffLoss = 2.5947, kgLoss = 0.0139
2025-04-08 18:38:38.888133: Training Step 27/354: batchLoss = 0.6686, diffLoss = 3.2775, kgLoss = 0.0164
2025-04-08 18:38:40.504478: Training Step 28/354: batchLoss = 0.5265, diffLoss = 2.5730, kgLoss = 0.0149
2025-04-08 18:38:42.125642: Training Step 29/354: batchLoss = 0.5681, diffLoss = 2.7806, kgLoss = 0.0150
2025-04-08 18:38:43.743824: Training Step 30/354: batchLoss = 0.5707, diffLoss = 2.8014, kgLoss = 0.0131
2025-04-08 18:38:45.365687: Training Step 31/354: batchLoss = 0.4784, diffLoss = 2.3325, kgLoss = 0.0149
2025-04-08 18:38:46.974398: Training Step 32/354: batchLoss = 0.4333, diffLoss = 2.1147, kgLoss = 0.0129
2025-04-08 18:38:48.587631: Training Step 33/354: batchLoss = 0.5233, diffLoss = 2.5591, kgLoss = 0.0143
2025-04-08 18:38:50.198168: Training Step 34/354: batchLoss = 0.5327, diffLoss = 2.6025, kgLoss = 0.0152
2025-04-08 18:38:51.818554: Training Step 35/354: batchLoss = 0.5548, diffLoss = 2.7140, kgLoss = 0.0150
2025-04-08 18:38:53.437808: Training Step 36/354: batchLoss = 0.5727, diffLoss = 2.8089, kgLoss = 0.0136
2025-04-08 18:38:55.058320: Training Step 37/354: batchLoss = 0.6315, diffLoss = 3.0708, kgLoss = 0.0216
2025-04-08 18:38:56.682026: Training Step 38/354: batchLoss = 0.5854, diffLoss = 2.8700, kgLoss = 0.0143
2025-04-08 18:38:58.299033: Training Step 39/354: batchLoss = 0.5584, diffLoss = 2.7357, kgLoss = 0.0140
2025-04-08 18:38:59.919419: Training Step 40/354: batchLoss = 0.5245, diffLoss = 2.5647, kgLoss = 0.0145
2025-04-08 18:39:01.531330: Training Step 41/354: batchLoss = 0.6711, diffLoss = 3.2917, kgLoss = 0.0160
2025-04-08 18:39:03.147828: Training Step 42/354: batchLoss = 0.5620, diffLoss = 2.7565, kgLoss = 0.0134
2025-04-08 18:39:04.760479: Training Step 43/354: batchLoss = 0.4992, diffLoss = 2.4412, kgLoss = 0.0137
2025-04-08 18:39:06.375884: Training Step 44/354: batchLoss = 0.4315, diffLoss = 2.1099, kgLoss = 0.0119
2025-04-08 18:39:07.998174: Training Step 45/354: batchLoss = 0.5134, diffLoss = 2.5067, kgLoss = 0.0151
2025-04-08 18:39:09.617758: Training Step 46/354: batchLoss = 0.6787, diffLoss = 3.3301, kgLoss = 0.0159
2025-04-08 18:39:11.234251: Training Step 47/354: batchLoss = 0.5369, diffLoss = 2.6275, kgLoss = 0.0142
2025-04-08 18:39:12.932318: Training Step 48/354: batchLoss = 0.4805, diffLoss = 2.3492, kgLoss = 0.0133
2025-04-08 18:39:14.556662: Training Step 49/354: batchLoss = 0.6124, diffLoss = 2.9974, kgLoss = 0.0162
2025-04-08 18:39:16.171608: Training Step 50/354: batchLoss = 0.6601, diffLoss = 3.2351, kgLoss = 0.0163
2025-04-08 18:39:17.790316: Training Step 51/354: batchLoss = 0.5724, diffLoss = 2.7974, kgLoss = 0.0162
2025-04-08 18:39:19.397245: Training Step 52/354: batchLoss = 0.5240, diffLoss = 2.5620, kgLoss = 0.0145
2025-04-08 18:39:21.013624: Training Step 53/354: batchLoss = 0.5807, diffLoss = 2.8406, kgLoss = 0.0157
2025-04-08 18:39:22.630568: Training Step 54/354: batchLoss = 0.6392, diffLoss = 3.1310, kgLoss = 0.0163
2025-04-08 18:39:24.250541: Training Step 55/354: batchLoss = 0.4956, diffLoss = 2.4245, kgLoss = 0.0134
2025-04-08 18:39:25.876273: Training Step 56/354: batchLoss = 0.4505, diffLoss = 2.2004, kgLoss = 0.0130
2025-04-08 18:39:27.498585: Training Step 57/354: batchLoss = 0.4700, diffLoss = 2.2928, kgLoss = 0.0143
2025-04-08 18:39:29.122019: Training Step 58/354: batchLoss = 0.5432, diffLoss = 2.6629, kgLoss = 0.0133
2025-04-08 18:39:30.744692: Training Step 59/354: batchLoss = 0.4964, diffLoss = 2.4239, kgLoss = 0.0146
2025-04-08 18:39:32.363068: Training Step 60/354: batchLoss = 0.5769, diffLoss = 2.8291, kgLoss = 0.0139
2025-04-08 18:39:33.975530: Training Step 61/354: batchLoss = 0.5086, diffLoss = 2.4876, kgLoss = 0.0138
2025-04-08 18:39:35.593085: Training Step 62/354: batchLoss = 0.5856, diffLoss = 2.8631, kgLoss = 0.0162
2025-04-08 18:39:37.207336: Training Step 63/354: batchLoss = 0.4921, diffLoss = 2.4096, kgLoss = 0.0127
2025-04-08 18:39:38.830962: Training Step 64/354: batchLoss = 0.5030, diffLoss = 2.4559, kgLoss = 0.0147
2025-04-08 18:39:40.455808: Training Step 65/354: batchLoss = 0.4491, diffLoss = 2.1971, kgLoss = 0.0121
2025-04-08 18:39:42.078257: Training Step 66/354: batchLoss = 0.4250, diffLoss = 2.0779, kgLoss = 0.0118
2025-04-08 18:39:43.696220: Training Step 67/354: batchLoss = 0.5962, diffLoss = 2.9235, kgLoss = 0.0144
2025-04-08 18:39:45.316362: Training Step 68/354: batchLoss = 0.6540, diffLoss = 3.2025, kgLoss = 0.0169
2025-04-08 18:39:46.936188: Training Step 69/354: batchLoss = 0.6169, diffLoss = 3.0211, kgLoss = 0.0159
2025-04-08 18:39:48.553717: Training Step 70/354: batchLoss = 0.5203, diffLoss = 2.5496, kgLoss = 0.0129
2025-04-08 18:39:50.167022: Training Step 71/354: batchLoss = 0.4660, diffLoss = 2.2835, kgLoss = 0.0117
2025-04-08 18:39:51.781554: Training Step 72/354: batchLoss = 0.5201, diffLoss = 2.5499, kgLoss = 0.0127
2025-04-08 18:39:53.398469: Training Step 73/354: batchLoss = 0.4467, diffLoss = 2.1875, kgLoss = 0.0115
2025-04-08 18:39:55.028039: Training Step 74/354: batchLoss = 0.5002, diffLoss = 2.4381, kgLoss = 0.0157
2025-04-08 18:39:56.649213: Training Step 75/354: batchLoss = 0.4653, diffLoss = 2.2733, kgLoss = 0.0133
2025-04-08 18:39:58.274031: Training Step 76/354: batchLoss = 0.5185, diffLoss = 2.5164, kgLoss = 0.0190
2025-04-08 18:39:59.897206: Training Step 77/354: batchLoss = 0.5883, diffLoss = 2.8793, kgLoss = 0.0155
2025-04-08 18:40:01.520829: Training Step 78/354: batchLoss = 0.6095, diffLoss = 2.9881, kgLoss = 0.0148
2025-04-08 18:40:03.148323: Training Step 79/354: batchLoss = 0.5345, diffLoss = 2.6112, kgLoss = 0.0154
2025-04-08 18:40:04.764755: Training Step 80/354: batchLoss = 0.5078, diffLoss = 2.4802, kgLoss = 0.0147
2025-04-08 18:40:06.382374: Training Step 81/354: batchLoss = 0.5555, diffLoss = 2.7180, kgLoss = 0.0149
2025-04-08 18:40:07.994824: Training Step 82/354: batchLoss = 0.6816, diffLoss = 3.3288, kgLoss = 0.0198
2025-04-08 18:40:09.612736: Training Step 83/354: batchLoss = 0.6364, diffLoss = 3.1206, kgLoss = 0.0153
2025-04-08 18:40:11.231025: Training Step 84/354: batchLoss = 0.5491, diffLoss = 2.6857, kgLoss = 0.0150
2025-04-08 18:40:12.854692: Training Step 85/354: batchLoss = 0.5380, diffLoss = 2.6361, kgLoss = 0.0135
2025-04-08 18:40:14.477611: Training Step 86/354: batchLoss = 0.5635, diffLoss = 2.7624, kgLoss = 0.0138
2025-04-08 18:40:16.108547: Training Step 87/354: batchLoss = 0.5718, diffLoss = 2.7918, kgLoss = 0.0168
2025-04-08 18:40:17.725347: Training Step 88/354: batchLoss = 0.4660, diffLoss = 2.2766, kgLoss = 0.0133
2025-04-08 18:40:19.344860: Training Step 89/354: batchLoss = 0.5039, diffLoss = 2.4592, kgLoss = 0.0151
2025-04-08 18:40:20.964775: Training Step 90/354: batchLoss = 0.5404, diffLoss = 2.6451, kgLoss = 0.0142
2025-04-08 18:40:22.581611: Training Step 91/354: batchLoss = 0.5867, diffLoss = 2.8700, kgLoss = 0.0158
2025-04-08 18:40:24.197647: Training Step 92/354: batchLoss = 0.5084, diffLoss = 2.4843, kgLoss = 0.0144
2025-04-08 18:40:25.811820: Training Step 93/354: batchLoss = 0.4590, diffLoss = 2.2456, kgLoss = 0.0124
2025-04-08 18:40:27.426800: Training Step 94/354: batchLoss = 0.6467, diffLoss = 3.1680, kgLoss = 0.0163
2025-04-08 18:40:29.047588: Training Step 95/354: batchLoss = 0.4007, diffLoss = 1.9577, kgLoss = 0.0114
2025-04-08 18:40:30.672934: Training Step 96/354: batchLoss = 0.5787, diffLoss = 2.8369, kgLoss = 0.0141
2025-04-08 18:40:32.297939: Training Step 97/354: batchLoss = 0.4479, diffLoss = 2.1876, kgLoss = 0.0130
2025-04-08 18:40:33.919244: Training Step 98/354: batchLoss = 0.6342, diffLoss = 3.1065, kgLoss = 0.0161
2025-04-08 18:40:35.537207: Training Step 99/354: batchLoss = 0.5097, diffLoss = 2.4947, kgLoss = 0.0134
2025-04-08 18:40:37.159614: Training Step 100/354: batchLoss = 0.6331, diffLoss = 3.0944, kgLoss = 0.0178
2025-04-08 18:40:38.774740: Training Step 101/354: batchLoss = 0.5224, diffLoss = 2.5621, kgLoss = 0.0125
2025-04-08 18:40:40.386778: Training Step 102/354: batchLoss = 0.4332, diffLoss = 2.1194, kgLoss = 0.0117
2025-04-08 18:40:41.997469: Training Step 103/354: batchLoss = 0.4812, diffLoss = 2.3541, kgLoss = 0.0130
2025-04-08 18:40:43.614542: Training Step 104/354: batchLoss = 0.5098, diffLoss = 2.4957, kgLoss = 0.0133
2025-04-08 18:40:45.238306: Training Step 105/354: batchLoss = 0.5463, diffLoss = 2.6673, kgLoss = 0.0160
2025-04-08 18:40:46.860866: Training Step 106/354: batchLoss = 0.4838, diffLoss = 2.3734, kgLoss = 0.0114
2025-04-08 18:40:48.481935: Training Step 107/354: batchLoss = 0.6669, diffLoss = 3.2637, kgLoss = 0.0177
2025-04-08 18:40:50.105824: Training Step 108/354: batchLoss = 0.4873, diffLoss = 2.3861, kgLoss = 0.0126
2025-04-08 18:40:51.722372: Training Step 109/354: batchLoss = 0.7161, diffLoss = 3.5097, kgLoss = 0.0177
2025-04-08 18:40:53.338188: Training Step 110/354: batchLoss = 0.5698, diffLoss = 2.7922, kgLoss = 0.0141
2025-04-08 18:40:54.959431: Training Step 111/354: batchLoss = 0.5584, diffLoss = 2.7339, kgLoss = 0.0145
2025-04-08 18:40:56.579578: Training Step 112/354: batchLoss = 0.4839, diffLoss = 2.3671, kgLoss = 0.0132
2025-04-08 18:40:58.189104: Training Step 113/354: batchLoss = 0.4953, diffLoss = 2.4312, kgLoss = 0.0113
2025-04-08 18:40:59.805868: Training Step 114/354: batchLoss = 0.5428, diffLoss = 2.6450, kgLoss = 0.0172
2025-04-08 18:41:01.423451: Training Step 115/354: batchLoss = 0.5360, diffLoss = 2.6241, kgLoss = 0.0140
2025-04-08 18:41:03.037502: Training Step 116/354: batchLoss = 0.4573, diffLoss = 2.2312, kgLoss = 0.0139
2025-04-08 18:41:04.658798: Training Step 117/354: batchLoss = 0.5616, diffLoss = 2.7458, kgLoss = 0.0155
2025-04-08 18:41:06.276903: Training Step 118/354: batchLoss = 0.5970, diffLoss = 2.9226, kgLoss = 0.0156
2025-04-08 18:41:07.895149: Training Step 119/354: batchLoss = 0.5527, diffLoss = 2.6986, kgLoss = 0.0162
2025-04-08 18:41:09.506105: Training Step 120/354: batchLoss = 0.6332, diffLoss = 3.0916, kgLoss = 0.0186
2025-04-08 18:41:11.118945: Training Step 121/354: batchLoss = 0.5838, diffLoss = 2.8551, kgLoss = 0.0160
2025-04-08 18:41:12.732091: Training Step 122/354: batchLoss = 0.6254, diffLoss = 3.0592, kgLoss = 0.0169
2025-04-08 18:41:14.348603: Training Step 123/354: batchLoss = 0.4992, diffLoss = 2.4408, kgLoss = 0.0138
2025-04-08 18:41:15.967559: Training Step 124/354: batchLoss = 0.5232, diffLoss = 2.5520, kgLoss = 0.0159
2025-04-08 18:41:17.587721: Training Step 125/354: batchLoss = 0.4906, diffLoss = 2.3960, kgLoss = 0.0143
2025-04-08 18:41:19.207795: Training Step 126/354: batchLoss = 0.5134, diffLoss = 2.5131, kgLoss = 0.0135
2025-04-08 18:41:20.819940: Training Step 127/354: batchLoss = 0.6977, diffLoss = 3.4147, kgLoss = 0.0185
2025-04-08 18:41:22.431858: Training Step 128/354: batchLoss = 0.4889, diffLoss = 2.3905, kgLoss = 0.0135
2025-04-08 18:41:24.053040: Training Step 129/354: batchLoss = 0.4682, diffLoss = 2.2912, kgLoss = 0.0125
2025-04-08 18:41:25.671123: Training Step 130/354: batchLoss = 0.6106, diffLoss = 2.9913, kgLoss = 0.0154
2025-04-08 18:41:27.288531: Training Step 131/354: batchLoss = 0.4647, diffLoss = 2.2746, kgLoss = 0.0122
2025-04-08 18:41:28.904085: Training Step 132/354: batchLoss = 0.5877, diffLoss = 2.8727, kgLoss = 0.0164
2025-04-08 18:41:30.515965: Training Step 133/354: batchLoss = 0.5115, diffLoss = 2.5003, kgLoss = 0.0143
2025-04-08 18:41:32.134105: Training Step 134/354: batchLoss = 0.5409, diffLoss = 2.6453, kgLoss = 0.0148
2025-04-08 18:41:33.757759: Training Step 135/354: batchLoss = 0.6390, diffLoss = 3.1315, kgLoss = 0.0158
2025-04-08 18:41:35.375174: Training Step 136/354: batchLoss = 0.5938, diffLoss = 2.9009, kgLoss = 0.0170
2025-04-08 18:41:36.988241: Training Step 137/354: batchLoss = 0.7050, diffLoss = 3.4471, kgLoss = 0.0195
2025-04-08 18:41:38.608210: Training Step 138/354: batchLoss = 0.6257, diffLoss = 3.0632, kgLoss = 0.0163
2025-04-08 18:41:40.226196: Training Step 139/354: batchLoss = 0.5310, diffLoss = 2.6038, kgLoss = 0.0128
2025-04-08 18:41:41.836613: Training Step 140/354: batchLoss = 0.6081, diffLoss = 2.9774, kgLoss = 0.0157
2025-04-08 18:41:43.464343: Training Step 141/354: batchLoss = 0.6577, diffLoss = 3.2207, kgLoss = 0.0170
2025-04-08 18:41:45.077750: Training Step 142/354: batchLoss = 0.4758, diffLoss = 2.3248, kgLoss = 0.0135
2025-04-08 18:41:46.689668: Training Step 143/354: batchLoss = 0.5589, diffLoss = 2.7382, kgLoss = 0.0141
2025-04-08 18:41:48.319884: Training Step 144/354: batchLoss = 0.5330, diffLoss = 2.6098, kgLoss = 0.0138
2025-04-08 18:41:49.946787: Training Step 145/354: batchLoss = 0.6347, diffLoss = 3.1068, kgLoss = 0.0166
2025-04-08 18:41:51.572486: Training Step 146/354: batchLoss = 0.4587, diffLoss = 2.2413, kgLoss = 0.0130
2025-04-08 18:41:53.188469: Training Step 147/354: batchLoss = 0.6248, diffLoss = 3.0582, kgLoss = 0.0164
2025-04-08 18:41:54.816450: Training Step 148/354: batchLoss = 0.6119, diffLoss = 2.9991, kgLoss = 0.0151
2025-04-08 18:41:56.439570: Training Step 149/354: batchLoss = 0.4402, diffLoss = 2.1551, kgLoss = 0.0115
2025-04-08 18:41:58.050380: Training Step 150/354: batchLoss = 0.4818, diffLoss = 2.3574, kgLoss = 0.0129
2025-04-08 18:41:59.664107: Training Step 151/354: batchLoss = 0.3868, diffLoss = 1.8922, kgLoss = 0.0104
2025-04-08 18:42:01.281695: Training Step 152/354: batchLoss = 0.4541, diffLoss = 2.2226, kgLoss = 0.0120
2025-04-08 18:42:02.890496: Training Step 153/354: batchLoss = 0.5658, diffLoss = 2.7678, kgLoss = 0.0153
2025-04-08 18:42:04.508584: Training Step 154/354: batchLoss = 0.6026, diffLoss = 2.9539, kgLoss = 0.0147
2025-04-08 18:42:06.132720: Training Step 155/354: batchLoss = 0.5280, diffLoss = 2.5727, kgLoss = 0.0168
2025-04-08 18:42:07.752012: Training Step 156/354: batchLoss = 0.6199, diffLoss = 3.0331, kgLoss = 0.0166
2025-04-08 18:42:09.372635: Training Step 157/354: batchLoss = 0.7391, diffLoss = 3.6249, kgLoss = 0.0176
2025-04-08 18:42:10.989864: Training Step 158/354: batchLoss = 0.5135, diffLoss = 2.5168, kgLoss = 0.0126
2025-04-08 18:42:12.604903: Training Step 159/354: batchLoss = 0.4862, diffLoss = 2.2648, kgLoss = 0.0416
2025-04-08 18:42:14.227326: Training Step 160/354: batchLoss = 0.8989, diffLoss = 4.4070, kgLoss = 0.0219
2025-04-08 18:42:15.846821: Training Step 161/354: batchLoss = 0.5008, diffLoss = 2.4540, kgLoss = 0.0125
2025-04-08 18:42:17.458063: Training Step 162/354: batchLoss = 0.5304, diffLoss = 2.6034, kgLoss = 0.0122
2025-04-08 18:42:19.072705: Training Step 163/354: batchLoss = 0.6137, diffLoss = 3.0099, kgLoss = 0.0147
2025-04-08 18:42:20.692373: Training Step 164/354: batchLoss = 0.7486, diffLoss = 3.6703, kgLoss = 0.0181
2025-04-08 18:42:22.314654: Training Step 165/354: batchLoss = 0.4810, diffLoss = 2.3561, kgLoss = 0.0122
2025-04-08 18:42:23.936918: Training Step 166/354: batchLoss = 0.5625, diffLoss = 2.7535, kgLoss = 0.0148
2025-04-08 18:42:25.563368: Training Step 167/354: batchLoss = 0.5227, diffLoss = 2.5576, kgLoss = 0.0140
2025-04-08 18:42:27.187289: Training Step 168/354: batchLoss = 0.5172, diffLoss = 2.5320, kgLoss = 0.0136
2025-04-08 18:42:28.802570: Training Step 169/354: batchLoss = 0.5914, diffLoss = 2.8957, kgLoss = 0.0154
2025-04-08 18:42:30.416543: Training Step 170/354: batchLoss = 0.6249, diffLoss = 3.0548, kgLoss = 0.0174
2025-04-08 18:42:32.028262: Training Step 171/354: batchLoss = 0.4257, diffLoss = 2.0778, kgLoss = 0.0127
2025-04-08 18:42:33.658807: Training Step 172/354: batchLoss = 0.5548, diffLoss = 2.7124, kgLoss = 0.0154
2025-04-08 18:42:35.275214: Training Step 173/354: batchLoss = 0.5216, diffLoss = 2.5519, kgLoss = 0.0141
2025-04-08 18:42:36.893372: Training Step 174/354: batchLoss = 0.5270, diffLoss = 2.5759, kgLoss = 0.0147
2025-04-08 18:42:38.511500: Training Step 175/354: batchLoss = 0.6189, diffLoss = 3.0387, kgLoss = 0.0139
2025-04-08 18:42:40.122803: Training Step 176/354: batchLoss = 0.7015, diffLoss = 3.4408, kgLoss = 0.0167
2025-04-08 18:42:41.740674: Training Step 177/354: batchLoss = 0.5765, diffLoss = 2.8234, kgLoss = 0.0148
2025-04-08 18:42:43.359426: Training Step 178/354: batchLoss = 0.5507, diffLoss = 2.6986, kgLoss = 0.0137
2025-04-08 18:42:44.975236: Training Step 179/354: batchLoss = 0.4303, diffLoss = 2.0947, kgLoss = 0.0142
2025-04-08 18:42:46.592714: Training Step 180/354: batchLoss = 0.5174, diffLoss = 2.5328, kgLoss = 0.0136
2025-04-08 18:42:48.206254: Training Step 181/354: batchLoss = 0.5073, diffLoss = 2.4790, kgLoss = 0.0143
2025-04-08 18:42:49.820798: Training Step 182/354: batchLoss = 0.5271, diffLoss = 2.5749, kgLoss = 0.0152
2025-04-08 18:42:51.437609: Training Step 183/354: batchLoss = 0.4868, diffLoss = 2.3784, kgLoss = 0.0139
2025-04-08 18:42:53.059316: Training Step 184/354: batchLoss = 0.5965, diffLoss = 2.9218, kgLoss = 0.0152
2025-04-08 18:42:54.675721: Training Step 185/354: batchLoss = 0.5168, diffLoss = 2.5348, kgLoss = 0.0123
2025-04-08 18:42:56.293080: Training Step 186/354: batchLoss = 0.5182, diffLoss = 2.5376, kgLoss = 0.0133
2025-04-08 18:42:57.909827: Training Step 187/354: batchLoss = 0.4684, diffLoss = 2.2926, kgLoss = 0.0123
2025-04-08 18:42:59.527862: Training Step 188/354: batchLoss = 0.5156, diffLoss = 2.5174, kgLoss = 0.0151
2025-04-08 18:43:01.145715: Training Step 189/354: batchLoss = 0.5186, diffLoss = 2.5271, kgLoss = 0.0164
2025-04-08 18:43:02.759678: Training Step 190/354: batchLoss = 0.4781, diffLoss = 2.3471, kgLoss = 0.0108
2025-04-08 18:43:04.375191: Training Step 191/354: batchLoss = 0.6054, diffLoss = 2.9477, kgLoss = 0.0198
2025-04-08 18:43:05.992082: Training Step 192/354: batchLoss = 0.5310, diffLoss = 2.6000, kgLoss = 0.0138
2025-04-08 18:43:07.607817: Training Step 193/354: batchLoss = 0.6238, diffLoss = 3.0555, kgLoss = 0.0159
2025-04-08 18:43:09.230283: Training Step 194/354: batchLoss = 0.6375, diffLoss = 3.1227, kgLoss = 0.0162
2025-04-08 18:43:10.843393: Training Step 195/354: batchLoss = 0.5633, diffLoss = 2.7594, kgLoss = 0.0142
2025-04-08 18:43:12.461760: Training Step 196/354: batchLoss = 0.6639, diffLoss = 3.2491, kgLoss = 0.0176
2025-04-08 18:43:14.078069: Training Step 197/354: batchLoss = 0.4448, diffLoss = 2.1747, kgLoss = 0.0123
2025-04-08 18:43:15.692169: Training Step 198/354: batchLoss = 0.5803, diffLoss = 2.8400, kgLoss = 0.0154
2025-04-08 18:43:17.307914: Training Step 199/354: batchLoss = 0.5245, diffLoss = 2.5659, kgLoss = 0.0142
2025-04-08 18:43:18.922705: Training Step 200/354: batchLoss = 0.5065, diffLoss = 2.4861, kgLoss = 0.0116
2025-04-08 18:43:20.541626: Training Step 201/354: batchLoss = 0.5893, diffLoss = 2.8827, kgLoss = 0.0159
2025-04-08 18:43:22.151977: Training Step 202/354: batchLoss = 0.4573, diffLoss = 2.2439, kgLoss = 0.0107
2025-04-08 18:43:23.772099: Training Step 203/354: batchLoss = 0.5337, diffLoss = 2.6128, kgLoss = 0.0139
2025-04-08 18:43:25.397473: Training Step 204/354: batchLoss = 0.4929, diffLoss = 2.4153, kgLoss = 0.0123
2025-04-08 18:43:27.014562: Training Step 205/354: batchLoss = 0.4699, diffLoss = 2.2899, kgLoss = 0.0149
2025-04-08 18:43:28.637907: Training Step 206/354: batchLoss = 0.5018, diffLoss = 2.4564, kgLoss = 0.0131
2025-04-08 18:43:30.259518: Training Step 207/354: batchLoss = 0.5260, diffLoss = 2.5676, kgLoss = 0.0156
2025-04-08 18:43:31.884241: Training Step 208/354: batchLoss = 0.4903, diffLoss = 2.4092, kgLoss = 0.0106
2025-04-08 18:43:33.492164: Training Step 209/354: batchLoss = 0.5790, diffLoss = 2.8355, kgLoss = 0.0149
2025-04-08 18:43:35.113618: Training Step 210/354: batchLoss = 0.5222, diffLoss = 2.5633, kgLoss = 0.0119
2025-04-08 18:43:36.728354: Training Step 211/354: batchLoss = 0.5076, diffLoss = 2.4790, kgLoss = 0.0147
2025-04-08 18:43:38.356929: Training Step 212/354: batchLoss = 0.5343, diffLoss = 2.6113, kgLoss = 0.0151
2025-04-08 18:43:39.977783: Training Step 213/354: batchLoss = 0.4842, diffLoss = 2.3682, kgLoss = 0.0133
2025-04-08 18:43:41.595828: Training Step 214/354: batchLoss = 0.4879, diffLoss = 2.3898, kgLoss = 0.0124
2025-04-08 18:43:43.223929: Training Step 215/354: batchLoss = 0.6146, diffLoss = 3.0071, kgLoss = 0.0165
2025-04-08 18:43:44.850192: Training Step 216/354: batchLoss = 0.5185, diffLoss = 2.5363, kgLoss = 0.0140
2025-04-08 18:43:46.479554: Training Step 217/354: batchLoss = 0.4700, diffLoss = 2.2965, kgLoss = 0.0134
2025-04-08 18:43:48.098618: Training Step 218/354: batchLoss = 0.5078, diffLoss = 2.4845, kgLoss = 0.0136
2025-04-08 18:43:49.713184: Training Step 219/354: batchLoss = 0.5209, diffLoss = 2.5420, kgLoss = 0.0157
2025-04-08 18:43:51.329002: Training Step 220/354: batchLoss = 0.5400, diffLoss = 2.6438, kgLoss = 0.0140
2025-04-08 18:43:52.942277: Training Step 221/354: batchLoss = 0.5331, diffLoss = 2.6077, kgLoss = 0.0145
2025-04-08 18:43:54.562838: Training Step 222/354: batchLoss = 0.5189, diffLoss = 2.5377, kgLoss = 0.0142
2025-04-08 18:43:56.182530: Training Step 223/354: batchLoss = 0.6525, diffLoss = 3.1973, kgLoss = 0.0163
2025-04-08 18:43:57.806459: Training Step 224/354: batchLoss = 0.7140, diffLoss = 3.4942, kgLoss = 0.0189
2025-04-08 18:43:59.427519: Training Step 225/354: batchLoss = 0.5589, diffLoss = 2.7295, kgLoss = 0.0163
2025-04-08 18:44:01.046827: Training Step 226/354: batchLoss = 0.5611, diffLoss = 2.7514, kgLoss = 0.0136
2025-04-08 18:44:02.665543: Training Step 227/354: batchLoss = 0.4821, diffLoss = 2.3595, kgLoss = 0.0128
2025-04-08 18:44:04.283011: Training Step 228/354: batchLoss = 0.4863, diffLoss = 2.3807, kgLoss = 0.0127
2025-04-08 18:44:05.895649: Training Step 229/354: batchLoss = 0.4602, diffLoss = 2.2484, kgLoss = 0.0131
2025-04-08 18:44:07.509027: Training Step 230/354: batchLoss = 0.5306, diffLoss = 2.5989, kgLoss = 0.0135
2025-04-08 18:44:09.126993: Training Step 231/354: batchLoss = 0.5116, diffLoss = 2.5093, kgLoss = 0.0122
2025-04-08 18:44:10.741999: Training Step 232/354: batchLoss = 0.4950, diffLoss = 2.4267, kgLoss = 0.0120
2025-04-08 18:44:12.361423: Training Step 233/354: batchLoss = 0.5108, diffLoss = 2.5026, kgLoss = 0.0128
2025-04-08 18:44:13.972672: Training Step 234/354: batchLoss = 0.5502, diffLoss = 2.6927, kgLoss = 0.0145
2025-04-08 18:44:15.594522: Training Step 235/354: batchLoss = 0.5268, diffLoss = 2.5766, kgLoss = 0.0144
2025-04-08 18:44:17.223730: Training Step 236/354: batchLoss = 0.5732, diffLoss = 2.8036, kgLoss = 0.0155
2025-04-08 18:44:18.846092: Training Step 237/354: batchLoss = 0.5630, diffLoss = 2.7519, kgLoss = 0.0158
2025-04-08 18:44:20.467067: Training Step 238/354: batchLoss = 0.5884, diffLoss = 2.8790, kgLoss = 0.0158
2025-04-08 18:44:22.076441: Training Step 239/354: batchLoss = 0.5064, diffLoss = 2.4740, kgLoss = 0.0145
2025-04-08 18:44:23.697166: Training Step 240/354: batchLoss = 0.5511, diffLoss = 2.6993, kgLoss = 0.0141
2025-04-08 18:44:25.311625: Training Step 241/354: batchLoss = 0.5768, diffLoss = 2.8227, kgLoss = 0.0153
2025-04-08 18:44:26.930048: Training Step 242/354: batchLoss = 0.4905, diffLoss = 2.3979, kgLoss = 0.0136
2025-04-08 18:44:28.550556: Training Step 243/354: batchLoss = 0.4722, diffLoss = 2.3023, kgLoss = 0.0146
2025-04-08 18:44:30.170390: Training Step 244/354: batchLoss = 0.5421, diffLoss = 2.6534, kgLoss = 0.0142
2025-04-08 18:44:31.792130: Training Step 245/354: batchLoss = 0.4983, diffLoss = 2.4322, kgLoss = 0.0148
2025-04-08 18:44:33.418400: Training Step 246/354: batchLoss = 0.5889, diffLoss = 2.8811, kgLoss = 0.0158
2025-04-08 18:44:35.031474: Training Step 247/354: batchLoss = 0.5069, diffLoss = 2.4850, kgLoss = 0.0124
2025-04-08 18:44:36.649562: Training Step 248/354: batchLoss = 0.5018, diffLoss = 2.4618, kgLoss = 0.0118
2025-04-08 18:44:38.263823: Training Step 249/354: batchLoss = 0.6334, diffLoss = 3.1063, kgLoss = 0.0151
2025-04-08 18:44:39.878996: Training Step 250/354: batchLoss = 0.5830, diffLoss = 2.8525, kgLoss = 0.0156
2025-04-08 18:44:41.498156: Training Step 251/354: batchLoss = 0.4894, diffLoss = 2.3977, kgLoss = 0.0123
2025-04-08 18:44:43.114119: Training Step 252/354: batchLoss = 0.5611, diffLoss = 2.7479, kgLoss = 0.0144
2025-04-08 18:44:44.727190: Training Step 253/354: batchLoss = 0.7101, diffLoss = 3.4703, kgLoss = 0.0201
2025-04-08 18:44:46.347092: Training Step 254/354: batchLoss = 0.6270, diffLoss = 3.0672, kgLoss = 0.0169
2025-04-08 18:44:47.963613: Training Step 255/354: batchLoss = 0.4627, diffLoss = 2.2697, kgLoss = 0.0110
2025-04-08 18:44:49.577077: Training Step 256/354: batchLoss = 0.5457, diffLoss = 2.6588, kgLoss = 0.0174
2025-04-08 18:44:51.195933: Training Step 257/354: batchLoss = 0.5767, diffLoss = 2.8268, kgLoss = 0.0142
2025-04-08 18:44:52.808565: Training Step 258/354: batchLoss = 0.4933, diffLoss = 2.4160, kgLoss = 0.0126
2025-04-08 18:44:54.418607: Training Step 259/354: batchLoss = 0.4939, diffLoss = 2.4214, kgLoss = 0.0120
2025-04-08 18:44:56.026661: Training Step 260/354: batchLoss = 0.6510, diffLoss = 3.1910, kgLoss = 0.0160
2025-04-08 18:44:57.628280: Training Step 261/354: batchLoss = 0.5002, diffLoss = 2.4501, kgLoss = 0.0127
2025-04-08 18:44:59.238534: Training Step 262/354: batchLoss = 0.5900, diffLoss = 2.8902, kgLoss = 0.0149
2025-04-08 18:45:00.853338: Training Step 263/354: batchLoss = 0.5076, diffLoss = 2.4793, kgLoss = 0.0147
2025-04-08 18:45:02.472055: Training Step 264/354: batchLoss = 0.5235, diffLoss = 2.5605, kgLoss = 0.0143
2025-04-08 18:45:04.088210: Training Step 265/354: batchLoss = 0.6361, diffLoss = 3.1243, kgLoss = 0.0140
2025-04-08 18:45:05.711713: Training Step 266/354: batchLoss = 0.5319, diffLoss = 2.6010, kgLoss = 0.0146
2025-04-08 18:45:07.330554: Training Step 267/354: batchLoss = 0.5282, diffLoss = 2.5921, kgLoss = 0.0122
2025-04-08 18:45:08.944366: Training Step 268/354: batchLoss = 0.5453, diffLoss = 2.6704, kgLoss = 0.0140
2025-04-08 18:45:10.562607: Training Step 269/354: batchLoss = 0.5854, diffLoss = 2.8626, kgLoss = 0.0161
2025-04-08 18:45:12.175656: Training Step 270/354: batchLoss = 0.6142, diffLoss = 3.0122, kgLoss = 0.0147
2025-04-08 18:45:13.791220: Training Step 271/354: batchLoss = 0.6029, diffLoss = 2.9509, kgLoss = 0.0159
2025-04-08 18:45:15.412184: Training Step 272/354: batchLoss = 0.5131, diffLoss = 2.5098, kgLoss = 0.0139
2025-04-08 18:45:17.031506: Training Step 273/354: batchLoss = 0.6598, diffLoss = 3.2326, kgLoss = 0.0166
2025-04-08 18:45:18.643233: Training Step 274/354: batchLoss = 0.5833, diffLoss = 2.8439, kgLoss = 0.0181
2025-04-08 18:45:20.269291: Training Step 275/354: batchLoss = 0.5820, diffLoss = 2.8527, kgLoss = 0.0143
2025-04-08 18:45:21.888399: Training Step 276/354: batchLoss = 0.4698, diffLoss = 2.2926, kgLoss = 0.0141
2025-04-08 18:45:23.510772: Training Step 277/354: batchLoss = 0.7480, diffLoss = 3.6708, kgLoss = 0.0173
2025-04-08 18:45:25.127733: Training Step 278/354: batchLoss = 0.5952, diffLoss = 2.9074, kgLoss = 0.0172
2025-04-08 18:45:26.733264: Training Step 279/354: batchLoss = 0.4908, diffLoss = 2.4041, kgLoss = 0.0125
2025-04-08 18:45:28.338925: Training Step 280/354: batchLoss = 0.5305, diffLoss = 2.5909, kgLoss = 0.0154
2025-04-08 18:45:29.951115: Training Step 281/354: batchLoss = 0.7032, diffLoss = 3.4440, kgLoss = 0.0180
2025-04-08 18:45:31.571066: Training Step 282/354: batchLoss = 0.4904, diffLoss = 2.3831, kgLoss = 0.0172
2025-04-08 18:45:33.191441: Training Step 283/354: batchLoss = 0.5388, diffLoss = 2.6391, kgLoss = 0.0137
2025-04-08 18:45:34.813196: Training Step 284/354: batchLoss = 0.4589, diffLoss = 2.2484, kgLoss = 0.0116
2025-04-08 18:45:36.427151: Training Step 285/354: batchLoss = 0.5212, diffLoss = 2.5531, kgLoss = 0.0132
2025-04-08 18:45:38.045130: Training Step 286/354: batchLoss = 0.7360, diffLoss = 3.6084, kgLoss = 0.0179
2025-04-08 18:45:39.667708: Training Step 287/354: batchLoss = 0.6073, diffLoss = 2.9431, kgLoss = 0.0234
2025-04-08 18:45:41.287194: Training Step 288/354: batchLoss = 0.5395, diffLoss = 2.6446, kgLoss = 0.0132
2025-04-08 18:45:42.898185: Training Step 289/354: batchLoss = 0.5937, diffLoss = 2.9148, kgLoss = 0.0134
2025-04-08 18:45:44.516653: Training Step 290/354: batchLoss = 0.5842, diffLoss = 2.8633, kgLoss = 0.0144
2025-04-08 18:45:46.134726: Training Step 291/354: batchLoss = 0.5461, diffLoss = 2.6719, kgLoss = 0.0147
2025-04-08 18:45:47.755991: Training Step 292/354: batchLoss = 1.0399, diffLoss = 5.1030, kgLoss = 0.0241
2025-04-08 18:45:49.374322: Training Step 293/354: batchLoss = 0.4456, diffLoss = 2.1804, kgLoss = 0.0118
2025-04-08 18:45:50.990359: Training Step 294/354: batchLoss = 0.6118, diffLoss = 3.0028, kgLoss = 0.0141
2025-04-08 18:45:52.613729: Training Step 295/354: batchLoss = 0.6781, diffLoss = 3.3286, kgLoss = 0.0155
2025-04-08 18:45:54.228437: Training Step 296/354: batchLoss = 0.6075, diffLoss = 2.9746, kgLoss = 0.0157
2025-04-08 18:45:55.850393: Training Step 297/354: batchLoss = 0.7495, diffLoss = 3.6657, kgLoss = 0.0205
2025-04-08 18:45:57.466295: Training Step 298/354: batchLoss = 0.5011, diffLoss = 2.4530, kgLoss = 0.0131
2025-04-08 18:45:59.086976: Training Step 299/354: batchLoss = 0.5633, diffLoss = 2.7537, kgLoss = 0.0157
2025-04-08 18:46:00.705763: Training Step 300/354: batchLoss = 0.6299, diffLoss = 3.0828, kgLoss = 0.0167
2025-04-08 18:46:02.320346: Training Step 301/354: batchLoss = 0.5925, diffLoss = 2.8943, kgLoss = 0.0171
2025-04-08 18:46:03.939346: Training Step 302/354: batchLoss = 0.5558, diffLoss = 2.7273, kgLoss = 0.0130
2025-04-08 18:46:05.553350: Training Step 303/354: batchLoss = 0.5820, diffLoss = 2.8515, kgLoss = 0.0147
2025-04-08 18:46:07.178149: Training Step 304/354: batchLoss = 0.5297, diffLoss = 2.5920, kgLoss = 0.0142
2025-04-08 18:46:08.795445: Training Step 305/354: batchLoss = 0.4694, diffLoss = 2.2874, kgLoss = 0.0149
2025-04-08 18:46:10.415250: Training Step 306/354: batchLoss = 0.5562, diffLoss = 2.7251, kgLoss = 0.0140
2025-04-08 18:46:12.035594: Training Step 307/354: batchLoss = 0.6403, diffLoss = 3.1353, kgLoss = 0.0166
2025-04-08 18:46:13.655949: Training Step 308/354: batchLoss = 0.5855, diffLoss = 2.8768, kgLoss = 0.0127
2025-04-08 18:46:15.271691: Training Step 309/354: batchLoss = 0.5315, diffLoss = 2.6033, kgLoss = 0.0136
2025-04-08 18:46:16.889695: Training Step 310/354: batchLoss = 0.5463, diffLoss = 2.6762, kgLoss = 0.0138
2025-04-08 18:46:18.507718: Training Step 311/354: batchLoss = 0.5737, diffLoss = 2.8086, kgLoss = 0.0150
2025-04-08 18:46:20.125225: Training Step 312/354: batchLoss = 0.7217, diffLoss = 3.5354, kgLoss = 0.0183
2025-04-08 18:46:21.742203: Training Step 313/354: batchLoss = 0.4944, diffLoss = 2.4075, kgLoss = 0.0161
2025-04-08 18:46:23.362432: Training Step 314/354: batchLoss = 0.6797, diffLoss = 3.3313, kgLoss = 0.0168
2025-04-08 18:46:24.975910: Training Step 315/354: batchLoss = 0.5604, diffLoss = 2.7399, kgLoss = 0.0155
2025-04-08 18:46:26.595245: Training Step 316/354: batchLoss = 0.5296, diffLoss = 2.5984, kgLoss = 0.0125
2025-04-08 18:46:28.215865: Training Step 317/354: batchLoss = 0.5749, diffLoss = 2.8130, kgLoss = 0.0154
2025-04-08 18:46:29.831865: Training Step 318/354: batchLoss = 0.5807, diffLoss = 2.8414, kgLoss = 0.0156
2025-04-08 18:46:31.442866: Training Step 319/354: batchLoss = 0.6224, diffLoss = 3.0506, kgLoss = 0.0154
2025-04-08 18:46:33.054430: Training Step 320/354: batchLoss = 0.5605, diffLoss = 2.7408, kgLoss = 0.0154
2025-04-08 18:46:34.672339: Training Step 321/354: batchLoss = 0.4767, diffLoss = 2.3412, kgLoss = 0.0106
2025-04-08 18:46:36.287768: Training Step 322/354: batchLoss = 0.5679, diffLoss = 2.7819, kgLoss = 0.0144
2025-04-08 18:46:37.910298: Training Step 323/354: batchLoss = 0.5275, diffLoss = 2.5708, kgLoss = 0.0166
2025-04-08 18:46:39.529610: Training Step 324/354: batchLoss = 0.4476, diffLoss = 2.1879, kgLoss = 0.0126
2025-04-08 18:46:41.154544: Training Step 325/354: batchLoss = 0.6020, diffLoss = 2.9482, kgLoss = 0.0155
2025-04-08 18:46:42.777272: Training Step 326/354: batchLoss = 0.5726, diffLoss = 2.8027, kgLoss = 0.0151
2025-04-08 18:46:44.397879: Training Step 327/354: batchLoss = 0.5903, diffLoss = 2.8900, kgLoss = 0.0154
2025-04-08 18:46:46.017233: Training Step 328/354: batchLoss = 0.5311, diffLoss = 2.6007, kgLoss = 0.0138
2025-04-08 18:46:47.633650: Training Step 329/354: batchLoss = 0.4888, diffLoss = 2.3906, kgLoss = 0.0134
2025-04-08 18:46:49.244230: Training Step 330/354: batchLoss = 0.5695, diffLoss = 2.7916, kgLoss = 0.0139
2025-04-08 18:46:50.858456: Training Step 331/354: batchLoss = 0.4967, diffLoss = 2.4322, kgLoss = 0.0128
2025-04-08 18:46:52.475879: Training Step 332/354: batchLoss = 0.6126, diffLoss = 3.0040, kgLoss = 0.0147
2025-04-08 18:46:54.098707: Training Step 333/354: batchLoss = 0.5622, diffLoss = 2.7507, kgLoss = 0.0151
2025-04-08 18:46:55.718180: Training Step 334/354: batchLoss = 0.5934, diffLoss = 2.9125, kgLoss = 0.0137
2025-04-08 18:46:57.337459: Training Step 335/354: batchLoss = 0.6637, diffLoss = 3.2448, kgLoss = 0.0184
2025-04-08 18:46:58.972891: Training Step 336/354: batchLoss = 0.7258, diffLoss = 3.5566, kgLoss = 0.0181
2025-04-08 18:47:00.590228: Training Step 337/354: batchLoss = 0.5748, diffLoss = 2.8190, kgLoss = 0.0138
2025-04-08 18:47:02.206826: Training Step 338/354: batchLoss = 0.4465, diffLoss = 2.1801, kgLoss = 0.0131
2025-04-08 18:47:03.823436: Training Step 339/354: batchLoss = 0.5012, diffLoss = 2.4555, kgLoss = 0.0126
2025-04-08 18:47:05.441770: Training Step 340/354: batchLoss = 0.6528, diffLoss = 3.2006, kgLoss = 0.0158
2025-04-08 18:47:07.055204: Training Step 341/354: batchLoss = 0.6692, diffLoss = 3.2764, kgLoss = 0.0173
2025-04-08 18:47:08.676319: Training Step 342/354: batchLoss = 0.5666, diffLoss = 2.7749, kgLoss = 0.0146
2025-04-08 18:47:10.296958: Training Step 343/354: batchLoss = 0.4491, diffLoss = 2.1957, kgLoss = 0.0124
2025-04-08 18:47:11.914722: Training Step 344/354: batchLoss = 0.5274, diffLoss = 2.5860, kgLoss = 0.0128
2025-04-08 18:47:13.546033: Training Step 345/354: batchLoss = 0.5556, diffLoss = 2.7246, kgLoss = 0.0133
2025-04-08 18:47:15.163487: Training Step 346/354: batchLoss = 0.6182, diffLoss = 3.0265, kgLoss = 0.0161
2025-04-08 18:47:16.785290: Training Step 347/354: batchLoss = 0.4962, diffLoss = 2.4268, kgLoss = 0.0135
2025-04-08 18:47:18.401222: Training Step 348/354: batchLoss = 0.5328, diffLoss = 2.6072, kgLoss = 0.0142
2025-04-08 18:47:20.018726: Training Step 349/354: batchLoss = 0.5048, diffLoss = 2.4602, kgLoss = 0.0160
2025-04-08 18:47:21.640043: Training Step 350/354: batchLoss = 0.6605, diffLoss = 3.2380, kgLoss = 0.0161
2025-04-08 18:47:23.250765: Training Step 351/354: batchLoss = 0.5674, diffLoss = 2.7760, kgLoss = 0.0152
2025-04-08 18:47:24.846704: Training Step 352/354: batchLoss = 0.4807, diffLoss = 2.3518, kgLoss = 0.0129
2025-04-08 18:47:26.246484: Training Step 353/354: batchLoss = 0.5367, diffLoss = 2.6258, kgLoss = 0.0144
2025-04-08 18:47:26.335272: 
2025-04-08 18:47:26.335935: Epoch 29/1000, Train: epLoss = 0.9797, epDfLoss = 4.7939, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 18:47:27.667042: Steps 0/138: batch_recall = 47.56, batch_ndcg = 26.40 
2025-04-08 18:47:28.982445: Steps 1/138: batch_recall = 48.02, batch_ndcg = 28.02 
2025-04-08 18:47:30.290106: Steps 2/138: batch_recall = 59.20, batch_ndcg = 35.63 
2025-04-08 18:47:31.605784: Steps 3/138: batch_recall = 58.05, batch_ndcg = 33.95 
2025-04-08 18:47:32.907315: Steps 4/138: batch_recall = 66.25, batch_ndcg = 40.74 
2025-04-08 18:47:34.215136: Steps 5/138: batch_recall = 57.00, batch_ndcg = 31.03 
2025-04-08 18:47:35.514020: Steps 6/138: batch_recall = 52.41, batch_ndcg = 32.03 
2025-04-08 18:47:36.814819: Steps 7/138: batch_recall = 63.73, batch_ndcg = 41.81 
2025-04-08 18:47:38.114608: Steps 8/138: batch_recall = 61.78, batch_ndcg = 38.57 
2025-04-08 18:47:39.408381: Steps 9/138: batch_recall = 58.27, batch_ndcg = 34.75 
2025-04-08 18:47:40.711880: Steps 10/138: batch_recall = 57.35, batch_ndcg = 32.98 
2025-04-08 18:47:42.007336: Steps 11/138: batch_recall = 57.47, batch_ndcg = 33.18 
2025-04-08 18:47:43.306124: Steps 12/138: batch_recall = 51.57, batch_ndcg = 29.25 
2025-04-08 18:47:44.599582: Steps 13/138: batch_recall = 53.90, batch_ndcg = 31.78 
2025-04-08 18:47:45.898927: Steps 14/138: batch_recall = 55.18, batch_ndcg = 31.79 
2025-04-08 18:47:47.201555: Steps 15/138: batch_recall = 47.64, batch_ndcg = 29.86 
2025-04-08 18:47:48.498965: Steps 16/138: batch_recall = 60.86, batch_ndcg = 34.25 
2025-04-08 18:47:49.782461: Steps 17/138: batch_recall = 58.05, batch_ndcg = 33.97 
2025-04-08 18:47:51.072329: Steps 18/138: batch_recall = 52.97, batch_ndcg = 33.10 
2025-04-08 18:47:52.374073: Steps 19/138: batch_recall = 53.76, batch_ndcg = 32.39 
2025-04-08 18:47:53.666977: Steps 20/138: batch_recall = 61.62, batch_ndcg = 36.03 
2025-04-08 18:47:54.959099: Steps 21/138: batch_recall = 71.30, batch_ndcg = 41.14 
2025-04-08 18:47:56.261313: Steps 22/138: batch_recall = 55.76, batch_ndcg = 32.47 
2025-04-08 18:47:57.560119: Steps 23/138: batch_recall = 50.67, batch_ndcg = 30.15 
2025-04-08 18:47:58.869003: Steps 24/138: batch_recall = 56.01, batch_ndcg = 30.99 
2025-04-08 18:48:00.167114: Steps 25/138: batch_recall = 60.96, batch_ndcg = 35.06 
2025-04-08 18:48:01.471740: Steps 26/138: batch_recall = 57.25, batch_ndcg = 32.66 
2025-04-08 18:48:02.767773: Steps 27/138: batch_recall = 53.99, batch_ndcg = 31.48 
2025-04-08 18:48:04.056670: Steps 28/138: batch_recall = 59.58, batch_ndcg = 33.52 
2025-04-08 18:48:05.344354: Steps 29/138: batch_recall = 60.62, batch_ndcg = 32.06 
2025-04-08 18:48:06.633074: Steps 30/138: batch_recall = 57.14, batch_ndcg = 33.76 
2025-04-08 18:48:07.920207: Steps 31/138: batch_recall = 48.70, batch_ndcg = 27.70 
2025-04-08 18:48:09.201195: Steps 32/138: batch_recall = 53.51, batch_ndcg = 31.65 
2025-04-08 18:48:10.484955: Steps 33/138: batch_recall = 60.19, batch_ndcg = 33.56 
2025-04-08 18:48:11.778259: Steps 34/138: batch_recall = 56.55, batch_ndcg = 30.62 
2025-04-08 18:48:13.074448: Steps 35/138: batch_recall = 52.05, batch_ndcg = 29.96 
2025-04-08 18:48:14.361167: Steps 36/138: batch_recall = 46.44, batch_ndcg = 26.60 
2025-04-08 18:48:15.655723: Steps 37/138: batch_recall = 56.32, batch_ndcg = 33.91 
2025-04-08 18:48:16.943425: Steps 38/138: batch_recall = 58.26, batch_ndcg = 32.59 
2025-04-08 18:48:18.236541: Steps 39/138: batch_recall = 67.52, batch_ndcg = 38.81 
2025-04-08 18:48:19.526335: Steps 40/138: batch_recall = 63.28, batch_ndcg = 31.26 
2025-04-08 18:48:20.814781: Steps 41/138: batch_recall = 59.42, batch_ndcg = 33.69 
2025-04-08 18:48:22.093016: Steps 42/138: batch_recall = 54.11, batch_ndcg = 30.14 
2025-04-08 18:48:23.377459: Steps 43/138: batch_recall = 59.02, batch_ndcg = 36.58 
2025-04-08 18:48:24.663584: Steps 44/138: batch_recall = 56.63, batch_ndcg = 30.74 
2025-04-08 18:48:25.947882: Steps 45/138: batch_recall = 61.71, batch_ndcg = 35.63 
2025-04-08 18:48:27.230501: Steps 46/138: batch_recall = 61.08, batch_ndcg = 36.49 
2025-04-08 18:48:28.525947: Steps 47/138: batch_recall = 52.51, batch_ndcg = 31.88 
2025-04-08 18:48:29.814491: Steps 48/138: batch_recall = 62.74, batch_ndcg = 36.05 
2025-04-08 18:48:31.094788: Steps 49/138: batch_recall = 67.97, batch_ndcg = 38.31 
2025-04-08 18:48:32.378890: Steps 50/138: batch_recall = 58.67, batch_ndcg = 31.03 
2025-04-08 18:48:33.666254: Steps 51/138: batch_recall = 64.68, batch_ndcg = 36.87 
2025-04-08 18:48:34.951370: Steps 52/138: batch_recall = 66.72, batch_ndcg = 42.12 
2025-04-08 18:48:36.243111: Steps 53/138: batch_recall = 65.65, batch_ndcg = 34.69 
2025-04-08 18:48:37.523384: Steps 54/138: batch_recall = 69.07, batch_ndcg = 38.28 
2025-04-08 18:48:38.804491: Steps 55/138: batch_recall = 61.83, batch_ndcg = 34.90 
2025-04-08 18:48:40.085864: Steps 56/138: batch_recall = 61.13, batch_ndcg = 35.54 
2025-04-08 18:48:41.365122: Steps 57/138: batch_recall = 59.69, batch_ndcg = 32.78 
2025-04-08 18:48:42.654179: Steps 58/138: batch_recall = 70.52, batch_ndcg = 36.37 
2025-04-08 18:48:43.937527: Steps 59/138: batch_recall = 64.05, batch_ndcg = 38.65 
2025-04-08 18:48:45.221457: Steps 60/138: batch_recall = 66.34, batch_ndcg = 37.17 
2025-04-08 18:48:46.506524: Steps 61/138: batch_recall = 63.92, batch_ndcg = 34.90 
2025-04-08 18:48:47.795421: Steps 62/138: batch_recall = 85.79, batch_ndcg = 45.34 
2025-04-08 18:48:49.078642: Steps 63/138: batch_recall = 75.70, batch_ndcg = 42.87 
2025-04-08 18:48:50.356168: Steps 64/138: batch_recall = 62.84, batch_ndcg = 32.68 
2025-04-08 18:48:51.645139: Steps 65/138: batch_recall = 84.33, batch_ndcg = 46.86 
2025-04-08 18:48:52.932758: Steps 66/138: batch_recall = 70.07, batch_ndcg = 41.32 
2025-04-08 18:48:54.208646: Steps 67/138: batch_recall = 75.98, batch_ndcg = 45.97 
2025-04-08 18:48:55.484226: Steps 68/138: batch_recall = 58.81, batch_ndcg = 32.75 
2025-04-08 18:48:56.757205: Steps 69/138: batch_recall = 85.97, batch_ndcg = 51.50 
2025-04-08 18:48:58.030860: Steps 70/138: batch_recall = 79.45, batch_ndcg = 45.00 
2025-04-08 18:48:59.315866: Steps 71/138: batch_recall = 88.00, batch_ndcg = 51.49 
2025-04-08 18:49:00.597965: Steps 72/138: batch_recall = 85.51, batch_ndcg = 49.29 
2025-04-08 18:49:01.877822: Steps 73/138: batch_recall = 81.60, batch_ndcg = 46.09 
2025-04-08 18:49:03.161242: Steps 74/138: batch_recall = 76.25, batch_ndcg = 47.70 
2025-04-08 18:49:04.434815: Steps 75/138: batch_recall = 83.09, batch_ndcg = 48.81 
2025-04-08 18:49:05.718986: Steps 76/138: batch_recall = 94.21, batch_ndcg = 55.33 
2025-04-08 18:49:07.006078: Steps 77/138: batch_recall = 90.98, batch_ndcg = 51.55 
2025-04-08 18:49:08.284503: Steps 78/138: batch_recall = 89.20, batch_ndcg = 47.97 
2025-04-08 18:49:09.564385: Steps 79/138: batch_recall = 87.49, batch_ndcg = 48.71 
2025-04-08 18:49:10.845116: Steps 80/138: batch_recall = 70.26, batch_ndcg = 37.85 
2025-04-08 18:49:12.140828: Steps 81/138: batch_recall = 82.93, batch_ndcg = 48.34 
2025-04-08 18:49:13.414849: Steps 82/138: batch_recall = 87.45, batch_ndcg = 53.62 
2025-04-08 18:49:14.703063: Steps 83/138: batch_recall = 80.44, batch_ndcg = 46.94 
2025-04-08 18:49:15.977983: Steps 84/138: batch_recall = 100.16, batch_ndcg = 57.83 
2025-04-08 18:49:17.251020: Steps 85/138: batch_recall = 105.26, batch_ndcg = 61.48 
2025-04-08 18:49:18.530113: Steps 86/138: batch_recall = 124.04, batch_ndcg = 72.21 
2025-04-08 18:49:19.806991: Steps 87/138: batch_recall = 105.68, batch_ndcg = 55.42 
2025-04-08 18:49:21.086424: Steps 88/138: batch_recall = 101.44, batch_ndcg = 59.33 
2025-04-08 18:49:22.367490: Steps 89/138: batch_recall = 122.10, batch_ndcg = 70.15 
2025-04-08 18:49:23.646342: Steps 90/138: batch_recall = 101.14, batch_ndcg = 57.54 
2025-04-08 18:49:24.916860: Steps 91/138: batch_recall = 116.92, batch_ndcg = 64.46 
2025-04-08 18:49:26.186674: Steps 92/138: batch_recall = 122.38, batch_ndcg = 65.58 
2025-04-08 18:49:27.469489: Steps 93/138: batch_recall = 121.74, batch_ndcg = 69.15 
2025-04-08 18:49:28.740404: Steps 94/138: batch_recall = 125.98, batch_ndcg = 66.78 
2025-04-08 18:49:30.007283: Steps 95/138: batch_recall = 115.14, batch_ndcg = 67.20 
2025-04-08 18:49:31.274794: Steps 96/138: batch_recall = 129.07, batch_ndcg = 78.34 
2025-04-08 18:49:32.548698: Steps 97/138: batch_recall = 135.97, batch_ndcg = 86.90 
2025-04-08 18:49:33.827981: Steps 98/138: batch_recall = 107.88, batch_ndcg = 63.37 
2025-04-08 18:49:35.099532: Steps 99/138: batch_recall = 123.39, batch_ndcg = 71.81 
2025-04-08 18:49:36.370599: Steps 100/138: batch_recall = 129.74, batch_ndcg = 72.58 
2025-04-08 18:49:37.642942: Steps 101/138: batch_recall = 127.76, batch_ndcg = 70.16 
2025-04-08 18:49:38.917632: Steps 102/138: batch_recall = 123.76, batch_ndcg = 70.67 
2025-04-08 18:49:40.187068: Steps 103/138: batch_recall = 140.58, batch_ndcg = 80.44 
2025-04-08 18:49:41.462415: Steps 104/138: batch_recall = 134.14, batch_ndcg = 78.84 
2025-04-08 18:49:42.728256: Steps 105/138: batch_recall = 120.35, batch_ndcg = 67.09 
2025-04-08 18:49:44.002422: Steps 106/138: batch_recall = 104.47, batch_ndcg = 59.93 
2025-04-08 18:49:45.273768: Steps 107/138: batch_recall = 114.71, batch_ndcg = 64.74 
2025-04-08 18:49:46.546433: Steps 108/138: batch_recall = 116.51, batch_ndcg = 72.04 
2025-04-08 18:49:47.834292: Steps 109/138: batch_recall = 138.83, batch_ndcg = 78.01 
2025-04-08 18:49:49.110234: Steps 110/138: batch_recall = 120.04, batch_ndcg = 64.53 
2025-04-08 18:49:50.378241: Steps 111/138: batch_recall = 137.73, batch_ndcg = 84.76 
2025-04-08 18:49:51.657813: Steps 112/138: batch_recall = 166.13, batch_ndcg = 92.03 
2025-04-08 18:49:52.930691: Steps 113/138: batch_recall = 125.22, batch_ndcg = 69.95 
2025-04-08 18:49:54.200869: Steps 114/138: batch_recall = 123.28, batch_ndcg = 72.24 
2025-04-08 18:49:55.475928: Steps 115/138: batch_recall = 121.21, batch_ndcg = 63.72 
2025-04-08 18:49:56.748425: Steps 116/138: batch_recall = 122.47, batch_ndcg = 66.66 
2025-04-08 18:49:58.019601: Steps 117/138: batch_recall = 112.50, batch_ndcg = 66.09 
2025-04-08 18:49:59.288164: Steps 118/138: batch_recall = 121.75, batch_ndcg = 67.72 
2025-04-08 18:50:00.551682: Steps 119/138: batch_recall = 141.49, batch_ndcg = 77.16 
2025-04-08 18:50:01.819293: Steps 120/138: batch_recall = 126.41, batch_ndcg = 71.06 
2025-04-08 18:50:03.097213: Steps 121/138: batch_recall = 147.86, batch_ndcg = 78.11 
2025-04-08 18:50:04.374391: Steps 122/138: batch_recall = 148.37, batch_ndcg = 80.98 
2025-04-08 18:50:05.645992: Steps 123/138: batch_recall = 132.46, batch_ndcg = 75.21 
2025-04-08 18:50:06.928261: Steps 124/138: batch_recall = 152.66, batch_ndcg = 92.85 
2025-04-08 18:50:08.205508: Steps 125/138: batch_recall = 134.47, batch_ndcg = 74.25 
2025-04-08 18:50:09.491712: Steps 126/138: batch_recall = 158.19, batch_ndcg = 87.41 
2025-04-08 18:50:10.766773: Steps 127/138: batch_recall = 143.76, batch_ndcg = 81.63 
2025-04-08 18:50:12.039411: Steps 128/138: batch_recall = 128.16, batch_ndcg = 71.73 
2025-04-08 18:50:13.314047: Steps 129/138: batch_recall = 159.86, batch_ndcg = 91.88 
2025-04-08 18:50:14.577551: Steps 130/138: batch_recall = 131.48, batch_ndcg = 69.76 
2025-04-08 18:50:15.847570: Steps 131/138: batch_recall = 153.55, batch_ndcg = 88.69 
2025-04-08 18:50:17.113569: Steps 132/138: batch_recall = 149.63, batch_ndcg = 83.53 
2025-04-08 18:50:18.376258: Steps 133/138: batch_recall = 147.93, batch_ndcg = 85.74 
2025-04-08 18:50:19.641620: Steps 134/138: batch_recall = 141.88, batch_ndcg = 83.13 
2025-04-08 18:50:20.909091: Steps 135/138: batch_recall = 167.19, batch_ndcg = 95.57 
2025-04-08 18:50:22.180088: Steps 136/138: batch_recall = 153.89, batch_ndcg = 80.05 
2025-04-08 18:50:23.448645: Steps 137/138: batch_recall = 139.45, batch_ndcg = 86.28 
2025-04-08 18:50:23.449222: Epoch 29/1000, Test: Recall = 0.1761, NDCG = 0.1005  

2025-04-08 18:50:25.213689: Training Step 0/354: batchLoss = 0.5364, diffLoss = 2.6250, kgLoss = 0.0142
2025-04-08 18:50:26.834685: Training Step 1/354: batchLoss = 0.5345, diffLoss = 2.6169, kgLoss = 0.0139
2025-04-08 18:50:28.454728: Training Step 2/354: batchLoss = 0.5501, diffLoss = 2.6989, kgLoss = 0.0129
2025-04-08 18:50:30.068215: Training Step 3/354: batchLoss = 0.5056, diffLoss = 2.4826, kgLoss = 0.0114
2025-04-08 18:50:31.686063: Training Step 4/354: batchLoss = 0.4816, diffLoss = 2.3555, kgLoss = 0.0131
2025-04-08 18:50:33.298169: Training Step 5/354: batchLoss = 0.5803, diffLoss = 2.8341, kgLoss = 0.0168
2025-04-08 18:50:34.916065: Training Step 6/354: batchLoss = 0.4864, diffLoss = 2.3769, kgLoss = 0.0138
2025-04-08 18:50:36.535800: Training Step 7/354: batchLoss = 0.6422, diffLoss = 3.1429, kgLoss = 0.0170
2025-04-08 18:50:38.154362: Training Step 8/354: batchLoss = 0.4989, diffLoss = 2.4340, kgLoss = 0.0152
2025-04-08 18:50:39.778461: Training Step 9/354: batchLoss = 0.4994, diffLoss = 2.4476, kgLoss = 0.0124
2025-04-08 18:50:41.402730: Training Step 10/354: batchLoss = 0.4941, diffLoss = 2.4199, kgLoss = 0.0126
2025-04-08 18:50:43.026409: Training Step 11/354: batchLoss = 0.4986, diffLoss = 2.4391, kgLoss = 0.0134
2025-04-08 18:50:44.638613: Training Step 12/354: batchLoss = 0.5117, diffLoss = 2.5012, kgLoss = 0.0144
2025-04-08 18:50:46.253077: Training Step 13/354: batchLoss = 0.6328, diffLoss = 3.1017, kgLoss = 0.0155
2025-04-08 18:50:47.876287: Training Step 14/354: batchLoss = 0.5650, diffLoss = 2.7672, kgLoss = 0.0145
2025-04-08 18:50:49.494162: Training Step 15/354: batchLoss = 0.4775, diffLoss = 2.3370, kgLoss = 0.0126
2025-04-08 18:50:51.111485: Training Step 16/354: batchLoss = 0.5137, diffLoss = 2.5142, kgLoss = 0.0135
2025-04-08 18:50:52.735612: Training Step 17/354: batchLoss = 0.6443, diffLoss = 3.1591, kgLoss = 0.0156
2025-04-08 18:50:54.349926: Training Step 18/354: batchLoss = 0.5583, diffLoss = 2.7318, kgLoss = 0.0149
2025-04-08 18:50:55.971051: Training Step 19/354: batchLoss = 0.4827, diffLoss = 2.3579, kgLoss = 0.0139
2025-04-08 18:50:57.590570: Training Step 20/354: batchLoss = 0.4506, diffLoss = 2.2063, kgLoss = 0.0116
2025-04-08 18:50:59.211555: Training Step 21/354: batchLoss = 0.5296, diffLoss = 2.5727, kgLoss = 0.0189
2025-04-08 18:51:00.832493: Training Step 22/354: batchLoss = 0.5942, diffLoss = 2.9080, kgLoss = 0.0157
2025-04-08 18:51:02.448202: Training Step 23/354: batchLoss = 0.4630, diffLoss = 2.2622, kgLoss = 0.0132
2025-04-08 18:51:04.062191: Training Step 24/354: batchLoss = 0.6115, diffLoss = 2.9981, kgLoss = 0.0149
2025-04-08 18:51:05.682646: Training Step 25/354: batchLoss = 0.4820, diffLoss = 2.3682, kgLoss = 0.0105
2025-04-08 18:51:07.298823: Training Step 26/354: batchLoss = 0.5257, diffLoss = 2.5765, kgLoss = 0.0130
2025-04-08 18:51:08.916614: Training Step 27/354: batchLoss = 0.6282, diffLoss = 3.0793, kgLoss = 0.0155
2025-04-08 18:51:10.541653: Training Step 28/354: batchLoss = 0.6576, diffLoss = 3.2263, kgLoss = 0.0154
2025-04-08 18:51:12.161065: Training Step 29/354: batchLoss = 0.5770, diffLoss = 2.8189, kgLoss = 0.0166
2025-04-08 18:51:13.783489: Training Step 30/354: batchLoss = 0.5942, diffLoss = 2.9067, kgLoss = 0.0160
2025-04-08 18:51:15.410123: Training Step 31/354: batchLoss = 0.4991, diffLoss = 2.4403, kgLoss = 0.0138
2025-04-08 18:51:17.024237: Training Step 32/354: batchLoss = 0.6217, diffLoss = 3.0433, kgLoss = 0.0164
2025-04-08 18:51:18.640549: Training Step 33/354: batchLoss = 0.5188, diffLoss = 2.5381, kgLoss = 0.0140
2025-04-08 18:51:20.253849: Training Step 34/354: batchLoss = 0.5776, diffLoss = 2.8241, kgLoss = 0.0160
2025-04-08 18:51:21.869056: Training Step 35/354: batchLoss = 0.5705, diffLoss = 2.7948, kgLoss = 0.0144
2025-04-08 18:51:23.485909: Training Step 36/354: batchLoss = 0.4912, diffLoss = 2.4117, kgLoss = 0.0111
2025-04-08 18:51:25.113121: Training Step 37/354: batchLoss = 0.5214, diffLoss = 2.5521, kgLoss = 0.0138
2025-04-08 18:51:26.733518: Training Step 38/354: batchLoss = 0.4538, diffLoss = 2.2190, kgLoss = 0.0125
2025-04-08 18:51:28.352804: Training Step 39/354: batchLoss = 0.5798, diffLoss = 2.8403, kgLoss = 0.0147
2025-04-08 18:51:29.974295: Training Step 40/354: batchLoss = 0.6018, diffLoss = 2.9451, kgLoss = 0.0160
2025-04-08 18:51:31.597638: Training Step 41/354: batchLoss = 0.4995, diffLoss = 2.4338, kgLoss = 0.0160
2025-04-08 18:51:33.214071: Training Step 42/354: batchLoss = 0.5019, diffLoss = 2.4607, kgLoss = 0.0122
2025-04-08 18:51:34.820979: Training Step 43/354: batchLoss = 0.4952, diffLoss = 2.4232, kgLoss = 0.0132
2025-04-08 18:51:36.427374: Training Step 44/354: batchLoss = 0.5176, diffLoss = 2.5392, kgLoss = 0.0122
2025-04-08 18:51:38.037725: Training Step 45/354: batchLoss = 0.4115, diffLoss = 2.0158, kgLoss = 0.0105
2025-04-08 18:51:39.658217: Training Step 46/354: batchLoss = 0.5243, diffLoss = 2.5618, kgLoss = 0.0149
2025-04-08 18:51:41.288369: Training Step 47/354: batchLoss = 0.5289, diffLoss = 2.5851, kgLoss = 0.0149
2025-04-08 18:51:42.915819: Training Step 48/354: batchLoss = 0.5242, diffLoss = 2.5666, kgLoss = 0.0137
2025-04-08 18:51:44.538767: Training Step 49/354: batchLoss = 0.5175, diffLoss = 2.5367, kgLoss = 0.0126
2025-04-08 18:51:46.160832: Training Step 50/354: batchLoss = 0.5789, diffLoss = 2.7253, kgLoss = 0.0423
2025-04-08 18:51:47.779799: Training Step 51/354: batchLoss = 0.5659, diffLoss = 2.7643, kgLoss = 0.0163
2025-04-08 18:51:49.395720: Training Step 52/354: batchLoss = 0.5049, diffLoss = 2.4775, kgLoss = 0.0117
2025-04-08 18:51:51.017238: Training Step 53/354: batchLoss = 0.6096, diffLoss = 2.9831, kgLoss = 0.0163
2025-04-08 18:51:52.632297: Training Step 54/354: batchLoss = 0.5493, diffLoss = 2.6929, kgLoss = 0.0133
2025-04-08 18:51:54.241153: Training Step 55/354: batchLoss = 0.5111, diffLoss = 2.4980, kgLoss = 0.0144
2025-04-08 18:51:55.855155: Training Step 56/354: batchLoss = 0.7218, diffLoss = 3.5378, kgLoss = 0.0178
2025-04-08 18:51:57.486692: Training Step 57/354: batchLoss = 0.5939, diffLoss = 2.9129, kgLoss = 0.0142
2025-04-08 18:51:59.104256: Training Step 58/354: batchLoss = 0.6232, diffLoss = 3.0568, kgLoss = 0.0148
2025-04-08 18:52:00.719129: Training Step 59/354: batchLoss = 0.5560, diffLoss = 2.7235, kgLoss = 0.0141
2025-04-08 18:52:02.342817: Training Step 60/354: batchLoss = 0.6447, diffLoss = 3.1594, kgLoss = 0.0160
2025-04-08 18:52:03.965587: Training Step 61/354: batchLoss = 0.4857, diffLoss = 2.3753, kgLoss = 0.0133
2025-04-08 18:52:05.588480: Training Step 62/354: batchLoss = 0.4180, diffLoss = 2.0406, kgLoss = 0.0123
2025-04-08 18:52:07.199940: Training Step 63/354: batchLoss = 0.6675, diffLoss = 3.2739, kgLoss = 0.0159
2025-04-08 18:52:08.823234: Training Step 64/354: batchLoss = 0.7665, diffLoss = 3.7448, kgLoss = 0.0220
2025-04-08 18:52:10.438072: Training Step 65/354: batchLoss = 0.4813, diffLoss = 2.3459, kgLoss = 0.0152
2025-04-08 18:52:12.059474: Training Step 66/354: batchLoss = 0.5736, diffLoss = 2.8107, kgLoss = 0.0143
2025-04-08 18:52:13.680667: Training Step 67/354: batchLoss = 0.6665, diffLoss = 3.2586, kgLoss = 0.0185
2025-04-08 18:52:15.300688: Training Step 68/354: batchLoss = 0.4730, diffLoss = 2.3143, kgLoss = 0.0127
2025-04-08 18:52:16.924709: Training Step 69/354: batchLoss = 0.6904, diffLoss = 3.3793, kgLoss = 0.0181
2025-04-08 18:52:18.550740: Training Step 70/354: batchLoss = 0.5272, diffLoss = 2.5783, kgLoss = 0.0144
2025-04-08 18:52:20.177108: Training Step 71/354: batchLoss = 0.7016, diffLoss = 3.4411, kgLoss = 0.0167
2025-04-08 18:52:21.803721: Training Step 72/354: batchLoss = 0.4991, diffLoss = 2.4373, kgLoss = 0.0145
2025-04-08 18:52:23.423948: Training Step 73/354: batchLoss = 0.4924, diffLoss = 2.4133, kgLoss = 0.0122
2025-04-08 18:52:25.045309: Training Step 74/354: batchLoss = 0.5534, diffLoss = 2.7116, kgLoss = 0.0138
2025-04-08 18:52:26.658005: Training Step 75/354: batchLoss = 0.6214, diffLoss = 3.0430, kgLoss = 0.0159
2025-04-08 18:52:28.279222: Training Step 76/354: batchLoss = 0.5274, diffLoss = 2.5865, kgLoss = 0.0126
2025-04-08 18:52:29.903423: Training Step 77/354: batchLoss = 0.4865, diffLoss = 2.3774, kgLoss = 0.0138
2025-04-08 18:52:31.524388: Training Step 78/354: batchLoss = 0.6269, diffLoss = 3.0679, kgLoss = 0.0167
2025-04-08 18:52:33.147709: Training Step 79/354: batchLoss = 0.5854, diffLoss = 2.8633, kgLoss = 0.0159
2025-04-08 18:52:34.771666: Training Step 80/354: batchLoss = 0.5255, diffLoss = 2.5792, kgLoss = 0.0121
2025-04-08 18:52:36.401887: Training Step 81/354: batchLoss = 0.4837, diffLoss = 2.3727, kgLoss = 0.0114
2025-04-08 18:52:38.018698: Training Step 82/354: batchLoss = 0.4931, diffLoss = 2.4133, kgLoss = 0.0131
2025-04-08 18:52:39.638770: Training Step 83/354: batchLoss = 0.5703, diffLoss = 2.7895, kgLoss = 0.0155
2025-04-08 18:52:41.256742: Training Step 84/354: batchLoss = 0.4566, diffLoss = 2.2328, kgLoss = 0.0125
2025-04-08 18:52:42.872427: Training Step 85/354: batchLoss = 0.6170, diffLoss = 3.0251, kgLoss = 0.0149
2025-04-08 18:52:44.495732: Training Step 86/354: batchLoss = 0.4956, diffLoss = 2.4250, kgLoss = 0.0132
2025-04-08 18:52:46.118473: Training Step 87/354: batchLoss = 0.5779, diffLoss = 2.8344, kgLoss = 0.0138
2025-04-08 18:52:47.739428: Training Step 88/354: batchLoss = 0.5189, diffLoss = 2.5396, kgLoss = 0.0138
2025-04-08 18:52:49.367305: Training Step 89/354: batchLoss = 0.6167, diffLoss = 3.0179, kgLoss = 0.0164
2025-04-08 18:52:50.984414: Training Step 90/354: batchLoss = 0.5407, diffLoss = 2.6488, kgLoss = 0.0136
2025-04-08 18:52:52.605818: Training Step 91/354: batchLoss = 0.5931, diffLoss = 2.9013, kgLoss = 0.0160
2025-04-08 18:52:54.220953: Training Step 92/354: batchLoss = 0.4954, diffLoss = 2.4266, kgLoss = 0.0126
2025-04-08 18:52:55.835108: Training Step 93/354: batchLoss = 0.4351, diffLoss = 2.1283, kgLoss = 0.0118
2025-04-08 18:52:57.455237: Training Step 94/354: batchLoss = 0.5372, diffLoss = 2.6266, kgLoss = 0.0148
2025-04-08 18:52:59.072850: Training Step 95/354: batchLoss = 0.6663, diffLoss = 3.2672, kgLoss = 0.0161
2025-04-08 18:53:00.694881: Training Step 96/354: batchLoss = 0.4883, diffLoss = 2.3818, kgLoss = 0.0150
2025-04-08 18:53:02.319342: Training Step 97/354: batchLoss = 0.5474, diffLoss = 2.6778, kgLoss = 0.0147
2025-04-08 18:53:03.937307: Training Step 98/354: batchLoss = 0.8023, diffLoss = 3.9319, kgLoss = 0.0199
2025-04-08 18:53:05.560712: Training Step 99/354: batchLoss = 0.5434, diffLoss = 2.6532, kgLoss = 0.0159
2025-04-08 18:53:07.188502: Training Step 100/354: batchLoss = 0.4979, diffLoss = 2.4384, kgLoss = 0.0127
2025-04-08 18:53:08.812663: Training Step 101/354: batchLoss = 0.6906, diffLoss = 3.3779, kgLoss = 0.0187
2025-04-08 18:53:10.429225: Training Step 102/354: batchLoss = 0.5105, diffLoss = 2.5036, kgLoss = 0.0122
2025-04-08 18:53:12.043276: Training Step 103/354: batchLoss = 0.5411, diffLoss = 2.6491, kgLoss = 0.0142
2025-04-08 18:53:13.653688: Training Step 104/354: batchLoss = 0.5429, diffLoss = 2.6550, kgLoss = 0.0148
2025-04-08 18:53:15.270315: Training Step 105/354: batchLoss = 0.5861, diffLoss = 2.8696, kgLoss = 0.0152
2025-04-08 18:53:16.892863: Training Step 106/354: batchLoss = 0.5148, diffLoss = 2.5160, kgLoss = 0.0145
2025-04-08 18:53:18.513461: Training Step 107/354: batchLoss = 0.4764, diffLoss = 2.3303, kgLoss = 0.0129
2025-04-08 18:53:20.136808: Training Step 108/354: batchLoss = 0.5538, diffLoss = 2.7134, kgLoss = 0.0139
2025-04-08 18:53:21.759284: Training Step 109/354: batchLoss = 0.5672, diffLoss = 2.7770, kgLoss = 0.0148
2025-04-08 18:53:23.380753: Training Step 110/354: batchLoss = 0.6626, diffLoss = 3.2421, kgLoss = 0.0177
2025-04-08 18:53:25.001638: Training Step 111/354: batchLoss = 0.5588, diffLoss = 2.7340, kgLoss = 0.0150
2025-04-08 18:53:26.619969: Training Step 112/354: batchLoss = 0.5499, diffLoss = 2.6926, kgLoss = 0.0142
2025-04-08 18:53:28.234516: Training Step 113/354: batchLoss = 0.6203, diffLoss = 3.0439, kgLoss = 0.0144
2025-04-08 18:53:29.853265: Training Step 114/354: batchLoss = 0.5268, diffLoss = 2.5785, kgLoss = 0.0139
2025-04-08 18:53:31.467534: Training Step 115/354: batchLoss = 0.6320, diffLoss = 3.0893, kgLoss = 0.0176
2025-04-08 18:53:33.086334: Training Step 116/354: batchLoss = 0.5295, diffLoss = 2.5831, kgLoss = 0.0161
2025-04-08 18:53:34.719368: Training Step 117/354: batchLoss = 0.5350, diffLoss = 2.6171, kgLoss = 0.0145
2025-04-08 18:53:36.337070: Training Step 118/354: batchLoss = 0.6018, diffLoss = 2.9460, kgLoss = 0.0157
2025-04-08 18:53:37.963936: Training Step 119/354: batchLoss = 0.5634, diffLoss = 2.7548, kgLoss = 0.0156
2025-04-08 18:53:39.584744: Training Step 120/354: batchLoss = 0.5394, diffLoss = 2.6330, kgLoss = 0.0159
2025-04-08 18:53:41.204197: Training Step 121/354: batchLoss = 0.4985, diffLoss = 2.4406, kgLoss = 0.0130
2025-04-08 18:53:42.819841: Training Step 122/354: batchLoss = 0.5874, diffLoss = 2.8738, kgLoss = 0.0157
2025-04-08 18:53:44.439804: Training Step 123/354: batchLoss = 0.6024, diffLoss = 2.9492, kgLoss = 0.0157
2025-04-08 18:53:46.058161: Training Step 124/354: batchLoss = 0.5650, diffLoss = 2.7669, kgLoss = 0.0145
2025-04-08 18:53:47.680535: Training Step 125/354: batchLoss = 0.6543, diffLoss = 3.2054, kgLoss = 0.0166
2025-04-08 18:53:49.303162: Training Step 126/354: batchLoss = 0.6375, diffLoss = 3.1239, kgLoss = 0.0160
2025-04-08 18:53:50.924718: Training Step 127/354: batchLoss = 0.4396, diffLoss = 2.1489, kgLoss = 0.0123
2025-04-08 18:53:52.542312: Training Step 128/354: batchLoss = 0.6317, diffLoss = 3.0955, kgLoss = 0.0157
2025-04-08 18:53:54.167570: Training Step 129/354: batchLoss = 0.4730, diffLoss = 2.3202, kgLoss = 0.0112
2025-04-08 18:53:55.792863: Training Step 130/354: batchLoss = 0.6113, diffLoss = 2.9951, kgLoss = 0.0153
2025-04-08 18:53:57.410198: Training Step 131/354: batchLoss = 0.6033, diffLoss = 2.9567, kgLoss = 0.0150
2025-04-08 18:53:59.029159: Training Step 132/354: batchLoss = 0.5286, diffLoss = 2.5851, kgLoss = 0.0145
2025-04-08 18:54:00.642421: Training Step 133/354: batchLoss = 0.4820, diffLoss = 2.3571, kgLoss = 0.0133
2025-04-08 18:54:02.263731: Training Step 134/354: batchLoss = 0.5596, diffLoss = 2.7358, kgLoss = 0.0156
2025-04-08 18:54:03.884000: Training Step 135/354: batchLoss = 0.6359, diffLoss = 3.1188, kgLoss = 0.0152
2025-04-08 18:54:05.503310: Training Step 136/354: batchLoss = 0.6939, diffLoss = 3.4017, kgLoss = 0.0170
2025-04-08 18:54:07.127428: Training Step 137/354: batchLoss = 0.4850, diffLoss = 2.3747, kgLoss = 0.0125
2025-04-08 18:54:08.754891: Training Step 138/354: batchLoss = 0.4822, diffLoss = 2.3551, kgLoss = 0.0139
2025-04-08 18:54:10.381752: Training Step 139/354: batchLoss = 0.5722, diffLoss = 2.7899, kgLoss = 0.0178
2025-04-08 18:54:12.005650: Training Step 140/354: batchLoss = 0.5401, diffLoss = 2.6461, kgLoss = 0.0136
2025-04-08 18:54:13.620278: Training Step 141/354: batchLoss = 0.5563, diffLoss = 2.7237, kgLoss = 0.0144
2025-04-08 18:54:15.241192: Training Step 142/354: batchLoss = 0.6153, diffLoss = 3.0112, kgLoss = 0.0164
2025-04-08 18:54:16.858039: Training Step 143/354: batchLoss = 0.7260, diffLoss = 3.5592, kgLoss = 0.0177
2025-04-08 18:54:18.475879: Training Step 144/354: batchLoss = 0.5950, diffLoss = 2.9120, kgLoss = 0.0157
2025-04-08 18:54:20.096621: Training Step 145/354: batchLoss = 0.6782, diffLoss = 3.3320, kgLoss = 0.0148
2025-04-08 18:54:21.724982: Training Step 146/354: batchLoss = 0.5464, diffLoss = 2.6735, kgLoss = 0.0146
2025-04-08 18:54:23.352972: Training Step 147/354: batchLoss = 0.5723, diffLoss = 2.8019, kgLoss = 0.0149
2025-04-08 18:54:24.980438: Training Step 148/354: batchLoss = 0.4936, diffLoss = 2.4114, kgLoss = 0.0142
2025-04-08 18:54:26.599958: Training Step 149/354: batchLoss = 0.4964, diffLoss = 2.4236, kgLoss = 0.0146
2025-04-08 18:54:28.221543: Training Step 150/354: batchLoss = 0.4906, diffLoss = 2.4018, kgLoss = 0.0128
2025-04-08 18:54:29.837209: Training Step 151/354: batchLoss = 0.4678, diffLoss = 2.2863, kgLoss = 0.0132
2025-04-08 18:54:31.454843: Training Step 152/354: batchLoss = 0.5392, diffLoss = 2.6380, kgLoss = 0.0145
2025-04-08 18:54:33.073284: Training Step 153/354: batchLoss = 0.5611, diffLoss = 2.7470, kgLoss = 0.0147
2025-04-08 18:54:34.693611: Training Step 154/354: batchLoss = 0.4604, diffLoss = 2.2479, kgLoss = 0.0135
2025-04-08 18:54:36.315006: Training Step 155/354: batchLoss = 0.6516, diffLoss = 3.1950, kgLoss = 0.0158
2025-04-08 18:54:37.935941: Training Step 156/354: batchLoss = 0.5467, diffLoss = 2.6801, kgLoss = 0.0133
2025-04-08 18:54:39.562274: Training Step 157/354: batchLoss = 0.6310, diffLoss = 3.0948, kgLoss = 0.0151
2025-04-08 18:54:41.184244: Training Step 158/354: batchLoss = 0.5596, diffLoss = 2.7404, kgLoss = 0.0144
2025-04-08 18:54:42.803414: Training Step 159/354: batchLoss = 0.5825, diffLoss = 2.8533, kgLoss = 0.0148
2025-04-08 18:54:44.425040: Training Step 160/354: batchLoss = 1.0061, diffLoss = 4.9254, kgLoss = 0.0262
2025-04-08 18:54:46.042311: Training Step 161/354: batchLoss = 0.5478, diffLoss = 2.6792, kgLoss = 0.0149
2025-04-08 18:54:47.660595: Training Step 162/354: batchLoss = 0.5007, diffLoss = 2.4513, kgLoss = 0.0130
2025-04-08 18:54:49.271752: Training Step 163/354: batchLoss = 0.5569, diffLoss = 2.7197, kgLoss = 0.0162
2025-04-08 18:54:50.888530: Training Step 164/354: batchLoss = 0.5375, diffLoss = 2.6341, kgLoss = 0.0133
2025-04-08 18:54:52.515417: Training Step 165/354: batchLoss = 0.5077, diffLoss = 2.4879, kgLoss = 0.0127
2025-04-08 18:54:54.136795: Training Step 166/354: batchLoss = 0.6488, diffLoss = 3.1816, kgLoss = 0.0156
2025-04-08 18:54:55.759090: Training Step 167/354: batchLoss = 0.5526, diffLoss = 2.7104, kgLoss = 0.0131
2025-04-08 18:54:57.378842: Training Step 168/354: batchLoss = 0.5356, diffLoss = 2.6205, kgLoss = 0.0144
2025-04-08 18:54:59.000402: Training Step 169/354: batchLoss = 0.7236, diffLoss = 3.5461, kgLoss = 0.0179
2025-04-08 18:55:00.618877: Training Step 170/354: batchLoss = 0.5127, diffLoss = 2.5066, kgLoss = 0.0143
2025-04-08 18:55:02.236943: Training Step 171/354: batchLoss = 0.5923, diffLoss = 2.9039, kgLoss = 0.0144
2025-04-08 18:55:03.856475: Training Step 172/354: batchLoss = 0.5015, diffLoss = 2.4601, kgLoss = 0.0119
2025-04-08 18:55:05.472710: Training Step 173/354: batchLoss = 0.6109, diffLoss = 2.9869, kgLoss = 0.0170
2025-04-08 18:55:07.087464: Training Step 174/354: batchLoss = 0.5779, diffLoss = 2.8227, kgLoss = 0.0167
2025-04-08 18:55:08.705202: Training Step 175/354: batchLoss = 0.5923, diffLoss = 2.9036, kgLoss = 0.0144
2025-04-08 18:55:10.323828: Training Step 176/354: batchLoss = 0.4308, diffLoss = 2.0994, kgLoss = 0.0136
2025-04-08 18:55:11.951533: Training Step 177/354: batchLoss = 0.5203, diffLoss = 2.5484, kgLoss = 0.0132
2025-04-08 18:55:13.577976: Training Step 178/354: batchLoss = 0.5755, diffLoss = 2.8204, kgLoss = 0.0143
2025-04-08 18:55:15.200747: Training Step 179/354: batchLoss = 0.5050, diffLoss = 2.4672, kgLoss = 0.0144
2025-04-08 18:55:16.816357: Training Step 180/354: batchLoss = 0.4450, diffLoss = 2.1796, kgLoss = 0.0114
2025-04-08 18:55:18.435363: Training Step 181/354: batchLoss = 0.5682, diffLoss = 2.7801, kgLoss = 0.0153
2025-04-08 18:55:20.050990: Training Step 182/354: batchLoss = 0.5546, diffLoss = 2.7180, kgLoss = 0.0137
2025-04-08 18:55:21.665739: Training Step 183/354: batchLoss = 0.5219, diffLoss = 2.5520, kgLoss = 0.0143
2025-04-08 18:55:23.299513: Training Step 184/354: batchLoss = 0.4056, diffLoss = 1.9830, kgLoss = 0.0113
2025-04-08 18:55:24.925653: Training Step 185/354: batchLoss = 0.6214, diffLoss = 3.0426, kgLoss = 0.0161
2025-04-08 18:55:26.547401: Training Step 186/354: batchLoss = 0.7073, diffLoss = 3.4632, kgLoss = 0.0183
2025-04-08 18:55:28.170665: Training Step 187/354: batchLoss = 0.4836, diffLoss = 2.3615, kgLoss = 0.0142
2025-04-08 18:55:29.793810: Training Step 188/354: batchLoss = 0.6128, diffLoss = 2.9986, kgLoss = 0.0163
2025-04-08 18:55:31.417387: Training Step 189/354: batchLoss = 0.4452, diffLoss = 2.1751, kgLoss = 0.0127
2025-04-08 18:55:33.044871: Training Step 190/354: batchLoss = 0.5145, diffLoss = 2.5151, kgLoss = 0.0144
2025-04-08 18:55:34.663503: Training Step 191/354: batchLoss = 0.6374, diffLoss = 3.1141, kgLoss = 0.0182
2025-04-08 18:55:36.280391: Training Step 192/354: batchLoss = 0.5333, diffLoss = 2.6031, kgLoss = 0.0159
2025-04-08 18:55:37.895482: Training Step 193/354: batchLoss = 0.5604, diffLoss = 2.7416, kgLoss = 0.0152
2025-04-08 18:55:39.518469: Training Step 194/354: batchLoss = 0.6044, diffLoss = 2.9548, kgLoss = 0.0168
2025-04-08 18:55:41.138802: Training Step 195/354: batchLoss = 0.7885, diffLoss = 3.8711, kgLoss = 0.0179
2025-04-08 18:55:42.760509: Training Step 196/354: batchLoss = 0.5615, diffLoss = 2.7442, kgLoss = 0.0158
2025-04-08 18:55:44.385342: Training Step 197/354: batchLoss = 0.4529, diffLoss = 2.2183, kgLoss = 0.0116
2025-04-08 18:55:46.004846: Training Step 198/354: batchLoss = 0.5545, diffLoss = 2.7145, kgLoss = 0.0145
2025-04-08 18:55:47.627417: Training Step 199/354: batchLoss = 0.5303, diffLoss = 2.5955, kgLoss = 0.0140
2025-04-08 18:55:49.244613: Training Step 200/354: batchLoss = 0.6333, diffLoss = 3.0846, kgLoss = 0.0204
2025-04-08 18:55:50.867838: Training Step 201/354: batchLoss = 0.5214, diffLoss = 2.5474, kgLoss = 0.0149
2025-04-08 18:55:52.486441: Training Step 202/354: batchLoss = 0.4295, diffLoss = 2.0912, kgLoss = 0.0140
2025-04-08 18:55:54.101073: Training Step 203/354: batchLoss = 0.5217, diffLoss = 2.5511, kgLoss = 0.0144
2025-04-08 18:55:55.721949: Training Step 204/354: batchLoss = 0.4992, diffLoss = 2.4428, kgLoss = 0.0133
2025-04-08 18:55:57.341746: Training Step 205/354: batchLoss = 0.4526, diffLoss = 2.2158, kgLoss = 0.0118
2025-04-08 18:55:58.969605: Training Step 206/354: batchLoss = 0.5840, diffLoss = 2.8630, kgLoss = 0.0143
2025-04-08 18:56:00.595065: Training Step 207/354: batchLoss = 0.5110, diffLoss = 2.4990, kgLoss = 0.0140
2025-04-08 18:56:02.213684: Training Step 208/354: batchLoss = 0.5068, diffLoss = 2.4814, kgLoss = 0.0132
2025-04-08 18:56:03.834701: Training Step 209/354: batchLoss = 0.4821, diffLoss = 2.3519, kgLoss = 0.0147
2025-04-08 18:56:05.451508: Training Step 210/354: batchLoss = 0.5259, diffLoss = 2.5772, kgLoss = 0.0131
2025-04-08 18:56:07.072090: Training Step 211/354: batchLoss = 0.4366, diffLoss = 2.1365, kgLoss = 0.0116
2025-04-08 18:56:08.690120: Training Step 212/354: batchLoss = 0.4936, diffLoss = 2.4175, kgLoss = 0.0126
2025-04-08 18:56:10.308841: Training Step 213/354: batchLoss = 0.5284, diffLoss = 2.5864, kgLoss = 0.0138
2025-04-08 18:56:11.933431: Training Step 214/354: batchLoss = 0.5461, diffLoss = 2.6763, kgLoss = 0.0136
2025-04-08 18:56:13.555348: Training Step 215/354: batchLoss = 0.5277, diffLoss = 2.5794, kgLoss = 0.0148
2025-04-08 18:56:15.178284: Training Step 216/354: batchLoss = 0.5717, diffLoss = 2.7964, kgLoss = 0.0156
2025-04-08 18:56:16.799876: Training Step 217/354: batchLoss = 0.4814, diffLoss = 2.3555, kgLoss = 0.0128
2025-04-08 18:56:18.439424: Training Step 218/354: batchLoss = 0.5235, diffLoss = 2.5587, kgLoss = 0.0147
2025-04-08 18:56:20.059632: Training Step 219/354: batchLoss = 0.5267, diffLoss = 2.5841, kgLoss = 0.0124
2025-04-08 18:56:21.678958: Training Step 220/354: batchLoss = 0.6667, diffLoss = 3.2595, kgLoss = 0.0186
2025-04-08 18:56:23.294363: Training Step 221/354: batchLoss = 0.5477, diffLoss = 2.6837, kgLoss = 0.0136
2025-04-08 18:56:24.911525: Training Step 222/354: batchLoss = 0.5520, diffLoss = 2.6988, kgLoss = 0.0154
2025-04-08 18:56:26.532085: Training Step 223/354: batchLoss = 0.6762, diffLoss = 3.3098, kgLoss = 0.0178
2025-04-08 18:56:28.154405: Training Step 224/354: batchLoss = 0.6814, diffLoss = 3.3379, kgLoss = 0.0172
2025-04-08 18:56:29.771943: Training Step 225/354: batchLoss = 0.5011, diffLoss = 2.4512, kgLoss = 0.0136
2025-04-08 18:56:31.394432: Training Step 226/354: batchLoss = 0.7047, diffLoss = 3.4438, kgLoss = 0.0199
2025-04-08 18:56:33.014085: Training Step 227/354: batchLoss = 0.4920, diffLoss = 2.4046, kgLoss = 0.0139
2025-04-08 18:56:34.640438: Training Step 228/354: batchLoss = 0.4700, diffLoss = 2.2831, kgLoss = 0.0168
2025-04-08 18:56:36.258848: Training Step 229/354: batchLoss = 0.4701, diffLoss = 2.2997, kgLoss = 0.0127
2025-04-08 18:56:37.951184: Training Step 230/354: batchLoss = 0.5766, diffLoss = 2.8186, kgLoss = 0.0161
2025-04-08 18:56:39.561267: Training Step 231/354: batchLoss = 0.4347, diffLoss = 2.1264, kgLoss = 0.0118
2025-04-08 18:56:41.175650: Training Step 232/354: batchLoss = 0.4883, diffLoss = 2.3927, kgLoss = 0.0122
2025-04-08 18:56:42.789720: Training Step 233/354: batchLoss = 0.5134, diffLoss = 2.5160, kgLoss = 0.0128
2025-04-08 18:56:44.410595: Training Step 234/354: batchLoss = 0.5066, diffLoss = 2.4687, kgLoss = 0.0161
2025-04-08 18:56:46.033320: Training Step 235/354: batchLoss = 0.5911, diffLoss = 2.8941, kgLoss = 0.0153
2025-04-08 18:56:47.652721: Training Step 236/354: batchLoss = 0.5222, diffLoss = 2.5629, kgLoss = 0.0120
2025-04-08 18:56:49.270408: Training Step 237/354: batchLoss = 0.5123, diffLoss = 2.5105, kgLoss = 0.0128
2025-04-08 18:56:50.884605: Training Step 238/354: batchLoss = 0.5891, diffLoss = 2.8857, kgLoss = 0.0149
2025-04-08 18:56:52.502296: Training Step 239/354: batchLoss = 0.7879, diffLoss = 3.8583, kgLoss = 0.0203
2025-04-08 18:56:54.120417: Training Step 240/354: batchLoss = 0.4430, diffLoss = 2.1667, kgLoss = 0.0121
2025-04-08 18:56:55.736583: Training Step 241/354: batchLoss = 0.5971, diffLoss = 2.9293, kgLoss = 0.0140
2025-04-08 18:56:57.350701: Training Step 242/354: batchLoss = 0.5011, diffLoss = 2.4490, kgLoss = 0.0142
2025-04-08 18:56:58.969786: Training Step 243/354: batchLoss = 0.4158, diffLoss = 2.0337, kgLoss = 0.0114
2025-04-08 18:57:00.593195: Training Step 244/354: batchLoss = 0.4921, diffLoss = 2.4109, kgLoss = 0.0124
2025-04-08 18:57:02.207308: Training Step 245/354: batchLoss = 0.5583, diffLoss = 2.7347, kgLoss = 0.0142
2025-04-08 18:57:03.824845: Training Step 246/354: batchLoss = 0.4749, diffLoss = 2.3243, kgLoss = 0.0126
2025-04-08 18:57:05.445242: Training Step 247/354: batchLoss = 0.5569, diffLoss = 2.7257, kgLoss = 0.0147
2025-04-08 18:57:07.059154: Training Step 248/354: batchLoss = 0.5824, diffLoss = 2.8543, kgLoss = 0.0144
2025-04-08 18:57:08.678011: Training Step 249/354: batchLoss = 0.5407, diffLoss = 2.6419, kgLoss = 0.0154
2025-04-08 18:57:10.289543: Training Step 250/354: batchLoss = 0.6630, diffLoss = 3.2480, kgLoss = 0.0168
2025-04-08 18:57:11.903446: Training Step 251/354: batchLoss = 0.6956, diffLoss = 3.4064, kgLoss = 0.0179
2025-04-08 18:57:13.522815: Training Step 252/354: batchLoss = 0.7177, diffLoss = 3.5203, kgLoss = 0.0170
2025-04-08 18:57:15.152630: Training Step 253/354: batchLoss = 0.6918, diffLoss = 3.3845, kgLoss = 0.0186
2025-04-08 18:57:16.770100: Training Step 254/354: batchLoss = 0.6579, diffLoss = 3.2217, kgLoss = 0.0169
2025-04-08 18:57:18.393951: Training Step 255/354: batchLoss = 0.5081, diffLoss = 2.4863, kgLoss = 0.0135
2025-04-08 18:57:20.018534: Training Step 256/354: batchLoss = 0.6003, diffLoss = 2.9407, kgLoss = 0.0153
2025-04-08 18:57:21.638651: Training Step 257/354: batchLoss = 0.5446, diffLoss = 2.6704, kgLoss = 0.0132
2025-04-08 18:57:23.258600: Training Step 258/354: batchLoss = 0.6565, diffLoss = 3.2188, kgLoss = 0.0159
2025-04-08 18:57:24.874891: Training Step 259/354: batchLoss = 0.5449, diffLoss = 2.6609, kgLoss = 0.0159
2025-04-08 18:57:26.489589: Training Step 260/354: batchLoss = 0.5540, diffLoss = 2.7117, kgLoss = 0.0146
2025-04-08 18:57:28.111719: Training Step 261/354: batchLoss = 0.4601, diffLoss = 2.2484, kgLoss = 0.0131
2025-04-08 18:57:29.732440: Training Step 262/354: batchLoss = 0.5960, diffLoss = 2.9230, kgLoss = 0.0143
2025-04-08 18:57:31.354701: Training Step 263/354: batchLoss = 0.5184, diffLoss = 2.5367, kgLoss = 0.0138
2025-04-08 18:57:32.980710: Training Step 264/354: batchLoss = 0.8066, diffLoss = 3.9548, kgLoss = 0.0195
2025-04-08 18:57:34.602995: Training Step 265/354: batchLoss = 0.6013, diffLoss = 2.9427, kgLoss = 0.0160
2025-04-08 18:57:36.223814: Training Step 266/354: batchLoss = 0.5192, diffLoss = 2.5376, kgLoss = 0.0146
2025-04-08 18:57:37.853602: Training Step 267/354: batchLoss = 0.4593, diffLoss = 2.2452, kgLoss = 0.0128
2025-04-08 18:57:39.470161: Training Step 268/354: batchLoss = 0.5825, diffLoss = 2.8461, kgLoss = 0.0166
2025-04-08 18:57:41.090050: Training Step 269/354: batchLoss = 0.5289, diffLoss = 2.5884, kgLoss = 0.0141
2025-04-08 18:57:42.704457: Training Step 270/354: batchLoss = 0.5222, diffLoss = 2.5585, kgLoss = 0.0131
2025-04-08 18:57:44.319835: Training Step 271/354: batchLoss = 0.5504, diffLoss = 2.6868, kgLoss = 0.0163
2025-04-08 18:57:45.937679: Training Step 272/354: batchLoss = 0.6113, diffLoss = 2.9949, kgLoss = 0.0154
2025-04-08 18:57:47.560701: Training Step 273/354: batchLoss = 0.5635, diffLoss = 2.7585, kgLoss = 0.0147
2025-04-08 18:57:49.186621: Training Step 274/354: batchLoss = 0.6804, diffLoss = 3.3395, kgLoss = 0.0157
2025-04-08 18:57:50.811174: Training Step 275/354: batchLoss = 0.5430, diffLoss = 2.6521, kgLoss = 0.0157
2025-04-08 18:57:52.431848: Training Step 276/354: batchLoss = 0.5056, diffLoss = 2.4753, kgLoss = 0.0132
2025-04-08 18:57:54.055731: Training Step 277/354: batchLoss = 0.5792, diffLoss = 2.8332, kgLoss = 0.0158
2025-04-08 18:57:55.678337: Training Step 278/354: batchLoss = 0.6663, diffLoss = 3.2627, kgLoss = 0.0172
2025-04-08 18:57:57.297650: Training Step 279/354: batchLoss = 0.4341, diffLoss = 2.1223, kgLoss = 0.0120
2025-04-08 18:57:58.913321: Training Step 280/354: batchLoss = 0.5191, diffLoss = 2.5378, kgLoss = 0.0144
2025-04-08 18:58:00.529690: Training Step 281/354: batchLoss = 0.5762, diffLoss = 2.8177, kgLoss = 0.0158
2025-04-08 18:58:02.149531: Training Step 282/354: batchLoss = 0.5166, diffLoss = 2.5284, kgLoss = 0.0136
2025-04-08 18:58:03.771035: Training Step 283/354: batchLoss = 0.5875, diffLoss = 2.8787, kgLoss = 0.0147
2025-04-08 18:58:05.392366: Training Step 284/354: batchLoss = 0.4418, diffLoss = 2.1546, kgLoss = 0.0136
2025-04-08 18:58:07.019722: Training Step 285/354: batchLoss = 0.5145, diffLoss = 2.5200, kgLoss = 0.0131
2025-04-08 18:58:08.636403: Training Step 286/354: batchLoss = 0.4754, diffLoss = 2.3287, kgLoss = 0.0121
2025-04-08 18:58:10.260033: Training Step 287/354: batchLoss = 0.4642, diffLoss = 2.2761, kgLoss = 0.0113
2025-04-08 18:58:11.876896: Training Step 288/354: batchLoss = 0.5420, diffLoss = 2.6509, kgLoss = 0.0147
2025-04-08 18:58:13.496583: Training Step 289/354: batchLoss = 0.4872, diffLoss = 2.3782, kgLoss = 0.0144
2025-04-08 18:58:15.112519: Training Step 290/354: batchLoss = 0.4657, diffLoss = 2.2816, kgLoss = 0.0117
2025-04-08 18:58:16.728770: Training Step 291/354: batchLoss = 0.6147, diffLoss = 3.0019, kgLoss = 0.0179
2025-04-08 18:58:18.340654: Training Step 292/354: batchLoss = 0.4867, diffLoss = 2.3818, kgLoss = 0.0130
2025-04-08 18:58:19.964314: Training Step 293/354: batchLoss = 0.6085, diffLoss = 2.9723, kgLoss = 0.0175
2025-04-08 18:58:21.584694: Training Step 294/354: batchLoss = 0.4952, diffLoss = 2.4192, kgLoss = 0.0142
2025-04-08 18:58:23.198292: Training Step 295/354: batchLoss = 0.5429, diffLoss = 2.6558, kgLoss = 0.0146
2025-04-08 18:58:24.818525: Training Step 296/354: batchLoss = 0.6980, diffLoss = 3.4152, kgLoss = 0.0186
2025-04-08 18:58:26.437832: Training Step 297/354: batchLoss = 0.5650, diffLoss = 2.7662, kgLoss = 0.0148
2025-04-08 18:58:28.054854: Training Step 298/354: batchLoss = 0.5620, diffLoss = 2.7505, kgLoss = 0.0149
2025-04-08 18:58:29.672037: Training Step 299/354: batchLoss = 0.5263, diffLoss = 2.5763, kgLoss = 0.0138
2025-04-08 18:58:31.290829: Training Step 300/354: batchLoss = 0.7973, diffLoss = 3.9145, kgLoss = 0.0180
2025-04-08 18:58:32.901980: Training Step 301/354: batchLoss = 0.5926, diffLoss = 2.8903, kgLoss = 0.0181
2025-04-08 18:58:34.517436: Training Step 302/354: batchLoss = 0.5920, diffLoss = 2.9009, kgLoss = 0.0148
2025-04-08 18:58:36.141230: Training Step 303/354: batchLoss = 0.4695, diffLoss = 2.2976, kgLoss = 0.0125
2025-04-08 18:58:37.759642: Training Step 304/354: batchLoss = 0.4574, diffLoss = 2.2428, kgLoss = 0.0110
2025-04-08 18:58:39.373905: Training Step 305/354: batchLoss = 0.5036, diffLoss = 2.4605, kgLoss = 0.0144
2025-04-08 18:58:40.990955: Training Step 306/354: batchLoss = 0.5204, diffLoss = 2.5410, kgLoss = 0.0152
2025-04-08 18:58:42.609742: Training Step 307/354: batchLoss = 0.4685, diffLoss = 2.2958, kgLoss = 0.0117
2025-04-08 18:58:44.238446: Training Step 308/354: batchLoss = 0.5341, diffLoss = 2.6186, kgLoss = 0.0130
2025-04-08 18:58:45.856132: Training Step 309/354: batchLoss = 0.6836, diffLoss = 3.3565, kgLoss = 0.0154
2025-04-08 18:58:47.473187: Training Step 310/354: batchLoss = 0.4637, diffLoss = 2.2705, kgLoss = 0.0120
2025-04-08 18:58:49.083485: Training Step 311/354: batchLoss = 0.5299, diffLoss = 2.5862, kgLoss = 0.0158
2025-04-08 18:58:50.706110: Training Step 312/354: batchLoss = 0.6569, diffLoss = 3.2125, kgLoss = 0.0180
2025-04-08 18:58:52.322700: Training Step 313/354: batchLoss = 0.5060, diffLoss = 2.4743, kgLoss = 0.0139
2025-04-08 18:58:53.939726: Training Step 314/354: batchLoss = 0.6077, diffLoss = 2.9690, kgLoss = 0.0173
2025-04-08 18:58:55.568986: Training Step 315/354: batchLoss = 0.4760, diffLoss = 2.3343, kgLoss = 0.0115
2025-04-08 18:58:57.191047: Training Step 316/354: batchLoss = 0.5260, diffLoss = 2.5677, kgLoss = 0.0155
2025-04-08 18:58:58.812903: Training Step 317/354: batchLoss = 0.5618, diffLoss = 2.7488, kgLoss = 0.0151
2025-04-08 18:59:00.428833: Training Step 318/354: batchLoss = 0.6307, diffLoss = 3.0882, kgLoss = 0.0163
2025-04-08 18:59:02.043924: Training Step 319/354: batchLoss = 0.5262, diffLoss = 2.5726, kgLoss = 0.0146
2025-04-08 18:59:03.665968: Training Step 320/354: batchLoss = 0.5970, diffLoss = 2.9282, kgLoss = 0.0142
2025-04-08 18:59:05.279711: Training Step 321/354: batchLoss = 0.4879, diffLoss = 2.3891, kgLoss = 0.0126
2025-04-08 18:59:06.897322: Training Step 322/354: batchLoss = 0.5497, diffLoss = 2.6907, kgLoss = 0.0145
2025-04-08 18:59:08.513265: Training Step 323/354: batchLoss = 0.5889, diffLoss = 2.8772, kgLoss = 0.0168
2025-04-08 18:59:10.142016: Training Step 324/354: batchLoss = 0.4756, diffLoss = 2.3260, kgLoss = 0.0130
2025-04-08 18:59:11.760092: Training Step 325/354: batchLoss = 0.6909, diffLoss = 3.3830, kgLoss = 0.0179
2025-04-08 18:59:13.378874: Training Step 326/354: batchLoss = 0.4788, diffLoss = 2.3437, kgLoss = 0.0126
2025-04-08 18:59:15.005454: Training Step 327/354: batchLoss = 0.4694, diffLoss = 2.2912, kgLoss = 0.0140
2025-04-08 18:59:16.618861: Training Step 328/354: batchLoss = 0.5710, diffLoss = 2.7933, kgLoss = 0.0154
2025-04-08 18:59:18.237041: Training Step 329/354: batchLoss = 0.5645, diffLoss = 2.7674, kgLoss = 0.0138
2025-04-08 18:59:19.855552: Training Step 330/354: batchLoss = 0.5832, diffLoss = 2.8528, kgLoss = 0.0158
2025-04-08 18:59:21.470717: Training Step 331/354: batchLoss = 0.5139, diffLoss = 2.5102, kgLoss = 0.0149
2025-04-08 18:59:23.086735: Training Step 332/354: batchLoss = 0.5454, diffLoss = 2.6649, kgLoss = 0.0156
2025-04-08 18:59:24.707832: Training Step 333/354: batchLoss = 0.5256, diffLoss = 2.5729, kgLoss = 0.0138
2025-04-08 18:59:26.327853: Training Step 334/354: batchLoss = 0.5352, diffLoss = 2.6197, kgLoss = 0.0141
2025-04-08 18:59:27.948087: Training Step 335/354: batchLoss = 0.5192, diffLoss = 2.5406, kgLoss = 0.0139
2025-04-08 18:59:29.567078: Training Step 336/354: batchLoss = 0.6218, diffLoss = 3.0491, kgLoss = 0.0149
2025-04-08 18:59:31.192682: Training Step 337/354: batchLoss = 0.6078, diffLoss = 2.9803, kgLoss = 0.0147
2025-04-08 18:59:32.815714: Training Step 338/354: batchLoss = 0.4587, diffLoss = 2.2459, kgLoss = 0.0119
2025-04-08 18:59:34.430694: Training Step 339/354: batchLoss = 0.5248, diffLoss = 2.5708, kgLoss = 0.0133
2025-04-08 18:59:36.043132: Training Step 340/354: batchLoss = 0.5458, diffLoss = 2.6690, kgLoss = 0.0150
2025-04-08 18:59:37.659783: Training Step 341/354: batchLoss = 0.5812, diffLoss = 2.8476, kgLoss = 0.0146
2025-04-08 18:59:39.274793: Training Step 342/354: batchLoss = 1.0711, diffLoss = 5.2402, kgLoss = 0.0288
2025-04-08 18:59:40.896800: Training Step 343/354: batchLoss = 0.7174, diffLoss = 3.5088, kgLoss = 0.0196
2025-04-08 18:59:42.519480: Training Step 344/354: batchLoss = 0.5707, diffLoss = 2.7906, kgLoss = 0.0157
2025-04-08 18:59:44.136205: Training Step 345/354: batchLoss = 0.6686, diffLoss = 3.2782, kgLoss = 0.0162
2025-04-08 18:59:45.754941: Training Step 346/354: batchLoss = 0.5562, diffLoss = 2.7335, kgLoss = 0.0119
2025-04-08 18:59:47.375408: Training Step 347/354: batchLoss = 0.4683, diffLoss = 2.2975, kgLoss = 0.0110
2025-04-08 18:59:48.994445: Training Step 348/354: batchLoss = 0.5306, diffLoss = 2.5939, kgLoss = 0.0148
2025-04-08 18:59:50.612140: Training Step 349/354: batchLoss = 0.5300, diffLoss = 2.5917, kgLoss = 0.0145
2025-04-08 18:59:52.223432: Training Step 350/354: batchLoss = 0.5893, diffLoss = 2.8827, kgLoss = 0.0159
2025-04-08 18:59:53.837032: Training Step 351/354: batchLoss = 0.5092, diffLoss = 2.4690, kgLoss = 0.0192
2025-04-08 18:59:55.429970: Training Step 352/354: batchLoss = 0.5241, diffLoss = 2.5709, kgLoss = 0.0124
2025-04-08 18:59:56.825126: Training Step 353/354: batchLoss = 0.4382, diffLoss = 2.1386, kgLoss = 0.0130
2025-04-08 18:59:56.913953: 
2025-04-08 18:59:56.914649: Epoch 30/1000, Train: epLoss = 0.9844, epDfLoss = 4.8174, epfTransLoss = 0.0000, epKgLoss = 0.0262  
2025-04-08 18:59:58.240059: Steps 0/138: batch_recall = 47.62, batch_ndcg = 26.83 
2025-04-08 18:59:59.559883: Steps 1/138: batch_recall = 48.27, batch_ndcg = 28.11 
2025-04-08 19:00:00.863492: Steps 2/138: batch_recall = 57.55, batch_ndcg = 35.42 
2025-04-08 19:00:02.185425: Steps 3/138: batch_recall = 58.88, batch_ndcg = 34.27 
2025-04-08 19:00:03.492031: Steps 4/138: batch_recall = 67.17, batch_ndcg = 41.02 
2025-04-08 19:00:04.804212: Steps 5/138: batch_recall = 58.30, batch_ndcg = 31.47 
2025-04-08 19:00:06.101441: Steps 6/138: batch_recall = 51.72, batch_ndcg = 31.89 
2025-04-08 19:00:07.404496: Steps 7/138: batch_recall = 63.78, batch_ndcg = 41.93 
2025-04-08 19:00:08.706659: Steps 8/138: batch_recall = 61.82, batch_ndcg = 38.83 
2025-04-08 19:00:10.000345: Steps 9/138: batch_recall = 59.27, batch_ndcg = 35.12 
2025-04-08 19:00:11.297985: Steps 10/138: batch_recall = 58.93, batch_ndcg = 33.35 
2025-04-08 19:00:12.594810: Steps 11/138: batch_recall = 56.82, batch_ndcg = 33.08 
2025-04-08 19:00:13.893651: Steps 12/138: batch_recall = 51.54, batch_ndcg = 29.13 
2025-04-08 19:00:15.187636: Steps 13/138: batch_recall = 54.61, batch_ndcg = 32.11 
2025-04-08 19:00:16.490820: Steps 14/138: batch_recall = 54.84, batch_ndcg = 31.81 
2025-04-08 19:00:17.786420: Steps 15/138: batch_recall = 48.17, batch_ndcg = 30.31 
2025-04-08 19:00:19.088008: Steps 16/138: batch_recall = 60.17, batch_ndcg = 34.05 
2025-04-08 19:00:20.376437: Steps 17/138: batch_recall = 58.52, batch_ndcg = 34.41 
2025-04-08 19:00:21.671001: Steps 18/138: batch_recall = 53.67, batch_ndcg = 33.41 
2025-04-08 19:00:22.963786: Steps 19/138: batch_recall = 54.02, batch_ndcg = 32.58 
2025-04-08 19:00:24.259188: Steps 20/138: batch_recall = 64.12, batch_ndcg = 36.69 
2025-04-08 19:00:25.554703: Steps 21/138: batch_recall = 71.57, batch_ndcg = 41.44 
2025-04-08 19:00:26.841711: Steps 22/138: batch_recall = 56.95, batch_ndcg = 32.82 
2025-04-08 19:00:28.141317: Steps 23/138: batch_recall = 50.60, batch_ndcg = 30.79 
2025-04-08 19:00:29.440836: Steps 24/138: batch_recall = 56.69, batch_ndcg = 31.22 
2025-04-08 19:00:30.746870: Steps 25/138: batch_recall = 60.80, batch_ndcg = 35.01 
2025-04-08 19:00:32.069132: Steps 26/138: batch_recall = 58.05, batch_ndcg = 32.96 
2025-04-08 19:00:33.358132: Steps 27/138: batch_recall = 55.99, batch_ndcg = 32.28 
2025-04-08 19:00:34.654743: Steps 28/138: batch_recall = 60.19, batch_ndcg = 33.95 
2025-04-08 19:00:35.943040: Steps 29/138: batch_recall = 63.08, batch_ndcg = 32.58 
2025-04-08 19:00:37.239230: Steps 30/138: batch_recall = 57.37, batch_ndcg = 33.71 
2025-04-08 19:00:38.529197: Steps 31/138: batch_recall = 49.03, batch_ndcg = 27.78 
2025-04-08 19:00:39.811982: Steps 32/138: batch_recall = 53.78, batch_ndcg = 31.68 
2025-04-08 19:00:41.089083: Steps 33/138: batch_recall = 59.87, batch_ndcg = 33.39 
2025-04-08 19:00:42.375203: Steps 34/138: batch_recall = 56.87, batch_ndcg = 30.84 
2025-04-08 19:00:43.667221: Steps 35/138: batch_recall = 53.47, batch_ndcg = 30.57 
2025-04-08 19:00:44.952549: Steps 36/138: batch_recall = 47.16, batch_ndcg = 27.62 
2025-04-08 19:00:46.232679: Steps 37/138: batch_recall = 58.49, batch_ndcg = 34.44 
2025-04-08 19:00:47.528562: Steps 38/138: batch_recall = 56.76, batch_ndcg = 32.27 
2025-04-08 19:00:48.812550: Steps 39/138: batch_recall = 69.95, batch_ndcg = 39.06 
2025-04-08 19:00:50.106851: Steps 40/138: batch_recall = 61.94, batch_ndcg = 31.04 
2025-04-08 19:00:51.394469: Steps 41/138: batch_recall = 58.76, batch_ndcg = 33.62 
2025-04-08 19:00:52.682913: Steps 42/138: batch_recall = 53.44, batch_ndcg = 30.18 
2025-04-08 19:00:53.963208: Steps 43/138: batch_recall = 56.05, batch_ndcg = 35.81 
2025-04-08 19:00:55.249114: Steps 44/138: batch_recall = 57.69, batch_ndcg = 31.15 
2025-04-08 19:00:56.524074: Steps 45/138: batch_recall = 63.68, batch_ndcg = 36.13 
2025-04-08 19:00:57.800476: Steps 46/138: batch_recall = 62.88, batch_ndcg = 36.58 
2025-04-08 19:00:59.088276: Steps 47/138: batch_recall = 55.20, batch_ndcg = 32.71 
2025-04-08 19:01:00.374420: Steps 48/138: batch_recall = 62.74, batch_ndcg = 35.96 
2025-04-08 19:01:01.655847: Steps 49/138: batch_recall = 67.23, batch_ndcg = 38.19 
2025-04-08 19:01:02.937346: Steps 50/138: batch_recall = 58.17, batch_ndcg = 30.82 
2025-04-08 19:01:04.221715: Steps 51/138: batch_recall = 64.43, batch_ndcg = 36.83 
2025-04-08 19:01:05.506954: Steps 52/138: batch_recall = 64.63, batch_ndcg = 41.65 
2025-04-08 19:01:06.797195: Steps 53/138: batch_recall = 66.15, batch_ndcg = 34.95 
2025-04-08 19:01:08.090091: Steps 54/138: batch_recall = 68.31, batch_ndcg = 38.28 
2025-04-08 19:01:09.369418: Steps 55/138: batch_recall = 60.62, batch_ndcg = 33.88 
2025-04-08 19:01:10.658086: Steps 56/138: batch_recall = 60.97, batch_ndcg = 35.25 
2025-04-08 19:01:11.941212: Steps 57/138: batch_recall = 58.07, batch_ndcg = 32.35 
2025-04-08 19:01:13.222988: Steps 58/138: batch_recall = 70.02, batch_ndcg = 36.45 
2025-04-08 19:01:14.498187: Steps 59/138: batch_recall = 64.26, batch_ndcg = 38.93 
2025-04-08 19:01:15.779029: Steps 60/138: batch_recall = 65.52, batch_ndcg = 37.30 
2025-04-08 19:01:17.063740: Steps 61/138: batch_recall = 63.77, batch_ndcg = 34.64 
2025-04-08 19:01:18.346240: Steps 62/138: batch_recall = 83.63, batch_ndcg = 45.08 
2025-04-08 19:01:19.627008: Steps 63/138: batch_recall = 75.51, batch_ndcg = 42.78 
2025-04-08 19:01:20.912835: Steps 64/138: batch_recall = 60.51, batch_ndcg = 32.04 
2025-04-08 19:01:22.195018: Steps 65/138: batch_recall = 85.88, batch_ndcg = 46.91 
2025-04-08 19:01:23.486034: Steps 66/138: batch_recall = 69.79, batch_ndcg = 41.26 
2025-04-08 19:01:24.768072: Steps 67/138: batch_recall = 77.45, batch_ndcg = 46.48 
2025-04-08 19:01:26.044232: Steps 68/138: batch_recall = 60.75, batch_ndcg = 33.68 
2025-04-08 19:01:27.313428: Steps 69/138: batch_recall = 85.26, batch_ndcg = 51.48 
2025-04-08 19:01:28.591549: Steps 70/138: batch_recall = 79.38, batch_ndcg = 45.32 
2025-04-08 19:01:29.867605: Steps 71/138: batch_recall = 85.95, batch_ndcg = 50.53 
2025-04-08 19:01:31.150278: Steps 72/138: batch_recall = 84.10, batch_ndcg = 49.21 
2025-04-08 19:01:32.427177: Steps 73/138: batch_recall = 82.54, batch_ndcg = 46.60 
2025-04-08 19:01:33.717366: Steps 74/138: batch_recall = 77.13, batch_ndcg = 48.33 
2025-04-08 19:01:35.000368: Steps 75/138: batch_recall = 83.87, batch_ndcg = 49.30 
2025-04-08 19:01:36.288133: Steps 76/138: batch_recall = 94.54, batch_ndcg = 55.80 
2025-04-08 19:01:37.576870: Steps 77/138: batch_recall = 88.78, batch_ndcg = 50.93 
2025-04-08 19:01:38.857811: Steps 78/138: batch_recall = 90.51, batch_ndcg = 48.43 
2025-04-08 19:01:40.154155: Steps 79/138: batch_recall = 87.28, batch_ndcg = 48.84 
2025-04-08 19:01:41.431226: Steps 80/138: batch_recall = 69.49, batch_ndcg = 38.06 
2025-04-08 19:01:42.704272: Steps 81/138: batch_recall = 84.74, batch_ndcg = 49.19 
2025-04-08 19:01:43.982584: Steps 82/138: batch_recall = 87.71, batch_ndcg = 53.97 
2025-04-08 19:01:45.262562: Steps 83/138: batch_recall = 83.34, batch_ndcg = 47.76 
2025-04-08 19:01:46.536011: Steps 84/138: batch_recall = 101.49, batch_ndcg = 58.37 
2025-04-08 19:01:47.808155: Steps 85/138: batch_recall = 106.65, batch_ndcg = 61.80 
2025-04-08 19:01:49.086006: Steps 86/138: batch_recall = 122.01, batch_ndcg = 71.67 
2025-04-08 19:01:50.371574: Steps 87/138: batch_recall = 105.85, batch_ndcg = 55.56 
2025-04-08 19:01:51.660034: Steps 88/138: batch_recall = 103.27, batch_ndcg = 60.01 
2025-04-08 19:01:52.942725: Steps 89/138: batch_recall = 120.65, batch_ndcg = 69.09 
2025-04-08 19:01:54.221771: Steps 90/138: batch_recall = 102.23, batch_ndcg = 57.93 
2025-04-08 19:01:55.500596: Steps 91/138: batch_recall = 115.12, batch_ndcg = 64.25 
2025-04-08 19:01:56.775814: Steps 92/138: batch_recall = 121.41, batch_ndcg = 65.86 
2025-04-08 19:01:58.042904: Steps 93/138: batch_recall = 121.95, batch_ndcg = 69.28 
2025-04-08 19:01:59.309977: Steps 94/138: batch_recall = 125.61, batch_ndcg = 66.95 
2025-04-08 19:02:00.573745: Steps 95/138: batch_recall = 114.77, batch_ndcg = 67.33 
2025-04-08 19:02:01.847803: Steps 96/138: batch_recall = 134.26, batch_ndcg = 80.73 
2025-04-08 19:02:03.118575: Steps 97/138: batch_recall = 151.21, batch_ndcg = 91.90 
2025-04-08 19:02:04.402498: Steps 98/138: batch_recall = 108.84, batch_ndcg = 63.64 
2025-04-08 19:02:05.682242: Steps 99/138: batch_recall = 123.47, batch_ndcg = 71.63 
2025-04-08 19:02:06.956047: Steps 100/138: batch_recall = 129.92, batch_ndcg = 72.75 
2025-04-08 19:02:08.231532: Steps 101/138: batch_recall = 128.34, batch_ndcg = 70.83 
2025-04-08 19:02:09.509956: Steps 102/138: batch_recall = 119.51, batch_ndcg = 70.10 
2025-04-08 19:02:10.784026: Steps 103/138: batch_recall = 139.13, batch_ndcg = 80.39 
2025-04-08 19:02:12.052871: Steps 104/138: batch_recall = 137.34, batch_ndcg = 79.90 
2025-04-08 19:02:13.322422: Steps 105/138: batch_recall = 120.87, batch_ndcg = 67.28 
2025-04-08 19:02:14.589470: Steps 106/138: batch_recall = 105.49, batch_ndcg = 60.15 
2025-04-08 19:02:15.859339: Steps 107/138: batch_recall = 114.26, batch_ndcg = 64.75 
2025-04-08 19:02:17.130387: Steps 108/138: batch_recall = 118.14, batch_ndcg = 72.68 
2025-04-08 19:02:18.393041: Steps 109/138: batch_recall = 137.91, batch_ndcg = 77.37 
2025-04-08 19:02:19.668740: Steps 110/138: batch_recall = 121.82, batch_ndcg = 64.89 
2025-04-08 19:02:20.941230: Steps 111/138: batch_recall = 138.92, batch_ndcg = 84.22 
2025-04-08 19:02:22.217481: Steps 112/138: batch_recall = 165.72, batch_ndcg = 92.49 
2025-04-08 19:02:23.492258: Steps 113/138: batch_recall = 121.95, batch_ndcg = 69.97 
2025-04-08 19:02:24.780985: Steps 114/138: batch_recall = 126.95, batch_ndcg = 73.47 
2025-04-08 19:02:26.069642: Steps 115/138: batch_recall = 123.79, batch_ndcg = 64.30 
2025-04-08 19:02:27.341830: Steps 116/138: batch_recall = 125.27, batch_ndcg = 67.52 
2025-04-08 19:02:28.615292: Steps 117/138: batch_recall = 112.31, batch_ndcg = 66.13 
2025-04-08 19:02:29.881753: Steps 118/138: batch_recall = 123.05, batch_ndcg = 68.11 
2025-04-08 19:02:31.145533: Steps 119/138: batch_recall = 141.91, batch_ndcg = 76.30 
2025-04-08 19:02:32.417907: Steps 120/138: batch_recall = 126.72, batch_ndcg = 71.71 
2025-04-08 19:02:33.684542: Steps 121/138: batch_recall = 150.90, batch_ndcg = 79.11 
2025-04-08 19:02:34.952214: Steps 122/138: batch_recall = 150.33, batch_ndcg = 81.16 
2025-04-08 19:02:36.223561: Steps 123/138: batch_recall = 131.59, batch_ndcg = 74.71 
2025-04-08 19:02:37.501222: Steps 124/138: batch_recall = 152.91, batch_ndcg = 92.92 
2025-04-08 19:02:38.776171: Steps 125/138: batch_recall = 135.97, batch_ndcg = 74.85 
2025-04-08 19:02:40.042357: Steps 126/138: batch_recall = 163.27, batch_ndcg = 89.52 
2025-04-08 19:02:41.307517: Steps 127/138: batch_recall = 146.17, batch_ndcg = 81.87 
2025-04-08 19:02:42.576342: Steps 128/138: batch_recall = 128.86, batch_ndcg = 72.58 
2025-04-08 19:02:43.842694: Steps 129/138: batch_recall = 161.67, batch_ndcg = 92.86 
2025-04-08 19:02:45.102479: Steps 130/138: batch_recall = 134.52, batch_ndcg = 70.43 
2025-04-08 19:02:46.363134: Steps 131/138: batch_recall = 152.05, batch_ndcg = 89.27 
2025-04-08 19:02:47.627680: Steps 132/138: batch_recall = 149.54, batch_ndcg = 84.33 
2025-04-08 19:02:48.894762: Steps 133/138: batch_recall = 145.43, batch_ndcg = 85.59 
2025-04-08 19:02:50.159980: Steps 134/138: batch_recall = 142.21, batch_ndcg = 83.05 
2025-04-08 19:02:51.423801: Steps 135/138: batch_recall = 168.69, batch_ndcg = 95.93 
2025-04-08 19:02:52.692294: Steps 136/138: batch_recall = 152.89, batch_ndcg = 79.71 
2025-04-08 19:02:53.966122: Steps 137/138: batch_recall = 137.09, batch_ndcg = 86.14 
2025-04-08 19:02:53.966656: Epoch 30/1000, Test: Recall = 0.1770, NDCG = 0.1010  

2025-04-08 19:02:55.733820: Training Step 0/354: batchLoss = 0.7081, diffLoss = 3.4717, kgLoss = 0.0172
2025-04-08 19:02:57.347276: Training Step 1/354: batchLoss = 0.5599, diffLoss = 2.7428, kgLoss = 0.0141
2025-04-08 19:02:58.967277: Training Step 2/354: batchLoss = 0.5062, diffLoss = 2.4789, kgLoss = 0.0131
2025-04-08 19:03:00.582310: Training Step 3/354: batchLoss = 0.5741, diffLoss = 2.8181, kgLoss = 0.0131
2025-04-08 19:03:02.195499: Training Step 4/354: batchLoss = 0.5403, diffLoss = 2.6429, kgLoss = 0.0146
2025-04-08 19:03:03.809134: Training Step 5/354: batchLoss = 0.5527, diffLoss = 2.7007, kgLoss = 0.0157
2025-04-08 19:03:05.425378: Training Step 6/354: batchLoss = 0.4914, diffLoss = 2.3999, kgLoss = 0.0143
2025-04-08 19:03:07.040412: Training Step 7/354: batchLoss = 0.4808, diffLoss = 2.3534, kgLoss = 0.0126
2025-04-08 19:03:08.661867: Training Step 8/354: batchLoss = 0.6128, diffLoss = 3.0005, kgLoss = 0.0159
2025-04-08 19:03:10.277996: Training Step 9/354: batchLoss = 0.5033, diffLoss = 2.4604, kgLoss = 0.0140
2025-04-08 19:03:11.891627: Training Step 10/354: batchLoss = 0.4771, diffLoss = 2.3330, kgLoss = 0.0131
2025-04-08 19:03:13.508034: Training Step 11/354: batchLoss = 0.4582, diffLoss = 2.2467, kgLoss = 0.0111
2025-04-08 19:03:15.126641: Training Step 12/354: batchLoss = 1.0055, diffLoss = 4.9138, kgLoss = 0.0285
2025-04-08 19:03:16.742624: Training Step 13/354: batchLoss = 0.5395, diffLoss = 2.6409, kgLoss = 0.0141
2025-04-08 19:03:18.360668: Training Step 14/354: batchLoss = 0.5207, diffLoss = 2.5494, kgLoss = 0.0136
2025-04-08 19:03:19.975187: Training Step 15/354: batchLoss = 0.4749, diffLoss = 2.3155, kgLoss = 0.0148
2025-04-08 19:03:21.589499: Training Step 16/354: batchLoss = 0.4605, diffLoss = 2.2494, kgLoss = 0.0132
2025-04-08 19:03:23.198697: Training Step 17/354: batchLoss = 0.5948, diffLoss = 2.9103, kgLoss = 0.0159
2025-04-08 19:03:24.816678: Training Step 18/354: batchLoss = 0.6752, diffLoss = 3.3084, kgLoss = 0.0169
2025-04-08 19:03:26.436406: Training Step 19/354: batchLoss = 0.6050, diffLoss = 2.9617, kgLoss = 0.0158
2025-04-08 19:03:28.056736: Training Step 20/354: batchLoss = 0.5533, diffLoss = 2.7069, kgLoss = 0.0149
2025-04-08 19:03:29.673024: Training Step 21/354: batchLoss = 0.5610, diffLoss = 2.7458, kgLoss = 0.0148
2025-04-08 19:03:31.287024: Training Step 22/354: batchLoss = 0.5804, diffLoss = 2.8431, kgLoss = 0.0147
2025-04-08 19:03:32.907772: Training Step 23/354: batchLoss = 0.6010, diffLoss = 2.9488, kgLoss = 0.0140
2025-04-08 19:03:34.519980: Training Step 24/354: batchLoss = 0.6480, diffLoss = 3.1752, kgLoss = 0.0162
2025-04-08 19:03:36.136363: Training Step 25/354: batchLoss = 0.6424, diffLoss = 3.1510, kgLoss = 0.0153
2025-04-08 19:03:37.751548: Training Step 26/354: batchLoss = 0.5300, diffLoss = 2.5953, kgLoss = 0.0137
2025-04-08 19:03:39.367887: Training Step 27/354: batchLoss = 0.4054, diffLoss = 1.9795, kgLoss = 0.0118
2025-04-08 19:03:40.988631: Training Step 28/354: batchLoss = 0.4478, diffLoss = 2.1884, kgLoss = 0.0126
2025-04-08 19:03:42.606947: Training Step 29/354: batchLoss = 0.5674, diffLoss = 2.7833, kgLoss = 0.0134
2025-04-08 19:03:44.226811: Training Step 30/354: batchLoss = 0.5495, diffLoss = 2.6897, kgLoss = 0.0145
2025-04-08 19:03:45.856440: Training Step 31/354: batchLoss = 0.4930, diffLoss = 2.4087, kgLoss = 0.0141
2025-04-08 19:03:47.469591: Training Step 32/354: batchLoss = 0.4829, diffLoss = 2.3640, kgLoss = 0.0126
2025-04-08 19:03:49.085987: Training Step 33/354: batchLoss = 0.5531, diffLoss = 2.7086, kgLoss = 0.0143
2025-04-08 19:03:50.702038: Training Step 34/354: batchLoss = 0.6197, diffLoss = 3.0364, kgLoss = 0.0156
2025-04-08 19:03:52.325412: Training Step 35/354: batchLoss = 0.5230, diffLoss = 2.5630, kgLoss = 0.0130
2025-04-08 19:03:53.942196: Training Step 36/354: batchLoss = 0.5899, diffLoss = 2.8888, kgLoss = 0.0152
2025-04-08 19:03:55.558362: Training Step 37/354: batchLoss = 0.5251, diffLoss = 2.5717, kgLoss = 0.0135
2025-04-08 19:03:57.179240: Training Step 38/354: batchLoss = 0.5681, diffLoss = 2.7774, kgLoss = 0.0157
2025-04-08 19:03:58.804496: Training Step 39/354: batchLoss = 0.4440, diffLoss = 2.1715, kgLoss = 0.0121
2025-04-08 19:04:00.424543: Training Step 40/354: batchLoss = 0.4794, diffLoss = 2.3427, kgLoss = 0.0136
2025-04-08 19:04:02.039328: Training Step 41/354: batchLoss = 0.5090, diffLoss = 2.4814, kgLoss = 0.0159
2025-04-08 19:04:03.656411: Training Step 42/354: batchLoss = 0.4913, diffLoss = 2.4077, kgLoss = 0.0122
2025-04-08 19:04:05.273861: Training Step 43/354: batchLoss = 0.5506, diffLoss = 2.6983, kgLoss = 0.0137
2025-04-08 19:04:06.892726: Training Step 44/354: batchLoss = 0.5321, diffLoss = 2.6101, kgLoss = 0.0125
2025-04-08 19:04:08.503188: Training Step 45/354: batchLoss = 0.4849, diffLoss = 2.3686, kgLoss = 0.0140
2025-04-08 19:04:10.121527: Training Step 46/354: batchLoss = 0.5384, diffLoss = 2.6357, kgLoss = 0.0140
2025-04-08 19:04:11.736459: Training Step 47/354: batchLoss = 0.5311, diffLoss = 2.5975, kgLoss = 0.0145
2025-04-08 19:04:13.353005: Training Step 48/354: batchLoss = 0.5775, diffLoss = 2.8323, kgLoss = 0.0139
2025-04-08 19:04:14.971342: Training Step 49/354: batchLoss = 0.4154, diffLoss = 2.0260, kgLoss = 0.0128
2025-04-08 19:04:16.587130: Training Step 50/354: batchLoss = 0.6004, diffLoss = 2.9381, kgLoss = 0.0160
2025-04-08 19:04:18.202274: Training Step 51/354: batchLoss = 0.5077, diffLoss = 2.4840, kgLoss = 0.0136
2025-04-08 19:04:19.814385: Training Step 52/354: batchLoss = 0.4931, diffLoss = 2.4090, kgLoss = 0.0141
2025-04-08 19:04:21.434747: Training Step 53/354: batchLoss = 0.7592, diffLoss = 3.7119, kgLoss = 0.0210
2025-04-08 19:04:23.052443: Training Step 54/354: batchLoss = 0.4618, diffLoss = 2.2594, kgLoss = 0.0124
2025-04-08 19:04:24.670919: Training Step 55/354: batchLoss = 0.4691, diffLoss = 2.2957, kgLoss = 0.0125
2025-04-08 19:04:26.285378: Training Step 56/354: batchLoss = 0.4674, diffLoss = 2.2833, kgLoss = 0.0135
2025-04-08 19:04:27.902858: Training Step 57/354: batchLoss = 0.6125, diffLoss = 2.9986, kgLoss = 0.0160
2025-04-08 19:04:29.519181: Training Step 58/354: batchLoss = 0.6698, diffLoss = 3.2773, kgLoss = 0.0179
2025-04-08 19:04:31.134041: Training Step 59/354: batchLoss = 0.7050, diffLoss = 3.4522, kgLoss = 0.0182
2025-04-08 19:04:32.745798: Training Step 60/354: batchLoss = 0.4859, diffLoss = 2.3807, kgLoss = 0.0122
2025-04-08 19:04:34.361366: Training Step 61/354: batchLoss = 0.5727, diffLoss = 2.8027, kgLoss = 0.0152
2025-04-08 19:04:35.978674: Training Step 62/354: batchLoss = 1.0003, diffLoss = 4.9004, kgLoss = 0.0253
2025-04-08 19:04:37.590733: Training Step 63/354: batchLoss = 0.4977, diffLoss = 2.4395, kgLoss = 0.0122
2025-04-08 19:04:39.208645: Training Step 64/354: batchLoss = 0.5334, diffLoss = 2.6068, kgLoss = 0.0151
2025-04-08 19:04:40.823095: Training Step 65/354: batchLoss = 0.5795, diffLoss = 2.8355, kgLoss = 0.0156
2025-04-08 19:04:42.434053: Training Step 66/354: batchLoss = 0.5515, diffLoss = 2.6991, kgLoss = 0.0146
2025-04-08 19:04:44.049338: Training Step 67/354: batchLoss = 0.5409, diffLoss = 2.6441, kgLoss = 0.0150
2025-04-08 19:04:45.674594: Training Step 68/354: batchLoss = 0.6093, diffLoss = 2.9816, kgLoss = 0.0163
2025-04-08 19:04:47.292555: Training Step 69/354: batchLoss = 0.5415, diffLoss = 2.6534, kgLoss = 0.0135
2025-04-08 19:04:48.910401: Training Step 70/354: batchLoss = 0.4611, diffLoss = 2.2514, kgLoss = 0.0135
2025-04-08 19:04:50.526308: Training Step 71/354: batchLoss = 0.5530, diffLoss = 2.7096, kgLoss = 0.0138
2025-04-08 19:04:52.145427: Training Step 72/354: batchLoss = 0.6687, diffLoss = 3.2719, kgLoss = 0.0179
2025-04-08 19:04:53.769596: Training Step 73/354: batchLoss = 0.4884, diffLoss = 2.3918, kgLoss = 0.0125
2025-04-08 19:04:55.387239: Training Step 74/354: batchLoss = 0.5305, diffLoss = 2.5960, kgLoss = 0.0141
2025-04-08 19:04:57.001440: Training Step 75/354: batchLoss = 0.5444, diffLoss = 2.6663, kgLoss = 0.0139
2025-04-08 19:04:58.617490: Training Step 76/354: batchLoss = 0.3831, diffLoss = 1.8633, kgLoss = 0.0131
2025-04-08 19:05:00.239851: Training Step 77/354: batchLoss = 0.5447, diffLoss = 2.6649, kgLoss = 0.0146
2025-04-08 19:05:01.855719: Training Step 78/354: batchLoss = 0.4933, diffLoss = 2.4133, kgLoss = 0.0132
2025-04-08 19:05:03.474332: Training Step 79/354: batchLoss = 0.5286, diffLoss = 2.5864, kgLoss = 0.0142
2025-04-08 19:05:05.092380: Training Step 80/354: batchLoss = 0.4910, diffLoss = 2.4051, kgLoss = 0.0124
2025-04-08 19:05:06.704713: Training Step 81/354: batchLoss = 0.6345, diffLoss = 3.0992, kgLoss = 0.0183
2025-04-08 19:05:08.325046: Training Step 82/354: batchLoss = 0.4877, diffLoss = 2.3853, kgLoss = 0.0133
2025-04-08 19:05:09.943071: Training Step 83/354: batchLoss = 0.4308, diffLoss = 2.1108, kgLoss = 0.0108
2025-04-08 19:05:11.560983: Training Step 84/354: batchLoss = 0.5752, diffLoss = 2.8113, kgLoss = 0.0162
2025-04-08 19:05:13.180323: Training Step 85/354: batchLoss = 0.4856, diffLoss = 2.3693, kgLoss = 0.0147
2025-04-08 19:05:14.797389: Training Step 86/354: batchLoss = 0.5206, diffLoss = 2.5513, kgLoss = 0.0129
2025-04-08 19:05:16.419357: Training Step 87/354: batchLoss = 0.5690, diffLoss = 2.7856, kgLoss = 0.0148
2025-04-08 19:05:18.036007: Training Step 88/354: batchLoss = 0.6309, diffLoss = 3.0986, kgLoss = 0.0140
2025-04-08 19:05:19.651888: Training Step 89/354: batchLoss = 0.5252, diffLoss = 2.5743, kgLoss = 0.0129
2025-04-08 19:05:21.274672: Training Step 90/354: batchLoss = 0.5818, diffLoss = 2.8483, kgLoss = 0.0152
2025-04-08 19:05:22.897620: Training Step 91/354: batchLoss = 0.5213, diffLoss = 2.5518, kgLoss = 0.0136
2025-04-08 19:05:24.518244: Training Step 92/354: batchLoss = 0.5715, diffLoss = 2.7932, kgLoss = 0.0161
2025-04-08 19:05:26.134153: Training Step 93/354: batchLoss = 0.5002, diffLoss = 2.4468, kgLoss = 0.0136
2025-04-08 19:05:27.755047: Training Step 94/354: batchLoss = 0.6228, diffLoss = 3.0512, kgLoss = 0.0157
2025-04-08 19:05:29.374563: Training Step 95/354: batchLoss = 0.5056, diffLoss = 2.4742, kgLoss = 0.0135
2025-04-08 19:05:30.993366: Training Step 96/354: batchLoss = 0.5557, diffLoss = 2.7202, kgLoss = 0.0146
2025-04-08 19:05:32.612344: Training Step 97/354: batchLoss = 0.5615, diffLoss = 2.7455, kgLoss = 0.0155
2025-04-08 19:05:34.229933: Training Step 98/354: batchLoss = 0.6030, diffLoss = 2.9550, kgLoss = 0.0150
2025-04-08 19:05:35.850765: Training Step 99/354: batchLoss = 0.5094, diffLoss = 2.4970, kgLoss = 0.0125
2025-04-08 19:05:37.479133: Training Step 100/354: batchLoss = 0.5395, diffLoss = 2.6440, kgLoss = 0.0133
2025-04-08 19:05:39.099154: Training Step 101/354: batchLoss = 0.5275, diffLoss = 2.5828, kgLoss = 0.0137
2025-04-08 19:05:40.722046: Training Step 102/354: batchLoss = 0.5448, diffLoss = 2.6602, kgLoss = 0.0159
2025-04-08 19:05:42.338334: Training Step 103/354: batchLoss = 0.5725, diffLoss = 2.8012, kgLoss = 0.0154
2025-04-08 19:05:43.958546: Training Step 104/354: batchLoss = 0.6823, diffLoss = 3.3404, kgLoss = 0.0178
2025-04-08 19:05:45.577439: Training Step 105/354: batchLoss = 0.5261, diffLoss = 2.5745, kgLoss = 0.0140
2025-04-08 19:05:47.194838: Training Step 106/354: batchLoss = 0.4872, diffLoss = 2.3802, kgLoss = 0.0140
2025-04-08 19:05:48.824262: Training Step 107/354: batchLoss = 0.3852, diffLoss = 1.8761, kgLoss = 0.0124
2025-04-08 19:05:50.451462: Training Step 108/354: batchLoss = 0.5556, diffLoss = 2.7151, kgLoss = 0.0157
2025-04-08 19:05:52.081440: Training Step 109/354: batchLoss = 0.5339, diffLoss = 2.6126, kgLoss = 0.0143
2025-04-08 19:05:53.706771: Training Step 110/354: batchLoss = 0.6081, diffLoss = 2.9829, kgLoss = 0.0143
2025-04-08 19:05:55.324606: Training Step 111/354: batchLoss = 0.5153, diffLoss = 2.5183, kgLoss = 0.0145
2025-04-08 19:05:56.951870: Training Step 112/354: batchLoss = 0.5065, diffLoss = 2.4788, kgLoss = 0.0134
2025-04-08 19:05:58.566648: Training Step 113/354: batchLoss = 0.5007, diffLoss = 2.4537, kgLoss = 0.0124
2025-04-08 19:06:00.186176: Training Step 114/354: batchLoss = 0.5069, diffLoss = 2.4823, kgLoss = 0.0130
2025-04-08 19:06:01.801109: Training Step 115/354: batchLoss = 0.5122, diffLoss = 2.5070, kgLoss = 0.0135
2025-04-08 19:06:03.418640: Training Step 116/354: batchLoss = 0.4835, diffLoss = 2.3651, kgLoss = 0.0131
2025-04-08 19:06:05.036258: Training Step 117/354: batchLoss = 0.6510, diffLoss = 3.1894, kgLoss = 0.0164
2025-04-08 19:06:06.655209: Training Step 118/354: batchLoss = 0.4938, diffLoss = 2.4198, kgLoss = 0.0123
2025-04-08 19:06:08.272504: Training Step 119/354: batchLoss = 0.4593, diffLoss = 2.2507, kgLoss = 0.0115
2025-04-08 19:06:09.898759: Training Step 120/354: batchLoss = 0.6212, diffLoss = 3.0409, kgLoss = 0.0163
2025-04-08 19:06:11.523696: Training Step 121/354: batchLoss = 0.7016, diffLoss = 3.4434, kgLoss = 0.0161
2025-04-08 19:06:13.148964: Training Step 122/354: batchLoss = 0.4896, diffLoss = 2.3927, kgLoss = 0.0138
2025-04-08 19:06:14.761099: Training Step 123/354: batchLoss = 0.6439, diffLoss = 3.1458, kgLoss = 0.0184
2025-04-08 19:06:16.375624: Training Step 124/354: batchLoss = 0.4763, diffLoss = 2.3319, kgLoss = 0.0124
2025-04-08 19:06:17.993646: Training Step 125/354: batchLoss = 0.5077, diffLoss = 2.4795, kgLoss = 0.0147
2025-04-08 19:06:19.612260: Training Step 126/354: batchLoss = 0.5900, diffLoss = 2.8867, kgLoss = 0.0158
2025-04-08 19:06:21.228673: Training Step 127/354: batchLoss = 0.5201, diffLoss = 2.5523, kgLoss = 0.0120
2025-04-08 19:06:22.847533: Training Step 128/354: batchLoss = 0.6499, diffLoss = 3.1836, kgLoss = 0.0165
2025-04-08 19:06:24.465437: Training Step 129/354: batchLoss = 0.6958, diffLoss = 3.4127, kgLoss = 0.0166
2025-04-08 19:06:26.088086: Training Step 130/354: batchLoss = 0.6117, diffLoss = 2.9994, kgLoss = 0.0148
2025-04-08 19:06:27.714711: Training Step 131/354: batchLoss = 0.5318, diffLoss = 2.5983, kgLoss = 0.0152
2025-04-08 19:06:29.330676: Training Step 132/354: batchLoss = 0.6677, diffLoss = 3.2709, kgLoss = 0.0168
2025-04-08 19:06:30.946477: Training Step 133/354: batchLoss = 0.4558, diffLoss = 2.2357, kgLoss = 0.0109
2025-04-08 19:06:32.562648: Training Step 134/354: batchLoss = 0.5507, diffLoss = 2.6949, kgLoss = 0.0147
2025-04-08 19:06:34.178478: Training Step 135/354: batchLoss = 0.5309, diffLoss = 2.5991, kgLoss = 0.0139
2025-04-08 19:06:35.794489: Training Step 136/354: batchLoss = 0.6560, diffLoss = 3.2176, kgLoss = 0.0156
2025-04-08 19:06:37.409332: Training Step 137/354: batchLoss = 0.6208, diffLoss = 3.0389, kgLoss = 0.0162
2025-04-08 19:06:39.031329: Training Step 138/354: batchLoss = 0.4871, diffLoss = 2.3824, kgLoss = 0.0133
2025-04-08 19:06:40.655739: Training Step 139/354: batchLoss = 0.5648, diffLoss = 2.7695, kgLoss = 0.0136
2025-04-08 19:06:42.275488: Training Step 140/354: batchLoss = 0.5173, diffLoss = 2.5290, kgLoss = 0.0143
2025-04-08 19:06:43.887551: Training Step 141/354: batchLoss = 0.5641, diffLoss = 2.7614, kgLoss = 0.0148
2025-04-08 19:06:45.500749: Training Step 142/354: batchLoss = 0.5247, diffLoss = 2.5744, kgLoss = 0.0123
2025-04-08 19:06:47.113235: Training Step 143/354: batchLoss = 0.6151, diffLoss = 3.0150, kgLoss = 0.0151
2025-04-08 19:06:48.725460: Training Step 144/354: batchLoss = 0.5685, diffLoss = 2.7705, kgLoss = 0.0180
2025-04-08 19:06:50.340616: Training Step 145/354: batchLoss = 0.5049, diffLoss = 2.4779, kgLoss = 0.0116
2025-04-08 19:06:51.960071: Training Step 146/354: batchLoss = 0.7304, diffLoss = 3.5783, kgLoss = 0.0184
2025-04-08 19:06:53.584119: Training Step 147/354: batchLoss = 0.6402, diffLoss = 3.1330, kgLoss = 0.0170
2025-04-08 19:06:55.211385: Training Step 148/354: batchLoss = 0.5869, diffLoss = 2.8846, kgLoss = 0.0125
2025-04-08 19:06:56.835566: Training Step 149/354: batchLoss = 0.5411, diffLoss = 2.6516, kgLoss = 0.0134
2025-04-08 19:06:58.460706: Training Step 150/354: batchLoss = 0.5309, diffLoss = 2.5947, kgLoss = 0.0149
2025-04-08 19:07:00.082296: Training Step 151/354: batchLoss = 0.5052, diffLoss = 2.4749, kgLoss = 0.0128
2025-04-08 19:07:01.699724: Training Step 152/354: batchLoss = 0.5586, diffLoss = 2.7324, kgLoss = 0.0152
2025-04-08 19:07:03.322913: Training Step 153/354: batchLoss = 0.5960, diffLoss = 2.9187, kgLoss = 0.0154
2025-04-08 19:07:04.937471: Training Step 154/354: batchLoss = 0.6090, diffLoss = 2.9793, kgLoss = 0.0164
2025-04-08 19:07:06.552822: Training Step 155/354: batchLoss = 0.4847, diffLoss = 2.3688, kgLoss = 0.0136
2025-04-08 19:07:08.172098: Training Step 156/354: batchLoss = 0.5495, diffLoss = 2.6989, kgLoss = 0.0121
2025-04-08 19:07:09.785977: Training Step 157/354: batchLoss = 0.6897, diffLoss = 3.3778, kgLoss = 0.0177
2025-04-08 19:07:11.403439: Training Step 158/354: batchLoss = 0.4958, diffLoss = 2.4251, kgLoss = 0.0134
2025-04-08 19:07:13.018911: Training Step 159/354: batchLoss = 0.5498, diffLoss = 2.6823, kgLoss = 0.0167
2025-04-08 19:07:14.636376: Training Step 160/354: batchLoss = 0.5366, diffLoss = 2.6273, kgLoss = 0.0139
2025-04-08 19:07:16.250299: Training Step 161/354: batchLoss = 0.6343, diffLoss = 3.0965, kgLoss = 0.0187
2025-04-08 19:07:17.864381: Training Step 162/354: batchLoss = 0.5633, diffLoss = 2.7561, kgLoss = 0.0152
2025-04-08 19:07:19.483667: Training Step 163/354: batchLoss = 0.5745, diffLoss = 2.8172, kgLoss = 0.0138
2025-04-08 19:07:21.107291: Training Step 164/354: batchLoss = 0.5945, diffLoss = 2.9090, kgLoss = 0.0159
2025-04-08 19:07:22.722153: Training Step 165/354: batchLoss = 0.7464, diffLoss = 3.6537, kgLoss = 0.0195
2025-04-08 19:07:24.340345: Training Step 166/354: batchLoss = 0.6094, diffLoss = 2.9806, kgLoss = 0.0166
2025-04-08 19:07:25.953920: Training Step 167/354: batchLoss = 0.5967, diffLoss = 2.9240, kgLoss = 0.0149
2025-04-08 19:07:27.571696: Training Step 168/354: batchLoss = 0.6685, diffLoss = 3.2752, kgLoss = 0.0168
2025-04-08 19:07:29.197017: Training Step 169/354: batchLoss = 0.4409, diffLoss = 2.1524, kgLoss = 0.0130
2025-04-08 19:07:30.813771: Training Step 170/354: batchLoss = 0.4930, diffLoss = 2.4042, kgLoss = 0.0152
2025-04-08 19:07:32.429942: Training Step 171/354: batchLoss = 0.5318, diffLoss = 2.6067, kgLoss = 0.0131
2025-04-08 19:07:34.044853: Training Step 172/354: batchLoss = 0.5444, diffLoss = 2.6677, kgLoss = 0.0136
2025-04-08 19:07:35.662250: Training Step 173/354: batchLoss = 0.5056, diffLoss = 2.4762, kgLoss = 0.0130
2025-04-08 19:07:37.274343: Training Step 174/354: batchLoss = 0.4918, diffLoss = 2.4034, kgLoss = 0.0140
2025-04-08 19:07:38.889205: Training Step 175/354: batchLoss = 0.5613, diffLoss = 2.7495, kgLoss = 0.0143
2025-04-08 19:07:40.505000: Training Step 176/354: batchLoss = 0.5207, diffLoss = 2.5443, kgLoss = 0.0148
2025-04-08 19:07:42.114555: Training Step 177/354: batchLoss = 0.5994, diffLoss = 2.9408, kgLoss = 0.0140
2025-04-08 19:07:43.731904: Training Step 178/354: batchLoss = 0.5592, diffLoss = 2.7259, kgLoss = 0.0176
2025-04-08 19:07:45.349923: Training Step 179/354: batchLoss = 0.6851, diffLoss = 3.3568, kgLoss = 0.0172
2025-04-08 19:07:46.973569: Training Step 180/354: batchLoss = 0.4901, diffLoss = 2.3994, kgLoss = 0.0128
2025-04-08 19:07:48.593626: Training Step 181/354: batchLoss = 0.5362, diffLoss = 2.6165, kgLoss = 0.0162
2025-04-08 19:07:50.209715: Training Step 182/354: batchLoss = 0.6039, diffLoss = 2.9562, kgLoss = 0.0158
2025-04-08 19:07:51.823364: Training Step 183/354: batchLoss = 0.6189, diffLoss = 3.0292, kgLoss = 0.0163
2025-04-08 19:07:53.437771: Training Step 184/354: batchLoss = 0.5083, diffLoss = 2.4830, kgLoss = 0.0147
2025-04-08 19:07:55.063232: Training Step 185/354: batchLoss = 0.6628, diffLoss = 3.2513, kgLoss = 0.0157
2025-04-08 19:07:56.680828: Training Step 186/354: batchLoss = 0.5518, diffLoss = 2.6995, kgLoss = 0.0149
2025-04-08 19:07:58.300069: Training Step 187/354: batchLoss = 0.4630, diffLoss = 2.2650, kgLoss = 0.0125
2025-04-08 19:07:59.920246: Training Step 188/354: batchLoss = 0.5031, diffLoss = 2.4623, kgLoss = 0.0133
2025-04-08 19:08:01.538651: Training Step 189/354: batchLoss = 0.5841, diffLoss = 2.8628, kgLoss = 0.0144
2025-04-08 19:08:03.159007: Training Step 190/354: batchLoss = 0.6064, diffLoss = 2.9658, kgLoss = 0.0165
2025-04-08 19:08:04.776472: Training Step 191/354: batchLoss = 0.5138, diffLoss = 2.5071, kgLoss = 0.0154
2025-04-08 19:08:06.392072: Training Step 192/354: batchLoss = 0.5447, diffLoss = 2.6644, kgLoss = 0.0147
2025-04-08 19:08:08.005165: Training Step 193/354: batchLoss = 0.6209, diffLoss = 3.0376, kgLoss = 0.0168
2025-04-08 19:08:09.618753: Training Step 194/354: batchLoss = 0.5490, diffLoss = 2.6874, kgLoss = 0.0144
2025-04-08 19:08:11.231855: Training Step 195/354: batchLoss = 0.6069, diffLoss = 2.9765, kgLoss = 0.0145
2025-04-08 19:08:12.848869: Training Step 196/354: batchLoss = 0.4351, diffLoss = 2.1337, kgLoss = 0.0104
2025-04-08 19:08:14.464406: Training Step 197/354: batchLoss = 0.6245, diffLoss = 3.0584, kgLoss = 0.0161
2025-04-08 19:08:16.086662: Training Step 198/354: batchLoss = 0.5967, diffLoss = 2.9327, kgLoss = 0.0127
2025-04-08 19:08:17.701103: Training Step 199/354: batchLoss = 0.5694, diffLoss = 2.7859, kgLoss = 0.0153
2025-04-08 19:08:19.315167: Training Step 200/354: batchLoss = 0.5981, diffLoss = 2.9328, kgLoss = 0.0144
2025-04-08 19:08:20.933080: Training Step 201/354: batchLoss = 0.6036, diffLoss = 2.9636, kgLoss = 0.0137
2025-04-08 19:08:22.547401: Training Step 202/354: batchLoss = 0.4907, diffLoss = 2.3961, kgLoss = 0.0143
2025-04-08 19:08:24.152552: Training Step 203/354: batchLoss = 0.5455, diffLoss = 2.6680, kgLoss = 0.0149
2025-04-08 19:08:25.769023: Training Step 204/354: batchLoss = 0.4681, diffLoss = 2.2987, kgLoss = 0.0104
2025-04-08 19:08:27.388329: Training Step 205/354: batchLoss = 0.5221, diffLoss = 2.5525, kgLoss = 0.0146
2025-04-08 19:08:29.007462: Training Step 206/354: batchLoss = 0.5009, diffLoss = 2.4514, kgLoss = 0.0132
2025-04-08 19:08:30.629566: Training Step 207/354: batchLoss = 0.5262, diffLoss = 2.5633, kgLoss = 0.0169
2025-04-08 19:08:32.247243: Training Step 208/354: batchLoss = 0.5214, diffLoss = 2.5563, kgLoss = 0.0126
2025-04-08 19:08:33.864555: Training Step 209/354: batchLoss = 0.5023, diffLoss = 2.4531, kgLoss = 0.0146
2025-04-08 19:08:35.525779: Training Step 210/354: batchLoss = 0.5388, diffLoss = 2.6429, kgLoss = 0.0128
2025-04-08 19:08:37.144846: Training Step 211/354: batchLoss = 0.6163, diffLoss = 3.0252, kgLoss = 0.0140
2025-04-08 19:08:38.761008: Training Step 212/354: batchLoss = 0.5277, diffLoss = 2.5838, kgLoss = 0.0136
2025-04-08 19:08:40.368637: Training Step 213/354: batchLoss = 0.4990, diffLoss = 2.4420, kgLoss = 0.0133
2025-04-08 19:08:41.982853: Training Step 214/354: batchLoss = 0.4486, diffLoss = 2.1947, kgLoss = 0.0121
2025-04-08 19:08:43.600120: Training Step 215/354: batchLoss = 0.4546, diffLoss = 2.2230, kgLoss = 0.0125
2025-04-08 19:08:45.220942: Training Step 216/354: batchLoss = 0.5427, diffLoss = 2.6583, kgLoss = 0.0138
2025-04-08 19:08:46.842008: Training Step 217/354: batchLoss = 0.3914, diffLoss = 1.9128, kgLoss = 0.0111
2025-04-08 19:08:48.461351: Training Step 218/354: batchLoss = 0.6838, diffLoss = 3.3474, kgLoss = 0.0178
2025-04-08 19:08:50.091737: Training Step 219/354: batchLoss = 0.7218, diffLoss = 3.5370, kgLoss = 0.0180
2025-04-08 19:08:51.707380: Training Step 220/354: batchLoss = 0.4942, diffLoss = 2.4218, kgLoss = 0.0123
2025-04-08 19:08:53.328115: Training Step 221/354: batchLoss = 0.5130, diffLoss = 2.5128, kgLoss = 0.0130
2025-04-08 19:08:54.943471: Training Step 222/354: batchLoss = 0.5631, diffLoss = 2.7574, kgLoss = 0.0146
2025-04-08 19:08:56.556985: Training Step 223/354: batchLoss = 0.5237, diffLoss = 2.5624, kgLoss = 0.0140
2025-04-08 19:08:58.175488: Training Step 224/354: batchLoss = 0.5897, diffLoss = 2.8908, kgLoss = 0.0144
2025-04-08 19:08:59.793681: Training Step 225/354: batchLoss = 0.7213, diffLoss = 3.5375, kgLoss = 0.0172
2025-04-08 19:09:01.414826: Training Step 226/354: batchLoss = 0.5158, diffLoss = 2.5287, kgLoss = 0.0125
2025-04-08 19:09:03.034468: Training Step 227/354: batchLoss = 0.5495, diffLoss = 2.6929, kgLoss = 0.0137
2025-04-08 19:09:04.683398: Training Step 228/354: batchLoss = 0.5131, diffLoss = 2.5017, kgLoss = 0.0159
2025-04-08 19:09:06.302208: Training Step 229/354: batchLoss = 0.6146, diffLoss = 3.0164, kgLoss = 0.0141
2025-04-08 19:09:07.925118: Training Step 230/354: batchLoss = 0.7296, diffLoss = 3.5801, kgLoss = 0.0169
2025-04-08 19:09:09.538466: Training Step 231/354: batchLoss = 0.5244, diffLoss = 2.5681, kgLoss = 0.0134
2025-04-08 19:09:11.154425: Training Step 232/354: batchLoss = 0.5607, diffLoss = 2.7440, kgLoss = 0.0149
2025-04-08 19:09:12.767763: Training Step 233/354: batchLoss = 0.4564, diffLoss = 2.2346, kgLoss = 0.0118
2025-04-08 19:09:14.392162: Training Step 234/354: batchLoss = 0.5061, diffLoss = 2.4768, kgLoss = 0.0134
2025-04-08 19:09:16.009364: Training Step 235/354: batchLoss = 0.5367, diffLoss = 2.6315, kgLoss = 0.0130
2025-04-08 19:09:17.628125: Training Step 236/354: batchLoss = 0.4880, diffLoss = 2.3890, kgLoss = 0.0128
2025-04-08 19:09:19.246988: Training Step 237/354: batchLoss = 0.5524, diffLoss = 2.7008, kgLoss = 0.0153
2025-04-08 19:09:20.872416: Training Step 238/354: batchLoss = 0.5282, diffLoss = 2.5816, kgLoss = 0.0148
2025-04-08 19:09:22.484983: Training Step 239/354: batchLoss = 0.6022, diffLoss = 2.9485, kgLoss = 0.0156
2025-04-08 19:09:24.105208: Training Step 240/354: batchLoss = 0.5459, diffLoss = 2.6740, kgLoss = 0.0139
2025-04-08 19:09:25.721611: Training Step 241/354: batchLoss = 0.4817, diffLoss = 2.3601, kgLoss = 0.0121
2025-04-08 19:09:27.343947: Training Step 242/354: batchLoss = 0.5963, diffLoss = 2.9169, kgLoss = 0.0161
2025-04-08 19:09:28.962848: Training Step 243/354: batchLoss = 0.7349, diffLoss = 3.6003, kgLoss = 0.0186
2025-04-08 19:09:30.577575: Training Step 244/354: batchLoss = 0.5143, diffLoss = 2.5193, kgLoss = 0.0131
2025-04-08 19:09:32.194080: Training Step 245/354: batchLoss = 0.5039, diffLoss = 2.4660, kgLoss = 0.0133
2025-04-08 19:09:33.809988: Training Step 246/354: batchLoss = 0.5338, diffLoss = 2.6168, kgLoss = 0.0130
2025-04-08 19:09:35.421642: Training Step 247/354: batchLoss = 0.5171, diffLoss = 2.5246, kgLoss = 0.0152
2025-04-08 19:09:37.038043: Training Step 248/354: batchLoss = 0.5060, diffLoss = 2.4713, kgLoss = 0.0147
2025-04-08 19:09:38.658317: Training Step 249/354: batchLoss = 0.5437, diffLoss = 2.6666, kgLoss = 0.0130
2025-04-08 19:09:40.272247: Training Step 250/354: batchLoss = 0.5430, diffLoss = 2.6553, kgLoss = 0.0150
2025-04-08 19:09:41.887903: Training Step 251/354: batchLoss = 0.5775, diffLoss = 2.8108, kgLoss = 0.0192
2025-04-08 19:09:43.500953: Training Step 252/354: batchLoss = 0.5102, diffLoss = 2.4975, kgLoss = 0.0133
2025-04-08 19:09:45.113830: Training Step 253/354: batchLoss = 0.4412, diffLoss = 2.1581, kgLoss = 0.0120
2025-04-08 19:09:46.730382: Training Step 254/354: batchLoss = 0.6550, diffLoss = 3.2038, kgLoss = 0.0177
2025-04-08 19:09:48.344853: Training Step 255/354: batchLoss = 0.6291, diffLoss = 3.0756, kgLoss = 0.0174
2025-04-08 19:09:49.958288: Training Step 256/354: batchLoss = 0.5520, diffLoss = 2.7055, kgLoss = 0.0136
2025-04-08 19:09:51.575437: Training Step 257/354: batchLoss = 0.5621, diffLoss = 2.7460, kgLoss = 0.0161
2025-04-08 19:09:53.193697: Training Step 258/354: batchLoss = 0.6017, diffLoss = 2.9425, kgLoss = 0.0165
2025-04-08 19:09:54.815662: Training Step 259/354: batchLoss = 0.5857, diffLoss = 2.8589, kgLoss = 0.0173
2025-04-08 19:09:56.433505: Training Step 260/354: batchLoss = 0.5679, diffLoss = 2.7731, kgLoss = 0.0166
2025-04-08 19:09:58.048088: Training Step 261/354: batchLoss = 0.5255, diffLoss = 2.5698, kgLoss = 0.0145
2025-04-08 19:09:59.662780: Training Step 262/354: batchLoss = 0.5160, diffLoss = 2.5291, kgLoss = 0.0128
2025-04-08 19:10:01.275352: Training Step 263/354: batchLoss = 0.5910, diffLoss = 2.8833, kgLoss = 0.0180
2025-04-08 19:10:02.891424: Training Step 264/354: batchLoss = 0.7186, diffLoss = 3.5264, kgLoss = 0.0167
2025-04-08 19:10:04.509347: Training Step 265/354: batchLoss = 0.6913, diffLoss = 3.3908, kgLoss = 0.0165
2025-04-08 19:10:06.129271: Training Step 266/354: batchLoss = 0.5533, diffLoss = 2.7109, kgLoss = 0.0139
2025-04-08 19:10:07.744946: Training Step 267/354: batchLoss = 0.5949, diffLoss = 2.9146, kgLoss = 0.0150
2025-04-08 19:10:09.360774: Training Step 268/354: batchLoss = 0.5005, diffLoss = 2.4427, kgLoss = 0.0149
2025-04-08 19:10:10.975298: Training Step 269/354: batchLoss = 0.5578, diffLoss = 2.7389, kgLoss = 0.0126
2025-04-08 19:10:12.589934: Training Step 270/354: batchLoss = 0.5138, diffLoss = 2.5209, kgLoss = 0.0121
2025-04-08 19:10:14.203644: Training Step 271/354: batchLoss = 0.5552, diffLoss = 2.7216, kgLoss = 0.0135
2025-04-08 19:10:15.820503: Training Step 272/354: batchLoss = 0.5933, diffLoss = 2.9040, kgLoss = 0.0156
2025-04-08 19:10:17.434614: Training Step 273/354: batchLoss = 0.4416, diffLoss = 2.1648, kgLoss = 0.0108
2025-04-08 19:10:19.070343: Training Step 274/354: batchLoss = 0.5577, diffLoss = 2.7295, kgLoss = 0.0148
2025-04-08 19:10:20.680926: Training Step 275/354: batchLoss = 0.4719, diffLoss = 2.3084, kgLoss = 0.0127
2025-04-08 19:10:22.301350: Training Step 276/354: batchLoss = 0.4983, diffLoss = 2.4397, kgLoss = 0.0129
2025-04-08 19:10:23.924216: Training Step 277/354: batchLoss = 0.4904, diffLoss = 2.4014, kgLoss = 0.0127
2025-04-08 19:10:25.538778: Training Step 278/354: batchLoss = 0.4638, diffLoss = 2.2694, kgLoss = 0.0124
2025-04-08 19:10:27.153760: Training Step 279/354: batchLoss = 0.4442, diffLoss = 2.1739, kgLoss = 0.0118
2025-04-08 19:10:28.769640: Training Step 280/354: batchLoss = 0.4778, diffLoss = 2.3340, kgLoss = 0.0137
2025-04-08 19:10:30.384416: Training Step 281/354: batchLoss = 0.5910, diffLoss = 2.8997, kgLoss = 0.0138
2025-04-08 19:10:31.993865: Training Step 282/354: batchLoss = 0.6449, diffLoss = 3.1692, kgLoss = 0.0138
2025-04-08 19:10:33.606519: Training Step 283/354: batchLoss = 0.5566, diffLoss = 2.7274, kgLoss = 0.0139
2025-04-08 19:10:35.222373: Training Step 284/354: batchLoss = 0.6637, diffLoss = 3.2537, kgLoss = 0.0162
2025-04-08 19:10:36.846704: Training Step 285/354: batchLoss = 0.7294, diffLoss = 3.5758, kgLoss = 0.0179
2025-04-08 19:10:38.458937: Training Step 286/354: batchLoss = 0.5365, diffLoss = 2.6229, kgLoss = 0.0149
2025-04-08 19:10:40.075702: Training Step 287/354: batchLoss = 0.5458, diffLoss = 2.6741, kgLoss = 0.0138
2025-04-08 19:10:41.693820: Training Step 288/354: batchLoss = 1.8010, diffLoss = 8.8465, kgLoss = 0.0396
2025-04-08 19:10:43.304122: Training Step 289/354: batchLoss = 0.4637, diffLoss = 2.2683, kgLoss = 0.0125
2025-04-08 19:10:44.920275: Training Step 290/354: batchLoss = 0.5422, diffLoss = 2.6518, kgLoss = 0.0148
2025-04-08 19:10:46.535272: Training Step 291/354: batchLoss = 0.4945, diffLoss = 2.4231, kgLoss = 0.0123
2025-04-08 19:10:48.150182: Training Step 292/354: batchLoss = 0.7363, diffLoss = 3.6027, kgLoss = 0.0196
2025-04-08 19:10:49.762285: Training Step 293/354: batchLoss = 0.4947, diffLoss = 2.4177, kgLoss = 0.0139
2025-04-08 19:10:51.378046: Training Step 294/354: batchLoss = 0.5432, diffLoss = 2.6547, kgLoss = 0.0153
2025-04-08 19:10:52.999605: Training Step 295/354: batchLoss = 0.5502, diffLoss = 2.6888, kgLoss = 0.0156
2025-04-08 19:10:54.616640: Training Step 296/354: batchLoss = 0.7099, diffLoss = 3.4742, kgLoss = 0.0188
2025-04-08 19:10:56.232802: Training Step 297/354: batchLoss = 0.5811, diffLoss = 2.8426, kgLoss = 0.0157
2025-04-08 19:10:57.851815: Training Step 298/354: batchLoss = 0.7711, diffLoss = 3.7795, kgLoss = 0.0190
2025-04-08 19:10:59.473848: Training Step 299/354: batchLoss = 0.5748, diffLoss = 2.8119, kgLoss = 0.0155
2025-04-08 19:11:01.088893: Training Step 300/354: batchLoss = 0.5603, diffLoss = 2.7490, kgLoss = 0.0132
2025-04-08 19:11:02.699334: Training Step 301/354: batchLoss = 0.4990, diffLoss = 2.4382, kgLoss = 0.0142
2025-04-08 19:11:04.324394: Training Step 302/354: batchLoss = 0.5093, diffLoss = 2.5006, kgLoss = 0.0115
2025-04-08 19:11:05.939350: Training Step 303/354: batchLoss = 0.4694, diffLoss = 2.2945, kgLoss = 0.0132
2025-04-08 19:11:07.555354: Training Step 304/354: batchLoss = 0.4802, diffLoss = 2.3436, kgLoss = 0.0144
2025-04-08 19:11:09.172655: Training Step 305/354: batchLoss = 0.5331, diffLoss = 2.6069, kgLoss = 0.0147
2025-04-08 19:11:10.795910: Training Step 306/354: batchLoss = 0.6222, diffLoss = 3.0500, kgLoss = 0.0152
2025-04-08 19:11:12.415908: Training Step 307/354: batchLoss = 0.4982, diffLoss = 2.4405, kgLoss = 0.0126
2025-04-08 19:11:14.032426: Training Step 308/354: batchLoss = 0.6280, diffLoss = 3.0654, kgLoss = 0.0186
2025-04-08 19:11:15.648304: Training Step 309/354: batchLoss = 0.6790, diffLoss = 3.3233, kgLoss = 0.0179
2025-04-08 19:11:17.263485: Training Step 310/354: batchLoss = 0.6269, diffLoss = 3.0690, kgLoss = 0.0163
2025-04-08 19:11:18.877191: Training Step 311/354: batchLoss = 0.6088, diffLoss = 2.9780, kgLoss = 0.0166
2025-04-08 19:11:20.493657: Training Step 312/354: batchLoss = 0.5222, diffLoss = 2.5606, kgLoss = 0.0127
2025-04-08 19:11:22.112095: Training Step 313/354: batchLoss = 0.6380, diffLoss = 3.1310, kgLoss = 0.0147
2025-04-08 19:11:23.728464: Training Step 314/354: batchLoss = 0.5326, diffLoss = 2.6088, kgLoss = 0.0135
2025-04-08 19:11:25.351619: Training Step 315/354: batchLoss = 0.6168, diffLoss = 3.0262, kgLoss = 0.0144
2025-04-08 19:11:26.967324: Training Step 316/354: batchLoss = 0.4297, diffLoss = 2.1040, kgLoss = 0.0111
2025-04-08 19:11:28.583170: Training Step 317/354: batchLoss = 0.5116, diffLoss = 2.5046, kgLoss = 0.0134
2025-04-08 19:11:30.196940: Training Step 318/354: batchLoss = 0.5634, diffLoss = 2.7605, kgLoss = 0.0141
2025-04-08 19:11:31.813512: Training Step 319/354: batchLoss = 0.5789, diffLoss = 2.8351, kgLoss = 0.0149
2025-04-08 19:11:33.431126: Training Step 320/354: batchLoss = 0.7623, diffLoss = 3.7299, kgLoss = 0.0204
2025-04-08 19:11:35.047680: Training Step 321/354: batchLoss = 0.5475, diffLoss = 2.6769, kgLoss = 0.0152
2025-04-08 19:11:36.665635: Training Step 322/354: batchLoss = 0.5271, diffLoss = 2.5770, kgLoss = 0.0146
2025-04-08 19:11:38.289002: Training Step 323/354: batchLoss = 0.5052, diffLoss = 2.4771, kgLoss = 0.0122
2025-04-08 19:11:39.911192: Training Step 324/354: batchLoss = 0.6043, diffLoss = 2.9527, kgLoss = 0.0171
2025-04-08 19:11:41.529027: Training Step 325/354: batchLoss = 0.4655, diffLoss = 2.2771, kgLoss = 0.0125
2025-04-08 19:11:43.147951: Training Step 326/354: batchLoss = 0.5481, diffLoss = 2.6810, kgLoss = 0.0149
2025-04-08 19:11:44.803624: Training Step 327/354: batchLoss = 0.5145, diffLoss = 2.5212, kgLoss = 0.0129
2025-04-08 19:11:46.419986: Training Step 328/354: batchLoss = 0.5142, diffLoss = 2.5111, kgLoss = 0.0150
2025-04-08 19:11:48.032248: Training Step 329/354: batchLoss = 0.5688, diffLoss = 2.7846, kgLoss = 0.0149
2025-04-08 19:11:49.642079: Training Step 330/354: batchLoss = 0.5312, diffLoss = 2.6008, kgLoss = 0.0138
2025-04-08 19:11:51.261253: Training Step 331/354: batchLoss = 0.5867, diffLoss = 2.8721, kgLoss = 0.0153
2025-04-08 19:11:52.878649: Training Step 332/354: batchLoss = 0.5121, diffLoss = 2.5025, kgLoss = 0.0145
2025-04-08 19:11:54.493587: Training Step 333/354: batchLoss = 0.4895, diffLoss = 2.3867, kgLoss = 0.0152
2025-04-08 19:11:56.112453: Training Step 334/354: batchLoss = 0.6402, diffLoss = 3.1413, kgLoss = 0.0149
2025-04-08 19:11:57.728334: Training Step 335/354: batchLoss = 0.7131, diffLoss = 3.4888, kgLoss = 0.0192
2025-04-08 19:11:59.349098: Training Step 336/354: batchLoss = 0.6392, diffLoss = 3.1291, kgLoss = 0.0168
2025-04-08 19:12:00.974705: Training Step 337/354: batchLoss = 0.5388, diffLoss = 2.6368, kgLoss = 0.0143
2025-04-08 19:12:02.591906: Training Step 338/354: batchLoss = 0.5680, diffLoss = 2.7869, kgLoss = 0.0132
2025-04-08 19:12:04.208740: Training Step 339/354: batchLoss = 0.4746, diffLoss = 2.3210, kgLoss = 0.0130
2025-04-08 19:12:05.825790: Training Step 340/354: batchLoss = 0.5880, diffLoss = 2.8746, kgLoss = 0.0163
2025-04-08 19:12:07.444042: Training Step 341/354: batchLoss = 0.6516, diffLoss = 3.1876, kgLoss = 0.0176
2025-04-08 19:12:09.061742: Training Step 342/354: batchLoss = 0.6159, diffLoss = 3.0136, kgLoss = 0.0164
2025-04-08 19:12:10.677328: Training Step 343/354: batchLoss = 0.6121, diffLoss = 2.9982, kgLoss = 0.0156
2025-04-08 19:12:12.295465: Training Step 344/354: batchLoss = 0.5357, diffLoss = 2.6220, kgLoss = 0.0141
2025-04-08 19:12:13.915769: Training Step 345/354: batchLoss = 0.6059, diffLoss = 2.9657, kgLoss = 0.0160
2025-04-08 19:12:15.533628: Training Step 346/354: batchLoss = 0.4522, diffLoss = 2.2084, kgLoss = 0.0131
2025-04-08 19:12:17.171740: Training Step 347/354: batchLoss = 0.4840, diffLoss = 2.3643, kgLoss = 0.0139
2025-04-08 19:12:18.783851: Training Step 348/354: batchLoss = 0.5786, diffLoss = 2.8397, kgLoss = 0.0133
2025-04-08 19:12:20.391715: Training Step 349/354: batchLoss = 0.5548, diffLoss = 2.7125, kgLoss = 0.0154
2025-04-08 19:12:22.007091: Training Step 350/354: batchLoss = 0.5791, diffLoss = 2.8336, kgLoss = 0.0155
2025-04-08 19:12:23.618835: Training Step 351/354: batchLoss = 0.5248, diffLoss = 2.5718, kgLoss = 0.0131
2025-04-08 19:12:25.211316: Training Step 352/354: batchLoss = 0.4919, diffLoss = 2.3972, kgLoss = 0.0156
2025-04-08 19:12:26.617325: Training Step 353/354: batchLoss = 0.6624, diffLoss = 3.2408, kgLoss = 0.0178
2025-04-08 19:12:26.707986: 
2025-04-08 19:12:26.708849: Epoch 31/1000, Train: epLoss = 0.9916, epDfLoss = 4.8539, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 19:12:28.019070: Steps 0/138: batch_recall = 47.41, batch_ndcg = 27.01 
2025-04-08 19:12:29.327690: Steps 1/138: batch_recall = 48.36, batch_ndcg = 28.09 
2025-04-08 19:12:30.631066: Steps 2/138: batch_recall = 59.34, batch_ndcg = 35.70 
2025-04-08 19:12:31.944158: Steps 3/138: batch_recall = 58.31, batch_ndcg = 33.96 
2025-04-08 19:12:33.247686: Steps 4/138: batch_recall = 67.51, batch_ndcg = 41.02 
2025-04-08 19:12:34.556838: Steps 5/138: batch_recall = 58.42, batch_ndcg = 31.42 
2025-04-08 19:12:35.865155: Steps 6/138: batch_recall = 51.24, batch_ndcg = 32.01 
2025-04-08 19:12:37.166706: Steps 7/138: batch_recall = 62.29, batch_ndcg = 41.64 
2025-04-08 19:12:38.450748: Steps 8/138: batch_recall = 61.20, batch_ndcg = 38.84 
2025-04-08 19:12:39.755205: Steps 9/138: batch_recall = 58.94, batch_ndcg = 34.64 
2025-04-08 19:12:41.045767: Steps 10/138: batch_recall = 56.63, batch_ndcg = 33.16 
2025-04-08 19:12:42.336468: Steps 11/138: batch_recall = 56.32, batch_ndcg = 32.79 
2025-04-08 19:12:43.625772: Steps 12/138: batch_recall = 51.26, batch_ndcg = 29.29 
2025-04-08 19:12:44.915731: Steps 13/138: batch_recall = 53.98, batch_ndcg = 31.88 
2025-04-08 19:12:46.205074: Steps 14/138: batch_recall = 54.70, batch_ndcg = 31.33 
2025-04-08 19:12:47.495372: Steps 15/138: batch_recall = 49.22, batch_ndcg = 30.13 
2025-04-08 19:12:48.784103: Steps 16/138: batch_recall = 60.65, batch_ndcg = 34.22 
2025-04-08 19:12:50.066991: Steps 17/138: batch_recall = 58.29, batch_ndcg = 33.84 
2025-04-08 19:12:51.362417: Steps 18/138: batch_recall = 53.62, batch_ndcg = 32.99 
2025-04-08 19:12:52.656204: Steps 19/138: batch_recall = 53.76, batch_ndcg = 33.20 
2025-04-08 19:12:53.944015: Steps 20/138: batch_recall = 63.87, batch_ndcg = 36.74 
2025-04-08 19:12:55.233063: Steps 21/138: batch_recall = 68.99, batch_ndcg = 40.61 
2025-04-08 19:12:56.513808: Steps 22/138: batch_recall = 54.68, batch_ndcg = 32.22 
2025-04-08 19:12:57.802641: Steps 23/138: batch_recall = 51.12, batch_ndcg = 30.96 
2025-04-08 19:12:59.093601: Steps 24/138: batch_recall = 57.49, batch_ndcg = 31.37 
2025-04-08 19:13:00.385439: Steps 25/138: batch_recall = 61.32, batch_ndcg = 35.35 
2025-04-08 19:13:01.675915: Steps 26/138: batch_recall = 56.33, batch_ndcg = 32.51 
2025-04-08 19:13:02.964903: Steps 27/138: batch_recall = 56.34, batch_ndcg = 32.09 
2025-04-08 19:13:04.251399: Steps 28/138: batch_recall = 60.69, batch_ndcg = 34.01 
2025-04-08 19:13:05.533962: Steps 29/138: batch_recall = 62.47, batch_ndcg = 32.18 
2025-04-08 19:13:06.825592: Steps 30/138: batch_recall = 57.55, batch_ndcg = 33.87 
2025-04-08 19:13:08.111650: Steps 31/138: batch_recall = 49.19, batch_ndcg = 27.90 
2025-04-08 19:13:09.395187: Steps 32/138: batch_recall = 54.02, batch_ndcg = 31.52 
2025-04-08 19:13:10.677122: Steps 33/138: batch_recall = 61.20, batch_ndcg = 33.92 
2025-04-08 19:13:11.963168: Steps 34/138: batch_recall = 54.47, batch_ndcg = 30.03 
2025-04-08 19:13:13.247029: Steps 35/138: batch_recall = 51.37, batch_ndcg = 30.12 
2025-04-08 19:13:14.531877: Steps 36/138: batch_recall = 46.71, batch_ndcg = 26.92 
2025-04-08 19:13:15.826252: Steps 37/138: batch_recall = 57.99, batch_ndcg = 34.24 
2025-04-08 19:13:17.113462: Steps 38/138: batch_recall = 57.46, batch_ndcg = 32.20 
2025-04-08 19:13:18.401327: Steps 39/138: batch_recall = 71.14, batch_ndcg = 39.31 
2025-04-08 19:13:19.689778: Steps 40/138: batch_recall = 59.51, batch_ndcg = 30.15 
2025-04-08 19:13:20.979587: Steps 41/138: batch_recall = 60.28, batch_ndcg = 34.18 
2025-04-08 19:13:22.268291: Steps 42/138: batch_recall = 53.61, batch_ndcg = 30.40 
2025-04-08 19:13:23.558548: Steps 43/138: batch_recall = 58.18, batch_ndcg = 36.50 
2025-04-08 19:13:24.838842: Steps 44/138: batch_recall = 56.29, batch_ndcg = 30.70 
2025-04-08 19:13:26.132667: Steps 45/138: batch_recall = 63.81, batch_ndcg = 36.11 
2025-04-08 19:13:27.403324: Steps 46/138: batch_recall = 61.87, batch_ndcg = 36.70 
2025-04-08 19:13:28.683909: Steps 47/138: batch_recall = 53.92, batch_ndcg = 32.34 
2025-04-08 19:13:29.976061: Steps 48/138: batch_recall = 62.50, batch_ndcg = 35.75 
2025-04-08 19:13:31.321620: Steps 49/138: batch_recall = 66.23, batch_ndcg = 38.20 
2025-04-08 19:13:32.609145: Steps 50/138: batch_recall = 58.71, batch_ndcg = 31.02 
2025-04-08 19:13:33.896419: Steps 51/138: batch_recall = 62.93, batch_ndcg = 36.18 
2025-04-08 19:13:35.189778: Steps 52/138: batch_recall = 64.30, batch_ndcg = 41.70 
2025-04-08 19:13:36.482432: Steps 53/138: batch_recall = 64.98, batch_ndcg = 34.13 
2025-04-08 19:13:37.777260: Steps 54/138: batch_recall = 68.27, batch_ndcg = 38.39 
2025-04-08 19:13:39.073073: Steps 55/138: batch_recall = 60.39, batch_ndcg = 33.83 
2025-04-08 19:13:40.363380: Steps 56/138: batch_recall = 61.02, batch_ndcg = 35.51 
2025-04-08 19:13:41.651866: Steps 57/138: batch_recall = 58.59, batch_ndcg = 32.60 
2025-04-08 19:13:42.931678: Steps 58/138: batch_recall = 71.48, batch_ndcg = 36.99 
2025-04-08 19:13:44.216285: Steps 59/138: batch_recall = 64.41, batch_ndcg = 38.75 
2025-04-08 19:13:45.500828: Steps 60/138: batch_recall = 67.22, batch_ndcg = 37.94 
2025-04-08 19:13:46.792530: Steps 61/138: batch_recall = 62.13, batch_ndcg = 34.57 
2025-04-08 19:13:48.073051: Steps 62/138: batch_recall = 84.96, batch_ndcg = 45.22 
2025-04-08 19:13:49.365981: Steps 63/138: batch_recall = 74.64, batch_ndcg = 42.78 
2025-04-08 19:13:50.654709: Steps 64/138: batch_recall = 60.42, batch_ndcg = 32.21 
2025-04-08 19:13:51.945032: Steps 65/138: batch_recall = 87.25, batch_ndcg = 48.07 
2025-04-08 19:13:53.232806: Steps 66/138: batch_recall = 69.87, batch_ndcg = 41.54 
2025-04-08 19:13:54.521209: Steps 67/138: batch_recall = 76.56, batch_ndcg = 46.22 
2025-04-08 19:13:55.809135: Steps 68/138: batch_recall = 61.77, batch_ndcg = 33.69 
2025-04-08 19:13:57.090223: Steps 69/138: batch_recall = 85.56, batch_ndcg = 51.09 
2025-04-08 19:13:58.366305: Steps 70/138: batch_recall = 79.80, batch_ndcg = 45.37 
2025-04-08 19:13:59.646983: Steps 71/138: batch_recall = 86.44, batch_ndcg = 51.80 
2025-04-08 19:14:00.928240: Steps 72/138: batch_recall = 84.10, batch_ndcg = 48.72 
2025-04-08 19:14:02.208024: Steps 73/138: batch_recall = 86.59, batch_ndcg = 47.70 
2025-04-08 19:14:03.494868: Steps 74/138: batch_recall = 77.13, batch_ndcg = 48.24 
2025-04-08 19:14:04.775813: Steps 75/138: batch_recall = 85.37, batch_ndcg = 50.15 
2025-04-08 19:14:06.058743: Steps 76/138: batch_recall = 95.20, batch_ndcg = 55.67 
2025-04-08 19:14:07.340318: Steps 77/138: batch_recall = 89.08, batch_ndcg = 51.77 
2025-04-08 19:14:08.625211: Steps 78/138: batch_recall = 90.34, batch_ndcg = 48.21 
2025-04-08 19:14:09.911789: Steps 79/138: batch_recall = 90.34, batch_ndcg = 49.65 
2025-04-08 19:14:11.190990: Steps 80/138: batch_recall = 71.46, batch_ndcg = 38.91 
2025-04-08 19:14:12.479346: Steps 81/138: batch_recall = 82.46, batch_ndcg = 48.32 
2025-04-08 19:14:13.754721: Steps 82/138: batch_recall = 85.46, batch_ndcg = 53.12 
2025-04-08 19:14:15.031787: Steps 83/138: batch_recall = 83.73, batch_ndcg = 48.10 
2025-04-08 19:14:16.300790: Steps 84/138: batch_recall = 99.05, batch_ndcg = 57.99 
2025-04-08 19:14:17.579696: Steps 85/138: batch_recall = 105.45, batch_ndcg = 61.39 
2025-04-08 19:14:18.867813: Steps 86/138: batch_recall = 118.10, batch_ndcg = 70.89 
2025-04-08 19:14:20.147443: Steps 87/138: batch_recall = 104.93, batch_ndcg = 55.41 
2025-04-08 19:14:21.421458: Steps 88/138: batch_recall = 103.52, batch_ndcg = 60.09 
2025-04-08 19:14:22.697972: Steps 89/138: batch_recall = 122.90, batch_ndcg = 69.29 
2025-04-08 19:14:23.975077: Steps 90/138: batch_recall = 102.95, batch_ndcg = 57.83 
2025-04-08 19:14:25.255722: Steps 91/138: batch_recall = 116.26, batch_ndcg = 64.28 
2025-04-08 19:14:26.727386: Steps 92/138: batch_recall = 120.38, batch_ndcg = 65.83 
2025-04-08 19:14:27.993464: Steps 93/138: batch_recall = 121.57, batch_ndcg = 69.23 
2025-04-08 19:14:29.259842: Steps 94/138: batch_recall = 124.59, batch_ndcg = 66.17 
2025-04-08 19:14:30.535568: Steps 95/138: batch_recall = 114.51, batch_ndcg = 67.47 
2025-04-08 19:14:31.799917: Steps 96/138: batch_recall = 134.29, batch_ndcg = 79.56 
2025-04-08 19:14:33.077365: Steps 97/138: batch_recall = 149.27, batch_ndcg = 90.60 
2025-04-08 19:14:34.350688: Steps 98/138: batch_recall = 109.84, batch_ndcg = 63.97 
2025-04-08 19:14:35.625463: Steps 99/138: batch_recall = 124.81, batch_ndcg = 71.84 
2025-04-08 19:14:36.905503: Steps 100/138: batch_recall = 132.25, batch_ndcg = 73.54 
2025-04-08 19:14:38.182714: Steps 101/138: batch_recall = 126.64, batch_ndcg = 69.91 
2025-04-08 19:14:39.466051: Steps 102/138: batch_recall = 123.04, batch_ndcg = 70.39 
2025-04-08 19:14:40.734785: Steps 103/138: batch_recall = 140.85, batch_ndcg = 80.51 
2025-04-08 19:14:42.018522: Steps 104/138: batch_recall = 136.01, batch_ndcg = 79.57 
2025-04-08 19:14:43.293471: Steps 105/138: batch_recall = 120.87, batch_ndcg = 66.92 
2025-04-08 19:14:44.563577: Steps 106/138: batch_recall = 106.02, batch_ndcg = 60.11 
2025-04-08 19:14:45.836627: Steps 107/138: batch_recall = 115.30, batch_ndcg = 64.74 
2025-04-08 19:14:47.111397: Steps 108/138: batch_recall = 119.69, batch_ndcg = 73.12 
2025-04-08 19:14:48.376874: Steps 109/138: batch_recall = 138.91, batch_ndcg = 76.94 
2025-04-08 19:14:49.648604: Steps 110/138: batch_recall = 123.60, batch_ndcg = 64.89 
2025-04-08 19:14:50.918889: Steps 111/138: batch_recall = 135.81, batch_ndcg = 84.50 
2025-04-08 19:14:52.207305: Steps 112/138: batch_recall = 166.88, batch_ndcg = 92.37 
2025-04-08 19:14:53.484661: Steps 113/138: batch_recall = 121.06, batch_ndcg = 68.91 
2025-04-08 19:14:54.763372: Steps 114/138: batch_recall = 124.62, batch_ndcg = 72.75 
2025-04-08 19:14:56.034032: Steps 115/138: batch_recall = 120.70, batch_ndcg = 63.65 
2025-04-08 19:14:57.305275: Steps 116/138: batch_recall = 128.34, batch_ndcg = 68.90 
2025-04-08 19:14:58.576495: Steps 117/138: batch_recall = 111.89, batch_ndcg = 66.26 
2025-04-08 19:14:59.840426: Steps 118/138: batch_recall = 125.28, batch_ndcg = 69.96 
2025-04-08 19:15:01.117912: Steps 119/138: batch_recall = 141.52, batch_ndcg = 76.88 
2025-04-08 19:15:02.390900: Steps 120/138: batch_recall = 124.08, batch_ndcg = 70.69 
2025-04-08 19:15:03.654980: Steps 121/138: batch_recall = 146.56, batch_ndcg = 78.77 
2025-04-08 19:15:04.939144: Steps 122/138: batch_recall = 151.33, batch_ndcg = 81.40 
2025-04-08 19:15:06.211223: Steps 123/138: batch_recall = 132.87, batch_ndcg = 75.43 
2025-04-08 19:15:07.482019: Steps 124/138: batch_recall = 154.16, batch_ndcg = 92.88 
2025-04-08 19:15:08.752161: Steps 125/138: batch_recall = 136.63, batch_ndcg = 74.74 
2025-04-08 19:15:10.022591: Steps 126/138: batch_recall = 161.39, batch_ndcg = 89.01 
2025-04-08 19:15:11.284843: Steps 127/138: batch_recall = 144.34, batch_ndcg = 82.18 
2025-04-08 19:15:12.561504: Steps 128/138: batch_recall = 131.02, batch_ndcg = 72.98 
2025-04-08 19:15:13.825118: Steps 129/138: batch_recall = 160.30, batch_ndcg = 91.89 
2025-04-08 19:15:15.095110: Steps 130/138: batch_recall = 132.35, batch_ndcg = 70.17 
2025-04-08 19:15:16.364119: Steps 131/138: batch_recall = 151.46, batch_ndcg = 88.46 
2025-04-08 19:15:17.630748: Steps 132/138: batch_recall = 151.22, batch_ndcg = 85.08 
2025-04-08 19:15:18.899333: Steps 133/138: batch_recall = 146.77, batch_ndcg = 85.59 
2025-04-08 19:15:20.173100: Steps 134/138: batch_recall = 146.88, batch_ndcg = 84.07 
2025-04-08 19:15:21.441325: Steps 135/138: batch_recall = 168.36, batch_ndcg = 96.06 
2025-04-08 19:15:22.717162: Steps 136/138: batch_recall = 151.39, batch_ndcg = 79.69 
2025-04-08 19:15:23.993856: Steps 137/138: batch_recall = 138.95, batch_ndcg = 86.86 
2025-04-08 19:15:23.994382: Epoch 31/1000, Test: Recall = 0.1769, NDCG = 0.1010  

2025-04-08 19:15:25.759867: Training Step 0/354: batchLoss = 0.5029, diffLoss = 2.4646, kgLoss = 0.0124
2025-04-08 19:15:27.390627: Training Step 1/354: batchLoss = 0.5447, diffLoss = 2.6681, kgLoss = 0.0139
2025-04-08 19:15:29.007303: Training Step 2/354: batchLoss = 0.7276, diffLoss = 3.5641, kgLoss = 0.0185
2025-04-08 19:15:30.627639: Training Step 3/354: batchLoss = 0.4883, diffLoss = 2.3944, kgLoss = 0.0117
2025-04-08 19:15:32.244585: Training Step 4/354: batchLoss = 0.4423, diffLoss = 2.1649, kgLoss = 0.0117
2025-04-08 19:15:33.857371: Training Step 5/354: batchLoss = 0.6161, diffLoss = 3.0117, kgLoss = 0.0171
2025-04-08 19:15:35.476218: Training Step 6/354: batchLoss = 0.6883, diffLoss = 3.3713, kgLoss = 0.0176
2025-04-08 19:15:37.093807: Training Step 7/354: batchLoss = 0.4946, diffLoss = 2.4163, kgLoss = 0.0142
2025-04-08 19:15:38.708426: Training Step 8/354: batchLoss = 0.5643, diffLoss = 2.7654, kgLoss = 0.0140
2025-04-08 19:15:40.329408: Training Step 9/354: batchLoss = 0.5059, diffLoss = 2.4726, kgLoss = 0.0143
2025-04-08 19:15:41.952249: Training Step 10/354: batchLoss = 0.4966, diffLoss = 2.4344, kgLoss = 0.0121
2025-04-08 19:15:43.574655: Training Step 11/354: batchLoss = 0.4950, diffLoss = 2.4182, kgLoss = 0.0142
2025-04-08 19:15:45.192148: Training Step 12/354: batchLoss = 0.4708, diffLoss = 2.3038, kgLoss = 0.0126
2025-04-08 19:15:46.810468: Training Step 13/354: batchLoss = 0.5055, diffLoss = 2.4701, kgLoss = 0.0143
2025-04-08 19:15:48.430124: Training Step 14/354: batchLoss = 0.4973, diffLoss = 2.4228, kgLoss = 0.0159
2025-04-08 19:15:50.040497: Training Step 15/354: batchLoss = 0.4786, diffLoss = 2.3432, kgLoss = 0.0124
2025-04-08 19:15:51.657495: Training Step 16/354: batchLoss = 0.4553, diffLoss = 2.2268, kgLoss = 0.0124
2025-04-08 19:15:53.268334: Training Step 17/354: batchLoss = 0.5594, diffLoss = 2.7361, kgLoss = 0.0153
2025-04-08 19:15:54.884702: Training Step 18/354: batchLoss = 0.5114, diffLoss = 2.5000, kgLoss = 0.0143
2025-04-08 19:15:56.504847: Training Step 19/354: batchLoss = 0.6631, diffLoss = 3.2490, kgLoss = 0.0166
2025-04-08 19:15:58.128980: Training Step 20/354: batchLoss = 0.6408, diffLoss = 3.1375, kgLoss = 0.0166
2025-04-08 19:15:59.751355: Training Step 21/354: batchLoss = 0.6340, diffLoss = 3.1108, kgLoss = 0.0148
2025-04-08 19:16:01.369261: Training Step 22/354: batchLoss = 0.5233, diffLoss = 2.5606, kgLoss = 0.0140
2025-04-08 19:16:02.987793: Training Step 23/354: batchLoss = 0.5376, diffLoss = 2.6319, kgLoss = 0.0140
2025-04-08 19:16:04.602079: Training Step 24/354: batchLoss = 0.4757, diffLoss = 2.3255, kgLoss = 0.0132
2025-04-08 19:16:06.218915: Training Step 25/354: batchLoss = 0.5060, diffLoss = 2.4713, kgLoss = 0.0147
2025-04-08 19:16:07.830701: Training Step 26/354: batchLoss = 0.5883, diffLoss = 2.8755, kgLoss = 0.0165
2025-04-08 19:16:09.447081: Training Step 27/354: batchLoss = 0.5404, diffLoss = 2.6497, kgLoss = 0.0131
2025-04-08 19:16:11.065882: Training Step 28/354: batchLoss = 0.6870, diffLoss = 3.3691, kgLoss = 0.0164
2025-04-08 19:16:12.686761: Training Step 29/354: batchLoss = 0.5147, diffLoss = 2.5170, kgLoss = 0.0142
2025-04-08 19:16:14.306707: Training Step 30/354: batchLoss = 0.5827, diffLoss = 2.8582, kgLoss = 0.0139
2025-04-08 19:16:15.920819: Training Step 31/354: batchLoss = 0.4927, diffLoss = 2.4046, kgLoss = 0.0147
2025-04-08 19:16:17.541194: Training Step 32/354: batchLoss = 0.6186, diffLoss = 3.0254, kgLoss = 0.0169
2025-04-08 19:16:19.159702: Training Step 33/354: batchLoss = 0.5955, diffLoss = 2.9163, kgLoss = 0.0153
2025-04-08 19:16:20.774798: Training Step 34/354: batchLoss = 0.5532, diffLoss = 2.7063, kgLoss = 0.0149
2025-04-08 19:16:22.386026: Training Step 35/354: batchLoss = 0.6212, diffLoss = 3.0407, kgLoss = 0.0163
2025-04-08 19:16:24.005842: Training Step 36/354: batchLoss = 0.5863, diffLoss = 2.8714, kgLoss = 0.0150
2025-04-08 19:16:25.623392: Training Step 37/354: batchLoss = 0.6012, diffLoss = 2.9465, kgLoss = 0.0148
2025-04-08 19:16:27.243868: Training Step 38/354: batchLoss = 0.5413, diffLoss = 2.6511, kgLoss = 0.0139
2025-04-08 19:16:28.864162: Training Step 39/354: batchLoss = 0.6750, diffLoss = 3.3069, kgLoss = 0.0170
2025-04-08 19:16:30.481965: Training Step 40/354: batchLoss = 0.5258, diffLoss = 2.5761, kgLoss = 0.0132
2025-04-08 19:16:32.102491: Training Step 41/354: batchLoss = 0.4689, diffLoss = 2.2951, kgLoss = 0.0124
2025-04-08 19:16:33.722784: Training Step 42/354: batchLoss = 0.4731, diffLoss = 2.3144, kgLoss = 0.0128
2025-04-08 19:16:35.345454: Training Step 43/354: batchLoss = 0.6033, diffLoss = 2.9599, kgLoss = 0.0142
2025-04-08 19:16:36.959113: Training Step 44/354: batchLoss = 0.5366, diffLoss = 2.6255, kgLoss = 0.0144
2025-04-08 19:16:38.574723: Training Step 45/354: batchLoss = 0.5634, diffLoss = 2.7476, kgLoss = 0.0174
2025-04-08 19:16:40.187974: Training Step 46/354: batchLoss = 0.5293, diffLoss = 2.5873, kgLoss = 0.0147
2025-04-08 19:16:41.813285: Training Step 47/354: batchLoss = 0.5659, diffLoss = 2.7737, kgLoss = 0.0139
2025-04-08 19:16:43.432703: Training Step 48/354: batchLoss = 0.6653, diffLoss = 3.2603, kgLoss = 0.0165
2025-04-08 19:16:45.054830: Training Step 49/354: batchLoss = 0.4402, diffLoss = 2.1573, kgLoss = 0.0110
2025-04-08 19:16:46.672624: Training Step 50/354: batchLoss = 0.6273, diffLoss = 3.0711, kgLoss = 0.0163
2025-04-08 19:16:48.290428: Training Step 51/354: batchLoss = 0.4456, diffLoss = 2.1777, kgLoss = 0.0125
2025-04-08 19:16:49.904035: Training Step 52/354: batchLoss = 0.4876, diffLoss = 2.3848, kgLoss = 0.0133
2025-04-08 19:16:51.519431: Training Step 53/354: batchLoss = 0.5557, diffLoss = 2.7205, kgLoss = 0.0144
2025-04-08 19:16:53.135278: Training Step 54/354: batchLoss = 0.5447, diffLoss = 2.6438, kgLoss = 0.0199
2025-04-08 19:16:54.751525: Training Step 55/354: batchLoss = 0.6129, diffLoss = 2.9968, kgLoss = 0.0169
2025-04-08 19:16:56.370000: Training Step 56/354: batchLoss = 0.5019, diffLoss = 2.4597, kgLoss = 0.0124
2025-04-08 19:16:58.403291: Training Step 57/354: batchLoss = 0.5098, diffLoss = 2.4967, kgLoss = 0.0130
2025-04-08 19:17:00.091517: Training Step 58/354: batchLoss = 0.6750, diffLoss = 3.3086, kgLoss = 0.0166
2025-04-08 19:17:01.706504: Training Step 59/354: batchLoss = 0.8187, diffLoss = 4.0148, kgLoss = 0.0196
2025-04-08 19:17:03.325947: Training Step 60/354: batchLoss = 0.5887, diffLoss = 2.8860, kgLoss = 0.0144
2025-04-08 19:17:04.940604: Training Step 61/354: batchLoss = 0.5703, diffLoss = 2.7897, kgLoss = 0.0154
2025-04-08 19:17:06.563612: Training Step 62/354: batchLoss = 0.5498, diffLoss = 2.6927, kgLoss = 0.0141
2025-04-08 19:17:08.185285: Training Step 63/354: batchLoss = 0.5977, diffLoss = 2.9306, kgLoss = 0.0145
2025-04-08 19:17:09.796679: Training Step 64/354: batchLoss = 0.7088, diffLoss = 3.4698, kgLoss = 0.0185
2025-04-08 19:17:11.415349: Training Step 65/354: batchLoss = 0.6790, diffLoss = 3.3207, kgLoss = 0.0186
2025-04-08 19:17:13.029670: Training Step 66/354: batchLoss = 0.4608, diffLoss = 2.2443, kgLoss = 0.0150
2025-04-08 19:17:14.655682: Training Step 67/354: batchLoss = 0.6120, diffLoss = 2.9945, kgLoss = 0.0164
2025-04-08 19:17:16.279624: Training Step 68/354: batchLoss = 0.5486, diffLoss = 2.6816, kgLoss = 0.0153
2025-04-08 19:17:17.895621: Training Step 69/354: batchLoss = 0.6049, diffLoss = 2.9637, kgLoss = 0.0152
2025-04-08 19:17:19.518762: Training Step 70/354: batchLoss = 0.6039, diffLoss = 2.9552, kgLoss = 0.0161
2025-04-08 19:17:21.135798: Training Step 71/354: batchLoss = 0.5776, diffLoss = 2.8296, kgLoss = 0.0146
2025-04-08 19:17:22.755792: Training Step 72/354: batchLoss = 0.5514, diffLoss = 2.6951, kgLoss = 0.0155
2025-04-08 19:17:24.369550: Training Step 73/354: batchLoss = 0.4481, diffLoss = 2.1839, kgLoss = 0.0141
2025-04-08 19:17:25.985064: Training Step 74/354: batchLoss = 0.4481, diffLoss = 2.1831, kgLoss = 0.0144
2025-04-08 19:17:27.602799: Training Step 75/354: batchLoss = 0.5775, diffLoss = 2.8223, kgLoss = 0.0163
2025-04-08 19:17:29.225224: Training Step 76/354: batchLoss = 0.6924, diffLoss = 3.3955, kgLoss = 0.0166
2025-04-08 19:17:30.849676: Training Step 77/354: batchLoss = 0.4985, diffLoss = 2.4388, kgLoss = 0.0134
2025-04-08 19:17:32.470986: Training Step 78/354: batchLoss = 0.4923, diffLoss = 2.4051, kgLoss = 0.0141
2025-04-08 19:17:34.096954: Training Step 79/354: batchLoss = 0.5878, diffLoss = 2.8710, kgLoss = 0.0171
2025-04-08 19:17:35.722233: Training Step 80/354: batchLoss = 0.5167, diffLoss = 2.5314, kgLoss = 0.0130
2025-04-08 19:17:37.344528: Training Step 81/354: batchLoss = 0.5685, diffLoss = 2.7823, kgLoss = 0.0151
2025-04-08 19:17:38.963072: Training Step 82/354: batchLoss = 0.5649, diffLoss = 2.7695, kgLoss = 0.0138
2025-04-08 19:17:40.581432: Training Step 83/354: batchLoss = 0.6412, diffLoss = 3.1308, kgLoss = 0.0189
2025-04-08 19:17:42.199355: Training Step 84/354: batchLoss = 0.5170, diffLoss = 2.5381, kgLoss = 0.0117
2025-04-08 19:17:43.877545: Training Step 85/354: batchLoss = 0.5924, diffLoss = 2.8984, kgLoss = 0.0158
2025-04-08 19:17:45.592218: Training Step 86/354: batchLoss = 0.4757, diffLoss = 2.3341, kgLoss = 0.0111
2025-04-08 19:17:47.205279: Training Step 87/354: batchLoss = 0.5808, diffLoss = 2.8462, kgLoss = 0.0145
2025-04-08 19:17:48.820862: Training Step 88/354: batchLoss = 0.5276, diffLoss = 2.5833, kgLoss = 0.0137
2025-04-08 19:17:50.439673: Training Step 89/354: batchLoss = 0.6175, diffLoss = 3.0234, kgLoss = 0.0161
2025-04-08 19:17:52.058244: Training Step 90/354: batchLoss = 0.5136, diffLoss = 2.5138, kgLoss = 0.0135
2025-04-08 19:17:53.674756: Training Step 91/354: batchLoss = 0.5542, diffLoss = 2.7097, kgLoss = 0.0153
2025-04-08 19:17:55.289405: Training Step 92/354: batchLoss = 0.4830, diffLoss = 2.3627, kgLoss = 0.0131
2025-04-08 19:17:56.906891: Training Step 93/354: batchLoss = 0.4511, diffLoss = 2.1981, kgLoss = 0.0143
2025-04-08 19:17:58.523129: Training Step 94/354: batchLoss = 0.5572, diffLoss = 2.7246, kgLoss = 0.0153
2025-04-08 19:18:00.135556: Training Step 95/354: batchLoss = 0.4979, diffLoss = 2.4381, kgLoss = 0.0128
2025-04-08 19:18:01.756138: Training Step 96/354: batchLoss = 0.6470, diffLoss = 3.1659, kgLoss = 0.0173
2025-04-08 19:18:03.385365: Training Step 97/354: batchLoss = 0.5056, diffLoss = 2.4737, kgLoss = 0.0135
2025-04-08 19:18:05.005607: Training Step 98/354: batchLoss = 0.3955, diffLoss = 1.9278, kgLoss = 0.0125
2025-04-08 19:18:06.627513: Training Step 99/354: batchLoss = 0.6503, diffLoss = 3.1883, kgLoss = 0.0158
2025-04-08 19:18:08.244011: Training Step 100/354: batchLoss = 0.5365, diffLoss = 2.6233, kgLoss = 0.0148
2025-04-08 19:18:09.864295: Training Step 101/354: batchLoss = 0.5618, diffLoss = 2.7452, kgLoss = 0.0159
2025-04-08 19:18:11.480737: Training Step 102/354: batchLoss = 0.7354, diffLoss = 3.6076, kgLoss = 0.0174
2025-04-08 19:18:13.096493: Training Step 103/354: batchLoss = 0.4965, diffLoss = 2.4307, kgLoss = 0.0130
2025-04-08 19:18:14.710204: Training Step 104/354: batchLoss = 0.5507, diffLoss = 2.6939, kgLoss = 0.0149
2025-04-08 19:18:16.327068: Training Step 105/354: batchLoss = 0.4759, diffLoss = 2.3246, kgLoss = 0.0137
2025-04-08 19:18:17.944749: Training Step 106/354: batchLoss = 0.4650, diffLoss = 2.2641, kgLoss = 0.0152
2025-04-08 19:18:19.929156: Training Step 107/354: batchLoss = 0.5319, diffLoss = 2.6000, kgLoss = 0.0149
2025-04-08 19:18:21.550842: Training Step 108/354: batchLoss = 0.6908, diffLoss = 3.3856, kgLoss = 0.0171
2025-04-08 19:18:23.162232: Training Step 109/354: batchLoss = 0.4357, diffLoss = 2.1322, kgLoss = 0.0116
2025-04-08 19:18:24.780405: Training Step 110/354: batchLoss = 0.5450, diffLoss = 2.6668, kgLoss = 0.0145
2025-04-08 19:18:26.396186: Training Step 111/354: batchLoss = 0.5273, diffLoss = 2.5883, kgLoss = 0.0121
2025-04-08 19:18:28.008519: Training Step 112/354: batchLoss = 0.5543, diffLoss = 2.7160, kgLoss = 0.0139
2025-04-08 19:18:29.628027: Training Step 113/354: batchLoss = 0.4043, diffLoss = 1.9784, kgLoss = 0.0107
2025-04-08 19:18:31.247699: Training Step 114/354: batchLoss = 0.6053, diffLoss = 2.9624, kgLoss = 0.0160
2025-04-08 19:18:32.867225: Training Step 115/354: batchLoss = 0.5887, diffLoss = 2.8890, kgLoss = 0.0137
2025-04-08 19:18:34.487062: Training Step 116/354: batchLoss = 0.6418, diffLoss = 3.1461, kgLoss = 0.0158
2025-04-08 19:18:36.107646: Training Step 117/354: batchLoss = 0.4496, diffLoss = 2.2008, kgLoss = 0.0118
2025-04-08 19:18:37.726839: Training Step 118/354: batchLoss = 0.3776, diffLoss = 1.8389, kgLoss = 0.0122
2025-04-08 19:18:39.349742: Training Step 119/354: batchLoss = 0.5534, diffLoss = 2.7129, kgLoss = 0.0135
2025-04-08 19:18:40.968155: Training Step 120/354: batchLoss = 0.6633, diffLoss = 3.2502, kgLoss = 0.0166
2025-04-08 19:18:42.583308: Training Step 121/354: batchLoss = 0.5838, diffLoss = 2.8564, kgLoss = 0.0156
2025-04-08 19:18:44.198709: Training Step 122/354: batchLoss = 0.6052, diffLoss = 2.9654, kgLoss = 0.0151
2025-04-08 19:18:45.809099: Training Step 123/354: batchLoss = 0.6288, diffLoss = 3.0724, kgLoss = 0.0179
2025-04-08 19:18:47.418682: Training Step 124/354: batchLoss = 0.6442, diffLoss = 3.1438, kgLoss = 0.0193
2025-04-08 19:18:49.036623: Training Step 125/354: batchLoss = 0.5963, diffLoss = 2.9183, kgLoss = 0.0158
2025-04-08 19:18:50.655179: Training Step 126/354: batchLoss = 0.4999, diffLoss = 2.4437, kgLoss = 0.0140
2025-04-08 19:18:52.270545: Training Step 127/354: batchLoss = 0.5627, diffLoss = 2.7613, kgLoss = 0.0130
2025-04-08 19:18:53.899604: Training Step 128/354: batchLoss = 0.5213, diffLoss = 2.5554, kgLoss = 0.0127
2025-04-08 19:18:55.522194: Training Step 129/354: batchLoss = 0.4653, diffLoss = 2.2745, kgLoss = 0.0130
2025-04-08 19:18:57.143673: Training Step 130/354: batchLoss = 0.5598, diffLoss = 2.7439, kgLoss = 0.0138
2025-04-08 19:18:58.764325: Training Step 131/354: batchLoss = 0.4865, diffLoss = 2.3655, kgLoss = 0.0167
2025-04-08 19:19:00.375220: Training Step 132/354: batchLoss = 0.5178, diffLoss = 2.5360, kgLoss = 0.0133
2025-04-08 19:19:01.981332: Training Step 133/354: batchLoss = 0.6261, diffLoss = 3.0734, kgLoss = 0.0143
2025-04-08 19:19:03.595935: Training Step 134/354: batchLoss = 0.5109, diffLoss = 2.4867, kgLoss = 0.0170
2025-04-08 19:19:05.203815: Training Step 135/354: batchLoss = 0.5040, diffLoss = 2.4639, kgLoss = 0.0140
2025-04-08 19:19:06.832233: Training Step 136/354: batchLoss = 0.4670, diffLoss = 2.2845, kgLoss = 0.0126
2025-04-08 19:19:08.452936: Training Step 137/354: batchLoss = 0.5307, diffLoss = 2.5976, kgLoss = 0.0140
2025-04-08 19:19:10.075757: Training Step 138/354: batchLoss = 0.4324, diffLoss = 2.1119, kgLoss = 0.0126
2025-04-08 19:19:11.692889: Training Step 139/354: batchLoss = 0.5062, diffLoss = 2.4708, kgLoss = 0.0150
2025-04-08 19:19:13.320767: Training Step 140/354: batchLoss = 0.6196, diffLoss = 3.0238, kgLoss = 0.0186
2025-04-08 19:19:14.941086: Training Step 141/354: batchLoss = 0.5465, diffLoss = 2.6705, kgLoss = 0.0155
2025-04-08 19:19:16.553203: Training Step 142/354: batchLoss = 0.5454, diffLoss = 2.6676, kgLoss = 0.0149
2025-04-08 19:19:18.168802: Training Step 143/354: batchLoss = 0.4826, diffLoss = 2.3598, kgLoss = 0.0133
2025-04-08 19:19:19.775685: Training Step 144/354: batchLoss = 0.4507, diffLoss = 2.1983, kgLoss = 0.0138
2025-04-08 19:19:21.394217: Training Step 145/354: batchLoss = 0.5469, diffLoss = 2.6703, kgLoss = 0.0160
2025-04-08 19:19:23.022306: Training Step 146/354: batchLoss = 0.5974, diffLoss = 2.9289, kgLoss = 0.0145
2025-04-08 19:19:24.640821: Training Step 147/354: batchLoss = 0.6255, diffLoss = 3.0592, kgLoss = 0.0171
2025-04-08 19:19:26.263230: Training Step 148/354: batchLoss = 0.5306, diffLoss = 2.5961, kgLoss = 0.0142
2025-04-08 19:19:27.880222: Training Step 149/354: batchLoss = 0.5178, diffLoss = 2.5299, kgLoss = 0.0147
2025-04-08 19:19:29.495917: Training Step 150/354: batchLoss = 0.7015, diffLoss = 3.4307, kgLoss = 0.0192
2025-04-08 19:19:31.110933: Training Step 151/354: batchLoss = 0.5832, diffLoss = 2.8524, kgLoss = 0.0159
2025-04-08 19:19:32.733126: Training Step 152/354: batchLoss = 1.6485, diffLoss = 8.0879, kgLoss = 0.0386
2025-04-08 19:19:34.343819: Training Step 153/354: batchLoss = 0.4862, diffLoss = 2.3745, kgLoss = 0.0142
2025-04-08 19:19:35.960609: Training Step 154/354: batchLoss = 0.5286, diffLoss = 2.5865, kgLoss = 0.0141
2025-04-08 19:19:37.577318: Training Step 155/354: batchLoss = 0.5034, diffLoss = 2.4591, kgLoss = 0.0145
2025-04-08 19:19:39.194760: Training Step 156/354: batchLoss = 0.4670, diffLoss = 2.2862, kgLoss = 0.0122
2025-04-08 19:19:40.810320: Training Step 157/354: batchLoss = 0.5970, diffLoss = 2.9200, kgLoss = 0.0162
2025-04-08 19:19:42.445406: Training Step 158/354: batchLoss = 0.5426, diffLoss = 2.6382, kgLoss = 0.0188
2025-04-08 19:19:44.067049: Training Step 159/354: batchLoss = 0.5128, diffLoss = 2.5092, kgLoss = 0.0137
2025-04-08 19:19:45.684162: Training Step 160/354: batchLoss = 0.5538, diffLoss = 2.7068, kgLoss = 0.0156
2025-04-08 19:19:47.628506: Training Step 161/354: batchLoss = 0.5554, diffLoss = 2.7236, kgLoss = 0.0134
2025-04-08 19:19:49.244309: Training Step 162/354: batchLoss = 0.6034, diffLoss = 2.9430, kgLoss = 0.0185
2025-04-08 19:19:50.859717: Training Step 163/354: batchLoss = 0.5887, diffLoss = 2.8783, kgLoss = 0.0163
2025-04-08 19:19:52.474483: Training Step 164/354: batchLoss = 0.5296, diffLoss = 2.5938, kgLoss = 0.0135
2025-04-08 19:19:54.091855: Training Step 165/354: batchLoss = 0.6854, diffLoss = 3.3566, kgLoss = 0.0176
2025-04-08 19:19:55.709975: Training Step 166/354: batchLoss = 0.6965, diffLoss = 3.4077, kgLoss = 0.0188
2025-04-08 19:19:57.329329: Training Step 167/354: batchLoss = 0.5044, diffLoss = 2.4647, kgLoss = 0.0144
2025-04-08 19:19:58.949801: Training Step 168/354: batchLoss = 0.7083, diffLoss = 3.4680, kgLoss = 0.0184
2025-04-08 19:20:00.568687: Training Step 169/354: batchLoss = 0.4127, diffLoss = 2.0155, kgLoss = 0.0119
2025-04-08 19:20:02.189411: Training Step 170/354: batchLoss = 0.8711, diffLoss = 4.2665, kgLoss = 0.0222
2025-04-08 19:20:03.806263: Training Step 171/354: batchLoss = 0.5481, diffLoss = 2.6836, kgLoss = 0.0142
2025-04-08 19:20:05.420603: Training Step 172/354: batchLoss = 0.5974, diffLoss = 2.9297, kgLoss = 0.0144
2025-04-08 19:20:07.038762: Training Step 173/354: batchLoss = 0.4409, diffLoss = 2.1576, kgLoss = 0.0117
2025-04-08 19:20:08.653302: Training Step 174/354: batchLoss = 0.6100, diffLoss = 2.9878, kgLoss = 0.0156
2025-04-08 19:20:10.273672: Training Step 175/354: batchLoss = 0.5043, diffLoss = 2.4718, kgLoss = 0.0125
2025-04-08 19:20:11.894404: Training Step 176/354: batchLoss = 0.6245, diffLoss = 3.0645, kgLoss = 0.0144
2025-04-08 19:20:13.518233: Training Step 177/354: batchLoss = 0.4922, diffLoss = 2.4129, kgLoss = 0.0121
2025-04-08 19:20:15.207440: Training Step 178/354: batchLoss = 0.4743, diffLoss = 2.3217, kgLoss = 0.0124
2025-04-08 19:20:16.829085: Training Step 179/354: batchLoss = 0.4609, diffLoss = 2.2578, kgLoss = 0.0117
2025-04-08 19:20:18.447073: Training Step 180/354: batchLoss = 0.6745, diffLoss = 3.3101, kgLoss = 0.0156
2025-04-08 19:20:20.068606: Training Step 181/354: batchLoss = 0.4855, diffLoss = 2.3777, kgLoss = 0.0125
2025-04-08 19:20:21.677231: Training Step 182/354: batchLoss = 0.5675, diffLoss = 2.7844, kgLoss = 0.0133
2025-04-08 19:20:23.285007: Training Step 183/354: batchLoss = 0.4733, diffLoss = 2.3218, kgLoss = 0.0112
2025-04-08 19:20:24.901084: Training Step 184/354: batchLoss = 0.5446, diffLoss = 2.6655, kgLoss = 0.0143
2025-04-08 19:20:26.523719: Training Step 185/354: batchLoss = 0.5900, diffLoss = 2.8864, kgLoss = 0.0160
2025-04-08 19:20:28.144993: Training Step 186/354: batchLoss = 0.7006, diffLoss = 3.4339, kgLoss = 0.0172
2025-04-08 19:20:29.765460: Training Step 187/354: batchLoss = 0.4783, diffLoss = 2.3399, kgLoss = 0.0129
2025-04-08 19:20:31.389711: Training Step 188/354: batchLoss = 0.4535, diffLoss = 2.2159, kgLoss = 0.0129
2025-04-08 19:20:33.018947: Training Step 189/354: batchLoss = 0.6189, diffLoss = 3.0258, kgLoss = 0.0172
2025-04-08 19:20:34.640448: Training Step 190/354: batchLoss = 0.4560, diffLoss = 2.2295, kgLoss = 0.0126
2025-04-08 19:20:36.264649: Training Step 191/354: batchLoss = 0.5100, diffLoss = 2.4967, kgLoss = 0.0133
2025-04-08 19:20:37.880970: Training Step 192/354: batchLoss = 0.5358, diffLoss = 2.6250, kgLoss = 0.0135
2025-04-08 19:20:39.500569: Training Step 193/354: batchLoss = 0.5319, diffLoss = 2.6036, kgLoss = 0.0139
2025-04-08 19:20:41.120561: Training Step 194/354: batchLoss = 0.6905, diffLoss = 3.3780, kgLoss = 0.0186
2025-04-08 19:20:42.739672: Training Step 195/354: batchLoss = 0.6545, diffLoss = 3.2027, kgLoss = 0.0175
2025-04-08 19:20:44.355863: Training Step 196/354: batchLoss = 0.5922, diffLoss = 2.8986, kgLoss = 0.0156
2025-04-08 19:20:45.986054: Training Step 197/354: batchLoss = 0.4579, diffLoss = 2.2438, kgLoss = 0.0114
2025-04-08 19:20:47.611447: Training Step 198/354: batchLoss = 0.5229, diffLoss = 2.5576, kgLoss = 0.0142
2025-04-08 19:20:49.226966: Training Step 199/354: batchLoss = 0.5147, diffLoss = 2.5183, kgLoss = 0.0138
2025-04-08 19:20:50.861175: Training Step 200/354: batchLoss = 0.4469, diffLoss = 2.1891, kgLoss = 0.0113
2025-04-08 19:20:52.484012: Training Step 201/354: batchLoss = 0.6191, diffLoss = 3.0320, kgLoss = 0.0159
2025-04-08 19:20:54.103124: Training Step 202/354: batchLoss = 0.6638, diffLoss = 3.2530, kgLoss = 0.0165
2025-04-08 19:20:55.761482: Training Step 203/354: batchLoss = 0.5501, diffLoss = 2.6889, kgLoss = 0.0154
2025-04-08 19:20:57.385285: Training Step 204/354: batchLoss = 0.4968, diffLoss = 2.4283, kgLoss = 0.0139
2025-04-08 19:20:58.995879: Training Step 205/354: batchLoss = 0.7953, diffLoss = 3.8939, kgLoss = 0.0206
2025-04-08 19:21:00.614250: Training Step 206/354: batchLoss = 0.5440, diffLoss = 2.6639, kgLoss = 0.0140
2025-04-08 19:21:02.239723: Training Step 207/354: batchLoss = 0.4242, diffLoss = 2.0749, kgLoss = 0.0116
2025-04-08 19:21:03.864712: Training Step 208/354: batchLoss = 0.5674, diffLoss = 2.7693, kgLoss = 0.0169
2025-04-08 19:21:05.486255: Training Step 209/354: batchLoss = 0.4654, diffLoss = 2.2690, kgLoss = 0.0145
2025-04-08 19:21:07.112364: Training Step 210/354: batchLoss = 0.5829, diffLoss = 2.8519, kgLoss = 0.0156
2025-04-08 19:21:08.733727: Training Step 211/354: batchLoss = 0.4582, diffLoss = 2.2384, kgLoss = 0.0132
2025-04-08 19:21:10.349891: Training Step 212/354: batchLoss = 0.6078, diffLoss = 2.9774, kgLoss = 0.0153
2025-04-08 19:21:11.962825: Training Step 213/354: batchLoss = 0.6226, diffLoss = 3.0427, kgLoss = 0.0176
2025-04-08 19:21:13.574245: Training Step 214/354: batchLoss = 0.4823, diffLoss = 2.3625, kgLoss = 0.0123
2025-04-08 19:21:15.207717: Training Step 215/354: batchLoss = 0.5536, diffLoss = 2.7117, kgLoss = 0.0141
2025-04-08 19:21:16.861971: Training Step 216/354: batchLoss = 0.5179, diffLoss = 2.5364, kgLoss = 0.0133
2025-04-08 19:21:18.478779: Training Step 217/354: batchLoss = 0.6722, diffLoss = 3.2905, kgLoss = 0.0177
2025-04-08 19:21:20.097393: Training Step 218/354: batchLoss = 0.6710, diffLoss = 3.2882, kgLoss = 0.0167
2025-04-08 19:21:21.713868: Training Step 219/354: batchLoss = 0.4771, diffLoss = 2.3407, kgLoss = 0.0112
2025-04-08 19:21:23.330093: Training Step 220/354: batchLoss = 0.5977, diffLoss = 2.9308, kgLoss = 0.0144
2025-04-08 19:21:24.939986: Training Step 221/354: batchLoss = 0.6148, diffLoss = 3.0088, kgLoss = 0.0164
2025-04-08 19:21:26.554582: Training Step 222/354: batchLoss = 0.4744, diffLoss = 2.3209, kgLoss = 0.0128
2025-04-08 19:21:28.166689: Training Step 223/354: batchLoss = 0.6174, diffLoss = 3.0231, kgLoss = 0.0160
2025-04-08 19:21:29.783706: Training Step 224/354: batchLoss = 0.4341, diffLoss = 2.1283, kgLoss = 0.0106
2025-04-08 19:21:31.402632: Training Step 225/354: batchLoss = 0.5461, diffLoss = 2.6781, kgLoss = 0.0131
2025-04-08 19:21:33.022647: Training Step 226/354: batchLoss = 0.5469, diffLoss = 2.6798, kgLoss = 0.0137
2025-04-08 19:21:34.649644: Training Step 227/354: batchLoss = 0.5331, diffLoss = 2.6076, kgLoss = 0.0145
2025-04-08 19:21:36.270596: Training Step 228/354: batchLoss = 0.6085, diffLoss = 2.9814, kgLoss = 0.0152
2025-04-08 19:21:37.888901: Training Step 229/354: batchLoss = 0.6029, diffLoss = 2.9418, kgLoss = 0.0181
2025-04-08 19:21:39.512685: Training Step 230/354: batchLoss = 0.4720, diffLoss = 2.3073, kgLoss = 0.0131
2025-04-08 19:21:41.130145: Training Step 231/354: batchLoss = 0.5690, diffLoss = 2.7848, kgLoss = 0.0150
2025-04-08 19:21:42.747448: Training Step 232/354: batchLoss = 0.5176, diffLoss = 2.5298, kgLoss = 0.0145
2025-04-08 19:21:44.363697: Training Step 233/354: batchLoss = 0.5490, diffLoss = 2.6897, kgLoss = 0.0138
2025-04-08 19:21:45.985966: Training Step 234/354: batchLoss = 0.7004, diffLoss = 3.4355, kgLoss = 0.0167
2025-04-08 19:21:47.603107: Training Step 235/354: batchLoss = 0.5371, diffLoss = 2.6173, kgLoss = 0.0170
2025-04-08 19:21:49.229059: Training Step 236/354: batchLoss = 0.5707, diffLoss = 2.7938, kgLoss = 0.0149
2025-04-08 19:21:50.853282: Training Step 237/354: batchLoss = 0.5684, diffLoss = 2.7690, kgLoss = 0.0183
2025-04-08 19:21:52.465814: Training Step 238/354: batchLoss = 0.6396, diffLoss = 3.1359, kgLoss = 0.0156
2025-04-08 19:21:54.083947: Training Step 239/354: batchLoss = 0.5526, diffLoss = 2.6998, kgLoss = 0.0158
2025-04-08 19:21:55.702815: Training Step 240/354: batchLoss = 0.5136, diffLoss = 2.5113, kgLoss = 0.0142
2025-04-08 19:21:57.325630: Training Step 241/354: batchLoss = 0.5659, diffLoss = 2.7743, kgLoss = 0.0138
2025-04-08 19:21:58.940420: Training Step 242/354: batchLoss = 0.5531, diffLoss = 2.7052, kgLoss = 0.0150
2025-04-08 19:22:00.553565: Training Step 243/354: batchLoss = 0.5689, diffLoss = 2.7798, kgLoss = 0.0161
2025-04-08 19:22:02.168121: Training Step 244/354: batchLoss = 0.5161, diffLoss = 2.5212, kgLoss = 0.0148
2025-04-08 19:22:03.791300: Training Step 245/354: batchLoss = 0.5124, diffLoss = 2.5093, kgLoss = 0.0132
2025-04-08 19:22:05.413020: Training Step 246/354: batchLoss = 0.4998, diffLoss = 2.4460, kgLoss = 0.0132
2025-04-08 19:22:07.032875: Training Step 247/354: batchLoss = 0.4997, diffLoss = 2.4463, kgLoss = 0.0130
2025-04-08 19:22:08.653729: Training Step 248/354: batchLoss = 0.6446, diffLoss = 3.1595, kgLoss = 0.0159
2025-04-08 19:22:10.274724: Training Step 249/354: batchLoss = 0.5065, diffLoss = 2.4824, kgLoss = 0.0125
2025-04-08 19:22:11.893748: Training Step 250/354: batchLoss = 0.9112, diffLoss = 4.4551, kgLoss = 0.0252
2025-04-08 19:22:13.513075: Training Step 251/354: batchLoss = 0.5440, diffLoss = 2.6633, kgLoss = 0.0142
2025-04-08 19:22:15.130070: Training Step 252/354: batchLoss = 0.5309, diffLoss = 2.5981, kgLoss = 0.0141
2025-04-08 19:22:16.753072: Training Step 253/354: batchLoss = 0.6057, diffLoss = 2.9609, kgLoss = 0.0168
2025-04-08 19:22:18.376359: Training Step 254/354: batchLoss = 0.4311, diffLoss = 2.1070, kgLoss = 0.0121
2025-04-08 19:22:19.993258: Training Step 255/354: batchLoss = 0.4365, diffLoss = 2.1296, kgLoss = 0.0132
2025-04-08 19:22:21.607562: Training Step 256/354: batchLoss = 0.5572, diffLoss = 2.7260, kgLoss = 0.0150
2025-04-08 19:22:23.234196: Training Step 257/354: batchLoss = 0.5957, diffLoss = 2.9242, kgLoss = 0.0136
2025-04-08 19:22:24.850414: Training Step 258/354: batchLoss = 0.5964, diffLoss = 2.9178, kgLoss = 0.0160
2025-04-08 19:22:26.476562: Training Step 259/354: batchLoss = 0.6176, diffLoss = 3.0296, kgLoss = 0.0146
2025-04-08 19:22:28.096769: Training Step 260/354: batchLoss = 0.4580, diffLoss = 2.2405, kgLoss = 0.0124
2025-04-08 19:22:29.714179: Training Step 261/354: batchLoss = 0.6158, diffLoss = 3.0176, kgLoss = 0.0153
2025-04-08 19:22:31.335428: Training Step 262/354: batchLoss = 0.5150, diffLoss = 2.5222, kgLoss = 0.0132
2025-04-08 19:22:32.952756: Training Step 263/354: batchLoss = 0.6473, diffLoss = 3.1673, kgLoss = 0.0173
2025-04-08 19:22:34.570810: Training Step 264/354: batchLoss = 0.4691, diffLoss = 2.2915, kgLoss = 0.0135
2025-04-08 19:22:36.192924: Training Step 265/354: batchLoss = 0.6159, diffLoss = 3.0123, kgLoss = 0.0168
2025-04-08 19:22:37.815262: Training Step 266/354: batchLoss = 0.6249, diffLoss = 3.0624, kgLoss = 0.0155
2025-04-08 19:22:39.434414: Training Step 267/354: batchLoss = 0.5789, diffLoss = 2.8362, kgLoss = 0.0146
2025-04-08 19:22:41.055002: Training Step 268/354: batchLoss = 0.5541, diffLoss = 2.7137, kgLoss = 0.0143
2025-04-08 19:22:42.675754: Training Step 269/354: batchLoss = 0.4898, diffLoss = 2.3839, kgLoss = 0.0163
2025-04-08 19:22:44.293581: Training Step 270/354: batchLoss = 0.5577, diffLoss = 2.7172, kgLoss = 0.0178
2025-04-08 19:22:45.911030: Training Step 271/354: batchLoss = 0.4890, diffLoss = 2.3883, kgLoss = 0.0142
2025-04-08 19:22:47.527996: Training Step 272/354: batchLoss = 0.6654, diffLoss = 3.2550, kgLoss = 0.0179
2025-04-08 19:22:49.148204: Training Step 273/354: batchLoss = 0.5663, diffLoss = 2.7787, kgLoss = 0.0132
2025-04-08 19:22:50.769057: Training Step 274/354: batchLoss = 0.5113, diffLoss = 2.4992, kgLoss = 0.0143
2025-04-08 19:22:52.388147: Training Step 275/354: batchLoss = 0.5781, diffLoss = 2.8275, kgLoss = 0.0158
2025-04-08 19:22:54.005943: Training Step 276/354: batchLoss = 0.4750, diffLoss = 2.3269, kgLoss = 0.0120
2025-04-08 19:22:55.623954: Training Step 277/354: batchLoss = 0.6135, diffLoss = 2.9955, kgLoss = 0.0180
2025-04-08 19:22:57.247696: Training Step 278/354: batchLoss = 0.5624, diffLoss = 2.7474, kgLoss = 0.0161
2025-04-08 19:22:58.866966: Training Step 279/354: batchLoss = 0.5584, diffLoss = 2.7309, kgLoss = 0.0153
2025-04-08 19:23:00.483384: Training Step 280/354: batchLoss = 0.5004, diffLoss = 2.4453, kgLoss = 0.0142
2025-04-08 19:23:02.096805: Training Step 281/354: batchLoss = 0.5386, diffLoss = 2.6366, kgLoss = 0.0142
2025-04-08 19:23:03.714490: Training Step 282/354: batchLoss = 0.5890, diffLoss = 2.8832, kgLoss = 0.0154
2025-04-08 19:23:05.326521: Training Step 283/354: batchLoss = 0.4671, diffLoss = 2.2835, kgLoss = 0.0130
2025-04-08 19:23:06.939281: Training Step 284/354: batchLoss = 0.5486, diffLoss = 2.6887, kgLoss = 0.0136
2025-04-08 19:23:08.559647: Training Step 285/354: batchLoss = 0.4708, diffLoss = 2.3076, kgLoss = 0.0116
2025-04-08 19:23:10.174982: Training Step 286/354: batchLoss = 0.5051, diffLoss = 2.4733, kgLoss = 0.0130
2025-04-08 19:23:11.794652: Training Step 287/354: batchLoss = 0.4960, diffLoss = 2.4237, kgLoss = 0.0141
2025-04-08 19:23:13.416514: Training Step 288/354: batchLoss = 0.5273, diffLoss = 2.5828, kgLoss = 0.0134
2025-04-08 19:23:15.035189: Training Step 289/354: batchLoss = 0.4627, diffLoss = 2.2587, kgLoss = 0.0137
2025-04-08 19:23:16.655186: Training Step 290/354: batchLoss = 0.6344, diffLoss = 3.1094, kgLoss = 0.0156
2025-04-08 19:23:18.268024: Training Step 291/354: batchLoss = 0.4915, diffLoss = 2.4012, kgLoss = 0.0141
2025-04-08 19:23:19.881138: Training Step 292/354: batchLoss = 0.4866, diffLoss = 2.3844, kgLoss = 0.0122
2025-04-08 19:23:21.501374: Training Step 293/354: batchLoss = 0.5330, diffLoss = 2.6101, kgLoss = 0.0137
2025-04-08 19:23:23.116694: Training Step 294/354: batchLoss = 0.5386, diffLoss = 2.6361, kgLoss = 0.0143
2025-04-08 19:23:24.737929: Training Step 295/354: batchLoss = 0.6426, diffLoss = 3.1477, kgLoss = 0.0164
2025-04-08 19:23:26.354546: Training Step 296/354: batchLoss = 0.4981, diffLoss = 2.4383, kgLoss = 0.0131
2025-04-08 19:23:27.974553: Training Step 297/354: batchLoss = 0.5582, diffLoss = 2.7362, kgLoss = 0.0137
2025-04-08 19:23:29.595022: Training Step 298/354: batchLoss = 0.5215, diffLoss = 2.5423, kgLoss = 0.0163
2025-04-08 19:23:31.213978: Training Step 299/354: batchLoss = 0.5331, diffLoss = 2.6086, kgLoss = 0.0142
2025-04-08 19:23:32.830251: Training Step 300/354: batchLoss = 0.4922, diffLoss = 2.4103, kgLoss = 0.0127
2025-04-08 19:23:34.450130: Training Step 301/354: batchLoss = 0.4669, diffLoss = 2.2794, kgLoss = 0.0137
2025-04-08 19:23:36.067291: Training Step 302/354: batchLoss = 0.5234, diffLoss = 2.5612, kgLoss = 0.0140
2025-04-08 19:23:37.681503: Training Step 303/354: batchLoss = 0.5031, diffLoss = 2.4577, kgLoss = 0.0145
2025-04-08 19:23:39.303845: Training Step 304/354: batchLoss = 0.5735, diffLoss = 2.8137, kgLoss = 0.0135
2025-04-08 19:23:40.918282: Training Step 305/354: batchLoss = 0.6309, diffLoss = 3.0871, kgLoss = 0.0168
2025-04-08 19:23:42.543785: Training Step 306/354: batchLoss = 0.5365, diffLoss = 2.6210, kgLoss = 0.0153
2025-04-08 19:23:44.162152: Training Step 307/354: batchLoss = 0.6180, diffLoss = 3.0258, kgLoss = 0.0160
2025-04-08 19:23:45.784301: Training Step 308/354: batchLoss = 0.4624, diffLoss = 2.2637, kgLoss = 0.0120
2025-04-08 19:23:47.401693: Training Step 309/354: batchLoss = 0.4666, diffLoss = 2.2847, kgLoss = 0.0121
2025-04-08 19:23:49.017257: Training Step 310/354: batchLoss = 0.5958, diffLoss = 2.9149, kgLoss = 0.0160
2025-04-08 19:23:50.628765: Training Step 311/354: batchLoss = 0.5550, diffLoss = 2.7200, kgLoss = 0.0137
2025-04-08 19:23:52.245417: Training Step 312/354: batchLoss = 0.5232, diffLoss = 2.5650, kgLoss = 0.0128
2025-04-08 19:23:53.868218: Training Step 313/354: batchLoss = 0.4561, diffLoss = 2.2362, kgLoss = 0.0111
2025-04-08 19:23:55.495004: Training Step 314/354: batchLoss = 0.4752, diffLoss = 2.3183, kgLoss = 0.0144
2025-04-08 19:23:57.110516: Training Step 315/354: batchLoss = 0.5290, diffLoss = 2.5819, kgLoss = 0.0158
2025-04-08 19:23:58.734484: Training Step 316/354: batchLoss = 0.5536, diffLoss = 2.7098, kgLoss = 0.0145
2025-04-08 19:24:00.356273: Training Step 317/354: batchLoss = 0.4698, diffLoss = 2.2975, kgLoss = 0.0129
2025-04-08 19:24:01.979823: Training Step 318/354: batchLoss = 0.5281, diffLoss = 2.5783, kgLoss = 0.0156
2025-04-08 19:24:03.597576: Training Step 319/354: batchLoss = 0.5986, diffLoss = 2.9292, kgLoss = 0.0160
2025-04-08 19:24:05.207696: Training Step 320/354: batchLoss = 0.6031, diffLoss = 2.9514, kgLoss = 0.0160
2025-04-08 19:24:06.824646: Training Step 321/354: batchLoss = 0.4908, diffLoss = 2.3955, kgLoss = 0.0146
2025-04-08 19:24:08.448382: Training Step 322/354: batchLoss = 0.4673, diffLoss = 2.2870, kgLoss = 0.0124
2025-04-08 19:24:10.072830: Training Step 323/354: batchLoss = 0.4892, diffLoss = 2.3866, kgLoss = 0.0148
2025-04-08 19:24:11.697186: Training Step 324/354: batchLoss = 0.4922, diffLoss = 2.4117, kgLoss = 0.0124
2025-04-08 19:24:13.334625: Training Step 325/354: batchLoss = 0.4825, diffLoss = 2.3631, kgLoss = 0.0123
2025-04-08 19:24:14.953353: Training Step 326/354: batchLoss = 0.7772, diffLoss = 3.8100, kgLoss = 0.0190
2025-04-08 19:24:16.577323: Training Step 327/354: batchLoss = 0.6073, diffLoss = 2.9784, kgLoss = 0.0145
2025-04-08 19:24:18.202095: Training Step 328/354: batchLoss = 0.4868, diffLoss = 2.3837, kgLoss = 0.0125
2025-04-08 19:24:19.818259: Training Step 329/354: batchLoss = 0.6082, diffLoss = 2.9763, kgLoss = 0.0161
2025-04-08 19:24:21.429317: Training Step 330/354: batchLoss = 0.7247, diffLoss = 3.5473, kgLoss = 0.0191
2025-04-08 19:24:23.042864: Training Step 331/354: batchLoss = 0.5107, diffLoss = 2.4992, kgLoss = 0.0136
2025-04-08 19:24:24.660591: Training Step 332/354: batchLoss = 0.4509, diffLoss = 2.2011, kgLoss = 0.0134
2025-04-08 19:24:26.284695: Training Step 333/354: batchLoss = 0.5018, diffLoss = 2.4463, kgLoss = 0.0157
2025-04-08 19:24:27.903350: Training Step 334/354: batchLoss = 0.5658, diffLoss = 2.7708, kgLoss = 0.0146
2025-04-08 19:24:29.521416: Training Step 335/354: batchLoss = 0.4833, diffLoss = 2.3659, kgLoss = 0.0126
2025-04-08 19:24:31.140167: Training Step 336/354: batchLoss = 0.6008, diffLoss = 2.9486, kgLoss = 0.0138
2025-04-08 19:24:32.762149: Training Step 337/354: batchLoss = 0.5321, diffLoss = 2.6068, kgLoss = 0.0134
2025-04-08 19:24:34.382216: Training Step 338/354: batchLoss = 0.9363, diffLoss = 4.5933, kgLoss = 0.0221
2025-04-08 19:24:36.000916: Training Step 339/354: batchLoss = 0.5183, diffLoss = 2.5367, kgLoss = 0.0138
2025-04-08 19:24:37.624384: Training Step 340/354: batchLoss = 0.5001, diffLoss = 2.4480, kgLoss = 0.0132
2025-04-08 19:24:39.241620: Training Step 341/354: batchLoss = 0.5470, diffLoss = 2.6786, kgLoss = 0.0140
2025-04-08 19:24:40.858624: Training Step 342/354: batchLoss = 0.5555, diffLoss = 2.7242, kgLoss = 0.0133
2025-04-08 19:24:42.476954: Training Step 343/354: batchLoss = 0.4909, diffLoss = 2.4006, kgLoss = 0.0134
2025-04-08 19:24:44.094455: Training Step 344/354: batchLoss = 0.4493, diffLoss = 2.1968, kgLoss = 0.0125
2025-04-08 19:24:45.713567: Training Step 345/354: batchLoss = 0.4926, diffLoss = 2.4099, kgLoss = 0.0133
2025-04-08 19:24:47.331437: Training Step 346/354: batchLoss = 0.5578, diffLoss = 2.7240, kgLoss = 0.0162
2025-04-08 19:24:48.953041: Training Step 347/354: batchLoss = 0.5023, diffLoss = 2.4601, kgLoss = 0.0128
2025-04-08 19:24:50.574433: Training Step 348/354: batchLoss = 0.5356, diffLoss = 2.6076, kgLoss = 0.0177
2025-04-08 19:24:52.189656: Training Step 349/354: batchLoss = 0.5186, diffLoss = 2.5400, kgLoss = 0.0132
2025-04-08 19:24:53.802834: Training Step 350/354: batchLoss = 0.5402, diffLoss = 2.6434, kgLoss = 0.0144
2025-04-08 19:24:55.416947: Training Step 351/354: batchLoss = 0.5649, diffLoss = 2.7716, kgLoss = 0.0133
2025-04-08 19:24:57.013415: Training Step 352/354: batchLoss = 0.5707, diffLoss = 2.7934, kgLoss = 0.0150
2025-04-08 19:24:58.420953: Training Step 353/354: batchLoss = 0.4459, diffLoss = 2.1757, kgLoss = 0.0135
2025-04-08 19:24:58.512328: 
2025-04-08 19:24:58.512916: Epoch 32/1000, Train: epLoss = 0.9817, epDfLoss = 4.8041, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 19:24:59.830423: Steps 0/138: batch_recall = 47.70, batch_ndcg = 26.89 
2025-04-08 19:25:01.166377: Steps 1/138: batch_recall = 48.21, batch_ndcg = 28.28 
2025-04-08 19:25:02.486921: Steps 2/138: batch_recall = 57.75, batch_ndcg = 35.24 
2025-04-08 19:25:03.823956: Steps 3/138: batch_recall = 58.69, batch_ndcg = 34.12 
2025-04-08 19:25:05.129181: Steps 4/138: batch_recall = 66.63, batch_ndcg = 40.86 
2025-04-08 19:25:06.441105: Steps 5/138: batch_recall = 59.02, batch_ndcg = 31.63 
2025-04-08 19:25:07.750226: Steps 6/138: batch_recall = 51.68, batch_ndcg = 32.21 
2025-04-08 19:25:09.050209: Steps 7/138: batch_recall = 63.16, batch_ndcg = 41.87 
2025-04-08 19:25:10.348631: Steps 8/138: batch_recall = 61.48, batch_ndcg = 38.60 
2025-04-08 19:25:11.657372: Steps 9/138: batch_recall = 58.54, batch_ndcg = 35.13 
2025-04-08 19:25:12.944909: Steps 10/138: batch_recall = 57.57, batch_ndcg = 33.45 
2025-04-08 19:25:14.240793: Steps 11/138: batch_recall = 55.07, batch_ndcg = 32.68 
2025-04-08 19:25:15.543153: Steps 12/138: batch_recall = 51.24, batch_ndcg = 29.20 
2025-04-08 19:25:16.838870: Steps 13/138: batch_recall = 54.06, batch_ndcg = 32.02 
2025-04-08 19:25:18.140269: Steps 14/138: batch_recall = 54.81, batch_ndcg = 31.81 
2025-04-08 19:25:19.441355: Steps 15/138: batch_recall = 47.74, batch_ndcg = 29.75 
2025-04-08 19:25:20.745266: Steps 16/138: batch_recall = 60.26, batch_ndcg = 33.82 
2025-04-08 19:25:22.041489: Steps 17/138: batch_recall = 58.29, batch_ndcg = 33.70 
2025-04-08 19:25:23.343519: Steps 18/138: batch_recall = 53.07, batch_ndcg = 33.64 
2025-04-08 19:25:24.645221: Steps 19/138: batch_recall = 54.93, batch_ndcg = 33.43 
2025-04-08 19:25:25.936657: Steps 20/138: batch_recall = 63.97, batch_ndcg = 36.71 
2025-04-08 19:25:27.239620: Steps 21/138: batch_recall = 69.04, batch_ndcg = 41.04 
2025-04-08 19:25:28.530895: Steps 22/138: batch_recall = 54.61, batch_ndcg = 32.15 
2025-04-08 19:25:29.827963: Steps 23/138: batch_recall = 49.92, batch_ndcg = 30.17 
2025-04-08 19:25:31.137750: Steps 24/138: batch_recall = 55.71, batch_ndcg = 31.07 
2025-04-08 19:25:32.439342: Steps 25/138: batch_recall = 60.89, batch_ndcg = 35.07 
2025-04-08 19:25:33.742936: Steps 26/138: batch_recall = 58.12, batch_ndcg = 32.91 
2025-04-08 19:25:35.038216: Steps 27/138: batch_recall = 56.06, batch_ndcg = 32.22 
2025-04-08 19:25:36.334898: Steps 28/138: batch_recall = 58.90, batch_ndcg = 33.49 
2025-04-08 19:25:37.636157: Steps 29/138: batch_recall = 60.66, batch_ndcg = 31.78 
2025-04-08 19:25:38.935744: Steps 30/138: batch_recall = 56.51, batch_ndcg = 33.46 
2025-04-08 19:25:40.224016: Steps 31/138: batch_recall = 50.16, batch_ndcg = 28.05 
2025-04-08 19:25:41.507657: Steps 32/138: batch_recall = 53.74, batch_ndcg = 31.96 
2025-04-08 19:25:42.797699: Steps 33/138: batch_recall = 60.19, batch_ndcg = 33.63 
2025-04-08 19:25:44.079893: Steps 34/138: batch_recall = 54.85, batch_ndcg = 30.00 
2025-04-08 19:25:45.372798: Steps 35/138: batch_recall = 51.70, batch_ndcg = 29.96 
2025-04-08 19:25:46.665161: Steps 36/138: batch_recall = 46.10, batch_ndcg = 26.65 
2025-04-08 19:25:47.964542: Steps 37/138: batch_recall = 59.42, batch_ndcg = 34.61 
2025-04-08 19:25:49.264445: Steps 38/138: batch_recall = 56.35, batch_ndcg = 32.11 
2025-04-08 19:25:50.562625: Steps 39/138: batch_recall = 69.22, batch_ndcg = 38.67 
2025-04-08 19:25:51.855736: Steps 40/138: batch_recall = 59.01, batch_ndcg = 30.12 
2025-04-08 19:25:53.148937: Steps 41/138: batch_recall = 59.45, batch_ndcg = 33.84 
2025-04-08 19:25:54.436960: Steps 42/138: batch_recall = 53.94, batch_ndcg = 30.61 
2025-04-08 19:25:55.720387: Steps 43/138: batch_recall = 57.22, batch_ndcg = 36.39 
2025-04-08 19:25:57.012160: Steps 44/138: batch_recall = 56.71, batch_ndcg = 30.70 
2025-04-08 19:25:58.302580: Steps 45/138: batch_recall = 63.69, batch_ndcg = 35.95 
2025-04-08 19:25:59.584391: Steps 46/138: batch_recall = 61.57, batch_ndcg = 36.53 
2025-04-08 19:26:00.871232: Steps 47/138: batch_recall = 53.47, batch_ndcg = 31.86 
2025-04-08 19:26:02.160672: Steps 48/138: batch_recall = 62.17, batch_ndcg = 35.74 
2025-04-08 19:26:03.457891: Steps 49/138: batch_recall = 64.92, batch_ndcg = 37.79 
2025-04-08 19:26:04.747511: Steps 50/138: batch_recall = 58.50, batch_ndcg = 31.01 
2025-04-08 19:26:06.034235: Steps 51/138: batch_recall = 64.19, batch_ndcg = 36.58 
2025-04-08 19:26:07.319654: Steps 52/138: batch_recall = 65.05, batch_ndcg = 42.58 
2025-04-08 19:26:08.593674: Steps 53/138: batch_recall = 64.55, batch_ndcg = 34.61 
2025-04-08 19:26:09.874818: Steps 54/138: batch_recall = 69.27, batch_ndcg = 38.42 
2025-04-08 19:26:11.158333: Steps 55/138: batch_recall = 63.17, batch_ndcg = 35.38 
2025-04-08 19:26:12.439631: Steps 56/138: batch_recall = 58.52, batch_ndcg = 34.75 
2025-04-08 19:26:13.722183: Steps 57/138: batch_recall = 57.71, batch_ndcg = 32.42 
2025-04-08 19:26:14.999544: Steps 58/138: batch_recall = 68.11, batch_ndcg = 35.90 
2025-04-08 19:26:16.274191: Steps 59/138: batch_recall = 64.00, batch_ndcg = 38.50 
2025-04-08 19:26:17.555060: Steps 60/138: batch_recall = 67.28, batch_ndcg = 37.59 
2025-04-08 19:26:18.838437: Steps 61/138: batch_recall = 63.14, batch_ndcg = 35.18 
2025-04-08 19:26:20.130712: Steps 62/138: batch_recall = 83.38, batch_ndcg = 45.11 
2025-04-08 19:26:21.428522: Steps 63/138: batch_recall = 74.45, batch_ndcg = 42.54 
2025-04-08 19:26:22.720055: Steps 64/138: batch_recall = 60.26, batch_ndcg = 31.98 
2025-04-08 19:26:24.012649: Steps 65/138: batch_recall = 86.60, batch_ndcg = 47.93 
2025-04-08 19:26:25.303887: Steps 66/138: batch_recall = 69.39, batch_ndcg = 41.24 
2025-04-08 19:26:26.599015: Steps 67/138: batch_recall = 75.05, batch_ndcg = 46.01 
2025-04-08 19:26:27.883862: Steps 68/138: batch_recall = 63.21, batch_ndcg = 34.24 
2025-04-08 19:26:29.169958: Steps 69/138: batch_recall = 85.47, batch_ndcg = 51.48 
2025-04-08 19:26:30.440520: Steps 70/138: batch_recall = 78.53, batch_ndcg = 44.89 
2025-04-08 19:26:31.717857: Steps 71/138: batch_recall = 87.12, batch_ndcg = 51.52 
2025-04-08 19:26:32.997734: Steps 72/138: batch_recall = 82.79, batch_ndcg = 48.02 
2025-04-08 19:26:34.272775: Steps 73/138: batch_recall = 83.34, batch_ndcg = 46.43 
2025-04-08 19:26:35.552465: Steps 74/138: batch_recall = 77.16, batch_ndcg = 48.11 
2025-04-08 19:26:36.833656: Steps 75/138: batch_recall = 81.73, batch_ndcg = 48.97 
2025-04-08 19:26:38.115066: Steps 76/138: batch_recall = 93.49, batch_ndcg = 54.93 
2025-04-08 19:26:39.396484: Steps 77/138: batch_recall = 88.64, batch_ndcg = 51.50 
2025-04-08 19:26:40.676180: Steps 78/138: batch_recall = 89.45, batch_ndcg = 47.87 
2025-04-08 19:26:41.959912: Steps 79/138: batch_recall = 87.49, batch_ndcg = 48.97 
2025-04-08 19:26:43.242273: Steps 80/138: batch_recall = 71.46, batch_ndcg = 38.62 
2025-04-08 19:26:44.522610: Steps 81/138: batch_recall = 80.74, batch_ndcg = 47.85 
2025-04-08 19:26:45.797939: Steps 82/138: batch_recall = 86.06, batch_ndcg = 53.50 
2025-04-08 19:26:47.076468: Steps 83/138: batch_recall = 83.42, batch_ndcg = 48.09 
2025-04-08 19:26:48.341966: Steps 84/138: batch_recall = 96.67, batch_ndcg = 56.77 
2025-04-08 19:26:49.612974: Steps 85/138: batch_recall = 107.45, batch_ndcg = 61.38 
2025-04-08 19:26:50.897956: Steps 86/138: batch_recall = 119.62, batch_ndcg = 71.44 
2025-04-08 19:26:52.182552: Steps 87/138: batch_recall = 101.76, batch_ndcg = 54.99 
2025-04-08 19:26:53.463905: Steps 88/138: batch_recall = 105.52, batch_ndcg = 60.49 
2025-04-08 19:26:54.741601: Steps 89/138: batch_recall = 119.21, batch_ndcg = 68.35 
2025-04-08 19:26:56.028262: Steps 90/138: batch_recall = 101.22, batch_ndcg = 57.00 
2025-04-08 19:26:57.304454: Steps 91/138: batch_recall = 116.62, batch_ndcg = 65.13 
2025-04-08 19:26:58.593499: Steps 92/138: batch_recall = 122.05, batch_ndcg = 66.24 
2025-04-08 19:26:59.870320: Steps 93/138: batch_recall = 122.45, batch_ndcg = 69.68 
2025-04-08 19:27:01.136132: Steps 94/138: batch_recall = 121.86, batch_ndcg = 65.52 
2025-04-08 19:27:02.409573: Steps 95/138: batch_recall = 115.93, batch_ndcg = 67.83 
2025-04-08 19:27:03.684180: Steps 96/138: batch_recall = 125.01, batch_ndcg = 76.71 
2025-04-08 19:27:04.951507: Steps 97/138: batch_recall = 135.99, batch_ndcg = 87.72 
2025-04-08 19:27:06.230795: Steps 98/138: batch_recall = 105.76, batch_ndcg = 62.79 
2025-04-08 19:27:07.512476: Steps 99/138: batch_recall = 123.97, batch_ndcg = 71.37 
2025-04-08 19:27:08.791579: Steps 100/138: batch_recall = 131.13, batch_ndcg = 72.21 
2025-04-08 19:27:10.067343: Steps 101/138: batch_recall = 127.84, batch_ndcg = 69.99 
2025-04-08 19:27:11.343534: Steps 102/138: batch_recall = 119.50, batch_ndcg = 69.59 
2025-04-08 19:27:12.623660: Steps 103/138: batch_recall = 140.36, batch_ndcg = 80.06 
2025-04-08 19:27:13.901705: Steps 104/138: batch_recall = 134.76, batch_ndcg = 78.82 
2025-04-08 19:27:15.182778: Steps 105/138: batch_recall = 120.90, batch_ndcg = 67.79 
2025-04-08 19:27:16.457565: Steps 106/138: batch_recall = 106.35, batch_ndcg = 60.61 
2025-04-08 19:27:17.738070: Steps 107/138: batch_recall = 114.13, batch_ndcg = 65.62 
2025-04-08 19:27:19.016331: Steps 108/138: batch_recall = 118.19, batch_ndcg = 71.84 
2025-04-08 19:27:20.286509: Steps 109/138: batch_recall = 136.33, batch_ndcg = 76.28 
2025-04-08 19:27:21.561116: Steps 110/138: batch_recall = 122.10, batch_ndcg = 64.33 
2025-04-08 19:27:22.838788: Steps 111/138: batch_recall = 135.73, batch_ndcg = 85.13 
2025-04-08 19:27:24.127564: Steps 112/138: batch_recall = 153.44, batch_ndcg = 88.80 
2025-04-08 19:27:25.407602: Steps 113/138: batch_recall = 119.16, batch_ndcg = 68.27 
2025-04-08 19:27:26.688111: Steps 114/138: batch_recall = 121.77, batch_ndcg = 71.55 
2025-04-08 19:27:27.964719: Steps 115/138: batch_recall = 120.26, batch_ndcg = 63.81 
2025-04-08 19:27:29.244637: Steps 116/138: batch_recall = 123.47, batch_ndcg = 66.43 
2025-04-08 19:27:30.515334: Steps 117/138: batch_recall = 112.08, batch_ndcg = 65.50 
2025-04-08 19:27:31.775170: Steps 118/138: batch_recall = 121.88, batch_ndcg = 68.13 
2025-04-08 19:27:33.047206: Steps 119/138: batch_recall = 138.77, batch_ndcg = 76.00 
2025-04-08 19:27:34.323833: Steps 120/138: batch_recall = 121.89, batch_ndcg = 70.15 
2025-04-08 19:27:35.586851: Steps 121/138: batch_recall = 140.69, batch_ndcg = 76.31 
2025-04-08 19:27:36.857709: Steps 122/138: batch_recall = 146.74, batch_ndcg = 80.12 
2025-04-08 19:27:38.133980: Steps 123/138: batch_recall = 128.17, batch_ndcg = 73.98 
2025-04-08 19:27:39.407221: Steps 124/138: batch_recall = 154.16, batch_ndcg = 92.41 
2025-04-08 19:27:40.681098: Steps 125/138: batch_recall = 134.63, batch_ndcg = 74.82 
2025-04-08 19:27:41.956579: Steps 126/138: batch_recall = 161.77, batch_ndcg = 87.91 
2025-04-08 19:27:43.231651: Steps 127/138: batch_recall = 144.59, batch_ndcg = 82.37 
2025-04-08 19:27:44.507022: Steps 128/138: batch_recall = 131.50, batch_ndcg = 72.78 
2025-04-08 19:27:45.782705: Steps 129/138: batch_recall = 160.27, batch_ndcg = 92.07 
2025-04-08 19:27:47.056277: Steps 130/138: batch_recall = 133.10, batch_ndcg = 69.92 
2025-04-08 19:27:48.319672: Steps 131/138: batch_recall = 150.55, batch_ndcg = 87.56 
2025-04-08 19:27:49.586556: Steps 132/138: batch_recall = 151.35, batch_ndcg = 84.92 
2025-04-08 19:27:50.856600: Steps 133/138: batch_recall = 145.60, batch_ndcg = 85.55 
2025-04-08 19:27:52.125221: Steps 134/138: batch_recall = 145.88, batch_ndcg = 83.90 
2025-04-08 19:27:53.395473: Steps 135/138: batch_recall = 171.36, batch_ndcg = 96.52 
2025-04-08 19:27:54.663723: Steps 136/138: batch_recall = 152.56, batch_ndcg = 79.44 
2025-04-08 19:27:55.942901: Steps 137/138: batch_recall = 137.84, batch_ndcg = 85.92 
2025-04-08 19:27:55.943452: Epoch 32/1000, Test: Recall = 0.1751, NDCG = 0.1004  

2025-04-08 19:27:57.716667: Training Step 0/354: batchLoss = 0.5460, diffLoss = 2.6783, kgLoss = 0.0130
2025-04-08 19:27:59.337502: Training Step 1/354: batchLoss = 0.4601, diffLoss = 2.2548, kgLoss = 0.0114
2025-04-08 19:28:00.957812: Training Step 2/354: batchLoss = 0.6533, diffLoss = 3.1924, kgLoss = 0.0186
2025-04-08 19:28:02.583210: Training Step 3/354: batchLoss = 0.8901, diffLoss = 4.3615, kgLoss = 0.0223
2025-04-08 19:28:04.202016: Training Step 4/354: batchLoss = 0.4789, diffLoss = 2.3268, kgLoss = 0.0169
2025-04-08 19:28:05.811666: Training Step 5/354: batchLoss = 0.7239, diffLoss = 3.5474, kgLoss = 0.0180
2025-04-08 19:28:07.423724: Training Step 6/354: batchLoss = 0.5236, diffLoss = 2.5605, kgLoss = 0.0144
2025-04-08 19:28:09.034465: Training Step 7/354: batchLoss = 0.6963, diffLoss = 3.3969, kgLoss = 0.0211
2025-04-08 19:28:10.649565: Training Step 8/354: batchLoss = 0.6572, diffLoss = 3.2191, kgLoss = 0.0168
2025-04-08 19:28:12.269100: Training Step 9/354: batchLoss = 0.5568, diffLoss = 2.7250, kgLoss = 0.0147
2025-04-08 19:28:13.892059: Training Step 10/354: batchLoss = 0.5221, diffLoss = 2.5607, kgLoss = 0.0125
2025-04-08 19:28:15.509091: Training Step 11/354: batchLoss = 0.5808, diffLoss = 2.8505, kgLoss = 0.0134
2025-04-08 19:28:17.130901: Training Step 12/354: batchLoss = 0.4640, diffLoss = 2.2672, kgLoss = 0.0132
2025-04-08 19:28:18.754015: Training Step 13/354: batchLoss = 0.6970, diffLoss = 3.4160, kgLoss = 0.0173
2025-04-08 19:28:20.370547: Training Step 14/354: batchLoss = 0.4941, diffLoss = 2.4181, kgLoss = 0.0131
2025-04-08 19:28:21.984556: Training Step 15/354: batchLoss = 0.5328, diffLoss = 2.6089, kgLoss = 0.0138
2025-04-08 19:28:23.600057: Training Step 16/354: batchLoss = 0.5871, diffLoss = 2.8751, kgLoss = 0.0150
2025-04-08 19:28:25.213551: Training Step 17/354: batchLoss = 0.5820, diffLoss = 2.8504, kgLoss = 0.0149
2025-04-08 19:28:26.834000: Training Step 18/354: batchLoss = 0.4518, diffLoss = 2.2032, kgLoss = 0.0139
2025-04-08 19:28:28.453361: Training Step 19/354: batchLoss = 0.4784, diffLoss = 2.3418, kgLoss = 0.0126
2025-04-08 19:28:30.087940: Training Step 20/354: batchLoss = 0.6067, diffLoss = 2.9706, kgLoss = 0.0157
2025-04-08 19:28:31.711927: Training Step 21/354: batchLoss = 0.6080, diffLoss = 2.9758, kgLoss = 0.0161
2025-04-08 19:28:33.334690: Training Step 22/354: batchLoss = 0.5240, diffLoss = 2.5618, kgLoss = 0.0145
2025-04-08 19:28:34.959919: Training Step 23/354: batchLoss = 1.5801, diffLoss = 7.7385, kgLoss = 0.0405
2025-04-08 19:28:36.581103: Training Step 24/354: batchLoss = 0.6240, diffLoss = 3.0563, kgLoss = 0.0159
2025-04-08 19:28:38.205145: Training Step 25/354: batchLoss = 0.4631, diffLoss = 2.2638, kgLoss = 0.0130
2025-04-08 19:28:39.818917: Training Step 26/354: batchLoss = 0.6045, diffLoss = 2.9532, kgLoss = 0.0173
2025-04-08 19:28:41.434577: Training Step 27/354: batchLoss = 0.5974, diffLoss = 2.9263, kgLoss = 0.0151
2025-04-08 19:28:43.061917: Training Step 28/354: batchLoss = 0.5171, diffLoss = 2.5289, kgLoss = 0.0141
2025-04-08 19:28:44.689755: Training Step 29/354: batchLoss = 0.5236, diffLoss = 2.5653, kgLoss = 0.0132
2025-04-08 19:28:46.315784: Training Step 30/354: batchLoss = 0.5377, diffLoss = 2.6300, kgLoss = 0.0147
2025-04-08 19:28:47.939780: Training Step 31/354: batchLoss = 0.4871, diffLoss = 2.3815, kgLoss = 0.0136
2025-04-08 19:28:49.559146: Training Step 32/354: batchLoss = 0.6356, diffLoss = 3.1164, kgLoss = 0.0154
2025-04-08 19:28:51.185153: Training Step 33/354: batchLoss = 0.5221, diffLoss = 2.5329, kgLoss = 0.0194
2025-04-08 19:28:52.800328: Training Step 34/354: batchLoss = 0.5952, diffLoss = 2.9166, kgLoss = 0.0149
2025-04-08 19:28:54.422073: Training Step 35/354: batchLoss = 0.5496, diffLoss = 2.6879, kgLoss = 0.0150
2025-04-08 19:28:56.035332: Training Step 36/354: batchLoss = 0.4317, diffLoss = 2.1080, kgLoss = 0.0126
2025-04-08 19:28:57.652921: Training Step 37/354: batchLoss = 0.4850, diffLoss = 2.3644, kgLoss = 0.0151
2025-04-08 19:28:59.275054: Training Step 38/354: batchLoss = 0.6042, diffLoss = 2.9631, kgLoss = 0.0144
2025-04-08 19:29:00.896619: Training Step 39/354: batchLoss = 0.5197, diffLoss = 2.5454, kgLoss = 0.0132
2025-04-08 19:29:02.525205: Training Step 40/354: batchLoss = 0.6054, diffLoss = 2.9622, kgLoss = 0.0162
2025-04-08 19:29:04.146184: Training Step 41/354: batchLoss = 0.7005, diffLoss = 3.4290, kgLoss = 0.0184
2025-04-08 19:29:05.766508: Training Step 42/354: batchLoss = 0.7103, diffLoss = 3.4821, kgLoss = 0.0174
2025-04-08 19:29:07.385941: Training Step 43/354: batchLoss = 0.4787, diffLoss = 2.3364, kgLoss = 0.0143
2025-04-08 19:29:08.999854: Training Step 44/354: batchLoss = 0.7188, diffLoss = 3.5289, kgLoss = 0.0163
2025-04-08 19:29:10.620527: Training Step 45/354: batchLoss = 0.5967, diffLoss = 2.9238, kgLoss = 0.0149
2025-04-08 19:29:12.237841: Training Step 46/354: batchLoss = 0.4702, diffLoss = 2.3011, kgLoss = 0.0125
2025-04-08 19:29:13.860388: Training Step 47/354: batchLoss = 0.6865, diffLoss = 3.3623, kgLoss = 0.0176
2025-04-08 19:29:15.492438: Training Step 48/354: batchLoss = 0.5244, diffLoss = 2.5614, kgLoss = 0.0152
2025-04-08 19:29:17.112195: Training Step 49/354: batchLoss = 0.5331, diffLoss = 2.6106, kgLoss = 0.0138
2025-04-08 19:29:18.735108: Training Step 50/354: batchLoss = 0.6300, diffLoss = 3.0815, kgLoss = 0.0171
2025-04-08 19:29:20.366884: Training Step 51/354: batchLoss = 0.5505, diffLoss = 2.6982, kgLoss = 0.0136
2025-04-08 19:29:21.992637: Training Step 52/354: batchLoss = 0.5758, diffLoss = 2.8178, kgLoss = 0.0153
2025-04-08 19:29:23.611535: Training Step 53/354: batchLoss = 0.4407, diffLoss = 2.1543, kgLoss = 0.0123
2025-04-08 19:29:25.225859: Training Step 54/354: batchLoss = 0.4753, diffLoss = 2.3274, kgLoss = 0.0123
2025-04-08 19:29:26.838893: Training Step 55/354: batchLoss = 0.6503, diffLoss = 3.1894, kgLoss = 0.0156
2025-04-08 19:29:28.452085: Training Step 56/354: batchLoss = 0.7112, diffLoss = 3.4874, kgLoss = 0.0172
2025-04-08 19:29:30.065795: Training Step 57/354: batchLoss = 0.4648, diffLoss = 2.2716, kgLoss = 0.0131
2025-04-08 19:29:31.684977: Training Step 58/354: batchLoss = 0.5424, diffLoss = 2.6527, kgLoss = 0.0148
2025-04-08 19:29:33.305487: Training Step 59/354: batchLoss = 0.4842, diffLoss = 2.3561, kgLoss = 0.0162
2025-04-08 19:29:34.927206: Training Step 60/354: batchLoss = 0.6245, diffLoss = 3.0603, kgLoss = 0.0155
2025-04-08 19:29:36.549139: Training Step 61/354: batchLoss = 0.4555, diffLoss = 2.2281, kgLoss = 0.0123
2025-04-08 19:29:38.173043: Training Step 62/354: batchLoss = 0.6085, diffLoss = 2.9786, kgLoss = 0.0159
2025-04-08 19:29:39.792365: Training Step 63/354: batchLoss = 0.6370, diffLoss = 3.1215, kgLoss = 0.0158
2025-04-08 19:29:41.407531: Training Step 64/354: batchLoss = 0.5372, diffLoss = 2.6304, kgLoss = 0.0139
2025-04-08 19:29:43.022382: Training Step 65/354: batchLoss = 0.5224, diffLoss = 2.5581, kgLoss = 0.0135
2025-04-08 19:29:44.639243: Training Step 66/354: batchLoss = 0.5439, diffLoss = 2.6668, kgLoss = 0.0131
2025-04-08 19:29:46.259927: Training Step 67/354: batchLoss = 0.5777, diffLoss = 2.8326, kgLoss = 0.0140
2025-04-08 19:29:47.878411: Training Step 68/354: batchLoss = 0.4983, diffLoss = 2.4378, kgLoss = 0.0134
2025-04-08 19:29:49.499706: Training Step 69/354: batchLoss = 0.5655, diffLoss = 2.7645, kgLoss = 0.0157
2025-04-08 19:29:51.122634: Training Step 70/354: batchLoss = 0.5502, diffLoss = 2.6893, kgLoss = 0.0154
2025-04-08 19:29:52.741815: Training Step 71/354: batchLoss = 0.5418, diffLoss = 2.6531, kgLoss = 0.0139
2025-04-08 19:29:54.359081: Training Step 72/354: batchLoss = 0.5807, diffLoss = 2.8441, kgLoss = 0.0149
2025-04-08 19:29:55.978908: Training Step 73/354: batchLoss = 0.4839, diffLoss = 2.3687, kgLoss = 0.0126
2025-04-08 19:29:57.597237: Training Step 74/354: batchLoss = 0.4625, diffLoss = 2.2583, kgLoss = 0.0135
2025-04-08 19:29:59.213345: Training Step 75/354: batchLoss = 0.6992, diffLoss = 3.4283, kgLoss = 0.0170
2025-04-08 19:30:00.831661: Training Step 76/354: batchLoss = 0.4713, diffLoss = 2.3090, kgLoss = 0.0119
2025-04-08 19:30:02.439107: Training Step 77/354: batchLoss = 0.6017, diffLoss = 2.9491, kgLoss = 0.0149
2025-04-08 19:30:04.064050: Training Step 78/354: batchLoss = 0.5703, diffLoss = 2.7896, kgLoss = 0.0154
2025-04-08 19:30:05.682855: Training Step 79/354: batchLoss = 0.6653, diffLoss = 3.2612, kgLoss = 0.0163
2025-04-08 19:30:07.301378: Training Step 80/354: batchLoss = 0.4294, diffLoss = 2.1002, kgLoss = 0.0117
2025-04-08 19:30:08.917508: Training Step 81/354: batchLoss = 0.5006, diffLoss = 2.4477, kgLoss = 0.0138
2025-04-08 19:30:10.537145: Training Step 82/354: batchLoss = 0.6012, diffLoss = 2.9439, kgLoss = 0.0155
2025-04-08 19:30:12.159279: Training Step 83/354: batchLoss = 0.6276, diffLoss = 3.0750, kgLoss = 0.0157
2025-04-08 19:30:13.778548: Training Step 84/354: batchLoss = 0.5652, diffLoss = 2.7675, kgLoss = 0.0146
2025-04-08 19:30:15.390291: Training Step 85/354: batchLoss = 0.4988, diffLoss = 2.4350, kgLoss = 0.0147
2025-04-08 19:30:17.002948: Training Step 86/354: batchLoss = 0.6907, diffLoss = 3.3775, kgLoss = 0.0190
2025-04-08 19:30:18.621654: Training Step 87/354: batchLoss = 0.6093, diffLoss = 2.9814, kgLoss = 0.0163
2025-04-08 19:30:20.245602: Training Step 88/354: batchLoss = 0.4749, diffLoss = 2.3209, kgLoss = 0.0134
2025-04-08 19:30:21.871474: Training Step 89/354: batchLoss = 0.4532, diffLoss = 2.2122, kgLoss = 0.0135
2025-04-08 19:30:23.496353: Training Step 90/354: batchLoss = 0.6183, diffLoss = 3.0319, kgLoss = 0.0150
2025-04-08 19:30:25.122824: Training Step 91/354: batchLoss = 0.6759, diffLoss = 3.3194, kgLoss = 0.0150
2025-04-08 19:30:26.743258: Training Step 92/354: batchLoss = 0.5670, diffLoss = 2.7788, kgLoss = 0.0141
2025-04-08 19:30:28.364035: Training Step 93/354: batchLoss = 0.4873, diffLoss = 2.3778, kgLoss = 0.0147
2025-04-08 19:30:29.987170: Training Step 94/354: batchLoss = 0.5351, diffLoss = 2.6175, kgLoss = 0.0145
2025-04-08 19:30:31.596873: Training Step 95/354: batchLoss = 0.5818, diffLoss = 2.8521, kgLoss = 0.0142
2025-04-08 19:30:33.213071: Training Step 96/354: batchLoss = 0.4646, diffLoss = 2.2736, kgLoss = 0.0123
2025-04-08 19:30:34.919227: Training Step 97/354: batchLoss = 0.5457, diffLoss = 2.6670, kgLoss = 0.0154
2025-04-08 19:30:36.672290: Training Step 98/354: batchLoss = 0.6934, diffLoss = 3.3982, kgLoss = 0.0172
2025-04-08 19:30:38.289427: Training Step 99/354: batchLoss = 0.6537, diffLoss = 3.2082, kgLoss = 0.0151
2025-04-08 19:30:39.911487: Training Step 100/354: batchLoss = 0.7456, diffLoss = 3.6510, kgLoss = 0.0193
2025-04-08 19:30:41.531922: Training Step 101/354: batchLoss = 0.6053, diffLoss = 2.9629, kgLoss = 0.0159
2025-04-08 19:30:43.152568: Training Step 102/354: batchLoss = 0.6090, diffLoss = 2.9803, kgLoss = 0.0161
2025-04-08 19:30:44.765718: Training Step 103/354: batchLoss = 0.5726, diffLoss = 2.8047, kgLoss = 0.0146
2025-04-08 19:30:46.374156: Training Step 104/354: batchLoss = 0.4375, diffLoss = 2.1399, kgLoss = 0.0119
2025-04-08 19:30:47.986469: Training Step 105/354: batchLoss = 0.5318, diffLoss = 2.6032, kgLoss = 0.0140
2025-04-08 19:30:49.603050: Training Step 106/354: batchLoss = 0.5289, diffLoss = 2.5897, kgLoss = 0.0137
2025-04-08 19:30:51.223421: Training Step 107/354: batchLoss = 0.4582, diffLoss = 2.2434, kgLoss = 0.0119
2025-04-08 19:30:52.844332: Training Step 108/354: batchLoss = 0.4835, diffLoss = 2.3656, kgLoss = 0.0129
2025-04-08 19:30:54.464614: Training Step 109/354: batchLoss = 0.5315, diffLoss = 2.6003, kgLoss = 0.0144
2025-04-08 19:30:56.092858: Training Step 110/354: batchLoss = 0.4349, diffLoss = 2.1299, kgLoss = 0.0111
2025-04-08 19:30:57.711490: Training Step 111/354: batchLoss = 0.5228, diffLoss = 2.5589, kgLoss = 0.0137
2025-04-08 19:30:59.335350: Training Step 112/354: batchLoss = 0.4792, diffLoss = 2.3474, kgLoss = 0.0122
2025-04-08 19:31:00.958555: Training Step 113/354: batchLoss = 0.6151, diffLoss = 3.0152, kgLoss = 0.0151
2025-04-08 19:31:02.580167: Training Step 114/354: batchLoss = 0.5803, diffLoss = 2.8414, kgLoss = 0.0151
2025-04-08 19:31:04.197770: Training Step 115/354: batchLoss = 0.5231, diffLoss = 2.5669, kgLoss = 0.0122
2025-04-08 19:31:05.820125: Training Step 116/354: batchLoss = 0.6442, diffLoss = 3.1599, kgLoss = 0.0153
2025-04-08 19:31:07.442373: Training Step 117/354: batchLoss = 0.6239, diffLoss = 3.0561, kgLoss = 0.0159
2025-04-08 19:31:09.066673: Training Step 118/354: batchLoss = 0.5586, diffLoss = 2.7280, kgLoss = 0.0163
2025-04-08 19:31:10.685702: Training Step 119/354: batchLoss = 0.6233, diffLoss = 3.0521, kgLoss = 0.0161
2025-04-08 19:31:12.306555: Training Step 120/354: batchLoss = 0.5534, diffLoss = 2.7026, kgLoss = 0.0161
2025-04-08 19:31:13.925547: Training Step 121/354: batchLoss = 0.4418, diffLoss = 2.1602, kgLoss = 0.0121
2025-04-08 19:31:15.546388: Training Step 122/354: batchLoss = 0.5069, diffLoss = 2.4829, kgLoss = 0.0130
2025-04-08 19:31:17.304079: Training Step 123/354: batchLoss = 0.5304, diffLoss = 2.5930, kgLoss = 0.0148
2025-04-08 19:31:18.931834: Training Step 124/354: batchLoss = 0.5881, diffLoss = 2.8761, kgLoss = 0.0161
2025-04-08 19:31:20.541695: Training Step 125/354: batchLoss = 0.4814, diffLoss = 2.3618, kgLoss = 0.0113
2025-04-08 19:31:22.157342: Training Step 126/354: batchLoss = 0.5957, diffLoss = 2.9165, kgLoss = 0.0155
2025-04-08 19:31:23.777741: Training Step 127/354: batchLoss = 0.5254, diffLoss = 2.5741, kgLoss = 0.0132
2025-04-08 19:31:25.397414: Training Step 128/354: batchLoss = 0.6207, diffLoss = 3.0448, kgLoss = 0.0147
2025-04-08 19:31:27.019127: Training Step 129/354: batchLoss = 0.6391, diffLoss = 3.1297, kgLoss = 0.0164
2025-04-08 19:31:28.644072: Training Step 130/354: batchLoss = 0.6528, diffLoss = 3.1970, kgLoss = 0.0168
2025-04-08 19:31:30.265734: Training Step 131/354: batchLoss = 0.5232, diffLoss = 2.5594, kgLoss = 0.0141
2025-04-08 19:31:31.886584: Training Step 132/354: batchLoss = 0.6286, diffLoss = 3.0875, kgLoss = 0.0139
2025-04-08 19:31:33.502217: Training Step 133/354: batchLoss = 0.6493, diffLoss = 3.1784, kgLoss = 0.0170
2025-04-08 19:31:35.119006: Training Step 134/354: batchLoss = 0.4737, diffLoss = 2.3136, kgLoss = 0.0137
2025-04-08 19:31:36.735838: Training Step 135/354: batchLoss = 0.5015, diffLoss = 2.4537, kgLoss = 0.0134
2025-04-08 19:31:38.353667: Training Step 136/354: batchLoss = 0.6466, diffLoss = 3.1698, kgLoss = 0.0159
2025-04-08 19:31:39.972632: Training Step 137/354: batchLoss = 0.5160, diffLoss = 2.5255, kgLoss = 0.0136
2025-04-08 19:31:41.592279: Training Step 138/354: batchLoss = 0.6215, diffLoss = 3.0447, kgLoss = 0.0157
2025-04-08 19:31:43.211991: Training Step 139/354: batchLoss = 0.4490, diffLoss = 2.1914, kgLoss = 0.0133
2025-04-08 19:31:44.866006: Training Step 140/354: batchLoss = 0.5816, diffLoss = 2.8442, kgLoss = 0.0160
2025-04-08 19:31:46.740634: Training Step 141/354: batchLoss = 0.5325, diffLoss = 2.5994, kgLoss = 0.0158
2025-04-08 19:31:48.358077: Training Step 142/354: batchLoss = 0.5326, diffLoss = 2.6029, kgLoss = 0.0150
2025-04-08 19:31:49.975497: Training Step 143/354: batchLoss = 0.4879, diffLoss = 2.3825, kgLoss = 0.0143
2025-04-08 19:31:51.587363: Training Step 144/354: batchLoss = 0.5468, diffLoss = 2.6769, kgLoss = 0.0143
2025-04-08 19:31:53.205126: Training Step 145/354: batchLoss = 0.6420, diffLoss = 3.1434, kgLoss = 0.0166
2025-04-08 19:31:54.819768: Training Step 146/354: batchLoss = 0.5013, diffLoss = 2.4577, kgLoss = 0.0122
2025-04-08 19:31:56.442042: Training Step 147/354: batchLoss = 0.4942, diffLoss = 2.4165, kgLoss = 0.0137
2025-04-08 19:31:58.063548: Training Step 148/354: batchLoss = 0.5541, diffLoss = 2.7115, kgLoss = 0.0147
2025-04-08 19:31:59.685695: Training Step 149/354: batchLoss = 0.4437, diffLoss = 2.1583, kgLoss = 0.0151
2025-04-08 19:32:01.309926: Training Step 150/354: batchLoss = 0.4437, diffLoss = 2.1674, kgLoss = 0.0127
2025-04-08 19:32:02.936953: Training Step 151/354: batchLoss = 0.5211, diffLoss = 2.5500, kgLoss = 0.0139
2025-04-08 19:32:04.559115: Training Step 152/354: batchLoss = 0.5507, diffLoss = 2.6909, kgLoss = 0.0157
2025-04-08 19:32:06.173999: Training Step 153/354: batchLoss = 0.5374, diffLoss = 2.6342, kgLoss = 0.0132
2025-04-08 19:32:07.791151: Training Step 154/354: batchLoss = 0.5453, diffLoss = 2.6742, kgLoss = 0.0131
2025-04-08 19:32:09.404357: Training Step 155/354: batchLoss = 0.4883, diffLoss = 2.3912, kgLoss = 0.0126
2025-04-08 19:32:11.026968: Training Step 156/354: batchLoss = 0.5358, diffLoss = 2.6323, kgLoss = 0.0117
2025-04-08 19:32:12.657997: Training Step 157/354: batchLoss = 0.5000, diffLoss = 2.4506, kgLoss = 0.0124
2025-04-08 19:32:14.278585: Training Step 158/354: batchLoss = 0.6565, diffLoss = 3.2081, kgLoss = 0.0185
2025-04-08 19:32:15.915661: Training Step 159/354: batchLoss = 0.4984, diffLoss = 2.4384, kgLoss = 0.0134
2025-04-08 19:32:17.538783: Training Step 160/354: batchLoss = 0.4629, diffLoss = 2.2614, kgLoss = 0.0132
2025-04-08 19:32:19.160804: Training Step 161/354: batchLoss = 0.5724, diffLoss = 2.8060, kgLoss = 0.0140
2025-04-08 19:32:20.782579: Training Step 162/354: batchLoss = 0.4496, diffLoss = 2.2012, kgLoss = 0.0117
2025-04-08 19:32:22.394462: Training Step 163/354: batchLoss = 0.5593, diffLoss = 2.7395, kgLoss = 0.0142
2025-04-08 19:32:24.012408: Training Step 164/354: batchLoss = 0.5330, diffLoss = 2.5993, kgLoss = 0.0164
2025-04-08 19:32:25.625245: Training Step 165/354: batchLoss = 0.5833, diffLoss = 2.8488, kgLoss = 0.0170
2025-04-08 19:32:27.246006: Training Step 166/354: batchLoss = 0.4962, diffLoss = 2.4311, kgLoss = 0.0125
2025-04-08 19:32:28.873809: Training Step 167/354: batchLoss = 0.7370, diffLoss = 3.6105, kgLoss = 0.0186
2025-04-08 19:32:30.501563: Training Step 168/354: batchLoss = 0.4976, diffLoss = 2.4279, kgLoss = 0.0150
2025-04-08 19:32:32.130601: Training Step 169/354: batchLoss = 0.4444, diffLoss = 2.1731, kgLoss = 0.0122
2025-04-08 19:32:33.750721: Training Step 170/354: batchLoss = 0.4690, diffLoss = 2.2940, kgLoss = 0.0127
2025-04-08 19:32:35.371731: Training Step 171/354: batchLoss = 0.6021, diffLoss = 2.9468, kgLoss = 0.0160
2025-04-08 19:32:36.989216: Training Step 172/354: batchLoss = 0.5506, diffLoss = 2.6987, kgLoss = 0.0136
2025-04-08 19:32:38.606998: Training Step 173/354: batchLoss = 0.5874, diffLoss = 2.8782, kgLoss = 0.0147
2025-04-08 19:32:40.225338: Training Step 174/354: batchLoss = 0.5751, diffLoss = 2.8178, kgLoss = 0.0145
2025-04-08 19:32:41.837832: Training Step 175/354: batchLoss = 0.4768, diffLoss = 2.3272, kgLoss = 0.0142
2025-04-08 19:32:43.456343: Training Step 176/354: batchLoss = 0.6258, diffLoss = 3.0755, kgLoss = 0.0134
2025-04-08 19:32:45.080240: Training Step 177/354: batchLoss = 0.5616, diffLoss = 2.7487, kgLoss = 0.0149
2025-04-08 19:32:46.703660: Training Step 178/354: batchLoss = 0.4909, diffLoss = 2.4046, kgLoss = 0.0124
2025-04-08 19:32:48.321893: Training Step 179/354: batchLoss = 0.5659, diffLoss = 2.7708, kgLoss = 0.0147
2025-04-08 19:32:49.941271: Training Step 180/354: batchLoss = 0.7088, diffLoss = 3.4759, kgLoss = 0.0170
2025-04-08 19:32:51.562515: Training Step 181/354: batchLoss = 0.5422, diffLoss = 2.6547, kgLoss = 0.0141
2025-04-08 19:32:53.180363: Training Step 182/354: batchLoss = 0.6741, diffLoss = 3.2948, kgLoss = 0.0190
2025-04-08 19:32:54.792585: Training Step 183/354: batchLoss = 0.4324, diffLoss = 2.1191, kgLoss = 0.0108
2025-04-08 19:32:56.410951: Training Step 184/354: batchLoss = 0.5123, diffLoss = 2.5105, kgLoss = 0.0127
2025-04-08 19:32:58.025457: Training Step 185/354: batchLoss = 0.5240, diffLoss = 2.5637, kgLoss = 0.0140
2025-04-08 19:32:59.649901: Training Step 186/354: batchLoss = 0.5889, diffLoss = 2.8880, kgLoss = 0.0141
2025-04-08 19:33:01.280561: Training Step 187/354: batchLoss = 0.5221, diffLoss = 2.5576, kgLoss = 0.0132
2025-04-08 19:33:02.899628: Training Step 188/354: batchLoss = 0.4615, diffLoss = 2.2546, kgLoss = 0.0133
2025-04-08 19:33:04.526815: Training Step 189/354: batchLoss = 0.6609, diffLoss = 3.2368, kgLoss = 0.0169
2025-04-08 19:33:06.149629: Training Step 190/354: batchLoss = 0.5130, diffLoss = 2.5103, kgLoss = 0.0137
2025-04-08 19:33:07.774384: Training Step 191/354: batchLoss = 0.5763, diffLoss = 2.8243, kgLoss = 0.0143
2025-04-08 19:33:09.389545: Training Step 192/354: batchLoss = 0.5489, diffLoss = 2.6891, kgLoss = 0.0138
2025-04-08 19:33:11.006504: Training Step 193/354: batchLoss = 0.5475, diffLoss = 2.6779, kgLoss = 0.0149
2025-04-08 19:33:12.616924: Training Step 194/354: batchLoss = 0.5136, diffLoss = 2.5158, kgLoss = 0.0130
2025-04-08 19:33:14.230932: Training Step 195/354: batchLoss = 0.5221, diffLoss = 2.5533, kgLoss = 0.0142
2025-04-08 19:33:15.851665: Training Step 196/354: batchLoss = 0.6598, diffLoss = 3.2180, kgLoss = 0.0202
2025-04-08 19:33:17.475985: Training Step 197/354: batchLoss = 0.5641, diffLoss = 2.7597, kgLoss = 0.0152
2025-04-08 19:33:19.099583: Training Step 198/354: batchLoss = 0.4926, diffLoss = 2.4093, kgLoss = 0.0135
2025-04-08 19:33:20.722096: Training Step 199/354: batchLoss = 0.6007, diffLoss = 2.9465, kgLoss = 0.0143
2025-04-08 19:33:22.341953: Training Step 200/354: batchLoss = 0.5543, diffLoss = 2.7148, kgLoss = 0.0142
2025-04-08 19:33:23.966547: Training Step 201/354: batchLoss = 0.5885, diffLoss = 2.8810, kgLoss = 0.0154
2025-04-08 19:33:25.584293: Training Step 202/354: batchLoss = 0.5413, diffLoss = 2.6382, kgLoss = 0.0171
2025-04-08 19:33:27.197942: Training Step 203/354: batchLoss = 0.7027, diffLoss = 3.4442, kgLoss = 0.0173
2025-04-08 19:33:28.816447: Training Step 204/354: batchLoss = 0.5297, diffLoss = 2.5939, kgLoss = 0.0136
2025-04-08 19:33:30.434529: Training Step 205/354: batchLoss = 0.5217, diffLoss = 2.5543, kgLoss = 0.0136
2025-04-08 19:33:32.053898: Training Step 206/354: batchLoss = 0.5100, diffLoss = 2.5032, kgLoss = 0.0117
2025-04-08 19:33:33.671842: Training Step 207/354: batchLoss = 0.5502, diffLoss = 2.6884, kgLoss = 0.0157
2025-04-08 19:33:35.293056: Training Step 208/354: batchLoss = 0.5343, diffLoss = 2.6177, kgLoss = 0.0134
2025-04-08 19:33:36.918304: Training Step 209/354: batchLoss = 0.4459, diffLoss = 2.1760, kgLoss = 0.0133
2025-04-08 19:33:38.536043: Training Step 210/354: batchLoss = 0.4910, diffLoss = 2.4008, kgLoss = 0.0135
2025-04-08 19:33:40.156561: Training Step 211/354: batchLoss = 0.5108, diffLoss = 2.4989, kgLoss = 0.0138
2025-04-08 19:33:41.769736: Training Step 212/354: batchLoss = 0.6174, diffLoss = 3.0213, kgLoss = 0.0164
2025-04-08 19:33:43.388059: Training Step 213/354: batchLoss = 0.4881, diffLoss = 2.3838, kgLoss = 0.0141
2025-04-08 19:33:45.002755: Training Step 214/354: batchLoss = 0.5345, diffLoss = 2.6219, kgLoss = 0.0127
2025-04-08 19:33:46.622244: Training Step 215/354: batchLoss = 0.6866, diffLoss = 3.3611, kgLoss = 0.0179
2025-04-08 19:33:48.245270: Training Step 216/354: batchLoss = 0.5757, diffLoss = 2.8194, kgLoss = 0.0147
2025-04-08 19:33:49.867228: Training Step 217/354: batchLoss = 0.5059, diffLoss = 2.4749, kgLoss = 0.0137
2025-04-08 19:33:51.487887: Training Step 218/354: batchLoss = 0.5710, diffLoss = 2.7910, kgLoss = 0.0159
2025-04-08 19:33:53.109610: Training Step 219/354: batchLoss = 0.4626, diffLoss = 2.2641, kgLoss = 0.0122
2025-04-08 19:33:54.729026: Training Step 220/354: batchLoss = 0.6817, diffLoss = 3.3434, kgLoss = 0.0162
2025-04-08 19:33:56.350990: Training Step 221/354: batchLoss = 0.4865, diffLoss = 2.3758, kgLoss = 0.0142
2025-04-08 19:33:57.967607: Training Step 222/354: batchLoss = 0.4377, diffLoss = 2.1365, kgLoss = 0.0130
2025-04-08 19:33:59.580986: Training Step 223/354: batchLoss = 0.4616, diffLoss = 2.2614, kgLoss = 0.0117
2025-04-08 19:34:01.202737: Training Step 224/354: batchLoss = 0.5376, diffLoss = 2.6290, kgLoss = 0.0148
2025-04-08 19:34:02.820803: Training Step 225/354: batchLoss = 0.4830, diffLoss = 2.3647, kgLoss = 0.0125
2025-04-08 19:34:04.439522: Training Step 226/354: batchLoss = 0.6137, diffLoss = 3.0096, kgLoss = 0.0147
2025-04-08 19:34:06.073169: Training Step 227/354: batchLoss = 0.5670, diffLoss = 2.7726, kgLoss = 0.0155
2025-04-08 19:34:07.692346: Training Step 228/354: batchLoss = 0.4523, diffLoss = 2.2114, kgLoss = 0.0125
2025-04-08 19:34:09.315094: Training Step 229/354: batchLoss = 0.6371, diffLoss = 3.1225, kgLoss = 0.0157
2025-04-08 19:34:10.941311: Training Step 230/354: batchLoss = 0.6954, diffLoss = 3.4013, kgLoss = 0.0189
2025-04-08 19:34:12.573766: Training Step 231/354: batchLoss = 0.5591, diffLoss = 2.7386, kgLoss = 0.0142
2025-04-08 19:34:14.190258: Training Step 232/354: batchLoss = 0.4734, diffLoss = 2.3148, kgLoss = 0.0131
2025-04-08 19:34:15.808526: Training Step 233/354: batchLoss = 0.5576, diffLoss = 2.7291, kgLoss = 0.0147
2025-04-08 19:34:17.423134: Training Step 234/354: batchLoss = 0.5310, diffLoss = 2.5968, kgLoss = 0.0145
2025-04-08 19:34:19.037565: Training Step 235/354: batchLoss = 0.5038, diffLoss = 2.4614, kgLoss = 0.0144
2025-04-08 19:34:20.663876: Training Step 236/354: batchLoss = 0.6057, diffLoss = 2.9659, kgLoss = 0.0156
2025-04-08 19:34:22.290158: Training Step 237/354: batchLoss = 0.4670, diffLoss = 2.2842, kgLoss = 0.0127
2025-04-08 19:34:23.910415: Training Step 238/354: batchLoss = 0.4446, diffLoss = 2.1679, kgLoss = 0.0138
2025-04-08 19:34:25.532313: Training Step 239/354: batchLoss = 0.4990, diffLoss = 2.4384, kgLoss = 0.0142
2025-04-08 19:34:27.153073: Training Step 240/354: batchLoss = 0.4038, diffLoss = 1.9682, kgLoss = 0.0127
2025-04-08 19:34:28.773648: Training Step 241/354: batchLoss = 0.6680, diffLoss = 3.2649, kgLoss = 0.0188
2025-04-08 19:34:30.389664: Training Step 242/354: batchLoss = 0.4344, diffLoss = 2.1248, kgLoss = 0.0119
2025-04-08 19:34:32.007773: Training Step 243/354: batchLoss = 0.5638, diffLoss = 2.7593, kgLoss = 0.0150
2025-04-08 19:34:33.623775: Training Step 244/354: batchLoss = 0.4744, diffLoss = 2.3215, kgLoss = 0.0126
2025-04-08 19:34:35.241902: Training Step 245/354: batchLoss = 0.6569, diffLoss = 3.2192, kgLoss = 0.0163
2025-04-08 19:34:36.860721: Training Step 246/354: batchLoss = 0.5372, diffLoss = 2.6291, kgLoss = 0.0142
2025-04-08 19:34:38.485583: Training Step 247/354: batchLoss = 0.4521, diffLoss = 2.2180, kgLoss = 0.0106
2025-04-08 19:34:40.106932: Training Step 248/354: batchLoss = 0.6025, diffLoss = 2.9498, kgLoss = 0.0157
2025-04-08 19:34:41.722096: Training Step 249/354: batchLoss = 0.5949, diffLoss = 2.9127, kgLoss = 0.0154
2025-04-08 19:34:43.347910: Training Step 250/354: batchLoss = 0.4980, diffLoss = 2.4435, kgLoss = 0.0116
2025-04-08 19:34:44.970397: Training Step 251/354: batchLoss = 0.5608, diffLoss = 2.7477, kgLoss = 0.0141
2025-04-08 19:34:46.657788: Training Step 252/354: batchLoss = 0.4544, diffLoss = 2.2149, kgLoss = 0.0143
2025-04-08 19:34:48.273689: Training Step 253/354: batchLoss = 0.6452, diffLoss = 3.1593, kgLoss = 0.0166
2025-04-08 19:34:49.891805: Training Step 254/354: batchLoss = 0.5654, diffLoss = 2.7708, kgLoss = 0.0141
2025-04-08 19:34:51.505164: Training Step 255/354: batchLoss = 0.5211, diffLoss = 2.5519, kgLoss = 0.0135
2025-04-08 19:34:53.124988: Training Step 256/354: batchLoss = 0.5905, diffLoss = 2.8923, kgLoss = 0.0151
2025-04-08 19:34:54.747745: Training Step 257/354: batchLoss = 0.4426, diffLoss = 2.1651, kgLoss = 0.0120
2025-04-08 19:34:56.366631: Training Step 258/354: batchLoss = 0.5013, diffLoss = 2.4436, kgLoss = 0.0158
2025-04-08 19:34:57.987222: Training Step 259/354: batchLoss = 0.7041, diffLoss = 3.4485, kgLoss = 0.0180
2025-04-08 19:34:59.608399: Training Step 260/354: batchLoss = 0.5815, diffLoss = 2.8436, kgLoss = 0.0159
2025-04-08 19:35:01.228197: Training Step 261/354: batchLoss = 0.5942, diffLoss = 2.9146, kgLoss = 0.0141
2025-04-08 19:35:02.842113: Training Step 262/354: batchLoss = 0.6333, diffLoss = 3.1014, kgLoss = 0.0163
2025-04-08 19:35:04.458262: Training Step 263/354: batchLoss = 0.4425, diffLoss = 2.1604, kgLoss = 0.0130
2025-04-08 19:35:06.074793: Training Step 264/354: batchLoss = 0.4851, diffLoss = 2.3755, kgLoss = 0.0125
2025-04-08 19:35:07.682497: Training Step 265/354: batchLoss = 0.5552, diffLoss = 2.7180, kgLoss = 0.0144
2025-04-08 19:35:09.304975: Training Step 266/354: batchLoss = 0.4503, diffLoss = 2.1996, kgLoss = 0.0130
2025-04-08 19:35:10.932388: Training Step 267/354: batchLoss = 0.6740, diffLoss = 3.2990, kgLoss = 0.0178
2025-04-08 19:35:12.558814: Training Step 268/354: batchLoss = 0.4965, diffLoss = 2.4325, kgLoss = 0.0125
2025-04-08 19:35:14.179425: Training Step 269/354: batchLoss = 0.5409, diffLoss = 2.6477, kgLoss = 0.0143
2025-04-08 19:35:15.802794: Training Step 270/354: batchLoss = 0.5897, diffLoss = 2.8836, kgLoss = 0.0162
2025-04-08 19:35:17.422846: Training Step 271/354: batchLoss = 0.6210, diffLoss = 3.0439, kgLoss = 0.0152
2025-04-08 19:35:19.038229: Training Step 272/354: batchLoss = 0.6295, diffLoss = 3.0822, kgLoss = 0.0163
2025-04-08 19:35:20.651741: Training Step 273/354: batchLoss = 0.5228, diffLoss = 2.5586, kgLoss = 0.0139
2025-04-08 19:35:22.272500: Training Step 274/354: batchLoss = 0.4984, diffLoss = 2.4390, kgLoss = 0.0132
2025-04-08 19:35:23.893801: Training Step 275/354: batchLoss = 0.4758, diffLoss = 2.3268, kgLoss = 0.0130
2025-04-08 19:35:25.514096: Training Step 276/354: batchLoss = 0.6585, diffLoss = 3.2218, kgLoss = 0.0176
2025-04-08 19:35:27.134691: Training Step 277/354: batchLoss = 0.4909, diffLoss = 2.3966, kgLoss = 0.0145
2025-04-08 19:35:28.759965: Training Step 278/354: batchLoss = 0.5979, diffLoss = 2.9237, kgLoss = 0.0165
2025-04-08 19:35:30.384583: Training Step 279/354: batchLoss = 0.6565, diffLoss = 3.2145, kgLoss = 0.0170
2025-04-08 19:35:32.003870: Training Step 280/354: batchLoss = 0.4706, diffLoss = 2.3001, kgLoss = 0.0133
2025-04-08 19:35:33.620635: Training Step 281/354: batchLoss = 0.4631, diffLoss = 2.2663, kgLoss = 0.0123
2025-04-08 19:35:35.240910: Training Step 282/354: batchLoss = 0.5295, diffLoss = 2.5973, kgLoss = 0.0125
2025-04-08 19:35:36.854526: Training Step 283/354: batchLoss = 0.5789, diffLoss = 2.8344, kgLoss = 0.0151
2025-04-08 19:35:38.464651: Training Step 284/354: batchLoss = 0.5282, diffLoss = 2.5882, kgLoss = 0.0131
2025-04-08 19:35:40.086426: Training Step 285/354: batchLoss = 0.6246, diffLoss = 3.0598, kgLoss = 0.0158
2025-04-08 19:35:41.712174: Training Step 286/354: batchLoss = 0.7054, diffLoss = 3.4600, kgLoss = 0.0167
2025-04-08 19:35:43.336072: Training Step 287/354: batchLoss = 0.5389, diffLoss = 2.6401, kgLoss = 0.0136
2025-04-08 19:35:44.958300: Training Step 288/354: batchLoss = 0.5122, diffLoss = 2.5034, kgLoss = 0.0143
2025-04-08 19:35:46.581988: Training Step 289/354: batchLoss = 0.6066, diffLoss = 2.9752, kgLoss = 0.0145
2025-04-08 19:35:48.199755: Training Step 290/354: batchLoss = 0.4829, diffLoss = 2.3634, kgLoss = 0.0128
2025-04-08 19:35:49.815834: Training Step 291/354: batchLoss = 0.6097, diffLoss = 2.9859, kgLoss = 0.0156
2025-04-08 19:35:51.432310: Training Step 292/354: batchLoss = 0.6395, diffLoss = 3.1332, kgLoss = 0.0160
2025-04-08 19:35:53.041133: Training Step 293/354: batchLoss = 0.4887, diffLoss = 2.3949, kgLoss = 0.0122
2025-04-08 19:35:54.654445: Training Step 294/354: batchLoss = 0.5474, diffLoss = 2.6779, kgLoss = 0.0147
2025-04-08 19:35:56.269264: Training Step 295/354: batchLoss = 0.4931, diffLoss = 2.4091, kgLoss = 0.0141
2025-04-08 19:35:57.894661: Training Step 296/354: batchLoss = 0.5262, diffLoss = 2.5813, kgLoss = 0.0125
2025-04-08 19:35:59.518305: Training Step 297/354: batchLoss = 0.5401, diffLoss = 2.5945, kgLoss = 0.0265
2025-04-08 19:36:01.138670: Training Step 298/354: batchLoss = 0.5581, diffLoss = 2.7282, kgLoss = 0.0156
2025-04-08 19:36:02.762773: Training Step 299/354: batchLoss = 0.5763, diffLoss = 2.8186, kgLoss = 0.0158
2025-04-08 19:36:04.389384: Training Step 300/354: batchLoss = 0.5613, diffLoss = 2.7404, kgLoss = 0.0165
2025-04-08 19:36:06.012387: Training Step 301/354: batchLoss = 0.4921, diffLoss = 2.4100, kgLoss = 0.0126
2025-04-08 19:36:07.630239: Training Step 302/354: batchLoss = 0.5555, diffLoss = 2.7168, kgLoss = 0.0151
2025-04-08 19:36:09.248403: Training Step 303/354: batchLoss = 0.5962, diffLoss = 2.9219, kgLoss = 0.0148
2025-04-08 19:36:10.865213: Training Step 304/354: batchLoss = 0.4686, diffLoss = 2.2937, kgLoss = 0.0124
2025-04-08 19:36:12.488383: Training Step 305/354: batchLoss = 0.4885, diffLoss = 2.3905, kgLoss = 0.0130
2025-04-08 19:36:14.111806: Training Step 306/354: batchLoss = 0.4278, diffLoss = 2.0901, kgLoss = 0.0122
2025-04-08 19:36:15.731897: Training Step 307/354: batchLoss = 0.5704, diffLoss = 2.7978, kgLoss = 0.0136
2025-04-08 19:36:17.350434: Training Step 308/354: batchLoss = 0.5454, diffLoss = 2.6676, kgLoss = 0.0148
2025-04-08 19:36:18.969640: Training Step 309/354: batchLoss = 0.6673, diffLoss = 3.2686, kgLoss = 0.0170
2025-04-08 19:36:20.594265: Training Step 310/354: batchLoss = 0.5214, diffLoss = 2.5467, kgLoss = 0.0150
2025-04-08 19:36:22.209468: Training Step 311/354: batchLoss = 0.6703, diffLoss = 3.2797, kgLoss = 0.0179
2025-04-08 19:36:23.818624: Training Step 312/354: batchLoss = 0.5140, diffLoss = 2.5110, kgLoss = 0.0147
2025-04-08 19:36:25.436024: Training Step 313/354: batchLoss = 0.7141, diffLoss = 3.5039, kgLoss = 0.0167
2025-04-08 19:36:27.054601: Training Step 314/354: batchLoss = 0.4744, diffLoss = 2.3285, kgLoss = 0.0109
2025-04-08 19:36:28.677348: Training Step 315/354: batchLoss = 0.5650, diffLoss = 2.7560, kgLoss = 0.0173
2025-04-08 19:36:30.296414: Training Step 316/354: batchLoss = 0.5930, diffLoss = 2.9042, kgLoss = 0.0152
2025-04-08 19:36:31.920364: Training Step 317/354: batchLoss = 0.5716, diffLoss = 2.8043, kgLoss = 0.0135
2025-04-08 19:36:33.542506: Training Step 318/354: batchLoss = 0.5189, diffLoss = 2.5335, kgLoss = 0.0153
2025-04-08 19:36:35.161085: Training Step 319/354: batchLoss = 0.6869, diffLoss = 3.3641, kgLoss = 0.0176
2025-04-08 19:36:36.778138: Training Step 320/354: batchLoss = 0.4774, diffLoss = 2.3293, kgLoss = 0.0144
2025-04-08 19:36:38.394911: Training Step 321/354: batchLoss = 0.5865, diffLoss = 2.8741, kgLoss = 0.0147
2025-04-08 19:36:40.009714: Training Step 322/354: batchLoss = 0.5143, diffLoss = 2.5134, kgLoss = 0.0145
2025-04-08 19:36:41.621286: Training Step 323/354: batchLoss = 0.5247, diffLoss = 2.5650, kgLoss = 0.0146
2025-04-08 19:36:43.237266: Training Step 324/354: batchLoss = 0.7281, diffLoss = 3.5651, kgLoss = 0.0189
2025-04-08 19:36:44.859544: Training Step 325/354: batchLoss = 0.6379, diffLoss = 3.1280, kgLoss = 0.0153
2025-04-08 19:36:46.477744: Training Step 326/354: batchLoss = 0.5747, diffLoss = 2.8084, kgLoss = 0.0163
2025-04-08 19:36:48.094673: Training Step 327/354: batchLoss = 0.4866, diffLoss = 2.3814, kgLoss = 0.0129
2025-04-08 19:36:49.718393: Training Step 328/354: batchLoss = 0.5512, diffLoss = 2.6949, kgLoss = 0.0152
2025-04-08 19:36:51.333403: Training Step 329/354: batchLoss = 0.5871, diffLoss = 2.8716, kgLoss = 0.0159
2025-04-08 19:36:52.946789: Training Step 330/354: batchLoss = 0.4959, diffLoss = 2.4247, kgLoss = 0.0136
2025-04-08 19:36:54.561719: Training Step 331/354: batchLoss = 0.5252, diffLoss = 2.5661, kgLoss = 0.0150
2025-04-08 19:36:56.174393: Training Step 332/354: batchLoss = 0.4646, diffLoss = 2.2746, kgLoss = 0.0121
2025-04-08 19:36:57.787171: Training Step 333/354: batchLoss = 0.5817, diffLoss = 2.8453, kgLoss = 0.0158
2025-04-08 19:36:59.397688: Training Step 334/354: batchLoss = 0.5608, diffLoss = 2.7431, kgLoss = 0.0152
2025-04-08 19:37:01.026242: Training Step 335/354: batchLoss = 0.5372, diffLoss = 2.6278, kgLoss = 0.0145
2025-04-08 19:37:02.646046: Training Step 336/354: batchLoss = 0.6569, diffLoss = 3.2113, kgLoss = 0.0182
2025-04-08 19:37:04.267184: Training Step 337/354: batchLoss = 0.5303, diffLoss = 2.5974, kgLoss = 0.0136
2025-04-08 19:37:05.889689: Training Step 338/354: batchLoss = 0.5336, diffLoss = 2.6125, kgLoss = 0.0138
2025-04-08 19:37:07.510784: Training Step 339/354: batchLoss = 0.4567, diffLoss = 2.2343, kgLoss = 0.0124
2025-04-08 19:37:09.128661: Training Step 340/354: batchLoss = 0.5015, diffLoss = 2.4520, kgLoss = 0.0139
2025-04-08 19:37:10.744201: Training Step 341/354: batchLoss = 0.5339, diffLoss = 2.6162, kgLoss = 0.0133
2025-04-08 19:37:12.362194: Training Step 342/354: batchLoss = 0.6163, diffLoss = 3.0137, kgLoss = 0.0169
2025-04-08 19:37:13.978658: Training Step 343/354: batchLoss = 0.6707, diffLoss = 3.2778, kgLoss = 0.0189
2025-04-08 19:37:15.594208: Training Step 344/354: batchLoss = 0.5541, diffLoss = 2.7086, kgLoss = 0.0154
2025-04-08 19:37:17.213141: Training Step 345/354: batchLoss = 0.5211, diffLoss = 2.5516, kgLoss = 0.0135
2025-04-08 19:37:18.829492: Training Step 346/354: batchLoss = 0.5377, diffLoss = 2.6186, kgLoss = 0.0175
2025-04-08 19:37:20.450799: Training Step 347/354: batchLoss = 0.7136, diffLoss = 3.4963, kgLoss = 0.0180
2025-04-08 19:37:22.073874: Training Step 348/354: batchLoss = 0.5161, diffLoss = 2.5239, kgLoss = 0.0142
2025-04-08 19:37:23.697181: Training Step 349/354: batchLoss = 0.5143, diffLoss = 2.5179, kgLoss = 0.0133
2025-04-08 19:37:25.320067: Training Step 350/354: batchLoss = 0.5926, diffLoss = 2.9056, kgLoss = 0.0144
2025-04-08 19:37:26.930185: Training Step 351/354: batchLoss = 0.4673, diffLoss = 2.2814, kgLoss = 0.0138
2025-04-08 19:37:28.524571: Training Step 352/354: batchLoss = 0.5797, diffLoss = 2.8466, kgLoss = 0.0130
2025-04-08 19:37:29.918999: Training Step 353/354: batchLoss = 0.4386, diffLoss = 2.1403, kgLoss = 0.0131
2025-04-08 19:37:30.010828: 
2025-04-08 19:37:30.011481: Epoch 33/1000, Train: epLoss = 0.9857, epDfLoss = 4.8241, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 19:37:31.331904: Steps 0/138: batch_recall = 46.26, batch_ndcg = 26.57 
2025-04-08 19:37:32.656448: Steps 1/138: batch_recall = 48.00, batch_ndcg = 28.09 
2025-04-08 19:37:33.964130: Steps 2/138: batch_recall = 57.83, batch_ndcg = 35.32 
2025-04-08 19:37:35.282080: Steps 3/138: batch_recall = 58.20, batch_ndcg = 33.85 
2025-04-08 19:37:36.592039: Steps 4/138: batch_recall = 67.20, batch_ndcg = 40.92 
2025-04-08 19:37:37.901044: Steps 5/138: batch_recall = 57.19, batch_ndcg = 31.23 
2025-04-08 19:37:39.211606: Steps 6/138: batch_recall = 51.64, batch_ndcg = 32.18 
2025-04-08 19:37:40.520463: Steps 7/138: batch_recall = 62.63, batch_ndcg = 41.74 
2025-04-08 19:37:41.821631: Steps 8/138: batch_recall = 60.82, batch_ndcg = 38.39 
2025-04-08 19:37:43.134911: Steps 9/138: batch_recall = 58.37, batch_ndcg = 34.57 
2025-04-08 19:37:44.433282: Steps 10/138: batch_recall = 59.49, batch_ndcg = 33.72 
2025-04-08 19:37:45.734964: Steps 11/138: batch_recall = 57.48, batch_ndcg = 33.33 
2025-04-08 19:37:47.033187: Steps 12/138: batch_recall = 51.89, batch_ndcg = 29.62 
2025-04-08 19:37:48.321515: Steps 13/138: batch_recall = 53.83, batch_ndcg = 31.95 
2025-04-08 19:37:49.615863: Steps 14/138: batch_recall = 53.92, batch_ndcg = 31.41 
2025-04-08 19:37:50.905578: Steps 15/138: batch_recall = 47.72, batch_ndcg = 29.73 
2025-04-08 19:37:52.202324: Steps 16/138: batch_recall = 61.08, batch_ndcg = 34.17 
2025-04-08 19:37:53.486986: Steps 17/138: batch_recall = 58.83, batch_ndcg = 33.83 
2025-04-08 19:37:54.778633: Steps 18/138: batch_recall = 54.00, batch_ndcg = 33.40 
2025-04-08 19:37:56.085502: Steps 19/138: batch_recall = 55.83, batch_ndcg = 33.84 
2025-04-08 19:37:57.383871: Steps 20/138: batch_recall = 62.39, batch_ndcg = 36.12 
2025-04-08 19:37:58.679079: Steps 21/138: batch_recall = 68.18, batch_ndcg = 39.97 
2025-04-08 19:37:59.972149: Steps 22/138: batch_recall = 56.85, batch_ndcg = 32.74 
2025-04-08 19:38:01.267698: Steps 23/138: batch_recall = 50.33, batch_ndcg = 30.03 
2025-04-08 19:38:02.554523: Steps 24/138: batch_recall = 56.25, batch_ndcg = 30.91 
2025-04-08 19:38:03.848218: Steps 25/138: batch_recall = 60.71, batch_ndcg = 34.99 
2025-04-08 19:38:05.132291: Steps 26/138: batch_recall = 58.78, batch_ndcg = 33.04 
2025-04-08 19:38:06.420789: Steps 27/138: batch_recall = 57.40, batch_ndcg = 32.36 
2025-04-08 19:38:07.703754: Steps 28/138: batch_recall = 59.85, batch_ndcg = 34.01 
2025-04-08 19:38:08.995000: Steps 29/138: batch_recall = 60.51, batch_ndcg = 31.49 
2025-04-08 19:38:10.291867: Steps 30/138: batch_recall = 58.32, batch_ndcg = 34.18 
2025-04-08 19:38:11.581138: Steps 31/138: batch_recall = 47.66, batch_ndcg = 27.51 
2025-04-08 19:38:12.870333: Steps 32/138: batch_recall = 53.75, batch_ndcg = 31.69 
2025-04-08 19:38:14.153218: Steps 33/138: batch_recall = 59.74, batch_ndcg = 33.32 
2025-04-08 19:38:15.452414: Steps 34/138: batch_recall = 54.19, batch_ndcg = 29.67 
2025-04-08 19:38:16.745469: Steps 35/138: batch_recall = 50.95, batch_ndcg = 29.93 
2025-04-08 19:38:18.032186: Steps 36/138: batch_recall = 47.29, batch_ndcg = 26.97 
2025-04-08 19:38:19.320912: Steps 37/138: batch_recall = 59.62, batch_ndcg = 34.62 
2025-04-08 19:38:20.605031: Steps 38/138: batch_recall = 57.38, batch_ndcg = 32.46 
2025-04-08 19:38:21.887101: Steps 39/138: batch_recall = 68.79, batch_ndcg = 38.72 
2025-04-08 19:38:23.182097: Steps 40/138: batch_recall = 58.41, batch_ndcg = 30.03 
2025-04-08 19:38:24.464090: Steps 41/138: batch_recall = 59.15, batch_ndcg = 33.77 
2025-04-08 19:38:25.742170: Steps 42/138: batch_recall = 54.61, batch_ndcg = 30.85 
2025-04-08 19:38:27.028366: Steps 43/138: batch_recall = 58.55, batch_ndcg = 36.62 
2025-04-08 19:38:28.317628: Steps 44/138: batch_recall = 55.69, batch_ndcg = 30.32 
2025-04-08 19:38:29.605431: Steps 45/138: batch_recall = 62.53, batch_ndcg = 35.75 
2025-04-08 19:38:30.887277: Steps 46/138: batch_recall = 61.74, batch_ndcg = 36.44 
2025-04-08 19:38:32.163818: Steps 47/138: batch_recall = 54.85, batch_ndcg = 32.30 
2025-04-08 19:38:33.440713: Steps 48/138: batch_recall = 63.50, batch_ndcg = 36.25 
2025-04-08 19:38:34.715300: Steps 49/138: batch_recall = 65.29, batch_ndcg = 37.29 
2025-04-08 19:38:35.995691: Steps 50/138: batch_recall = 58.21, batch_ndcg = 30.84 
2025-04-08 19:38:37.283598: Steps 51/138: batch_recall = 65.52, batch_ndcg = 36.91 
2025-04-08 19:38:38.573671: Steps 52/138: batch_recall = 66.22, batch_ndcg = 42.63 
2025-04-08 19:38:39.863825: Steps 53/138: batch_recall = 63.73, batch_ndcg = 34.33 
2025-04-08 19:38:41.177558: Steps 54/138: batch_recall = 68.61, batch_ndcg = 37.93 
2025-04-08 19:38:42.457847: Steps 55/138: batch_recall = 62.68, batch_ndcg = 35.20 
2025-04-08 19:38:43.745746: Steps 56/138: batch_recall = 58.47, batch_ndcg = 34.63 
2025-04-08 19:38:45.037194: Steps 57/138: batch_recall = 55.33, batch_ndcg = 31.94 
2025-04-08 19:38:46.324146: Steps 58/138: batch_recall = 69.94, batch_ndcg = 36.71 
2025-04-08 19:38:47.605147: Steps 59/138: batch_recall = 65.03, batch_ndcg = 38.61 
2025-04-08 19:38:48.882876: Steps 60/138: batch_recall = 66.64, batch_ndcg = 37.71 
2025-04-08 19:38:50.153053: Steps 61/138: batch_recall = 65.34, batch_ndcg = 35.53 
2025-04-08 19:38:51.432760: Steps 62/138: batch_recall = 84.88, batch_ndcg = 44.79 
2025-04-08 19:38:52.719895: Steps 63/138: batch_recall = 74.01, batch_ndcg = 42.71 
2025-04-08 19:38:54.000763: Steps 64/138: batch_recall = 59.76, batch_ndcg = 31.90 
2025-04-08 19:38:55.284222: Steps 65/138: batch_recall = 84.64, batch_ndcg = 46.98 
2025-04-08 19:38:56.570951: Steps 66/138: batch_recall = 69.11, batch_ndcg = 41.47 
2025-04-08 19:38:57.853665: Steps 67/138: batch_recall = 74.67, batch_ndcg = 45.52 
2025-04-08 19:38:59.140341: Steps 68/138: batch_recall = 62.29, batch_ndcg = 33.77 
2025-04-08 19:39:00.421317: Steps 69/138: batch_recall = 87.04, batch_ndcg = 51.77 
2025-04-08 19:39:01.699731: Steps 70/138: batch_recall = 77.80, batch_ndcg = 44.49 
2025-04-08 19:39:02.979111: Steps 71/138: batch_recall = 86.41, batch_ndcg = 51.95 
2025-04-08 19:39:04.258433: Steps 72/138: batch_recall = 83.41, batch_ndcg = 48.09 
2025-04-08 19:39:05.532694: Steps 73/138: batch_recall = 84.70, batch_ndcg = 47.17 
2025-04-08 19:39:06.814152: Steps 74/138: batch_recall = 77.58, batch_ndcg = 48.25 
2025-04-08 19:39:08.092572: Steps 75/138: batch_recall = 81.01, batch_ndcg = 48.98 
2025-04-08 19:39:09.381618: Steps 76/138: batch_recall = 94.19, batch_ndcg = 55.30 
2025-04-08 19:39:10.669625: Steps 77/138: batch_recall = 90.88, batch_ndcg = 52.03 
2025-04-08 19:39:11.944752: Steps 78/138: batch_recall = 90.45, batch_ndcg = 48.08 
2025-04-08 19:39:13.240636: Steps 79/138: batch_recall = 89.55, batch_ndcg = 49.22 
2025-04-08 19:39:14.521649: Steps 80/138: batch_recall = 71.16, batch_ndcg = 39.00 
2025-04-08 19:39:15.801730: Steps 81/138: batch_recall = 78.55, batch_ndcg = 47.11 
2025-04-08 19:39:17.082114: Steps 82/138: batch_recall = 86.50, batch_ndcg = 53.57 
2025-04-08 19:39:18.364365: Steps 83/138: batch_recall = 82.40, batch_ndcg = 47.69 
2025-04-08 19:39:19.633585: Steps 84/138: batch_recall = 97.06, batch_ndcg = 57.19 
2025-04-08 19:39:20.906629: Steps 85/138: batch_recall = 108.34, batch_ndcg = 61.96 
2025-04-08 19:39:22.183828: Steps 86/138: batch_recall = 122.38, batch_ndcg = 72.63 
2025-04-08 19:39:23.459120: Steps 87/138: batch_recall = 103.53, batch_ndcg = 55.03 
2025-04-08 19:39:24.735403: Steps 88/138: batch_recall = 103.52, batch_ndcg = 60.03 
2025-04-08 19:39:26.016403: Steps 89/138: batch_recall = 118.48, batch_ndcg = 68.21 
2025-04-08 19:39:27.291305: Steps 90/138: batch_recall = 102.59, batch_ndcg = 57.62 
2025-04-08 19:39:28.565466: Steps 91/138: batch_recall = 116.24, batch_ndcg = 64.25 
2025-04-08 19:39:29.839778: Steps 92/138: batch_recall = 118.88, batch_ndcg = 65.27 
2025-04-08 19:39:31.112558: Steps 93/138: batch_recall = 120.70, batch_ndcg = 69.07 
2025-04-08 19:39:32.386684: Steps 94/138: batch_recall = 123.39, batch_ndcg = 66.05 
2025-04-08 19:39:33.660853: Steps 95/138: batch_recall = 114.82, batch_ndcg = 67.81 
2025-04-08 19:39:34.927831: Steps 96/138: batch_recall = 132.95, batch_ndcg = 80.31 
2025-04-08 19:39:36.204873: Steps 97/138: batch_recall = 153.47, batch_ndcg = 95.09 
2025-04-08 19:39:37.474496: Steps 98/138: batch_recall = 110.34, batch_ndcg = 64.68 
2025-04-08 19:39:38.747323: Steps 99/138: batch_recall = 125.56, batch_ndcg = 71.99 
2025-04-08 19:39:40.025855: Steps 100/138: batch_recall = 131.38, batch_ndcg = 72.34 
2025-04-08 19:39:41.294309: Steps 101/138: batch_recall = 127.92, batch_ndcg = 70.36 
2025-04-08 19:39:42.574740: Steps 102/138: batch_recall = 119.37, batch_ndcg = 68.80 
2025-04-08 19:39:43.843790: Steps 103/138: batch_recall = 143.00, batch_ndcg = 80.64 
2025-04-08 19:39:45.111025: Steps 104/138: batch_recall = 134.51, batch_ndcg = 79.03 
2025-04-08 19:39:46.379543: Steps 105/138: batch_recall = 118.77, batch_ndcg = 67.23 
2025-04-08 19:39:47.649822: Steps 106/138: batch_recall = 108.02, batch_ndcg = 61.10 
2025-04-08 19:39:48.921503: Steps 107/138: batch_recall = 115.55, batch_ndcg = 65.41 
2025-04-08 19:39:50.189837: Steps 108/138: batch_recall = 117.38, batch_ndcg = 71.85 
2025-04-08 19:39:51.455124: Steps 109/138: batch_recall = 137.08, batch_ndcg = 76.39 
2025-04-08 19:39:52.729073: Steps 110/138: batch_recall = 121.14, batch_ndcg = 64.62 
2025-04-08 19:39:54.004308: Steps 111/138: batch_recall = 135.06, batch_ndcg = 84.13 
2025-04-08 19:39:55.273972: Steps 112/138: batch_recall = 170.80, batch_ndcg = 97.29 
2025-04-08 19:39:56.548014: Steps 113/138: batch_recall = 127.29, batch_ndcg = 70.84 
2025-04-08 19:39:57.816010: Steps 114/138: batch_recall = 123.35, batch_ndcg = 72.13 
2025-04-08 19:39:59.087840: Steps 115/138: batch_recall = 118.67, batch_ndcg = 63.68 
2025-04-08 19:40:00.366272: Steps 116/138: batch_recall = 125.31, batch_ndcg = 67.29 
2025-04-08 19:40:01.644117: Steps 117/138: batch_recall = 113.56, batch_ndcg = 66.27 
2025-04-08 19:40:02.914926: Steps 118/138: batch_recall = 125.58, batch_ndcg = 69.91 
2025-04-08 19:40:04.178315: Steps 119/138: batch_recall = 141.11, batch_ndcg = 76.50 
2025-04-08 19:40:05.459747: Steps 120/138: batch_recall = 120.91, batch_ndcg = 70.26 
2025-04-08 19:40:06.734419: Steps 121/138: batch_recall = 148.06, batch_ndcg = 79.06 
2025-04-08 19:40:08.023534: Steps 122/138: batch_recall = 152.49, batch_ndcg = 82.99 
2025-04-08 19:40:09.300870: Steps 123/138: batch_recall = 129.92, batch_ndcg = 74.78 
2025-04-08 19:40:10.584076: Steps 124/138: batch_recall = 152.99, batch_ndcg = 92.96 
2025-04-08 19:40:11.859819: Steps 125/138: batch_recall = 133.13, batch_ndcg = 74.34 
2025-04-08 19:40:13.128204: Steps 126/138: batch_recall = 162.02, batch_ndcg = 88.23 
2025-04-08 19:40:14.397936: Steps 127/138: batch_recall = 141.84, batch_ndcg = 82.33 
2025-04-08 19:40:15.665738: Steps 128/138: batch_recall = 129.70, batch_ndcg = 72.15 
2025-04-08 19:40:16.934006: Steps 129/138: batch_recall = 159.00, batch_ndcg = 91.88 
2025-04-08 19:40:18.196922: Steps 130/138: batch_recall = 132.18, batch_ndcg = 70.29 
2025-04-08 19:40:19.461613: Steps 131/138: batch_recall = 150.63, batch_ndcg = 88.95 
2025-04-08 19:40:20.725937: Steps 132/138: batch_recall = 149.93, batch_ndcg = 84.97 
2025-04-08 19:40:21.997333: Steps 133/138: batch_recall = 145.02, batch_ndcg = 85.30 
2025-04-08 19:40:23.268513: Steps 134/138: batch_recall = 146.38, batch_ndcg = 84.22 
2025-04-08 19:40:24.533755: Steps 135/138: batch_recall = 168.19, batch_ndcg = 95.80 
2025-04-08 19:40:25.794568: Steps 136/138: batch_recall = 155.89, batch_ndcg = 80.57 
2025-04-08 19:40:27.071917: Steps 137/138: batch_recall = 137.45, batch_ndcg = 86.38 
2025-04-08 19:40:27.072444: Epoch 33/1000, Test: Recall = 0.1763, NDCG = 0.1009  

2025-04-08 19:40:28.831112: Training Step 0/354: batchLoss = 0.5891, diffLoss = 2.8845, kgLoss = 0.0153
2025-04-08 19:40:30.457312: Training Step 1/354: batchLoss = 0.5187, diffLoss = 2.5357, kgLoss = 0.0145
2025-04-08 19:40:32.071778: Training Step 2/354: batchLoss = 0.4472, diffLoss = 2.1847, kgLoss = 0.0129
2025-04-08 19:40:33.697022: Training Step 3/354: batchLoss = 0.5939, diffLoss = 2.9109, kgLoss = 0.0146
2025-04-08 19:40:35.312531: Training Step 4/354: batchLoss = 0.6859, diffLoss = 3.3581, kgLoss = 0.0178
2025-04-08 19:40:36.930214: Training Step 5/354: batchLoss = 0.6100, diffLoss = 2.9847, kgLoss = 0.0164
2025-04-08 19:40:38.544228: Training Step 6/354: batchLoss = 0.6102, diffLoss = 2.9872, kgLoss = 0.0160
2025-04-08 19:40:40.164511: Training Step 7/354: batchLoss = 0.5099, diffLoss = 2.4974, kgLoss = 0.0131
2025-04-08 19:40:41.792978: Training Step 8/354: batchLoss = 0.5691, diffLoss = 2.7789, kgLoss = 0.0167
2025-04-08 19:40:43.411882: Training Step 9/354: batchLoss = 0.5139, diffLoss = 2.5189, kgLoss = 0.0126
2025-04-08 19:40:45.029488: Training Step 10/354: batchLoss = 0.7206, diffLoss = 3.5308, kgLoss = 0.0181
2025-04-08 19:40:46.635834: Training Step 11/354: batchLoss = 0.5273, diffLoss = 2.5854, kgLoss = 0.0127
2025-04-08 19:40:48.251383: Training Step 12/354: batchLoss = 0.9215, diffLoss = 4.5098, kgLoss = 0.0244
2025-04-08 19:40:49.862450: Training Step 13/354: batchLoss = 0.5298, diffLoss = 2.5983, kgLoss = 0.0126
2025-04-08 19:40:51.483765: Training Step 14/354: batchLoss = 0.6125, diffLoss = 2.9951, kgLoss = 0.0168
2025-04-08 19:40:53.103422: Training Step 15/354: batchLoss = 0.4951, diffLoss = 2.4225, kgLoss = 0.0133
2025-04-08 19:40:54.722805: Training Step 16/354: batchLoss = 0.6007, diffLoss = 2.9442, kgLoss = 0.0148
2025-04-08 19:40:56.338902: Training Step 17/354: batchLoss = 0.5435, diffLoss = 2.6648, kgLoss = 0.0132
2025-04-08 19:40:57.955871: Training Step 18/354: batchLoss = 0.5982, diffLoss = 2.9280, kgLoss = 0.0157
2025-04-08 19:40:59.571679: Training Step 19/354: batchLoss = 0.5988, diffLoss = 2.9372, kgLoss = 0.0142
2025-04-08 19:41:01.190425: Training Step 20/354: batchLoss = 0.5419, diffLoss = 2.6505, kgLoss = 0.0148
2025-04-08 19:41:02.801733: Training Step 21/354: batchLoss = 0.5568, diffLoss = 2.7276, kgLoss = 0.0141
2025-04-08 19:41:04.416423: Training Step 22/354: batchLoss = 0.5708, diffLoss = 2.7981, kgLoss = 0.0139
2025-04-08 19:41:06.037910: Training Step 23/354: batchLoss = 0.5236, diffLoss = 2.5578, kgLoss = 0.0151
2025-04-08 19:41:07.658071: Training Step 24/354: batchLoss = 0.5691, diffLoss = 2.7879, kgLoss = 0.0144
2025-04-08 19:41:09.274535: Training Step 25/354: batchLoss = 0.5237, diffLoss = 2.5669, kgLoss = 0.0129
2025-04-08 19:41:10.888431: Training Step 26/354: batchLoss = 0.5356, diffLoss = 2.6260, kgLoss = 0.0130
2025-04-08 19:41:12.506262: Training Step 27/354: batchLoss = 0.6276, diffLoss = 3.0766, kgLoss = 0.0154
2025-04-08 19:41:14.125955: Training Step 28/354: batchLoss = 0.6827, diffLoss = 3.3364, kgLoss = 0.0193
2025-04-08 19:41:15.743355: Training Step 29/354: batchLoss = 0.4994, diffLoss = 2.4448, kgLoss = 0.0130
2025-04-08 19:41:17.363952: Training Step 30/354: batchLoss = 0.4556, diffLoss = 2.2279, kgLoss = 0.0125
2025-04-08 19:41:18.976732: Training Step 31/354: batchLoss = 0.6612, diffLoss = 3.2438, kgLoss = 0.0156
2025-04-08 19:41:20.593800: Training Step 32/354: batchLoss = 0.5655, diffLoss = 2.7696, kgLoss = 0.0145
2025-04-08 19:41:22.213550: Training Step 33/354: batchLoss = 0.5261, diffLoss = 2.5727, kgLoss = 0.0144
2025-04-08 19:41:23.837758: Training Step 34/354: batchLoss = 0.6701, diffLoss = 3.2826, kgLoss = 0.0169
2025-04-08 19:41:25.462422: Training Step 35/354: batchLoss = 0.4990, diffLoss = 2.4400, kgLoss = 0.0138
2025-04-08 19:41:27.083043: Training Step 36/354: batchLoss = 0.4887, diffLoss = 2.3914, kgLoss = 0.0130
2025-04-08 19:41:28.704219: Training Step 37/354: batchLoss = 0.6021, diffLoss = 2.9539, kgLoss = 0.0142
2025-04-08 19:41:30.321776: Training Step 38/354: batchLoss = 0.4396, diffLoss = 2.1485, kgLoss = 0.0124
2025-04-08 19:41:31.939022: Training Step 39/354: batchLoss = 0.5098, diffLoss = 2.4944, kgLoss = 0.0136
2025-04-08 19:41:33.557177: Training Step 40/354: batchLoss = 0.4698, diffLoss = 2.3001, kgLoss = 0.0123
2025-04-08 19:41:35.176014: Training Step 41/354: batchLoss = 0.5114, diffLoss = 2.5046, kgLoss = 0.0131
2025-04-08 19:41:36.794451: Training Step 42/354: batchLoss = 0.4804, diffLoss = 2.3517, kgLoss = 0.0126
2025-04-08 19:41:38.412019: Training Step 43/354: batchLoss = 0.5942, diffLoss = 2.9053, kgLoss = 0.0165
2025-04-08 19:41:40.026073: Training Step 44/354: batchLoss = 0.5047, diffLoss = 2.4696, kgLoss = 0.0134
2025-04-08 19:41:41.644685: Training Step 45/354: batchLoss = 0.4857, diffLoss = 2.3746, kgLoss = 0.0135
2025-04-08 19:41:43.262801: Training Step 46/354: batchLoss = 0.4024, diffLoss = 1.9580, kgLoss = 0.0135
2025-04-08 19:41:44.880496: Training Step 47/354: batchLoss = 0.4983, diffLoss = 2.4344, kgLoss = 0.0143
2025-04-08 19:41:46.497118: Training Step 48/354: batchLoss = 0.5136, diffLoss = 2.5104, kgLoss = 0.0144
2025-04-08 19:41:48.110634: Training Step 49/354: batchLoss = 0.4348, diffLoss = 2.1250, kgLoss = 0.0122
2025-04-08 19:41:49.725597: Training Step 50/354: batchLoss = 0.6710, diffLoss = 3.2827, kgLoss = 0.0181
2025-04-08 19:41:51.341567: Training Step 51/354: batchLoss = 0.5379, diffLoss = 2.6309, kgLoss = 0.0147
2025-04-08 19:41:52.952460: Training Step 52/354: batchLoss = 0.6466, diffLoss = 3.1614, kgLoss = 0.0179
2025-04-08 19:41:54.577313: Training Step 53/354: batchLoss = 0.6273, diffLoss = 3.0716, kgLoss = 0.0162
2025-04-08 19:41:56.196345: Training Step 54/354: batchLoss = 0.5777, diffLoss = 2.8190, kgLoss = 0.0174
2025-04-08 19:41:57.816261: Training Step 55/354: batchLoss = 0.4838, diffLoss = 2.3674, kgLoss = 0.0129
2025-04-08 19:41:59.437043: Training Step 56/354: batchLoss = 0.5652, diffLoss = 2.7639, kgLoss = 0.0155
2025-04-08 19:42:01.052197: Training Step 57/354: batchLoss = 0.4591, diffLoss = 2.2402, kgLoss = 0.0139
2025-04-08 19:42:02.671032: Training Step 58/354: batchLoss = 0.4312, diffLoss = 2.1094, kgLoss = 0.0116
2025-04-08 19:42:04.279832: Training Step 59/354: batchLoss = 0.6145, diffLoss = 3.0008, kgLoss = 0.0179
2025-04-08 19:42:05.896730: Training Step 60/354: batchLoss = 0.5375, diffLoss = 2.6308, kgLoss = 0.0141
2025-04-08 19:42:07.515577: Training Step 61/354: batchLoss = 0.7074, diffLoss = 3.4520, kgLoss = 0.0212
2025-04-08 19:42:09.133322: Training Step 62/354: batchLoss = 0.5945, diffLoss = 2.9018, kgLoss = 0.0177
2025-04-08 19:42:10.751814: Training Step 63/354: batchLoss = 0.4840, diffLoss = 2.3718, kgLoss = 0.0121
2025-04-08 19:42:12.379852: Training Step 64/354: batchLoss = 0.5304, diffLoss = 2.5940, kgLoss = 0.0145
2025-04-08 19:42:14.003370: Training Step 65/354: batchLoss = 0.6699, diffLoss = 3.2826, kgLoss = 0.0167
2025-04-08 19:42:15.615555: Training Step 66/354: batchLoss = 0.6066, diffLoss = 2.9725, kgLoss = 0.0151
2025-04-08 19:42:17.225785: Training Step 67/354: batchLoss = 0.4755, diffLoss = 2.3318, kgLoss = 0.0115
2025-04-08 19:42:18.846442: Training Step 68/354: batchLoss = 0.5806, diffLoss = 2.8449, kgLoss = 0.0146
2025-04-08 19:42:20.461526: Training Step 69/354: batchLoss = 0.5510, diffLoss = 2.6992, kgLoss = 0.0139
2025-04-08 19:42:22.080312: Training Step 70/354: batchLoss = 0.5356, diffLoss = 2.6176, kgLoss = 0.0151
2025-04-08 19:42:23.699503: Training Step 71/354: batchLoss = 0.5670, diffLoss = 2.7769, kgLoss = 0.0145
2025-04-08 19:42:25.320952: Training Step 72/354: batchLoss = 0.4110, diffLoss = 2.0119, kgLoss = 0.0107
2025-04-08 19:42:26.947545: Training Step 73/354: batchLoss = 0.4916, diffLoss = 2.4094, kgLoss = 0.0122
2025-04-08 19:42:28.564018: Training Step 74/354: batchLoss = 0.5025, diffLoss = 2.4581, kgLoss = 0.0137
2025-04-08 19:42:30.179180: Training Step 75/354: batchLoss = 0.4313, diffLoss = 2.1054, kgLoss = 0.0128
2025-04-08 19:42:31.794982: Training Step 76/354: batchLoss = 0.5200, diffLoss = 2.5484, kgLoss = 0.0129
2025-04-08 19:42:33.411606: Training Step 77/354: batchLoss = 0.5309, diffLoss = 2.5931, kgLoss = 0.0154
2025-04-08 19:42:35.030517: Training Step 78/354: batchLoss = 0.4994, diffLoss = 2.4457, kgLoss = 0.0128
2025-04-08 19:42:36.646780: Training Step 79/354: batchLoss = 0.5428, diffLoss = 2.6598, kgLoss = 0.0135
2025-04-08 19:42:38.265599: Training Step 80/354: batchLoss = 0.6171, diffLoss = 3.0250, kgLoss = 0.0151
2025-04-08 19:42:39.885679: Training Step 81/354: batchLoss = 0.5481, diffLoss = 2.6838, kgLoss = 0.0142
2025-04-08 19:42:41.506812: Training Step 82/354: batchLoss = 0.5149, diffLoss = 2.5119, kgLoss = 0.0156
2025-04-08 19:42:43.126531: Training Step 83/354: batchLoss = 0.5221, diffLoss = 2.5561, kgLoss = 0.0136
2025-04-08 19:42:44.741613: Training Step 84/354: batchLoss = 0.5621, diffLoss = 2.7504, kgLoss = 0.0150
2025-04-08 19:42:46.358367: Training Step 85/354: batchLoss = 0.4463, diffLoss = 2.1824, kgLoss = 0.0123
2025-04-08 19:42:47.969608: Training Step 86/354: batchLoss = 0.6045, diffLoss = 2.9613, kgLoss = 0.0153
2025-04-08 19:42:49.588096: Training Step 87/354: batchLoss = 0.5118, diffLoss = 2.5010, kgLoss = 0.0145
2025-04-08 19:42:51.205638: Training Step 88/354: batchLoss = 0.5701, diffLoss = 2.7913, kgLoss = 0.0148
2025-04-08 19:42:52.820698: Training Step 89/354: batchLoss = 0.4953, diffLoss = 2.4230, kgLoss = 0.0134
2025-04-08 19:42:54.435863: Training Step 90/354: batchLoss = 0.6260, diffLoss = 3.0613, kgLoss = 0.0171
2025-04-08 19:42:56.055501: Training Step 91/354: batchLoss = 0.5829, diffLoss = 2.8555, kgLoss = 0.0148
2025-04-08 19:42:57.675467: Training Step 92/354: batchLoss = 0.5389, diffLoss = 2.6348, kgLoss = 0.0149
2025-04-08 19:42:59.288963: Training Step 93/354: batchLoss = 0.4382, diffLoss = 2.1357, kgLoss = 0.0138
2025-04-08 19:43:00.904796: Training Step 94/354: batchLoss = 0.6923, diffLoss = 3.3804, kgLoss = 0.0203
2025-04-08 19:43:02.517875: Training Step 95/354: batchLoss = 0.4924, diffLoss = 2.4069, kgLoss = 0.0137
2025-04-08 19:43:04.135651: Training Step 96/354: batchLoss = 0.4186, diffLoss = 2.0421, kgLoss = 0.0127
2025-04-08 19:43:05.767269: Training Step 97/354: batchLoss = 0.5018, diffLoss = 2.4608, kgLoss = 0.0121
2025-04-08 19:43:07.390547: Training Step 98/354: batchLoss = 0.5554, diffLoss = 2.7187, kgLoss = 0.0146
2025-04-08 19:43:09.011203: Training Step 99/354: batchLoss = 0.5076, diffLoss = 2.4857, kgLoss = 0.0131
2025-04-08 19:43:10.626593: Training Step 100/354: batchLoss = 0.5119, diffLoss = 2.5038, kgLoss = 0.0139
2025-04-08 19:43:12.244311: Training Step 101/354: batchLoss = 0.4972, diffLoss = 2.4378, kgLoss = 0.0121
2025-04-08 19:43:13.865784: Training Step 102/354: batchLoss = 0.5727, diffLoss = 2.8073, kgLoss = 0.0141
2025-04-08 19:43:15.486061: Training Step 103/354: batchLoss = 0.5162, diffLoss = 2.5212, kgLoss = 0.0150
2025-04-08 19:43:17.104690: Training Step 104/354: batchLoss = 0.4661, diffLoss = 2.2832, kgLoss = 0.0119
2025-04-08 19:43:18.722439: Training Step 105/354: batchLoss = 0.5079, diffLoss = 2.4847, kgLoss = 0.0137
2025-04-08 19:43:20.343776: Training Step 106/354: batchLoss = 0.5468, diffLoss = 2.6751, kgLoss = 0.0148
2025-04-08 19:43:21.958238: Training Step 107/354: batchLoss = 0.5690, diffLoss = 2.7926, kgLoss = 0.0131
2025-04-08 19:43:23.580698: Training Step 108/354: batchLoss = 0.6727, diffLoss = 3.2926, kgLoss = 0.0177
2025-04-08 19:43:25.200818: Training Step 109/354: batchLoss = 0.5486, diffLoss = 2.6872, kgLoss = 0.0140
2025-04-08 19:43:26.822311: Training Step 110/354: batchLoss = 0.4668, diffLoss = 2.2826, kgLoss = 0.0129
2025-04-08 19:43:28.439480: Training Step 111/354: batchLoss = 0.5511, diffLoss = 2.6942, kgLoss = 0.0153
2025-04-08 19:43:30.050402: Training Step 112/354: batchLoss = 0.5014, diffLoss = 2.4529, kgLoss = 0.0135
2025-04-08 19:43:31.665739: Training Step 113/354: batchLoss = 0.5831, diffLoss = 2.8555, kgLoss = 0.0150
2025-04-08 19:43:33.280374: Training Step 114/354: batchLoss = 0.5694, diffLoss = 2.7950, kgLoss = 0.0130
2025-04-08 19:43:34.900356: Training Step 115/354: batchLoss = 0.4723, diffLoss = 2.3099, kgLoss = 0.0129
2025-04-08 19:43:36.517769: Training Step 116/354: batchLoss = 0.5637, diffLoss = 2.7606, kgLoss = 0.0145
2025-04-08 19:43:38.143682: Training Step 117/354: batchLoss = 0.4324, diffLoss = 2.1095, kgLoss = 0.0132
2025-04-08 19:43:39.762688: Training Step 118/354: batchLoss = 0.5718, diffLoss = 2.7998, kgLoss = 0.0148
2025-04-08 19:43:41.385387: Training Step 119/354: batchLoss = 0.6246, diffLoss = 3.0592, kgLoss = 0.0159
2025-04-08 19:43:43.005914: Training Step 120/354: batchLoss = 0.6406, diffLoss = 3.1326, kgLoss = 0.0176
2025-04-08 19:43:44.622353: Training Step 121/354: batchLoss = 0.6478, diffLoss = 3.1700, kgLoss = 0.0173
2025-04-08 19:43:46.241202: Training Step 122/354: batchLoss = 0.5096, diffLoss = 2.4921, kgLoss = 0.0140
2025-04-08 19:43:47.854795: Training Step 123/354: batchLoss = 0.4377, diffLoss = 2.1431, kgLoss = 0.0114
2025-04-08 19:43:49.481976: Training Step 124/354: batchLoss = 0.5798, diffLoss = 2.8381, kgLoss = 0.0152
2025-04-08 19:43:51.102633: Training Step 125/354: batchLoss = 0.5260, diffLoss = 2.5793, kgLoss = 0.0126
2025-04-08 19:43:52.733020: Training Step 126/354: batchLoss = 0.4886, diffLoss = 2.3931, kgLoss = 0.0125
2025-04-08 19:43:54.354624: Training Step 127/354: batchLoss = 0.4547, diffLoss = 2.2191, kgLoss = 0.0136
2025-04-08 19:43:55.978991: Training Step 128/354: batchLoss = 0.4824, diffLoss = 2.3447, kgLoss = 0.0168
2025-04-08 19:43:57.594140: Training Step 129/354: batchLoss = 0.4715, diffLoss = 2.3035, kgLoss = 0.0135
2025-04-08 19:43:59.212555: Training Step 130/354: batchLoss = 0.5333, diffLoss = 2.6103, kgLoss = 0.0140
2025-04-08 19:44:00.832653: Training Step 131/354: batchLoss = 0.4598, diffLoss = 2.2516, kgLoss = 0.0118
2025-04-08 19:44:02.458942: Training Step 132/354: batchLoss = 0.5084, diffLoss = 2.4856, kgLoss = 0.0140
2025-04-08 19:44:04.080731: Training Step 133/354: batchLoss = 0.5681, diffLoss = 2.7839, kgLoss = 0.0142
2025-04-08 19:44:05.698439: Training Step 134/354: batchLoss = 0.6622, diffLoss = 3.2452, kgLoss = 0.0165
2025-04-08 19:44:07.320482: Training Step 135/354: batchLoss = 0.5948, diffLoss = 2.9102, kgLoss = 0.0159
2025-04-08 19:44:08.948591: Training Step 136/354: batchLoss = 0.5256, diffLoss = 2.5760, kgLoss = 0.0130
2025-04-08 19:44:10.577701: Training Step 137/354: batchLoss = 0.5080, diffLoss = 2.4871, kgLoss = 0.0132
2025-04-08 19:44:12.199288: Training Step 138/354: batchLoss = 0.5193, diffLoss = 2.5385, kgLoss = 0.0146
2025-04-08 19:44:13.814226: Training Step 139/354: batchLoss = 0.5050, diffLoss = 2.4667, kgLoss = 0.0146
2025-04-08 19:44:15.427477: Training Step 140/354: batchLoss = 0.4967, diffLoss = 2.4342, kgLoss = 0.0123
2025-04-08 19:44:17.049890: Training Step 141/354: batchLoss = 0.5250, diffLoss = 2.5674, kgLoss = 0.0145
2025-04-08 19:44:18.678427: Training Step 142/354: batchLoss = 0.6340, diffLoss = 3.1072, kgLoss = 0.0157
2025-04-08 19:44:20.297512: Training Step 143/354: batchLoss = 0.6048, diffLoss = 2.9606, kgLoss = 0.0158
2025-04-08 19:44:21.918740: Training Step 144/354: batchLoss = 0.5653, diffLoss = 2.7671, kgLoss = 0.0149
2025-04-08 19:44:23.537276: Training Step 145/354: batchLoss = 0.5630, diffLoss = 2.7459, kgLoss = 0.0173
2025-04-08 19:44:25.160704: Training Step 146/354: batchLoss = 0.5463, diffLoss = 2.6771, kgLoss = 0.0136
2025-04-08 19:44:26.778467: Training Step 147/354: batchLoss = 0.7018, diffLoss = 3.4339, kgLoss = 0.0187
2025-04-08 19:44:28.395002: Training Step 148/354: batchLoss = 0.6298, diffLoss = 3.0794, kgLoss = 0.0174
2025-04-08 19:44:30.009702: Training Step 149/354: batchLoss = 0.5180, diffLoss = 2.5370, kgLoss = 0.0132
2025-04-08 19:44:31.624417: Training Step 150/354: batchLoss = 0.4766, diffLoss = 2.3367, kgLoss = 0.0116
2025-04-08 19:44:33.246711: Training Step 151/354: batchLoss = 0.5220, diffLoss = 2.5572, kgLoss = 0.0132
2025-04-08 19:44:34.867213: Training Step 152/354: batchLoss = 0.6262, diffLoss = 3.0666, kgLoss = 0.0161
2025-04-08 19:44:36.487286: Training Step 153/354: batchLoss = 0.7029, diffLoss = 3.4429, kgLoss = 0.0178
2025-04-08 19:44:38.108398: Training Step 154/354: batchLoss = 0.4794, diffLoss = 2.3450, kgLoss = 0.0130
2025-04-08 19:44:39.734273: Training Step 155/354: batchLoss = 0.6247, diffLoss = 3.0583, kgLoss = 0.0162
2025-04-08 19:44:41.356939: Training Step 156/354: batchLoss = 0.5757, diffLoss = 2.8185, kgLoss = 0.0150
2025-04-08 19:44:42.967957: Training Step 157/354: batchLoss = 0.4665, diffLoss = 2.2833, kgLoss = 0.0123
2025-04-08 19:44:44.583511: Training Step 158/354: batchLoss = 0.5625, diffLoss = 2.7578, kgLoss = 0.0137
2025-04-08 19:44:46.203185: Training Step 159/354: batchLoss = 0.7202, diffLoss = 3.5297, kgLoss = 0.0179
2025-04-08 19:44:47.825448: Training Step 160/354: batchLoss = 0.5421, diffLoss = 2.6536, kgLoss = 0.0143
2025-04-08 19:44:49.447607: Training Step 161/354: batchLoss = 0.5350, diffLoss = 2.6205, kgLoss = 0.0136
2025-04-08 19:44:51.068414: Training Step 162/354: batchLoss = 0.4793, diffLoss = 2.3467, kgLoss = 0.0125
2025-04-08 19:44:52.689090: Training Step 163/354: batchLoss = 0.5833, diffLoss = 2.8512, kgLoss = 0.0163
2025-04-08 19:44:54.311560: Training Step 164/354: batchLoss = 0.5081, diffLoss = 2.4889, kgLoss = 0.0128
2025-04-08 19:44:55.930020: Training Step 165/354: batchLoss = 0.4753, diffLoss = 2.3274, kgLoss = 0.0123
2025-04-08 19:44:57.548716: Training Step 166/354: batchLoss = 0.5589, diffLoss = 2.7217, kgLoss = 0.0183
2025-04-08 19:44:59.166069: Training Step 167/354: batchLoss = 0.5838, diffLoss = 2.8612, kgLoss = 0.0144
2025-04-08 19:45:00.770482: Training Step 168/354: batchLoss = 0.6624, diffLoss = 3.2426, kgLoss = 0.0173
2025-04-08 19:45:02.385291: Training Step 169/354: batchLoss = 0.4293, diffLoss = 2.1019, kgLoss = 0.0112
2025-04-08 19:45:04.007739: Training Step 170/354: batchLoss = 0.4951, diffLoss = 2.4234, kgLoss = 0.0130
2025-04-08 19:45:05.622700: Training Step 171/354: batchLoss = 0.5098, diffLoss = 2.4945, kgLoss = 0.0136
2025-04-08 19:45:07.244990: Training Step 172/354: batchLoss = 0.4696, diffLoss = 2.2977, kgLoss = 0.0126
2025-04-08 19:45:08.865014: Training Step 173/354: batchLoss = 0.5194, diffLoss = 2.5406, kgLoss = 0.0140
2025-04-08 19:45:10.490313: Training Step 174/354: batchLoss = 0.5036, diffLoss = 2.4690, kgLoss = 0.0123
2025-04-08 19:45:12.106253: Training Step 175/354: batchLoss = 0.5334, diffLoss = 2.6156, kgLoss = 0.0129
2025-04-08 19:45:13.719161: Training Step 176/354: batchLoss = 0.4838, diffLoss = 2.3690, kgLoss = 0.0125
2025-04-08 19:45:15.331249: Training Step 177/354: batchLoss = 0.5118, diffLoss = 2.5066, kgLoss = 0.0131
2025-04-08 19:45:16.944088: Training Step 178/354: batchLoss = 0.5347, diffLoss = 2.6141, kgLoss = 0.0148
2025-04-08 19:45:18.565555: Training Step 179/354: batchLoss = 0.5436, diffLoss = 2.6626, kgLoss = 0.0138
2025-04-08 19:45:20.189080: Training Step 180/354: batchLoss = 0.5817, diffLoss = 2.8518, kgLoss = 0.0141
2025-04-08 19:45:21.809236: Training Step 181/354: batchLoss = 0.5009, diffLoss = 2.4490, kgLoss = 0.0139
2025-04-08 19:45:23.431679: Training Step 182/354: batchLoss = 0.6399, diffLoss = 3.1403, kgLoss = 0.0148
2025-04-08 19:45:25.049766: Training Step 183/354: batchLoss = 0.6520, diffLoss = 3.1887, kgLoss = 0.0178
2025-04-08 19:45:26.669844: Training Step 184/354: batchLoss = 0.5077, diffLoss = 2.4858, kgLoss = 0.0131
2025-04-08 19:45:28.283954: Training Step 185/354: batchLoss = 0.5190, diffLoss = 2.5362, kgLoss = 0.0146
2025-04-08 19:45:29.900193: Training Step 186/354: batchLoss = 0.5750, diffLoss = 2.8173, kgLoss = 0.0144
2025-04-08 19:45:31.518120: Training Step 187/354: batchLoss = 0.4500, diffLoss = 2.1937, kgLoss = 0.0141
2025-04-08 19:45:33.134783: Training Step 188/354: batchLoss = 0.4376, diffLoss = 2.1371, kgLoss = 0.0127
2025-04-08 19:45:34.756388: Training Step 189/354: batchLoss = 0.4221, diffLoss = 2.0607, kgLoss = 0.0125
2025-04-08 19:45:36.376294: Training Step 190/354: batchLoss = 0.5259, diffLoss = 2.5801, kgLoss = 0.0123
2025-04-08 19:45:37.996095: Training Step 191/354: batchLoss = 0.5567, diffLoss = 2.7237, kgLoss = 0.0150
2025-04-08 19:45:39.618026: Training Step 192/354: batchLoss = 0.5538, diffLoss = 2.7152, kgLoss = 0.0134
2025-04-08 19:45:41.235906: Training Step 193/354: batchLoss = 0.5832, diffLoss = 2.8547, kgLoss = 0.0153
2025-04-08 19:45:42.852949: Training Step 194/354: batchLoss = 0.5680, diffLoss = 2.7788, kgLoss = 0.0154
2025-04-08 19:45:44.471873: Training Step 195/354: batchLoss = 0.5311, diffLoss = 2.6025, kgLoss = 0.0133
2025-04-08 19:45:46.082556: Training Step 196/354: batchLoss = 0.4374, diffLoss = 2.1348, kgLoss = 0.0130
2025-04-08 19:45:47.707826: Training Step 197/354: batchLoss = 0.5257, diffLoss = 2.5720, kgLoss = 0.0142
2025-04-08 19:45:49.331429: Training Step 198/354: batchLoss = 0.5570, diffLoss = 2.7299, kgLoss = 0.0138
2025-04-08 19:45:50.945760: Training Step 199/354: batchLoss = 0.6075, diffLoss = 2.9796, kgLoss = 0.0145
2025-04-08 19:45:52.563944: Training Step 200/354: batchLoss = 0.6895, diffLoss = 3.3707, kgLoss = 0.0192
2025-04-08 19:45:54.185928: Training Step 201/354: batchLoss = 0.5551, diffLoss = 2.7119, kgLoss = 0.0158
2025-04-08 19:45:55.810476: Training Step 202/354: batchLoss = 0.4483, diffLoss = 2.1923, kgLoss = 0.0122
2025-04-08 19:45:57.429168: Training Step 203/354: batchLoss = 0.5542, diffLoss = 2.7200, kgLoss = 0.0128
2025-04-08 19:45:59.046994: Training Step 204/354: batchLoss = 0.5078, diffLoss = 2.4825, kgLoss = 0.0141
2025-04-08 19:46:00.663154: Training Step 205/354: batchLoss = 0.6587, diffLoss = 3.2284, kgLoss = 0.0163
2025-04-08 19:46:02.290793: Training Step 206/354: batchLoss = 0.5152, diffLoss = 2.5223, kgLoss = 0.0134
2025-04-08 19:46:03.912089: Training Step 207/354: batchLoss = 0.5192, diffLoss = 2.5389, kgLoss = 0.0142
2025-04-08 19:46:05.532062: Training Step 208/354: batchLoss = 0.4889, diffLoss = 2.3770, kgLoss = 0.0169
2025-04-08 19:46:07.154779: Training Step 209/354: batchLoss = 0.5656, diffLoss = 2.7636, kgLoss = 0.0160
2025-04-08 19:46:08.776492: Training Step 210/354: batchLoss = 0.6303, diffLoss = 3.0840, kgLoss = 0.0169
2025-04-08 19:46:10.394745: Training Step 211/354: batchLoss = 0.5315, diffLoss = 2.6001, kgLoss = 0.0143
2025-04-08 19:46:12.009564: Training Step 212/354: batchLoss = 0.5325, diffLoss = 2.6042, kgLoss = 0.0146
2025-04-08 19:46:13.623753: Training Step 213/354: batchLoss = 0.5612, diffLoss = 2.7352, kgLoss = 0.0177
2025-04-08 19:46:15.241671: Training Step 214/354: batchLoss = 0.5773, diffLoss = 2.8276, kgLoss = 0.0147
2025-04-08 19:46:16.863530: Training Step 215/354: batchLoss = 0.5831, diffLoss = 2.8612, kgLoss = 0.0136
2025-04-08 19:46:18.484236: Training Step 216/354: batchLoss = 0.5684, diffLoss = 2.7785, kgLoss = 0.0158
2025-04-08 19:46:20.100279: Training Step 217/354: batchLoss = 0.6892, diffLoss = 3.3804, kgLoss = 0.0163
2025-04-08 19:46:21.719269: Training Step 218/354: batchLoss = 0.5700, diffLoss = 2.7856, kgLoss = 0.0162
2025-04-08 19:46:23.340736: Training Step 219/354: batchLoss = 0.4941, diffLoss = 2.4196, kgLoss = 0.0127
2025-04-08 19:46:24.962226: Training Step 220/354: batchLoss = 0.6235, diffLoss = 3.0547, kgLoss = 0.0157
2025-04-08 19:46:26.576419: Training Step 221/354: batchLoss = 0.5224, diffLoss = 2.5546, kgLoss = 0.0144
2025-04-08 19:46:28.193032: Training Step 222/354: batchLoss = 0.5050, diffLoss = 2.4700, kgLoss = 0.0137
2025-04-08 19:46:29.810650: Training Step 223/354: batchLoss = 0.4828, diffLoss = 2.3624, kgLoss = 0.0130
2025-04-08 19:46:31.436197: Training Step 224/354: batchLoss = 0.5918, diffLoss = 2.8975, kgLoss = 0.0154
2025-04-08 19:46:33.053643: Training Step 225/354: batchLoss = 0.5466, diffLoss = 2.6779, kgLoss = 0.0138
2025-04-08 19:46:34.674987: Training Step 226/354: batchLoss = 0.5720, diffLoss = 2.8119, kgLoss = 0.0121
2025-04-08 19:46:36.296768: Training Step 227/354: batchLoss = 0.6426, diffLoss = 3.1470, kgLoss = 0.0165
2025-04-08 19:46:37.921331: Training Step 228/354: batchLoss = 0.5743, diffLoss = 2.8134, kgLoss = 0.0145
2025-04-08 19:46:39.545078: Training Step 229/354: batchLoss = 0.4729, diffLoss = 2.3106, kgLoss = 0.0135
2025-04-08 19:46:41.159579: Training Step 230/354: batchLoss = 0.5658, diffLoss = 2.7702, kgLoss = 0.0146
2025-04-08 19:46:42.773736: Training Step 231/354: batchLoss = 0.6220, diffLoss = 3.0423, kgLoss = 0.0169
2025-04-08 19:46:44.390194: Training Step 232/354: batchLoss = 0.5044, diffLoss = 2.4715, kgLoss = 0.0126
2025-04-08 19:46:46.002583: Training Step 233/354: batchLoss = 0.5164, diffLoss = 2.5270, kgLoss = 0.0137
2025-04-08 19:46:47.622998: Training Step 234/354: batchLoss = 0.5893, diffLoss = 2.8806, kgLoss = 0.0164
2025-04-08 19:46:49.243851: Training Step 235/354: batchLoss = 0.7279, diffLoss = 3.5586, kgLoss = 0.0202
2025-04-08 19:46:50.868308: Training Step 236/354: batchLoss = 0.5353, diffLoss = 2.6158, kgLoss = 0.0152
2025-04-08 19:46:52.493152: Training Step 237/354: batchLoss = 0.5622, diffLoss = 2.7572, kgLoss = 0.0135
2025-04-08 19:46:54.112987: Training Step 238/354: batchLoss = 0.4764, diffLoss = 2.3290, kgLoss = 0.0133
2025-04-08 19:46:55.720595: Training Step 239/354: batchLoss = 0.6502, diffLoss = 3.1827, kgLoss = 0.0170
2025-04-08 19:46:57.332066: Training Step 240/354: batchLoss = 0.4766, diffLoss = 2.3364, kgLoss = 0.0116
2025-04-08 19:46:58.942109: Training Step 241/354: batchLoss = 0.5515, diffLoss = 2.6995, kgLoss = 0.0145
2025-04-08 19:47:00.559979: Training Step 242/354: batchLoss = 0.5800, diffLoss = 2.8365, kgLoss = 0.0158
2025-04-08 19:47:02.178727: Training Step 243/354: batchLoss = 0.5227, diffLoss = 2.5560, kgLoss = 0.0143
2025-04-08 19:47:03.800946: Training Step 244/354: batchLoss = 0.5571, diffLoss = 2.7304, kgLoss = 0.0137
2025-04-08 19:47:05.422476: Training Step 245/354: batchLoss = 0.5620, diffLoss = 2.7498, kgLoss = 0.0150
2025-04-08 19:47:07.037799: Training Step 246/354: batchLoss = 0.5269, diffLoss = 2.5772, kgLoss = 0.0144
2025-04-08 19:47:08.656420: Training Step 247/354: batchLoss = 0.5287, diffLoss = 2.5920, kgLoss = 0.0129
2025-04-08 19:47:10.264340: Training Step 248/354: batchLoss = 0.6107, diffLoss = 2.9902, kgLoss = 0.0159
2025-04-08 19:47:11.877595: Training Step 249/354: batchLoss = 0.5100, diffLoss = 2.4969, kgLoss = 0.0133
2025-04-08 19:47:13.497509: Training Step 250/354: batchLoss = 0.5062, diffLoss = 2.4767, kgLoss = 0.0136
2025-04-08 19:47:15.113954: Training Step 251/354: batchLoss = 0.5853, diffLoss = 2.8584, kgLoss = 0.0171
2025-04-08 19:47:16.725685: Training Step 252/354: batchLoss = 0.6298, diffLoss = 3.0742, kgLoss = 0.0187
2025-04-08 19:47:18.345865: Training Step 253/354: batchLoss = 0.6139, diffLoss = 3.0038, kgLoss = 0.0164
2025-04-08 19:47:19.964864: Training Step 254/354: batchLoss = 0.5390, diffLoss = 2.6387, kgLoss = 0.0141
2025-04-08 19:47:21.586326: Training Step 255/354: batchLoss = 0.5912, diffLoss = 2.8883, kgLoss = 0.0169
2025-04-08 19:47:23.207512: Training Step 256/354: batchLoss = 0.4467, diffLoss = 2.1885, kgLoss = 0.0113
2025-04-08 19:47:24.828196: Training Step 257/354: batchLoss = 0.4936, diffLoss = 2.4168, kgLoss = 0.0128
2025-04-08 19:47:26.440898: Training Step 258/354: batchLoss = 0.5182, diffLoss = 2.5369, kgLoss = 0.0136
2025-04-08 19:47:28.056044: Training Step 259/354: batchLoss = 0.4355, diffLoss = 2.1254, kgLoss = 0.0130
2025-04-08 19:47:29.679231: Training Step 260/354: batchLoss = 0.5074, diffLoss = 2.4802, kgLoss = 0.0142
2025-04-08 19:47:31.294845: Training Step 261/354: batchLoss = 0.5919, diffLoss = 2.8855, kgLoss = 0.0184
2025-04-08 19:47:32.916099: Training Step 262/354: batchLoss = 0.6537, diffLoss = 3.2028, kgLoss = 0.0164
2025-04-08 19:47:34.533753: Training Step 263/354: batchLoss = 0.6333, diffLoss = 3.1055, kgLoss = 0.0153
2025-04-08 19:47:36.155056: Training Step 264/354: batchLoss = 0.5021, diffLoss = 2.4521, kgLoss = 0.0147
2025-04-08 19:47:37.775438: Training Step 265/354: batchLoss = 0.5831, diffLoss = 2.8484, kgLoss = 0.0167
2025-04-08 19:47:39.389009: Training Step 266/354: batchLoss = 0.4616, diffLoss = 2.2545, kgLoss = 0.0133
2025-04-08 19:47:41.003530: Training Step 267/354: batchLoss = 0.5593, diffLoss = 2.7406, kgLoss = 0.0139
2025-04-08 19:47:42.618505: Training Step 268/354: batchLoss = 0.5157, diffLoss = 2.5222, kgLoss = 0.0141
2025-04-08 19:47:44.236712: Training Step 269/354: batchLoss = 0.5565, diffLoss = 2.7263, kgLoss = 0.0140
2025-04-08 19:47:45.855706: Training Step 270/354: batchLoss = 0.4898, diffLoss = 2.3813, kgLoss = 0.0170
2025-04-08 19:47:47.480386: Training Step 271/354: batchLoss = 0.6227, diffLoss = 3.0506, kgLoss = 0.0157
2025-04-08 19:47:49.099070: Training Step 272/354: batchLoss = 0.5527, diffLoss = 2.7044, kgLoss = 0.0148
2025-04-08 19:47:50.716411: Training Step 273/354: batchLoss = 0.4912, diffLoss = 2.4020, kgLoss = 0.0135
2025-04-08 19:47:52.337777: Training Step 274/354: batchLoss = 0.4675, diffLoss = 2.2910, kgLoss = 0.0117
2025-04-08 19:47:53.961800: Training Step 275/354: batchLoss = 0.6185, diffLoss = 3.0251, kgLoss = 0.0169
2025-04-08 19:47:55.579638: Training Step 276/354: batchLoss = 0.4953, diffLoss = 2.4220, kgLoss = 0.0137
2025-04-08 19:47:57.198045: Training Step 277/354: batchLoss = 0.4916, diffLoss = 2.4062, kgLoss = 0.0129
2025-04-08 19:47:58.805054: Training Step 278/354: batchLoss = 0.5656, diffLoss = 2.7663, kgLoss = 0.0154
2025-04-08 19:48:00.412633: Training Step 279/354: batchLoss = 0.6062, diffLoss = 2.9660, kgLoss = 0.0163
2025-04-08 19:48:02.032766: Training Step 280/354: batchLoss = 0.6474, diffLoss = 3.1760, kgLoss = 0.0152
2025-04-08 19:48:03.654427: Training Step 281/354: batchLoss = 0.4611, diffLoss = 2.2517, kgLoss = 0.0134
2025-04-08 19:48:05.283772: Training Step 282/354: batchLoss = 0.5837, diffLoss = 2.8611, kgLoss = 0.0144
2025-04-08 19:48:06.903205: Training Step 283/354: batchLoss = 0.4603, diffLoss = 2.2523, kgLoss = 0.0124
2025-04-08 19:48:08.528871: Training Step 284/354: batchLoss = 0.5561, diffLoss = 2.7186, kgLoss = 0.0155
2025-04-08 19:48:10.158585: Training Step 285/354: batchLoss = 0.5283, diffLoss = 2.5807, kgLoss = 0.0152
2025-04-08 19:48:11.782205: Training Step 286/354: batchLoss = 0.6050, diffLoss = 2.9590, kgLoss = 0.0165
2025-04-08 19:48:13.403675: Training Step 287/354: batchLoss = 0.6519, diffLoss = 3.1962, kgLoss = 0.0159
2025-04-08 19:48:15.024408: Training Step 288/354: batchLoss = 0.4902, diffLoss = 2.4015, kgLoss = 0.0123
2025-04-08 19:48:16.644321: Training Step 289/354: batchLoss = 0.5559, diffLoss = 2.7130, kgLoss = 0.0166
2025-04-08 19:48:18.258921: Training Step 290/354: batchLoss = 0.5674, diffLoss = 2.7823, kgLoss = 0.0137
2025-04-08 19:48:19.880207: Training Step 291/354: batchLoss = 0.4848, diffLoss = 2.3695, kgLoss = 0.0136
2025-04-08 19:48:21.501959: Training Step 292/354: batchLoss = 0.5907, diffLoss = 2.8925, kgLoss = 0.0152
2025-04-08 19:48:23.121607: Training Step 293/354: batchLoss = 1.0117, diffLoss = 4.9459, kgLoss = 0.0282
2025-04-08 19:48:24.760416: Training Step 294/354: batchLoss = 0.4290, diffLoss = 2.0960, kgLoss = 0.0123
2025-04-08 19:48:26.380152: Training Step 295/354: batchLoss = 0.6443, diffLoss = 3.1577, kgLoss = 0.0160
2025-04-08 19:48:28.002726: Training Step 296/354: batchLoss = 0.5330, diffLoss = 2.6042, kgLoss = 0.0152
2025-04-08 19:48:29.624950: Training Step 297/354: batchLoss = 1.7656, diffLoss = 8.6617, kgLoss = 0.0416
2025-04-08 19:48:31.245054: Training Step 298/354: batchLoss = 0.5485, diffLoss = 2.6900, kgLoss = 0.0132
2025-04-08 19:48:32.861104: Training Step 299/354: batchLoss = 0.5016, diffLoss = 2.4550, kgLoss = 0.0132
2025-04-08 19:48:34.482748: Training Step 300/354: batchLoss = 0.6558, diffLoss = 3.2127, kgLoss = 0.0166
2025-04-08 19:48:36.108002: Training Step 301/354: batchLoss = 0.5194, diffLoss = 2.5338, kgLoss = 0.0158
2025-04-08 19:48:37.736103: Training Step 302/354: batchLoss = 0.4880, diffLoss = 2.3874, kgLoss = 0.0131
2025-04-08 19:48:39.361892: Training Step 303/354: batchLoss = 0.4708, diffLoss = 2.2971, kgLoss = 0.0142
2025-04-08 19:48:40.979861: Training Step 304/354: batchLoss = 0.6910, diffLoss = 3.3791, kgLoss = 0.0190
2025-04-08 19:48:42.600494: Training Step 305/354: batchLoss = 0.6034, diffLoss = 2.9573, kgLoss = 0.0149
2025-04-08 19:48:44.222943: Training Step 306/354: batchLoss = 0.4927, diffLoss = 2.4168, kgLoss = 0.0117
2025-04-08 19:48:45.842938: Training Step 307/354: batchLoss = 0.6073, diffLoss = 2.9676, kgLoss = 0.0173
2025-04-08 19:48:47.460140: Training Step 308/354: batchLoss = 0.5442, diffLoss = 2.6649, kgLoss = 0.0140
2025-04-08 19:48:49.079988: Training Step 309/354: batchLoss = 0.5447, diffLoss = 2.6738, kgLoss = 0.0124
2025-04-08 19:48:50.697370: Training Step 310/354: batchLoss = 0.5514, diffLoss = 2.7016, kgLoss = 0.0138
2025-04-08 19:48:52.322263: Training Step 311/354: batchLoss = 0.5817, diffLoss = 2.8492, kgLoss = 0.0149
2025-04-08 19:48:53.942712: Training Step 312/354: batchLoss = 0.5889, diffLoss = 2.8836, kgLoss = 0.0153
2025-04-08 19:48:55.568518: Training Step 313/354: batchLoss = 0.5933, diffLoss = 2.8996, kgLoss = 0.0167
2025-04-08 19:48:57.188584: Training Step 314/354: batchLoss = 0.4987, diffLoss = 2.4461, kgLoss = 0.0118
2025-04-08 19:48:58.808653: Training Step 315/354: batchLoss = 0.6263, diffLoss = 3.0625, kgLoss = 0.0173
2025-04-08 19:49:00.439390: Training Step 316/354: batchLoss = 0.5443, diffLoss = 2.6664, kgLoss = 0.0137
2025-04-08 19:49:02.057357: Training Step 317/354: batchLoss = 0.4990, diffLoss = 2.4425, kgLoss = 0.0131
2025-04-08 19:49:03.679527: Training Step 318/354: batchLoss = 0.4951, diffLoss = 2.4214, kgLoss = 0.0136
2025-04-08 19:49:05.291382: Training Step 319/354: batchLoss = 0.6236, diffLoss = 3.0524, kgLoss = 0.0164
2025-04-08 19:49:06.905212: Training Step 320/354: batchLoss = 0.5845, diffLoss = 2.8613, kgLoss = 0.0153
2025-04-08 19:49:08.537891: Training Step 321/354: batchLoss = 0.4259, diffLoss = 2.0786, kgLoss = 0.0127
2025-04-08 19:49:10.155858: Training Step 322/354: batchLoss = 0.8947, diffLoss = 4.3881, kgLoss = 0.0214
2025-04-08 19:49:11.778095: Training Step 323/354: batchLoss = 0.5136, diffLoss = 2.5157, kgLoss = 0.0130
2025-04-08 19:49:13.411615: Training Step 324/354: batchLoss = 0.6236, diffLoss = 3.0584, kgLoss = 0.0150
2025-04-08 19:49:15.028382: Training Step 325/354: batchLoss = 0.5121, diffLoss = 2.4997, kgLoss = 0.0153
2025-04-08 19:49:16.653503: Training Step 326/354: batchLoss = 0.4925, diffLoss = 2.4108, kgLoss = 0.0129
2025-04-08 19:49:18.274076: Training Step 327/354: batchLoss = 0.4857, diffLoss = 2.3797, kgLoss = 0.0122
2025-04-08 19:49:19.888203: Training Step 328/354: batchLoss = 0.6331, diffLoss = 3.1024, kgLoss = 0.0158
2025-04-08 19:49:21.507509: Training Step 329/354: batchLoss = 0.5229, diffLoss = 2.5600, kgLoss = 0.0136
2025-04-08 19:49:23.126732: Training Step 330/354: batchLoss = 0.5749, diffLoss = 2.8184, kgLoss = 0.0140
2025-04-08 19:49:24.757048: Training Step 331/354: batchLoss = 0.5596, diffLoss = 2.7448, kgLoss = 0.0133
2025-04-08 19:49:26.381771: Training Step 332/354: batchLoss = 0.5919, diffLoss = 2.8937, kgLoss = 0.0164
2025-04-08 19:49:28.003657: Training Step 333/354: batchLoss = 0.5905, diffLoss = 2.8920, kgLoss = 0.0151
2025-04-08 19:49:29.630438: Training Step 334/354: batchLoss = 0.7192, diffLoss = 3.5213, kgLoss = 0.0186
2025-04-08 19:49:31.261806: Training Step 335/354: batchLoss = 0.5247, diffLoss = 2.5709, kgLoss = 0.0132
2025-04-08 19:49:32.884601: Training Step 336/354: batchLoss = 0.5063, diffLoss = 2.4742, kgLoss = 0.0144
2025-04-08 19:49:34.503556: Training Step 337/354: batchLoss = 0.4736, diffLoss = 2.3189, kgLoss = 0.0123
2025-04-08 19:49:36.112308: Training Step 338/354: batchLoss = 0.5897, diffLoss = 2.8868, kgLoss = 0.0154
2025-04-08 19:49:37.729464: Training Step 339/354: batchLoss = 0.7244, diffLoss = 3.5518, kgLoss = 0.0176
2025-04-08 19:49:39.348762: Training Step 340/354: batchLoss = 0.6004, diffLoss = 2.9314, kgLoss = 0.0177
2025-04-08 19:49:40.972955: Training Step 341/354: batchLoss = 0.7041, diffLoss = 3.4409, kgLoss = 0.0199
2025-04-08 19:49:42.602153: Training Step 342/354: batchLoss = 0.5396, diffLoss = 2.6373, kgLoss = 0.0152
2025-04-08 19:49:44.228421: Training Step 343/354: batchLoss = 0.6009, diffLoss = 2.9342, kgLoss = 0.0176
2025-04-08 19:49:45.856231: Training Step 344/354: batchLoss = 0.5715, diffLoss = 2.8003, kgLoss = 0.0143
2025-04-08 19:49:47.481292: Training Step 345/354: batchLoss = 0.5251, diffLoss = 2.5717, kgLoss = 0.0134
2025-04-08 19:49:49.101438: Training Step 346/354: batchLoss = 0.5148, diffLoss = 2.5165, kgLoss = 0.0144
2025-04-08 19:49:50.716080: Training Step 347/354: batchLoss = 0.5348, diffLoss = 2.6180, kgLoss = 0.0139
2025-04-08 19:49:52.337238: Training Step 348/354: batchLoss = 0.5423, diffLoss = 2.6396, kgLoss = 0.0180
2025-04-08 19:49:53.949079: Training Step 349/354: batchLoss = 0.5084, diffLoss = 2.4901, kgLoss = 0.0130
2025-04-08 19:49:55.569909: Training Step 350/354: batchLoss = 0.5685, diffLoss = 2.7785, kgLoss = 0.0159
2025-04-08 19:49:57.183936: Training Step 351/354: batchLoss = 0.6383, diffLoss = 3.1232, kgLoss = 0.0171
2025-04-08 19:49:58.787149: Training Step 352/354: batchLoss = 0.6981, diffLoss = 3.4045, kgLoss = 0.0215
2025-04-08 19:50:00.197518: Training Step 353/354: batchLoss = 0.6872, diffLoss = 3.3665, kgLoss = 0.0174
2025-04-08 19:50:00.287558: 
2025-04-08 19:50:00.288197: Epoch 34/1000, Train: epLoss = 0.9843, epDfLoss = 4.8171, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 19:50:01.609363: Steps 0/138: batch_recall = 46.44, batch_ndcg = 26.74 
2025-04-08 19:50:02.932658: Steps 1/138: batch_recall = 47.82, batch_ndcg = 28.26 
2025-04-08 19:50:04.241916: Steps 2/138: batch_recall = 58.15, batch_ndcg = 35.56 
2025-04-08 19:50:05.561895: Steps 3/138: batch_recall = 57.51, batch_ndcg = 34.12 
2025-04-08 19:50:06.867919: Steps 4/138: batch_recall = 68.15, batch_ndcg = 41.18 
2025-04-08 19:50:08.172358: Steps 5/138: batch_recall = 59.76, batch_ndcg = 31.87 
2025-04-08 19:50:09.469089: Steps 6/138: batch_recall = 51.71, batch_ndcg = 31.55 
2025-04-08 19:50:10.779239: Steps 7/138: batch_recall = 63.56, batch_ndcg = 42.06 
2025-04-08 19:50:12.088414: Steps 8/138: batch_recall = 60.42, batch_ndcg = 38.47 
2025-04-08 19:50:13.403368: Steps 9/138: batch_recall = 59.64, batch_ndcg = 35.17 
2025-04-08 19:50:14.708002: Steps 10/138: batch_recall = 59.18, batch_ndcg = 33.51 
2025-04-08 19:50:16.011542: Steps 11/138: batch_recall = 58.75, batch_ndcg = 33.83 
2025-04-08 19:50:17.318751: Steps 12/138: batch_recall = 50.89, batch_ndcg = 29.48 
2025-04-08 19:50:18.618299: Steps 13/138: batch_recall = 53.78, batch_ndcg = 32.14 
2025-04-08 19:50:19.923590: Steps 14/138: batch_recall = 54.00, batch_ndcg = 31.58 
2025-04-08 19:50:21.223161: Steps 15/138: batch_recall = 47.43, batch_ndcg = 29.72 
2025-04-08 19:50:22.522544: Steps 16/138: batch_recall = 61.07, batch_ndcg = 34.47 
2025-04-08 19:50:23.812398: Steps 17/138: batch_recall = 57.50, batch_ndcg = 33.55 
2025-04-08 19:50:25.111242: Steps 18/138: batch_recall = 52.61, batch_ndcg = 33.33 
2025-04-08 19:50:26.415336: Steps 19/138: batch_recall = 56.52, batch_ndcg = 34.05 
2025-04-08 19:50:27.716017: Steps 20/138: batch_recall = 63.58, batch_ndcg = 36.49 
2025-04-08 19:50:29.022900: Steps 21/138: batch_recall = 68.51, batch_ndcg = 40.18 
2025-04-08 19:50:30.326200: Steps 22/138: batch_recall = 55.85, batch_ndcg = 32.45 
2025-04-08 19:50:31.635474: Steps 23/138: batch_recall = 49.96, batch_ndcg = 30.34 
2025-04-08 19:50:32.936429: Steps 24/138: batch_recall = 58.49, batch_ndcg = 31.75 
2025-04-08 19:50:34.245800: Steps 25/138: batch_recall = 60.84, batch_ndcg = 35.09 
2025-04-08 19:50:35.552094: Steps 26/138: batch_recall = 59.07, batch_ndcg = 33.36 
2025-04-08 19:50:36.849880: Steps 27/138: batch_recall = 55.87, batch_ndcg = 32.30 
2025-04-08 19:50:38.134205: Steps 28/138: batch_recall = 58.88, batch_ndcg = 33.65 
2025-04-08 19:50:39.420718: Steps 29/138: batch_recall = 60.32, batch_ndcg = 31.56 
2025-04-08 19:50:40.711040: Steps 30/138: batch_recall = 56.94, batch_ndcg = 33.78 
2025-04-08 19:50:42.000070: Steps 31/138: batch_recall = 46.66, batch_ndcg = 27.02 
2025-04-08 19:50:43.287635: Steps 32/138: batch_recall = 54.13, batch_ndcg = 31.78 
2025-04-08 19:50:44.580362: Steps 33/138: batch_recall = 61.44, batch_ndcg = 33.54 
2025-04-08 19:50:45.879909: Steps 34/138: batch_recall = 54.44, batch_ndcg = 29.79 
2025-04-08 19:50:47.182621: Steps 35/138: batch_recall = 49.81, batch_ndcg = 29.39 
2025-04-08 19:50:48.489227: Steps 36/138: batch_recall = 46.62, batch_ndcg = 26.91 
2025-04-08 19:50:49.779879: Steps 37/138: batch_recall = 58.12, batch_ndcg = 34.26 
2025-04-08 19:50:51.074106: Steps 38/138: batch_recall = 59.65, batch_ndcg = 33.03 
2025-04-08 19:50:52.374828: Steps 39/138: batch_recall = 65.61, batch_ndcg = 37.91 
2025-04-08 19:50:53.655575: Steps 40/138: batch_recall = 61.01, batch_ndcg = 30.98 
2025-04-08 19:50:54.938495: Steps 41/138: batch_recall = 60.84, batch_ndcg = 34.71 
2025-04-08 19:50:56.218460: Steps 42/138: batch_recall = 55.04, batch_ndcg = 30.83 
2025-04-08 19:50:57.502866: Steps 43/138: batch_recall = 60.15, batch_ndcg = 37.43 
2025-04-08 19:50:58.786224: Steps 44/138: batch_recall = 56.38, batch_ndcg = 30.67 
2025-04-08 19:51:00.081766: Steps 45/138: batch_recall = 62.49, batch_ndcg = 35.96 
2025-04-08 19:51:01.376632: Steps 46/138: batch_recall = 62.08, batch_ndcg = 36.44 
2025-04-08 19:51:02.669268: Steps 47/138: batch_recall = 55.01, batch_ndcg = 32.73 
2025-04-08 19:51:03.956874: Steps 48/138: batch_recall = 61.47, batch_ndcg = 35.67 
2025-04-08 19:51:05.247244: Steps 49/138: batch_recall = 66.00, batch_ndcg = 37.44 
2025-04-08 19:51:06.539435: Steps 50/138: batch_recall = 58.75, batch_ndcg = 31.10 
2025-04-08 19:51:07.826365: Steps 51/138: batch_recall = 64.68, batch_ndcg = 36.71 
2025-04-08 19:51:09.113913: Steps 52/138: batch_recall = 65.55, batch_ndcg = 42.41 
2025-04-08 19:51:10.396329: Steps 53/138: batch_recall = 65.26, batch_ndcg = 34.63 
2025-04-08 19:51:11.667983: Steps 54/138: batch_recall = 69.84, batch_ndcg = 38.34 
2025-04-08 19:51:12.947357: Steps 55/138: batch_recall = 63.04, batch_ndcg = 35.41 
2025-04-08 19:51:14.225987: Steps 56/138: batch_recall = 60.77, batch_ndcg = 35.35 
2025-04-08 19:51:15.521206: Steps 57/138: batch_recall = 57.00, batch_ndcg = 32.22 
2025-04-08 19:51:16.809362: Steps 58/138: batch_recall = 67.81, batch_ndcg = 36.10 
2025-04-08 19:51:18.097342: Steps 59/138: batch_recall = 63.82, batch_ndcg = 37.97 
2025-04-08 19:51:19.387408: Steps 60/138: batch_recall = 68.14, batch_ndcg = 37.85 
2025-04-08 19:51:20.671494: Steps 61/138: batch_recall = 64.87, batch_ndcg = 35.24 
2025-04-08 19:51:21.961014: Steps 62/138: batch_recall = 82.22, batch_ndcg = 44.07 
2025-04-08 19:51:23.251257: Steps 63/138: batch_recall = 75.52, batch_ndcg = 42.82 
2025-04-08 19:51:24.544772: Steps 64/138: batch_recall = 59.96, batch_ndcg = 32.23 
2025-04-08 19:51:25.826485: Steps 65/138: batch_recall = 84.26, batch_ndcg = 47.04 
2025-04-08 19:51:27.105020: Steps 66/138: batch_recall = 68.82, batch_ndcg = 41.08 
2025-04-08 19:51:28.386484: Steps 67/138: batch_recall = 74.81, batch_ndcg = 46.03 
2025-04-08 19:51:29.664841: Steps 68/138: batch_recall = 60.96, batch_ndcg = 32.97 
2025-04-08 19:51:30.944266: Steps 69/138: batch_recall = 88.48, batch_ndcg = 51.76 
2025-04-08 19:51:32.229389: Steps 70/138: batch_recall = 80.37, batch_ndcg = 45.07 
2025-04-08 19:51:33.512233: Steps 71/138: batch_recall = 88.54, batch_ndcg = 52.75 
2025-04-08 19:51:34.796005: Steps 72/138: batch_recall = 85.41, batch_ndcg = 48.93 
2025-04-08 19:51:36.084198: Steps 73/138: batch_recall = 84.29, batch_ndcg = 46.93 
2025-04-08 19:51:37.375294: Steps 74/138: batch_recall = 76.02, batch_ndcg = 48.03 
2025-04-08 19:51:38.654869: Steps 75/138: batch_recall = 81.71, batch_ndcg = 48.92 
2025-04-08 19:51:39.935663: Steps 76/138: batch_recall = 94.29, batch_ndcg = 55.79 
2025-04-08 19:51:41.218149: Steps 77/138: batch_recall = 90.29, batch_ndcg = 51.89 
2025-04-08 19:51:42.496490: Steps 78/138: batch_recall = 90.11, batch_ndcg = 47.92 
2025-04-08 19:51:43.785282: Steps 79/138: batch_recall = 86.43, batch_ndcg = 48.52 
2025-04-08 19:51:45.070401: Steps 80/138: batch_recall = 72.15, batch_ndcg = 38.90 
2025-04-08 19:51:46.357197: Steps 81/138: batch_recall = 78.24, batch_ndcg = 47.34 
2025-04-08 19:51:47.645189: Steps 82/138: batch_recall = 89.20, batch_ndcg = 54.54 
2025-04-08 19:51:48.941965: Steps 83/138: batch_recall = 83.81, batch_ndcg = 48.17 
2025-04-08 19:51:50.234506: Steps 84/138: batch_recall = 100.31, batch_ndcg = 57.73 
2025-04-08 19:51:51.520362: Steps 85/138: batch_recall = 108.18, batch_ndcg = 62.49 
2025-04-08 19:51:52.810344: Steps 86/138: batch_recall = 119.47, batch_ndcg = 71.67 
2025-04-08 19:51:54.095554: Steps 87/138: batch_recall = 105.18, batch_ndcg = 55.46 
2025-04-08 19:51:55.388016: Steps 88/138: batch_recall = 105.30, batch_ndcg = 60.36 
2025-04-08 19:51:56.675873: Steps 89/138: batch_recall = 120.79, batch_ndcg = 69.33 
2025-04-08 19:51:57.952069: Steps 90/138: batch_recall = 103.92, batch_ndcg = 58.44 
2025-04-08 19:51:59.216469: Steps 91/138: batch_recall = 114.73, batch_ndcg = 64.88 
2025-04-08 19:52:00.484858: Steps 92/138: batch_recall = 119.63, batch_ndcg = 66.15 
2025-04-08 19:52:01.757012: Steps 93/138: batch_recall = 119.84, batch_ndcg = 68.84 
2025-04-08 19:52:03.037218: Steps 94/138: batch_recall = 122.37, batch_ndcg = 66.28 
2025-04-08 19:52:04.317227: Steps 95/138: batch_recall = 115.39, batch_ndcg = 68.21 
2025-04-08 19:52:05.589528: Steps 96/138: batch_recall = 133.96, batch_ndcg = 80.19 
2025-04-08 19:52:06.873711: Steps 97/138: batch_recall = 151.74, batch_ndcg = 93.46 
2025-04-08 19:52:08.149234: Steps 98/138: batch_recall = 108.01, batch_ndcg = 64.59 
2025-04-08 19:52:09.425050: Steps 99/138: batch_recall = 125.72, batch_ndcg = 72.36 
2025-04-08 19:52:10.695914: Steps 100/138: batch_recall = 127.41, batch_ndcg = 71.54 
2025-04-08 19:52:11.975184: Steps 101/138: batch_recall = 128.89, batch_ndcg = 69.94 
2025-04-08 19:52:13.243105: Steps 102/138: batch_recall = 121.54, batch_ndcg = 69.44 
2025-04-08 19:52:14.517830: Steps 103/138: batch_recall = 140.40, batch_ndcg = 79.87 
2025-04-08 19:52:15.786426: Steps 104/138: batch_recall = 135.05, batch_ndcg = 79.78 
2025-04-08 19:52:17.059687: Steps 105/138: batch_recall = 117.82, batch_ndcg = 67.77 
2025-04-08 19:52:18.334822: Steps 106/138: batch_recall = 110.10, batch_ndcg = 61.50 
2025-04-08 19:52:19.631336: Steps 107/138: batch_recall = 116.50, batch_ndcg = 65.09 
2025-04-08 19:52:20.905627: Steps 108/138: batch_recall = 119.66, batch_ndcg = 72.29 
2025-04-08 19:52:22.190301: Steps 109/138: batch_recall = 135.83, batch_ndcg = 75.57 
2025-04-08 19:52:23.472667: Steps 110/138: batch_recall = 119.51, batch_ndcg = 64.01 
2025-04-08 19:52:24.753420: Steps 111/138: batch_recall = 133.89, batch_ndcg = 84.53 
2025-04-08 19:52:26.025480: Steps 112/138: batch_recall = 168.63, batch_ndcg = 95.78 
2025-04-08 19:52:27.305888: Steps 113/138: batch_recall = 128.89, batch_ndcg = 71.22 
2025-04-08 19:52:28.582411: Steps 114/138: batch_recall = 125.85, batch_ndcg = 73.47 
2025-04-08 19:52:29.852296: Steps 115/138: batch_recall = 118.78, batch_ndcg = 63.97 
2025-04-08 19:52:31.120247: Steps 116/138: batch_recall = 128.28, batch_ndcg = 68.41 
2025-04-08 19:52:32.391339: Steps 117/138: batch_recall = 110.69, batch_ndcg = 65.81 
2025-04-08 19:52:33.659421: Steps 118/138: batch_recall = 124.62, batch_ndcg = 68.69 
2025-04-08 19:52:34.932324: Steps 119/138: batch_recall = 143.69, batch_ndcg = 77.71 
2025-04-08 19:52:36.207720: Steps 120/138: batch_recall = 122.08, batch_ndcg = 71.06 
2025-04-08 19:52:37.489279: Steps 121/138: batch_recall = 148.73, batch_ndcg = 78.96 
2025-04-08 19:52:38.761943: Steps 122/138: batch_recall = 150.33, batch_ndcg = 81.45 
2025-04-08 19:52:40.050348: Steps 123/138: batch_recall = 132.17, batch_ndcg = 75.23 
2025-04-08 19:52:41.336304: Steps 124/138: batch_recall = 153.46, batch_ndcg = 93.21 
2025-04-08 19:52:42.622997: Steps 125/138: batch_recall = 135.63, batch_ndcg = 74.34 
2025-04-08 19:52:43.893390: Steps 126/138: batch_recall = 164.39, batch_ndcg = 89.04 
2025-04-08 19:52:45.165099: Steps 127/138: batch_recall = 144.81, batch_ndcg = 82.51 
2025-04-08 19:52:46.432964: Steps 128/138: batch_recall = 128.47, batch_ndcg = 71.91 
2025-04-08 19:52:47.706362: Steps 129/138: batch_recall = 159.36, batch_ndcg = 91.82 
2025-04-08 19:52:48.968137: Steps 130/138: batch_recall = 136.85, batch_ndcg = 71.18 
2025-04-08 19:52:50.237170: Steps 131/138: batch_recall = 152.75, batch_ndcg = 89.28 
2025-04-08 19:52:51.514097: Steps 132/138: batch_recall = 150.91, batch_ndcg = 84.86 
2025-04-08 19:52:52.783147: Steps 133/138: batch_recall = 146.68, batch_ndcg = 84.71 
2025-04-08 19:52:54.057969: Steps 134/138: batch_recall = 143.88, batch_ndcg = 83.81 
2025-04-08 19:52:55.332148: Steps 135/138: batch_recall = 168.83, batch_ndcg = 96.90 
2025-04-08 19:52:56.607719: Steps 136/138: batch_recall = 153.44, batch_ndcg = 79.84 
2025-04-08 19:52:57.888151: Steps 137/138: batch_recall = 138.95, batch_ndcg = 87.14 
2025-04-08 19:52:57.888726: Epoch 34/1000, Test: Recall = 0.1768, NDCG = 0.1011  

2025-04-08 19:52:59.661399: Training Step 0/354: batchLoss = 0.5868, diffLoss = 2.8671, kgLoss = 0.0167
2025-04-08 19:53:01.280654: Training Step 1/354: batchLoss = 0.7866, diffLoss = 3.8617, kgLoss = 0.0179
2025-04-08 19:53:02.898721: Training Step 2/354: batchLoss = 0.5980, diffLoss = 2.9302, kgLoss = 0.0149
2025-04-08 19:53:04.514844: Training Step 3/354: batchLoss = 0.6292, diffLoss = 3.0781, kgLoss = 0.0170
2025-04-08 19:53:06.136624: Training Step 4/354: batchLoss = 0.4825, diffLoss = 2.3614, kgLoss = 0.0128
2025-04-08 19:53:07.755248: Training Step 5/354: batchLoss = 0.4744, diffLoss = 2.3276, kgLoss = 0.0111
2025-04-08 19:53:09.381585: Training Step 6/354: batchLoss = 0.5362, diffLoss = 2.6264, kgLoss = 0.0137
2025-04-08 19:53:11.008908: Training Step 7/354: batchLoss = 0.4626, diffLoss = 2.2652, kgLoss = 0.0120
2025-04-08 19:53:12.631623: Training Step 8/354: batchLoss = 0.5268, diffLoss = 2.5779, kgLoss = 0.0140
2025-04-08 19:53:14.264815: Training Step 9/354: batchLoss = 0.4134, diffLoss = 2.0182, kgLoss = 0.0123
2025-04-08 19:53:15.891925: Training Step 10/354: batchLoss = 0.6106, diffLoss = 2.9820, kgLoss = 0.0177
2025-04-08 19:53:17.503868: Training Step 11/354: batchLoss = 0.5947, diffLoss = 2.9151, kgLoss = 0.0146
2025-04-08 19:53:19.121482: Training Step 12/354: batchLoss = 0.5314, diffLoss = 2.6015, kgLoss = 0.0139
2025-04-08 19:53:20.742637: Training Step 13/354: batchLoss = 0.6024, diffLoss = 2.9395, kgLoss = 0.0182
2025-04-08 19:53:22.361655: Training Step 14/354: batchLoss = 0.5089, diffLoss = 2.4986, kgLoss = 0.0114
2025-04-08 19:53:23.982883: Training Step 15/354: batchLoss = 0.4582, diffLoss = 2.2395, kgLoss = 0.0129
2025-04-08 19:53:25.611453: Training Step 16/354: batchLoss = 0.6091, diffLoss = 2.9813, kgLoss = 0.0160
2025-04-08 19:53:27.235529: Training Step 17/354: batchLoss = 0.5941, diffLoss = 2.9092, kgLoss = 0.0154
2025-04-08 19:53:28.860423: Training Step 18/354: batchLoss = 0.5782, diffLoss = 2.8310, kgLoss = 0.0150
2025-04-08 19:53:30.477299: Training Step 19/354: batchLoss = 0.4290, diffLoss = 2.0995, kgLoss = 0.0114
2025-04-08 19:53:32.102097: Training Step 20/354: batchLoss = 0.4860, diffLoss = 2.3799, kgLoss = 0.0125
2025-04-08 19:53:33.724918: Training Step 21/354: batchLoss = 0.5187, diffLoss = 2.5363, kgLoss = 0.0143
2025-04-08 19:53:35.342435: Training Step 22/354: batchLoss = 0.4770, diffLoss = 2.3283, kgLoss = 0.0142
2025-04-08 19:53:36.956068: Training Step 23/354: batchLoss = 0.5170, diffLoss = 2.5359, kgLoss = 0.0123
2025-04-08 19:53:38.576904: Training Step 24/354: batchLoss = 0.5223, diffLoss = 2.5451, kgLoss = 0.0166
2025-04-08 19:53:40.198188: Training Step 25/354: batchLoss = 0.5780, diffLoss = 2.8280, kgLoss = 0.0155
2025-04-08 19:53:41.825209: Training Step 26/354: batchLoss = 0.5421, diffLoss = 2.6506, kgLoss = 0.0149
2025-04-08 19:53:43.447169: Training Step 27/354: batchLoss = 0.5602, diffLoss = 2.7466, kgLoss = 0.0136
2025-04-08 19:53:45.064672: Training Step 28/354: batchLoss = 0.7181, diffLoss = 3.5154, kgLoss = 0.0188
2025-04-08 19:53:46.690063: Training Step 29/354: batchLoss = 0.4610, diffLoss = 2.2465, kgLoss = 0.0146
2025-04-08 19:53:48.307883: Training Step 30/354: batchLoss = 0.4795, diffLoss = 2.3468, kgLoss = 0.0126
2025-04-08 19:53:49.927152: Training Step 31/354: batchLoss = 0.5658, diffLoss = 2.7635, kgLoss = 0.0164
2025-04-08 19:53:51.542638: Training Step 32/354: batchLoss = 0.4833, diffLoss = 2.3680, kgLoss = 0.0121
2025-04-08 19:53:53.161549: Training Step 33/354: batchLoss = 0.5893, diffLoss = 2.8909, kgLoss = 0.0139
2025-04-08 19:53:54.784146: Training Step 34/354: batchLoss = 0.5160, diffLoss = 2.5270, kgLoss = 0.0132
2025-04-08 19:53:56.406336: Training Step 35/354: batchLoss = 0.4664, diffLoss = 2.2857, kgLoss = 0.0115
2025-04-08 19:53:58.027339: Training Step 36/354: batchLoss = 0.5568, diffLoss = 2.7302, kgLoss = 0.0135
2025-04-08 19:53:59.649358: Training Step 37/354: batchLoss = 0.8171, diffLoss = 4.0071, kgLoss = 0.0196
2025-04-08 19:54:01.261142: Training Step 38/354: batchLoss = 0.5878, diffLoss = 2.8770, kgLoss = 0.0155
2025-04-08 19:54:02.879102: Training Step 39/354: batchLoss = 0.4596, diffLoss = 2.2464, kgLoss = 0.0129
2025-04-08 19:54:04.507818: Training Step 40/354: batchLoss = 0.6567, diffLoss = 3.2185, kgLoss = 0.0163
2025-04-08 19:54:06.130745: Training Step 41/354: batchLoss = 0.5913, diffLoss = 2.8947, kgLoss = 0.0154
2025-04-08 19:54:07.755501: Training Step 42/354: batchLoss = 0.5924, diffLoss = 2.8923, kgLoss = 0.0175
2025-04-08 19:54:09.379250: Training Step 43/354: batchLoss = 0.5036, diffLoss = 2.4667, kgLoss = 0.0129
2025-04-08 19:54:11.010448: Training Step 44/354: batchLoss = 0.6008, diffLoss = 2.9461, kgLoss = 0.0145
2025-04-08 19:54:12.644292: Training Step 45/354: batchLoss = 0.5616, diffLoss = 2.7477, kgLoss = 0.0151
2025-04-08 19:54:14.267738: Training Step 46/354: batchLoss = 0.5954, diffLoss = 2.9137, kgLoss = 0.0158
2025-04-08 19:54:15.880587: Training Step 47/354: batchLoss = 0.4844, diffLoss = 2.3620, kgLoss = 0.0150
2025-04-08 19:54:17.497197: Training Step 48/354: batchLoss = 0.5472, diffLoss = 2.6724, kgLoss = 0.0159
2025-04-08 19:54:19.127651: Training Step 49/354: batchLoss = 0.5814, diffLoss = 2.8463, kgLoss = 0.0151
2025-04-08 19:54:20.748794: Training Step 50/354: batchLoss = 0.7435, diffLoss = 3.6437, kgLoss = 0.0185
2025-04-08 19:54:22.375209: Training Step 51/354: batchLoss = 0.4489, diffLoss = 2.1971, kgLoss = 0.0119
2025-04-08 19:54:23.994356: Training Step 52/354: batchLoss = 0.5267, diffLoss = 2.5794, kgLoss = 0.0135
2025-04-08 19:54:25.619615: Training Step 53/354: batchLoss = 0.4403, diffLoss = 2.1572, kgLoss = 0.0111
2025-04-08 19:54:27.246230: Training Step 54/354: batchLoss = 0.5164, diffLoss = 2.5376, kgLoss = 0.0111
2025-04-08 19:54:28.851081: Training Step 55/354: batchLoss = 0.7241, diffLoss = 3.5455, kgLoss = 0.0187
2025-04-08 19:54:30.463267: Training Step 56/354: batchLoss = 0.4808, diffLoss = 2.3571, kgLoss = 0.0118
2025-04-08 19:54:32.084155: Training Step 57/354: batchLoss = 0.5001, diffLoss = 2.4433, kgLoss = 0.0142
2025-04-08 19:54:33.697563: Training Step 58/354: batchLoss = 0.5410, diffLoss = 2.6364, kgLoss = 0.0171
2025-04-08 19:54:35.314221: Training Step 59/354: batchLoss = 0.7698, diffLoss = 3.7733, kgLoss = 0.0189
2025-04-08 19:54:36.919171: Training Step 60/354: batchLoss = 0.5710, diffLoss = 2.7927, kgLoss = 0.0156
2025-04-08 19:54:38.531995: Training Step 61/354: batchLoss = 0.6130, diffLoss = 3.0068, kgLoss = 0.0145
2025-04-08 19:54:40.156161: Training Step 62/354: batchLoss = 0.5976, diffLoss = 2.9249, kgLoss = 0.0158
2025-04-08 19:54:41.779176: Training Step 63/354: batchLoss = 0.5834, diffLoss = 2.8616, kgLoss = 0.0138
2025-04-08 19:54:43.395332: Training Step 64/354: batchLoss = 0.6774, diffLoss = 3.3260, kgLoss = 0.0152
2025-04-08 19:54:45.014029: Training Step 65/354: batchLoss = 0.5663, diffLoss = 2.7721, kgLoss = 0.0149
2025-04-08 19:54:46.625579: Training Step 66/354: batchLoss = 0.5624, diffLoss = 2.7494, kgLoss = 0.0156
2025-04-08 19:54:48.244884: Training Step 67/354: batchLoss = 0.5038, diffLoss = 2.4638, kgLoss = 0.0138
2025-04-08 19:54:49.859360: Training Step 68/354: batchLoss = 0.5164, diffLoss = 2.5280, kgLoss = 0.0135
2025-04-08 19:54:51.476020: Training Step 69/354: batchLoss = 0.5415, diffLoss = 2.6476, kgLoss = 0.0150
2025-04-08 19:54:53.098206: Training Step 70/354: batchLoss = 0.6164, diffLoss = 3.0146, kgLoss = 0.0169
2025-04-08 19:54:54.719424: Training Step 71/354: batchLoss = 0.5899, diffLoss = 2.8876, kgLoss = 0.0155
2025-04-08 19:54:56.335004: Training Step 72/354: batchLoss = 0.4799, diffLoss = 2.3471, kgLoss = 0.0131
2025-04-08 19:54:57.946020: Training Step 73/354: batchLoss = 0.4660, diffLoss = 2.2819, kgLoss = 0.0120
2025-04-08 19:54:59.568946: Training Step 74/354: batchLoss = 0.4500, diffLoss = 2.1924, kgLoss = 0.0145
2025-04-08 19:55:01.192656: Training Step 75/354: batchLoss = 0.5051, diffLoss = 2.4713, kgLoss = 0.0136
2025-04-08 19:55:02.800719: Training Step 76/354: batchLoss = 0.5040, diffLoss = 2.4707, kgLoss = 0.0123
2025-04-08 19:55:04.413585: Training Step 77/354: batchLoss = 0.4606, diffLoss = 2.2555, kgLoss = 0.0118
2025-04-08 19:55:06.031130: Training Step 78/354: batchLoss = 0.6375, diffLoss = 3.1221, kgLoss = 0.0163
2025-04-08 19:55:07.645384: Training Step 79/354: batchLoss = 0.5810, diffLoss = 2.8383, kgLoss = 0.0166
2025-04-08 19:55:09.322690: Training Step 80/354: batchLoss = 0.5086, diffLoss = 2.4706, kgLoss = 0.0181
2025-04-08 19:55:10.942198: Training Step 81/354: batchLoss = 0.7268, diffLoss = 3.5587, kgLoss = 0.0188
2025-04-08 19:55:12.564683: Training Step 82/354: batchLoss = 0.6156, diffLoss = 3.0188, kgLoss = 0.0148
2025-04-08 19:55:14.174763: Training Step 83/354: batchLoss = 0.4612, diffLoss = 2.2566, kgLoss = 0.0123
2025-04-08 19:55:15.788601: Training Step 84/354: batchLoss = 0.5616, diffLoss = 2.7114, kgLoss = 0.0241
2025-04-08 19:55:17.403444: Training Step 85/354: batchLoss = 0.5220, diffLoss = 2.5538, kgLoss = 0.0141
2025-04-08 19:55:19.019826: Training Step 86/354: batchLoss = 0.5581, diffLoss = 2.7300, kgLoss = 0.0152
2025-04-08 19:55:20.629596: Training Step 87/354: batchLoss = 0.5651, diffLoss = 2.7672, kgLoss = 0.0146
2025-04-08 19:55:22.248291: Training Step 88/354: batchLoss = 0.5014, diffLoss = 2.4563, kgLoss = 0.0127
2025-04-08 19:55:23.864285: Training Step 89/354: batchLoss = 0.5343, diffLoss = 2.6162, kgLoss = 0.0138
2025-04-08 19:55:25.486149: Training Step 90/354: batchLoss = 0.7033, diffLoss = 3.4510, kgLoss = 0.0164
2025-04-08 19:55:27.105793: Training Step 91/354: batchLoss = 0.5940, diffLoss = 2.9083, kgLoss = 0.0154
2025-04-08 19:55:28.725774: Training Step 92/354: batchLoss = 0.4996, diffLoss = 2.4451, kgLoss = 0.0133
2025-04-08 19:55:30.349865: Training Step 93/354: batchLoss = 0.5507, diffLoss = 2.6976, kgLoss = 0.0139
2025-04-08 19:55:31.971654: Training Step 94/354: batchLoss = 0.6523, diffLoss = 3.1982, kgLoss = 0.0158
2025-04-08 19:55:33.585323: Training Step 95/354: batchLoss = 0.4344, diffLoss = 2.1228, kgLoss = 0.0123
2025-04-08 19:55:35.197057: Training Step 96/354: batchLoss = 0.4564, diffLoss = 2.2307, kgLoss = 0.0129
2025-04-08 19:55:36.812632: Training Step 97/354: batchLoss = 0.5867, diffLoss = 2.8697, kgLoss = 0.0159
2025-04-08 19:55:38.436822: Training Step 98/354: batchLoss = 0.5283, diffLoss = 2.5892, kgLoss = 0.0131
2025-04-08 19:55:40.053134: Training Step 99/354: batchLoss = 0.5706, diffLoss = 2.7986, kgLoss = 0.0135
2025-04-08 19:55:41.675696: Training Step 100/354: batchLoss = 0.5009, diffLoss = 2.4497, kgLoss = 0.0137
2025-04-08 19:55:43.304584: Training Step 101/354: batchLoss = 0.5436, diffLoss = 2.6645, kgLoss = 0.0134
2025-04-08 19:55:44.927678: Training Step 102/354: batchLoss = 0.5162, diffLoss = 2.5286, kgLoss = 0.0130
2025-04-08 19:55:46.546157: Training Step 103/354: batchLoss = 0.5575, diffLoss = 2.7303, kgLoss = 0.0144
2025-04-08 19:55:48.169995: Training Step 104/354: batchLoss = 0.5532, diffLoss = 2.7093, kgLoss = 0.0142
2025-04-08 19:55:49.795430: Training Step 105/354: batchLoss = 0.5707, diffLoss = 2.7880, kgLoss = 0.0164
2025-04-08 19:55:51.412038: Training Step 106/354: batchLoss = 0.6046, diffLoss = 2.9599, kgLoss = 0.0158
2025-04-08 19:55:53.023854: Training Step 107/354: batchLoss = 0.5963, diffLoss = 2.9187, kgLoss = 0.0157
2025-04-08 19:55:54.641123: Training Step 108/354: batchLoss = 0.5044, diffLoss = 2.4581, kgLoss = 0.0160
2025-04-08 19:55:56.262764: Training Step 109/354: batchLoss = 0.4321, diffLoss = 2.1051, kgLoss = 0.0139
2025-04-08 19:55:57.887197: Training Step 110/354: batchLoss = 0.6280, diffLoss = 3.0673, kgLoss = 0.0182
2025-04-08 19:55:59.519257: Training Step 111/354: batchLoss = 0.5216, diffLoss = 2.5531, kgLoss = 0.0137
2025-04-08 19:56:01.146545: Training Step 112/354: batchLoss = 0.5716, diffLoss = 2.7965, kgLoss = 0.0153
2025-04-08 19:56:02.775302: Training Step 113/354: batchLoss = 0.5759, diffLoss = 2.8260, kgLoss = 0.0133
2025-04-08 19:56:04.404408: Training Step 114/354: batchLoss = 0.4743, diffLoss = 2.3186, kgLoss = 0.0133
2025-04-08 19:56:06.033003: Training Step 115/354: batchLoss = 0.7116, diffLoss = 3.4904, kgLoss = 0.0168
2025-04-08 19:56:07.652045: Training Step 116/354: batchLoss = 0.5413, diffLoss = 2.6508, kgLoss = 0.0139
2025-04-08 19:56:09.267036: Training Step 117/354: batchLoss = 0.5949, diffLoss = 2.9125, kgLoss = 0.0155
2025-04-08 19:56:10.884140: Training Step 118/354: batchLoss = 0.5128, diffLoss = 2.5107, kgLoss = 0.0134
2025-04-08 19:56:12.502356: Training Step 119/354: batchLoss = 0.5469, diffLoss = 2.6767, kgLoss = 0.0145
2025-04-08 19:56:14.130174: Training Step 120/354: batchLoss = 0.5388, diffLoss = 2.6343, kgLoss = 0.0150
2025-04-08 19:56:15.759251: Training Step 121/354: batchLoss = 0.5154, diffLoss = 2.5202, kgLoss = 0.0143
2025-04-08 19:56:17.380652: Training Step 122/354: batchLoss = 0.5959, diffLoss = 2.9204, kgLoss = 0.0148
2025-04-08 19:56:19.009252: Training Step 123/354: batchLoss = 0.4641, diffLoss = 2.2737, kgLoss = 0.0118
2025-04-08 19:56:20.639421: Training Step 124/354: batchLoss = 0.7620, diffLoss = 3.7350, kgLoss = 0.0188
2025-04-08 19:56:22.257781: Training Step 125/354: batchLoss = 0.4905, diffLoss = 2.3989, kgLoss = 0.0133
2025-04-08 19:56:23.873635: Training Step 126/354: batchLoss = 0.5778, diffLoss = 2.8261, kgLoss = 0.0157
2025-04-08 19:56:25.491809: Training Step 127/354: batchLoss = 0.5193, diffLoss = 2.5409, kgLoss = 0.0139
2025-04-08 19:56:27.110076: Training Step 128/354: batchLoss = 0.6689, diffLoss = 3.2696, kgLoss = 0.0187
2025-04-08 19:56:28.728117: Training Step 129/354: batchLoss = 0.4918, diffLoss = 2.4057, kgLoss = 0.0134
2025-04-08 19:56:30.353577: Training Step 130/354: batchLoss = 0.5443, diffLoss = 2.6657, kgLoss = 0.0139
2025-04-08 19:56:31.977999: Training Step 131/354: batchLoss = 0.5996, diffLoss = 2.9373, kgLoss = 0.0152
2025-04-08 19:56:33.602001: Training Step 132/354: batchLoss = 0.4798, diffLoss = 2.3476, kgLoss = 0.0129
2025-04-08 19:56:35.226081: Training Step 133/354: batchLoss = 0.5095, diffLoss = 2.4977, kgLoss = 0.0125
2025-04-08 19:56:36.854697: Training Step 134/354: batchLoss = 0.5782, diffLoss = 2.8329, kgLoss = 0.0145
2025-04-08 19:56:38.479044: Training Step 135/354: batchLoss = 0.4364, diffLoss = 2.1287, kgLoss = 0.0134
2025-04-08 19:56:40.091794: Training Step 136/354: batchLoss = 0.5434, diffLoss = 2.6603, kgLoss = 0.0142
2025-04-08 19:56:41.706607: Training Step 137/354: batchLoss = 0.5032, diffLoss = 2.4580, kgLoss = 0.0145
2025-04-08 19:56:43.323274: Training Step 138/354: batchLoss = 0.5330, diffLoss = 2.6025, kgLoss = 0.0156
2025-04-08 19:56:44.943247: Training Step 139/354: batchLoss = 0.5666, diffLoss = 2.7715, kgLoss = 0.0153
2025-04-08 19:56:46.561885: Training Step 140/354: batchLoss = 0.5587, diffLoss = 2.7384, kgLoss = 0.0137
2025-04-08 19:56:48.173972: Training Step 141/354: batchLoss = 0.6004, diffLoss = 2.9354, kgLoss = 0.0167
2025-04-08 19:56:49.777507: Training Step 142/354: batchLoss = 0.5699, diffLoss = 2.7867, kgLoss = 0.0157
2025-04-08 19:56:51.392501: Training Step 143/354: batchLoss = 0.5219, diffLoss = 2.5551, kgLoss = 0.0136
2025-04-08 19:56:53.017738: Training Step 144/354: batchLoss = 0.5965, diffLoss = 2.9184, kgLoss = 0.0160
2025-04-08 19:56:54.629188: Training Step 145/354: batchLoss = 0.5248, diffLoss = 2.5648, kgLoss = 0.0148
2025-04-08 19:56:56.244975: Training Step 146/354: batchLoss = 0.5533, diffLoss = 2.7123, kgLoss = 0.0136
2025-04-08 19:56:57.858040: Training Step 147/354: batchLoss = 0.5783, diffLoss = 2.8309, kgLoss = 0.0151
2025-04-08 19:56:59.474412: Training Step 148/354: batchLoss = 0.5264, diffLoss = 2.5780, kgLoss = 0.0136
2025-04-08 19:57:01.098063: Training Step 149/354: batchLoss = 0.4849, diffLoss = 2.3701, kgLoss = 0.0136
2025-04-08 19:57:02.727624: Training Step 150/354: batchLoss = 0.4919, diffLoss = 2.4064, kgLoss = 0.0133
2025-04-08 19:57:04.359637: Training Step 151/354: batchLoss = 0.5975, diffLoss = 2.9255, kgLoss = 0.0155
2025-04-08 19:57:05.992668: Training Step 152/354: batchLoss = 0.5074, diffLoss = 2.4731, kgLoss = 0.0160
2025-04-08 19:57:07.619762: Training Step 153/354: batchLoss = 0.6227, diffLoss = 3.0399, kgLoss = 0.0184
2025-04-08 19:57:09.248066: Training Step 154/354: batchLoss = 0.6125, diffLoss = 2.9900, kgLoss = 0.0182
2025-04-08 19:57:10.869823: Training Step 155/354: batchLoss = 0.5362, diffLoss = 2.6159, kgLoss = 0.0162
2025-04-08 19:57:12.486728: Training Step 156/354: batchLoss = 0.5521, diffLoss = 2.7049, kgLoss = 0.0139
2025-04-08 19:57:14.108291: Training Step 157/354: batchLoss = 0.4302, diffLoss = 2.0965, kgLoss = 0.0136
2025-04-08 19:57:15.728647: Training Step 158/354: batchLoss = 0.5007, diffLoss = 2.4523, kgLoss = 0.0128
2025-04-08 19:57:17.349460: Training Step 159/354: batchLoss = 0.4821, diffLoss = 2.3570, kgLoss = 0.0134
2025-04-08 19:57:18.977288: Training Step 160/354: batchLoss = 0.6060, diffLoss = 2.9684, kgLoss = 0.0154
2025-04-08 19:57:20.607299: Training Step 161/354: batchLoss = 0.4677, diffLoss = 2.2841, kgLoss = 0.0136
2025-04-08 19:57:22.233903: Training Step 162/354: batchLoss = 0.4205, diffLoss = 2.0512, kgLoss = 0.0128
2025-04-08 19:57:23.857810: Training Step 163/354: batchLoss = 0.6077, diffLoss = 2.9723, kgLoss = 0.0166
2025-04-08 19:57:25.483658: Training Step 164/354: batchLoss = 0.4738, diffLoss = 2.3120, kgLoss = 0.0142
2025-04-08 19:57:27.104812: Training Step 165/354: batchLoss = 0.4967, diffLoss = 2.4284, kgLoss = 0.0137
2025-04-08 19:57:28.732466: Training Step 166/354: batchLoss = 0.3945, diffLoss = 1.9229, kgLoss = 0.0123
2025-04-08 19:57:30.361058: Training Step 167/354: batchLoss = 0.6680, diffLoss = 3.2812, kgLoss = 0.0147
2025-04-08 19:57:31.987425: Training Step 168/354: batchLoss = 0.5357, diffLoss = 2.6207, kgLoss = 0.0145
2025-04-08 19:57:33.618640: Training Step 169/354: batchLoss = 0.5557, diffLoss = 2.7206, kgLoss = 0.0144
2025-04-08 19:57:35.255340: Training Step 170/354: batchLoss = 0.4731, diffLoss = 2.3085, kgLoss = 0.0142
2025-04-08 19:57:36.885088: Training Step 171/354: batchLoss = 0.6592, diffLoss = 3.1250, kgLoss = 0.0428
2025-04-08 19:57:38.519506: Training Step 172/354: batchLoss = 0.5425, diffLoss = 2.6557, kgLoss = 0.0142
2025-04-08 19:57:40.152061: Training Step 173/354: batchLoss = 0.5730, diffLoss = 2.7965, kgLoss = 0.0171
2025-04-08 19:57:41.784910: Training Step 174/354: batchLoss = 0.5514, diffLoss = 2.7027, kgLoss = 0.0135
2025-04-08 19:57:43.402025: Training Step 175/354: batchLoss = 0.5564, diffLoss = 2.7223, kgLoss = 0.0150
2025-04-08 19:57:45.015988: Training Step 176/354: batchLoss = 0.5446, diffLoss = 2.6733, kgLoss = 0.0125
2025-04-08 19:57:46.638387: Training Step 177/354: batchLoss = 0.4942, diffLoss = 2.4061, kgLoss = 0.0163
2025-04-08 19:57:48.261777: Training Step 178/354: batchLoss = 0.5994, diffLoss = 2.9299, kgLoss = 0.0168
2025-04-08 19:57:49.894935: Training Step 179/354: batchLoss = 0.5405, diffLoss = 2.6457, kgLoss = 0.0142
2025-04-08 19:57:51.527488: Training Step 180/354: batchLoss = 0.6888, diffLoss = 3.3764, kgLoss = 0.0169
2025-04-08 19:57:53.165815: Training Step 181/354: batchLoss = 0.5847, diffLoss = 2.8627, kgLoss = 0.0152
2025-04-08 19:57:54.799847: Training Step 182/354: batchLoss = 0.4558, diffLoss = 2.2320, kgLoss = 0.0117
2025-04-08 19:57:56.424680: Training Step 183/354: batchLoss = 0.6836, diffLoss = 3.3463, kgLoss = 0.0179
2025-04-08 19:57:58.059526: Training Step 184/354: batchLoss = 0.5086, diffLoss = 2.4879, kgLoss = 0.0138
2025-04-08 19:57:59.693412: Training Step 185/354: batchLoss = 0.6055, diffLoss = 2.9653, kgLoss = 0.0156
2025-04-08 19:58:01.315093: Training Step 186/354: batchLoss = 0.5054, diffLoss = 2.4809, kgLoss = 0.0116
2025-04-08 19:58:02.937856: Training Step 187/354: batchLoss = 0.5488, diffLoss = 2.6901, kgLoss = 0.0135
2025-04-08 19:58:04.571859: Training Step 188/354: batchLoss = 0.5152, diffLoss = 2.5211, kgLoss = 0.0137
2025-04-08 19:58:06.209726: Training Step 189/354: batchLoss = 0.5753, diffLoss = 2.8176, kgLoss = 0.0147
2025-04-08 19:58:07.840493: Training Step 190/354: batchLoss = 0.5677, diffLoss = 2.7828, kgLoss = 0.0139
2025-04-08 19:58:09.470949: Training Step 191/354: batchLoss = 0.5993, diffLoss = 2.9337, kgLoss = 0.0157
2025-04-08 19:58:11.103718: Training Step 192/354: batchLoss = 0.6271, diffLoss = 3.0718, kgLoss = 0.0159
2025-04-08 19:58:12.732219: Training Step 193/354: batchLoss = 0.5242, diffLoss = 2.5595, kgLoss = 0.0153
2025-04-08 19:58:14.367769: Training Step 194/354: batchLoss = 0.4726, diffLoss = 2.3079, kgLoss = 0.0138
2025-04-08 19:58:15.991798: Training Step 195/354: batchLoss = 0.5046, diffLoss = 2.4722, kgLoss = 0.0127
2025-04-08 19:58:17.615399: Training Step 196/354: batchLoss = 0.5205, diffLoss = 2.5460, kgLoss = 0.0142
2025-04-08 19:58:19.236912: Training Step 197/354: batchLoss = 0.4490, diffLoss = 2.2010, kgLoss = 0.0111
2025-04-08 19:58:20.860994: Training Step 198/354: batchLoss = 0.6303, diffLoss = 3.0794, kgLoss = 0.0181
2025-04-08 19:58:22.487131: Training Step 199/354: batchLoss = 0.4959, diffLoss = 2.4235, kgLoss = 0.0140
2025-04-08 19:58:24.121992: Training Step 200/354: batchLoss = 0.5726, diffLoss = 2.8038, kgLoss = 0.0148
2025-04-08 19:58:25.749910: Training Step 201/354: batchLoss = 0.5306, diffLoss = 2.5868, kgLoss = 0.0166
2025-04-08 19:58:27.381712: Training Step 202/354: batchLoss = 0.5359, diffLoss = 2.6222, kgLoss = 0.0143
2025-04-08 19:58:29.017213: Training Step 203/354: batchLoss = 0.5388, diffLoss = 2.6407, kgLoss = 0.0134
2025-04-08 19:58:30.638929: Training Step 204/354: batchLoss = 0.8372, diffLoss = 4.1025, kgLoss = 0.0209
2025-04-08 19:58:32.255956: Training Step 205/354: batchLoss = 0.4412, diffLoss = 2.1492, kgLoss = 0.0143
2025-04-08 19:58:33.870609: Training Step 206/354: batchLoss = 0.5302, diffLoss = 2.5958, kgLoss = 0.0138
2025-04-08 19:58:35.493341: Training Step 207/354: batchLoss = 0.5731, diffLoss = 2.8092, kgLoss = 0.0141
2025-04-08 19:58:37.122697: Training Step 208/354: batchLoss = 0.5397, diffLoss = 2.6487, kgLoss = 0.0124
2025-04-08 19:58:38.752362: Training Step 209/354: batchLoss = 0.5614, diffLoss = 2.7430, kgLoss = 0.0160
2025-04-08 19:58:40.387251: Training Step 210/354: batchLoss = 0.4594, diffLoss = 2.2535, kgLoss = 0.0109
2025-04-08 19:58:42.012717: Training Step 211/354: batchLoss = 0.4879, diffLoss = 2.3831, kgLoss = 0.0141
2025-04-08 19:58:43.641576: Training Step 212/354: batchLoss = 0.5986, diffLoss = 2.9335, kgLoss = 0.0149
2025-04-08 19:58:45.280249: Training Step 213/354: batchLoss = 0.5794, diffLoss = 2.8351, kgLoss = 0.0154
2025-04-08 19:58:46.899780: Training Step 214/354: batchLoss = 0.4765, diffLoss = 2.3301, kgLoss = 0.0131
2025-04-08 19:58:48.527916: Training Step 215/354: batchLoss = 0.6507, diffLoss = 3.1847, kgLoss = 0.0172
2025-04-08 19:58:50.153161: Training Step 216/354: batchLoss = 0.4480, diffLoss = 2.1882, kgLoss = 0.0130
2025-04-08 19:58:51.773420: Training Step 217/354: batchLoss = 0.5155, diffLoss = 2.5174, kgLoss = 0.0150
2025-04-08 19:58:53.400889: Training Step 218/354: batchLoss = 0.4937, diffLoss = 2.4175, kgLoss = 0.0128
2025-04-08 19:58:55.038585: Training Step 219/354: batchLoss = 0.4911, diffLoss = 2.4019, kgLoss = 0.0134
2025-04-08 19:58:56.683830: Training Step 220/354: batchLoss = 0.4407, diffLoss = 2.1541, kgLoss = 0.0124
2025-04-08 19:58:58.313277: Training Step 221/354: batchLoss = 0.5067, diffLoss = 2.4784, kgLoss = 0.0138
2025-04-08 19:58:59.945795: Training Step 222/354: batchLoss = 0.4869, diffLoss = 2.3856, kgLoss = 0.0122
2025-04-08 19:59:01.568696: Training Step 223/354: batchLoss = 0.3695, diffLoss = 1.8037, kgLoss = 0.0109
2025-04-08 19:59:03.191529: Training Step 224/354: batchLoss = 0.4439, diffLoss = 2.1594, kgLoss = 0.0150
2025-04-08 19:59:04.818809: Training Step 225/354: batchLoss = 0.6782, diffLoss = 3.3222, kgLoss = 0.0171
2025-04-08 19:59:06.434944: Training Step 226/354: batchLoss = 0.5587, diffLoss = 2.7290, kgLoss = 0.0161
2025-04-08 19:59:08.059362: Training Step 227/354: batchLoss = 0.6124, diffLoss = 3.0010, kgLoss = 0.0153
2025-04-08 19:59:09.691530: Training Step 228/354: batchLoss = 0.5659, diffLoss = 2.7768, kgLoss = 0.0132
2025-04-08 19:59:11.324682: Training Step 229/354: batchLoss = 0.5601, diffLoss = 2.7325, kgLoss = 0.0170
2025-04-08 19:59:12.946906: Training Step 230/354: batchLoss = 0.4434, diffLoss = 2.1642, kgLoss = 0.0132
2025-04-08 19:59:14.580936: Training Step 231/354: batchLoss = 0.5621, diffLoss = 2.7582, kgLoss = 0.0130
2025-04-08 19:59:16.208312: Training Step 232/354: batchLoss = 0.5800, diffLoss = 2.8447, kgLoss = 0.0138
2025-04-08 19:59:17.841637: Training Step 233/354: batchLoss = 0.4103, diffLoss = 2.0072, kgLoss = 0.0110
2025-04-08 19:59:19.463174: Training Step 234/354: batchLoss = 0.5010, diffLoss = 2.4475, kgLoss = 0.0143
2025-04-08 19:59:21.080479: Training Step 235/354: batchLoss = 0.6578, diffLoss = 3.2223, kgLoss = 0.0167
2025-04-08 19:59:22.704365: Training Step 236/354: batchLoss = 0.6061, diffLoss = 2.9705, kgLoss = 0.0150
2025-04-08 19:59:24.326821: Training Step 237/354: batchLoss = 0.5916, diffLoss = 2.9030, kgLoss = 0.0137
2025-04-08 19:59:25.954160: Training Step 238/354: batchLoss = 0.4833, diffLoss = 2.3648, kgLoss = 0.0129
2025-04-08 19:59:27.587502: Training Step 239/354: batchLoss = 0.5525, diffLoss = 2.7002, kgLoss = 0.0155
2025-04-08 19:59:29.224591: Training Step 240/354: batchLoss = 0.5574, diffLoss = 2.7304, kgLoss = 0.0141
2025-04-08 19:59:30.855601: Training Step 241/354: batchLoss = 0.5739, diffLoss = 2.8142, kgLoss = 0.0139
2025-04-08 19:59:32.478740: Training Step 242/354: batchLoss = 0.6136, diffLoss = 3.0006, kgLoss = 0.0168
2025-04-08 19:59:34.108204: Training Step 243/354: batchLoss = 0.5094, diffLoss = 2.4889, kgLoss = 0.0146
2025-04-08 19:59:35.730427: Training Step 244/354: batchLoss = 0.4916, diffLoss = 2.4041, kgLoss = 0.0135
2025-04-08 19:59:37.355095: Training Step 245/354: batchLoss = 0.5931, diffLoss = 2.9019, kgLoss = 0.0160
2025-04-08 19:59:38.981362: Training Step 246/354: batchLoss = 0.5524, diffLoss = 2.6926, kgLoss = 0.0174
2025-04-08 19:59:40.606323: Training Step 247/354: batchLoss = 0.4852, diffLoss = 2.3750, kgLoss = 0.0127
2025-04-08 19:59:42.234623: Training Step 248/354: batchLoss = 0.5248, diffLoss = 2.5697, kgLoss = 0.0135
2025-04-08 19:59:43.865782: Training Step 249/354: batchLoss = 0.5195, diffLoss = 2.5446, kgLoss = 0.0133
2025-04-08 19:59:45.494597: Training Step 250/354: batchLoss = 0.6556, diffLoss = 3.2128, kgLoss = 0.0163
2025-04-08 19:59:47.123885: Training Step 251/354: batchLoss = 0.5624, diffLoss = 2.7466, kgLoss = 0.0164
2025-04-08 19:59:48.752351: Training Step 252/354: batchLoss = 0.6528, diffLoss = 3.1892, kgLoss = 0.0187
2025-04-08 19:59:50.382451: Training Step 253/354: batchLoss = 0.5377, diffLoss = 2.6332, kgLoss = 0.0138
2025-04-08 19:59:52.003891: Training Step 254/354: batchLoss = 0.4871, diffLoss = 2.3796, kgLoss = 0.0140
2025-04-08 19:59:53.625939: Training Step 255/354: batchLoss = 0.5613, diffLoss = 2.7408, kgLoss = 0.0164
2025-04-08 19:59:55.249603: Training Step 256/354: batchLoss = 0.5161, diffLoss = 2.5242, kgLoss = 0.0140
2025-04-08 19:59:56.873774: Training Step 257/354: batchLoss = 0.5467, diffLoss = 2.6759, kgLoss = 0.0144
2025-04-08 19:59:58.502150: Training Step 258/354: batchLoss = 0.4589, diffLoss = 2.2447, kgLoss = 0.0124
2025-04-08 20:00:00.147409: Training Step 259/354: batchLoss = 0.5425, diffLoss = 2.6548, kgLoss = 0.0144
2025-04-08 20:00:01.778767: Training Step 260/354: batchLoss = 0.6291, diffLoss = 3.0825, kgLoss = 0.0158
2025-04-08 20:00:03.408962: Training Step 261/354: batchLoss = 0.5891, diffLoss = 2.8878, kgLoss = 0.0144
2025-04-08 20:00:05.036277: Training Step 262/354: batchLoss = 0.5063, diffLoss = 2.4783, kgLoss = 0.0132
2025-04-08 20:00:06.662276: Training Step 263/354: batchLoss = 0.5553, diffLoss = 2.7107, kgLoss = 0.0164
2025-04-08 20:00:08.284592: Training Step 264/354: batchLoss = 0.5890, diffLoss = 2.8857, kgLoss = 0.0148
2025-04-08 20:00:09.906488: Training Step 265/354: batchLoss = 0.5800, diffLoss = 2.8303, kgLoss = 0.0174
2025-04-08 20:00:11.531195: Training Step 266/354: batchLoss = 0.4496, diffLoss = 2.1974, kgLoss = 0.0126
2025-04-08 20:00:13.156721: Training Step 267/354: batchLoss = 0.5905, diffLoss = 2.8824, kgLoss = 0.0175
2025-04-08 20:00:14.786763: Training Step 268/354: batchLoss = 0.6648, diffLoss = 3.2614, kgLoss = 0.0156
2025-04-08 20:00:16.414311: Training Step 269/354: batchLoss = 0.6270, diffLoss = 3.0645, kgLoss = 0.0176
2025-04-08 20:00:18.055456: Training Step 270/354: batchLoss = 0.4565, diffLoss = 2.2286, kgLoss = 0.0135
2025-04-08 20:00:19.692556: Training Step 271/354: batchLoss = 0.4898, diffLoss = 2.3942, kgLoss = 0.0137
2025-04-08 20:00:21.322040: Training Step 272/354: batchLoss = 0.5419, diffLoss = 2.6532, kgLoss = 0.0140
2025-04-08 20:00:22.944626: Training Step 273/354: batchLoss = 0.5962, diffLoss = 2.9216, kgLoss = 0.0149
2025-04-08 20:00:24.569015: Training Step 274/354: batchLoss = 0.4820, diffLoss = 2.3570, kgLoss = 0.0133
2025-04-08 20:00:26.190682: Training Step 275/354: batchLoss = 0.5399, diffLoss = 2.6431, kgLoss = 0.0141
2025-04-08 20:00:27.810185: Training Step 276/354: batchLoss = 0.5768, diffLoss = 2.8268, kgLoss = 0.0142
2025-04-08 20:00:29.428678: Training Step 277/354: batchLoss = 0.5078, diffLoss = 2.4884, kgLoss = 0.0126
2025-04-08 20:00:31.056071: Training Step 278/354: batchLoss = 0.5713, diffLoss = 2.7983, kgLoss = 0.0146
2025-04-08 20:00:32.691075: Training Step 279/354: batchLoss = 0.5544, diffLoss = 2.7113, kgLoss = 0.0152
2025-04-08 20:00:34.321451: Training Step 280/354: batchLoss = 0.4714, diffLoss = 2.3069, kgLoss = 0.0125
2025-04-08 20:00:35.938300: Training Step 281/354: batchLoss = 0.5414, diffLoss = 2.6593, kgLoss = 0.0119
2025-04-08 20:00:37.565444: Training Step 282/354: batchLoss = 0.4126, diffLoss = 2.0154, kgLoss = 0.0118
2025-04-08 20:00:39.187592: Training Step 283/354: batchLoss = 0.5402, diffLoss = 2.6489, kgLoss = 0.0131
2025-04-08 20:00:40.807471: Training Step 284/354: batchLoss = 0.4409, diffLoss = 2.1558, kgLoss = 0.0122
2025-04-08 20:00:42.430783: Training Step 285/354: batchLoss = 0.6284, diffLoss = 3.0759, kgLoss = 0.0166
2025-04-08 20:00:44.050532: Training Step 286/354: batchLoss = 0.6581, diffLoss = 3.2259, kgLoss = 0.0161
2025-04-08 20:00:45.681281: Training Step 287/354: batchLoss = 0.6064, diffLoss = 2.9648, kgLoss = 0.0168
2025-04-08 20:00:47.312280: Training Step 288/354: batchLoss = 0.5662, diffLoss = 2.7728, kgLoss = 0.0145
2025-04-08 20:00:48.941474: Training Step 289/354: batchLoss = 0.6359, diffLoss = 3.1173, kgLoss = 0.0156
2025-04-08 20:00:50.575303: Training Step 290/354: batchLoss = 0.5714, diffLoss = 2.8004, kgLoss = 0.0142
2025-04-08 20:00:52.206953: Training Step 291/354: batchLoss = 0.4823, diffLoss = 2.3599, kgLoss = 0.0129
2025-04-08 20:00:53.835176: Training Step 292/354: batchLoss = 0.4426, diffLoss = 2.1674, kgLoss = 0.0114
2025-04-08 20:00:55.468118: Training Step 293/354: batchLoss = 0.5010, diffLoss = 2.4458, kgLoss = 0.0148
2025-04-08 20:00:57.089604: Training Step 294/354: batchLoss = 0.5747, diffLoss = 2.8130, kgLoss = 0.0151
2025-04-08 20:00:58.716894: Training Step 295/354: batchLoss = 0.6263, diffLoss = 3.0632, kgLoss = 0.0171
2025-04-08 20:01:00.339424: Training Step 296/354: batchLoss = 0.6421, diffLoss = 3.1387, kgLoss = 0.0180
2025-04-08 20:01:01.964604: Training Step 297/354: batchLoss = 0.5319, diffLoss = 2.6081, kgLoss = 0.0128
2025-04-08 20:01:03.587679: Training Step 298/354: batchLoss = 0.5043, diffLoss = 2.4717, kgLoss = 0.0124
2025-04-08 20:01:05.215966: Training Step 299/354: batchLoss = 0.5925, diffLoss = 2.9035, kgLoss = 0.0148
2025-04-08 20:01:06.838268: Training Step 300/354: batchLoss = 0.5891, diffLoss = 2.8893, kgLoss = 0.0140
2025-04-08 20:01:08.462361: Training Step 301/354: batchLoss = 0.5918, diffLoss = 2.8959, kgLoss = 0.0157
2025-04-08 20:01:10.097971: Training Step 302/354: batchLoss = 0.7386, diffLoss = 3.6227, kgLoss = 0.0176
2025-04-08 20:01:11.725221: Training Step 303/354: batchLoss = 0.4583, diffLoss = 2.2373, kgLoss = 0.0136
2025-04-08 20:01:13.344386: Training Step 304/354: batchLoss = 0.4390, diffLoss = 2.1478, kgLoss = 0.0118
2025-04-08 20:01:14.966424: Training Step 305/354: batchLoss = 0.4000, diffLoss = 1.9490, kgLoss = 0.0128
2025-04-08 20:01:16.586194: Training Step 306/354: batchLoss = 0.5759, diffLoss = 2.8157, kgLoss = 0.0160
2025-04-08 20:01:18.209377: Training Step 307/354: batchLoss = 0.4522, diffLoss = 2.2192, kgLoss = 0.0105
2025-04-08 20:01:19.835134: Training Step 308/354: batchLoss = 0.5294, diffLoss = 2.5933, kgLoss = 0.0134
2025-04-08 20:01:21.471133: Training Step 309/354: batchLoss = 0.6901, diffLoss = 3.3805, kgLoss = 0.0175
2025-04-08 20:01:23.099761: Training Step 310/354: batchLoss = 0.6620, diffLoss = 3.2419, kgLoss = 0.0170
2025-04-08 20:01:24.720094: Training Step 311/354: batchLoss = 0.4779, diffLoss = 2.3275, kgLoss = 0.0155
2025-04-08 20:01:26.348627: Training Step 312/354: batchLoss = 0.6298, diffLoss = 3.0876, kgLoss = 0.0153
2025-04-08 20:01:27.973331: Training Step 313/354: batchLoss = 0.5361, diffLoss = 2.6254, kgLoss = 0.0138
2025-04-08 20:01:29.599884: Training Step 314/354: batchLoss = 0.5668, diffLoss = 2.7845, kgLoss = 0.0124
2025-04-08 20:01:31.222112: Training Step 315/354: batchLoss = 0.4966, diffLoss = 2.4300, kgLoss = 0.0132
2025-04-08 20:01:32.836051: Training Step 316/354: batchLoss = 0.5353, diffLoss = 2.6216, kgLoss = 0.0138
2025-04-08 20:01:34.468210: Training Step 317/354: batchLoss = 0.6138, diffLoss = 3.0036, kgLoss = 0.0163
2025-04-08 20:01:36.093188: Training Step 318/354: batchLoss = 0.4930, diffLoss = 2.4172, kgLoss = 0.0119
2025-04-08 20:01:37.718919: Training Step 319/354: batchLoss = 0.5316, diffLoss = 2.6009, kgLoss = 0.0143
2025-04-08 20:01:39.345843: Training Step 320/354: batchLoss = 0.4444, diffLoss = 2.1751, kgLoss = 0.0117
2025-04-08 20:01:40.976134: Training Step 321/354: batchLoss = 0.5966, diffLoss = 2.9246, kgLoss = 0.0146
2025-04-08 20:01:42.605580: Training Step 322/354: batchLoss = 0.5499, diffLoss = 2.6914, kgLoss = 0.0146
2025-04-08 20:01:44.226856: Training Step 323/354: batchLoss = 0.5563, diffLoss = 2.7219, kgLoss = 0.0149
2025-04-08 20:01:45.846229: Training Step 324/354: batchLoss = 0.5048, diffLoss = 2.4678, kgLoss = 0.0141
2025-04-08 20:01:47.468320: Training Step 325/354: batchLoss = 0.6027, diffLoss = 2.9365, kgLoss = 0.0193
2025-04-08 20:01:49.094941: Training Step 326/354: batchLoss = 0.5163, diffLoss = 2.5296, kgLoss = 0.0129
2025-04-08 20:01:50.724230: Training Step 327/354: batchLoss = 0.5740, diffLoss = 2.8176, kgLoss = 0.0130
2025-04-08 20:01:52.355767: Training Step 328/354: batchLoss = 0.5327, diffLoss = 2.6005, kgLoss = 0.0158
2025-04-08 20:01:53.985426: Training Step 329/354: batchLoss = 0.4144, diffLoss = 2.0279, kgLoss = 0.0110
2025-04-08 20:01:55.617194: Training Step 330/354: batchLoss = 0.5702, diffLoss = 2.7903, kgLoss = 0.0151
2025-04-08 20:01:57.242341: Training Step 331/354: batchLoss = 0.5566, diffLoss = 2.7191, kgLoss = 0.0160
2025-04-08 20:01:58.868739: Training Step 332/354: batchLoss = 0.4609, diffLoss = 2.2525, kgLoss = 0.0130
2025-04-08 20:02:00.491917: Training Step 333/354: batchLoss = 0.5159, diffLoss = 2.5189, kgLoss = 0.0152
2025-04-08 20:02:02.114673: Training Step 334/354: batchLoss = 0.6204, diffLoss = 3.0382, kgLoss = 0.0159
2025-04-08 20:02:03.732770: Training Step 335/354: batchLoss = 0.5581, diffLoss = 2.7386, kgLoss = 0.0130
2025-04-08 20:02:05.354798: Training Step 336/354: batchLoss = 0.5817, diffLoss = 2.8483, kgLoss = 0.0151
2025-04-08 20:02:06.977276: Training Step 337/354: batchLoss = 0.5976, diffLoss = 2.9239, kgLoss = 0.0160
2025-04-08 20:02:08.599571: Training Step 338/354: batchLoss = 0.4924, diffLoss = 2.4140, kgLoss = 0.0120
2025-04-08 20:02:10.226205: Training Step 339/354: batchLoss = 0.4793, diffLoss = 2.3440, kgLoss = 0.0131
2025-04-08 20:02:11.848760: Training Step 340/354: batchLoss = 0.5003, diffLoss = 2.4498, kgLoss = 0.0130
2025-04-08 20:02:13.472513: Training Step 341/354: batchLoss = 0.6341, diffLoss = 3.1069, kgLoss = 0.0159
2025-04-08 20:02:15.087045: Training Step 342/354: batchLoss = 0.6907, diffLoss = 3.3858, kgLoss = 0.0169
2025-04-08 20:02:16.705174: Training Step 343/354: batchLoss = 0.6804, diffLoss = 3.3230, kgLoss = 0.0198
2025-04-08 20:02:18.318003: Training Step 344/354: batchLoss = 0.5521, diffLoss = 2.6975, kgLoss = 0.0158
2025-04-08 20:02:19.923964: Training Step 345/354: batchLoss = 0.6282, diffLoss = 3.0766, kgLoss = 0.0161
2025-04-08 20:02:21.542028: Training Step 346/354: batchLoss = 0.6865, diffLoss = 3.3623, kgLoss = 0.0176
2025-04-08 20:02:23.164221: Training Step 347/354: batchLoss = 0.6567, diffLoss = 3.2156, kgLoss = 0.0170
2025-04-08 20:02:24.786664: Training Step 348/354: batchLoss = 0.6448, diffLoss = 3.1560, kgLoss = 0.0170
2025-04-08 20:02:26.413676: Training Step 349/354: batchLoss = 0.6335, diffLoss = 3.1006, kgLoss = 0.0168
2025-04-08 20:02:28.030391: Training Step 350/354: batchLoss = 0.5707, diffLoss = 2.7987, kgLoss = 0.0137
2025-04-08 20:02:29.649850: Training Step 351/354: batchLoss = 0.9765, diffLoss = 4.7811, kgLoss = 0.0254
2025-04-08 20:02:31.246282: Training Step 352/354: batchLoss = 0.6890, diffLoss = 3.3794, kgLoss = 0.0164
2025-04-08 20:02:32.656591: Training Step 353/354: batchLoss = 0.7560, diffLoss = 3.7058, kgLoss = 0.0186
2025-04-08 20:02:32.747667: 
2025-04-08 20:02:32.748260: Epoch 35/1000, Train: epLoss = 0.9782, epDfLoss = 4.7865, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 20:02:34.079581: Steps 0/138: batch_recall = 45.95, batch_ndcg = 26.35 
2025-04-08 20:02:35.387953: Steps 1/138: batch_recall = 47.60, batch_ndcg = 27.96 
2025-04-08 20:02:36.684507: Steps 2/138: batch_recall = 59.70, batch_ndcg = 35.71 
2025-04-08 20:02:37.998394: Steps 3/138: batch_recall = 57.51, batch_ndcg = 33.77 
2025-04-08 20:02:39.302893: Steps 4/138: batch_recall = 67.37, batch_ndcg = 40.64 
2025-04-08 20:02:40.620813: Steps 5/138: batch_recall = 57.90, batch_ndcg = 31.20 
2025-04-08 20:02:41.934082: Steps 6/138: batch_recall = 51.64, batch_ndcg = 31.56 
2025-04-08 20:02:43.249428: Steps 7/138: batch_recall = 62.32, batch_ndcg = 41.43 
2025-04-08 20:02:44.562592: Steps 8/138: batch_recall = 61.63, batch_ndcg = 38.18 
2025-04-08 20:02:45.873267: Steps 9/138: batch_recall = 58.77, batch_ndcg = 34.70 
2025-04-08 20:02:47.175896: Steps 10/138: batch_recall = 57.68, batch_ndcg = 33.06 
2025-04-08 20:02:48.480218: Steps 11/138: batch_recall = 56.73, batch_ndcg = 33.42 
2025-04-08 20:02:49.792544: Steps 12/138: batch_recall = 48.72, batch_ndcg = 28.56 
2025-04-08 20:02:51.090243: Steps 13/138: batch_recall = 55.22, batch_ndcg = 32.33 
2025-04-08 20:02:52.382112: Steps 14/138: batch_recall = 56.17, batch_ndcg = 32.11 
2025-04-08 20:02:53.684142: Steps 15/138: batch_recall = 47.40, batch_ndcg = 29.96 
2025-04-08 20:02:54.977621: Steps 16/138: batch_recall = 60.83, batch_ndcg = 34.01 
2025-04-08 20:02:56.267695: Steps 17/138: batch_recall = 58.17, batch_ndcg = 33.73 
2025-04-08 20:02:57.580683: Steps 18/138: batch_recall = 53.15, batch_ndcg = 33.47 
2025-04-08 20:02:58.882764: Steps 19/138: batch_recall = 53.95, batch_ndcg = 32.56 
2025-04-08 20:03:00.185497: Steps 20/138: batch_recall = 62.44, batch_ndcg = 35.60 
2025-04-08 20:03:01.492159: Steps 21/138: batch_recall = 68.48, batch_ndcg = 40.34 
2025-04-08 20:03:02.789761: Steps 22/138: batch_recall = 56.11, batch_ndcg = 32.22 
2025-04-08 20:03:04.090514: Steps 23/138: batch_recall = 51.53, batch_ndcg = 30.72 
2025-04-08 20:03:05.402890: Steps 24/138: batch_recall = 58.34, batch_ndcg = 31.73 
2025-04-08 20:03:06.712225: Steps 25/138: batch_recall = 61.91, batch_ndcg = 35.33 
2025-04-08 20:03:08.017605: Steps 26/138: batch_recall = 59.98, batch_ndcg = 33.35 
2025-04-08 20:03:09.312693: Steps 27/138: batch_recall = 54.89, batch_ndcg = 31.99 
2025-04-08 20:03:10.605101: Steps 28/138: batch_recall = 58.57, batch_ndcg = 33.25 
2025-04-08 20:03:11.895143: Steps 29/138: batch_recall = 60.18, batch_ndcg = 31.81 
2025-04-08 20:03:13.191737: Steps 30/138: batch_recall = 59.00, batch_ndcg = 34.22 
2025-04-08 20:03:14.497534: Steps 31/138: batch_recall = 47.16, batch_ndcg = 27.25 
2025-04-08 20:03:15.807093: Steps 32/138: batch_recall = 53.89, batch_ndcg = 31.90 
2025-04-08 20:03:17.103593: Steps 33/138: batch_recall = 59.15, batch_ndcg = 33.01 
2025-04-08 20:03:18.397520: Steps 34/138: batch_recall = 54.94, batch_ndcg = 30.00 
2025-04-08 20:03:19.708659: Steps 35/138: batch_recall = 50.59, batch_ndcg = 29.63 
2025-04-08 20:03:21.017233: Steps 36/138: batch_recall = 46.07, batch_ndcg = 27.37 
2025-04-08 20:03:22.321686: Steps 37/138: batch_recall = 58.04, batch_ndcg = 34.27 
2025-04-08 20:03:23.623628: Steps 38/138: batch_recall = 57.88, batch_ndcg = 32.45 
2025-04-08 20:03:24.903086: Steps 39/138: batch_recall = 68.14, batch_ndcg = 38.56 
2025-04-08 20:03:26.194730: Steps 40/138: batch_recall = 61.70, batch_ndcg = 31.05 
2025-04-08 20:03:27.477088: Steps 41/138: batch_recall = 60.83, batch_ndcg = 34.45 
2025-04-08 20:03:28.763800: Steps 42/138: batch_recall = 55.44, batch_ndcg = 30.76 
2025-04-08 20:03:30.065153: Steps 43/138: batch_recall = 59.42, batch_ndcg = 37.15 
2025-04-08 20:03:31.369842: Steps 44/138: batch_recall = 55.79, batch_ndcg = 30.29 
2025-04-08 20:03:32.664552: Steps 45/138: batch_recall = 62.69, batch_ndcg = 35.82 
2025-04-08 20:03:33.967882: Steps 46/138: batch_recall = 62.86, batch_ndcg = 36.49 
2025-04-08 20:03:35.263563: Steps 47/138: batch_recall = 55.43, batch_ndcg = 32.05 
2025-04-08 20:03:36.561817: Steps 48/138: batch_recall = 60.95, batch_ndcg = 35.95 
2025-04-08 20:03:37.851146: Steps 49/138: batch_recall = 65.19, batch_ndcg = 37.72 
2025-04-08 20:03:39.142520: Steps 50/138: batch_recall = 58.46, batch_ndcg = 30.91 
2025-04-08 20:03:40.426793: Steps 51/138: batch_recall = 66.60, batch_ndcg = 37.22 
2025-04-08 20:03:41.714899: Steps 52/138: batch_recall = 65.99, batch_ndcg = 41.83 
2025-04-08 20:03:43.004701: Steps 53/138: batch_recall = 63.01, batch_ndcg = 34.20 
2025-04-08 20:03:44.289840: Steps 54/138: batch_recall = 67.86, batch_ndcg = 38.04 
2025-04-08 20:03:45.573706: Steps 55/138: batch_recall = 61.19, batch_ndcg = 34.07 
2025-04-08 20:03:46.873664: Steps 56/138: batch_recall = 60.97, batch_ndcg = 35.26 
2025-04-08 20:03:48.173995: Steps 57/138: batch_recall = 57.48, batch_ndcg = 32.08 
2025-04-08 20:03:49.465698: Steps 58/138: batch_recall = 66.65, batch_ndcg = 35.51 
2025-04-08 20:03:50.764421: Steps 59/138: batch_recall = 63.48, batch_ndcg = 37.95 
2025-04-08 20:03:52.061372: Steps 60/138: batch_recall = 66.80, batch_ndcg = 37.66 
2025-04-08 20:03:53.374388: Steps 61/138: batch_recall = 65.34, batch_ndcg = 35.32 
2025-04-08 20:03:54.676205: Steps 62/138: batch_recall = 82.55, batch_ndcg = 44.43 
2025-04-08 20:03:55.980164: Steps 63/138: batch_recall = 75.08, batch_ndcg = 42.65 
2025-04-08 20:03:57.264428: Steps 64/138: batch_recall = 61.31, batch_ndcg = 32.58 
2025-04-08 20:03:58.549924: Steps 65/138: batch_recall = 82.66, batch_ndcg = 46.40 
2025-04-08 20:03:59.846199: Steps 66/138: batch_recall = 69.31, batch_ndcg = 41.27 
2025-04-08 20:04:01.131062: Steps 67/138: batch_recall = 75.13, batch_ndcg = 46.35 
2025-04-08 20:04:02.421730: Steps 68/138: batch_recall = 60.96, batch_ndcg = 32.93 
2025-04-08 20:04:03.711574: Steps 69/138: batch_recall = 85.47, batch_ndcg = 51.21 
2025-04-08 20:04:05.004426: Steps 70/138: batch_recall = 76.44, batch_ndcg = 44.39 
2025-04-08 20:04:06.306866: Steps 71/138: batch_recall = 87.62, batch_ndcg = 52.03 
2025-04-08 20:04:07.603235: Steps 72/138: batch_recall = 86.06, batch_ndcg = 49.05 
2025-04-08 20:04:08.889040: Steps 73/138: batch_recall = 81.59, batch_ndcg = 46.10 
2025-04-08 20:04:10.187875: Steps 74/138: batch_recall = 78.61, batch_ndcg = 48.24 
2025-04-08 20:04:11.476548: Steps 75/138: batch_recall = 81.68, batch_ndcg = 48.84 
2025-04-08 20:04:12.768282: Steps 76/138: batch_recall = 96.57, batch_ndcg = 55.93 
2025-04-08 20:04:14.054680: Steps 77/138: batch_recall = 91.49, batch_ndcg = 51.69 
2025-04-08 20:04:15.337511: Steps 78/138: batch_recall = 89.14, batch_ndcg = 47.39 
2025-04-08 20:04:16.624311: Steps 79/138: batch_recall = 85.68, batch_ndcg = 48.18 
2025-04-08 20:04:17.913688: Steps 80/138: batch_recall = 73.68, batch_ndcg = 39.63 
2025-04-08 20:04:19.215057: Steps 81/138: batch_recall = 79.49, batch_ndcg = 48.13 
2025-04-08 20:04:20.502901: Steps 82/138: batch_recall = 88.68, batch_ndcg = 53.77 
2025-04-08 20:04:21.794293: Steps 83/138: batch_recall = 83.75, batch_ndcg = 48.11 
2025-04-08 20:04:23.071690: Steps 84/138: batch_recall = 98.69, batch_ndcg = 57.13 
2025-04-08 20:04:24.357265: Steps 85/138: batch_recall = 105.12, batch_ndcg = 60.84 
2025-04-08 20:04:25.655478: Steps 86/138: batch_recall = 117.17, batch_ndcg = 70.94 
2025-04-08 20:04:26.942220: Steps 87/138: batch_recall = 105.40, batch_ndcg = 55.75 
2025-04-08 20:04:28.224196: Steps 88/138: batch_recall = 98.77, batch_ndcg = 58.66 
2025-04-08 20:04:29.512471: Steps 89/138: batch_recall = 122.01, batch_ndcg = 69.22 
2025-04-08 20:04:30.793088: Steps 90/138: batch_recall = 99.72, batch_ndcg = 56.33 
2025-04-08 20:04:32.070246: Steps 91/138: batch_recall = 115.17, batch_ndcg = 64.76 
2025-04-08 20:04:33.346405: Steps 92/138: batch_recall = 119.08, batch_ndcg = 65.29 
2025-04-08 20:04:34.636194: Steps 93/138: batch_recall = 119.00, batch_ndcg = 67.87 
2025-04-08 20:04:35.930588: Steps 94/138: batch_recall = 121.73, batch_ndcg = 65.15 
2025-04-08 20:04:37.216618: Steps 95/138: batch_recall = 114.26, batch_ndcg = 67.51 
2025-04-08 20:04:38.502128: Steps 96/138: batch_recall = 128.26, batch_ndcg = 77.17 
2025-04-08 20:04:39.783355: Steps 97/138: batch_recall = 144.94, batch_ndcg = 89.68 
2025-04-08 20:04:41.069725: Steps 98/138: batch_recall = 106.67, batch_ndcg = 63.25 
2025-04-08 20:04:42.352156: Steps 99/138: batch_recall = 123.72, batch_ndcg = 71.40 
2025-04-08 20:04:43.629737: Steps 100/138: batch_recall = 126.21, batch_ndcg = 70.82 
2025-04-08 20:04:44.896227: Steps 101/138: batch_recall = 126.39, batch_ndcg = 68.99 
2025-04-08 20:04:46.173969: Steps 102/138: batch_recall = 123.50, batch_ndcg = 69.89 
2025-04-08 20:04:47.456353: Steps 103/138: batch_recall = 143.82, batch_ndcg = 80.05 
2025-04-08 20:04:48.733667: Steps 104/138: batch_recall = 135.43, batch_ndcg = 78.64 
2025-04-08 20:04:50.013447: Steps 105/138: batch_recall = 120.49, batch_ndcg = 68.86 
2025-04-08 20:04:51.307871: Steps 106/138: batch_recall = 106.02, batch_ndcg = 60.48 
2025-04-08 20:04:52.599491: Steps 107/138: batch_recall = 117.71, batch_ndcg = 65.39 
2025-04-08 20:04:53.883845: Steps 108/138: batch_recall = 118.80, batch_ndcg = 72.14 
2025-04-08 20:04:55.180579: Steps 109/138: batch_recall = 137.00, batch_ndcg = 75.75 
2025-04-08 20:04:56.468402: Steps 110/138: batch_recall = 121.50, batch_ndcg = 64.14 
2025-04-08 20:04:57.756441: Steps 111/138: batch_recall = 135.31, batch_ndcg = 83.93 
2025-04-08 20:04:59.041804: Steps 112/138: batch_recall = 161.22, batch_ndcg = 91.08 
2025-04-08 20:05:00.319157: Steps 113/138: batch_recall = 123.24, batch_ndcg = 69.62 
2025-04-08 20:05:01.597166: Steps 114/138: batch_recall = 124.70, batch_ndcg = 72.92 
2025-04-08 20:05:02.875355: Steps 115/138: batch_recall = 120.25, batch_ndcg = 64.44 
2025-04-08 20:05:04.151730: Steps 116/138: batch_recall = 127.37, batch_ndcg = 68.16 
2025-04-08 20:05:05.426415: Steps 117/138: batch_recall = 111.11, batch_ndcg = 65.99 
2025-04-08 20:05:06.709315: Steps 118/138: batch_recall = 121.12, batch_ndcg = 67.55 
2025-04-08 20:05:07.994646: Steps 119/138: batch_recall = 140.82, batch_ndcg = 76.86 
2025-04-08 20:05:09.284678: Steps 120/138: batch_recall = 120.30, batch_ndcg = 70.26 
2025-04-08 20:05:10.567970: Steps 121/138: batch_recall = 145.53, batch_ndcg = 78.10 
2025-04-08 20:05:11.856434: Steps 122/138: batch_recall = 149.63, batch_ndcg = 80.74 
2025-04-08 20:05:13.141465: Steps 123/138: batch_recall = 128.06, batch_ndcg = 73.24 
2025-04-08 20:05:14.429476: Steps 124/138: batch_recall = 153.07, batch_ndcg = 92.74 
2025-04-08 20:05:15.705396: Steps 125/138: batch_recall = 134.30, batch_ndcg = 74.10 
2025-04-08 20:05:16.978570: Steps 126/138: batch_recall = 158.56, batch_ndcg = 87.25 
2025-04-08 20:05:18.247826: Steps 127/138: batch_recall = 143.56, batch_ndcg = 81.21 
2025-04-08 20:05:19.522159: Steps 128/138: batch_recall = 126.89, batch_ndcg = 71.20 
2025-04-08 20:05:20.792301: Steps 129/138: batch_recall = 159.09, batch_ndcg = 91.72 
2025-04-08 20:05:22.070619: Steps 130/138: batch_recall = 133.02, batch_ndcg = 70.12 
2025-04-08 20:05:23.350623: Steps 131/138: batch_recall = 151.20, batch_ndcg = 87.80 
2025-04-08 20:05:24.638137: Steps 132/138: batch_recall = 148.65, batch_ndcg = 83.62 
2025-04-08 20:05:25.926602: Steps 133/138: batch_recall = 147.02, batch_ndcg = 85.44 
2025-04-08 20:05:27.212294: Steps 134/138: batch_recall = 144.21, batch_ndcg = 83.92 
2025-04-08 20:05:28.495555: Steps 135/138: batch_recall = 168.03, batch_ndcg = 95.55 
2025-04-08 20:05:29.768550: Steps 136/138: batch_recall = 152.14, batch_ndcg = 78.69 
2025-04-08 20:05:31.043308: Steps 137/138: batch_recall = 139.53, batch_ndcg = 87.15 
2025-04-08 20:05:31.043833: Epoch 35/1000, Test: Recall = 0.1755, NDCG = 0.1003  

2025-04-08 20:05:32.815406: Training Step 0/354: batchLoss = 0.4850, diffLoss = 2.3788, kgLoss = 0.0115
2025-04-08 20:05:34.437873: Training Step 1/354: batchLoss = 0.5737, diffLoss = 2.8079, kgLoss = 0.0151
2025-04-08 20:05:36.061422: Training Step 2/354: batchLoss = 0.5056, diffLoss = 2.4754, kgLoss = 0.0132
2025-04-08 20:05:37.686369: Training Step 3/354: batchLoss = 0.4856, diffLoss = 2.3761, kgLoss = 0.0129
2025-04-08 20:05:39.310694: Training Step 4/354: batchLoss = 0.5377, diffLoss = 2.6355, kgLoss = 0.0133
2025-04-08 20:05:40.936142: Training Step 5/354: batchLoss = 0.5141, diffLoss = 2.5202, kgLoss = 0.0125
2025-04-08 20:05:42.571728: Training Step 6/354: batchLoss = 0.4541, diffLoss = 2.2222, kgLoss = 0.0120
2025-04-08 20:05:44.203870: Training Step 7/354: batchLoss = 0.6060, diffLoss = 2.9723, kgLoss = 0.0144
2025-04-08 20:05:45.842566: Training Step 8/354: batchLoss = 0.5868, diffLoss = 2.8752, kgLoss = 0.0147
2025-04-08 20:05:47.466324: Training Step 9/354: batchLoss = 0.6340, diffLoss = 3.1096, kgLoss = 0.0150
2025-04-08 20:05:49.088886: Training Step 10/354: batchLoss = 0.4756, diffLoss = 2.3321, kgLoss = 0.0115
2025-04-08 20:05:50.715730: Training Step 11/354: batchLoss = 0.5917, diffLoss = 2.9013, kgLoss = 0.0143
2025-04-08 20:05:52.348035: Training Step 12/354: batchLoss = 0.5359, diffLoss = 2.6180, kgLoss = 0.0154
2025-04-08 20:05:53.974596: Training Step 13/354: batchLoss = 0.7266, diffLoss = 3.5590, kgLoss = 0.0185
2025-04-08 20:05:55.609029: Training Step 14/354: batchLoss = 0.5789, diffLoss = 2.8340, kgLoss = 0.0152
2025-04-08 20:05:57.242636: Training Step 15/354: batchLoss = 0.5207, diffLoss = 2.5450, kgLoss = 0.0146
2025-04-08 20:05:58.875609: Training Step 16/354: batchLoss = 0.4670, diffLoss = 2.2848, kgLoss = 0.0126
2025-04-08 20:06:00.510771: Training Step 17/354: batchLoss = 0.7705, diffLoss = 3.7714, kgLoss = 0.0202
2025-04-08 20:06:02.145551: Training Step 18/354: batchLoss = 0.5375, diffLoss = 2.6312, kgLoss = 0.0141
2025-04-08 20:06:03.776274: Training Step 19/354: batchLoss = 0.6658, diffLoss = 3.2589, kgLoss = 0.0175
2025-04-08 20:06:05.399782: Training Step 20/354: batchLoss = 0.5303, diffLoss = 2.6042, kgLoss = 0.0118
2025-04-08 20:06:07.025511: Training Step 21/354: batchLoss = 0.4853, diffLoss = 2.3751, kgLoss = 0.0129
2025-04-08 20:06:08.651478: Training Step 22/354: batchLoss = 0.6310, diffLoss = 3.0962, kgLoss = 0.0148
2025-04-08 20:06:10.277164: Training Step 23/354: batchLoss = 0.6094, diffLoss = 2.9726, kgLoss = 0.0185
2025-04-08 20:06:11.904370: Training Step 24/354: batchLoss = 0.4697, diffLoss = 2.2940, kgLoss = 0.0136
2025-04-08 20:06:13.541351: Training Step 25/354: batchLoss = 0.4760, diffLoss = 2.3311, kgLoss = 0.0123
2025-04-08 20:06:15.165419: Training Step 26/354: batchLoss = 0.5250, diffLoss = 2.5655, kgLoss = 0.0149
2025-04-08 20:06:16.796001: Training Step 27/354: batchLoss = 0.4106, diffLoss = 2.0114, kgLoss = 0.0104
2025-04-08 20:06:18.423403: Training Step 28/354: batchLoss = 0.5161, diffLoss = 2.5206, kgLoss = 0.0150
2025-04-08 20:06:20.046697: Training Step 29/354: batchLoss = 0.4899, diffLoss = 2.3983, kgLoss = 0.0128
2025-04-08 20:06:21.670426: Training Step 30/354: batchLoss = 0.4963, diffLoss = 2.4310, kgLoss = 0.0126
2025-04-08 20:06:23.294655: Training Step 31/354: batchLoss = 0.5143, diffLoss = 2.5174, kgLoss = 0.0135
2025-04-08 20:06:24.919116: Training Step 32/354: batchLoss = 0.5930, diffLoss = 2.9031, kgLoss = 0.0155
2025-04-08 20:06:26.552988: Training Step 33/354: batchLoss = 0.6013, diffLoss = 2.9393, kgLoss = 0.0168
2025-04-08 20:06:28.189008: Training Step 34/354: batchLoss = 0.6065, diffLoss = 2.9658, kgLoss = 0.0167
2025-04-08 20:06:29.817665: Training Step 35/354: batchLoss = 0.6249, diffLoss = 3.0612, kgLoss = 0.0158
2025-04-08 20:06:31.449606: Training Step 36/354: batchLoss = 0.4775, diffLoss = 2.3277, kgLoss = 0.0149
2025-04-08 20:06:33.076062: Training Step 37/354: batchLoss = 0.5539, diffLoss = 2.7111, kgLoss = 0.0146
2025-04-08 20:06:34.709682: Training Step 38/354: batchLoss = 0.5432, diffLoss = 2.6553, kgLoss = 0.0152
2025-04-08 20:06:36.337592: Training Step 39/354: batchLoss = 0.4021, diffLoss = 1.9556, kgLoss = 0.0137
2025-04-08 20:06:37.955234: Training Step 40/354: batchLoss = 0.5670, diffLoss = 2.7727, kgLoss = 0.0156
2025-04-08 20:06:39.570518: Training Step 41/354: batchLoss = 0.6181, diffLoss = 3.0212, kgLoss = 0.0174
2025-04-08 20:06:41.193423: Training Step 42/354: batchLoss = 0.5505, diffLoss = 2.6938, kgLoss = 0.0146
2025-04-08 20:06:42.825360: Training Step 43/354: batchLoss = 0.5834, diffLoss = 2.8570, kgLoss = 0.0150
2025-04-08 20:06:44.448242: Training Step 44/354: batchLoss = 0.5728, diffLoss = 2.8038, kgLoss = 0.0151
2025-04-08 20:06:46.076980: Training Step 45/354: batchLoss = 0.4667, diffLoss = 2.2823, kgLoss = 0.0128
2025-04-08 20:06:47.708343: Training Step 46/354: batchLoss = 0.5623, diffLoss = 2.7590, kgLoss = 0.0131
2025-04-08 20:06:49.336640: Training Step 47/354: batchLoss = 0.5015, diffLoss = 2.4575, kgLoss = 0.0125
2025-04-08 20:06:50.960480: Training Step 48/354: batchLoss = 0.6000, diffLoss = 2.9395, kgLoss = 0.0151
2025-04-08 20:06:52.576271: Training Step 49/354: batchLoss = 0.6730, diffLoss = 3.2993, kgLoss = 0.0165
2025-04-08 20:06:54.197591: Training Step 50/354: batchLoss = 0.4600, diffLoss = 2.2396, kgLoss = 0.0151
2025-04-08 20:06:55.811296: Training Step 51/354: batchLoss = 0.5188, diffLoss = 2.5423, kgLoss = 0.0129
2025-04-08 20:06:57.430447: Training Step 52/354: batchLoss = 0.6029, diffLoss = 2.9412, kgLoss = 0.0183
2025-04-08 20:06:59.060924: Training Step 53/354: batchLoss = 0.6733, diffLoss = 3.2992, kgLoss = 0.0168
2025-04-08 20:07:00.687057: Training Step 54/354: batchLoss = 0.5139, diffLoss = 2.5126, kgLoss = 0.0143
2025-04-08 20:07:02.315792: Training Step 55/354: batchLoss = 0.5987, diffLoss = 2.9335, kgLoss = 0.0151
2025-04-08 20:07:03.944820: Training Step 56/354: batchLoss = 0.4812, diffLoss = 2.3573, kgLoss = 0.0121
2025-04-08 20:07:05.575656: Training Step 57/354: batchLoss = 0.5461, diffLoss = 2.6802, kgLoss = 0.0126
2025-04-08 20:07:07.201352: Training Step 58/354: batchLoss = 0.5327, diffLoss = 2.6066, kgLoss = 0.0143
2025-04-08 20:07:08.822159: Training Step 59/354: batchLoss = 0.6679, diffLoss = 3.2753, kgLoss = 0.0161
2025-04-08 20:07:10.444076: Training Step 60/354: batchLoss = 0.5725, diffLoss = 2.7982, kgLoss = 0.0161
2025-04-08 20:07:12.061148: Training Step 61/354: batchLoss = 0.5459, diffLoss = 2.6665, kgLoss = 0.0157
2025-04-08 20:07:13.676770: Training Step 62/354: batchLoss = 0.5469, diffLoss = 2.6808, kgLoss = 0.0134
2025-04-08 20:07:15.304568: Training Step 63/354: batchLoss = 0.5896, diffLoss = 2.8913, kgLoss = 0.0142
2025-04-08 20:07:16.929296: Training Step 64/354: batchLoss = 0.4180, diffLoss = 2.0432, kgLoss = 0.0118
2025-04-08 20:07:18.559237: Training Step 65/354: batchLoss = 0.6555, diffLoss = 3.2117, kgLoss = 0.0164
2025-04-08 20:07:20.192453: Training Step 66/354: batchLoss = 0.5776, diffLoss = 2.8264, kgLoss = 0.0154
2025-04-08 20:07:21.829130: Training Step 67/354: batchLoss = 0.5436, diffLoss = 2.6648, kgLoss = 0.0133
2025-04-08 20:07:23.452622: Training Step 68/354: batchLoss = 0.6880, diffLoss = 3.3654, kgLoss = 0.0186
2025-04-08 20:07:25.079045: Training Step 69/354: batchLoss = 0.5109, diffLoss = 2.5028, kgLoss = 0.0130
2025-04-08 20:07:26.705197: Training Step 70/354: batchLoss = 0.5366, diffLoss = 2.6252, kgLoss = 0.0144
2025-04-08 20:07:28.324112: Training Step 71/354: batchLoss = 0.4505, diffLoss = 2.2047, kgLoss = 0.0120
2025-04-08 20:07:29.957645: Training Step 72/354: batchLoss = 0.4921, diffLoss = 2.4081, kgLoss = 0.0131
2025-04-08 20:07:31.599648: Training Step 73/354: batchLoss = 0.5337, diffLoss = 2.6144, kgLoss = 0.0135
2025-04-08 20:07:33.234421: Training Step 74/354: batchLoss = 0.5317, diffLoss = 2.6073, kgLoss = 0.0128
2025-04-08 20:07:34.866067: Training Step 75/354: batchLoss = 0.4163, diffLoss = 2.0362, kgLoss = 0.0114
2025-04-08 20:07:36.495513: Training Step 76/354: batchLoss = 0.5251, diffLoss = 2.5684, kgLoss = 0.0143
2025-04-08 20:07:38.130775: Training Step 77/354: batchLoss = 0.4957, diffLoss = 2.4182, kgLoss = 0.0151
2025-04-08 20:07:39.757763: Training Step 78/354: batchLoss = 0.5677, diffLoss = 2.7770, kgLoss = 0.0153
2025-04-08 20:07:41.380253: Training Step 79/354: batchLoss = 0.4605, diffLoss = 2.2488, kgLoss = 0.0134
2025-04-08 20:07:43.002425: Training Step 80/354: batchLoss = 0.5801, diffLoss = 2.8407, kgLoss = 0.0150
2025-04-08 20:07:44.631322: Training Step 81/354: batchLoss = 0.7219, diffLoss = 3.5409, kgLoss = 0.0171
2025-04-08 20:07:46.257859: Training Step 82/354: batchLoss = 0.5043, diffLoss = 2.4671, kgLoss = 0.0136
2025-04-08 20:07:47.891345: Training Step 83/354: batchLoss = 0.4821, diffLoss = 2.3597, kgLoss = 0.0127
2025-04-08 20:07:49.525632: Training Step 84/354: batchLoss = 0.4465, diffLoss = 2.1782, kgLoss = 0.0136
2025-04-08 20:07:51.167484: Training Step 85/354: batchLoss = 0.4989, diffLoss = 2.4486, kgLoss = 0.0114
2025-04-08 20:07:52.800299: Training Step 86/354: batchLoss = 0.5536, diffLoss = 2.7094, kgLoss = 0.0147
2025-04-08 20:07:54.431585: Training Step 87/354: batchLoss = 0.4734, diffLoss = 2.3223, kgLoss = 0.0112
2025-04-08 20:07:56.062035: Training Step 88/354: batchLoss = 0.8715, diffLoss = 4.2568, kgLoss = 0.0252
2025-04-08 20:07:57.681825: Training Step 89/354: batchLoss = 0.6242, diffLoss = 3.0614, kgLoss = 0.0149
2025-04-08 20:07:59.304519: Training Step 90/354: batchLoss = 0.5697, diffLoss = 2.7946, kgLoss = 0.0135
2025-04-08 20:08:00.927796: Training Step 91/354: batchLoss = 0.4813, diffLoss = 2.3479, kgLoss = 0.0146
2025-04-08 20:08:02.546328: Training Step 92/354: batchLoss = 0.7276, diffLoss = 3.5619, kgLoss = 0.0191
2025-04-08 20:08:04.175855: Training Step 93/354: batchLoss = 0.4350, diffLoss = 2.1301, kgLoss = 0.0112
2025-04-08 20:08:05.811152: Training Step 94/354: batchLoss = 0.5458, diffLoss = 2.6723, kgLoss = 0.0142
2025-04-08 20:08:07.435943: Training Step 95/354: batchLoss = 0.4767, diffLoss = 2.3410, kgLoss = 0.0107
2025-04-08 20:08:09.059281: Training Step 96/354: batchLoss = 0.5170, diffLoss = 2.5303, kgLoss = 0.0137
2025-04-08 20:08:10.691615: Training Step 97/354: batchLoss = 0.4897, diffLoss = 2.3975, kgLoss = 0.0128
2025-04-08 20:08:12.317400: Training Step 98/354: batchLoss = 0.5122, diffLoss = 2.4995, kgLoss = 0.0154
2025-04-08 20:08:13.940566: Training Step 99/354: batchLoss = 0.5774, diffLoss = 2.8212, kgLoss = 0.0165
2025-04-08 20:08:15.558034: Training Step 100/354: batchLoss = 0.4937, diffLoss = 2.4160, kgLoss = 0.0131
2025-04-08 20:08:17.177889: Training Step 101/354: batchLoss = 0.6948, diffLoss = 3.4048, kgLoss = 0.0173
2025-04-08 20:08:18.802168: Training Step 102/354: batchLoss = 0.6617, diffLoss = 3.2438, kgLoss = 0.0162
2025-04-08 20:08:20.432288: Training Step 103/354: batchLoss = 0.5567, diffLoss = 2.7263, kgLoss = 0.0143
2025-04-08 20:08:22.060707: Training Step 104/354: batchLoss = 0.6477, diffLoss = 3.1691, kgLoss = 0.0173
2025-04-08 20:08:23.691249: Training Step 105/354: batchLoss = 0.5629, diffLoss = 2.7524, kgLoss = 0.0155
2025-04-08 20:08:25.324637: Training Step 106/354: batchLoss = 0.5461, diffLoss = 2.6789, kgLoss = 0.0129
2025-04-08 20:08:26.956996: Training Step 107/354: batchLoss = 0.7018, diffLoss = 3.4373, kgLoss = 0.0179
2025-04-08 20:08:28.586542: Training Step 108/354: batchLoss = 0.5536, diffLoss = 2.7042, kgLoss = 0.0160
2025-04-08 20:08:30.209763: Training Step 109/354: batchLoss = 0.6163, diffLoss = 3.0246, kgLoss = 0.0142
2025-04-08 20:08:31.834591: Training Step 110/354: batchLoss = 0.5442, diffLoss = 2.6657, kgLoss = 0.0138
2025-04-08 20:08:33.455401: Training Step 111/354: batchLoss = 0.6006, diffLoss = 2.9470, kgLoss = 0.0139
2025-04-08 20:08:35.078917: Training Step 112/354: batchLoss = 0.5124, diffLoss = 2.5089, kgLoss = 0.0133
2025-04-08 20:08:36.713413: Training Step 113/354: batchLoss = 0.5298, diffLoss = 2.5858, kgLoss = 0.0158
2025-04-08 20:08:38.352842: Training Step 114/354: batchLoss = 0.5342, diffLoss = 2.6109, kgLoss = 0.0150
2025-04-08 20:08:39.994147: Training Step 115/354: batchLoss = 0.5555, diffLoss = 2.7192, kgLoss = 0.0146
2025-04-08 20:08:41.621406: Training Step 116/354: batchLoss = 0.5274, diffLoss = 2.5830, kgLoss = 0.0135
2025-04-08 20:08:43.251626: Training Step 117/354: batchLoss = 0.6300, diffLoss = 3.0832, kgLoss = 0.0166
2025-04-08 20:08:44.879745: Training Step 118/354: batchLoss = 0.5257, diffLoss = 2.5647, kgLoss = 0.0160
2025-04-08 20:08:46.495086: Training Step 119/354: batchLoss = 0.5017, diffLoss = 2.4558, kgLoss = 0.0131
2025-04-08 20:08:48.167279: Training Step 120/354: batchLoss = 0.4670, diffLoss = 2.2819, kgLoss = 0.0132
2025-04-08 20:08:49.788032: Training Step 121/354: batchLoss = 0.5346, diffLoss = 2.6144, kgLoss = 0.0147
2025-04-08 20:08:51.418896: Training Step 122/354: batchLoss = 0.4598, diffLoss = 2.2478, kgLoss = 0.0128
2025-04-08 20:08:53.046839: Training Step 123/354: batchLoss = 0.4085, diffLoss = 1.9903, kgLoss = 0.0130
2025-04-08 20:08:54.672559: Training Step 124/354: batchLoss = 0.4881, diffLoss = 2.3818, kgLoss = 0.0147
2025-04-08 20:08:56.308910: Training Step 125/354: batchLoss = 0.5571, diffLoss = 2.7238, kgLoss = 0.0154
2025-04-08 20:08:57.943707: Training Step 126/354: batchLoss = 0.5908, diffLoss = 2.8927, kgLoss = 0.0153
2025-04-08 20:08:59.576260: Training Step 127/354: batchLoss = 0.4723, diffLoss = 2.3113, kgLoss = 0.0126
2025-04-08 20:09:01.194472: Training Step 128/354: batchLoss = 0.4842, diffLoss = 2.3665, kgLoss = 0.0136
2025-04-08 20:09:02.821101: Training Step 129/354: batchLoss = 0.4117, diffLoss = 2.0097, kgLoss = 0.0122
2025-04-08 20:09:04.444237: Training Step 130/354: batchLoss = 0.5210, diffLoss = 2.5539, kgLoss = 0.0127
2025-04-08 20:09:06.066099: Training Step 131/354: batchLoss = 0.4976, diffLoss = 2.4370, kgLoss = 0.0128
2025-04-08 20:09:07.689884: Training Step 132/354: batchLoss = 0.5469, diffLoss = 2.6751, kgLoss = 0.0148
2025-04-08 20:09:09.317013: Training Step 133/354: batchLoss = 0.6682, diffLoss = 3.2699, kgLoss = 0.0178
2025-04-08 20:09:10.947100: Training Step 134/354: batchLoss = 0.5316, diffLoss = 2.6006, kgLoss = 0.0144
2025-04-08 20:09:12.577197: Training Step 135/354: batchLoss = 0.6607, diffLoss = 3.2371, kgLoss = 0.0167
2025-04-08 20:09:14.205198: Training Step 136/354: batchLoss = 0.4684, diffLoss = 2.2916, kgLoss = 0.0126
2025-04-08 20:09:15.836100: Training Step 137/354: batchLoss = 0.6317, diffLoss = 3.0930, kgLoss = 0.0164
2025-04-08 20:09:17.461558: Training Step 138/354: batchLoss = 0.5900, diffLoss = 2.8939, kgLoss = 0.0140
2025-04-08 20:09:19.083906: Training Step 139/354: batchLoss = 0.6311, diffLoss = 3.0844, kgLoss = 0.0177
2025-04-08 20:09:20.707132: Training Step 140/354: batchLoss = 0.5878, diffLoss = 2.8755, kgLoss = 0.0159
2025-04-08 20:09:22.346236: Training Step 141/354: batchLoss = 0.4610, diffLoss = 2.2507, kgLoss = 0.0136
2025-04-08 20:09:23.977579: Training Step 142/354: batchLoss = 0.4839, diffLoss = 2.3662, kgLoss = 0.0133
2025-04-08 20:09:25.609908: Training Step 143/354: batchLoss = 0.5355, diffLoss = 2.6217, kgLoss = 0.0139
2025-04-08 20:09:27.246344: Training Step 144/354: batchLoss = 0.4345, diffLoss = 2.1224, kgLoss = 0.0125
2025-04-08 20:09:28.870685: Training Step 145/354: batchLoss = 0.5688, diffLoss = 2.7866, kgLoss = 0.0143
2025-04-08 20:09:30.511065: Training Step 146/354: batchLoss = 0.6785, diffLoss = 3.3249, kgLoss = 0.0169
2025-04-08 20:09:32.131379: Training Step 147/354: batchLoss = 0.4390, diffLoss = 2.1421, kgLoss = 0.0132
2025-04-08 20:09:33.743720: Training Step 148/354: batchLoss = 0.6753, diffLoss = 3.3106, kgLoss = 0.0165
2025-04-08 20:09:35.361809: Training Step 149/354: batchLoss = 0.5330, diffLoss = 2.6032, kgLoss = 0.0154
2025-04-08 20:09:36.985638: Training Step 150/354: batchLoss = 0.5953, diffLoss = 2.9129, kgLoss = 0.0160
2025-04-08 20:09:38.611408: Training Step 151/354: batchLoss = 0.4145, diffLoss = 2.0279, kgLoss = 0.0111
2025-04-08 20:09:40.252925: Training Step 152/354: batchLoss = 0.4493, diffLoss = 2.1930, kgLoss = 0.0133
2025-04-08 20:09:41.886643: Training Step 153/354: batchLoss = 0.5059, diffLoss = 2.4679, kgLoss = 0.0154
2025-04-08 20:09:43.517933: Training Step 154/354: batchLoss = 0.5898, diffLoss = 2.8909, kgLoss = 0.0146
2025-04-08 20:09:45.152594: Training Step 155/354: batchLoss = 0.5544, diffLoss = 2.7118, kgLoss = 0.0150
2025-04-08 20:09:46.783502: Training Step 156/354: batchLoss = 0.5829, diffLoss = 2.8529, kgLoss = 0.0154
2025-04-08 20:09:48.414356: Training Step 157/354: batchLoss = 0.5689, diffLoss = 2.7786, kgLoss = 0.0165
2025-04-08 20:09:50.031138: Training Step 158/354: batchLoss = 0.5554, diffLoss = 2.7208, kgLoss = 0.0141
2025-04-08 20:09:51.653240: Training Step 159/354: batchLoss = 0.4981, diffLoss = 2.4350, kgLoss = 0.0139
2025-04-08 20:09:53.278585: Training Step 160/354: batchLoss = 0.5678, diffLoss = 2.7776, kgLoss = 0.0153
2025-04-08 20:09:54.903206: Training Step 161/354: batchLoss = 0.5954, diffLoss = 2.9164, kgLoss = 0.0151
2025-04-08 20:09:56.537254: Training Step 162/354: batchLoss = 0.4493, diffLoss = 2.2004, kgLoss = 0.0115
2025-04-08 20:09:58.159613: Training Step 163/354: batchLoss = 0.5763, diffLoss = 2.8233, kgLoss = 0.0146
2025-04-08 20:09:59.788176: Training Step 164/354: batchLoss = 0.4627, diffLoss = 2.2716, kgLoss = 0.0104
2025-04-08 20:10:01.421137: Training Step 165/354: batchLoss = 0.5556, diffLoss = 2.7154, kgLoss = 0.0157
2025-04-08 20:10:03.043996: Training Step 166/354: batchLoss = 0.6100, diffLoss = 2.9875, kgLoss = 0.0157
2025-04-08 20:10:04.662616: Training Step 167/354: batchLoss = 0.5259, diffLoss = 2.5741, kgLoss = 0.0139
2025-04-08 20:10:06.283066: Training Step 168/354: batchLoss = 0.5856, diffLoss = 2.8570, kgLoss = 0.0177
2025-04-08 20:10:07.909198: Training Step 169/354: batchLoss = 0.5405, diffLoss = 2.6437, kgLoss = 0.0148
2025-04-08 20:10:09.539245: Training Step 170/354: batchLoss = 0.5946, diffLoss = 2.9064, kgLoss = 0.0166
2025-04-08 20:10:11.165892: Training Step 171/354: batchLoss = 0.5191, diffLoss = 2.5430, kgLoss = 0.0131
2025-04-08 20:10:12.793613: Training Step 172/354: batchLoss = 0.4923, diffLoss = 2.4102, kgLoss = 0.0128
2025-04-08 20:10:14.419630: Training Step 173/354: batchLoss = 0.6618, diffLoss = 3.2433, kgLoss = 0.0164
2025-04-08 20:10:16.050534: Training Step 174/354: batchLoss = 0.4725, diffLoss = 2.3064, kgLoss = 0.0140
2025-04-08 20:10:17.683483: Training Step 175/354: batchLoss = 0.5527, diffLoss = 2.7004, kgLoss = 0.0158
2025-04-08 20:10:19.317034: Training Step 176/354: batchLoss = 0.4038, diffLoss = 1.9694, kgLoss = 0.0124
2025-04-08 20:10:20.945187: Training Step 177/354: batchLoss = 0.5210, diffLoss = 2.5475, kgLoss = 0.0144
2025-04-08 20:10:22.568128: Training Step 178/354: batchLoss = 0.5447, diffLoss = 2.6697, kgLoss = 0.0135
2025-04-08 20:10:24.194694: Training Step 179/354: batchLoss = 0.7020, diffLoss = 3.4417, kgLoss = 0.0171
2025-04-08 20:10:25.817326: Training Step 180/354: batchLoss = 0.5411, diffLoss = 2.6533, kgLoss = 0.0130
2025-04-08 20:10:27.445219: Training Step 181/354: batchLoss = 0.6339, diffLoss = 3.1096, kgLoss = 0.0150
2025-04-08 20:10:29.085439: Training Step 182/354: batchLoss = 0.6813, diffLoss = 3.3325, kgLoss = 0.0185
2025-04-08 20:10:30.714505: Training Step 183/354: batchLoss = 0.5926, diffLoss = 2.8987, kgLoss = 0.0161
2025-04-08 20:10:32.347724: Training Step 184/354: batchLoss = 0.5896, diffLoss = 2.8862, kgLoss = 0.0154
2025-04-08 20:10:33.979822: Training Step 185/354: batchLoss = 0.5514, diffLoss = 2.6985, kgLoss = 0.0146
2025-04-08 20:10:35.613671: Training Step 186/354: batchLoss = 0.5449, diffLoss = 2.6677, kgLoss = 0.0142
2025-04-08 20:10:37.234946: Training Step 187/354: batchLoss = 0.5108, diffLoss = 2.5014, kgLoss = 0.0132
2025-04-08 20:10:38.863123: Training Step 188/354: batchLoss = 0.5856, diffLoss = 2.8732, kgLoss = 0.0137
2025-04-08 20:10:40.486585: Training Step 189/354: batchLoss = 0.5822, diffLoss = 2.8529, kgLoss = 0.0145
2025-04-08 20:10:42.107751: Training Step 190/354: batchLoss = 0.5875, diffLoss = 2.8769, kgLoss = 0.0151
2025-04-08 20:10:43.739579: Training Step 191/354: batchLoss = 0.6134, diffLoss = 3.0032, kgLoss = 0.0159
2025-04-08 20:10:45.376738: Training Step 192/354: batchLoss = 0.4901, diffLoss = 2.3988, kgLoss = 0.0129
2025-04-08 20:10:47.006010: Training Step 193/354: batchLoss = 0.4611, diffLoss = 2.2591, kgLoss = 0.0116
2025-04-08 20:10:48.643147: Training Step 194/354: batchLoss = 0.4778, diffLoss = 2.3381, kgLoss = 0.0127
2025-04-08 20:10:50.263324: Training Step 195/354: batchLoss = 0.4368, diffLoss = 2.1390, kgLoss = 0.0113
2025-04-08 20:10:51.884235: Training Step 196/354: batchLoss = 0.6039, diffLoss = 2.9560, kgLoss = 0.0159
2025-04-08 20:10:53.501501: Training Step 197/354: batchLoss = 0.6877, diffLoss = 3.3757, kgLoss = 0.0157
2025-04-08 20:10:55.119017: Training Step 198/354: batchLoss = 0.4984, diffLoss = 2.4392, kgLoss = 0.0132
2025-04-08 20:10:56.739120: Training Step 199/354: batchLoss = 0.4451, diffLoss = 2.1755, kgLoss = 0.0126
2025-04-08 20:10:58.356057: Training Step 200/354: batchLoss = 0.5751, diffLoss = 2.8165, kgLoss = 0.0147
2025-04-08 20:10:59.980054: Training Step 201/354: batchLoss = 0.5982, diffLoss = 2.9263, kgLoss = 0.0162
2025-04-08 20:11:01.600595: Training Step 202/354: batchLoss = 0.5568, diffLoss = 2.7259, kgLoss = 0.0145
2025-04-08 20:11:03.221610: Training Step 203/354: batchLoss = 0.4945, diffLoss = 2.4143, kgLoss = 0.0146
2025-04-08 20:11:04.841477: Training Step 204/354: batchLoss = 0.4522, diffLoss = 2.2037, kgLoss = 0.0144
2025-04-08 20:11:06.457751: Training Step 205/354: batchLoss = 0.6321, diffLoss = 3.0971, kgLoss = 0.0159
2025-04-08 20:11:08.072798: Training Step 206/354: batchLoss = 0.5892, diffLoss = 2.8792, kgLoss = 0.0167
2025-04-08 20:11:09.683867: Training Step 207/354: batchLoss = 0.6293, diffLoss = 3.0861, kgLoss = 0.0151
2025-04-08 20:11:11.292969: Training Step 208/354: batchLoss = 0.5382, diffLoss = 2.6292, kgLoss = 0.0154
2025-04-08 20:11:12.910811: Training Step 209/354: batchLoss = 0.4368, diffLoss = 2.1391, kgLoss = 0.0112
2025-04-08 20:11:14.527296: Training Step 210/354: batchLoss = 0.5136, diffLoss = 2.5182, kgLoss = 0.0124
2025-04-08 20:11:16.158553: Training Step 211/354: batchLoss = 0.4563, diffLoss = 2.2334, kgLoss = 0.0120
2025-04-08 20:11:17.787498: Training Step 212/354: batchLoss = 1.1821, diffLoss = 5.7889, kgLoss = 0.0304
2025-04-08 20:11:19.413946: Training Step 213/354: batchLoss = 0.4291, diffLoss = 2.0953, kgLoss = 0.0125
2025-04-08 20:11:21.051221: Training Step 214/354: batchLoss = 0.5908, diffLoss = 2.9003, kgLoss = 0.0134
2025-04-08 20:11:22.674731: Training Step 215/354: batchLoss = 0.7269, diffLoss = 3.5631, kgLoss = 0.0178
2025-04-08 20:11:24.296927: Training Step 216/354: batchLoss = 0.5325, diffLoss = 2.6090, kgLoss = 0.0134
2025-04-08 20:11:25.918233: Training Step 217/354: batchLoss = 0.5944, diffLoss = 2.9042, kgLoss = 0.0169
2025-04-08 20:11:27.540731: Training Step 218/354: batchLoss = 0.5333, diffLoss = 2.6153, kgLoss = 0.0128
2025-04-08 20:11:29.167989: Training Step 219/354: batchLoss = 0.4957, diffLoss = 2.4254, kgLoss = 0.0133
2025-04-08 20:11:30.800222: Training Step 220/354: batchLoss = 0.5996, diffLoss = 2.9372, kgLoss = 0.0152
2025-04-08 20:11:32.436414: Training Step 221/354: batchLoss = 0.4323, diffLoss = 2.1103, kgLoss = 0.0128
2025-04-08 20:11:34.067806: Training Step 222/354: batchLoss = 0.5415, diffLoss = 2.6563, kgLoss = 0.0128
2025-04-08 20:11:35.695878: Training Step 223/354: batchLoss = 0.5705, diffLoss = 2.7937, kgLoss = 0.0147
2025-04-08 20:11:37.332712: Training Step 224/354: batchLoss = 0.5840, diffLoss = 2.8668, kgLoss = 0.0133
2025-04-08 20:11:38.959631: Training Step 225/354: batchLoss = 0.5520, diffLoss = 2.7015, kgLoss = 0.0146
2025-04-08 20:11:40.581882: Training Step 226/354: batchLoss = 0.5961, diffLoss = 2.9094, kgLoss = 0.0178
2025-04-08 20:11:42.200801: Training Step 227/354: batchLoss = 0.5596, diffLoss = 2.7443, kgLoss = 0.0135
2025-04-08 20:11:43.819678: Training Step 228/354: batchLoss = 0.5836, diffLoss = 2.8588, kgLoss = 0.0148
2025-04-08 20:11:45.439282: Training Step 229/354: batchLoss = 0.4890, diffLoss = 2.3966, kgLoss = 0.0121
2025-04-08 20:11:47.055014: Training Step 230/354: batchLoss = 0.6377, diffLoss = 3.1222, kgLoss = 0.0165
2025-04-08 20:11:48.687753: Training Step 231/354: batchLoss = 0.5615, diffLoss = 2.7522, kgLoss = 0.0138
2025-04-08 20:11:50.321083: Training Step 232/354: batchLoss = 0.5710, diffLoss = 2.7967, kgLoss = 0.0145
2025-04-08 20:11:51.951562: Training Step 233/354: batchLoss = 0.6428, diffLoss = 3.1501, kgLoss = 0.0160
2025-04-08 20:11:53.579994: Training Step 234/354: batchLoss = 0.4475, diffLoss = 2.1883, kgLoss = 0.0123
2025-04-08 20:11:55.207166: Training Step 235/354: batchLoss = 0.5961, diffLoss = 2.9202, kgLoss = 0.0150
2025-04-08 20:11:56.841690: Training Step 236/354: batchLoss = 0.5703, diffLoss = 2.7889, kgLoss = 0.0156
2025-04-08 20:11:58.455726: Training Step 237/354: batchLoss = 0.5155, diffLoss = 2.5261, kgLoss = 0.0129
2025-04-08 20:12:00.072211: Training Step 238/354: batchLoss = 0.5512, diffLoss = 2.6779, kgLoss = 0.0195
2025-04-08 20:12:01.698685: Training Step 239/354: batchLoss = 0.5019, diffLoss = 2.4535, kgLoss = 0.0140
2025-04-08 20:12:03.319011: Training Step 240/354: batchLoss = 0.7099, diffLoss = 3.4805, kgLoss = 0.0173
2025-04-08 20:12:04.947052: Training Step 241/354: batchLoss = 0.5339, diffLoss = 2.6060, kgLoss = 0.0158
2025-04-08 20:12:06.573706: Training Step 242/354: batchLoss = 0.5413, diffLoss = 2.6508, kgLoss = 0.0140
2025-04-08 20:12:08.202296: Training Step 243/354: batchLoss = 0.4807, diffLoss = 2.3490, kgLoss = 0.0136
2025-04-08 20:12:09.828544: Training Step 244/354: batchLoss = 0.4802, diffLoss = 2.3510, kgLoss = 0.0125
2025-04-08 20:12:11.463698: Training Step 245/354: batchLoss = 0.6504, diffLoss = 3.1841, kgLoss = 0.0170
2025-04-08 20:12:13.094438: Training Step 246/354: batchLoss = 0.4716, diffLoss = 2.3050, kgLoss = 0.0132
2025-04-08 20:12:14.723625: Training Step 247/354: batchLoss = 0.6129, diffLoss = 2.9959, kgLoss = 0.0171
2025-04-08 20:12:16.346552: Training Step 248/354: batchLoss = 0.5136, diffLoss = 2.5069, kgLoss = 0.0152
2025-04-08 20:12:17.973317: Training Step 249/354: batchLoss = 0.5842, diffLoss = 2.8583, kgLoss = 0.0156
2025-04-08 20:12:19.596938: Training Step 250/354: batchLoss = 0.5428, diffLoss = 2.6588, kgLoss = 0.0138
2025-04-08 20:12:21.225323: Training Step 251/354: batchLoss = 0.5220, diffLoss = 2.5558, kgLoss = 0.0136
2025-04-08 20:12:22.856072: Training Step 252/354: batchLoss = 0.8476, diffLoss = 4.1584, kgLoss = 0.0199
2025-04-08 20:12:24.477949: Training Step 253/354: batchLoss = 0.6871, diffLoss = 3.3644, kgLoss = 0.0177
2025-04-08 20:12:26.107348: Training Step 254/354: batchLoss = 0.5567, diffLoss = 2.7279, kgLoss = 0.0138
2025-04-08 20:12:27.742178: Training Step 255/354: batchLoss = 0.6000, diffLoss = 2.9295, kgLoss = 0.0176
2025-04-08 20:12:29.371254: Training Step 256/354: batchLoss = 0.4936, diffLoss = 2.4180, kgLoss = 0.0125
2025-04-08 20:12:30.998962: Training Step 257/354: batchLoss = 0.4867, diffLoss = 2.3783, kgLoss = 0.0137
2025-04-08 20:12:32.626227: Training Step 258/354: batchLoss = 0.5951, diffLoss = 2.9198, kgLoss = 0.0140
2025-04-08 20:12:34.255769: Training Step 259/354: batchLoss = 0.6808, diffLoss = 3.3368, kgLoss = 0.0168
2025-04-08 20:12:35.882850: Training Step 260/354: batchLoss = 0.6038, diffLoss = 2.9582, kgLoss = 0.0152
2025-04-08 20:12:37.509584: Training Step 261/354: batchLoss = 0.6368, diffLoss = 3.1215, kgLoss = 0.0156
2025-04-08 20:12:39.143222: Training Step 262/354: batchLoss = 0.5297, diffLoss = 2.5933, kgLoss = 0.0137
2025-04-08 20:12:40.779844: Training Step 263/354: batchLoss = 0.5576, diffLoss = 2.7318, kgLoss = 0.0141
2025-04-08 20:12:42.408971: Training Step 264/354: batchLoss = 0.3614, diffLoss = 1.7568, kgLoss = 0.0125
2025-04-08 20:12:44.038867: Training Step 265/354: batchLoss = 0.5890, diffLoss = 2.8758, kgLoss = 0.0173
2025-04-08 20:12:45.668275: Training Step 266/354: batchLoss = 0.5223, diffLoss = 2.5598, kgLoss = 0.0129
2025-04-08 20:12:47.295388: Training Step 267/354: batchLoss = 0.4972, diffLoss = 2.4244, kgLoss = 0.0155
2025-04-08 20:12:48.924989: Training Step 268/354: batchLoss = 0.5531, diffLoss = 2.7026, kgLoss = 0.0157
2025-04-08 20:12:50.547379: Training Step 269/354: batchLoss = 0.5542, diffLoss = 2.7133, kgLoss = 0.0144
2025-04-08 20:12:52.176269: Training Step 270/354: batchLoss = 0.5950, diffLoss = 2.9123, kgLoss = 0.0156
2025-04-08 20:12:53.818850: Training Step 271/354: batchLoss = 0.4685, diffLoss = 2.2972, kgLoss = 0.0113
2025-04-08 20:12:55.454406: Training Step 272/354: batchLoss = 0.5544, diffLoss = 2.7090, kgLoss = 0.0158
2025-04-08 20:12:57.083237: Training Step 273/354: batchLoss = 0.4872, diffLoss = 2.3779, kgLoss = 0.0145
2025-04-08 20:12:58.781205: Training Step 274/354: batchLoss = 0.5494, diffLoss = 2.6908, kgLoss = 0.0140
2025-04-08 20:13:00.420037: Training Step 275/354: batchLoss = 0.6316, diffLoss = 3.0835, kgLoss = 0.0186
2025-04-08 20:13:02.050885: Training Step 276/354: batchLoss = 0.5233, diffLoss = 2.5512, kgLoss = 0.0164
2025-04-08 20:13:03.671600: Training Step 277/354: batchLoss = 0.5871, diffLoss = 2.8751, kgLoss = 0.0150
2025-04-08 20:13:05.292195: Training Step 278/354: batchLoss = 0.6791, diffLoss = 3.3240, kgLoss = 0.0179
2025-04-08 20:13:06.923686: Training Step 279/354: batchLoss = 0.5280, diffLoss = 2.5837, kgLoss = 0.0141
2025-04-08 20:13:08.553079: Training Step 280/354: batchLoss = 0.4348, diffLoss = 2.1273, kgLoss = 0.0117
2025-04-08 20:13:10.185697: Training Step 281/354: batchLoss = 0.7118, diffLoss = 3.4881, kgLoss = 0.0177
2025-04-08 20:13:11.813698: Training Step 282/354: batchLoss = 0.5526, diffLoss = 2.7015, kgLoss = 0.0154
2025-04-08 20:13:13.450543: Training Step 283/354: batchLoss = 0.4689, diffLoss = 2.2946, kgLoss = 0.0124
2025-04-08 20:13:15.081945: Training Step 284/354: batchLoss = 0.4544, diffLoss = 2.2286, kgLoss = 0.0109
2025-04-08 20:13:16.719728: Training Step 285/354: batchLoss = 0.5423, diffLoss = 2.6513, kgLoss = 0.0151
2025-04-08 20:13:18.346820: Training Step 286/354: batchLoss = 0.6426, diffLoss = 3.1331, kgLoss = 0.0200
2025-04-08 20:13:19.969253: Training Step 287/354: batchLoss = 0.5783, diffLoss = 2.8359, kgLoss = 0.0139
2025-04-08 20:13:21.593413: Training Step 288/354: batchLoss = 0.5195, diffLoss = 2.5442, kgLoss = 0.0133
2025-04-08 20:13:23.218806: Training Step 289/354: batchLoss = 0.5280, diffLoss = 2.5780, kgLoss = 0.0155
2025-04-08 20:13:24.846693: Training Step 290/354: batchLoss = 0.4346, diffLoss = 2.1238, kgLoss = 0.0123
2025-04-08 20:13:26.470462: Training Step 291/354: batchLoss = 0.4287, diffLoss = 2.0917, kgLoss = 0.0129
2025-04-08 20:13:28.104507: Training Step 292/354: batchLoss = 0.4951, diffLoss = 2.3138, kgLoss = 0.0404
2025-04-08 20:13:29.728656: Training Step 293/354: batchLoss = 0.4921, diffLoss = 2.4117, kgLoss = 0.0122
2025-04-08 20:13:31.361759: Training Step 294/354: batchLoss = 0.6026, diffLoss = 2.9504, kgLoss = 0.0157
2025-04-08 20:13:32.993495: Training Step 295/354: batchLoss = 0.5428, diffLoss = 2.6553, kgLoss = 0.0147
2025-04-08 20:13:34.609617: Training Step 296/354: batchLoss = 0.6414, diffLoss = 3.1424, kgLoss = 0.0162
2025-04-08 20:13:36.229815: Training Step 297/354: batchLoss = 0.5611, diffLoss = 2.7496, kgLoss = 0.0140
2025-04-08 20:13:37.850740: Training Step 298/354: batchLoss = 0.5191, diffLoss = 2.5395, kgLoss = 0.0140
2025-04-08 20:13:39.470826: Training Step 299/354: batchLoss = 0.5430, diffLoss = 2.6528, kgLoss = 0.0156
2025-04-08 20:13:41.099946: Training Step 300/354: batchLoss = 0.5128, diffLoss = 2.5077, kgLoss = 0.0141
2025-04-08 20:13:42.729378: Training Step 301/354: batchLoss = 0.5593, diffLoss = 2.7340, kgLoss = 0.0156
2025-04-08 20:13:44.349830: Training Step 302/354: batchLoss = 0.4870, diffLoss = 2.3878, kgLoss = 0.0118
2025-04-08 20:13:45.982184: Training Step 303/354: batchLoss = 0.5273, diffLoss = 2.5765, kgLoss = 0.0150
2025-04-08 20:13:47.617267: Training Step 304/354: batchLoss = 0.5424, diffLoss = 2.6556, kgLoss = 0.0141
2025-04-08 20:13:49.251828: Training Step 305/354: batchLoss = 0.5533, diffLoss = 2.7073, kgLoss = 0.0148
2025-04-08 20:13:50.871584: Training Step 306/354: batchLoss = 0.4595, diffLoss = 2.2388, kgLoss = 0.0146
2025-04-08 20:13:52.493762: Training Step 307/354: batchLoss = 0.4685, diffLoss = 2.2935, kgLoss = 0.0122
2025-04-08 20:13:54.109227: Training Step 308/354: batchLoss = 0.4677, diffLoss = 2.2796, kgLoss = 0.0147
2025-04-08 20:13:55.726311: Training Step 309/354: batchLoss = 0.4721, diffLoss = 2.3021, kgLoss = 0.0145
2025-04-08 20:13:57.358667: Training Step 310/354: batchLoss = 0.4948, diffLoss = 2.4176, kgLoss = 0.0141
2025-04-08 20:13:58.986412: Training Step 311/354: batchLoss = 0.5032, diffLoss = 2.4623, kgLoss = 0.0134
2025-04-08 20:14:00.618071: Training Step 312/354: batchLoss = 0.4398, diffLoss = 2.1518, kgLoss = 0.0118
2025-04-08 20:14:02.245941: Training Step 313/354: batchLoss = 0.5298, diffLoss = 2.5910, kgLoss = 0.0145
2025-04-08 20:14:03.880784: Training Step 314/354: batchLoss = 0.5319, diffLoss = 2.6066, kgLoss = 0.0132
2025-04-08 20:14:05.512009: Training Step 315/354: batchLoss = 0.7200, diffLoss = 3.5293, kgLoss = 0.0176
2025-04-08 20:14:07.136293: Training Step 316/354: batchLoss = 0.5373, diffLoss = 2.6295, kgLoss = 0.0143
2025-04-08 20:14:08.761797: Training Step 317/354: batchLoss = 0.5900, diffLoss = 2.8901, kgLoss = 0.0149
2025-04-08 20:14:10.387813: Training Step 318/354: batchLoss = 0.5698, diffLoss = 2.7932, kgLoss = 0.0139
2025-04-08 20:14:12.007375: Training Step 319/354: batchLoss = 0.5133, diffLoss = 2.5102, kgLoss = 0.0141
2025-04-08 20:14:13.636170: Training Step 320/354: batchLoss = 0.7566, diffLoss = 3.6992, kgLoss = 0.0209
2025-04-08 20:14:15.266471: Training Step 321/354: batchLoss = 0.6066, diffLoss = 2.9623, kgLoss = 0.0177
2025-04-08 20:14:16.889169: Training Step 322/354: batchLoss = 0.5098, diffLoss = 2.4928, kgLoss = 0.0140
2025-04-08 20:14:18.515344: Training Step 323/354: batchLoss = 0.6074, diffLoss = 2.9809, kgLoss = 0.0140
2025-04-08 20:14:20.150429: Training Step 324/354: batchLoss = 0.6465, diffLoss = 3.1647, kgLoss = 0.0169
2025-04-08 20:14:21.776106: Training Step 325/354: batchLoss = 0.5847, diffLoss = 2.8561, kgLoss = 0.0168
2025-04-08 20:14:23.395416: Training Step 326/354: batchLoss = 0.5404, diffLoss = 2.6461, kgLoss = 0.0140
2025-04-08 20:14:25.019979: Training Step 327/354: batchLoss = 0.5361, diffLoss = 2.6304, kgLoss = 0.0125
2025-04-08 20:14:26.644502: Training Step 328/354: batchLoss = 0.4435, diffLoss = 2.1669, kgLoss = 0.0126
2025-04-08 20:14:28.274825: Training Step 329/354: batchLoss = 0.4391, diffLoss = 2.1441, kgLoss = 0.0128
2025-04-08 20:14:29.899716: Training Step 330/354: batchLoss = 0.4857, diffLoss = 2.3826, kgLoss = 0.0115
2025-04-08 20:14:31.528212: Training Step 331/354: batchLoss = 0.5942, diffLoss = 2.9032, kgLoss = 0.0170
2025-04-08 20:14:33.155368: Training Step 332/354: batchLoss = 0.5523, diffLoss = 2.7056, kgLoss = 0.0140
2025-04-08 20:14:34.788900: Training Step 333/354: batchLoss = 0.5139, diffLoss = 2.5182, kgLoss = 0.0128
2025-04-08 20:14:36.420992: Training Step 334/354: batchLoss = 0.5135, diffLoss = 2.5159, kgLoss = 0.0130
2025-04-08 20:14:38.040966: Training Step 335/354: batchLoss = 0.5199, diffLoss = 2.5424, kgLoss = 0.0143
2025-04-08 20:14:39.666747: Training Step 336/354: batchLoss = 0.6246, diffLoss = 3.0638, kgLoss = 0.0148
2025-04-08 20:14:41.289194: Training Step 337/354: batchLoss = 0.6437, diffLoss = 3.1558, kgLoss = 0.0157
2025-04-08 20:14:42.905455: Training Step 338/354: batchLoss = 0.5278, diffLoss = 2.5795, kgLoss = 0.0148
2025-04-08 20:14:44.526579: Training Step 339/354: batchLoss = 0.5558, diffLoss = 2.7229, kgLoss = 0.0140
2025-04-08 20:14:46.165307: Training Step 340/354: batchLoss = 0.7076, diffLoss = 3.4703, kgLoss = 0.0169
2025-04-08 20:14:47.802716: Training Step 341/354: batchLoss = 0.6297, diffLoss = 3.0855, kgLoss = 0.0158
2025-04-08 20:14:49.428443: Training Step 342/354: batchLoss = 0.5152, diffLoss = 2.5174, kgLoss = 0.0147
2025-04-08 20:14:51.064745: Training Step 343/354: batchLoss = 0.5523, diffLoss = 2.7064, kgLoss = 0.0137
2025-04-08 20:14:52.713548: Training Step 344/354: batchLoss = 0.4482, diffLoss = 2.1878, kgLoss = 0.0133
2025-04-08 20:14:54.331965: Training Step 345/354: batchLoss = 0.5603, diffLoss = 2.7381, kgLoss = 0.0159
2025-04-08 20:14:55.949964: Training Step 346/354: batchLoss = 0.7573, diffLoss = 3.7183, kgLoss = 0.0171
2025-04-08 20:14:57.572867: Training Step 347/354: batchLoss = 0.6360, diffLoss = 3.1096, kgLoss = 0.0176
2025-04-08 20:14:59.193937: Training Step 348/354: batchLoss = 0.5959, diffLoss = 2.9242, kgLoss = 0.0139
2025-04-08 20:15:00.821700: Training Step 349/354: batchLoss = 0.6332, diffLoss = 3.0963, kgLoss = 0.0175
2025-04-08 20:15:02.444486: Training Step 350/354: batchLoss = 0.5228, diffLoss = 2.5652, kgLoss = 0.0122
2025-04-08 20:15:04.068652: Training Step 351/354: batchLoss = 0.5720, diffLoss = 2.7924, kgLoss = 0.0169
2025-04-08 20:15:05.674713: Training Step 352/354: batchLoss = 0.5293, diffLoss = 2.5889, kgLoss = 0.0144
2025-04-08 20:15:07.089447: Training Step 353/354: batchLoss = 0.5225, diffLoss = 2.5402, kgLoss = 0.0180
2025-04-08 20:15:07.183045: 
2025-04-08 20:15:07.183886: Epoch 36/1000, Train: epLoss = 0.9777, epDfLoss = 4.7844, epfTransLoss = 0.0000, epKgLoss = 0.0260  
2025-04-08 20:15:08.507294: Steps 0/138: batch_recall = 47.93, batch_ndcg = 26.99 
2025-04-08 20:15:09.843401: Steps 1/138: batch_recall = 47.87, batch_ndcg = 28.08 
2025-04-08 20:15:11.149023: Steps 2/138: batch_recall = 59.74, batch_ndcg = 35.90 
2025-04-08 20:15:12.461193: Steps 3/138: batch_recall = 57.34, batch_ndcg = 33.45 
2025-04-08 20:15:13.765708: Steps 4/138: batch_recall = 66.59, batch_ndcg = 40.53 
2025-04-08 20:15:15.089643: Steps 5/138: batch_recall = 56.69, batch_ndcg = 30.77 
2025-04-08 20:15:16.401816: Steps 6/138: batch_recall = 51.38, batch_ndcg = 31.59 
2025-04-08 20:15:17.722465: Steps 7/138: batch_recall = 62.56, batch_ndcg = 41.77 
2025-04-08 20:15:19.033271: Steps 8/138: batch_recall = 61.53, batch_ndcg = 38.47 
2025-04-08 20:15:20.347670: Steps 9/138: batch_recall = 59.36, batch_ndcg = 34.95 
2025-04-08 20:15:21.660636: Steps 10/138: batch_recall = 57.82, batch_ndcg = 33.47 
2025-04-08 20:15:22.971272: Steps 11/138: batch_recall = 56.15, batch_ndcg = 33.15 
2025-04-08 20:15:24.282577: Steps 12/138: batch_recall = 51.24, batch_ndcg = 29.53 
2025-04-08 20:15:25.585524: Steps 13/138: batch_recall = 52.97, batch_ndcg = 31.83 
2025-04-08 20:15:26.880427: Steps 14/138: batch_recall = 54.42, batch_ndcg = 31.86 
2025-04-08 20:15:28.179072: Steps 15/138: batch_recall = 46.75, batch_ndcg = 29.62 
2025-04-08 20:15:29.472182: Steps 16/138: batch_recall = 60.33, batch_ndcg = 33.95 
2025-04-08 20:15:30.752088: Steps 17/138: batch_recall = 57.83, batch_ndcg = 33.72 
2025-04-08 20:15:32.048549: Steps 18/138: batch_recall = 54.50, batch_ndcg = 34.02 
2025-04-08 20:15:33.356425: Steps 19/138: batch_recall = 53.62, batch_ndcg = 32.68 
2025-04-08 20:15:34.660733: Steps 20/138: batch_recall = 61.39, batch_ndcg = 35.48 
2025-04-08 20:15:35.993092: Steps 21/138: batch_recall = 68.39, batch_ndcg = 40.96 
2025-04-08 20:15:37.290564: Steps 22/138: batch_recall = 56.76, batch_ndcg = 32.62 
2025-04-08 20:15:38.591082: Steps 23/138: batch_recall = 50.82, batch_ndcg = 30.33 
2025-04-08 20:15:39.889755: Steps 24/138: batch_recall = 57.74, batch_ndcg = 31.53 
2025-04-08 20:15:41.193218: Steps 25/138: batch_recall = 60.99, batch_ndcg = 35.10 
2025-04-08 20:15:42.501242: Steps 26/138: batch_recall = 59.15, batch_ndcg = 33.41 
2025-04-08 20:15:43.802346: Steps 27/138: batch_recall = 55.89, batch_ndcg = 32.20 
2025-04-08 20:15:45.080388: Steps 28/138: batch_recall = 58.05, batch_ndcg = 32.94 
2025-04-08 20:15:46.370486: Steps 29/138: batch_recall = 59.66, batch_ndcg = 31.41 
2025-04-08 20:15:47.667645: Steps 30/138: batch_recall = 59.36, batch_ndcg = 34.21 
2025-04-08 20:15:48.961379: Steps 31/138: batch_recall = 48.86, batch_ndcg = 27.77 
2025-04-08 20:15:50.255467: Steps 32/138: batch_recall = 53.19, batch_ndcg = 31.65 
2025-04-08 20:15:51.559121: Steps 33/138: batch_recall = 57.87, batch_ndcg = 32.50 
2025-04-08 20:15:52.858336: Steps 34/138: batch_recall = 54.19, batch_ndcg = 29.12 
2025-04-08 20:15:54.165267: Steps 35/138: batch_recall = 49.59, batch_ndcg = 29.48 
2025-04-08 20:15:55.466364: Steps 36/138: batch_recall = 46.29, batch_ndcg = 26.66 
2025-04-08 20:15:56.774864: Steps 37/138: batch_recall = 58.59, batch_ndcg = 34.28 
2025-04-08 20:15:58.071935: Steps 38/138: batch_recall = 58.72, batch_ndcg = 32.61 
2025-04-08 20:15:59.356260: Steps 39/138: batch_recall = 67.14, batch_ndcg = 38.57 
2025-04-08 20:16:00.640208: Steps 40/138: batch_recall = 61.20, batch_ndcg = 30.86 
2025-04-08 20:16:01.933384: Steps 41/138: batch_recall = 58.99, batch_ndcg = 33.82 
2025-04-08 20:16:03.215264: Steps 42/138: batch_recall = 54.77, batch_ndcg = 30.59 
2025-04-08 20:16:04.495380: Steps 43/138: batch_recall = 58.15, batch_ndcg = 36.92 
2025-04-08 20:16:05.779461: Steps 44/138: batch_recall = 55.54, batch_ndcg = 30.25 
2025-04-08 20:16:07.069712: Steps 45/138: batch_recall = 63.15, batch_ndcg = 36.16 
2025-04-08 20:16:08.358437: Steps 46/138: batch_recall = 61.52, batch_ndcg = 36.20 
2025-04-08 20:16:09.658124: Steps 47/138: batch_recall = 53.54, batch_ndcg = 31.61 
2025-04-08 20:16:10.956527: Steps 48/138: batch_recall = 60.28, batch_ndcg = 35.85 
2025-04-08 20:16:12.259023: Steps 49/138: batch_recall = 64.09, batch_ndcg = 37.20 
2025-04-08 20:16:13.551012: Steps 50/138: batch_recall = 57.38, batch_ndcg = 30.68 
2025-04-08 20:16:14.847852: Steps 51/138: batch_recall = 64.68, batch_ndcg = 36.51 
2025-04-08 20:16:16.131783: Steps 52/138: batch_recall = 67.80, batch_ndcg = 42.24 
2025-04-08 20:16:17.411606: Steps 53/138: batch_recall = 64.50, batch_ndcg = 34.48 
2025-04-08 20:16:18.698024: Steps 54/138: batch_recall = 69.06, batch_ndcg = 38.52 
2025-04-08 20:16:19.974866: Steps 55/138: batch_recall = 61.76, batch_ndcg = 34.12 
2025-04-08 20:16:21.270945: Steps 56/138: batch_recall = 61.81, batch_ndcg = 35.35 
2025-04-08 20:16:22.568259: Steps 57/138: batch_recall = 56.22, batch_ndcg = 31.60 
2025-04-08 20:16:23.871032: Steps 58/138: batch_recall = 64.45, batch_ndcg = 35.17 
2025-04-08 20:16:25.157467: Steps 59/138: batch_recall = 62.62, batch_ndcg = 37.40 
2025-04-08 20:16:26.452779: Steps 60/138: batch_recall = 65.89, batch_ndcg = 37.56 
2025-04-08 20:16:27.750438: Steps 61/138: batch_recall = 63.07, batch_ndcg = 34.81 
2025-04-08 20:16:29.038284: Steps 62/138: batch_recall = 82.38, batch_ndcg = 44.23 
2025-04-08 20:16:30.336862: Steps 63/138: batch_recall = 76.17, batch_ndcg = 42.97 
2025-04-08 20:16:31.617795: Steps 64/138: batch_recall = 60.64, batch_ndcg = 32.31 
2025-04-08 20:16:32.914308: Steps 65/138: batch_recall = 84.83, batch_ndcg = 46.89 
2025-04-08 20:16:34.198612: Steps 66/138: batch_recall = 69.46, batch_ndcg = 40.81 
2025-04-08 20:16:35.484085: Steps 67/138: batch_recall = 76.90, batch_ndcg = 46.68 
2025-04-08 20:16:36.772898: Steps 68/138: batch_recall = 61.13, batch_ndcg = 33.70 
2025-04-08 20:16:38.068410: Steps 69/138: batch_recall = 88.17, batch_ndcg = 52.03 
2025-04-08 20:16:39.367508: Steps 70/138: batch_recall = 78.12, batch_ndcg = 44.66 
2025-04-08 20:16:40.665380: Steps 71/138: batch_recall = 86.62, batch_ndcg = 51.95 
2025-04-08 20:16:41.962976: Steps 72/138: batch_recall = 85.99, batch_ndcg = 49.34 
2025-04-08 20:16:43.254989: Steps 73/138: batch_recall = 82.76, batch_ndcg = 46.65 
2025-04-08 20:16:44.535679: Steps 74/138: batch_recall = 78.65, batch_ndcg = 48.57 
2025-04-08 20:16:45.828972: Steps 75/138: batch_recall = 80.66, batch_ndcg = 48.60 
2025-04-08 20:16:47.112989: Steps 76/138: batch_recall = 95.08, batch_ndcg = 55.73 
2025-04-08 20:16:48.388981: Steps 77/138: batch_recall = 91.74, batch_ndcg = 51.80 
2025-04-08 20:16:49.679592: Steps 78/138: batch_recall = 91.26, batch_ndcg = 48.28 
2025-04-08 20:16:50.954986: Steps 79/138: batch_recall = 86.57, batch_ndcg = 48.52 
2025-04-08 20:16:52.238436: Steps 80/138: batch_recall = 72.34, batch_ndcg = 39.06 
2025-04-08 20:16:53.540159: Steps 81/138: batch_recall = 79.74, batch_ndcg = 47.93 
2025-04-08 20:16:54.843257: Steps 82/138: batch_recall = 87.01, batch_ndcg = 53.86 
2025-04-08 20:16:56.139329: Steps 83/138: batch_recall = 84.84, batch_ndcg = 48.21 
2025-04-08 20:16:57.429524: Steps 84/138: batch_recall = 100.53, batch_ndcg = 57.84 
2025-04-08 20:16:58.729187: Steps 85/138: batch_recall = 104.26, batch_ndcg = 61.04 
2025-04-08 20:17:00.011136: Steps 86/138: batch_recall = 117.96, batch_ndcg = 71.19 
2025-04-08 20:17:01.298302: Steps 87/138: batch_recall = 104.95, batch_ndcg = 55.51 
2025-04-08 20:17:02.577691: Steps 88/138: batch_recall = 101.48, batch_ndcg = 59.09 
2025-04-08 20:17:03.854638: Steps 89/138: batch_recall = 122.29, batch_ndcg = 69.04 
2025-04-08 20:17:05.134014: Steps 90/138: batch_recall = 100.19, batch_ndcg = 56.72 
2025-04-08 20:17:06.404237: Steps 91/138: batch_recall = 114.72, batch_ndcg = 64.53 
2025-04-08 20:17:07.676041: Steps 92/138: batch_recall = 122.02, batch_ndcg = 66.72 
2025-04-08 20:17:08.953989: Steps 93/138: batch_recall = 117.67, batch_ndcg = 67.48 
2025-04-08 20:17:10.245325: Steps 94/138: batch_recall = 124.17, batch_ndcg = 66.19 
2025-04-08 20:17:11.526587: Steps 95/138: batch_recall = 113.26, batch_ndcg = 66.97 
2025-04-08 20:17:12.805948: Steps 96/138: batch_recall = 130.09, batch_ndcg = 77.89 
2025-04-08 20:17:14.094208: Steps 97/138: batch_recall = 156.61, batch_ndcg = 92.89 
2025-04-08 20:17:15.386948: Steps 98/138: batch_recall = 110.34, batch_ndcg = 64.24 
2025-04-08 20:17:16.668805: Steps 99/138: batch_recall = 124.56, batch_ndcg = 71.77 
2025-04-08 20:17:17.941927: Steps 100/138: batch_recall = 124.57, batch_ndcg = 70.16 
2025-04-08 20:17:19.208667: Steps 101/138: batch_recall = 128.89, batch_ndcg = 69.58 
2025-04-08 20:17:20.489839: Steps 102/138: batch_recall = 118.55, batch_ndcg = 68.15 
2025-04-08 20:17:21.757546: Steps 103/138: batch_recall = 140.66, batch_ndcg = 79.57 
2025-04-08 20:17:23.029360: Steps 104/138: batch_recall = 134.69, batch_ndcg = 78.61 
2025-04-08 20:17:24.308490: Steps 105/138: batch_recall = 120.12, batch_ndcg = 67.80 
2025-04-08 20:17:25.596242: Steps 106/138: batch_recall = 103.54, batch_ndcg = 59.20 
2025-04-08 20:17:26.882140: Steps 107/138: batch_recall = 117.43, batch_ndcg = 64.81 
2025-04-08 20:17:28.180395: Steps 108/138: batch_recall = 118.07, batch_ndcg = 71.76 
2025-04-08 20:17:29.463202: Steps 109/138: batch_recall = 138.75, batch_ndcg = 75.83 
2025-04-08 20:17:30.751353: Steps 110/138: batch_recall = 121.70, batch_ndcg = 64.59 
2025-04-08 20:17:32.029007: Steps 111/138: batch_recall = 133.17, batch_ndcg = 84.33 
2025-04-08 20:17:33.310637: Steps 112/138: batch_recall = 167.19, batch_ndcg = 93.50 
2025-04-08 20:17:34.595521: Steps 113/138: batch_recall = 122.39, batch_ndcg = 68.73 
2025-04-08 20:17:35.864978: Steps 114/138: batch_recall = 125.10, batch_ndcg = 72.87 
2025-04-08 20:17:37.130823: Steps 115/138: batch_recall = 118.84, batch_ndcg = 63.77 
2025-04-08 20:17:38.402111: Steps 116/138: batch_recall = 127.29, batch_ndcg = 68.27 
2025-04-08 20:17:39.671464: Steps 117/138: batch_recall = 110.31, batch_ndcg = 65.31 
2025-04-08 20:17:40.950986: Steps 118/138: batch_recall = 122.62, batch_ndcg = 68.43 
2025-04-08 20:17:42.231263: Steps 119/138: batch_recall = 142.74, batch_ndcg = 77.58 
2025-04-08 20:17:43.515256: Steps 120/138: batch_recall = 123.72, batch_ndcg = 70.43 
2025-04-08 20:17:44.798384: Steps 121/138: batch_recall = 147.90, batch_ndcg = 78.68 
2025-04-08 20:17:46.076015: Steps 122/138: batch_recall = 147.99, batch_ndcg = 80.56 
2025-04-08 20:17:47.360927: Steps 123/138: batch_recall = 129.89, batch_ndcg = 74.17 
2025-04-08 20:17:48.635391: Steps 124/138: batch_recall = 153.65, batch_ndcg = 93.18 
2025-04-08 20:17:49.913024: Steps 125/138: batch_recall = 137.30, batch_ndcg = 75.37 
2025-04-08 20:17:51.190893: Steps 126/138: batch_recall = 157.39, batch_ndcg = 86.51 
2025-04-08 20:17:52.456199: Steps 127/138: batch_recall = 143.36, batch_ndcg = 81.26 
2025-04-08 20:17:53.731068: Steps 128/138: batch_recall = 129.35, batch_ndcg = 71.94 
2025-04-08 20:17:54.992448: Steps 129/138: batch_recall = 159.34, batch_ndcg = 92.20 
2025-04-08 20:17:56.282068: Steps 130/138: batch_recall = 135.52, batch_ndcg = 70.94 
2025-04-08 20:17:57.564827: Steps 131/138: batch_recall = 150.00, batch_ndcg = 87.98 
2025-04-08 20:17:58.849534: Steps 132/138: batch_recall = 149.43, batch_ndcg = 84.06 
2025-04-08 20:18:00.123346: Steps 133/138: batch_recall = 148.52, batch_ndcg = 84.96 
2025-04-08 20:18:01.405532: Steps 134/138: batch_recall = 145.38, batch_ndcg = 84.51 
2025-04-08 20:18:02.678650: Steps 135/138: batch_recall = 166.69, batch_ndcg = 95.25 
2025-04-08 20:18:03.956728: Steps 136/138: batch_recall = 152.28, batch_ndcg = 78.89 
2025-04-08 20:18:05.234147: Steps 137/138: batch_recall = 136.78, batch_ndcg = 86.70 
2025-04-08 20:18:05.234925: Epoch 36/1000, Test: Recall = 0.1758, NDCG = 0.1004  

2025-04-08 20:18:07.000394: Training Step 0/354: batchLoss = 0.4929, diffLoss = 2.4095, kgLoss = 0.0137
2025-04-08 20:18:08.620905: Training Step 1/354: batchLoss = 0.6642, diffLoss = 3.2556, kgLoss = 0.0164
2025-04-08 20:18:10.245956: Training Step 2/354: batchLoss = 1.7291, diffLoss = 8.4795, kgLoss = 0.0414
2025-04-08 20:18:11.869877: Training Step 3/354: batchLoss = 0.4218, diffLoss = 2.0595, kgLoss = 0.0124
2025-04-08 20:18:13.503359: Training Step 4/354: batchLoss = 0.6020, diffLoss = 2.9463, kgLoss = 0.0159
2025-04-08 20:18:15.131529: Training Step 5/354: batchLoss = 0.5800, diffLoss = 2.8397, kgLoss = 0.0151
2025-04-08 20:18:16.760016: Training Step 6/354: batchLoss = 0.7066, diffLoss = 3.4672, kgLoss = 0.0164
2025-04-08 20:18:18.385637: Training Step 7/354: batchLoss = 0.5193, diffLoss = 2.5427, kgLoss = 0.0134
2025-04-08 20:18:20.011392: Training Step 8/354: batchLoss = 0.5389, diffLoss = 2.6377, kgLoss = 0.0141
2025-04-08 20:18:21.637082: Training Step 9/354: batchLoss = 0.6097, diffLoss = 2.9864, kgLoss = 0.0155
2025-04-08 20:18:23.259075: Training Step 10/354: batchLoss = 0.6168, diffLoss = 3.0175, kgLoss = 0.0166
2025-04-08 20:18:24.876127: Training Step 11/354: batchLoss = 0.4953, diffLoss = 2.4218, kgLoss = 0.0136
2025-04-08 20:18:26.495205: Training Step 12/354: batchLoss = 0.4704, diffLoss = 2.2987, kgLoss = 0.0133
2025-04-08 20:18:28.117871: Training Step 13/354: batchLoss = 0.5472, diffLoss = 2.6822, kgLoss = 0.0134
2025-04-08 20:18:29.753170: Training Step 14/354: batchLoss = 0.5035, diffLoss = 2.4632, kgLoss = 0.0135
2025-04-08 20:18:31.376728: Training Step 15/354: batchLoss = 0.4889, diffLoss = 2.3936, kgLoss = 0.0127
2025-04-08 20:18:33.005256: Training Step 16/354: batchLoss = 0.5334, diffLoss = 2.6142, kgLoss = 0.0132
2025-04-08 20:18:34.629145: Training Step 17/354: batchLoss = 0.5175, diffLoss = 2.5304, kgLoss = 0.0142
2025-04-08 20:18:36.250444: Training Step 18/354: batchLoss = 0.5316, diffLoss = 2.6024, kgLoss = 0.0139
2025-04-08 20:18:37.876919: Training Step 19/354: batchLoss = 0.5303, diffLoss = 2.5909, kgLoss = 0.0152
2025-04-08 20:18:39.496864: Training Step 20/354: batchLoss = 0.4307, diffLoss = 2.1028, kgLoss = 0.0126
2025-04-08 20:18:41.105712: Training Step 21/354: batchLoss = 0.4983, diffLoss = 2.4381, kgLoss = 0.0134
2025-04-08 20:18:42.731870: Training Step 22/354: batchLoss = 0.4962, diffLoss = 2.4227, kgLoss = 0.0146
2025-04-08 20:18:44.355472: Training Step 23/354: batchLoss = 0.5783, diffLoss = 2.8337, kgLoss = 0.0145
2025-04-08 20:18:45.988705: Training Step 24/354: batchLoss = 0.5711, diffLoss = 2.7974, kgLoss = 0.0145
2025-04-08 20:18:47.622427: Training Step 25/354: batchLoss = 0.6043, diffLoss = 2.9584, kgLoss = 0.0158
2025-04-08 20:18:49.251606: Training Step 26/354: batchLoss = 0.7030, diffLoss = 3.4506, kgLoss = 0.0161
2025-04-08 20:18:50.877277: Training Step 27/354: batchLoss = 0.4953, diffLoss = 2.4245, kgLoss = 0.0130
2025-04-08 20:18:52.497944: Training Step 28/354: batchLoss = 0.5261, diffLoss = 2.5788, kgLoss = 0.0130
2025-04-08 20:18:54.121852: Training Step 29/354: batchLoss = 0.4308, diffLoss = 2.1039, kgLoss = 0.0126
2025-04-08 20:18:55.740977: Training Step 30/354: batchLoss = 0.5705, diffLoss = 2.7884, kgLoss = 0.0161
2025-04-08 20:18:57.360759: Training Step 31/354: batchLoss = 0.5832, diffLoss = 2.8600, kgLoss = 0.0140
2025-04-08 20:18:58.979500: Training Step 32/354: batchLoss = 0.4627, diffLoss = 2.2632, kgLoss = 0.0125
2025-04-08 20:19:00.610823: Training Step 33/354: batchLoss = 0.6364, diffLoss = 3.1143, kgLoss = 0.0169
2025-04-08 20:19:02.240423: Training Step 34/354: batchLoss = 0.5281, diffLoss = 2.5903, kgLoss = 0.0125
2025-04-08 20:19:03.870811: Training Step 35/354: batchLoss = 0.5278, diffLoss = 2.5737, kgLoss = 0.0163
2025-04-08 20:19:05.500259: Training Step 36/354: batchLoss = 0.5716, diffLoss = 2.8010, kgLoss = 0.0142
2025-04-08 20:19:07.136580: Training Step 37/354: batchLoss = 0.6772, diffLoss = 3.3173, kgLoss = 0.0171
2025-04-08 20:19:08.763930: Training Step 38/354: batchLoss = 0.5221, diffLoss = 2.5608, kgLoss = 0.0125
2025-04-08 20:19:10.393864: Training Step 39/354: batchLoss = 0.3810, diffLoss = 1.8497, kgLoss = 0.0138
2025-04-08 20:19:12.015764: Training Step 40/354: batchLoss = 0.4563, diffLoss = 2.2312, kgLoss = 0.0126
2025-04-08 20:19:13.645079: Training Step 41/354: batchLoss = 0.5091, diffLoss = 2.4876, kgLoss = 0.0144
2025-04-08 20:19:15.268331: Training Step 42/354: batchLoss = 0.4094, diffLoss = 2.0014, kgLoss = 0.0114
2025-04-08 20:19:16.898212: Training Step 43/354: batchLoss = 0.5290, diffLoss = 2.5943, kgLoss = 0.0126
2025-04-08 20:19:18.529197: Training Step 44/354: batchLoss = 0.6793, diffLoss = 3.3323, kgLoss = 0.0161
2025-04-08 20:19:20.158766: Training Step 45/354: batchLoss = 0.5970, diffLoss = 2.9282, kgLoss = 0.0142
2025-04-08 20:19:21.796096: Training Step 46/354: batchLoss = 0.6412, diffLoss = 3.1423, kgLoss = 0.0159
2025-04-08 20:19:23.423733: Training Step 47/354: batchLoss = 0.5132, diffLoss = 2.5127, kgLoss = 0.0133
2025-04-08 20:19:25.047153: Training Step 48/354: batchLoss = 0.5068, diffLoss = 2.4749, kgLoss = 0.0147
2025-04-08 20:19:26.673476: Training Step 49/354: batchLoss = 0.5336, diffLoss = 2.6101, kgLoss = 0.0144
2025-04-08 20:19:28.291330: Training Step 50/354: batchLoss = 0.5272, diffLoss = 2.5786, kgLoss = 0.0144
2025-04-08 20:19:29.911038: Training Step 51/354: batchLoss = 0.5266, diffLoss = 2.5739, kgLoss = 0.0147
2025-04-08 20:19:31.527985: Training Step 52/354: batchLoss = 0.4482, diffLoss = 2.1914, kgLoss = 0.0123
2025-04-08 20:19:33.147360: Training Step 53/354: batchLoss = 0.5414, diffLoss = 2.6434, kgLoss = 0.0160
2025-04-08 20:19:34.769064: Training Step 54/354: batchLoss = 0.5410, diffLoss = 2.6484, kgLoss = 0.0142
2025-04-08 20:19:36.393773: Training Step 55/354: batchLoss = 0.5137, diffLoss = 2.5063, kgLoss = 0.0156
2025-04-08 20:19:38.020146: Training Step 56/354: batchLoss = 0.4565, diffLoss = 2.2327, kgLoss = 0.0125
2025-04-08 20:19:39.626082: Training Step 57/354: batchLoss = 0.4898, diffLoss = 2.3993, kgLoss = 0.0124
2025-04-08 20:19:41.244962: Training Step 58/354: batchLoss = 0.5244, diffLoss = 2.5645, kgLoss = 0.0144
2025-04-08 20:19:42.867626: Training Step 59/354: batchLoss = 0.6541, diffLoss = 3.2085, kgLoss = 0.0155
2025-04-08 20:19:44.480346: Training Step 60/354: batchLoss = 0.5086, diffLoss = 2.4944, kgLoss = 0.0122
2025-04-08 20:19:46.093019: Training Step 61/354: batchLoss = 0.5518, diffLoss = 2.7019, kgLoss = 0.0143
2025-04-08 20:19:47.704038: Training Step 62/354: batchLoss = 0.5122, diffLoss = 2.5096, kgLoss = 0.0129
2025-04-08 20:19:49.327651: Training Step 63/354: batchLoss = 0.6218, diffLoss = 3.0453, kgLoss = 0.0159
2025-04-08 20:19:50.957680: Training Step 64/354: batchLoss = 0.5622, diffLoss = 2.7597, kgLoss = 0.0128
2025-04-08 20:19:52.580054: Training Step 65/354: batchLoss = 0.4975, diffLoss = 2.4344, kgLoss = 0.0133
2025-04-08 20:19:54.210997: Training Step 66/354: batchLoss = 0.5674, diffLoss = 2.7792, kgLoss = 0.0145
2025-04-08 20:19:55.837704: Training Step 67/354: batchLoss = 0.4840, diffLoss = 2.3536, kgLoss = 0.0166
2025-04-08 20:19:57.472212: Training Step 68/354: batchLoss = 0.6119, diffLoss = 3.0008, kgLoss = 0.0147
2025-04-08 20:19:59.106178: Training Step 69/354: batchLoss = 0.5384, diffLoss = 2.6375, kgLoss = 0.0136
2025-04-08 20:20:00.724074: Training Step 70/354: batchLoss = 0.4893, diffLoss = 2.3919, kgLoss = 0.0136
2025-04-08 20:20:02.351710: Training Step 71/354: batchLoss = 0.6474, diffLoss = 3.1691, kgLoss = 0.0170
2025-04-08 20:20:03.978935: Training Step 72/354: batchLoss = 0.5624, diffLoss = 2.7569, kgLoss = 0.0138
2025-04-08 20:20:05.602551: Training Step 73/354: batchLoss = 0.4721, diffLoss = 2.3079, kgLoss = 0.0131
2025-04-08 20:20:07.219632: Training Step 74/354: batchLoss = 0.5983, diffLoss = 2.9162, kgLoss = 0.0188
2025-04-08 20:20:08.848651: Training Step 75/354: batchLoss = 0.4958, diffLoss = 2.4245, kgLoss = 0.0136
2025-04-08 20:20:10.473500: Training Step 76/354: batchLoss = 0.5412, diffLoss = 2.6513, kgLoss = 0.0136
2025-04-08 20:20:12.099220: Training Step 77/354: batchLoss = 0.6187, diffLoss = 3.0233, kgLoss = 0.0176
2025-04-08 20:20:13.728147: Training Step 78/354: batchLoss = 0.5262, diffLoss = 2.5712, kgLoss = 0.0150
2025-04-08 20:20:15.360832: Training Step 79/354: batchLoss = 0.8384, diffLoss = 4.1128, kgLoss = 0.0198
2025-04-08 20:20:16.986127: Training Step 80/354: batchLoss = 0.4619, diffLoss = 2.2588, kgLoss = 0.0126
2025-04-08 20:20:18.614680: Training Step 81/354: batchLoss = 0.6148, diffLoss = 3.0180, kgLoss = 0.0140
2025-04-08 20:20:20.241109: Training Step 82/354: batchLoss = 0.5284, diffLoss = 2.5851, kgLoss = 0.0142
2025-04-08 20:20:21.867441: Training Step 83/354: batchLoss = 0.4570, diffLoss = 2.2337, kgLoss = 0.0129
2025-04-08 20:20:23.487728: Training Step 84/354: batchLoss = 0.7343, diffLoss = 3.5976, kgLoss = 0.0185
2025-04-08 20:20:25.117785: Training Step 85/354: batchLoss = 0.6681, diffLoss = 3.2765, kgLoss = 0.0160
2025-04-08 20:20:26.743306: Training Step 86/354: batchLoss = 0.5102, diffLoss = 2.4985, kgLoss = 0.0131
2025-04-08 20:20:28.357162: Training Step 87/354: batchLoss = 0.5296, diffLoss = 2.5925, kgLoss = 0.0138
2025-04-08 20:20:29.984671: Training Step 88/354: batchLoss = 0.6346, diffLoss = 3.1069, kgLoss = 0.0165
2025-04-08 20:20:31.615185: Training Step 89/354: batchLoss = 0.4804, diffLoss = 2.3440, kgLoss = 0.0145
2025-04-08 20:20:33.232860: Training Step 90/354: batchLoss = 0.4808, diffLoss = 2.3526, kgLoss = 0.0128
2025-04-08 20:20:34.853846: Training Step 91/354: batchLoss = 0.6279, diffLoss = 3.0767, kgLoss = 0.0157
2025-04-08 20:20:36.472019: Training Step 92/354: batchLoss = 0.4987, diffLoss = 2.4424, kgLoss = 0.0128
2025-04-08 20:20:38.105227: Training Step 93/354: batchLoss = 0.5774, diffLoss = 2.8298, kgLoss = 0.0143
2025-04-08 20:20:39.724918: Training Step 94/354: batchLoss = 0.5606, diffLoss = 2.7457, kgLoss = 0.0143
2025-04-08 20:20:41.356381: Training Step 95/354: batchLoss = 0.5925, diffLoss = 2.9065, kgLoss = 0.0140
2025-04-08 20:20:42.983666: Training Step 96/354: batchLoss = 0.4921, diffLoss = 2.4089, kgLoss = 0.0129
2025-04-08 20:20:44.613274: Training Step 97/354: batchLoss = 0.5067, diffLoss = 2.4772, kgLoss = 0.0141
2025-04-08 20:20:46.238306: Training Step 98/354: batchLoss = 0.4516, diffLoss = 2.2085, kgLoss = 0.0124
2025-04-08 20:20:47.860230: Training Step 99/354: batchLoss = 0.4873, diffLoss = 2.3830, kgLoss = 0.0134
2025-04-08 20:20:49.483288: Training Step 100/354: batchLoss = 0.5169, diffLoss = 2.5214, kgLoss = 0.0157
2025-04-08 20:20:51.105873: Training Step 101/354: batchLoss = 0.5473, diffLoss = 2.6853, kgLoss = 0.0127
2025-04-08 20:20:52.725149: Training Step 102/354: batchLoss = 0.5345, diffLoss = 2.6193, kgLoss = 0.0133
2025-04-08 20:20:54.347754: Training Step 103/354: batchLoss = 0.5540, diffLoss = 2.7080, kgLoss = 0.0155
2025-04-08 20:20:55.975591: Training Step 104/354: batchLoss = 0.4853, diffLoss = 2.3724, kgLoss = 0.0136
2025-04-08 20:20:57.607080: Training Step 105/354: batchLoss = 0.5542, diffLoss = 2.7103, kgLoss = 0.0152
2025-04-08 20:20:59.235699: Training Step 106/354: batchLoss = 0.7744, diffLoss = 3.7958, kgLoss = 0.0191
2025-04-08 20:21:00.864700: Training Step 107/354: batchLoss = 0.5427, diffLoss = 2.6599, kgLoss = 0.0134
2025-04-08 20:21:02.497334: Training Step 108/354: batchLoss = 0.5458, diffLoss = 2.6709, kgLoss = 0.0145
2025-04-08 20:21:04.124926: Training Step 109/354: batchLoss = 0.6194, diffLoss = 3.0297, kgLoss = 0.0169
2025-04-08 20:21:05.748488: Training Step 110/354: batchLoss = 0.5874, diffLoss = 2.8815, kgLoss = 0.0138
2025-04-08 20:21:07.370967: Training Step 111/354: batchLoss = 0.5035, diffLoss = 2.4627, kgLoss = 0.0137
2025-04-08 20:21:08.985276: Training Step 112/354: batchLoss = 0.5200, diffLoss = 2.5478, kgLoss = 0.0130
2025-04-08 20:21:10.612108: Training Step 113/354: batchLoss = 0.4402, diffLoss = 2.1548, kgLoss = 0.0116
2025-04-08 20:21:12.240320: Training Step 114/354: batchLoss = 0.6206, diffLoss = 3.0393, kgLoss = 0.0159
2025-04-08 20:21:13.866400: Training Step 115/354: batchLoss = 0.5209, diffLoss = 2.5509, kgLoss = 0.0134
2025-04-08 20:21:15.494216: Training Step 116/354: batchLoss = 0.7067, diffLoss = 3.4615, kgLoss = 0.0181
2025-04-08 20:21:17.131392: Training Step 117/354: batchLoss = 0.5199, diffLoss = 2.5454, kgLoss = 0.0135
2025-04-08 20:21:18.762837: Training Step 118/354: batchLoss = 0.7284, diffLoss = 3.5646, kgLoss = 0.0193
2025-04-08 20:21:20.392988: Training Step 119/354: batchLoss = 0.4951, diffLoss = 2.4238, kgLoss = 0.0129
2025-04-08 20:21:22.019324: Training Step 120/354: batchLoss = 0.5567, diffLoss = 2.7253, kgLoss = 0.0145
2025-04-08 20:21:23.635048: Training Step 121/354: batchLoss = 0.5652, diffLoss = 2.7675, kgLoss = 0.0147
2025-04-08 20:21:25.251918: Training Step 122/354: batchLoss = 0.4512, diffLoss = 2.2075, kgLoss = 0.0122
2025-04-08 20:21:26.870514: Training Step 123/354: batchLoss = 0.5484, diffLoss = 2.6896, kgLoss = 0.0131
2025-04-08 20:21:28.508133: Training Step 124/354: batchLoss = 0.5200, diffLoss = 2.5480, kgLoss = 0.0130
2025-04-08 20:21:30.131670: Training Step 125/354: batchLoss = 0.6703, diffLoss = 3.2777, kgLoss = 0.0184
2025-04-08 20:21:31.759700: Training Step 126/354: batchLoss = 0.5526, diffLoss = 2.6934, kgLoss = 0.0174
2025-04-08 20:21:33.393215: Training Step 127/354: batchLoss = 0.5541, diffLoss = 2.7084, kgLoss = 0.0155
2025-04-08 20:21:35.013804: Training Step 128/354: batchLoss = 0.6640, diffLoss = 3.2538, kgLoss = 0.0166
2025-04-08 20:21:36.636114: Training Step 129/354: batchLoss = 0.7196, diffLoss = 3.5270, kgLoss = 0.0177
2025-04-08 20:21:38.259767: Training Step 130/354: batchLoss = 0.5880, diffLoss = 2.8836, kgLoss = 0.0141
2025-04-08 20:21:39.876513: Training Step 131/354: batchLoss = 0.5564, diffLoss = 2.7302, kgLoss = 0.0129
2025-04-08 20:21:41.499291: Training Step 132/354: batchLoss = 0.4944, diffLoss = 2.4140, kgLoss = 0.0145
2025-04-08 20:21:43.127508: Training Step 133/354: batchLoss = 0.5149, diffLoss = 2.5175, kgLoss = 0.0142
2025-04-08 20:21:44.759400: Training Step 134/354: batchLoss = 0.6459, diffLoss = 3.1697, kgLoss = 0.0149
2025-04-08 20:21:46.383555: Training Step 135/354: batchLoss = 0.6509, diffLoss = 3.1868, kgLoss = 0.0169
2025-04-08 20:21:48.013924: Training Step 136/354: batchLoss = 0.4719, diffLoss = 2.3025, kgLoss = 0.0142
2025-04-08 20:21:49.645337: Training Step 137/354: batchLoss = 0.6471, diffLoss = 3.1670, kgLoss = 0.0172
2025-04-08 20:21:51.271713: Training Step 138/354: batchLoss = 0.5538, diffLoss = 2.7070, kgLoss = 0.0155
2025-04-08 20:21:52.897911: Training Step 139/354: batchLoss = 0.5443, diffLoss = 2.6600, kgLoss = 0.0154
2025-04-08 20:21:54.517309: Training Step 140/354: batchLoss = 0.4077, diffLoss = 1.9920, kgLoss = 0.0116
2025-04-08 20:21:56.135730: Training Step 141/354: batchLoss = 0.5698, diffLoss = 2.7888, kgLoss = 0.0151
2025-04-08 20:21:57.757666: Training Step 142/354: batchLoss = 0.5517, diffLoss = 2.7018, kgLoss = 0.0142
2025-04-08 20:21:59.388739: Training Step 143/354: batchLoss = 0.6003, diffLoss = 2.9414, kgLoss = 0.0150
2025-04-08 20:22:01.011202: Training Step 144/354: batchLoss = 0.5624, diffLoss = 2.7490, kgLoss = 0.0157
2025-04-08 20:22:02.640676: Training Step 145/354: batchLoss = 0.4946, diffLoss = 2.4159, kgLoss = 0.0143
2025-04-08 20:22:04.273597: Training Step 146/354: batchLoss = 0.5364, diffLoss = 2.6260, kgLoss = 0.0140
2025-04-08 20:22:05.903082: Training Step 147/354: batchLoss = 0.4635, diffLoss = 2.2672, kgLoss = 0.0125
2025-04-08 20:22:07.525957: Training Step 148/354: batchLoss = 0.6586, diffLoss = 3.2238, kgLoss = 0.0173
2025-04-08 20:22:09.151148: Training Step 149/354: batchLoss = 0.5301, diffLoss = 2.5882, kgLoss = 0.0155
2025-04-08 20:22:10.774000: Training Step 150/354: batchLoss = 0.4579, diffLoss = 2.2350, kgLoss = 0.0136
2025-04-08 20:22:12.390703: Training Step 151/354: batchLoss = 0.4607, diffLoss = 2.2499, kgLoss = 0.0133
2025-04-08 20:22:14.019355: Training Step 152/354: batchLoss = 0.4401, diffLoss = 2.1498, kgLoss = 0.0127
2025-04-08 20:22:15.639339: Training Step 153/354: batchLoss = 0.6257, diffLoss = 3.0554, kgLoss = 0.0183
2025-04-08 20:22:17.264275: Training Step 154/354: batchLoss = 0.5300, diffLoss = 2.5974, kgLoss = 0.0132
2025-04-08 20:22:18.897061: Training Step 155/354: batchLoss = 0.6920, diffLoss = 3.3996, kgLoss = 0.0152
2025-04-08 20:22:20.529149: Training Step 156/354: batchLoss = 0.4518, diffLoss = 2.2060, kgLoss = 0.0133
2025-04-08 20:22:22.158457: Training Step 157/354: batchLoss = 0.5383, diffLoss = 2.6384, kgLoss = 0.0133
2025-04-08 20:22:23.778714: Training Step 158/354: batchLoss = 0.5156, diffLoss = 2.5221, kgLoss = 0.0140
2025-04-08 20:22:25.398122: Training Step 159/354: batchLoss = 0.5681, diffLoss = 2.7793, kgLoss = 0.0153
2025-04-08 20:22:27.021497: Training Step 160/354: batchLoss = 0.4694, diffLoss = 2.2918, kgLoss = 0.0138
2025-04-08 20:22:28.643354: Training Step 161/354: batchLoss = 0.5827, diffLoss = 2.8533, kgLoss = 0.0150
2025-04-08 20:22:30.257765: Training Step 162/354: batchLoss = 0.5111, diffLoss = 2.4983, kgLoss = 0.0142
2025-04-08 20:22:31.881415: Training Step 163/354: batchLoss = 0.4973, diffLoss = 2.4333, kgLoss = 0.0133
2025-04-08 20:22:33.501664: Training Step 164/354: batchLoss = 0.5607, diffLoss = 2.7500, kgLoss = 0.0134
2025-04-08 20:22:35.129387: Training Step 165/354: batchLoss = 0.5443, diffLoss = 2.6507, kgLoss = 0.0178
2025-04-08 20:22:36.748317: Training Step 166/354: batchLoss = 0.5361, diffLoss = 2.6226, kgLoss = 0.0145
2025-04-08 20:22:38.378208: Training Step 167/354: batchLoss = 0.5181, diffLoss = 2.5360, kgLoss = 0.0137
2025-04-08 20:22:40.002786: Training Step 168/354: batchLoss = 0.4389, diffLoss = 2.1428, kgLoss = 0.0129
2025-04-08 20:22:41.616412: Training Step 169/354: batchLoss = 0.5650, diffLoss = 2.7705, kgLoss = 0.0137
2025-04-08 20:22:43.231531: Training Step 170/354: batchLoss = 0.5109, diffLoss = 2.4994, kgLoss = 0.0137
2025-04-08 20:22:44.853643: Training Step 171/354: batchLoss = 0.5158, diffLoss = 2.5224, kgLoss = 0.0142
2025-04-08 20:22:46.478418: Training Step 172/354: batchLoss = 0.6420, diffLoss = 3.1452, kgLoss = 0.0162
2025-04-08 20:22:48.107991: Training Step 173/354: batchLoss = 0.6298, diffLoss = 3.0923, kgLoss = 0.0142
2025-04-08 20:22:49.737872: Training Step 174/354: batchLoss = 0.5605, diffLoss = 2.7417, kgLoss = 0.0152
2025-04-08 20:22:51.362844: Training Step 175/354: batchLoss = 0.5934, diffLoss = 2.8985, kgLoss = 0.0171
2025-04-08 20:22:52.995121: Training Step 176/354: batchLoss = 0.6948, diffLoss = 3.4090, kgLoss = 0.0163
2025-04-08 20:22:54.624726: Training Step 177/354: batchLoss = 0.5616, diffLoss = 2.7531, kgLoss = 0.0137
2025-04-08 20:22:56.246015: Training Step 178/354: batchLoss = 0.4647, diffLoss = 2.2756, kgLoss = 0.0120
2025-04-08 20:22:57.867468: Training Step 179/354: batchLoss = 0.5607, diffLoss = 2.7391, kgLoss = 0.0161
2025-04-08 20:22:59.488793: Training Step 180/354: batchLoss = 0.5212, diffLoss = 2.5473, kgLoss = 0.0146
2025-04-08 20:23:01.109406: Training Step 181/354: batchLoss = 0.6917, diffLoss = 3.3918, kgLoss = 0.0167
2025-04-08 20:23:02.736974: Training Step 182/354: batchLoss = 0.5151, diffLoss = 2.5227, kgLoss = 0.0132
2025-04-08 20:23:04.376283: Training Step 183/354: batchLoss = 0.5511, diffLoss = 2.6933, kgLoss = 0.0156
2025-04-08 20:23:06.009603: Training Step 184/354: batchLoss = 0.5383, diffLoss = 2.6240, kgLoss = 0.0169
2025-04-08 20:23:07.638737: Training Step 185/354: batchLoss = 0.4742, diffLoss = 2.3214, kgLoss = 0.0124
2025-04-08 20:23:09.268615: Training Step 186/354: batchLoss = 0.5774, diffLoss = 2.8163, kgLoss = 0.0177
2025-04-08 20:23:10.907321: Training Step 187/354: batchLoss = 0.6500, diffLoss = 3.1754, kgLoss = 0.0186
2025-04-08 20:23:12.530000: Training Step 188/354: batchLoss = 0.4946, diffLoss = 2.4184, kgLoss = 0.0136
2025-04-08 20:23:14.152524: Training Step 189/354: batchLoss = 0.5959, diffLoss = 2.9207, kgLoss = 0.0146
2025-04-08 20:23:15.782040: Training Step 190/354: batchLoss = 0.4976, diffLoss = 2.4265, kgLoss = 0.0154
2025-04-08 20:23:17.397346: Training Step 191/354: batchLoss = 0.5907, diffLoss = 2.8937, kgLoss = 0.0149
2025-04-08 20:23:19.047547: Training Step 192/354: batchLoss = 0.6135, diffLoss = 2.9966, kgLoss = 0.0177
2025-04-08 20:23:20.677612: Training Step 193/354: batchLoss = 0.5843, diffLoss = 2.8625, kgLoss = 0.0147
2025-04-08 20:23:22.299625: Training Step 194/354: batchLoss = 0.4668, diffLoss = 2.2846, kgLoss = 0.0124
2025-04-08 20:23:23.924159: Training Step 195/354: batchLoss = 0.5788, diffLoss = 2.8359, kgLoss = 0.0146
2025-04-08 20:23:25.550501: Training Step 196/354: batchLoss = 0.4697, diffLoss = 2.2937, kgLoss = 0.0137
2025-04-08 20:23:27.178185: Training Step 197/354: batchLoss = 0.5746, diffLoss = 2.8069, kgLoss = 0.0165
2025-04-08 20:23:28.798987: Training Step 198/354: batchLoss = 0.6271, diffLoss = 3.0714, kgLoss = 0.0160
2025-04-08 20:23:30.415548: Training Step 199/354: batchLoss = 0.6043, diffLoss = 2.9610, kgLoss = 0.0151
2025-04-08 20:23:32.036388: Training Step 200/354: batchLoss = 0.7261, diffLoss = 3.5612, kgLoss = 0.0173
2025-04-08 20:23:33.660701: Training Step 201/354: batchLoss = 0.6230, diffLoss = 3.0559, kgLoss = 0.0147
2025-04-08 20:23:35.284201: Training Step 202/354: batchLoss = 0.5703, diffLoss = 2.7839, kgLoss = 0.0169
2025-04-08 20:23:36.910534: Training Step 203/354: batchLoss = 0.4503, diffLoss = 2.2074, kgLoss = 0.0111
2025-04-08 20:23:38.537385: Training Step 204/354: batchLoss = 0.4475, diffLoss = 2.1877, kgLoss = 0.0124
2025-04-08 20:23:40.161765: Training Step 205/354: batchLoss = 0.5756, diffLoss = 2.8157, kgLoss = 0.0156
2025-04-08 20:23:41.780890: Training Step 206/354: batchLoss = 0.5407, diffLoss = 2.6445, kgLoss = 0.0147
2025-04-08 20:23:43.403805: Training Step 207/354: batchLoss = 0.4944, diffLoss = 2.4210, kgLoss = 0.0128
2025-04-08 20:23:45.017453: Training Step 208/354: batchLoss = 0.6555, diffLoss = 3.2105, kgLoss = 0.0168
2025-04-08 20:23:46.639971: Training Step 209/354: batchLoss = 0.6394, diffLoss = 3.1246, kgLoss = 0.0181
2025-04-08 20:23:48.264807: Training Step 210/354: batchLoss = 0.6068, diffLoss = 2.9646, kgLoss = 0.0174
2025-04-08 20:23:49.884458: Training Step 211/354: batchLoss = 0.4981, diffLoss = 2.4421, kgLoss = 0.0121
2025-04-08 20:23:51.562699: Training Step 212/354: batchLoss = 0.6513, diffLoss = 3.1937, kgLoss = 0.0157
2025-04-08 20:23:53.188521: Training Step 213/354: batchLoss = 0.4252, diffLoss = 2.0732, kgLoss = 0.0132
2025-04-08 20:23:54.815691: Training Step 214/354: batchLoss = 0.5539, diffLoss = 2.7180, kgLoss = 0.0129
2025-04-08 20:23:56.445993: Training Step 215/354: batchLoss = 0.4761, diffLoss = 2.3279, kgLoss = 0.0131
2025-04-08 20:23:58.073753: Training Step 216/354: batchLoss = 0.5485, diffLoss = 2.6871, kgLoss = 0.0138
2025-04-08 20:23:59.699807: Training Step 217/354: batchLoss = 0.5476, diffLoss = 2.6856, kgLoss = 0.0131
2025-04-08 20:24:01.320547: Training Step 218/354: batchLoss = 0.4702, diffLoss = 2.3041, kgLoss = 0.0118
2025-04-08 20:24:02.941791: Training Step 219/354: batchLoss = 0.5853, diffLoss = 2.8583, kgLoss = 0.0170
2025-04-08 20:24:04.560793: Training Step 220/354: batchLoss = 0.5280, diffLoss = 2.5831, kgLoss = 0.0142
2025-04-08 20:24:06.188392: Training Step 221/354: batchLoss = 0.4954, diffLoss = 2.4218, kgLoss = 0.0138
2025-04-08 20:24:07.814240: Training Step 222/354: batchLoss = 0.6510, diffLoss = 3.1896, kgLoss = 0.0163
2025-04-08 20:24:09.442254: Training Step 223/354: batchLoss = 0.7081, diffLoss = 3.4763, kgLoss = 0.0161
2025-04-08 20:24:11.074053: Training Step 224/354: batchLoss = 0.5089, diffLoss = 2.4896, kgLoss = 0.0137
2025-04-08 20:24:12.701753: Training Step 225/354: batchLoss = 0.5871, diffLoss = 2.8744, kgLoss = 0.0152
2025-04-08 20:24:14.328882: Training Step 226/354: batchLoss = 0.5980, diffLoss = 2.9236, kgLoss = 0.0167
2025-04-08 20:24:15.952643: Training Step 227/354: batchLoss = 0.4782, diffLoss = 2.3408, kgLoss = 0.0126
2025-04-08 20:24:17.575311: Training Step 228/354: batchLoss = 0.5977, diffLoss = 2.9247, kgLoss = 0.0160
2025-04-08 20:24:19.196458: Training Step 229/354: batchLoss = 0.6109, diffLoss = 2.9817, kgLoss = 0.0182
2025-04-08 20:24:20.826076: Training Step 230/354: batchLoss = 0.9024, diffLoss = 4.4239, kgLoss = 0.0220
2025-04-08 20:24:22.448684: Training Step 231/354: batchLoss = 0.5245, diffLoss = 2.5713, kgLoss = 0.0128
2025-04-08 20:24:24.088083: Training Step 232/354: batchLoss = 0.6771, diffLoss = 3.3177, kgLoss = 0.0169
2025-04-08 20:24:25.718733: Training Step 233/354: batchLoss = 0.4973, diffLoss = 2.4359, kgLoss = 0.0127
2025-04-08 20:24:27.347906: Training Step 234/354: batchLoss = 0.6051, diffLoss = 2.9640, kgLoss = 0.0154
2025-04-08 20:24:28.973661: Training Step 235/354: batchLoss = 0.5453, diffLoss = 2.6692, kgLoss = 0.0143
2025-04-08 20:24:30.608597: Training Step 236/354: batchLoss = 0.5451, diffLoss = 2.6674, kgLoss = 0.0145
2025-04-08 20:24:32.240066: Training Step 237/354: batchLoss = 0.6299, diffLoss = 3.0911, kgLoss = 0.0146
2025-04-08 20:24:33.852941: Training Step 238/354: batchLoss = 0.5608, diffLoss = 2.7430, kgLoss = 0.0152
2025-04-08 20:24:35.474387: Training Step 239/354: batchLoss = 0.6062, diffLoss = 2.9728, kgLoss = 0.0146
2025-04-08 20:24:37.091780: Training Step 240/354: batchLoss = 0.4364, diffLoss = 2.1326, kgLoss = 0.0123
2025-04-08 20:24:38.712038: Training Step 241/354: batchLoss = 0.7423, diffLoss = 3.6319, kgLoss = 0.0198
2025-04-08 20:24:40.348269: Training Step 242/354: batchLoss = 0.6132, diffLoss = 2.9995, kgLoss = 0.0166
2025-04-08 20:24:41.973840: Training Step 243/354: batchLoss = 0.4526, diffLoss = 2.2160, kgLoss = 0.0118
2025-04-08 20:24:43.601911: Training Step 244/354: batchLoss = 0.4970, diffLoss = 2.4273, kgLoss = 0.0145
2025-04-08 20:24:45.233458: Training Step 245/354: batchLoss = 0.5619, diffLoss = 2.7498, kgLoss = 0.0149
2025-04-08 20:24:46.868205: Training Step 246/354: batchLoss = 0.4501, diffLoss = 2.1938, kgLoss = 0.0142
2025-04-08 20:24:48.489702: Training Step 247/354: batchLoss = 0.6245, diffLoss = 3.0553, kgLoss = 0.0168
2025-04-08 20:24:50.116117: Training Step 248/354: batchLoss = 0.4319, diffLoss = 2.1063, kgLoss = 0.0133
2025-04-08 20:24:51.732980: Training Step 249/354: batchLoss = 0.5633, diffLoss = 2.7568, kgLoss = 0.0149
2025-04-08 20:24:53.354291: Training Step 250/354: batchLoss = 0.7682, diffLoss = 3.7607, kgLoss = 0.0200
2025-04-08 20:24:54.984372: Training Step 251/354: batchLoss = 0.6064, diffLoss = 2.9717, kgLoss = 0.0151
2025-04-08 20:24:56.624986: Training Step 252/354: batchLoss = 0.5397, diffLoss = 2.6396, kgLoss = 0.0147
2025-04-08 20:24:58.255644: Training Step 253/354: batchLoss = 0.5380, diffLoss = 2.6403, kgLoss = 0.0124
2025-04-08 20:24:59.884211: Training Step 254/354: batchLoss = 0.5504, diffLoss = 2.6940, kgLoss = 0.0146
2025-04-08 20:25:01.512259: Training Step 255/354: batchLoss = 0.5378, diffLoss = 2.6333, kgLoss = 0.0140
2025-04-08 20:25:03.146375: Training Step 256/354: batchLoss = 0.6445, diffLoss = 3.1586, kgLoss = 0.0159
2025-04-08 20:25:04.771420: Training Step 257/354: batchLoss = 0.5681, diffLoss = 2.7734, kgLoss = 0.0167
2025-04-08 20:25:06.392291: Training Step 258/354: batchLoss = 0.5730, diffLoss = 2.8032, kgLoss = 0.0154
2025-04-08 20:25:08.015768: Training Step 259/354: batchLoss = 0.4906, diffLoss = 2.4045, kgLoss = 0.0121
2025-04-08 20:25:09.636076: Training Step 260/354: batchLoss = 0.6158, diffLoss = 3.0164, kgLoss = 0.0156
2025-04-08 20:25:11.263873: Training Step 261/354: batchLoss = 0.5786, diffLoss = 2.8306, kgLoss = 0.0157
2025-04-08 20:25:12.889580: Training Step 262/354: batchLoss = 0.4394, diffLoss = 2.1524, kgLoss = 0.0112
2025-04-08 20:25:14.515003: Training Step 263/354: batchLoss = 0.5063, diffLoss = 2.4597, kgLoss = 0.0180
2025-04-08 20:25:16.148658: Training Step 264/354: batchLoss = 0.4826, diffLoss = 2.3569, kgLoss = 0.0140
2025-04-08 20:25:17.776691: Training Step 265/354: batchLoss = 0.4961, diffLoss = 2.4239, kgLoss = 0.0142
2025-04-08 20:25:19.403917: Training Step 266/354: batchLoss = 0.4662, diffLoss = 2.2730, kgLoss = 0.0145
2025-04-08 20:25:21.021620: Training Step 267/354: batchLoss = 0.6073, diffLoss = 2.9771, kgLoss = 0.0148
2025-04-08 20:25:22.646755: Training Step 268/354: batchLoss = 0.6463, diffLoss = 3.1708, kgLoss = 0.0151
2025-04-08 20:25:24.275036: Training Step 269/354: batchLoss = 0.6302, diffLoss = 3.0888, kgLoss = 0.0156
2025-04-08 20:25:25.907686: Training Step 270/354: batchLoss = 0.6231, diffLoss = 3.0503, kgLoss = 0.0163
2025-04-08 20:25:27.539331: Training Step 271/354: batchLoss = 0.5139, diffLoss = 2.5129, kgLoss = 0.0142
2025-04-08 20:25:29.166631: Training Step 272/354: batchLoss = 0.5506, diffLoss = 2.6920, kgLoss = 0.0153
2025-04-08 20:25:30.787315: Training Step 273/354: batchLoss = 0.5407, diffLoss = 2.6468, kgLoss = 0.0142
2025-04-08 20:25:32.409460: Training Step 274/354: batchLoss = 0.4544, diffLoss = 2.2167, kgLoss = 0.0138
2025-04-08 20:25:34.034474: Training Step 275/354: batchLoss = 0.5871, diffLoss = 2.8788, kgLoss = 0.0142
2025-04-08 20:25:35.658693: Training Step 276/354: batchLoss = 0.4889, diffLoss = 2.3815, kgLoss = 0.0157
2025-04-08 20:25:37.277260: Training Step 277/354: batchLoss = 0.5095, diffLoss = 2.4936, kgLoss = 0.0134
2025-04-08 20:25:38.897609: Training Step 278/354: batchLoss = 0.5263, diffLoss = 2.5744, kgLoss = 0.0143
2025-04-08 20:25:40.521997: Training Step 279/354: batchLoss = 0.5328, diffLoss = 2.6029, kgLoss = 0.0153
2025-04-08 20:25:42.150766: Training Step 280/354: batchLoss = 0.4942, diffLoss = 2.4200, kgLoss = 0.0128
2025-04-08 20:25:43.790443: Training Step 281/354: batchLoss = 0.6166, diffLoss = 3.0172, kgLoss = 0.0165
2025-04-08 20:25:45.424490: Training Step 282/354: batchLoss = 0.6622, diffLoss = 3.2371, kgLoss = 0.0185
2025-04-08 20:25:47.050243: Training Step 283/354: batchLoss = 0.4986, diffLoss = 2.4401, kgLoss = 0.0133
2025-04-08 20:25:48.682246: Training Step 284/354: batchLoss = 0.5200, diffLoss = 2.5453, kgLoss = 0.0137
2025-04-08 20:25:50.317485: Training Step 285/354: batchLoss = 0.4526, diffLoss = 2.2105, kgLoss = 0.0131
2025-04-08 20:25:51.942277: Training Step 286/354: batchLoss = 0.4573, diffLoss = 2.2368, kgLoss = 0.0124
2025-04-08 20:25:53.565246: Training Step 287/354: batchLoss = 0.4600, diffLoss = 2.2519, kgLoss = 0.0120
2025-04-08 20:25:55.183896: Training Step 288/354: batchLoss = 0.5729, diffLoss = 2.8021, kgLoss = 0.0156
2025-04-08 20:25:56.806876: Training Step 289/354: batchLoss = 0.4804, diffLoss = 2.3440, kgLoss = 0.0145
2025-04-08 20:25:58.439584: Training Step 290/354: batchLoss = 0.5442, diffLoss = 2.6629, kgLoss = 0.0145
2025-04-08 20:26:00.070675: Training Step 291/354: batchLoss = 0.5211, diffLoss = 2.5512, kgLoss = 0.0136
2025-04-08 20:26:01.702025: Training Step 292/354: batchLoss = 0.5296, diffLoss = 2.5916, kgLoss = 0.0142
2025-04-08 20:26:03.330730: Training Step 293/354: batchLoss = 0.7182, diffLoss = 3.5196, kgLoss = 0.0178
2025-04-08 20:26:04.969486: Training Step 294/354: batchLoss = 0.4831, diffLoss = 2.3632, kgLoss = 0.0130
2025-04-08 20:26:06.605966: Training Step 295/354: batchLoss = 0.4857, diffLoss = 2.3798, kgLoss = 0.0121
2025-04-08 20:26:08.229017: Training Step 296/354: batchLoss = 0.6418, diffLoss = 3.1436, kgLoss = 0.0164
2025-04-08 20:26:09.855549: Training Step 297/354: batchLoss = 0.4951, diffLoss = 2.4209, kgLoss = 0.0136
2025-04-08 20:26:11.476744: Training Step 298/354: batchLoss = 0.6101, diffLoss = 2.9930, kgLoss = 0.0144
2025-04-08 20:26:13.108733: Training Step 299/354: batchLoss = 0.5263, diffLoss = 2.5755, kgLoss = 0.0140
2025-04-08 20:26:14.740418: Training Step 300/354: batchLoss = 0.6030, diffLoss = 2.9517, kgLoss = 0.0159
2025-04-08 20:26:16.371547: Training Step 301/354: batchLoss = 0.5114, diffLoss = 2.5019, kgLoss = 0.0137
2025-04-08 20:26:18.006732: Training Step 302/354: batchLoss = 0.6328, diffLoss = 3.1017, kgLoss = 0.0156
2025-04-08 20:26:19.638400: Training Step 303/354: batchLoss = 0.5185, diffLoss = 2.5379, kgLoss = 0.0137
2025-04-08 20:26:21.273006: Training Step 304/354: batchLoss = 0.4264, diffLoss = 2.0776, kgLoss = 0.0136
2025-04-08 20:26:22.904757: Training Step 305/354: batchLoss = 0.5531, diffLoss = 2.7057, kgLoss = 0.0150
2025-04-08 20:26:24.530830: Training Step 306/354: batchLoss = 0.5575, diffLoss = 2.6892, kgLoss = 0.0246
2025-04-08 20:26:26.147157: Training Step 307/354: batchLoss = 0.4872, diffLoss = 2.3780, kgLoss = 0.0145
2025-04-08 20:26:27.764168: Training Step 308/354: batchLoss = 0.5711, diffLoss = 2.7997, kgLoss = 0.0140
2025-04-08 20:26:29.388844: Training Step 309/354: batchLoss = 0.4467, diffLoss = 2.1838, kgLoss = 0.0124
2025-04-08 20:26:31.016253: Training Step 310/354: batchLoss = 0.5282, diffLoss = 2.5933, kgLoss = 0.0119
2025-04-08 20:26:32.651695: Training Step 311/354: batchLoss = 0.5063, diffLoss = 2.4738, kgLoss = 0.0144
2025-04-08 20:26:34.278576: Training Step 312/354: batchLoss = 0.6726, diffLoss = 3.2976, kgLoss = 0.0163
2025-04-08 20:26:35.906579: Training Step 313/354: batchLoss = 0.5503, diffLoss = 2.6916, kgLoss = 0.0150
2025-04-08 20:26:37.539902: Training Step 314/354: batchLoss = 0.4712, diffLoss = 2.3052, kgLoss = 0.0127
2025-04-08 20:26:39.164930: Training Step 315/354: batchLoss = 0.5473, diffLoss = 2.6801, kgLoss = 0.0142
2025-04-08 20:26:40.798684: Training Step 316/354: batchLoss = 0.4547, diffLoss = 2.2194, kgLoss = 0.0136
2025-04-08 20:26:42.417850: Training Step 317/354: batchLoss = 0.6068, diffLoss = 2.9722, kgLoss = 0.0155
2025-04-08 20:26:44.040663: Training Step 318/354: batchLoss = 0.5648, diffLoss = 2.7604, kgLoss = 0.0159
2025-04-08 20:26:45.662662: Training Step 319/354: batchLoss = 0.5474, diffLoss = 2.6758, kgLoss = 0.0154
2025-04-08 20:26:47.298172: Training Step 320/354: batchLoss = 0.6783, diffLoss = 3.3107, kgLoss = 0.0201
2025-04-08 20:26:48.933842: Training Step 321/354: batchLoss = 0.5669, diffLoss = 2.7716, kgLoss = 0.0157
2025-04-08 20:26:50.565849: Training Step 322/354: batchLoss = 0.5316, diffLoss = 2.6103, kgLoss = 0.0119
2025-04-08 20:26:52.195036: Training Step 323/354: batchLoss = 0.4718, diffLoss = 2.3073, kgLoss = 0.0130
2025-04-08 20:26:53.830548: Training Step 324/354: batchLoss = 0.5675, diffLoss = 2.7749, kgLoss = 0.0156
2025-04-08 20:26:55.456447: Training Step 325/354: batchLoss = 0.6012, diffLoss = 2.9431, kgLoss = 0.0158
2025-04-08 20:26:57.079935: Training Step 326/354: batchLoss = 0.6663, diffLoss = 3.2668, kgLoss = 0.0162
2025-04-08 20:26:58.710427: Training Step 327/354: batchLoss = 0.6480, diffLoss = 3.1784, kgLoss = 0.0154
2025-04-08 20:27:00.332714: Training Step 328/354: batchLoss = 0.5101, diffLoss = 2.5006, kgLoss = 0.0125
2025-04-08 20:27:01.961052: Training Step 329/354: batchLoss = 0.4570, diffLoss = 2.2361, kgLoss = 0.0122
2025-04-08 20:27:03.591170: Training Step 330/354: batchLoss = 0.5522, diffLoss = 2.7029, kgLoss = 0.0145
2025-04-08 20:27:05.225819: Training Step 331/354: batchLoss = 0.4890, diffLoss = 2.3887, kgLoss = 0.0141
2025-04-08 20:27:06.855909: Training Step 332/354: batchLoss = 0.5458, diffLoss = 2.6668, kgLoss = 0.0156
2025-04-08 20:27:08.490464: Training Step 333/354: batchLoss = 0.4955, diffLoss = 2.4251, kgLoss = 0.0131
2025-04-08 20:27:10.122626: Training Step 334/354: batchLoss = 0.4802, diffLoss = 2.3486, kgLoss = 0.0131
2025-04-08 20:27:11.755299: Training Step 335/354: batchLoss = 0.4766, diffLoss = 2.3316, kgLoss = 0.0129
2025-04-08 20:27:13.376408: Training Step 336/354: batchLoss = 0.6082, diffLoss = 2.9747, kgLoss = 0.0166
2025-04-08 20:27:15.004259: Training Step 337/354: batchLoss = 0.5890, diffLoss = 2.8765, kgLoss = 0.0171
2025-04-08 20:27:16.618148: Training Step 338/354: batchLoss = 0.5266, diffLoss = 2.5760, kgLoss = 0.0143
2025-04-08 20:27:18.243320: Training Step 339/354: batchLoss = 0.6527, diffLoss = 3.2025, kgLoss = 0.0153
2025-04-08 20:27:19.878031: Training Step 340/354: batchLoss = 0.4571, diffLoss = 2.2370, kgLoss = 0.0122
2025-04-08 20:27:21.508169: Training Step 341/354: batchLoss = 0.4670, diffLoss = 2.2867, kgLoss = 0.0121
2025-04-08 20:27:23.145308: Training Step 342/354: batchLoss = 0.5501, diffLoss = 2.6941, kgLoss = 0.0141
2025-04-08 20:27:24.781626: Training Step 343/354: batchLoss = 0.4615, diffLoss = 2.2591, kgLoss = 0.0122
2025-04-08 20:27:26.402345: Training Step 344/354: batchLoss = 0.4712, diffLoss = 2.3064, kgLoss = 0.0124
2025-04-08 20:27:28.030057: Training Step 345/354: batchLoss = 0.6486, diffLoss = 3.1844, kgLoss = 0.0147
2025-04-08 20:27:29.649241: Training Step 346/354: batchLoss = 0.5446, diffLoss = 2.6565, kgLoss = 0.0167
2025-04-08 20:27:31.263081: Training Step 347/354: batchLoss = 0.5925, diffLoss = 2.9045, kgLoss = 0.0145
2025-04-08 20:27:32.880467: Training Step 348/354: batchLoss = 0.5086, diffLoss = 2.4912, kgLoss = 0.0129
2025-04-08 20:27:34.500913: Training Step 349/354: batchLoss = 0.5758, diffLoss = 2.8111, kgLoss = 0.0170
2025-04-08 20:27:36.130860: Training Step 350/354: batchLoss = 0.5465, diffLoss = 2.6703, kgLoss = 0.0156
2025-04-08 20:27:37.756504: Training Step 351/354: batchLoss = 0.5468, diffLoss = 2.6755, kgLoss = 0.0146
2025-04-08 20:27:39.365915: Training Step 352/354: batchLoss = 0.5235, diffLoss = 2.5573, kgLoss = 0.0151
2025-04-08 20:27:40.781086: Training Step 353/354: batchLoss = 0.5365, diffLoss = 2.6269, kgLoss = 0.0139
2025-04-08 20:27:40.868933: 
2025-04-08 20:27:40.869566: Epoch 37/1000, Train: epLoss = 0.9813, epDfLoss = 4.8020, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 20:27:42.195247: Steps 0/138: batch_recall = 46.80, batch_ndcg = 26.51 
2025-04-08 20:27:43.525878: Steps 1/138: batch_recall = 47.63, batch_ndcg = 28.06 
2025-04-08 20:27:44.819682: Steps 2/138: batch_recall = 59.92, batch_ndcg = 36.09 
2025-04-08 20:27:46.139010: Steps 3/138: batch_recall = 56.71, batch_ndcg = 33.04 
2025-04-08 20:27:47.441086: Steps 4/138: batch_recall = 66.46, batch_ndcg = 40.28 
2025-04-08 20:27:48.744097: Steps 5/138: batch_recall = 57.22, batch_ndcg = 31.02 
2025-04-08 20:27:50.051373: Steps 6/138: batch_recall = 51.78, batch_ndcg = 31.83 
2025-04-08 20:27:51.367147: Steps 7/138: batch_recall = 62.05, batch_ndcg = 41.55 
2025-04-08 20:27:52.677338: Steps 8/138: batch_recall = 61.35, batch_ndcg = 38.20 
2025-04-08 20:27:53.983946: Steps 9/138: batch_recall = 58.46, batch_ndcg = 34.49 
2025-04-08 20:27:55.299793: Steps 10/138: batch_recall = 57.36, batch_ndcg = 32.76 
2025-04-08 20:27:56.602934: Steps 11/138: batch_recall = 55.83, batch_ndcg = 33.04 
2025-04-08 20:27:57.915627: Steps 12/138: batch_recall = 48.24, batch_ndcg = 28.71 
2025-04-08 20:27:59.216705: Steps 13/138: batch_recall = 54.52, batch_ndcg = 32.16 
2025-04-08 20:28:00.509876: Steps 14/138: batch_recall = 55.25, batch_ndcg = 32.29 
2025-04-08 20:28:01.807158: Steps 15/138: batch_recall = 47.49, batch_ndcg = 30.13 
2025-04-08 20:28:03.112379: Steps 16/138: batch_recall = 61.22, batch_ndcg = 34.43 
2025-04-08 20:28:04.392287: Steps 17/138: batch_recall = 60.03, batch_ndcg = 34.20 
2025-04-08 20:28:05.688048: Steps 18/138: batch_recall = 54.15, batch_ndcg = 33.57 
2025-04-08 20:28:06.997955: Steps 19/138: batch_recall = 54.80, batch_ndcg = 33.03 
2025-04-08 20:28:08.297629: Steps 20/138: batch_recall = 61.61, batch_ndcg = 35.80 
2025-04-08 20:28:09.600972: Steps 21/138: batch_recall = 68.51, batch_ndcg = 40.61 
2025-04-08 20:28:10.903252: Steps 22/138: batch_recall = 57.31, batch_ndcg = 32.77 
2025-04-08 20:28:12.210468: Steps 23/138: batch_recall = 51.26, batch_ndcg = 30.31 
2025-04-08 20:28:13.518740: Steps 24/138: batch_recall = 57.93, batch_ndcg = 31.47 
2025-04-08 20:28:14.824663: Steps 25/138: batch_recall = 61.81, batch_ndcg = 35.32 
2025-04-08 20:28:16.133087: Steps 26/138: batch_recall = 58.21, batch_ndcg = 33.40 
2025-04-08 20:28:17.422565: Steps 27/138: batch_recall = 56.27, batch_ndcg = 32.30 
2025-04-08 20:28:18.710984: Steps 28/138: batch_recall = 58.16, batch_ndcg = 32.98 
2025-04-08 20:28:19.994170: Steps 29/138: batch_recall = 58.26, batch_ndcg = 30.99 
2025-04-08 20:28:21.287294: Steps 30/138: batch_recall = 58.55, batch_ndcg = 33.94 
2025-04-08 20:28:22.577507: Steps 31/138: batch_recall = 48.23, batch_ndcg = 27.71 
2025-04-08 20:28:23.877896: Steps 32/138: batch_recall = 53.31, batch_ndcg = 31.45 
2025-04-08 20:28:25.183210: Steps 33/138: batch_recall = 58.39, batch_ndcg = 32.75 
2025-04-08 20:28:26.477832: Steps 34/138: batch_recall = 55.02, batch_ndcg = 29.96 
2025-04-08 20:28:27.783524: Steps 35/138: batch_recall = 49.65, batch_ndcg = 29.51 
2025-04-08 20:28:29.086921: Steps 36/138: batch_recall = 46.92, batch_ndcg = 26.68 
2025-04-08 20:28:30.382705: Steps 37/138: batch_recall = 58.62, batch_ndcg = 34.43 
2025-04-08 20:28:31.688835: Steps 38/138: batch_recall = 58.21, batch_ndcg = 32.47 
2025-04-08 20:28:32.979163: Steps 39/138: batch_recall = 64.69, batch_ndcg = 37.75 
2025-04-08 20:28:34.259391: Steps 40/138: batch_recall = 60.84, batch_ndcg = 30.75 
2025-04-08 20:28:35.539032: Steps 41/138: batch_recall = 60.30, batch_ndcg = 34.25 
2025-04-08 20:28:36.825139: Steps 42/138: batch_recall = 55.61, batch_ndcg = 30.59 
2025-04-08 20:28:38.117675: Steps 43/138: batch_recall = 57.24, batch_ndcg = 36.75 
2025-04-08 20:28:39.422034: Steps 44/138: batch_recall = 56.61, batch_ndcg = 30.94 
2025-04-08 20:28:40.735923: Steps 45/138: batch_recall = 63.26, batch_ndcg = 36.21 
2025-04-08 20:28:42.025755: Steps 46/138: batch_recall = 63.55, batch_ndcg = 36.99 
2025-04-08 20:28:43.329141: Steps 47/138: batch_recall = 55.68, batch_ndcg = 32.32 
2025-04-08 20:28:44.625521: Steps 48/138: batch_recall = 59.95, batch_ndcg = 35.76 
2025-04-08 20:28:45.918503: Steps 49/138: batch_recall = 63.62, batch_ndcg = 37.50 
2025-04-08 20:28:47.207426: Steps 50/138: batch_recall = 57.20, batch_ndcg = 30.41 
2025-04-08 20:28:48.501282: Steps 51/138: batch_recall = 64.26, batch_ndcg = 36.36 
2025-04-08 20:28:49.786517: Steps 52/138: batch_recall = 66.79, batch_ndcg = 41.83 
2025-04-08 20:28:51.078963: Steps 53/138: batch_recall = 64.68, batch_ndcg = 34.57 
2025-04-08 20:28:52.367111: Steps 54/138: batch_recall = 68.44, batch_ndcg = 39.18 
2025-04-08 20:28:53.649637: Steps 55/138: batch_recall = 60.87, batch_ndcg = 33.95 
2025-04-08 20:28:54.937252: Steps 56/138: batch_recall = 61.11, batch_ndcg = 35.33 
2025-04-08 20:28:56.232656: Steps 57/138: batch_recall = 56.91, batch_ndcg = 31.96 
2025-04-08 20:28:57.526099: Steps 58/138: batch_recall = 63.90, batch_ndcg = 34.82 
2025-04-08 20:28:58.820904: Steps 59/138: batch_recall = 62.97, batch_ndcg = 38.13 
2025-04-08 20:29:00.112551: Steps 60/138: batch_recall = 64.66, batch_ndcg = 37.06 
2025-04-08 20:29:01.403992: Steps 61/138: batch_recall = 64.73, batch_ndcg = 35.00 
2025-04-08 20:29:02.702237: Steps 62/138: batch_recall = 82.00, batch_ndcg = 44.72 
2025-04-08 20:29:04.001201: Steps 63/138: batch_recall = 76.96, batch_ndcg = 43.37 
2025-04-08 20:29:05.299766: Steps 64/138: batch_recall = 61.06, batch_ndcg = 32.23 
2025-04-08 20:29:06.581467: Steps 65/138: batch_recall = 84.49, batch_ndcg = 47.12 
2025-04-08 20:29:07.864312: Steps 66/138: batch_recall = 68.00, batch_ndcg = 40.58 
2025-04-08 20:29:09.170505: Steps 67/138: batch_recall = 76.36, batch_ndcg = 46.52 
2025-04-08 20:29:10.455852: Steps 68/138: batch_recall = 60.15, batch_ndcg = 33.57 
2025-04-08 20:29:11.748323: Steps 69/138: batch_recall = 88.10, batch_ndcg = 52.14 
2025-04-08 20:29:13.051601: Steps 70/138: batch_recall = 76.78, batch_ndcg = 44.43 
2025-04-08 20:29:14.350878: Steps 71/138: batch_recall = 84.87, batch_ndcg = 51.72 
2025-04-08 20:29:15.646976: Steps 72/138: batch_recall = 83.05, batch_ndcg = 48.48 
2025-04-08 20:29:16.953678: Steps 73/138: batch_recall = 84.40, batch_ndcg = 46.79 
2025-04-08 20:29:18.256912: Steps 74/138: batch_recall = 78.48, batch_ndcg = 48.20 
2025-04-08 20:29:19.545510: Steps 75/138: batch_recall = 81.51, batch_ndcg = 48.72 
2025-04-08 20:29:20.829819: Steps 76/138: batch_recall = 95.32, batch_ndcg = 55.27 
2025-04-08 20:29:22.110158: Steps 77/138: batch_recall = 90.51, batch_ndcg = 51.36 
2025-04-08 20:29:23.391721: Steps 78/138: batch_recall = 90.36, batch_ndcg = 47.95 
2025-04-08 20:29:24.671167: Steps 79/138: batch_recall = 87.05, batch_ndcg = 48.60 
2025-04-08 20:29:25.960190: Steps 80/138: batch_recall = 72.81, batch_ndcg = 39.59 
2025-04-08 20:29:27.259806: Steps 81/138: batch_recall = 81.59, batch_ndcg = 48.47 
2025-04-08 20:29:28.554816: Steps 82/138: batch_recall = 87.51, batch_ndcg = 53.63 
2025-04-08 20:29:29.841167: Steps 83/138: batch_recall = 79.42, batch_ndcg = 47.27 
2025-04-08 20:29:31.124438: Steps 84/138: batch_recall = 98.02, batch_ndcg = 56.72 
2025-04-08 20:29:32.411021: Steps 85/138: batch_recall = 105.95, batch_ndcg = 61.32 
2025-04-08 20:29:33.694581: Steps 86/138: batch_recall = 118.57, batch_ndcg = 70.85 
2025-04-08 20:29:34.974501: Steps 87/138: batch_recall = 103.91, batch_ndcg = 54.82 
2025-04-08 20:29:36.262465: Steps 88/138: batch_recall = 100.90, batch_ndcg = 59.21 
2025-04-08 20:29:37.534915: Steps 89/138: batch_recall = 119.80, batch_ndcg = 67.52 
2025-04-08 20:29:38.820970: Steps 90/138: batch_recall = 98.79, batch_ndcg = 56.36 
2025-04-08 20:29:40.096755: Steps 91/138: batch_recall = 113.35, batch_ndcg = 64.39 
2025-04-08 20:29:41.378067: Steps 92/138: batch_recall = 123.13, batch_ndcg = 66.46 
2025-04-08 20:29:42.654746: Steps 93/138: batch_recall = 119.89, batch_ndcg = 67.66 
2025-04-08 20:29:43.931171: Steps 94/138: batch_recall = 123.49, batch_ndcg = 66.63 
2025-04-08 20:29:45.219489: Steps 95/138: batch_recall = 113.65, batch_ndcg = 67.04 
2025-04-08 20:29:46.503717: Steps 96/138: batch_recall = 130.73, batch_ndcg = 78.10 
2025-04-08 20:29:47.785415: Steps 97/138: batch_recall = 151.96, batch_ndcg = 91.27 
2025-04-08 20:29:49.069118: Steps 98/138: batch_recall = 109.37, batch_ndcg = 63.47 
2025-04-08 20:29:50.354099: Steps 99/138: batch_recall = 125.06, batch_ndcg = 71.53 
2025-04-08 20:29:51.636141: Steps 100/138: batch_recall = 124.23, batch_ndcg = 69.81 
2025-04-08 20:29:52.913836: Steps 101/138: batch_recall = 125.59, batch_ndcg = 69.06 
2025-04-08 20:29:54.191967: Steps 102/138: batch_recall = 121.30, batch_ndcg = 69.57 
2025-04-08 20:29:55.464035: Steps 103/138: batch_recall = 140.87, batch_ndcg = 79.91 
2025-04-08 20:29:56.736096: Steps 104/138: batch_recall = 134.01, batch_ndcg = 78.02 
2025-04-08 20:29:58.012574: Steps 105/138: batch_recall = 120.99, batch_ndcg = 67.99 
2025-04-08 20:29:59.295865: Steps 106/138: batch_recall = 106.40, batch_ndcg = 61.12 
2025-04-08 20:30:00.582071: Steps 107/138: batch_recall = 118.36, batch_ndcg = 65.21 
2025-04-08 20:30:01.874239: Steps 108/138: batch_recall = 118.94, batch_ndcg = 71.87 
2025-04-08 20:30:03.165225: Steps 109/138: batch_recall = 137.58, batch_ndcg = 76.35 
2025-04-08 20:30:04.462949: Steps 110/138: batch_recall = 122.37, batch_ndcg = 64.45 
2025-04-08 20:30:05.745926: Steps 111/138: batch_recall = 136.89, batch_ndcg = 84.61 
2025-04-08 20:30:07.034955: Steps 112/138: batch_recall = 165.80, batch_ndcg = 92.73 
2025-04-08 20:30:08.319120: Steps 113/138: batch_recall = 121.16, batch_ndcg = 68.89 
2025-04-08 20:30:09.591043: Steps 114/138: batch_recall = 122.45, batch_ndcg = 72.07 
2025-04-08 20:30:10.878989: Steps 115/138: batch_recall = 121.01, batch_ndcg = 64.64 
2025-04-08 20:30:12.156122: Steps 116/138: batch_recall = 124.83, batch_ndcg = 67.52 
2025-04-08 20:30:13.430303: Steps 117/138: batch_recall = 108.98, batch_ndcg = 64.94 
2025-04-08 20:30:14.708758: Steps 118/138: batch_recall = 120.12, batch_ndcg = 68.46 
2025-04-08 20:30:15.994904: Steps 119/138: batch_recall = 142.32, batch_ndcg = 77.49 
2025-04-08 20:30:17.287283: Steps 120/138: batch_recall = 123.39, batch_ndcg = 70.69 
2025-04-08 20:30:18.567662: Steps 121/138: batch_recall = 149.44, batch_ndcg = 79.05 
2025-04-08 20:30:19.846857: Steps 122/138: batch_recall = 149.83, batch_ndcg = 80.85 
2025-04-08 20:30:21.122697: Steps 123/138: batch_recall = 128.67, batch_ndcg = 74.80 
2025-04-08 20:30:22.411161: Steps 124/138: batch_recall = 152.95, batch_ndcg = 92.47 
2025-04-08 20:30:23.701008: Steps 125/138: batch_recall = 135.05, batch_ndcg = 74.89 
2025-04-08 20:30:24.970847: Steps 126/138: batch_recall = 156.31, batch_ndcg = 86.52 
2025-04-08 20:30:26.236199: Steps 127/138: batch_recall = 144.31, batch_ndcg = 82.27 
2025-04-08 20:30:27.509868: Steps 128/138: batch_recall = 122.52, batch_ndcg = 69.69 
2025-04-08 20:30:28.778526: Steps 129/138: batch_recall = 159.75, batch_ndcg = 91.97 
2025-04-08 20:30:30.045710: Steps 130/138: batch_recall = 135.27, batch_ndcg = 70.35 
2025-04-08 20:30:31.321712: Steps 131/138: batch_recall = 148.84, batch_ndcg = 87.44 
2025-04-08 20:30:32.601302: Steps 132/138: batch_recall = 147.32, batch_ndcg = 82.94 
2025-04-08 20:30:33.883028: Steps 133/138: batch_recall = 144.02, batch_ndcg = 84.66 
2025-04-08 20:30:35.161650: Steps 134/138: batch_recall = 141.88, batch_ndcg = 83.89 
2025-04-08 20:30:36.450648: Steps 135/138: batch_recall = 165.69, batch_ndcg = 95.10 
2025-04-08 20:30:37.729812: Steps 136/138: batch_recall = 153.19, batch_ndcg = 79.35 
2025-04-08 20:30:39.004175: Steps 137/138: batch_recall = 136.59, batch_ndcg = 86.86 
2025-04-08 20:30:39.004705: Epoch 37/1000, Test: Recall = 0.1752, NDCG = 0.1003  

2025-04-08 20:30:40.783709: Training Step 0/354: batchLoss = 0.4483, diffLoss = 2.1924, kgLoss = 0.0123
2025-04-08 20:30:42.406315: Training Step 1/354: batchLoss = 0.5460, diffLoss = 2.6747, kgLoss = 0.0139
2025-04-08 20:30:44.028914: Training Step 2/354: batchLoss = 0.4573, diffLoss = 2.2370, kgLoss = 0.0124
2025-04-08 20:30:45.651818: Training Step 3/354: batchLoss = 0.5164, diffLoss = 2.5329, kgLoss = 0.0123
2025-04-08 20:30:47.279069: Training Step 4/354: batchLoss = 0.6028, diffLoss = 2.9552, kgLoss = 0.0147
2025-04-08 20:30:48.900186: Training Step 5/354: batchLoss = 0.6786, diffLoss = 3.3207, kgLoss = 0.0180
2025-04-08 20:30:50.532383: Training Step 6/354: batchLoss = 0.5201, diffLoss = 2.5450, kgLoss = 0.0139
2025-04-08 20:30:52.152968: Training Step 7/354: batchLoss = 0.5383, diffLoss = 2.6372, kgLoss = 0.0136
2025-04-08 20:30:53.781101: Training Step 8/354: batchLoss = 0.6070, diffLoss = 2.9756, kgLoss = 0.0148
2025-04-08 20:30:55.413709: Training Step 9/354: batchLoss = 0.7041, diffLoss = 3.4544, kgLoss = 0.0165
2025-04-08 20:30:57.037714: Training Step 10/354: batchLoss = 0.5723, diffLoss = 2.8059, kgLoss = 0.0139
2025-04-08 20:30:58.664290: Training Step 11/354: batchLoss = 0.5354, diffLoss = 2.6125, kgLoss = 0.0161
2025-04-08 20:31:00.285120: Training Step 12/354: batchLoss = 0.4905, diffLoss = 2.4008, kgLoss = 0.0129
2025-04-08 20:31:01.906596: Training Step 13/354: batchLoss = 0.6272, diffLoss = 3.0770, kgLoss = 0.0147
2025-04-08 20:31:03.541738: Training Step 14/354: batchLoss = 0.6920, diffLoss = 3.3849, kgLoss = 0.0188
2025-04-08 20:31:05.177202: Training Step 15/354: batchLoss = 0.5187, diffLoss = 2.5446, kgLoss = 0.0122
2025-04-08 20:31:06.810937: Training Step 16/354: batchLoss = 0.5316, diffLoss = 2.6042, kgLoss = 0.0135
2025-04-08 20:31:08.444324: Training Step 17/354: batchLoss = 0.4678, diffLoss = 2.2887, kgLoss = 0.0125
2025-04-08 20:31:10.072598: Training Step 18/354: batchLoss = 0.5210, diffLoss = 2.5489, kgLoss = 0.0140
2025-04-08 20:31:11.701047: Training Step 19/354: batchLoss = 0.4953, diffLoss = 2.4234, kgLoss = 0.0132
2025-04-08 20:31:13.330752: Training Step 20/354: batchLoss = 0.5323, diffLoss = 2.6091, kgLoss = 0.0131
2025-04-08 20:31:14.952356: Training Step 21/354: batchLoss = 0.5799, diffLoss = 2.8436, kgLoss = 0.0140
2025-04-08 20:31:16.569764: Training Step 22/354: batchLoss = 0.5259, diffLoss = 2.5791, kgLoss = 0.0126
2025-04-08 20:31:18.193183: Training Step 23/354: batchLoss = 0.4416, diffLoss = 2.1524, kgLoss = 0.0139
2025-04-08 20:31:19.833057: Training Step 24/354: batchLoss = 0.6802, diffLoss = 3.3320, kgLoss = 0.0173
2025-04-08 20:31:21.465250: Training Step 25/354: batchLoss = 0.4672, diffLoss = 2.2868, kgLoss = 0.0123
2025-04-08 20:31:23.091447: Training Step 26/354: batchLoss = 0.5410, diffLoss = 2.6494, kgLoss = 0.0139
2025-04-08 20:31:24.716682: Training Step 27/354: batchLoss = 0.4654, diffLoss = 2.2786, kgLoss = 0.0121
2025-04-08 20:31:26.357109: Training Step 28/354: batchLoss = 0.6505, diffLoss = 3.1857, kgLoss = 0.0168
2025-04-08 20:31:27.987707: Training Step 29/354: batchLoss = 0.5963, diffLoss = 2.9179, kgLoss = 0.0159
2025-04-08 20:31:29.609782: Training Step 30/354: batchLoss = 0.6461, diffLoss = 3.1684, kgLoss = 0.0156
2025-04-08 20:31:31.223505: Training Step 31/354: batchLoss = 0.5863, diffLoss = 2.8723, kgLoss = 0.0147
2025-04-08 20:31:32.843558: Training Step 32/354: batchLoss = 0.4738, diffLoss = 2.3152, kgLoss = 0.0134
2025-04-08 20:31:34.462045: Training Step 33/354: batchLoss = 0.5969, diffLoss = 2.9188, kgLoss = 0.0164
2025-04-08 20:31:36.090350: Training Step 34/354: batchLoss = 0.5564, diffLoss = 2.7295, kgLoss = 0.0131
2025-04-08 20:31:37.719420: Training Step 35/354: batchLoss = 0.6362, diffLoss = 3.1176, kgLoss = 0.0158
2025-04-08 20:31:39.350360: Training Step 36/354: batchLoss = 0.5352, diffLoss = 2.6224, kgLoss = 0.0134
2025-04-08 20:31:40.978829: Training Step 37/354: batchLoss = 0.5347, diffLoss = 2.6170, kgLoss = 0.0141
2025-04-08 20:31:42.613522: Training Step 38/354: batchLoss = 0.6218, diffLoss = 3.0468, kgLoss = 0.0155
2025-04-08 20:31:44.246373: Training Step 39/354: batchLoss = 0.6071, diffLoss = 2.9795, kgLoss = 0.0139
2025-04-08 20:31:45.865519: Training Step 40/354: batchLoss = 0.5864, diffLoss = 2.8596, kgLoss = 0.0181
2025-04-08 20:31:47.484207: Training Step 41/354: batchLoss = 0.4474, diffLoss = 2.1921, kgLoss = 0.0113
2025-04-08 20:31:49.105130: Training Step 42/354: batchLoss = 0.4771, diffLoss = 2.3339, kgLoss = 0.0129
2025-04-08 20:31:50.724423: Training Step 43/354: batchLoss = 0.5206, diffLoss = 2.5555, kgLoss = 0.0119
2025-04-08 20:31:52.351863: Training Step 44/354: batchLoss = 0.4876, diffLoss = 2.3859, kgLoss = 0.0131
2025-04-08 20:31:53.984609: Training Step 45/354: batchLoss = 0.4767, diffLoss = 2.3351, kgLoss = 0.0121
2025-04-08 20:31:55.614647: Training Step 46/354: batchLoss = 0.5021, diffLoss = 2.4562, kgLoss = 0.0135
2025-04-08 20:31:57.248384: Training Step 47/354: batchLoss = 0.4928, diffLoss = 2.4145, kgLoss = 0.0124
2025-04-08 20:31:58.879439: Training Step 48/354: batchLoss = 0.6340, diffLoss = 3.1017, kgLoss = 0.0171
2025-04-08 20:32:00.507159: Training Step 49/354: batchLoss = 0.5166, diffLoss = 2.5293, kgLoss = 0.0135
2025-04-08 20:32:02.135089: Training Step 50/354: batchLoss = 0.4859, diffLoss = 2.3793, kgLoss = 0.0125
2025-04-08 20:32:03.751649: Training Step 51/354: batchLoss = 0.5007, diffLoss = 2.4464, kgLoss = 0.0143
2025-04-08 20:32:05.373008: Training Step 52/354: batchLoss = 0.5817, diffLoss = 2.8510, kgLoss = 0.0144
2025-04-08 20:32:06.999318: Training Step 53/354: batchLoss = 0.4213, diffLoss = 2.0567, kgLoss = 0.0124
2025-04-08 20:32:08.629233: Training Step 54/354: batchLoss = 0.5768, diffLoss = 2.8352, kgLoss = 0.0122
2025-04-08 20:32:10.258351: Training Step 55/354: batchLoss = 0.5789, diffLoss = 2.8357, kgLoss = 0.0146
2025-04-08 20:32:11.883407: Training Step 56/354: batchLoss = 0.6058, diffLoss = 2.9665, kgLoss = 0.0156
2025-04-08 20:32:13.519801: Training Step 57/354: batchLoss = 0.5840, diffLoss = 2.8615, kgLoss = 0.0147
2025-04-08 20:32:15.153907: Training Step 58/354: batchLoss = 0.4848, diffLoss = 2.3683, kgLoss = 0.0139
2025-04-08 20:32:16.780662: Training Step 59/354: batchLoss = 0.5131, diffLoss = 2.5095, kgLoss = 0.0140
2025-04-08 20:32:18.399354: Training Step 60/354: batchLoss = 0.5496, diffLoss = 2.6862, kgLoss = 0.0154
2025-04-08 20:32:20.027212: Training Step 61/354: batchLoss = 0.5899, diffLoss = 2.8893, kgLoss = 0.0150
2025-04-08 20:32:21.650358: Training Step 62/354: batchLoss = 0.5332, diffLoss = 2.6146, kgLoss = 0.0129
2025-04-08 20:32:23.276023: Training Step 63/354: batchLoss = 0.5817, diffLoss = 2.8474, kgLoss = 0.0152
2025-04-08 20:32:24.904526: Training Step 64/354: batchLoss = 0.4244, diffLoss = 2.0742, kgLoss = 0.0119
2025-04-08 20:32:26.538215: Training Step 65/354: batchLoss = 0.5772, diffLoss = 2.8146, kgLoss = 0.0178
2025-04-08 20:32:28.172981: Training Step 66/354: batchLoss = 0.4884, diffLoss = 2.3944, kgLoss = 0.0119
2025-04-08 20:32:29.803157: Training Step 67/354: batchLoss = 0.5839, diffLoss = 2.8527, kgLoss = 0.0167
2025-04-08 20:32:31.430162: Training Step 68/354: batchLoss = 0.5012, diffLoss = 2.4516, kgLoss = 0.0136
2025-04-08 20:32:33.059855: Training Step 69/354: batchLoss = 0.7368, diffLoss = 3.5995, kgLoss = 0.0211
2025-04-08 20:32:34.687897: Training Step 70/354: batchLoss = 0.5928, diffLoss = 2.8970, kgLoss = 0.0168
2025-04-08 20:32:36.310157: Training Step 71/354: batchLoss = 0.6733, diffLoss = 3.3008, kgLoss = 0.0165
2025-04-08 20:32:37.932881: Training Step 72/354: batchLoss = 0.5329, diffLoss = 2.6013, kgLoss = 0.0158
2025-04-08 20:32:39.563663: Training Step 73/354: batchLoss = 0.6642, diffLoss = 3.2515, kgLoss = 0.0174
2025-04-08 20:32:41.197666: Training Step 74/354: batchLoss = 0.5176, diffLoss = 2.5253, kgLoss = 0.0157
2025-04-08 20:32:42.828213: Training Step 75/354: batchLoss = 0.4849, diffLoss = 2.3737, kgLoss = 0.0127
2025-04-08 20:32:44.458731: Training Step 76/354: batchLoss = 0.6909, diffLoss = 3.3833, kgLoss = 0.0178
2025-04-08 20:32:46.091190: Training Step 77/354: batchLoss = 0.4729, diffLoss = 2.3116, kgLoss = 0.0133
2025-04-08 20:32:47.720290: Training Step 78/354: batchLoss = 0.4117, diffLoss = 2.0118, kgLoss = 0.0117
2025-04-08 20:32:49.350757: Training Step 79/354: batchLoss = 0.4676, diffLoss = 2.2789, kgLoss = 0.0148
2025-04-08 20:32:50.972634: Training Step 80/354: batchLoss = 0.4859, diffLoss = 2.3831, kgLoss = 0.0117
2025-04-08 20:32:52.592551: Training Step 81/354: batchLoss = 0.4371, diffLoss = 2.1355, kgLoss = 0.0125
2025-04-08 20:32:54.220077: Training Step 82/354: batchLoss = 0.5241, diffLoss = 2.5625, kgLoss = 0.0145
2025-04-08 20:32:55.848148: Training Step 83/354: batchLoss = 0.5716, diffLoss = 2.7901, kgLoss = 0.0169
2025-04-08 20:32:57.476557: Training Step 84/354: batchLoss = 0.4843, diffLoss = 2.3747, kgLoss = 0.0117
2025-04-08 20:32:59.107542: Training Step 85/354: batchLoss = 0.6492, diffLoss = 3.1722, kgLoss = 0.0184
2025-04-08 20:33:00.742236: Training Step 86/354: batchLoss = 0.4955, diffLoss = 2.4279, kgLoss = 0.0124
2025-04-08 20:33:02.365923: Training Step 87/354: batchLoss = 0.6227, diffLoss = 3.0511, kgLoss = 0.0156
2025-04-08 20:33:03.996792: Training Step 88/354: batchLoss = 0.5050, diffLoss = 2.4744, kgLoss = 0.0126
2025-04-08 20:33:05.627457: Training Step 89/354: batchLoss = 0.4793, diffLoss = 2.3451, kgLoss = 0.0129
2025-04-08 20:33:07.242975: Training Step 90/354: batchLoss = 0.4810, diffLoss = 2.3517, kgLoss = 0.0134
2025-04-08 20:33:08.867381: Training Step 91/354: batchLoss = 0.6426, diffLoss = 3.1504, kgLoss = 0.0157
2025-04-08 20:33:10.479831: Training Step 92/354: batchLoss = 0.6126, diffLoss = 3.0054, kgLoss = 0.0143
2025-04-08 20:33:12.111388: Training Step 93/354: batchLoss = 0.5305, diffLoss = 2.5996, kgLoss = 0.0133
2025-04-08 20:33:13.745200: Training Step 94/354: batchLoss = 0.5600, diffLoss = 2.7455, kgLoss = 0.0137
2025-04-08 20:33:15.373652: Training Step 95/354: batchLoss = 0.4799, diffLoss = 2.3532, kgLoss = 0.0116
2025-04-08 20:33:17.009352: Training Step 96/354: batchLoss = 0.4276, diffLoss = 2.0929, kgLoss = 0.0112
2025-04-08 20:33:18.646019: Training Step 97/354: batchLoss = 0.4913, diffLoss = 2.4022, kgLoss = 0.0136
2025-04-08 20:33:20.278139: Training Step 98/354: batchLoss = 0.4542, diffLoss = 2.2236, kgLoss = 0.0119
2025-04-08 20:33:21.901040: Training Step 99/354: batchLoss = 0.5457, diffLoss = 2.6709, kgLoss = 0.0144
2025-04-08 20:33:23.521957: Training Step 100/354: batchLoss = 0.4837, diffLoss = 2.3660, kgLoss = 0.0131
2025-04-08 20:33:25.142030: Training Step 101/354: batchLoss = 0.5182, diffLoss = 2.5295, kgLoss = 0.0154
2025-04-08 20:33:26.762408: Training Step 102/354: batchLoss = 0.5270, diffLoss = 2.5811, kgLoss = 0.0135
2025-04-08 20:33:28.382323: Training Step 103/354: batchLoss = 0.6839, diffLoss = 3.3483, kgLoss = 0.0178
2025-04-08 20:33:30.009930: Training Step 104/354: batchLoss = 0.5141, diffLoss = 2.5099, kgLoss = 0.0151
2025-04-08 20:33:31.639739: Training Step 105/354: batchLoss = 0.6707, diffLoss = 3.2808, kgLoss = 0.0181
2025-04-08 20:33:33.269479: Training Step 106/354: batchLoss = 0.5342, diffLoss = 2.6196, kgLoss = 0.0128
2025-04-08 20:33:34.901037: Training Step 107/354: batchLoss = 0.6302, diffLoss = 3.0838, kgLoss = 0.0167
2025-04-08 20:33:36.526828: Training Step 108/354: batchLoss = 0.6135, diffLoss = 3.0019, kgLoss = 0.0164
2025-04-08 20:33:38.150043: Training Step 109/354: batchLoss = 0.6038, diffLoss = 2.9587, kgLoss = 0.0151
2025-04-08 20:33:39.769049: Training Step 110/354: batchLoss = 0.4511, diffLoss = 2.2033, kgLoss = 0.0131
2025-04-08 20:33:41.394691: Training Step 111/354: batchLoss = 0.5769, diffLoss = 2.8245, kgLoss = 0.0150
2025-04-08 20:33:43.021613: Training Step 112/354: batchLoss = 0.6461, diffLoss = 3.1623, kgLoss = 0.0170
2025-04-08 20:33:44.648746: Training Step 113/354: batchLoss = 0.4763, diffLoss = 2.3280, kgLoss = 0.0134
2025-04-08 20:33:46.348741: Training Step 114/354: batchLoss = 0.5775, diffLoss = 2.8156, kgLoss = 0.0179
2025-04-08 20:33:47.977337: Training Step 115/354: batchLoss = 0.5212, diffLoss = 2.5401, kgLoss = 0.0164
2025-04-08 20:33:49.610490: Training Step 116/354: batchLoss = 0.5415, diffLoss = 2.6503, kgLoss = 0.0143
2025-04-08 20:33:51.235103: Training Step 117/354: batchLoss = 0.5970, diffLoss = 2.9244, kgLoss = 0.0152
2025-04-08 20:33:52.863842: Training Step 118/354: batchLoss = 0.5851, diffLoss = 2.8574, kgLoss = 0.0170
2025-04-08 20:33:54.481855: Training Step 119/354: batchLoss = 0.5847, diffLoss = 2.8574, kgLoss = 0.0165
2025-04-08 20:33:56.101009: Training Step 120/354: batchLoss = 0.5130, diffLoss = 2.5115, kgLoss = 0.0134
2025-04-08 20:33:57.723563: Training Step 121/354: batchLoss = 0.4962, diffLoss = 2.4314, kgLoss = 0.0124
2025-04-08 20:33:59.346908: Training Step 122/354: batchLoss = 0.5036, diffLoss = 2.4687, kgLoss = 0.0124
2025-04-08 20:34:00.979585: Training Step 123/354: batchLoss = 0.5053, diffLoss = 2.4756, kgLoss = 0.0128
2025-04-08 20:34:02.607448: Training Step 124/354: batchLoss = 0.6854, diffLoss = 3.3586, kgLoss = 0.0171
2025-04-08 20:34:04.233468: Training Step 125/354: batchLoss = 0.4455, diffLoss = 2.1728, kgLoss = 0.0137
2025-04-08 20:34:05.861151: Training Step 126/354: batchLoss = 0.4898, diffLoss = 2.3958, kgLoss = 0.0133
2025-04-08 20:34:07.493393: Training Step 127/354: batchLoss = 0.5328, diffLoss = 2.6040, kgLoss = 0.0150
2025-04-08 20:34:09.121712: Training Step 128/354: batchLoss = 0.5811, diffLoss = 2.8453, kgLoss = 0.0150
2025-04-08 20:34:10.743323: Training Step 129/354: batchLoss = 0.5907, diffLoss = 2.8985, kgLoss = 0.0137
2025-04-08 20:34:12.363057: Training Step 130/354: batchLoss = 0.4774, diffLoss = 2.3332, kgLoss = 0.0134
2025-04-08 20:34:13.981055: Training Step 131/354: batchLoss = 0.4758, diffLoss = 2.3296, kgLoss = 0.0123
2025-04-08 20:34:15.610319: Training Step 132/354: batchLoss = 0.5108, diffLoss = 2.5051, kgLoss = 0.0122
2025-04-08 20:34:17.246558: Training Step 133/354: batchLoss = 0.5503, diffLoss = 2.6896, kgLoss = 0.0155
2025-04-08 20:34:18.876273: Training Step 134/354: batchLoss = 0.4764, diffLoss = 2.3331, kgLoss = 0.0122
2025-04-08 20:34:20.509631: Training Step 135/354: batchLoss = 0.5764, diffLoss = 2.8245, kgLoss = 0.0144
2025-04-08 20:34:22.135139: Training Step 136/354: batchLoss = 0.5298, diffLoss = 2.5916, kgLoss = 0.0144
2025-04-08 20:34:23.780601: Training Step 137/354: batchLoss = 0.4877, diffLoss = 2.3844, kgLoss = 0.0135
2025-04-08 20:34:25.403273: Training Step 138/354: batchLoss = 0.5739, diffLoss = 2.8073, kgLoss = 0.0156
2025-04-08 20:34:27.021252: Training Step 139/354: batchLoss = 0.5738, diffLoss = 2.8033, kgLoss = 0.0164
2025-04-08 20:34:28.645902: Training Step 140/354: batchLoss = 0.5602, diffLoss = 2.7320, kgLoss = 0.0173
2025-04-08 20:34:30.269025: Training Step 141/354: batchLoss = 0.5288, diffLoss = 2.5818, kgLoss = 0.0156
2025-04-08 20:34:31.899541: Training Step 142/354: batchLoss = 0.5701, diffLoss = 2.7876, kgLoss = 0.0157
2025-04-08 20:34:33.527334: Training Step 143/354: batchLoss = 0.5525, diffLoss = 2.7007, kgLoss = 0.0154
2025-04-08 20:34:35.151810: Training Step 144/354: batchLoss = 0.5288, diffLoss = 2.5866, kgLoss = 0.0143
2025-04-08 20:34:36.783012: Training Step 145/354: batchLoss = 0.4285, diffLoss = 2.0937, kgLoss = 0.0122
2025-04-08 20:34:38.416162: Training Step 146/354: batchLoss = 0.5438, diffLoss = 2.6230, kgLoss = 0.0240
2025-04-08 20:34:40.048818: Training Step 147/354: batchLoss = 0.5068, diffLoss = 2.4764, kgLoss = 0.0143
2025-04-08 20:34:41.673342: Training Step 148/354: batchLoss = 0.5658, diffLoss = 2.7714, kgLoss = 0.0144
2025-04-08 20:34:43.298753: Training Step 149/354: batchLoss = 0.6811, diffLoss = 3.3335, kgLoss = 0.0180
2025-04-08 20:34:44.921520: Training Step 150/354: batchLoss = 0.6076, diffLoss = 2.9812, kgLoss = 0.0142
2025-04-08 20:34:46.544614: Training Step 151/354: batchLoss = 0.5016, diffLoss = 2.4553, kgLoss = 0.0131
2025-04-08 20:34:48.176331: Training Step 152/354: batchLoss = 0.5581, diffLoss = 2.7374, kgLoss = 0.0133
2025-04-08 20:34:49.807048: Training Step 153/354: batchLoss = 0.5392, diffLoss = 2.6377, kgLoss = 0.0145
2025-04-08 20:34:51.439658: Training Step 154/354: batchLoss = 0.4941, diffLoss = 2.4205, kgLoss = 0.0125
2025-04-08 20:34:53.071757: Training Step 155/354: batchLoss = 0.5037, diffLoss = 2.4708, kgLoss = 0.0119
2025-04-08 20:34:54.717312: Training Step 156/354: batchLoss = 0.5012, diffLoss = 2.4570, kgLoss = 0.0123
2025-04-08 20:34:56.349404: Training Step 157/354: batchLoss = 0.5409, diffLoss = 2.6491, kgLoss = 0.0139
2025-04-08 20:34:57.961001: Training Step 158/354: batchLoss = 0.4599, diffLoss = 2.2464, kgLoss = 0.0133
2025-04-08 20:34:59.580613: Training Step 159/354: batchLoss = 0.6700, diffLoss = 3.2787, kgLoss = 0.0178
2025-04-08 20:35:01.201382: Training Step 160/354: batchLoss = 0.4504, diffLoss = 2.2031, kgLoss = 0.0122
2025-04-08 20:35:02.833348: Training Step 161/354: batchLoss = 0.5540, diffLoss = 2.7126, kgLoss = 0.0143
2025-04-08 20:35:04.464475: Training Step 162/354: batchLoss = 0.4876, diffLoss = 2.3902, kgLoss = 0.0119
2025-04-08 20:35:06.096951: Training Step 163/354: batchLoss = 0.5230, diffLoss = 2.5509, kgLoss = 0.0160
2025-04-08 20:35:07.723371: Training Step 164/354: batchLoss = 0.5494, diffLoss = 2.6856, kgLoss = 0.0153
2025-04-08 20:35:09.353652: Training Step 165/354: batchLoss = 0.5464, diffLoss = 2.6704, kgLoss = 0.0154
2025-04-08 20:35:10.982348: Training Step 166/354: batchLoss = 0.5172, diffLoss = 2.5287, kgLoss = 0.0143
2025-04-08 20:35:12.610180: Training Step 167/354: batchLoss = 0.4611, diffLoss = 2.2562, kgLoss = 0.0123
2025-04-08 20:35:14.233364: Training Step 168/354: batchLoss = 0.5858, diffLoss = 2.8718, kgLoss = 0.0144
2025-04-08 20:35:15.858430: Training Step 169/354: batchLoss = 0.5004, diffLoss = 2.4520, kgLoss = 0.0125
2025-04-08 20:35:17.486449: Training Step 170/354: batchLoss = 0.5752, diffLoss = 2.8139, kgLoss = 0.0155
2025-04-08 20:35:19.103803: Training Step 171/354: batchLoss = 0.4605, diffLoss = 2.2532, kgLoss = 0.0124
2025-04-08 20:35:20.732646: Training Step 172/354: batchLoss = 0.4641, diffLoss = 2.2692, kgLoss = 0.0128
2025-04-08 20:35:22.352780: Training Step 173/354: batchLoss = 0.5114, diffLoss = 2.4893, kgLoss = 0.0169
2025-04-08 20:35:23.980572: Training Step 174/354: batchLoss = 0.4973, diffLoss = 2.4376, kgLoss = 0.0122
2025-04-08 20:35:25.609145: Training Step 175/354: batchLoss = 0.5037, diffLoss = 2.4657, kgLoss = 0.0132
2025-04-08 20:35:27.236529: Training Step 176/354: batchLoss = 0.5199, diffLoss = 2.5468, kgLoss = 0.0132
2025-04-08 20:35:28.863251: Training Step 177/354: batchLoss = 0.5514, diffLoss = 2.7038, kgLoss = 0.0133
2025-04-08 20:35:30.489759: Training Step 178/354: batchLoss = 0.6237, diffLoss = 3.0508, kgLoss = 0.0169
2025-04-08 20:35:32.108274: Training Step 179/354: batchLoss = 0.5278, diffLoss = 2.5835, kgLoss = 0.0138
2025-04-08 20:35:33.731003: Training Step 180/354: batchLoss = 0.6125, diffLoss = 2.9895, kgLoss = 0.0183
2025-04-08 20:35:35.356260: Training Step 181/354: batchLoss = 0.6043, diffLoss = 2.9582, kgLoss = 0.0159
2025-04-08 20:35:36.992963: Training Step 182/354: batchLoss = 0.5161, diffLoss = 2.5308, kgLoss = 0.0124
2025-04-08 20:35:38.629518: Training Step 183/354: batchLoss = 0.4241, diffLoss = 2.0724, kgLoss = 0.0120
2025-04-08 20:35:40.255946: Training Step 184/354: batchLoss = 0.5841, diffLoss = 2.8538, kgLoss = 0.0167
2025-04-08 20:35:41.886285: Training Step 185/354: batchLoss = 0.8227, diffLoss = 4.0234, kgLoss = 0.0225
2025-04-08 20:35:43.512586: Training Step 186/354: batchLoss = 0.5755, diffLoss = 2.8217, kgLoss = 0.0139
2025-04-08 20:35:45.138887: Training Step 187/354: batchLoss = 0.6373, diffLoss = 3.1271, kgLoss = 0.0148
2025-04-08 20:35:46.761499: Training Step 188/354: batchLoss = 0.5835, diffLoss = 2.8608, kgLoss = 0.0142
2025-04-08 20:35:48.386330: Training Step 189/354: batchLoss = 0.5902, diffLoss = 2.8930, kgLoss = 0.0145
2025-04-08 20:35:50.010334: Training Step 190/354: batchLoss = 0.5335, diffLoss = 2.6077, kgLoss = 0.0149
2025-04-08 20:35:51.634063: Training Step 191/354: batchLoss = 0.4520, diffLoss = 2.2000, kgLoss = 0.0150
2025-04-08 20:35:53.271736: Training Step 192/354: batchLoss = 0.5638, diffLoss = 2.7648, kgLoss = 0.0136
2025-04-08 20:35:54.908525: Training Step 193/354: batchLoss = 0.5211, diffLoss = 2.5492, kgLoss = 0.0141
2025-04-08 20:35:56.535636: Training Step 194/354: batchLoss = 0.5639, diffLoss = 2.7601, kgLoss = 0.0149
2025-04-08 20:35:58.166885: Training Step 195/354: batchLoss = 0.4346, diffLoss = 2.1182, kgLoss = 0.0137
2025-04-08 20:35:59.796307: Training Step 196/354: batchLoss = 0.6655, diffLoss = 3.2597, kgLoss = 0.0169
2025-04-08 20:36:01.423777: Training Step 197/354: batchLoss = 0.4836, diffLoss = 2.3670, kgLoss = 0.0127
2025-04-08 20:36:03.044161: Training Step 198/354: batchLoss = 0.5347, diffLoss = 2.6210, kgLoss = 0.0131
2025-04-08 20:36:04.667801: Training Step 199/354: batchLoss = 0.5965, diffLoss = 2.9216, kgLoss = 0.0153
2025-04-08 20:36:06.296840: Training Step 200/354: batchLoss = 0.4807, diffLoss = 2.3528, kgLoss = 0.0127
2025-04-08 20:36:07.931573: Training Step 201/354: batchLoss = 0.7390, diffLoss = 3.6170, kgLoss = 0.0195
2025-04-08 20:36:09.561821: Training Step 202/354: batchLoss = 0.5511, diffLoss = 2.6993, kgLoss = 0.0141
2025-04-08 20:36:11.197827: Training Step 203/354: batchLoss = 0.5939, diffLoss = 2.9001, kgLoss = 0.0174
2025-04-08 20:36:12.827760: Training Step 204/354: batchLoss = 0.4802, diffLoss = 2.3481, kgLoss = 0.0132
2025-04-08 20:36:14.460298: Training Step 205/354: batchLoss = 0.4982, diffLoss = 2.4329, kgLoss = 0.0145
2025-04-08 20:36:16.092998: Training Step 206/354: batchLoss = 0.4948, diffLoss = 2.4214, kgLoss = 0.0131
2025-04-08 20:36:17.721476: Training Step 207/354: batchLoss = 0.5349, diffLoss = 2.6140, kgLoss = 0.0152
2025-04-08 20:36:19.339138: Training Step 208/354: batchLoss = 0.5515, diffLoss = 2.6946, kgLoss = 0.0157
2025-04-08 20:36:20.957275: Training Step 209/354: batchLoss = 0.8037, diffLoss = 3.9389, kgLoss = 0.0199
2025-04-08 20:36:22.578429: Training Step 210/354: batchLoss = 0.6168, diffLoss = 3.0224, kgLoss = 0.0154
2025-04-08 20:36:24.202033: Training Step 211/354: batchLoss = 0.7089, diffLoss = 3.4712, kgLoss = 0.0183
2025-04-08 20:36:25.832119: Training Step 212/354: batchLoss = 0.5983, diffLoss = 2.9302, kgLoss = 0.0154
2025-04-08 20:36:27.453423: Training Step 213/354: batchLoss = 0.5899, diffLoss = 2.8909, kgLoss = 0.0147
2025-04-08 20:36:29.082646: Training Step 214/354: batchLoss = 0.6291, diffLoss = 3.0849, kgLoss = 0.0151
2025-04-08 20:36:30.713721: Training Step 215/354: batchLoss = 0.5508, diffLoss = 2.6972, kgLoss = 0.0142
2025-04-08 20:36:32.344998: Training Step 216/354: batchLoss = 0.5764, diffLoss = 2.8238, kgLoss = 0.0146
2025-04-08 20:36:33.965250: Training Step 217/354: batchLoss = 0.5993, diffLoss = 2.9377, kgLoss = 0.0147
2025-04-08 20:36:35.575602: Training Step 218/354: batchLoss = 0.6546, diffLoss = 3.2047, kgLoss = 0.0170
2025-04-08 20:36:37.189763: Training Step 219/354: batchLoss = 0.5782, diffLoss = 2.8287, kgLoss = 0.0156
2025-04-08 20:36:38.805833: Training Step 220/354: batchLoss = 1.0169, diffLoss = 4.9849, kgLoss = 0.0249
2025-04-08 20:36:40.427028: Training Step 221/354: batchLoss = 0.6284, diffLoss = 3.0804, kgLoss = 0.0155
2025-04-08 20:36:42.046589: Training Step 222/354: batchLoss = 0.5858, diffLoss = 2.8659, kgLoss = 0.0158
2025-04-08 20:36:43.670329: Training Step 223/354: batchLoss = 0.6766, diffLoss = 3.3109, kgLoss = 0.0180
2025-04-08 20:36:45.290747: Training Step 224/354: batchLoss = 0.6751, diffLoss = 3.3084, kgLoss = 0.0168
2025-04-08 20:36:46.910240: Training Step 225/354: batchLoss = 0.5149, diffLoss = 2.5191, kgLoss = 0.0139
2025-04-08 20:36:48.534246: Training Step 226/354: batchLoss = 1.0010, diffLoss = 4.9070, kgLoss = 0.0245
2025-04-08 20:36:50.162853: Training Step 227/354: batchLoss = 0.5945, diffLoss = 2.9067, kgLoss = 0.0164
2025-04-08 20:36:51.776943: Training Step 228/354: batchLoss = 0.5148, diffLoss = 2.5193, kgLoss = 0.0136
2025-04-08 20:36:53.383374: Training Step 229/354: batchLoss = 0.5472, diffLoss = 2.6821, kgLoss = 0.0134
2025-04-08 20:36:55.004112: Training Step 230/354: batchLoss = 0.5012, diffLoss = 2.4594, kgLoss = 0.0117
2025-04-08 20:36:56.623327: Training Step 231/354: batchLoss = 0.5060, diffLoss = 2.4705, kgLoss = 0.0149
2025-04-08 20:36:58.254103: Training Step 232/354: batchLoss = 0.5030, diffLoss = 2.4588, kgLoss = 0.0140
2025-04-08 20:36:59.889099: Training Step 233/354: batchLoss = 0.5168, diffLoss = 2.5296, kgLoss = 0.0136
2025-04-08 20:37:01.527061: Training Step 234/354: batchLoss = 0.5906, diffLoss = 2.8845, kgLoss = 0.0171
2025-04-08 20:37:03.158613: Training Step 235/354: batchLoss = 0.5847, diffLoss = 2.8575, kgLoss = 0.0165
2025-04-08 20:37:04.789438: Training Step 236/354: batchLoss = 0.4477, diffLoss = 2.1885, kgLoss = 0.0126
2025-04-08 20:37:06.425054: Training Step 237/354: batchLoss = 0.5364, diffLoss = 2.6255, kgLoss = 0.0141
2025-04-08 20:37:08.039936: Training Step 238/354: batchLoss = 0.5258, diffLoss = 2.5689, kgLoss = 0.0150
2025-04-08 20:37:09.662900: Training Step 239/354: batchLoss = 0.4727, diffLoss = 2.3167, kgLoss = 0.0117
2025-04-08 20:37:11.285949: Training Step 240/354: batchLoss = 0.6224, diffLoss = 3.0478, kgLoss = 0.0160
2025-04-08 20:37:12.907255: Training Step 241/354: batchLoss = 0.8075, diffLoss = 3.9567, kgLoss = 0.0202
2025-04-08 20:37:14.536708: Training Step 242/354: batchLoss = 0.6236, diffLoss = 3.0536, kgLoss = 0.0161
2025-04-08 20:37:16.166295: Training Step 243/354: batchLoss = 0.4950, diffLoss = 2.4250, kgLoss = 0.0126
2025-04-08 20:37:17.801094: Training Step 244/354: batchLoss = 0.5496, diffLoss = 2.6837, kgLoss = 0.0161
2025-04-08 20:37:19.433533: Training Step 245/354: batchLoss = 0.6593, diffLoss = 3.2291, kgLoss = 0.0169
2025-04-08 20:37:21.069501: Training Step 246/354: batchLoss = 0.5799, diffLoss = 2.8425, kgLoss = 0.0143
2025-04-08 20:37:22.694872: Training Step 247/354: batchLoss = 0.5994, diffLoss = 2.9370, kgLoss = 0.0150
2025-04-08 20:37:24.317859: Training Step 248/354: batchLoss = 0.4799, diffLoss = 2.3440, kgLoss = 0.0139
2025-04-08 20:37:25.945290: Training Step 249/354: batchLoss = 0.5727, diffLoss = 2.8073, kgLoss = 0.0140
2025-04-08 20:37:27.561490: Training Step 250/354: batchLoss = 0.4969, diffLoss = 2.4276, kgLoss = 0.0142
2025-04-08 20:37:29.184829: Training Step 251/354: batchLoss = 0.5402, diffLoss = 2.6475, kgLoss = 0.0134
2025-04-08 20:37:30.814506: Training Step 252/354: batchLoss = 0.4511, diffLoss = 2.2065, kgLoss = 0.0122
2025-04-08 20:37:32.444022: Training Step 253/354: batchLoss = 0.5130, diffLoss = 2.5043, kgLoss = 0.0152
2025-04-08 20:37:34.071135: Training Step 254/354: batchLoss = 0.5345, diffLoss = 2.6143, kgLoss = 0.0146
2025-04-08 20:37:35.700194: Training Step 255/354: batchLoss = 0.5200, diffLoss = 2.5447, kgLoss = 0.0138
2025-04-08 20:37:37.333208: Training Step 256/354: batchLoss = 0.4724, diffLoss = 2.3057, kgLoss = 0.0140
2025-04-08 20:37:38.962316: Training Step 257/354: batchLoss = 0.6518, diffLoss = 3.1791, kgLoss = 0.0199
2025-04-08 20:37:40.583385: Training Step 258/354: batchLoss = 0.5461, diffLoss = 2.6731, kgLoss = 0.0144
2025-04-08 20:37:42.206804: Training Step 259/354: batchLoss = 0.5516, diffLoss = 2.7010, kgLoss = 0.0143
2025-04-08 20:37:43.830404: Training Step 260/354: batchLoss = 0.5121, diffLoss = 2.5070, kgLoss = 0.0133
2025-04-08 20:37:45.456703: Training Step 261/354: batchLoss = 0.5354, diffLoss = 2.6189, kgLoss = 0.0145
2025-04-08 20:37:47.080599: Training Step 262/354: batchLoss = 0.5000, diffLoss = 2.4465, kgLoss = 0.0134
2025-04-08 20:37:48.711845: Training Step 263/354: batchLoss = 0.5917, diffLoss = 2.8905, kgLoss = 0.0170
2025-04-08 20:37:50.338980: Training Step 264/354: batchLoss = 0.5641, diffLoss = 2.7604, kgLoss = 0.0150
2025-04-08 20:37:51.961789: Training Step 265/354: batchLoss = 0.5134, diffLoss = 2.5143, kgLoss = 0.0131
2025-04-08 20:37:53.587746: Training Step 266/354: batchLoss = 0.4971, diffLoss = 2.4429, kgLoss = 0.0107
2025-04-08 20:37:55.221419: Training Step 267/354: batchLoss = 0.5734, diffLoss = 2.8035, kgLoss = 0.0158
2025-04-08 20:37:56.837805: Training Step 268/354: batchLoss = 0.5911, diffLoss = 2.8974, kgLoss = 0.0145
2025-04-08 20:37:58.461620: Training Step 269/354: batchLoss = 2.0899, diffLoss = 10.2550, kgLoss = 0.0486
2025-04-08 20:38:00.080519: Training Step 270/354: batchLoss = 0.4757, diffLoss = 2.3278, kgLoss = 0.0126
2025-04-08 20:38:01.708771: Training Step 271/354: batchLoss = 0.8200, diffLoss = 4.0161, kgLoss = 0.0210
2025-04-08 20:38:03.342212: Training Step 272/354: batchLoss = 0.4942, diffLoss = 2.4099, kgLoss = 0.0153
2025-04-08 20:38:04.975278: Training Step 273/354: batchLoss = 0.7353, diffLoss = 3.5965, kgLoss = 0.0200
2025-04-08 20:38:06.605690: Training Step 274/354: batchLoss = 0.5224, diffLoss = 2.5647, kgLoss = 0.0118
2025-04-08 20:38:08.228942: Training Step 275/354: batchLoss = 0.5112, diffLoss = 2.5008, kgLoss = 0.0138
2025-04-08 20:38:09.866270: Training Step 276/354: batchLoss = 0.4561, diffLoss = 2.2302, kgLoss = 0.0126
2025-04-08 20:38:11.490211: Training Step 277/354: batchLoss = 0.4900, diffLoss = 2.3971, kgLoss = 0.0133
2025-04-08 20:38:13.111963: Training Step 278/354: batchLoss = 0.5520, diffLoss = 2.7026, kgLoss = 0.0143
2025-04-08 20:38:14.730459: Training Step 279/354: batchLoss = 0.6716, diffLoss = 3.2846, kgLoss = 0.0183
2025-04-08 20:38:16.351895: Training Step 280/354: batchLoss = 0.6034, diffLoss = 2.9592, kgLoss = 0.0145
2025-04-08 20:38:17.977861: Training Step 281/354: batchLoss = 0.6365, diffLoss = 3.1182, kgLoss = 0.0161
2025-04-08 20:38:19.610156: Training Step 282/354: batchLoss = 0.5856, diffLoss = 2.8659, kgLoss = 0.0155
2025-04-08 20:38:21.238864: Training Step 283/354: batchLoss = 0.5491, diffLoss = 2.6776, kgLoss = 0.0170
2025-04-08 20:38:22.872253: Training Step 284/354: batchLoss = 0.5764, diffLoss = 2.8126, kgLoss = 0.0174
2025-04-08 20:38:24.504726: Training Step 285/354: batchLoss = 0.4873, diffLoss = 2.3840, kgLoss = 0.0131
2025-04-08 20:38:26.130918: Training Step 286/354: batchLoss = 0.4454, diffLoss = 2.1787, kgLoss = 0.0120
2025-04-08 20:38:27.749195: Training Step 287/354: batchLoss = 0.4823, diffLoss = 2.3647, kgLoss = 0.0117
2025-04-08 20:38:29.364824: Training Step 288/354: batchLoss = 0.6782, diffLoss = 3.3198, kgLoss = 0.0178
2025-04-08 20:38:30.990798: Training Step 289/354: batchLoss = 0.6041, diffLoss = 2.9602, kgLoss = 0.0151
2025-04-08 20:38:32.617584: Training Step 290/354: batchLoss = 0.5445, diffLoss = 2.6683, kgLoss = 0.0135
2025-04-08 20:38:34.253983: Training Step 291/354: batchLoss = 0.6621, diffLoss = 3.2446, kgLoss = 0.0164
2025-04-08 20:38:35.885635: Training Step 292/354: batchLoss = 0.4385, diffLoss = 2.1473, kgLoss = 0.0114
2025-04-08 20:38:37.515922: Training Step 293/354: batchLoss = 0.5452, diffLoss = 2.6728, kgLoss = 0.0133
2025-04-08 20:38:39.147441: Training Step 294/354: batchLoss = 0.5409, diffLoss = 2.6483, kgLoss = 0.0141
2025-04-08 20:38:40.786797: Training Step 295/354: batchLoss = 0.7202, diffLoss = 3.5289, kgLoss = 0.0181
2025-04-08 20:38:42.415061: Training Step 296/354: batchLoss = 0.6756, diffLoss = 3.3059, kgLoss = 0.0180
2025-04-08 20:38:44.028183: Training Step 297/354: batchLoss = 0.5880, diffLoss = 2.8780, kgLoss = 0.0155
2025-04-08 20:38:45.639962: Training Step 298/354: batchLoss = 0.6241, diffLoss = 3.0543, kgLoss = 0.0165
2025-04-08 20:38:47.261354: Training Step 299/354: batchLoss = 0.5282, diffLoss = 2.5774, kgLoss = 0.0159
2025-04-08 20:38:48.884778: Training Step 300/354: batchLoss = 0.4277, diffLoss = 2.0928, kgLoss = 0.0115
2025-04-08 20:38:50.512173: Training Step 301/354: batchLoss = 0.5539, diffLoss = 2.7069, kgLoss = 0.0157
2025-04-08 20:38:52.148310: Training Step 302/354: batchLoss = 0.4830, diffLoss = 2.3696, kgLoss = 0.0113
2025-04-08 20:38:53.776353: Training Step 303/354: batchLoss = 0.5025, diffLoss = 2.4587, kgLoss = 0.0135
2025-04-08 20:38:55.406514: Training Step 304/354: batchLoss = 0.5888, diffLoss = 2.8848, kgLoss = 0.0148
2025-04-08 20:38:57.041445: Training Step 305/354: batchLoss = 0.5280, diffLoss = 2.5843, kgLoss = 0.0139
2025-04-08 20:38:58.671178: Training Step 306/354: batchLoss = 0.5729, diffLoss = 2.7965, kgLoss = 0.0170
2025-04-08 20:39:00.298558: Training Step 307/354: batchLoss = 0.5168, diffLoss = 2.5253, kgLoss = 0.0147
2025-04-08 20:39:01.921070: Training Step 308/354: batchLoss = 0.3802, diffLoss = 1.8566, kgLoss = 0.0111
2025-04-08 20:39:03.541233: Training Step 309/354: batchLoss = 0.6117, diffLoss = 2.9938, kgLoss = 0.0162
2025-04-08 20:39:05.168834: Training Step 310/354: batchLoss = 0.4817, diffLoss = 2.3490, kgLoss = 0.0148
2025-04-08 20:39:06.801818: Training Step 311/354: batchLoss = 0.5100, diffLoss = 2.4965, kgLoss = 0.0134
2025-04-08 20:39:08.436637: Training Step 312/354: batchLoss = 0.5482, diffLoss = 2.6866, kgLoss = 0.0136
2025-04-08 20:39:10.066015: Training Step 313/354: batchLoss = 0.6057, diffLoss = 2.9633, kgLoss = 0.0163
2025-04-08 20:39:11.702800: Training Step 314/354: batchLoss = 0.5635, diffLoss = 2.7594, kgLoss = 0.0145
2025-04-08 20:39:13.330344: Training Step 315/354: batchLoss = 0.4396, diffLoss = 2.1510, kgLoss = 0.0117
2025-04-08 20:39:14.963209: Training Step 316/354: batchLoss = 0.4356, diffLoss = 2.1269, kgLoss = 0.0127
2025-04-08 20:39:16.580281: Training Step 317/354: batchLoss = 0.6170, diffLoss = 3.0113, kgLoss = 0.0184
2025-04-08 20:39:18.206222: Training Step 318/354: batchLoss = 0.5082, diffLoss = 2.4798, kgLoss = 0.0153
2025-04-08 20:39:19.821000: Training Step 319/354: batchLoss = 0.5722, diffLoss = 2.7992, kgLoss = 0.0155
2025-04-08 20:39:21.450475: Training Step 320/354: batchLoss = 0.5196, diffLoss = 2.5477, kgLoss = 0.0126
2025-04-08 20:39:23.084238: Training Step 321/354: batchLoss = 0.5625, diffLoss = 2.7505, kgLoss = 0.0155
2025-04-08 20:39:24.712359: Training Step 322/354: batchLoss = 0.5268, diffLoss = 2.5749, kgLoss = 0.0147
2025-04-08 20:39:26.337242: Training Step 323/354: batchLoss = 0.5149, diffLoss = 2.5211, kgLoss = 0.0134
2025-04-08 20:39:27.969211: Training Step 324/354: batchLoss = 0.5335, diffLoss = 2.6107, kgLoss = 0.0142
2025-04-08 20:39:29.596619: Training Step 325/354: batchLoss = 0.5211, diffLoss = 2.5527, kgLoss = 0.0132
2025-04-08 20:39:31.219931: Training Step 326/354: batchLoss = 0.4572, diffLoss = 2.2361, kgLoss = 0.0125
2025-04-08 20:39:32.836007: Training Step 327/354: batchLoss = 0.5509, diffLoss = 2.6972, kgLoss = 0.0143
2025-04-08 20:39:34.456556: Training Step 328/354: batchLoss = 0.4712, diffLoss = 2.2999, kgLoss = 0.0140
2025-04-08 20:39:36.076418: Training Step 329/354: batchLoss = 0.5698, diffLoss = 2.7890, kgLoss = 0.0151
2025-04-08 20:39:37.706443: Training Step 330/354: batchLoss = 0.4529, diffLoss = 2.2174, kgLoss = 0.0118
2025-04-08 20:39:39.333502: Training Step 331/354: batchLoss = 0.6364, diffLoss = 3.1172, kgLoss = 0.0162
2025-04-08 20:39:40.962513: Training Step 332/354: batchLoss = 0.5229, diffLoss = 2.5587, kgLoss = 0.0139
2025-04-08 20:39:42.590915: Training Step 333/354: batchLoss = 0.5758, diffLoss = 2.8279, kgLoss = 0.0128
2025-04-08 20:39:44.215568: Training Step 334/354: batchLoss = 0.5274, diffLoss = 2.5841, kgLoss = 0.0132
2025-04-08 20:39:45.836732: Training Step 335/354: batchLoss = 0.6807, diffLoss = 3.3329, kgLoss = 0.0177
2025-04-08 20:39:47.449224: Training Step 336/354: batchLoss = 0.5391, diffLoss = 2.6400, kgLoss = 0.0139
2025-04-08 20:39:49.071580: Training Step 337/354: batchLoss = 0.4789, diffLoss = 2.3421, kgLoss = 0.0130
2025-04-08 20:39:50.688847: Training Step 338/354: batchLoss = 0.5880, diffLoss = 2.8829, kgLoss = 0.0143
2025-04-08 20:39:52.310141: Training Step 339/354: batchLoss = 0.5108, diffLoss = 2.5036, kgLoss = 0.0126
2025-04-08 20:39:53.936511: Training Step 340/354: batchLoss = 0.5931, diffLoss = 2.9032, kgLoss = 0.0156
2025-04-08 20:39:55.566501: Training Step 341/354: batchLoss = 0.5062, diffLoss = 2.4781, kgLoss = 0.0132
2025-04-08 20:39:57.203379: Training Step 342/354: batchLoss = 0.5744, diffLoss = 2.8028, kgLoss = 0.0173
2025-04-08 20:39:58.833776: Training Step 343/354: batchLoss = 0.5171, diffLoss = 2.5395, kgLoss = 0.0115
2025-04-08 20:40:00.459336: Training Step 344/354: batchLoss = 0.4504, diffLoss = 2.2020, kgLoss = 0.0125
2025-04-08 20:40:02.085074: Training Step 345/354: batchLoss = 0.5187, diffLoss = 2.5387, kgLoss = 0.0137
2025-04-08 20:40:03.706545: Training Step 346/354: batchLoss = 0.6423, diffLoss = 3.1428, kgLoss = 0.0172
2025-04-08 20:40:05.317905: Training Step 347/354: batchLoss = 0.6079, diffLoss = 2.9785, kgLoss = 0.0152
2025-04-08 20:40:06.940340: Training Step 348/354: batchLoss = 0.6181, diffLoss = 3.0219, kgLoss = 0.0171
2025-04-08 20:40:08.553900: Training Step 349/354: batchLoss = 0.5751, diffLoss = 2.8147, kgLoss = 0.0152
2025-04-08 20:40:10.176873: Training Step 350/354: batchLoss = 0.5684, diffLoss = 2.7875, kgLoss = 0.0137
2025-04-08 20:40:11.799857: Training Step 351/354: batchLoss = 0.5120, diffLoss = 2.5116, kgLoss = 0.0121
2025-04-08 20:40:13.409866: Training Step 352/354: batchLoss = 0.4958, diffLoss = 2.4328, kgLoss = 0.0116
2025-04-08 20:40:14.819126: Training Step 353/354: batchLoss = 0.7592, diffLoss = 3.7096, kgLoss = 0.0216
2025-04-08 20:40:14.912898: 
2025-04-08 20:40:14.913515: Epoch 38/1000, Train: epLoss = 0.9861, epDfLoss = 4.8260, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 20:40:16.219969: Steps 0/138: batch_recall = 47.16, batch_ndcg = 26.67 
2025-04-08 20:40:17.557971: Steps 1/138: batch_recall = 48.03, batch_ndcg = 28.36 
2025-04-08 20:40:18.887301: Steps 2/138: batch_recall = 59.27, batch_ndcg = 35.90 
2025-04-08 20:40:20.216672: Steps 3/138: batch_recall = 57.16, batch_ndcg = 33.29 
2025-04-08 20:40:21.525910: Steps 4/138: batch_recall = 66.48, batch_ndcg = 40.63 
2025-04-08 20:40:22.838962: Steps 5/138: batch_recall = 59.66, batch_ndcg = 31.82 
2025-04-08 20:40:24.155763: Steps 6/138: batch_recall = 51.88, batch_ndcg = 32.08 
2025-04-08 20:40:25.454241: Steps 7/138: batch_recall = 61.84, batch_ndcg = 41.60 
2025-04-08 20:40:26.762640: Steps 8/138: batch_recall = 62.77, batch_ndcg = 38.73 
2025-04-08 20:40:28.078113: Steps 9/138: batch_recall = 59.45, batch_ndcg = 34.77 
2025-04-08 20:40:29.388805: Steps 10/138: batch_recall = 57.34, batch_ndcg = 32.97 
2025-04-08 20:40:30.691658: Steps 11/138: batch_recall = 58.15, batch_ndcg = 33.65 
2025-04-08 20:40:32.001770: Steps 12/138: batch_recall = 49.40, batch_ndcg = 29.28 
2025-04-08 20:40:33.305270: Steps 13/138: batch_recall = 53.05, batch_ndcg = 31.90 
2025-04-08 20:40:34.605798: Steps 14/138: batch_recall = 55.91, batch_ndcg = 32.47 
2025-04-08 20:40:35.902864: Steps 15/138: batch_recall = 48.05, batch_ndcg = 30.18 
2025-04-08 20:40:37.195492: Steps 16/138: batch_recall = 61.38, batch_ndcg = 34.50 
2025-04-08 20:40:38.472942: Steps 17/138: batch_recall = 60.07, batch_ndcg = 34.23 
2025-04-08 20:40:39.765362: Steps 18/138: batch_recall = 54.57, batch_ndcg = 33.67 
2025-04-08 20:40:41.071767: Steps 19/138: batch_recall = 55.97, batch_ndcg = 33.28 
2025-04-08 20:40:42.373849: Steps 20/138: batch_recall = 59.41, batch_ndcg = 35.13 
2025-04-08 20:40:43.681267: Steps 21/138: batch_recall = 68.68, batch_ndcg = 40.48 
2025-04-08 20:40:44.986450: Steps 22/138: batch_recall = 57.31, batch_ndcg = 32.81 
2025-04-08 20:40:46.283388: Steps 23/138: batch_recall = 52.36, batch_ndcg = 30.74 
2025-04-08 20:40:47.580894: Steps 24/138: batch_recall = 57.89, batch_ndcg = 31.28 
2025-04-08 20:40:48.881749: Steps 25/138: batch_recall = 62.77, batch_ndcg = 35.65 
2025-04-08 20:40:50.182754: Steps 26/138: batch_recall = 59.08, batch_ndcg = 33.53 
2025-04-08 20:40:51.475752: Steps 27/138: batch_recall = 55.56, batch_ndcg = 32.03 
2025-04-08 20:40:52.758598: Steps 28/138: batch_recall = 56.82, batch_ndcg = 32.87 
2025-04-08 20:40:54.040860: Steps 29/138: batch_recall = 59.81, batch_ndcg = 31.42 
2025-04-08 20:40:55.341780: Steps 30/138: batch_recall = 59.93, batch_ndcg = 34.13 
2025-04-08 20:40:56.639447: Steps 31/138: batch_recall = 48.70, batch_ndcg = 27.59 
2025-04-08 20:40:57.930454: Steps 32/138: batch_recall = 54.44, batch_ndcg = 31.83 
2025-04-08 20:40:59.226507: Steps 33/138: batch_recall = 57.92, batch_ndcg = 32.71 
2025-04-08 20:41:00.542646: Steps 34/138: batch_recall = 55.81, batch_ndcg = 30.06 
2025-04-08 20:41:01.845874: Steps 35/138: batch_recall = 49.06, batch_ndcg = 29.21 
2025-04-08 20:41:03.137559: Steps 36/138: batch_recall = 47.03, batch_ndcg = 26.73 
2025-04-08 20:41:04.437855: Steps 37/138: batch_recall = 56.92, batch_ndcg = 33.89 
2025-04-08 20:41:05.732921: Steps 38/138: batch_recall = 59.71, batch_ndcg = 32.90 
2025-04-08 20:41:07.024310: Steps 39/138: batch_recall = 68.48, batch_ndcg = 38.84 
2025-04-08 20:41:08.315791: Steps 40/138: batch_recall = 58.90, batch_ndcg = 30.40 
2025-04-08 20:41:09.600020: Steps 41/138: batch_recall = 58.47, batch_ndcg = 33.96 
2025-04-08 20:41:10.877569: Steps 42/138: batch_recall = 53.94, batch_ndcg = 30.24 
2025-04-08 20:41:12.160997: Steps 43/138: batch_recall = 59.32, batch_ndcg = 37.16 
2025-04-08 20:41:13.450701: Steps 44/138: batch_recall = 56.11, batch_ndcg = 30.18 
2025-04-08 20:41:14.746569: Steps 45/138: batch_recall = 62.45, batch_ndcg = 35.99 
2025-04-08 20:41:16.037602: Steps 46/138: batch_recall = 62.88, batch_ndcg = 36.56 
2025-04-08 20:41:17.328038: Steps 47/138: batch_recall = 55.07, batch_ndcg = 32.69 
2025-04-08 20:41:18.625954: Steps 48/138: batch_recall = 60.30, batch_ndcg = 35.78 
2025-04-08 20:41:19.921820: Steps 49/138: batch_recall = 65.80, batch_ndcg = 38.01 
2025-04-08 20:41:21.208085: Steps 50/138: batch_recall = 59.29, batch_ndcg = 31.03 
2025-04-08 20:41:22.504315: Steps 51/138: batch_recall = 64.68, batch_ndcg = 36.28 
2025-04-08 20:41:23.803601: Steps 52/138: batch_recall = 69.15, batch_ndcg = 42.24 
2025-04-08 20:41:25.103524: Steps 53/138: batch_recall = 65.38, batch_ndcg = 34.82 
2025-04-08 20:41:26.398305: Steps 54/138: batch_recall = 68.44, batch_ndcg = 38.78 
2025-04-08 20:41:27.680207: Steps 55/138: batch_recall = 62.64, batch_ndcg = 34.36 
2025-04-08 20:41:28.965970: Steps 56/138: batch_recall = 58.88, batch_ndcg = 35.08 
2025-04-08 20:41:30.259397: Steps 57/138: batch_recall = 55.10, batch_ndcg = 31.88 
2025-04-08 20:41:31.565236: Steps 58/138: batch_recall = 65.73, batch_ndcg = 35.21 
2025-04-08 20:41:32.854364: Steps 59/138: batch_recall = 62.29, batch_ndcg = 37.75 
2025-04-08 20:41:34.133027: Steps 60/138: batch_recall = 65.37, batch_ndcg = 37.58 
2025-04-08 20:41:35.419899: Steps 61/138: batch_recall = 65.02, batch_ndcg = 35.28 
2025-04-08 20:41:36.696404: Steps 62/138: batch_recall = 81.04, batch_ndcg = 44.36 
2025-04-08 20:41:37.990164: Steps 63/138: batch_recall = 75.54, batch_ndcg = 42.77 
2025-04-08 20:41:39.282947: Steps 64/138: batch_recall = 61.26, batch_ndcg = 32.58 
2025-04-08 20:41:40.564252: Steps 65/138: batch_recall = 82.99, batch_ndcg = 46.28 
2025-04-08 20:41:41.857999: Steps 66/138: batch_recall = 69.09, batch_ndcg = 40.76 
2025-04-08 20:41:43.141968: Steps 67/138: batch_recall = 75.71, batch_ndcg = 45.84 
2025-04-08 20:41:44.424505: Steps 68/138: batch_recall = 61.31, batch_ndcg = 33.82 
2025-04-08 20:41:45.710091: Steps 69/138: batch_recall = 87.16, batch_ndcg = 51.71 
2025-04-08 20:41:47.008140: Steps 70/138: batch_recall = 77.94, batch_ndcg = 44.81 
2025-04-08 20:41:48.299255: Steps 71/138: batch_recall = 87.28, batch_ndcg = 52.25 
2025-04-08 20:41:49.592570: Steps 72/138: batch_recall = 82.89, batch_ndcg = 48.31 
2025-04-08 20:41:50.875735: Steps 73/138: batch_recall = 83.40, batch_ndcg = 46.50 
2025-04-08 20:41:52.171009: Steps 74/138: batch_recall = 78.32, batch_ndcg = 47.84 
2025-04-08 20:41:53.453017: Steps 75/138: batch_recall = 80.43, batch_ndcg = 48.31 
2025-04-08 20:41:54.746134: Steps 76/138: batch_recall = 94.85, batch_ndcg = 55.36 
2025-04-08 20:41:56.031832: Steps 77/138: batch_recall = 90.84, batch_ndcg = 51.51 
2025-04-08 20:41:57.310900: Steps 78/138: batch_recall = 90.47, batch_ndcg = 47.94 
2025-04-08 20:41:58.593185: Steps 79/138: batch_recall = 85.07, batch_ndcg = 48.39 
2025-04-08 20:41:59.866104: Steps 80/138: batch_recall = 72.35, batch_ndcg = 39.29 
2025-04-08 20:42:01.155760: Steps 81/138: batch_recall = 79.91, batch_ndcg = 48.06 
2025-04-08 20:42:02.452981: Steps 82/138: batch_recall = 86.31, batch_ndcg = 53.53 
2025-04-08 20:42:03.746201: Steps 83/138: batch_recall = 84.18, batch_ndcg = 48.51 
2025-04-08 20:42:05.040490: Steps 84/138: batch_recall = 98.05, batch_ndcg = 56.45 
2025-04-08 20:42:06.318548: Steps 85/138: batch_recall = 105.68, batch_ndcg = 61.30 
2025-04-08 20:42:07.600873: Steps 86/138: batch_recall = 118.23, batch_ndcg = 71.42 
2025-04-08 20:42:08.893303: Steps 87/138: batch_recall = 101.98, batch_ndcg = 54.60 
2025-04-08 20:42:10.180235: Steps 88/138: batch_recall = 100.72, batch_ndcg = 58.88 
2025-04-08 20:42:11.467672: Steps 89/138: batch_recall = 119.93, batch_ndcg = 66.26 
2025-04-08 20:42:12.747932: Steps 90/138: batch_recall = 100.52, batch_ndcg = 56.68 
2025-04-08 20:42:14.032328: Steps 91/138: batch_recall = 115.89, batch_ndcg = 65.17 
2025-04-08 20:42:15.310612: Steps 92/138: batch_recall = 122.45, batch_ndcg = 67.67 
2025-04-08 20:42:16.579953: Steps 93/138: batch_recall = 118.17, batch_ndcg = 67.00 
2025-04-08 20:42:17.862403: Steps 94/138: batch_recall = 126.20, batch_ndcg = 67.64 
2025-04-08 20:42:19.146916: Steps 95/138: batch_recall = 110.56, batch_ndcg = 66.44 
2025-04-08 20:42:20.442430: Steps 96/138: batch_recall = 133.23, batch_ndcg = 78.90 
2025-04-08 20:42:21.716929: Steps 97/138: batch_recall = 152.05, batch_ndcg = 92.06 
2025-04-08 20:42:22.989461: Steps 98/138: batch_recall = 110.54, batch_ndcg = 64.42 
2025-04-08 20:42:24.264823: Steps 99/138: batch_recall = 124.89, batch_ndcg = 71.51 
2025-04-08 20:42:25.543001: Steps 100/138: batch_recall = 122.85, batch_ndcg = 69.73 
2025-04-08 20:42:26.818093: Steps 101/138: batch_recall = 125.76, batch_ndcg = 69.07 
2025-04-08 20:42:28.090177: Steps 102/138: batch_recall = 121.60, batch_ndcg = 68.75 
2025-04-08 20:42:29.360850: Steps 103/138: batch_recall = 140.17, batch_ndcg = 79.13 
2025-04-08 20:42:30.635497: Steps 104/138: batch_recall = 134.09, batch_ndcg = 78.82 
2025-04-08 20:42:31.916085: Steps 105/138: batch_recall = 122.38, batch_ndcg = 68.62 
2025-04-08 20:42:33.187569: Steps 106/138: batch_recall = 102.19, batch_ndcg = 59.85 
2025-04-08 20:42:34.472437: Steps 107/138: batch_recall = 115.56, batch_ndcg = 64.80 
2025-04-08 20:42:35.754240: Steps 108/138: batch_recall = 117.94, batch_ndcg = 72.09 
2025-04-08 20:42:37.032881: Steps 109/138: batch_recall = 134.91, batch_ndcg = 76.17 
2025-04-08 20:42:38.321309: Steps 110/138: batch_recall = 120.20, batch_ndcg = 64.12 
2025-04-08 20:42:39.593928: Steps 111/138: batch_recall = 136.23, batch_ndcg = 85.03 
2025-04-08 20:42:40.875498: Steps 112/138: batch_recall = 164.13, batch_ndcg = 91.99 
2025-04-08 20:42:42.158320: Steps 113/138: batch_recall = 121.89, batch_ndcg = 68.87 
2025-04-08 20:42:43.439505: Steps 114/138: batch_recall = 125.27, batch_ndcg = 72.40 
2025-04-08 20:42:44.720178: Steps 115/138: batch_recall = 122.64, batch_ndcg = 65.04 
2025-04-08 20:42:45.993356: Steps 116/138: batch_recall = 123.46, batch_ndcg = 67.21 
2025-04-08 20:42:47.264923: Steps 117/138: batch_recall = 109.31, batch_ndcg = 64.92 
2025-04-08 20:42:48.536780: Steps 118/138: batch_recall = 120.37, batch_ndcg = 68.04 
2025-04-08 20:42:49.813946: Steps 119/138: batch_recall = 142.02, batch_ndcg = 78.27 
2025-04-08 20:42:51.098522: Steps 120/138: batch_recall = 123.39, batch_ndcg = 70.91 
2025-04-08 20:42:52.379576: Steps 121/138: batch_recall = 147.44, batch_ndcg = 78.75 
2025-04-08 20:42:53.649871: Steps 122/138: batch_recall = 151.83, batch_ndcg = 81.55 
2025-04-08 20:42:54.923026: Steps 123/138: batch_recall = 128.59, batch_ndcg = 75.04 
2025-04-08 20:42:56.207889: Steps 124/138: batch_recall = 154.49, batch_ndcg = 93.36 
2025-04-08 20:42:57.485041: Steps 125/138: batch_recall = 140.13, batch_ndcg = 76.69 
2025-04-08 20:42:58.757827: Steps 126/138: batch_recall = 158.44, batch_ndcg = 87.10 
2025-04-08 20:43:00.024927: Steps 127/138: batch_recall = 146.06, batch_ndcg = 82.27 
2025-04-08 20:43:01.294871: Steps 128/138: batch_recall = 128.51, batch_ndcg = 71.86 
2025-04-08 20:43:02.556926: Steps 129/138: batch_recall = 163.11, batch_ndcg = 92.58 
2025-04-08 20:43:03.822331: Steps 130/138: batch_recall = 135.35, batch_ndcg = 70.53 
2025-04-08 20:43:05.099504: Steps 131/138: batch_recall = 152.21, batch_ndcg = 88.41 
2025-04-08 20:43:06.372726: Steps 132/138: batch_recall = 148.15, batch_ndcg = 83.54 
2025-04-08 20:43:07.647718: Steps 133/138: batch_recall = 145.35, batch_ndcg = 84.58 
2025-04-08 20:43:08.933272: Steps 134/138: batch_recall = 143.88, batch_ndcg = 84.64 
2025-04-08 20:43:10.218663: Steps 135/138: batch_recall = 164.19, batch_ndcg = 95.62 
2025-04-08 20:43:11.492692: Steps 136/138: batch_recall = 151.94, batch_ndcg = 79.79 
2025-04-08 20:43:12.768672: Steps 137/138: batch_recall = 135.09, batch_ndcg = 86.35 
2025-04-08 20:43:12.769194: Epoch 38/1000, Test: Recall = 0.1756, NDCG = 0.1005  

2025-04-08 20:43:14.547213: Training Step 0/354: batchLoss = 0.5306, diffLoss = 2.5981, kgLoss = 0.0137
2025-04-08 20:43:16.174619: Training Step 1/354: batchLoss = 0.5036, diffLoss = 2.4643, kgLoss = 0.0134
2025-04-08 20:43:17.800216: Training Step 2/354: batchLoss = 0.5665, diffLoss = 2.7762, kgLoss = 0.0141
2025-04-08 20:43:19.423221: Training Step 3/354: batchLoss = 0.5036, diffLoss = 2.4666, kgLoss = 0.0128
2025-04-08 20:43:21.045380: Training Step 4/354: batchLoss = 0.5641, diffLoss = 2.7611, kgLoss = 0.0149
2025-04-08 20:43:22.669049: Training Step 5/354: batchLoss = 0.4773, diffLoss = 2.3216, kgLoss = 0.0162
2025-04-08 20:43:24.293350: Training Step 6/354: batchLoss = 0.6386, diffLoss = 3.1324, kgLoss = 0.0151
2025-04-08 20:43:25.919843: Training Step 7/354: batchLoss = 0.5083, diffLoss = 2.4838, kgLoss = 0.0144
2025-04-08 20:43:27.550899: Training Step 8/354: batchLoss = 0.5717, diffLoss = 2.7992, kgLoss = 0.0148
2025-04-08 20:43:29.167516: Training Step 9/354: batchLoss = 1.5379, diffLoss = 7.5340, kgLoss = 0.0388
2025-04-08 20:43:30.793493: Training Step 10/354: batchLoss = 0.4921, diffLoss = 2.4000, kgLoss = 0.0152
2025-04-08 20:43:32.415857: Training Step 11/354: batchLoss = 0.4809, diffLoss = 2.3530, kgLoss = 0.0129
2025-04-08 20:43:34.038789: Training Step 12/354: batchLoss = 0.5080, diffLoss = 2.4854, kgLoss = 0.0136
2025-04-08 20:43:35.664653: Training Step 13/354: batchLoss = 0.6047, diffLoss = 2.9577, kgLoss = 0.0165
2025-04-08 20:43:37.286080: Training Step 14/354: batchLoss = 0.4793, diffLoss = 2.3471, kgLoss = 0.0124
2025-04-08 20:43:38.913769: Training Step 15/354: batchLoss = 0.5305, diffLoss = 2.5940, kgLoss = 0.0146
2025-04-08 20:43:40.546581: Training Step 16/354: batchLoss = 0.5846, diffLoss = 2.8513, kgLoss = 0.0179
2025-04-08 20:43:42.171689: Training Step 17/354: batchLoss = 0.5412, diffLoss = 2.6487, kgLoss = 0.0143
2025-04-08 20:43:43.797059: Training Step 18/354: batchLoss = 0.5425, diffLoss = 2.6566, kgLoss = 0.0140
2025-04-08 20:43:45.424557: Training Step 19/354: batchLoss = 0.4645, diffLoss = 2.2618, kgLoss = 0.0151
2025-04-08 20:43:47.050284: Training Step 20/354: batchLoss = 0.6723, diffLoss = 3.2852, kgLoss = 0.0190
2025-04-08 20:43:48.672348: Training Step 21/354: batchLoss = 0.6225, diffLoss = 3.0501, kgLoss = 0.0156
2025-04-08 20:43:50.292164: Training Step 22/354: batchLoss = 0.5137, diffLoss = 2.5086, kgLoss = 0.0150
2025-04-08 20:43:51.918797: Training Step 23/354: batchLoss = 0.5212, diffLoss = 2.5502, kgLoss = 0.0139
2025-04-08 20:43:53.541942: Training Step 24/354: batchLoss = 0.5319, diffLoss = 2.6052, kgLoss = 0.0136
2025-04-08 20:43:55.177586: Training Step 25/354: batchLoss = 0.5289, diffLoss = 2.5945, kgLoss = 0.0126
2025-04-08 20:43:56.814822: Training Step 26/354: batchLoss = 0.5510, diffLoss = 2.6912, kgLoss = 0.0160
2025-04-08 20:43:58.445627: Training Step 27/354: batchLoss = 0.5132, diffLoss = 2.5084, kgLoss = 0.0144
2025-04-08 20:44:00.074392: Training Step 28/354: batchLoss = 0.5246, diffLoss = 2.5729, kgLoss = 0.0126
2025-04-08 20:44:01.700790: Training Step 29/354: batchLoss = 0.4882, diffLoss = 2.3806, kgLoss = 0.0151
2025-04-08 20:44:03.335755: Training Step 30/354: batchLoss = 0.4903, diffLoss = 2.4019, kgLoss = 0.0124
2025-04-08 20:44:04.955373: Training Step 31/354: batchLoss = 0.4745, diffLoss = 2.3190, kgLoss = 0.0134
2025-04-08 20:44:06.572907: Training Step 32/354: batchLoss = 0.5207, diffLoss = 2.5512, kgLoss = 0.0131
2025-04-08 20:44:08.194549: Training Step 33/354: batchLoss = 0.9326, diffLoss = 4.5605, kgLoss = 0.0256
2025-04-08 20:44:09.815534: Training Step 34/354: batchLoss = 0.6920, diffLoss = 3.3890, kgLoss = 0.0178
2025-04-08 20:44:11.442686: Training Step 35/354: batchLoss = 0.5582, diffLoss = 2.7296, kgLoss = 0.0154
2025-04-08 20:44:13.076768: Training Step 36/354: batchLoss = 0.5749, diffLoss = 2.8151, kgLoss = 0.0149
2025-04-08 20:44:14.709754: Training Step 37/354: batchLoss = 0.3989, diffLoss = 1.9453, kgLoss = 0.0123
2025-04-08 20:44:16.340882: Training Step 38/354: batchLoss = 0.5205, diffLoss = 2.5421, kgLoss = 0.0151
2025-04-08 20:44:17.973676: Training Step 39/354: batchLoss = 0.4753, diffLoss = 2.3190, kgLoss = 0.0144
2025-04-08 20:44:19.603496: Training Step 40/354: batchLoss = 0.5305, diffLoss = 2.5951, kgLoss = 0.0143
2025-04-08 20:44:21.230471: Training Step 41/354: batchLoss = 0.7356, diffLoss = 3.6117, kgLoss = 0.0165
2025-04-08 20:44:22.857099: Training Step 42/354: batchLoss = 0.5350, diffLoss = 2.6184, kgLoss = 0.0141
2025-04-08 20:44:24.478007: Training Step 43/354: batchLoss = 0.5618, diffLoss = 2.7493, kgLoss = 0.0149
2025-04-08 20:44:26.097890: Training Step 44/354: batchLoss = 0.5468, diffLoss = 2.6760, kgLoss = 0.0145
2025-04-08 20:44:27.707652: Training Step 45/354: batchLoss = 0.5751, diffLoss = 2.8137, kgLoss = 0.0155
2025-04-08 20:44:29.335424: Training Step 46/354: batchLoss = 0.6428, diffLoss = 3.1462, kgLoss = 0.0169
2025-04-08 20:44:30.963206: Training Step 47/354: batchLoss = 0.8815, diffLoss = 4.3229, kgLoss = 0.0212
2025-04-08 20:44:32.590200: Training Step 48/354: batchLoss = 0.5223, diffLoss = 2.5564, kgLoss = 0.0138
2025-04-08 20:44:34.225258: Training Step 49/354: batchLoss = 0.5297, diffLoss = 2.6037, kgLoss = 0.0112
2025-04-08 20:44:35.858218: Training Step 50/354: batchLoss = 0.5449, diffLoss = 2.6655, kgLoss = 0.0148
2025-04-08 20:44:37.484298: Training Step 51/354: batchLoss = 0.5453, diffLoss = 2.6656, kgLoss = 0.0153
2025-04-08 20:44:39.104683: Training Step 52/354: batchLoss = 0.5115, diffLoss = 2.5101, kgLoss = 0.0118
2025-04-08 20:44:40.725528: Training Step 53/354: batchLoss = 0.5505, diffLoss = 2.6938, kgLoss = 0.0146
2025-04-08 20:44:42.349626: Training Step 54/354: batchLoss = 0.7492, diffLoss = 3.6753, kgLoss = 0.0177
2025-04-08 20:44:43.978046: Training Step 55/354: batchLoss = 0.5168, diffLoss = 2.5300, kgLoss = 0.0135
2025-04-08 20:44:45.603704: Training Step 56/354: batchLoss = 0.4671, diffLoss = 2.2852, kgLoss = 0.0126
2025-04-08 20:44:47.234872: Training Step 57/354: batchLoss = 0.4359, diffLoss = 2.1337, kgLoss = 0.0114
2025-04-08 20:44:48.870105: Training Step 58/354: batchLoss = 0.4861, diffLoss = 2.3785, kgLoss = 0.0130
2025-04-08 20:44:50.495929: Training Step 59/354: batchLoss = 0.5499, diffLoss = 2.6947, kgLoss = 0.0136
2025-04-08 20:44:52.126733: Training Step 60/354: batchLoss = 0.4690, diffLoss = 2.2881, kgLoss = 0.0143
2025-04-08 20:44:53.748663: Training Step 61/354: batchLoss = 0.4871, diffLoss = 2.3841, kgLoss = 0.0129
2025-04-08 20:44:55.379016: Training Step 62/354: batchLoss = 0.5408, diffLoss = 2.6507, kgLoss = 0.0133
2025-04-08 20:44:56.997730: Training Step 63/354: batchLoss = 0.5577, diffLoss = 2.7353, kgLoss = 0.0133
2025-04-08 20:44:58.624135: Training Step 64/354: batchLoss = 0.4831, diffLoss = 2.3663, kgLoss = 0.0123
2025-04-08 20:45:00.250174: Training Step 65/354: batchLoss = 0.5695, diffLoss = 2.7906, kgLoss = 0.0143
2025-04-08 20:45:01.887950: Training Step 66/354: batchLoss = 0.5804, diffLoss = 2.8474, kgLoss = 0.0136
2025-04-08 20:45:03.523377: Training Step 67/354: batchLoss = 0.7823, diffLoss = 3.8331, kgLoss = 0.0196
2025-04-08 20:45:05.155904: Training Step 68/354: batchLoss = 0.5045, diffLoss = 2.4682, kgLoss = 0.0136
2025-04-08 20:45:06.788291: Training Step 69/354: batchLoss = 0.6217, diffLoss = 3.0482, kgLoss = 0.0151
2025-04-08 20:45:08.416179: Training Step 70/354: batchLoss = 0.6886, diffLoss = 3.3761, kgLoss = 0.0167
2025-04-08 20:45:10.031689: Training Step 71/354: batchLoss = 0.5927, diffLoss = 2.9034, kgLoss = 0.0150
2025-04-08 20:45:11.649132: Training Step 72/354: batchLoss = 0.5527, diffLoss = 2.7062, kgLoss = 0.0143
2025-04-08 20:45:13.269695: Training Step 73/354: batchLoss = 0.4434, diffLoss = 2.1606, kgLoss = 0.0140
2025-04-08 20:45:14.887065: Training Step 74/354: batchLoss = 0.4853, diffLoss = 2.3743, kgLoss = 0.0130
2025-04-08 20:45:16.510432: Training Step 75/354: batchLoss = 0.6194, diffLoss = 3.0333, kgLoss = 0.0159
2025-04-08 20:45:18.128316: Training Step 76/354: batchLoss = 0.4971, diffLoss = 2.4329, kgLoss = 0.0131
2025-04-08 20:45:19.749161: Training Step 77/354: batchLoss = 0.5805, diffLoss = 2.8440, kgLoss = 0.0146
2025-04-08 20:45:21.367312: Training Step 78/354: batchLoss = 0.5146, diffLoss = 2.5282, kgLoss = 0.0111
2025-04-08 20:45:22.983093: Training Step 79/354: batchLoss = 0.6403, diffLoss = 3.1381, kgLoss = 0.0159
2025-04-08 20:45:24.606004: Training Step 80/354: batchLoss = 0.5343, diffLoss = 2.6159, kgLoss = 0.0139
2025-04-08 20:45:26.220712: Training Step 81/354: batchLoss = 0.4968, diffLoss = 2.4293, kgLoss = 0.0137
2025-04-08 20:45:27.828229: Training Step 82/354: batchLoss = 0.4699, diffLoss = 2.2923, kgLoss = 0.0143
2025-04-08 20:45:29.446412: Training Step 83/354: batchLoss = 0.5421, diffLoss = 2.6545, kgLoss = 0.0140
2025-04-08 20:45:31.059475: Training Step 84/354: batchLoss = 0.6074, diffLoss = 2.9747, kgLoss = 0.0156
2025-04-08 20:45:32.690177: Training Step 85/354: batchLoss = 0.4528, diffLoss = 2.2112, kgLoss = 0.0132
2025-04-08 20:45:34.312701: Training Step 86/354: batchLoss = 0.5682, diffLoss = 2.7819, kgLoss = 0.0147
2025-04-08 20:45:35.941231: Training Step 87/354: batchLoss = 0.5815, diffLoss = 2.8448, kgLoss = 0.0157
2025-04-08 20:45:37.564805: Training Step 88/354: batchLoss = 0.6276, diffLoss = 3.0731, kgLoss = 0.0162
2025-04-08 20:45:39.191481: Training Step 89/354: batchLoss = 0.5344, diffLoss = 2.6201, kgLoss = 0.0130
2025-04-08 20:45:40.820193: Training Step 90/354: batchLoss = 0.7082, diffLoss = 3.4724, kgLoss = 0.0171
2025-04-08 20:45:42.453067: Training Step 91/354: batchLoss = 0.4835, diffLoss = 2.3695, kgLoss = 0.0120
2025-04-08 20:45:44.068423: Training Step 92/354: batchLoss = 0.5735, diffLoss = 2.8085, kgLoss = 0.0147
2025-04-08 20:45:45.690004: Training Step 93/354: batchLoss = 0.5223, diffLoss = 2.5578, kgLoss = 0.0134
2025-04-08 20:45:47.315708: Training Step 94/354: batchLoss = 0.5470, diffLoss = 2.6820, kgLoss = 0.0133
2025-04-08 20:45:48.942196: Training Step 95/354: batchLoss = 0.6498, diffLoss = 3.1841, kgLoss = 0.0162
2025-04-08 20:45:50.566878: Training Step 96/354: batchLoss = 0.5973, diffLoss = 2.9201, kgLoss = 0.0166
2025-04-08 20:45:52.201215: Training Step 97/354: batchLoss = 0.5624, diffLoss = 2.7499, kgLoss = 0.0155
2025-04-08 20:45:53.828635: Training Step 98/354: batchLoss = 0.5054, diffLoss = 2.4799, kgLoss = 0.0117
2025-04-08 20:45:55.457343: Training Step 99/354: batchLoss = 0.5141, diffLoss = 2.5065, kgLoss = 0.0160
2025-04-08 20:45:57.082767: Training Step 100/354: batchLoss = 0.5774, diffLoss = 2.8250, kgLoss = 0.0155
2025-04-08 20:45:58.697487: Training Step 101/354: batchLoss = 0.5621, diffLoss = 2.7456, kgLoss = 0.0162
2025-04-08 20:46:00.314497: Training Step 102/354: batchLoss = 0.6355, diffLoss = 3.1164, kgLoss = 0.0153
2025-04-08 20:46:01.932094: Training Step 103/354: batchLoss = 0.4253, diffLoss = 2.0798, kgLoss = 0.0116
2025-04-08 20:46:03.555499: Training Step 104/354: batchLoss = 0.6407, diffLoss = 3.1337, kgLoss = 0.0175
2025-04-08 20:46:05.183195: Training Step 105/354: batchLoss = 0.6696, diffLoss = 3.2743, kgLoss = 0.0184
2025-04-08 20:46:06.818172: Training Step 106/354: batchLoss = 0.5104, diffLoss = 2.4887, kgLoss = 0.0159
2025-04-08 20:46:08.447226: Training Step 107/354: batchLoss = 0.5106, diffLoss = 2.4998, kgLoss = 0.0133
2025-04-08 20:46:10.076761: Training Step 108/354: batchLoss = 0.5692, diffLoss = 2.7868, kgLoss = 0.0148
2025-04-08 20:46:11.716760: Training Step 109/354: batchLoss = 0.4695, diffLoss = 2.2996, kgLoss = 0.0119
2025-04-08 20:46:13.342940: Training Step 110/354: batchLoss = 0.6076, diffLoss = 2.9741, kgLoss = 0.0160
2025-04-08 20:46:14.960055: Training Step 111/354: batchLoss = 0.6137, diffLoss = 3.0018, kgLoss = 0.0167
2025-04-08 20:46:16.582706: Training Step 112/354: batchLoss = 0.5353, diffLoss = 2.6157, kgLoss = 0.0152
2025-04-08 20:46:18.195398: Training Step 113/354: batchLoss = 0.6131, diffLoss = 3.0001, kgLoss = 0.0163
2025-04-08 20:46:19.819881: Training Step 114/354: batchLoss = 0.5569, diffLoss = 2.7263, kgLoss = 0.0145
2025-04-08 20:46:21.449011: Training Step 115/354: batchLoss = 0.5343, diffLoss = 2.6171, kgLoss = 0.0136
2025-04-08 20:46:23.082747: Training Step 116/354: batchLoss = 0.5536, diffLoss = 2.7118, kgLoss = 0.0141
2025-04-08 20:46:24.709142: Training Step 117/354: batchLoss = 0.5828, diffLoss = 2.8568, kgLoss = 0.0142
2025-04-08 20:46:26.343382: Training Step 118/354: batchLoss = 0.5206, diffLoss = 2.5504, kgLoss = 0.0132
2025-04-08 20:46:27.970419: Training Step 119/354: batchLoss = 0.8352, diffLoss = 4.0907, kgLoss = 0.0214
2025-04-08 20:46:29.586347: Training Step 120/354: batchLoss = 0.4314, diffLoss = 2.1133, kgLoss = 0.0109
2025-04-08 20:46:31.207271: Training Step 121/354: batchLoss = 0.5402, diffLoss = 2.6427, kgLoss = 0.0145
2025-04-08 20:46:32.824807: Training Step 122/354: batchLoss = 0.5264, diffLoss = 2.5750, kgLoss = 0.0142
2025-04-08 20:46:34.444659: Training Step 123/354: batchLoss = 0.7035, diffLoss = 3.4460, kgLoss = 0.0179
2025-04-08 20:46:36.068611: Training Step 124/354: batchLoss = 0.6281, diffLoss = 3.0803, kgLoss = 0.0151
2025-04-08 20:46:37.694282: Training Step 125/354: batchLoss = 0.5611, diffLoss = 2.7451, kgLoss = 0.0151
2025-04-08 20:46:39.322911: Training Step 126/354: batchLoss = 0.5133, diffLoss = 2.5070, kgLoss = 0.0149
2025-04-08 20:46:40.956666: Training Step 127/354: batchLoss = 0.4344, diffLoss = 2.1265, kgLoss = 0.0114
2025-04-08 20:46:42.585399: Training Step 128/354: batchLoss = 0.5079, diffLoss = 2.4844, kgLoss = 0.0138
2025-04-08 20:46:44.212175: Training Step 129/354: batchLoss = 0.4753, diffLoss = 2.3316, kgLoss = 0.0112
2025-04-08 20:46:45.836308: Training Step 130/354: batchLoss = 0.5515, diffLoss = 2.6977, kgLoss = 0.0150
2025-04-08 20:46:47.460877: Training Step 131/354: batchLoss = 0.6897, diffLoss = 3.3728, kgLoss = 0.0190
2025-04-08 20:46:49.087353: Training Step 132/354: batchLoss = 0.4033, diffLoss = 1.9706, kgLoss = 0.0115
2025-04-08 20:46:50.712409: Training Step 133/354: batchLoss = 0.5529, diffLoss = 2.7088, kgLoss = 0.0140
2025-04-08 20:46:52.340297: Training Step 134/354: batchLoss = 0.4968, diffLoss = 2.4343, kgLoss = 0.0124
2025-04-08 20:46:53.975515: Training Step 135/354: batchLoss = 0.5750, diffLoss = 2.8176, kgLoss = 0.0143
2025-04-08 20:46:55.605972: Training Step 136/354: batchLoss = 0.5828, diffLoss = 2.8591, kgLoss = 0.0137
2025-04-08 20:46:57.232679: Training Step 137/354: batchLoss = 0.4488, diffLoss = 2.1869, kgLoss = 0.0143
2025-04-08 20:46:58.859753: Training Step 138/354: batchLoss = 0.4661, diffLoss = 2.2842, kgLoss = 0.0115
2025-04-08 20:47:00.494670: Training Step 139/354: batchLoss = 0.4715, diffLoss = 2.3087, kgLoss = 0.0122
2025-04-08 20:47:02.116347: Training Step 140/354: batchLoss = 0.4933, diffLoss = 2.4165, kgLoss = 0.0125
2025-04-08 20:47:03.738371: Training Step 141/354: batchLoss = 0.6285, diffLoss = 3.0788, kgLoss = 0.0159
2025-04-08 20:47:05.358626: Training Step 142/354: batchLoss = 0.5246, diffLoss = 2.5690, kgLoss = 0.0136
2025-04-08 20:47:06.992632: Training Step 143/354: batchLoss = 0.5050, diffLoss = 2.4710, kgLoss = 0.0135
2025-04-08 20:47:08.621269: Training Step 144/354: batchLoss = 0.5523, diffLoss = 2.6972, kgLoss = 0.0161
2025-04-08 20:47:10.248951: Training Step 145/354: batchLoss = 0.5483, diffLoss = 2.6848, kgLoss = 0.0142
2025-04-08 20:47:11.874392: Training Step 146/354: batchLoss = 0.4338, diffLoss = 2.1185, kgLoss = 0.0126
2025-04-08 20:47:13.507187: Training Step 147/354: batchLoss = 0.5610, diffLoss = 2.7457, kgLoss = 0.0149
2025-04-08 20:47:15.145163: Training Step 148/354: batchLoss = 0.4304, diffLoss = 2.1020, kgLoss = 0.0125
2025-04-08 20:47:16.774849: Training Step 149/354: batchLoss = 0.4491, diffLoss = 2.1977, kgLoss = 0.0120
2025-04-08 20:47:18.401849: Training Step 150/354: batchLoss = 0.5244, diffLoss = 2.5601, kgLoss = 0.0155
2025-04-08 20:47:20.017701: Training Step 151/354: batchLoss = 0.5390, diffLoss = 2.6406, kgLoss = 0.0136
2025-04-08 20:47:21.640493: Training Step 152/354: batchLoss = 0.5018, diffLoss = 2.4556, kgLoss = 0.0133
2025-04-08 20:47:23.264264: Training Step 153/354: batchLoss = 0.6434, diffLoss = 3.1530, kgLoss = 0.0160
2025-04-08 20:47:24.890593: Training Step 154/354: batchLoss = 0.6326, diffLoss = 3.1009, kgLoss = 0.0155
2025-04-08 20:47:26.521545: Training Step 155/354: batchLoss = 0.4636, diffLoss = 2.2604, kgLoss = 0.0144
2025-04-08 20:47:28.156880: Training Step 156/354: batchLoss = 0.6772, diffLoss = 3.3169, kgLoss = 0.0172
2025-04-08 20:47:29.781411: Training Step 157/354: batchLoss = 0.4931, diffLoss = 2.4135, kgLoss = 0.0130
2025-04-08 20:47:31.410271: Training Step 158/354: batchLoss = 0.6905, diffLoss = 3.3824, kgLoss = 0.0175
2025-04-08 20:47:33.038292: Training Step 159/354: batchLoss = 0.5360, diffLoss = 2.6278, kgLoss = 0.0130
2025-04-08 20:47:34.654010: Training Step 160/354: batchLoss = 0.5566, diffLoss = 2.7208, kgLoss = 0.0155
2025-04-08 20:47:36.280354: Training Step 161/354: batchLoss = 0.6164, diffLoss = 3.0175, kgLoss = 0.0162
2025-04-08 20:47:37.892974: Training Step 162/354: batchLoss = 0.5868, diffLoss = 2.8759, kgLoss = 0.0145
2025-04-08 20:47:39.520263: Training Step 163/354: batchLoss = 0.5256, diffLoss = 2.5745, kgLoss = 0.0134
2025-04-08 20:47:41.147937: Training Step 164/354: batchLoss = 0.6361, diffLoss = 3.1173, kgLoss = 0.0158
2025-04-08 20:47:42.776584: Training Step 165/354: batchLoss = 0.5514, diffLoss = 2.6974, kgLoss = 0.0149
2025-04-08 20:47:44.411648: Training Step 166/354: batchLoss = 0.4925, diffLoss = 2.4093, kgLoss = 0.0132
2025-04-08 20:47:46.042577: Training Step 167/354: batchLoss = 0.5538, diffLoss = 2.7053, kgLoss = 0.0160
2025-04-08 20:47:47.671723: Training Step 168/354: batchLoss = 0.6294, diffLoss = 3.0827, kgLoss = 0.0161
2025-04-08 20:47:49.301722: Training Step 169/354: batchLoss = 0.5223, diffLoss = 2.5556, kgLoss = 0.0139
2025-04-08 20:47:50.922592: Training Step 170/354: batchLoss = 0.4494, diffLoss = 2.2025, kgLoss = 0.0111
2025-04-08 20:47:52.542601: Training Step 171/354: batchLoss = 0.5723, diffLoss = 2.8027, kgLoss = 0.0147
2025-04-08 20:47:54.165534: Training Step 172/354: batchLoss = 0.6387, diffLoss = 3.1233, kgLoss = 0.0176
2025-04-08 20:47:55.795990: Training Step 173/354: batchLoss = 0.4092, diffLoss = 2.0016, kgLoss = 0.0110
2025-04-08 20:47:57.428112: Training Step 174/354: batchLoss = 0.4958, diffLoss = 2.4280, kgLoss = 0.0127
2025-04-08 20:47:59.064416: Training Step 175/354: batchLoss = 0.6468, diffLoss = 3.1628, kgLoss = 0.0179
2025-04-08 20:48:00.697713: Training Step 176/354: batchLoss = 0.6562, diffLoss = 3.2114, kgLoss = 0.0174
2025-04-08 20:48:02.326482: Training Step 177/354: batchLoss = 0.5509, diffLoss = 2.6987, kgLoss = 0.0139
2025-04-08 20:48:03.952298: Training Step 178/354: batchLoss = 0.5693, diffLoss = 2.7865, kgLoss = 0.0150
2025-04-08 20:48:05.582373: Training Step 179/354: batchLoss = 0.5145, diffLoss = 2.5185, kgLoss = 0.0134
2025-04-08 20:48:07.206370: Training Step 180/354: batchLoss = 0.6075, diffLoss = 2.9797, kgLoss = 0.0144
2025-04-08 20:48:08.833973: Training Step 181/354: batchLoss = 0.8370, diffLoss = 4.1038, kgLoss = 0.0203
2025-04-08 20:48:10.450369: Training Step 182/354: batchLoss = 0.4451, diffLoss = 2.1745, kgLoss = 0.0128
2025-04-08 20:48:12.080714: Training Step 183/354: batchLoss = 0.5016, diffLoss = 2.4608, kgLoss = 0.0118
2025-04-08 20:48:13.705415: Training Step 184/354: batchLoss = 0.5999, diffLoss = 2.9337, kgLoss = 0.0165
2025-04-08 20:48:15.331499: Training Step 185/354: batchLoss = 0.6063, diffLoss = 2.9740, kgLoss = 0.0143
2025-04-08 20:48:16.965215: Training Step 186/354: batchLoss = 0.5238, diffLoss = 2.5621, kgLoss = 0.0142
2025-04-08 20:48:18.596962: Training Step 187/354: batchLoss = 0.5402, diffLoss = 2.6259, kgLoss = 0.0188
2025-04-08 20:48:20.225875: Training Step 188/354: batchLoss = 0.5182, diffLoss = 2.5387, kgLoss = 0.0131
2025-04-08 20:48:21.850293: Training Step 189/354: batchLoss = 0.5152, diffLoss = 2.5208, kgLoss = 0.0138
2025-04-08 20:48:23.476070: Training Step 190/354: batchLoss = 0.5403, diffLoss = 2.6449, kgLoss = 0.0142
2025-04-08 20:48:25.098636: Training Step 191/354: batchLoss = 0.5143, diffLoss = 2.5148, kgLoss = 0.0142
2025-04-08 20:48:26.718042: Training Step 192/354: batchLoss = 0.6659, diffLoss = 3.2603, kgLoss = 0.0173
2025-04-08 20:48:28.345724: Training Step 193/354: batchLoss = 0.5501, diffLoss = 2.6918, kgLoss = 0.0146
2025-04-08 20:48:29.978734: Training Step 194/354: batchLoss = 0.5306, diffLoss = 2.6035, kgLoss = 0.0124
2025-04-08 20:48:31.610946: Training Step 195/354: batchLoss = 0.5678, diffLoss = 2.7717, kgLoss = 0.0169
2025-04-08 20:48:33.238677: Training Step 196/354: batchLoss = 0.5194, diffLoss = 2.5449, kgLoss = 0.0130
2025-04-08 20:48:34.864555: Training Step 197/354: batchLoss = 0.6705, diffLoss = 3.2863, kgLoss = 0.0166
2025-04-08 20:48:36.497565: Training Step 198/354: batchLoss = 0.4998, diffLoss = 2.4468, kgLoss = 0.0130
2025-04-08 20:48:38.122436: Training Step 199/354: batchLoss = 0.6236, diffLoss = 3.0618, kgLoss = 0.0140
2025-04-08 20:48:39.741934: Training Step 200/354: batchLoss = 0.5986, diffLoss = 2.9356, kgLoss = 0.0144
2025-04-08 20:48:41.364478: Training Step 201/354: batchLoss = 0.5754, diffLoss = 2.8095, kgLoss = 0.0169
2025-04-08 20:48:42.987828: Training Step 202/354: batchLoss = 0.5300, diffLoss = 2.5939, kgLoss = 0.0140
2025-04-08 20:48:44.618060: Training Step 203/354: batchLoss = 0.5139, diffLoss = 2.5196, kgLoss = 0.0125
2025-04-08 20:48:46.257058: Training Step 204/354: batchLoss = 0.4949, diffLoss = 2.4183, kgLoss = 0.0140
2025-04-08 20:48:47.889372: Training Step 205/354: batchLoss = 0.4213, diffLoss = 2.0679, kgLoss = 0.0096
2025-04-08 20:48:49.522530: Training Step 206/354: batchLoss = 0.5095, diffLoss = 2.4969, kgLoss = 0.0126
2025-04-08 20:48:51.156362: Training Step 207/354: batchLoss = 0.6274, diffLoss = 3.0585, kgLoss = 0.0196
2025-04-08 20:48:52.788879: Training Step 208/354: batchLoss = 0.8706, diffLoss = 4.2627, kgLoss = 0.0226
2025-04-08 20:48:54.406115: Training Step 209/354: batchLoss = 0.5368, diffLoss = 2.6256, kgLoss = 0.0146
2025-04-08 20:48:56.025299: Training Step 210/354: batchLoss = 0.5790, diffLoss = 2.8393, kgLoss = 0.0139
2025-04-08 20:48:57.644843: Training Step 211/354: batchLoss = 0.5266, diffLoss = 2.5720, kgLoss = 0.0153
2025-04-08 20:48:59.273279: Training Step 212/354: batchLoss = 0.6816, diffLoss = 3.3379, kgLoss = 0.0175
2025-04-08 20:49:00.906098: Training Step 213/354: batchLoss = 0.5886, diffLoss = 2.8798, kgLoss = 0.0158
2025-04-08 20:49:02.542912: Training Step 214/354: batchLoss = 0.6240, diffLoss = 3.0582, kgLoss = 0.0154
2025-04-08 20:49:04.174900: Training Step 215/354: batchLoss = 0.5213, diffLoss = 2.5537, kgLoss = 0.0131
2025-04-08 20:49:05.810402: Training Step 216/354: batchLoss = 0.4967, diffLoss = 2.4316, kgLoss = 0.0130
2025-04-08 20:49:07.443651: Training Step 217/354: batchLoss = 0.6153, diffLoss = 3.0162, kgLoss = 0.0150
2025-04-08 20:49:09.083285: Training Step 218/354: batchLoss = 0.6614, diffLoss = 3.2368, kgLoss = 0.0176
2025-04-08 20:49:10.705257: Training Step 219/354: batchLoss = 0.5464, diffLoss = 2.6734, kgLoss = 0.0147
2025-04-08 20:49:12.329033: Training Step 220/354: batchLoss = 0.7283, diffLoss = 3.5644, kgLoss = 0.0193
2025-04-08 20:49:13.944617: Training Step 221/354: batchLoss = 0.6862, diffLoss = 3.3593, kgLoss = 0.0180
2025-04-08 20:49:15.562586: Training Step 222/354: batchLoss = 0.5607, diffLoss = 2.7422, kgLoss = 0.0153
2025-04-08 20:49:17.195765: Training Step 223/354: batchLoss = 0.5014, diffLoss = 2.4570, kgLoss = 0.0125
2025-04-08 20:49:18.830285: Training Step 224/354: batchLoss = 0.5710, diffLoss = 2.7944, kgLoss = 0.0151
2025-04-08 20:49:20.460556: Training Step 225/354: batchLoss = 0.5459, diffLoss = 2.6732, kgLoss = 0.0141
2025-04-08 20:49:22.102180: Training Step 226/354: batchLoss = 0.5175, diffLoss = 2.5269, kgLoss = 0.0152
2025-04-08 20:49:23.734434: Training Step 227/354: batchLoss = 0.5402, diffLoss = 2.6395, kgLoss = 0.0154
2025-04-08 20:49:25.363688: Training Step 228/354: batchLoss = 0.5248, diffLoss = 2.5768, kgLoss = 0.0118
2025-04-08 20:49:26.991930: Training Step 229/354: batchLoss = 0.5815, diffLoss = 2.8418, kgLoss = 0.0164
2025-04-08 20:49:28.612136: Training Step 230/354: batchLoss = 0.5910, diffLoss = 2.8898, kgLoss = 0.0163
2025-04-08 20:49:30.226728: Training Step 231/354: batchLoss = 0.4426, diffLoss = 2.1761, kgLoss = 0.0092
2025-04-08 20:49:31.846394: Training Step 232/354: batchLoss = 0.4929, diffLoss = 2.4094, kgLoss = 0.0138
2025-04-08 20:49:33.464584: Training Step 233/354: batchLoss = 0.4948, diffLoss = 2.4204, kgLoss = 0.0134
2025-04-08 20:49:35.094135: Training Step 234/354: batchLoss = 0.5452, diffLoss = 2.6712, kgLoss = 0.0137
2025-04-08 20:49:36.724797: Training Step 235/354: batchLoss = 0.5516, diffLoss = 2.6998, kgLoss = 0.0146
2025-04-08 20:49:38.353766: Training Step 236/354: batchLoss = 0.5171, diffLoss = 2.5331, kgLoss = 0.0131
2025-04-08 20:49:39.980212: Training Step 237/354: batchLoss = 0.5472, diffLoss = 2.6799, kgLoss = 0.0141
2025-04-08 20:49:41.613381: Training Step 238/354: batchLoss = 0.4959, diffLoss = 2.4218, kgLoss = 0.0144
2025-04-08 20:49:43.231518: Training Step 239/354: batchLoss = 0.6393, diffLoss = 3.1254, kgLoss = 0.0178
2025-04-08 20:49:44.844244: Training Step 240/354: batchLoss = 0.4883, diffLoss = 2.3906, kgLoss = 0.0127
2025-04-08 20:49:46.467268: Training Step 241/354: batchLoss = 0.6158, diffLoss = 3.0168, kgLoss = 0.0155
2025-04-08 20:49:48.094046: Training Step 242/354: batchLoss = 0.5312, diffLoss = 2.5927, kgLoss = 0.0158
2025-04-08 20:49:49.727054: Training Step 243/354: batchLoss = 0.4352, diffLoss = 2.1253, kgLoss = 0.0127
2025-04-08 20:49:51.355586: Training Step 244/354: batchLoss = 0.5309, diffLoss = 2.5943, kgLoss = 0.0150
2025-04-08 20:49:52.987793: Training Step 245/354: batchLoss = 0.5209, diffLoss = 2.5461, kgLoss = 0.0146
2025-04-08 20:49:54.610086: Training Step 246/354: batchLoss = 0.5739, diffLoss = 2.8138, kgLoss = 0.0139
2025-04-08 20:49:56.240556: Training Step 247/354: batchLoss = 0.6233, diffLoss = 3.0571, kgLoss = 0.0149
2025-04-08 20:49:57.870074: Training Step 248/354: batchLoss = 0.5694, diffLoss = 2.7897, kgLoss = 0.0143
2025-04-08 20:49:59.488304: Training Step 249/354: batchLoss = 0.8290, diffLoss = 4.0650, kgLoss = 0.0199
2025-04-08 20:50:01.105934: Training Step 250/354: batchLoss = 0.5490, diffLoss = 2.6822, kgLoss = 0.0157
2025-04-08 20:50:02.723956: Training Step 251/354: batchLoss = 0.4848, diffLoss = 2.3626, kgLoss = 0.0153
2025-04-08 20:50:04.341959: Training Step 252/354: batchLoss = 0.5683, diffLoss = 2.7803, kgLoss = 0.0153
2025-04-08 20:50:05.967818: Training Step 253/354: batchLoss = 0.5632, diffLoss = 2.7583, kgLoss = 0.0145
2025-04-08 20:50:07.591100: Training Step 254/354: batchLoss = 0.6452, diffLoss = 3.1616, kgLoss = 0.0161
2025-04-08 20:50:09.219113: Training Step 255/354: batchLoss = 0.5442, diffLoss = 2.6670, kgLoss = 0.0135
2025-04-08 20:50:10.843864: Training Step 256/354: batchLoss = 0.6024, diffLoss = 2.9546, kgLoss = 0.0143
2025-04-08 20:50:12.470714: Training Step 257/354: batchLoss = 0.4550, diffLoss = 2.2230, kgLoss = 0.0130
2025-04-08 20:50:14.091370: Training Step 258/354: batchLoss = 0.5152, diffLoss = 2.5208, kgLoss = 0.0138
2025-04-08 20:50:15.723761: Training Step 259/354: batchLoss = 0.4571, diffLoss = 2.2336, kgLoss = 0.0130
2025-04-08 20:50:17.349951: Training Step 260/354: batchLoss = 0.6159, diffLoss = 3.0183, kgLoss = 0.0153
2025-04-08 20:50:18.971716: Training Step 261/354: batchLoss = 0.6132, diffLoss = 2.9970, kgLoss = 0.0172
2025-04-08 20:50:20.596410: Training Step 262/354: batchLoss = 0.5964, diffLoss = 2.9160, kgLoss = 0.0165
2025-04-08 20:50:22.229905: Training Step 263/354: batchLoss = 0.6390, diffLoss = 3.1329, kgLoss = 0.0155
2025-04-08 20:50:23.860734: Training Step 264/354: batchLoss = 0.3946, diffLoss = 1.9241, kgLoss = 0.0122
2025-04-08 20:50:25.483517: Training Step 265/354: batchLoss = 0.6552, diffLoss = 3.2074, kgLoss = 0.0172
2025-04-08 20:50:27.114192: Training Step 266/354: batchLoss = 0.5402, diffLoss = 2.6409, kgLoss = 0.0150
2025-04-08 20:50:28.745938: Training Step 267/354: batchLoss = 0.5661, diffLoss = 2.7735, kgLoss = 0.0143
2025-04-08 20:50:30.366183: Training Step 268/354: batchLoss = 0.4848, diffLoss = 2.3696, kgLoss = 0.0136
2025-04-08 20:50:31.981624: Training Step 269/354: batchLoss = 0.4520, diffLoss = 2.2106, kgLoss = 0.0124
2025-04-08 20:50:33.603120: Training Step 270/354: batchLoss = 0.4522, diffLoss = 2.2104, kgLoss = 0.0127
2025-04-08 20:50:35.219933: Training Step 271/354: batchLoss = 0.6027, diffLoss = 2.9585, kgLoss = 0.0137
2025-04-08 20:50:36.850377: Training Step 272/354: batchLoss = 0.5864, diffLoss = 2.8643, kgLoss = 0.0169
2025-04-08 20:50:38.488663: Training Step 273/354: batchLoss = 0.5002, diffLoss = 2.4444, kgLoss = 0.0141
2025-04-08 20:50:40.118358: Training Step 274/354: batchLoss = 0.4609, diffLoss = 2.2563, kgLoss = 0.0120
2025-04-08 20:50:41.750863: Training Step 275/354: batchLoss = 0.5653, diffLoss = 2.7656, kgLoss = 0.0153
2025-04-08 20:50:43.376129: Training Step 276/354: batchLoss = 0.4857, diffLoss = 2.3762, kgLoss = 0.0130
2025-04-08 20:50:45.002929: Training Step 277/354: batchLoss = 0.5208, diffLoss = 2.5413, kgLoss = 0.0157
2025-04-08 20:50:46.622311: Training Step 278/354: batchLoss = 0.6062, diffLoss = 2.9694, kgLoss = 0.0154
2025-04-08 20:50:48.239661: Training Step 279/354: batchLoss = 0.7006, diffLoss = 3.4281, kgLoss = 0.0188
2025-04-08 20:50:49.860368: Training Step 280/354: batchLoss = 0.5096, diffLoss = 2.4941, kgLoss = 0.0135
2025-04-08 20:50:51.481102: Training Step 281/354: batchLoss = 0.5631, diffLoss = 2.7552, kgLoss = 0.0151
2025-04-08 20:50:53.103641: Training Step 282/354: batchLoss = 0.6336, diffLoss = 3.1071, kgLoss = 0.0152
2025-04-08 20:50:54.735359: Training Step 283/354: batchLoss = 0.6090, diffLoss = 2.9804, kgLoss = 0.0161
2025-04-08 20:50:56.371034: Training Step 284/354: batchLoss = 0.5141, diffLoss = 2.5129, kgLoss = 0.0144
2025-04-08 20:50:57.998981: Training Step 285/354: batchLoss = 0.5244, diffLoss = 2.5636, kgLoss = 0.0145
2025-04-08 20:50:59.629302: Training Step 286/354: batchLoss = 0.7290, diffLoss = 3.5730, kgLoss = 0.0180
2025-04-08 20:51:01.264321: Training Step 287/354: batchLoss = 0.4585, diffLoss = 2.2464, kgLoss = 0.0115
2025-04-08 20:51:02.886138: Training Step 288/354: batchLoss = 0.5114, diffLoss = 2.4956, kgLoss = 0.0154
2025-04-08 20:51:04.502184: Training Step 289/354: batchLoss = 0.4815, diffLoss = 2.3521, kgLoss = 0.0139
2025-04-08 20:51:06.113851: Training Step 290/354: batchLoss = 0.5369, diffLoss = 2.6336, kgLoss = 0.0127
2025-04-08 20:51:07.727947: Training Step 291/354: batchLoss = 0.5138, diffLoss = 2.5194, kgLoss = 0.0124
2025-04-08 20:51:09.352669: Training Step 292/354: batchLoss = 0.5727, diffLoss = 2.8030, kgLoss = 0.0151
2025-04-08 20:51:10.981239: Training Step 293/354: batchLoss = 0.5508, diffLoss = 2.6946, kgLoss = 0.0148
2025-04-08 20:51:12.614244: Training Step 294/354: batchLoss = 0.5960, diffLoss = 2.9205, kgLoss = 0.0148
2025-04-08 20:51:14.236099: Training Step 295/354: batchLoss = 0.6454, diffLoss = 3.1635, kgLoss = 0.0159
2025-04-08 20:51:15.870128: Training Step 296/354: batchLoss = 0.4799, diffLoss = 2.3559, kgLoss = 0.0109
2025-04-08 20:51:17.502574: Training Step 297/354: batchLoss = 0.5602, diffLoss = 2.7470, kgLoss = 0.0135
2025-04-08 20:51:19.118583: Training Step 298/354: batchLoss = 0.5895, diffLoss = 2.8880, kgLoss = 0.0148
2025-04-08 20:51:20.739131: Training Step 299/354: batchLoss = 0.5308, diffLoss = 2.5991, kgLoss = 0.0138
2025-04-08 20:51:22.354892: Training Step 300/354: batchLoss = 0.5153, diffLoss = 2.5194, kgLoss = 0.0143
2025-04-08 20:51:23.974202: Training Step 301/354: batchLoss = 0.4712, diffLoss = 2.3108, kgLoss = 0.0113
2025-04-08 20:51:25.596863: Training Step 302/354: batchLoss = 0.4434, diffLoss = 2.1656, kgLoss = 0.0128
2025-04-08 20:51:27.219941: Training Step 303/354: batchLoss = 0.5767, diffLoss = 2.8273, kgLoss = 0.0141
2025-04-08 20:51:28.840479: Training Step 304/354: batchLoss = 0.5843, diffLoss = 2.8528, kgLoss = 0.0172
2025-04-08 20:51:30.471349: Training Step 305/354: batchLoss = 0.5573, diffLoss = 2.7296, kgLoss = 0.0142
2025-04-08 20:51:32.101788: Training Step 306/354: batchLoss = 0.4654, diffLoss = 2.2823, kgLoss = 0.0112
2025-04-08 20:51:33.720704: Training Step 307/354: batchLoss = 0.5034, diffLoss = 2.4577, kgLoss = 0.0148
2025-04-08 20:51:35.416266: Training Step 308/354: batchLoss = 0.5044, diffLoss = 2.4658, kgLoss = 0.0141
2025-04-08 20:51:37.032835: Training Step 309/354: batchLoss = 0.4460, diffLoss = 2.1741, kgLoss = 0.0140
2025-04-08 20:51:38.648643: Training Step 310/354: batchLoss = 0.5968, diffLoss = 2.9240, kgLoss = 0.0151
2025-04-08 20:51:40.277122: Training Step 311/354: batchLoss = 0.5798, diffLoss = 2.8397, kgLoss = 0.0148
2025-04-08 20:51:41.922251: Training Step 312/354: batchLoss = 0.5784, diffLoss = 2.8340, kgLoss = 0.0145
2025-04-08 20:51:43.550976: Training Step 313/354: batchLoss = 0.4763, diffLoss = 2.3252, kgLoss = 0.0141
2025-04-08 20:51:45.182077: Training Step 314/354: batchLoss = 0.7448, diffLoss = 3.6469, kgLoss = 0.0192
2025-04-08 20:51:46.806612: Training Step 315/354: batchLoss = 0.5147, diffLoss = 2.5179, kgLoss = 0.0138
2025-04-08 20:51:48.425379: Training Step 316/354: batchLoss = 0.4404, diffLoss = 2.1521, kgLoss = 0.0124
2025-04-08 20:51:50.050007: Training Step 317/354: batchLoss = 0.6303, diffLoss = 3.0826, kgLoss = 0.0172
2025-04-08 20:51:51.670175: Training Step 318/354: batchLoss = 0.5183, diffLoss = 2.5334, kgLoss = 0.0145
2025-04-08 20:51:53.284881: Training Step 319/354: batchLoss = 0.4864, diffLoss = 2.3769, kgLoss = 0.0138
2025-04-08 20:51:54.905135: Training Step 320/354: batchLoss = 0.5712, diffLoss = 2.7920, kgLoss = 0.0160
2025-04-08 20:51:56.526047: Training Step 321/354: batchLoss = 0.4856, diffLoss = 2.3752, kgLoss = 0.0132
2025-04-08 20:51:58.150644: Training Step 322/354: batchLoss = 0.5625, diffLoss = 2.7563, kgLoss = 0.0141
2025-04-08 20:51:59.774830: Training Step 323/354: batchLoss = 0.6439, diffLoss = 3.1545, kgLoss = 0.0162
2025-04-08 20:52:01.401210: Training Step 324/354: batchLoss = 0.7528, diffLoss = 3.6803, kgLoss = 0.0209
2025-04-08 20:52:03.027281: Training Step 325/354: batchLoss = 0.5815, diffLoss = 2.8444, kgLoss = 0.0158
2025-04-08 20:52:04.651870: Training Step 326/354: batchLoss = 0.4581, diffLoss = 2.2445, kgLoss = 0.0115
2025-04-08 20:52:06.277031: Training Step 327/354: batchLoss = 0.3957, diffLoss = 1.9275, kgLoss = 0.0127
2025-04-08 20:52:07.904787: Training Step 328/354: batchLoss = 0.5851, diffLoss = 2.8689, kgLoss = 0.0141
2025-04-08 20:52:09.530971: Training Step 329/354: batchLoss = 0.4890, diffLoss = 2.3983, kgLoss = 0.0117
2025-04-08 20:52:11.149997: Training Step 330/354: batchLoss = 0.7415, diffLoss = 3.6305, kgLoss = 0.0193
2025-04-08 20:52:12.767275: Training Step 331/354: batchLoss = 0.6225, diffLoss = 3.0516, kgLoss = 0.0152
2025-04-08 20:52:14.392222: Training Step 332/354: batchLoss = 0.5586, diffLoss = 2.6891, kgLoss = 0.0260
2025-04-08 20:52:16.023786: Training Step 333/354: batchLoss = 0.4370, diffLoss = 2.1426, kgLoss = 0.0106
2025-04-08 20:52:17.650030: Training Step 334/354: batchLoss = 0.5794, diffLoss = 2.8337, kgLoss = 0.0158
2025-04-08 20:52:19.279933: Training Step 335/354: batchLoss = 0.6202, diffLoss = 3.0365, kgLoss = 0.0161
2025-04-08 20:52:20.910771: Training Step 336/354: batchLoss = 0.5528, diffLoss = 2.6934, kgLoss = 0.0176
2025-04-08 20:52:22.537648: Training Step 337/354: batchLoss = 0.4573, diffLoss = 2.2325, kgLoss = 0.0136
2025-04-08 20:52:24.157311: Training Step 338/354: batchLoss = 0.5850, diffLoss = 2.8637, kgLoss = 0.0153
2025-04-08 20:52:25.778431: Training Step 339/354: batchLoss = 0.4886, diffLoss = 2.3985, kgLoss = 0.0112
2025-04-08 20:52:27.400480: Training Step 340/354: batchLoss = 0.5882, diffLoss = 2.8776, kgLoss = 0.0159
2025-04-08 20:52:29.025740: Training Step 341/354: batchLoss = 0.5710, diffLoss = 2.7812, kgLoss = 0.0185
2025-04-08 20:52:30.651038: Training Step 342/354: batchLoss = 0.5923, diffLoss = 2.8987, kgLoss = 0.0157
2025-04-08 20:52:32.279515: Training Step 343/354: batchLoss = 0.4996, diffLoss = 2.4363, kgLoss = 0.0154
2025-04-08 20:52:33.908601: Training Step 344/354: batchLoss = 0.5079, diffLoss = 2.4866, kgLoss = 0.0132
2025-04-08 20:52:35.535714: Training Step 345/354: batchLoss = 0.5243, diffLoss = 2.5611, kgLoss = 0.0151
2025-04-08 20:52:37.161047: Training Step 346/354: batchLoss = 0.4618, diffLoss = 2.2571, kgLoss = 0.0129
2025-04-08 20:52:38.780873: Training Step 347/354: batchLoss = 0.4936, diffLoss = 2.4121, kgLoss = 0.0140
2025-04-08 20:52:40.398415: Training Step 348/354: batchLoss = 0.6790, diffLoss = 3.3251, kgLoss = 0.0175
2025-04-08 20:52:42.034897: Training Step 349/354: batchLoss = 0.5138, diffLoss = 2.5113, kgLoss = 0.0144
2025-04-08 20:52:43.657404: Training Step 350/354: batchLoss = 0.4793, diffLoss = 2.3458, kgLoss = 0.0127
2025-04-08 20:52:45.270809: Training Step 351/354: batchLoss = 0.4070, diffLoss = 1.9896, kgLoss = 0.0114
2025-04-08 20:52:46.880932: Training Step 352/354: batchLoss = 0.5774, diffLoss = 2.8241, kgLoss = 0.0158
2025-04-08 20:52:48.296258: Training Step 353/354: batchLoss = 0.5596, diffLoss = 2.7350, kgLoss = 0.0158
2025-04-08 20:52:48.387028: 
2025-04-08 20:52:48.387643: Epoch 39/1000, Train: epLoss = 0.9850, epDfLoss = 4.8208, epfTransLoss = 0.0000, epKgLoss = 0.0261  
2025-04-08 20:52:49.691812: Steps 0/138: batch_recall = 47.47, batch_ndcg = 26.72 
2025-04-08 20:52:51.017806: Steps 1/138: batch_recall = 48.01, batch_ndcg = 28.31 
2025-04-08 20:52:52.336679: Steps 2/138: batch_recall = 58.48, batch_ndcg = 35.72 
2025-04-08 20:52:53.673381: Steps 3/138: batch_recall = 57.13, batch_ndcg = 33.60 
2025-04-08 20:52:54.971523: Steps 4/138: batch_recall = 67.34, batch_ndcg = 40.90 
2025-04-08 20:52:56.280652: Steps 5/138: batch_recall = 58.75, batch_ndcg = 31.53 
2025-04-08 20:52:57.578244: Steps 6/138: batch_recall = 51.66, batch_ndcg = 31.98 
2025-04-08 20:52:58.887862: Steps 7/138: batch_recall = 61.84, batch_ndcg = 41.72 
2025-04-08 20:53:00.187367: Steps 8/138: batch_recall = 61.38, batch_ndcg = 38.65 
2025-04-08 20:53:01.497280: Steps 9/138: batch_recall = 58.97, batch_ndcg = 34.75 
2025-04-08 20:53:02.797818: Steps 10/138: batch_recall = 57.58, batch_ndcg = 33.34 
2025-04-08 20:53:04.097394: Steps 11/138: batch_recall = 58.89, batch_ndcg = 33.94 
2025-04-08 20:53:05.410204: Steps 12/138: batch_recall = 49.14, batch_ndcg = 29.21 
2025-04-08 20:53:06.706161: Steps 13/138: batch_recall = 53.60, batch_ndcg = 32.06 
2025-04-08 20:53:08.006076: Steps 14/138: batch_recall = 56.03, batch_ndcg = 32.60 
2025-04-08 20:53:09.300745: Steps 15/138: batch_recall = 47.83, batch_ndcg = 30.21 
2025-04-08 20:53:10.604709: Steps 16/138: batch_recall = 59.78, batch_ndcg = 34.44 
2025-04-08 20:53:11.892076: Steps 17/138: batch_recall = 57.15, batch_ndcg = 33.31 
2025-04-08 20:53:13.187491: Steps 18/138: batch_recall = 53.81, batch_ndcg = 33.59 
2025-04-08 20:53:14.485836: Steps 19/138: batch_recall = 54.29, batch_ndcg = 32.78 
2025-04-08 20:53:15.777128: Steps 20/138: batch_recall = 61.30, batch_ndcg = 35.43 
2025-04-08 20:53:17.071167: Steps 21/138: batch_recall = 68.39, batch_ndcg = 40.52 
2025-04-08 20:53:18.373429: Steps 22/138: batch_recall = 57.31, batch_ndcg = 32.79 
2025-04-08 20:53:19.681369: Steps 23/138: batch_recall = 52.13, batch_ndcg = 31.08 
2025-04-08 20:53:20.992746: Steps 24/138: batch_recall = 58.10, batch_ndcg = 31.80 
2025-04-08 20:53:22.293352: Steps 25/138: batch_recall = 62.72, batch_ndcg = 35.71 
2025-04-08 20:53:23.595602: Steps 26/138: batch_recall = 58.38, batch_ndcg = 33.52 
2025-04-08 20:53:24.898389: Steps 27/138: batch_recall = 56.39, batch_ndcg = 32.11 
2025-04-08 20:53:26.193857: Steps 28/138: batch_recall = 58.44, batch_ndcg = 33.19 
2025-04-08 20:53:27.488002: Steps 29/138: batch_recall = 58.99, batch_ndcg = 31.22 
2025-04-08 20:53:28.781873: Steps 30/138: batch_recall = 59.08, batch_ndcg = 34.47 
2025-04-08 20:53:30.072003: Steps 31/138: batch_recall = 48.07, batch_ndcg = 27.52 
2025-04-08 20:53:31.351412: Steps 32/138: batch_recall = 54.50, batch_ndcg = 31.96 
2025-04-08 20:53:32.637826: Steps 33/138: batch_recall = 57.32, batch_ndcg = 32.46 
2025-04-08 20:53:33.937254: Steps 34/138: batch_recall = 54.73, batch_ndcg = 29.12 
2025-04-08 20:53:35.238216: Steps 35/138: batch_recall = 50.08, batch_ndcg = 29.61 
2025-04-08 20:53:36.533222: Steps 36/138: batch_recall = 45.46, batch_ndcg = 26.33 
2025-04-08 20:53:37.829188: Steps 37/138: batch_recall = 58.92, batch_ndcg = 34.34 
2025-04-08 20:53:39.123760: Steps 38/138: batch_recall = 58.51, batch_ndcg = 32.64 
2025-04-08 20:53:40.407513: Steps 39/138: batch_recall = 67.01, batch_ndcg = 37.95 
2025-04-08 20:53:41.705220: Steps 40/138: batch_recall = 60.50, batch_ndcg = 30.61 
2025-04-08 20:53:42.992375: Steps 41/138: batch_recall = 61.34, batch_ndcg = 34.97 
2025-04-08 20:53:44.269872: Steps 42/138: batch_recall = 55.21, batch_ndcg = 30.54 
2025-04-08 20:53:45.553857: Steps 43/138: batch_recall = 59.03, batch_ndcg = 37.40 
2025-04-08 20:53:46.844309: Steps 44/138: batch_recall = 55.19, batch_ndcg = 30.35 
2025-04-08 20:53:48.131563: Steps 45/138: batch_recall = 63.11, batch_ndcg = 36.17 
2025-04-08 20:53:49.414437: Steps 46/138: batch_recall = 62.92, batch_ndcg = 36.48 
2025-04-08 20:53:50.699104: Steps 47/138: batch_recall = 53.17, batch_ndcg = 31.97 
2025-04-08 20:53:51.984471: Steps 48/138: batch_recall = 59.95, batch_ndcg = 35.65 
2025-04-08 20:53:53.265463: Steps 49/138: batch_recall = 63.54, batch_ndcg = 37.42 
2025-04-08 20:53:54.541265: Steps 50/138: batch_recall = 57.41, batch_ndcg = 30.24 
2025-04-08 20:53:55.821150: Steps 51/138: batch_recall = 63.01, batch_ndcg = 35.50 
2025-04-08 20:53:57.106916: Steps 52/138: batch_recall = 70.19, batch_ndcg = 42.50 
2025-04-08 20:53:58.471194: Steps 53/138: batch_recall = 63.52, batch_ndcg = 34.22 
2025-04-08 20:53:59.744635: Steps 54/138: batch_recall = 68.81, batch_ndcg = 38.37 
2025-04-08 20:54:01.021277: Steps 55/138: batch_recall = 60.49, batch_ndcg = 34.16 
2025-04-08 20:54:02.294890: Steps 56/138: batch_recall = 61.38, batch_ndcg = 36.10 
2025-04-08 20:54:03.580272: Steps 57/138: batch_recall = 56.49, batch_ndcg = 32.07 
2025-04-08 20:54:04.877896: Steps 58/138: batch_recall = 63.98, batch_ndcg = 34.95 
2025-04-08 20:54:06.166268: Steps 59/138: batch_recall = 62.72, batch_ndcg = 37.51 
2025-04-08 20:54:07.457293: Steps 60/138: batch_recall = 65.93, batch_ndcg = 38.01 
2025-04-08 20:54:08.743419: Steps 61/138: batch_recall = 63.32, batch_ndcg = 34.77 
2025-04-08 20:54:10.028601: Steps 62/138: batch_recall = 81.96, batch_ndcg = 44.70 
2025-04-08 20:54:11.324720: Steps 63/138: batch_recall = 75.51, batch_ndcg = 42.83 
2025-04-08 20:54:12.617767: Steps 64/138: batch_recall = 59.14, batch_ndcg = 31.83 
2025-04-08 20:54:13.900866: Steps 65/138: batch_recall = 84.33, batch_ndcg = 46.76 
2025-04-08 20:54:15.185146: Steps 66/138: batch_recall = 68.62, batch_ndcg = 40.88 
2025-04-08 20:54:16.465994: Steps 67/138: batch_recall = 74.56, batch_ndcg = 45.59 
2025-04-08 20:54:17.746669: Steps 68/138: batch_recall = 60.96, batch_ndcg = 33.55 
2025-04-08 20:54:19.023506: Steps 69/138: batch_recall = 85.88, batch_ndcg = 51.34 
2025-04-08 20:54:20.306831: Steps 70/138: batch_recall = 79.28, batch_ndcg = 44.96 
2025-04-08 20:54:21.612418: Steps 71/138: batch_recall = 86.95, batch_ndcg = 52.59 
2025-04-08 20:54:22.908356: Steps 72/138: batch_recall = 79.08, batch_ndcg = 47.26 
2025-04-08 20:54:24.203306: Steps 73/138: batch_recall = 83.40, batch_ndcg = 46.37 
2025-04-08 20:54:25.493896: Steps 74/138: batch_recall = 78.32, batch_ndcg = 48.37 
2025-04-08 20:54:26.780291: Steps 75/138: batch_recall = 81.40, batch_ndcg = 48.71 
2025-04-08 20:54:28.078466: Steps 76/138: batch_recall = 93.49, batch_ndcg = 55.33 
2025-04-08 20:54:29.364009: Steps 77/138: batch_recall = 92.94, batch_ndcg = 51.97 
2025-04-08 20:54:30.647448: Steps 78/138: batch_recall = 89.03, batch_ndcg = 47.70 
2025-04-08 20:54:31.926955: Steps 79/138: batch_recall = 85.55, batch_ndcg = 48.16 
2025-04-08 20:54:33.202908: Steps 80/138: batch_recall = 71.25, batch_ndcg = 38.69 
2025-04-08 20:54:34.482606: Steps 81/138: batch_recall = 80.55, batch_ndcg = 48.56 
2025-04-08 20:54:35.761250: Steps 82/138: batch_recall = 86.68, batch_ndcg = 53.46 
2025-04-08 20:54:37.048626: Steps 83/138: batch_recall = 82.24, batch_ndcg = 47.89 
2025-04-08 20:54:38.329317: Steps 84/138: batch_recall = 97.63, batch_ndcg = 56.13 
2025-04-08 20:54:39.612102: Steps 85/138: batch_recall = 104.82, batch_ndcg = 60.28 
2025-04-08 20:54:40.898891: Steps 86/138: batch_recall = 121.54, batch_ndcg = 72.46 
2025-04-08 20:54:42.188575: Steps 87/138: batch_recall = 102.06, batch_ndcg = 54.42 
2025-04-08 20:54:43.480644: Steps 88/138: batch_recall = 102.22, batch_ndcg = 59.31 
2025-04-08 20:54:44.757126: Steps 89/138: batch_recall = 120.63, batch_ndcg = 67.90 
2025-04-08 20:54:46.041762: Steps 90/138: batch_recall = 101.16, batch_ndcg = 56.65 
2025-04-08 20:54:47.317624: Steps 91/138: batch_recall = 116.17, batch_ndcg = 64.70 
2025-04-08 20:54:48.596522: Steps 92/138: batch_recall = 118.21, batch_ndcg = 65.78 
2025-04-08 20:54:49.871582: Steps 93/138: batch_recall = 119.25, batch_ndcg = 67.33 
2025-04-08 20:54:51.144880: Steps 94/138: batch_recall = 124.08, batch_ndcg = 66.83 
2025-04-08 20:54:52.422179: Steps 95/138: batch_recall = 113.65, batch_ndcg = 67.18 
2025-04-08 20:54:53.712667: Steps 96/138: batch_recall = 124.38, batch_ndcg = 73.35 
2025-04-08 20:54:54.996054: Steps 97/138: batch_recall = 138.77, batch_ndcg = 82.42 
2025-04-08 20:54:56.282803: Steps 98/138: batch_recall = 106.04, batch_ndcg = 63.48 
2025-04-08 20:54:57.552289: Steps 99/138: batch_recall = 124.56, batch_ndcg = 71.98 
2025-04-08 20:54:58.834409: Steps 100/138: batch_recall = 123.23, batch_ndcg = 69.82 
2025-04-08 20:55:00.117639: Steps 101/138: batch_recall = 127.83, batch_ndcg = 69.61 
2025-04-08 20:55:01.406138: Steps 102/138: batch_recall = 121.58, batch_ndcg = 69.59 
2025-04-08 20:55:02.677624: Steps 103/138: batch_recall = 139.90, batch_ndcg = 78.67 
2025-04-08 20:55:03.952111: Steps 104/138: batch_recall = 131.03, batch_ndcg = 77.63 
2025-04-08 20:55:05.216893: Steps 105/138: batch_recall = 124.12, batch_ndcg = 69.15 
2025-04-08 20:55:06.492933: Steps 106/138: batch_recall = 103.85, batch_ndcg = 60.17 
2025-04-08 20:55:07.770441: Steps 107/138: batch_recall = 117.20, batch_ndcg = 65.09 
2025-04-08 20:55:09.055837: Steps 108/138: batch_recall = 116.49, batch_ndcg = 71.28 
2025-04-08 20:55:10.333660: Steps 109/138: batch_recall = 133.58, batch_ndcg = 75.64 
2025-04-08 20:55:11.620912: Steps 110/138: batch_recall = 119.42, batch_ndcg = 64.20 
2025-04-08 20:55:12.900343: Steps 111/138: batch_recall = 135.94, batch_ndcg = 85.49 
2025-04-08 20:55:14.177125: Steps 112/138: batch_recall = 150.19, batch_ndcg = 87.50 
2025-04-08 20:55:15.468203: Steps 113/138: batch_recall = 118.92, batch_ndcg = 67.46 
2025-04-08 20:55:16.755553: Steps 114/138: batch_recall = 123.43, batch_ndcg = 72.65 
2025-04-08 20:55:18.036997: Steps 115/138: batch_recall = 119.85, batch_ndcg = 64.39 
2025-04-08 20:55:19.316131: Steps 116/138: batch_recall = 124.59, batch_ndcg = 67.51 
2025-04-08 20:55:20.585435: Steps 117/138: batch_recall = 109.30, batch_ndcg = 64.93 
2025-04-08 20:55:21.866743: Steps 118/138: batch_recall = 118.87, batch_ndcg = 67.88 
2025-04-08 20:55:23.131055: Steps 119/138: batch_recall = 142.02, batch_ndcg = 77.76 
2025-04-08 20:55:24.411817: Steps 120/138: batch_recall = 123.41, batch_ndcg = 71.78 
2025-04-08 20:55:25.688347: Steps 121/138: batch_recall = 144.97, batch_ndcg = 77.50 
2025-04-08 20:55:26.971096: Steps 122/138: batch_recall = 150.99, batch_ndcg = 81.15 
2025-04-08 20:55:28.251610: Steps 123/138: batch_recall = 130.85, batch_ndcg = 74.80 
2025-04-08 20:55:29.531241: Steps 124/138: batch_recall = 153.39, batch_ndcg = 93.24 
2025-04-08 20:55:30.811287: Steps 125/138: batch_recall = 137.13, batch_ndcg = 74.90 
2025-04-08 20:55:32.079744: Steps 126/138: batch_recall = 158.02, batch_ndcg = 86.88 
2025-04-08 20:55:33.357703: Steps 127/138: batch_recall = 144.47, batch_ndcg = 82.63 
2025-04-08 20:55:34.625195: Steps 128/138: batch_recall = 122.83, batch_ndcg = 70.02 
2025-04-08 20:55:35.894888: Steps 129/138: batch_recall = 161.36, batch_ndcg = 92.32 
2025-04-08 20:55:37.162078: Steps 130/138: batch_recall = 133.52, batch_ndcg = 70.36 
2025-04-08 20:55:38.428827: Steps 131/138: batch_recall = 151.05, batch_ndcg = 86.61 
2025-04-08 20:55:39.703051: Steps 132/138: batch_recall = 148.65, batch_ndcg = 83.71 
2025-04-08 20:55:40.984296: Steps 133/138: batch_recall = 146.68, batch_ndcg = 84.90 
2025-04-08 20:55:42.260049: Steps 134/138: batch_recall = 142.38, batch_ndcg = 83.82 
2025-04-08 20:55:43.534652: Steps 135/138: batch_recall = 164.66, batch_ndcg = 95.14 
2025-04-08 20:55:44.803279: Steps 136/138: batch_recall = 152.44, batch_ndcg = 80.40 
2025-04-08 20:55:46.081211: Steps 137/138: batch_recall = 138.18, batch_ndcg = 87.29 
2025-04-08 20:55:46.081733: Epoch 39/1000, Test: Recall = 0.1745, NDCG = 0.1001  

------------------
Exiting from training early
Best epoch :  20  , Recall :  0.17806158813546683  , NDCG :  0.10085646390621425

------------------------------------------------------------
Sender: LSF System <XXX>
Subject: Job 8133545: <RecommenderSys> in cluster <XXX> Done

Job <RecommenderSys> was submitted from host <XXX> by user <XXX> in cluster <XXX> at Tue Apr  8 12:28:00 2025
Job was executed on host(s) <XXX>, in queue <batch_a100>, as user <XXX> in cluster <XXX> at Tue Apr  8 12:34:09 2025
</home/XXX> was used as the home directory.
</home/XXX/recommenderSys/DiKGRec> was used as the working directory.
Started at Tue Apr  8 12:34:09 2025
Terminated at Tue Apr  8 20:55:52 2025
Results reported at Tue Apr  8 20:55:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python Main.py --data amazon-book --lr 5e-5 --lr2 5e-3 --kg_loss_ratio 0.8 --updateW 1 --oriW 1 --batch 200 --layer 4 --head 1
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   31438.00 sec.
    Max Memory :                                 4481 MB
    Average Memory :                             4109.53 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               5519.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   30111 sec.
    Turnaround time :                            30472 sec.

The output (if any) is above this job summary.



PS:

Read file <recommander.8133545.stderr> for stderr output of this job.

